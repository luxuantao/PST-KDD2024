<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Stream Multi-Class Fusion of Deep Networks for Video Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
							<email>zxwu@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Shanghai Key Lab of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Shanghai Key Lab of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Shanghai Key Lab of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Shanghai Key Lab of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
							<email>xyxue@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Shanghai Key Lab of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Stream Multi-Class Fusion of Deep Networks for Video Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">07A6D702479CD0C628E396083F20C591</idno>
					<idno type="DOI">10.1145/2964284.2964328</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video Classification</term>
					<term>CNN</term>
					<term>LSTM</term>
					<term>Fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper studies deep network architectures to address the problem of video classification. A multi-stream framework is proposed to fully utilize the rich multimodal information in videos. Specifically, we first train three Convolutional Neural Networks to model spatial, short-term motion and audio clues respectively. Long Short Term Memory networks are then adopted to explore long-term temporal dynamics. With the outputs of the individual streams on multiple classes, we propose to mine class relationships hidden in the data from the trained models. The automatically discovered relationships are then leveraged in the multi-stream multi-class fusion process as a prior, indicating which and how much information is needed from the remaining classes, to adaptively determine the optimal fusion weights for generating the final scores of each class. Our contributions are two-fold. First, the multi-stream framework is able to exploit multimodal features that are more comprehensive than those previously attempted. Second, our proposed fusion method not only learns the best weights of the multiple network streams for each class, but also takes class relationship into account, which is known as a helpful clue in multi-class visual classification tasks. Our framework produces significantly better results than the state of the arts on two popular benchmarks, 92.2% on UCF-101 (without using audio) and 84.9% on Columbia Consumer Videos.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The sheer volume of video data nowadays demands robust video classification techniques that can effectively recognize human actions and complex events for applications like video search, summarization, intelligent surveillance and etc. However, it is a particularly challenging problem due to * Corresponding author.</p><p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.  the complicated nature of videos, including large intra-class variations caused by different viewing conditions and multiple view points, noisy contents unrelated to the video topic, and complex temporal structures incurring understanding and computational difficulties. The fact that videos are intrinsically multimodal requires solutions that can explore not only static visual information, but also motion and auditory clues. Key to the development of video classification systems is the design of good features. Popular feature descriptors include the SIFT <ref type="bibr" target="#b33">[33]</ref>, the Mel-Frequency Cepstral Coefficients (MFCC) <ref type="bibr" target="#b67">[66]</ref>, the STIP <ref type="bibr" target="#b31">[31]</ref> and the dense trajectories <ref type="bibr" target="#b56">[56]</ref>, which can be encoded into video-level representations by bag-of-words (BoW) <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b69">68,</ref><ref type="bibr" target="#b37">37]</ref> or Fisher vectors (FV) <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b71">70]</ref>.</p><p>In contrast to the hand-engineered descriptors, the deep neural networks that can learn features automatically from raw data have demonstrated strong performance in various domains. In particular, the convolutional neural networks (ConvNets) are very successful on image analysis tasks like object detection <ref type="bibr" target="#b14">[14]</ref>, object recognition <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b51">51]</ref> and image segmentation <ref type="bibr" target="#b11">[11]</ref>. However, for video classification, most deep network based approaches <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b63">62]</ref> demonstrated worse or similar results to the hand-engineered features <ref type="bibr" target="#b56">[56]</ref>. This is largely due to the high complexity of the video data. Unlike images that only have static visual appearance information, videos also contain temporal motions and auditory soundtracks. For example, a "diving" action video usually involves a sequence of atoms, such as "jumping from a platform", "rotating in the air" and "falling into water", accompanied by cheering or clapping sounds. Some approaches <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b45">45]</ref> only focused on the static frames and short-term motion clues captured by a few adjacent frames, which are apparently not sufficient. A few very recent studies attempted to use recurrent neural networks (RNN) to model long-term temporal information and achieved competitive performance <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b65">64]</ref>. Nevertheless, the audio information has rarely been exploited. Furthermore, most existing approaches fused the outputs of multiple networks in a very straightforward way using simple classifiers like logistic regression, which could lead to sub-optimal performance.</p><p>In addition, existing works for video categorization often assign single or multiple class labels to a video sample independently without considering the relationships among video semantics. However, humans do not recognize an object (concept) separately but rely on the interconnections of objects (concepts). The presence of related classes could help better categorize the class of interest. For example, "marathon" and "marching band" contain similar human motion patterns (at least when compared with unrelated class pairs like "marathon" and "fishing"), and the confidence of a video containing "marathon" could potentially help recognize "marching band". In other words, if a video receives extremely low score of "marathon", it is also unlikely to be "marching band". To leverage semantic relationships (i.e., context knowledge), many existing methods rely on computational expensive models (e.g., CRF), which are not feasible for large-scale applications.</p><p>Realizing the above limitations, in this paper, we propose a multi-stream framework of deep neural networks to exploit the multimodal clues for video classification. Figure <ref type="figure" target="#fig_1">1</ref> illustrates the diagram of our approach. Three ConvNets are trained to model the static spatial information, short-term motion and auditory clues, respectively. The motion stream is computed on stacked optical flows over a short temporal windows and thus can only capture short-term motion. In order to model the long-term temporal clues, we employ a Recurrent Neural Network (RNN) model, namely the Long Short Term Memory (LSTM), on the frame-level spatial and motion features extracted by the ConvNets. The LSTM encodes history information in memory units regulated with non-linear gates to discover temporal dependencies. To combine the outputs from different networks, we develop a simple yet effective fusion method to learn the optimal fusion weights adaptively for each class. Note that the deep models are trained using state-of-the-art networks, they possess high discriminative power and hence contain valuable knowledge on how classes are correlated. Therefore, we propose to leverage the class relationships hidden in the data to constrain the learning process, by informing the classifier which classes are related and how much information is needed from the each of these related classes. In other words, to generate the final prediction of a class of interest, the classifier also considers the predictions of other correlated classes.</p><p>Our contributions are summarized as follows:</p><p>1. We introduce a multi-stream framework that integrates spatial, short-term motion, long-term temporal and auditory clues in videos. We demonstrate the multistream networks are able to digest complementary clues to receive significantly improved performance.</p><p>2. We propose a multi-stream multi-class fusion method to combine the outputs of the individual networks. The method not only learns the weights of individual network streams adaptively for each class, but also harnesses class relationships that can further improve the performance.</p><p>3. We conduct extensive experiments to validate the performance of the proposed framework, and we achieved superior performance on two popular datasets.</p><p>The rest of this paper is organized as follows. Section 2 reviews and discusses related works. Section 3 describes the proposed multi-stream multi-class framework in detail. Experimental results and comparisons are discussed in Section 4, followed by conclusions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORKS</head><p>As aforementioned, video classification has been extensively studied and significant efforts have been paid to design discriminative features or robust classifiers. We focus the review on recent works related to our proposed approach.</p><p>Hand-crafted Representations. There have been numerous works focusing on developing effective features that are expected to be robust to withstand intra-class variations and discriminative to separate different categories. For example, one can utilize image-based shape features, such as HOG and SIFT <ref type="bibr" target="#b33">[33]</ref>, to capture appearance information on individual frames. Different from frame-based features, motion features are designed to take the object movements into account, which is appealing since motion information is critical for understanding video contents. A popular way to obtain motion features is by extending frame-based local features into 3D space. For instance, Laptev et al. <ref type="bibr" target="#b31">[31]</ref> extended the Harris detector into 3D space to find space-time interest points. Instead of locating interest points using 3D detectors, Wang et al. obtained better performance on video classification tasks by sampling patches densely <ref type="bibr" target="#b58">[58]</ref>. In a later work, Wang et al. adopted the dense point trajectories, upon which several features are extracted from regions that are tracked with optical flow <ref type="bibr" target="#b56">[56]</ref>. In addition, audio features are also adopted as a complement of the visual channel, among which the Mel-frequency cepstral coefficients (MFCC) is the most popular one. CNN Representations. Motived by the promising results of deep networks (particularly the ConvNets) on image analysis tasks <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b14">14]</ref>, several works have exploited deep architectures for video classification. Ji et al. extended CNN models into spatial-temporal space by operating on stacked video frames <ref type="bibr" target="#b22">[22]</ref>. Karparthy et al. compared several architectures for action recognition <ref type="bibr" target="#b26">[26]</ref>. Tran et al. proposed to learn generic spatial-temporal features which can be computed efficiently <ref type="bibr" target="#b53">[53]</ref>. Xu et al. adopted advanced feature encoding strategies (i.e., VLAD) to promote the generalization ability of CNN representations <ref type="bibr" target="#b66">[65]</ref>. Zha et al. evaluated several options of using CNNs for event detection <ref type="bibr" target="#b71">[70]</ref>.</p><p>Simonyan and Zisserman <ref type="bibr" target="#b45">[45]</ref> introduced an interesting twostream approach, where two ConvNets are trained to explicitly capture spatial and short-term motion information using frames and stacked optical flows as inputs, respectively. Final predictions can be obtained by linearly averaging the prediction scores of the two ConvNets. A recent work by Wang et al. <ref type="bibr" target="#b59">[59]</ref> combined the two-stream approach with the traditional dense trajectories <ref type="bibr" target="#b56">[56]</ref> and reported strong results. In this paper, we also adopt two similar ConvNets as <ref type="bibr" target="#b45">[45]</ref>. However, as the two-stream approach is not able to model the auditory and the long-term temporal clues, we adopt additional networks to build a more comprehensive framework. A novel fusion method is also proposed to combine the multi-stream outputs, which is better than the simple linear fusion used in <ref type="bibr" target="#b45">[45]</ref>.</p><p>Temporal Structure. Extensive works have been conducted to explore the temporal dynamics in videos. For example, Tang et al. introduced a HMM model to capture the changes of states for videos with variable durations <ref type="bibr" target="#b52">[52]</ref>. Wang et al. combined feature templates with parts in a maxmargin hidden CRF framework <ref type="bibr" target="#b62">[61]</ref>. In addition to using graphical models, Fernando et al. proposed to train a linear ranking machine on the frames of a video, whose parameters will then be used to obtain a video-level representation <ref type="bibr" target="#b12">[12]</ref>. Ramanathan introduced an unsupervised way to learn temporal embeddings using context information like word vectors <ref type="bibr" target="#b42">[42]</ref>. More recently, RNN has been shown to be effective on many sequential modeling tasks, such as speech recognition <ref type="bibr" target="#b15">[15]</ref> and image/video analysis <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b68">67,</ref><ref type="bibr" target="#b72">71]</ref>. For long-term temporal modeling of the video data, Srivastava et al. proposed an LSTM encoder-decoder framework to learn video representations in an unsupervised manner <ref type="bibr" target="#b48">[48]</ref>. Donahua et al. <ref type="bibr" target="#b9">[9]</ref> and Wu et al. <ref type="bibr" target="#b65">[64]</ref> trained a two-layer LSTM network for action classification. Ng et al. <ref type="bibr" target="#b39">[39]</ref> further demonstrated that a five-layer LSTM network is slightly better. Veeriah et al. proposed a differential gating scheme for LSTM to emphasize on the change in information gain <ref type="bibr" target="#b55">[55]</ref>. More recently, Sharma et al. incorporated the attention mechanism into LSTM to identify the most relevant information (e.g., objects) for recognizing actions <ref type="bibr" target="#b44">[44]</ref>.</p><p>Fusion. Videos naturally contain abundant clues, and hence a decent video categorization system often fuses multiple sources of information for improved recognition performance <ref type="bibr" target="#b70">[69,</ref><ref type="bibr" target="#b20">20]</ref>. The simplest fusion strategy is linear weighted fusion, which has been adopted in many recent approaches like <ref type="bibr" target="#b45">[45]</ref>. Nandakumar et al. performed score fusion using a method called likelihood ratio test <ref type="bibr" target="#b36">[36]</ref>. More recently, Xu et al. <ref type="bibr" target="#b67">[66]</ref> and Ye et al. <ref type="bibr" target="#b69">[68]</ref> proposed robust late fusion methods by seeking a low rank matrix to remove the noise of individually trained classifiers. Liu et al. <ref type="bibr" target="#b32">[32]</ref> proposed to predict sample-specific weights in the fusion process. There are also a few works attempting to fuse multiple features with deep neural networks. Srivastava et al. proposed to combine features using deep Boltzmann Machines <ref type="bibr" target="#b49">[49]</ref>. Neverova formulated the fusion problem as a modality dropping process <ref type="bibr" target="#b38">[38]</ref>. Our paper relies on state-of-the-art deep models to characterize videos from different perspectives, and then performs a simple late fusion approach to further combine scores from multiple streams for final predictions.</p><p>Class Relationships. There are many studies using class relationships (a.k.a. context) to improve multi-class visual recognition performance. For instance, Rabinovich et al. uti-lized a Conditional Random Field (CRF) model to maximize object label agreement based on contextual relevance <ref type="bibr" target="#b41">[41]</ref>. Jiang et al. used a semantic diffusion algorithm to incorporate class relationships <ref type="bibr" target="#b24">[24]</ref>. Deng et al. proposed to jointly train a hierarchy and exclusion graph model with a Con-vNet to learn class relations for image classification <ref type="bibr" target="#b8">[8]</ref>. Assari et al. exploited class co-occurrences for improved video classification <ref type="bibr" target="#b2">[2]</ref>. Wu et al. <ref type="bibr" target="#b64">[63]</ref> proposed a regularized neural network to fuse features and explore class relationships, but used traditional hand-engineered features as inputs. Recently, Chen and Gupta utilized the class correlations in the form of confusion matrix to refine the classification scores from the softmax layer <ref type="bibr">[6]</ref>. Note that Multi-Task Learning (MTL) also attempts to improve recognition performance by enforcing similar zero/nonzero patterns in the weight matrix through structural norms, which enables knowledge sharing among highly related tasks (classes). In our work, we adopt a data-driven approach, mining the knowledge learned by the models themselves, to borrow useful information from classes with relatedness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">THE PROPOSED APPROACH</head><p>In this section, we first describe the individual network streams and then introduce the proposed fusion method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-Stream ConvNets</head><p>Carrying abundant multimodal information, videos normally show the movements and interactions of objects under certain scenes over time, accompanied by human voices or background sounds. Therefore, video data can be naturally decomposed into spatial, motion and audio streams. The spatial stream consisting of individual frames depicts the static appearance information, while the motion stream captures object or scene movements demonstrated by continuous frames. In addition, sounds in the audio stream provide crucial clues that are often complementary to the visual counterpart. Motivated by the recent two-stream approach <ref type="bibr" target="#b45">[45]</ref>, we train three ConvNets to exploit the multimodal information, as described below.</p><p>In brief, the spatial ConvNet uses the raw frames as inputs, where we adopt a deep architecture with superior performance on image recognition tasks <ref type="bibr" target="#b46">[46]</ref>. It can effectively recognize certain video semantics that have clear and discriminative appearance characteristics. For the motion stream, we train a ConvNet model operating on stacked optical flows following <ref type="bibr" target="#b45">[45]</ref>. More specifically, through computing displacement vectors in both horizontal and vertical ways, the optical flows encode subtle motion patterns of objects between each pair of adjacent frames, which can be converted into two flow images as the inputs of the motion stream Con-vNet. Previous studies have shown that further improvements can be obtained by stacking consecutive optical flow images in a short time window, owing to the inclusion of relatively more compact movements <ref type="bibr" target="#b45">[45]</ref>. In order to leverage the audio information, we first apply the Short-Time Fourier Transformation to convert the 1-d soundtrack into a 2-D image (namely spectrogram) with the horizontal axis and vertical axis being time-scale and frequency-scale respectively. Then we employ a ConvNet to operate on the spectrograms as suggested in <ref type="bibr" target="#b54">[54]</ref>. Notice that the ConvNet is well suited for modeling audio signals based on spectrograms with the weight sharing and max pooling mechanism to strive invariance of small frequency shifts <ref type="bibr" target="#b1">[1]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Long Term Temporal Modeling</head><p>As the motion stream ConvNet only captures short-term motion patterns, we further employ LSTM <ref type="bibr" target="#b18">[18]</ref> to model long-term temporal clues in the visual channel. LSTM is able to exploit temporal information of a data sequence with arbitrary length through recursively mapping the input sequence to output labels with hidden units. Each of the units maintains a built-in memory cell, which stores information over time guarded by several non-linear gate units to control the amount of changes and influence of the memory contents. To keep this paper self-contained, we briefly introduce LSTM as follows.</p><p>Figure <ref type="figure" target="#fig_2">2</ref> illustrates the typical structure of a hidden LSTM unit. In our framework, we denote xt as the feature representation of a video frame or a stacked optical flow image at the t-th time step. Generally, an LSTM maps an input sequence (x1, x2, . . . , xT ) to output labels (y1, y2, . . . , yT ) through computing activations of the units in the network recursively from t = 1 to t = T . At time t, the activation vectors of memory cell ct, output gate ot and hidden state ht are computed as:</p><formula xml:id="formula_0">ct = ft ct-1 + it tanh(Wxcxt + W hc ht-1 + bc), ot = σ(Wxoxt + W ho ht-1 + Wcoct + bo), ht = ot tanh(ct),<label>(1)</label></formula><p>where Wxc, W hc , Wxo, W ho , Wco are the weight matrices connecting two different units. bc, bo are the bias terms, σ is the sigmoid function, and is an element-wise product operator. Notice that it and ft are the activation vectors of input and forget gates, which are calculated with weight matrices as:</p><formula xml:id="formula_1">it = σ(Wxixt + W hi ht-1 + Wcict-1 + bi), ft = σ(W xf xt + W hf ht-1 + W cf ct-1 + b f ). (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>From the above equations, the contents of the memory cell at the t-th time step ct is computed as the weighted sum of the current inputs and the previous memory contents ct-1. The input and forget gates (i.e., it and ft) impose regularization to determine whether to consider new information or forget old information. In addition, the output gate ot controls the amount of information from the memory contents that is passed to the hidden state ht to influence the computation in the next time step.</p><p>As a neural network, the LSTM model can be easily deepened by stacking the hidden states from a layer l-1 as inputs of the next layer l. In order to obtain the prediction scores for a total of C classes at a time step t, a softmax layer is placed on top of the last LSTM layer L to estimate the posterior probability pc of the c-th class as:</p><formula xml:id="formula_3">pc = softmax(h L t ) = exp(uc T h L t + bc) c ∈C exp(u c T h L t + b c ) ,<label>(3)</label></formula><p>where uc and bc represent the corresponding weight vector and the bias term of the c-th class. Such an LSTM network can be trained using the Back-Propagation Through Time (BPTT) algorithm <ref type="bibr" target="#b16">[16]</ref>, which "unrolls" the model into a feed forward neural net and back-propagates to determine the optimal network parameters. We adopt the output from the last layer as the video-level prediction scores since this output is computed based on the information from the entire sequence. Our empirical results show that using the output of the last time step is better than pooling the predictions at all the time steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-Stream Fusion</head><p>Given the prediction scores of the multiple deep network streams (each stream outputs scores of multiple classes), we are able to capture the video characteristics from different aspects. It is critical to effectively fuse the scores to generate the final predictions. Different semantic classes associate with the multiple streams with different strength. For example, some classes are strongly associated with particular objects which could be effectively recognized with the spatial stream, while others may contain dramatic movements so the short-term motion and the long-term temporal clues can contribute more significantly. Traditional fusion methods are usually performed in a uniform way without considering the class-specific preferences.</p><p>More formally, we denote the prediction scores from the m-th stream as s m ∈ R C (m = 1, • • • , M ) with C being the number of classes, and let ŷ be the final predicted labels. A straightforward way of late fusion is to compute the final prediction as ŷ = f (s 1 , • • • , s M ). Here f is a transition function, which can be a linear function, a logistic function, etc. However, such a late fusion approach treats all the classes uniformly and relies on the assumption that scores from multiple networks are explicitly complementary.</p><p>Different from the uniform fusion methods, we attempt to adaptively integrate the predictions from multiple streams to determine the optimal fusion weights for each class. To this end, we first stack the multiple score vectors of a training sample n as a coefficient vector:</p><formula xml:id="formula_4">sn = s 1 n , • • • , s m n , • • • , s M n ∈ R CM<label>(4)</label></formula><p>Then the best class-specific fusion weights can be learned with simple classifiers like logistic regression:</p><formula xml:id="formula_5">W = arg min w,••• ,w C n,c log 1 + exp (1 -2yn,c)s T n wc ,<label>(5)</label></formula><p>where yn,c is the ground-truth label of the n-th sample for class c, and</p><formula xml:id="formula_6">W = [w1, • • • , wc, • • • , wC ] ∈ R CM ×C .</formula><p>However, the final prediction score of class c in this process not only comes from its own scores of different streams, but also utilizes knowledge from other classes, which incurs extra parameters that will often lead to over-fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Utilizing Class Relationships</head><p>To alleviate the over-fitting effect, one may use sparsity constraints (e.g., 1 or 21 norm) to force entries to be zero in the weight matrix as a means of feature selection. For example, 1 norm penalizes non-zero weights, which will lead to the parameter vector to be sparse ignoring certain information in the input data. Nevertheless, instead of recognizing objects/concepts independently, humans can utilize the relations among concepts to derive a better understanding of the object of interest. Researchers also confirm that class relationships (a.k.a. semantic context) are known as helpful clues in multi-class visual recognition tasks and have been popularly used for improved performance <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b8">8]</ref>. Therefore, we believe that injecting such intrinsic class relationships in the learning process can improve the result.</p><p>Different from enforcing zero patterns in the weight matrix, in this paper, we tackle this problem by resorting to existing knowledge to determine the information from which classes and to what extent will be needed, and hence the parameters do not need to be learned from scratch. For example, we can adopt off-the-shelf WordNet/ConceptNet to obtain the relations among multiple concepts. However, these relations are handcrafted, which might be semantically similar but ignore the visual patterns. Note that each stream is a well-trained deep network, consisting of valuable information on data distribution. Therefore, we propose to mine knowledge (i.e., class relationships) from the models themselves using the confusion matrix, which is a good indicator on how classes are related. More formally, we denote V m ∈ R C×C to be the similarity matrix of different classes for the m-th stream, and hence V m ij indicates the correlation between class Ci and Cj.</p><p>Confusion Matrix. We simply test the network on the validation set and adopt the confusion matrix to measure similarity among video classes:</p><formula xml:id="formula_7">V m ij = 1 |Ci| n∈C i 1 arg maxc(s m n )==C j .<label>(6)</label></formula><p>where 1(•) is the indicator function, Ci is the collection of training samples that belongs to class i, and |•| is the cardinality function. Here each entry Vij indicates the percentage of the samples with the ground-truth label of class i being wrongly classified into class j. We visualize the confusion matrix generated by the spatial ConvNet on CCV in Figure <ref type="figure" target="#fig_3">3</ref>, which demonstrates how classes are correlated. The reason of using a separate correlation matrix for each stream is that the captured class relationships in different streams are likely to be quite different. For instance, some classes are similar visually and some may share certain audio clues.</p><p>After obtaining the class relationship matrices of all the streams using the above equation, we stack the matrices</p><formula xml:id="formula_8">V = V 1 , • • • , V m , • • • , V M</formula><p>to constrain the weight learning process as:</p><formula xml:id="formula_9">min W L(S, Y; W) + λ1 W -V 2 F ,<label>(7)</label></formula><p>where the first term is the empirical loss defined in Equation 5. The second term constrains the fusion weights using the class correlation as a prior, with "F" indicating the Frobenius norm. For each similarity matrix V m , the nondiagonal entries demonstrate the similarities among different classes, which can be used to guide the weight learning process through borrowing information from highly related classes. Since the class relationship matrix V constrains which classes to be used and how much is needed, the best optimal weights could be learned easier without suffering over-fitting. In addition, we also incorporate an 1 norm term to impose sparsity on the weight matrix, which, to some extent, can help avoid information sharing across irrelevant classes. Integrating all the terms, we have the following optimization problem:</p><formula xml:id="formula_10">min W L(S, Y; W) + λ1 W -V 2 F + λ2 W 1 .<label>(8)</label></formula><p>Different from enforcing sparsity patterns as in standard MTL:</p><formula xml:id="formula_11">min W L(S, Y; W) + λ1 W p ,<label>(9)</label></formula><p>where the second term could be 1-norm, 2-norm or 2,1- norm, our fusion approach relies on the class relationship matrix to constrain the weights for improved performance. As our approach not only fuses the multiple network streams but also utilizes class relationship, we name it as multistream multi-class fusion. The contribution from each component of the objective function will be evaluated later.</p><p>Although the loss function in Equation 8 is convex, it is non-trivial to solve it due to the non-smooth term. To tackle the optimization problem efficiently, we adopt the proximal gradient descent method that splits the objective function into a smooth part and a non-smooth part:</p><formula xml:id="formula_12">g = L(S, Y; W) + λ1 W -V 2 F ,<label>(10)</label></formula><formula xml:id="formula_13">h = λ2 W 1 .<label>(11)</label></formula><p>The update of W at the k + 1 iteration can be simply computed as:</p><formula xml:id="formula_14">W k+1 = Prox h (W k -∇g(W k )),</formula><p>where Prox h denotes the soft-thresholding operator for the 1 norm <ref type="bibr" target="#b10">[10]</ref>. Note that the additional computational cost lies in the estimation of the proximal operator. Since it can be an-alytically solved in linear time <ref type="bibr" target="#b3">[3]</ref>, the above optimization process is fairly efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Implementation Details and Discussions</head><p>ConvNet Models. In this work, we adopt two ConvNet architectures, the CNN M <ref type="bibr" target="#b45">[45]</ref> model for capturing the shortterm motion and the audio clues and a recent deeper VGG 19 architecture for the spatial stream <ref type="bibr" target="#b46">[46]</ref>. The CNN M is basically a variant of the AlexNet <ref type="bibr" target="#b28">[28]</ref> with more filters included, which contains five convolutional layers followed by three fully connected layers. The VGG 19 not only reduces the size of the convolutional filters and the stride, but also extends the depth of the network to a total of 19 layers, equipping the architecture with the capacity of learning more robust representations. These two deep networks achieved 13.5% <ref type="bibr" target="#b45">[45]</ref> and 7.5% <ref type="bibr" target="#b46">[46]</ref> top-5 error rates on the ImageNet ILSVRC-2012 validation set, respectively. All the ConvNet models are trained using mini-batch stochastic gradient descent with a momentum fixed to 0.9. Our implementation is based on the publicly available Caffe toolbox <ref type="bibr" target="#b23">[23]</ref> with some modifications. The input video frame is uniformly fixed to the size of 224×224. In addition, we also perform simple data augmentations like cropping and flipping following <ref type="bibr" target="#b45">[45]</ref>.</p><p>The spatial and the audio ConvNets are first pre-trained using the ILSVRC-2012 training set with 1.2 million images and then fine-tuned using the training video data. This strategy has been observed effective in <ref type="bibr" target="#b45">[45]</ref> for the spatial stream, and we have observed it also helpful for the audio stream. To fine-tune the spatial and the audio ConvNets, we gradually decrease the learning rate from 10 -3 to 10 -4 after 14K iterations, then to 10 -5 after 20K iterations. In addition, dropout is applied to the fully connected layers with a ratio of 0.5 to avoid over-fitting.</p><p>To train the motion ConvNet, we first compute optical flow using the GPU implementation of <ref type="bibr" target="#b5">[5]</ref> and stack the optical flows in each 10-frame window to receive a 20-channel optical flow image as the input (one horizontal channel and one vertical channel for each frame pair). Unlike the spatial and the audio ConvNets, we train the motion ConvNet from scratch by adopting 0.7 dropout ratio and setting the learning rate to 10 -2 initially, which is reduced to 10 -3 after 100K iterations and then to 10 -4 after 200K iterations. Note that we also tried to use the VGG 19 network to train the motion ConvNet, but observed worse results as the network contains much more parameters that cannot be well-tuned using the limited training video data.</p><p>LSTM. We adopt the two-layer LSTM model proposed by Graves <ref type="bibr" target="#b16">[16]</ref> for temporal modeling. Two models are trained with features extracted from the first fully-connected layer of the spatial and the motion ConvNets respectively as inputs. Each LSTM has 1,024 hidden units in the first layer and 512 hidden units in the second layer. We utilize a parallel implementation of the BPTT algorithm with a mini-batch size of 10 to train the network weights, where the learning rate and momentum are set as 10 -4 and 0.9. In addition, we set the maximal training iterations to be 150K. Note that, in this paper, we focus on a multi-stream framework by utilizing the audio signal as a single stream for video classification. Further decomposing the audio track into multiple segments to extract more detailed temporal audio dynamics is feasible.</p><p>Fusion. As shown in Equation <ref type="formula" target="#formula_10">8</ref>, the proposed fusion method seeks a tradeoff among the three terms. We uniformly fix λ2 to be 10 -3 to encourage sparsity in the learned weight matrix. The parameter λ1 is selected among {10 -5 , 10 -4 , 10 -3 , 10 -2 } using cross-validation.</p><p>Discussions. Our proposed framework has the capability of modeling video data comprehensively by adaptively fusing audio, static spatial, short-term motion and long-term temporal clues. As described above, such a framework consists of multiple separately trained deep networks. Although being feasible to jointly train the entire framework, it is complicated and computationally demanding. A recent work performing joint training of the LSTM with a ConvNet improves the results on the UCF-101 benchmark from 70.5% (separate network training) to 71.1% <ref type="bibr" target="#b9">[9]</ref>, which is not very significant. In addition, training multiple deep networks separately makes the approach more flexible, where a component may be replaced without the need of re-training the entire framework. For instance, one can utilize more discriminative ConvNet models like the GoogLeNet <ref type="bibr" target="#b51">[51]</ref> and deeper RNN models <ref type="bibr" target="#b7">[7]</ref> to replace the current ConvNet and LSTM parts respectively for better performance. Therefore, in this work, we focus on presenting a general framework for video classification. With the proposed multi-stream multiclass fusion method, the framework is empirically proved to be effective for the video classification task, as discussed in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>In this section, we report results on two popular datasets. Experiments are designed to study the effectiveness of each individual stream and the proposed fusion method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets and Evaluation Measures. UCF-101 <ref type="bibr" target="#b47">[47]</ref> is a widely adopted dataset for human action recognition, containing 13,320 video clips annotated into 101 action classes. All the video clips have a fixed frame rate of 25 fps with a spatial resolution of 320 × 240 pixels. This dataset is challenging because most videos were captured under uncontrolled environments with camera motion, cluttered backgrounds and large intra-class variations. We follow the suggested experimental protocol and report mean accuracy over the three training and test splits <ref type="bibr" target="#b19">[19]</ref>.</p><p>The Columbia Consumer Videos (CCV) dataset <ref type="bibr" target="#b25">[25]</ref> contains 9,317 YouTube videos and 20 classes. Most of the classes are events like "basketball", "graduation ceremony" and "wedding dance". A few are scenes and objects like "beach" and "dog". Following <ref type="bibr" target="#b25">[25]</ref>, we adopt the suggested training and test split and compute the average precision (AP) for each class. Mean AP (mAP) is used to measure the overall performance on this dataset.</p><p>The two datasets possess very different characteristics. Besides the difference of the defined semantic classes, the average video duration of CCV is 80 seconds, which is around ten times longer than that of UCF-101. Testing on these two datasets is helpful for evaluating the effectiveness and the generalization capability of our multi-stream classification approach.</p><p>Compared Methods. To validate the effectiveness of our multi-stream multi-class fusion method, we compare with the following alternatives: (1) Average Fusion, where the mean scores of multiple network streams are used as the final prediction; (2) Weighted Fusion, where the scores are fused linearly with weights estimated by cross-validation;</p><p>(3) Kernel Average Fusion, where the scores are used as features and kernels computed from different network scores are averaged to train an SVM classifier; (4) Multiple Kernel Learning (MKL) Fusion, where the kernels are combined using the p-norm MKL algorithm <ref type="bibr" target="#b27">[27]</ref>; (5) Logistic Regression Fusion, where a logistic regression model is trained to estimate the fusion weights; (6) Domain Adaptive Semantic Diffusion (DASD) <ref type="bibr" target="#b24">[24]</ref>, which uses a graph diffusion formulation for context-based multi-class score fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Multi-Stream Networks</head><p>We first report the performance of each individual stream on both datasets. After that, average fusion is adopted to study whether two or more streams are complementary. The proposed fusion method will be evaluated later.</p><p>Table <ref type="table" target="#tab_0">1</ref> reports the results. Comparing the top two cells of results on UCF-101, it is interesting to observe that the spatial LSTM outperforms the spatial ConvNet and the motion LSTM is also comparable to the motion ConvNet. This is largely due to the fact that the long-term temporal clues are fully discarded in the ConvNet based classification, which can be exploited by the LSTM.</p><p>On the CCV dataset, the ConvNet achieves significantly better results than the LSTM on both spatial and motion streams. This is because the classes in CCV are either highlevel events or objects/scenes. Compared with human actions, the temporal clues of these classes are more obscure and thus difficult to be captured. Also, the CCV videos are temporally untrimmed, which may contain significant portions of contents irrelevant to the classes, making the temporal modeling task even more difficult.</p><p>One may notice that our motion networks perform worse than spatial networks, which is not consistent with the observations in dense trajectory features <ref type="bibr" target="#b56">[56]</ref>. We would like to underline that motion information extracted from optical flow images tends to be noisy, especially on temporally untrimmed long videos, as reported in <ref type="bibr" target="#b39">[39]</ref>. In addition, the dense trajectory features are hand-crafted without the need of training, while the motion stream networks demand extensive training. Unfortunately, labeled training data in the video domain is still quite limited. We expect to obtain better results from the motion stream network once sufficient training data is available.</p><p>The audio ConvNets operated on spectrograms produce 16.2% on UCF-101 and 21.5% on CCV. Note that only 51 classes in UCF-101 have audio signals, and the performance on the 51-class subset is actually 32.1%. The audio stream is much worse than the spatial and the motion streams on both datasets, confirming that the visual channel are more informative than the audio counterpart.</p><p>Next, we evaluate the combinations of multiple networks to study whether fusion can compensate the limitations of a single stream in describing complex video data. The simple average fusion is adopted. Results are summarized in the bottom three groups of Table <ref type="table" target="#tab_0">1</ref>. We first assess the gain from integrating the spatial and the motion information modeled by ConvNet and LSTM respectively. On UCF-101, significant improvements (about 6% for ConvNet and 3% for LSTM) are observed over the best single stream results. The gain on CCV is consistent but not as significant as that on UCF-101, indicating that the short-term motion is more critical for human action analysis. Note that the average fusion of the spatial and the motion ConvNets follows the same idea of the two-stream approach proposed in <ref type="bibr" target="#b45">[45]</ref>.</p><p>Our implementation of this approach produces slightly worse performance than that originally reported in <ref type="bibr" target="#b45">[45]</ref> (86.2% vs. 88.0%).</p><p>We also fuse ConvNet with LSTM separately on both streams to investigate the contribution of the long-term temporal modeling. Overall, we observe very consistent improvements on both datasets. In particular, on CCV, although the individual LSTM model is worse than ConvNet, the combination of them leads to significant improvements. Especially, a gain of nearly 12% is obtained on the motion stream. These results show that the long-term temporal clues are highly complementary to the ConvNet-based predictions, even in the case of modeling complex contents in the long CCV videos.</p><p>Finally, the combination of ConvNet and LSTM on both streams, indicated by "ConvNet+LSTM (spatial+motion)", achieves 90.1% and 81.7% on UCF-101 and CCV respectively. Further adding the audio ConvNet ("all the streams") can improve the results particularly on CCV which contains many classes that can be partly revealed by auditory clues (e.g., cheering sounds in the sports events). In summary, the fusion results clearly demonstrate that all the multimodal clues in our approach are useful and should be adopted in a successful video classification system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Multi-Stream Multi-Class Fusion</head><p>In this subsection, we evaluate the proposed fusion method and compare it with the alternative methods. Table <ref type="table" target="#tab_1">2</ref> gives the results. We see that all the methods produce better results than the individual streams. The simple average fusion and weighted fusion are slightly better than the learning based kernel fusion and logistic regression fusion, indicating that the learning based methods are prone to overfitting. Kernel average fusion shows slightly better results than MKL, which is consistent with the observations in several previous studies like <ref type="bibr" target="#b13">[13]</ref>. DASD produces similar results to weighted fusion as it is essentially an iterative weighted fusion method. Our proposed multi-stream multi-class fusion (the bottom group of results) outperforms all the alternatives with clear margins. To investigate the contributions of the class relationship term and the sparsity term in our approach, we set λ1 and λ2 to be zero respectively. As can be seen, both terms are very useful. Relatively, the class relationship (λ1) plays a more important role than the sparsity term (λ2). This corroborates the effectiveness of using the class relationship, even when it is roughly estimated based on prediction score correlations (see Section 3.3), which is very appealing. The two terms are complementary as the sparsity inducing norm further enhances robustness by alleviating incorrect information sharing. Note that when eliminating both terms, our fusion approach degenerates to the standard logistic regression fusion. In summary, these results show that it is helpful to fuse the outputs of both multiple network streams and multiple classes.</p><p>The contribution of the audio clues is similar on both datasets ("-A" indicates the same approach without using the audio ConvNet). Audio improves just 0.4% on UCF-101 because only half of the video clips contain soundtracks. Figure <ref type="figure" target="#fig_4">4</ref> further shows the per-class performance on CCV, where we can see that fusion leads to very consistent and significant improvements for all the classes.</p><p>Comparisons with different regularizers. Since sparsity regularizers are popular constraints to improve generalization ability as in standard MTL, we also compare the proposed approach with this line of work by replacing the proposed regularizers with alternative ones (see Eq. 9). The results are summarized in Table <ref type="table" target="#tab_2">3</ref>. As we can see from the table, the proposed regularization norm that utilizes class relationship as contextual information outperforms the competing regularizers. Different from forcing entries to be zero (i.e., 1 norm) or selecting the same set of parameters for related tasks (i.e., 21 norm), the proposed norm leverages the class relationships in the models to guide the fusion process, through constraining which and how much information is required from other classes. This corroborates the fact the scores from the softmax layer contain meaningful information rather than simply indicating the predicted label <ref type="bibr" target="#b17">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Computational Efficiency</head><p>The proposed approach can achieve effective recognition efficiently. For a CCV video clip with an average duration of 80 seconds, extracting the improved dense trajectories Our approach produces to-date the highest reported results on both datasets. "Ours (-A)" indicates the same framework without using the audio stream ConvNet.</p><p>We compare our approach with the state of the arts on both datasets. Results are listed in Table <ref type="table" target="#tab_3">4</ref>. Our proposed multi-stream approach achieves the highest performance on both datasets. On UCF-101, many works with competitive results are based on the hand-engineered dense trajectory features <ref type="bibr" target="#b57">[57,</ref><ref type="bibr" target="#b30">30]</ref>, while our approach fully relies on the deep networks. Compared with the original result of the twostream approach <ref type="bibr" target="#b45">[45]</ref>, our approach captures a more comprehensive set of useful clues with a more effective fusion method. Zha et al. <ref type="bibr" target="#b71">[70]</ref> combined the ConvNet features with the dense trajectories <ref type="bibr" target="#b56">[56]</ref> to achieve competitive results. The previous best performance on UCF-101 is from Wang et al. <ref type="bibr" target="#b59">[59]</ref>, who combined the two-stream approach <ref type="bibr" target="#b45">[45]</ref> and the dense trajectories. Note that a gain of 1% on the widely adopted UCF-101 dataset is generally considered as a significant progress.</p><p>In addition, the recent works in <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b65">64,</ref><ref type="bibr" target="#b39">39</ref>] also adopted the LSTM to model the temporal clues for video classification and reported promising performance, but did not explore the audio stream nor employ advanced fusion strategies.</p><p>On the CCV dataset, all the recent approaches were developed based on multiple features, either the hand-engineered descriptors or the ConvNet-based representations. Our approach produces better results than all of them. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>We have presented a multi-stream framework of deep networks for video classification. The framework harnesses multimodal features that are more comprehensive than those previously adopted. Specifically, standard ConvNets are applied to audio spectrograms, visual frames and stacked optical flows to exploit the audio, spatial and short-term motion clues in videos, respectively. LSTM is further adopted on the spatial and the short-term motion features from the ConvNets for long-term temporal modeling. The outputs from the different streams are then combined using a novel method called multi-stream multi-class fusion, which not only learns the best weights of the multi-stream networks for each class, but also considers class relationships for improved performance. Our results confirm that all the adopted streams are effective for modeling both simple human actions in short clips and complex events in temporally untrimmed Internet videos. Combining the multi-stream multi-class predictions by our proposed fusion method consistently outperforms peer approaches on two popular benchmarks.</p><p>This paper is among the limited number of studies showing strong video classification performance using deep networks. As aforementioned, unlike the spatial ConvNet that can be trained by fine-tuning a model pre-trained on the ImageNet dataset, the motion ConvNet has to be trained from scratch on videos. Therefore, one promising future direction is to pre-train the motion ConvNet using large video datasets like the Sports-1M <ref type="bibr" target="#b26">[26]</ref>, which may lead to much better results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>MM ' 16 ,</head><label>16</label><figDesc>October 15-19, 2016, Amsterdam, Netherlands c 2016 ACM. ISBN 978-1-4503-3603-1/16/10. . . $15.00 DOI: http://dx.doi.org/10.1145/2964284.2964328</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the proposed framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The structure of an LSTM unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The confusion matrix of the spatial Con-vNet on CCV.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Per-class performance on CCV. Multi-stream multi-class fusion of the deep network outputs produces consistently better results than the individual streams on all the classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance of each individual stream and their average fusion (indicated by "+"). * Note that the videos of only 51 classes in UCF-101 contain audio soundtracks. The audio ConvNet can produce an accuracy of 32.1% on the 51-class subset.</figDesc><table><row><cell>UCF-101 CCV</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of fusion methods. "-A" indicates that the audio stream ConvNet is not adopted. See texts for discussions.</figDesc><table><row><cell></cell><cell cols="2">UCF-101 CCV</cell></row><row><cell>Average fusion</cell><cell>90.3</cell><cell>82.4</cell></row><row><cell>Weighted fusion</cell><cell>90.6</cell><cell>82.7</cell></row><row><cell>Kernel average fusion</cell><cell>90.2</cell><cell>82.1</cell></row><row><cell>MKL fusion</cell><cell>89.6</cell><cell>81.8</cell></row><row><cell>Logistic regression fusion</cell><cell>89.8</cell><cell>82.0</cell></row><row><cell>DASD</cell><cell>90.4</cell><cell>82.9</cell></row><row><cell>Multi-stream fusion (λ1=0)</cell><cell>90.9</cell><cell>82.8</cell></row><row><cell>Multi-stream multi-class fusion (λ2=0)</cell><cell>91.6</cell><cell>83.7</cell></row><row><cell>Multi-stream multi-class fusion (-A)</cell><cell>92.2</cell><cell>84.0</cell></row><row><cell>Multi-stream multi-class fusion</cell><cell>92.6</cell><cell>84.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison with different regularization strategies.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">UCF-101 CCV</cell></row><row><cell cols="2">Fusion with 1 norm</cell><cell></cell><cell>90.9</cell><cell>82.8</cell></row><row><cell cols="2">Fusion with 21 norm</cell><cell></cell><cell>91.0</cell><cell>82.4</cell></row><row><cell cols="3">Fusion with ( 21 + 1) norm</cell><cell>91.4</cell><cell>83.2</cell></row><row><cell cols="3">Multi-stream multi-class fusion</cell><cell>92.6</cell><cell>84.9</cell></row><row><cell cols="5">requires 850 seconds, while it only takes 131 seconds for our</cell></row><row><cell cols="5">method to finish the entire process, which is evaluated on a</cell></row><row><cell cols="2">single NVIDIA Telsa K40 GPU.</cell><cell></cell><cell></cell></row><row><cell cols="4">4.2.4 Comparison with State of the Arts</cell></row><row><cell>UCF-101</cell><cell></cell><cell></cell><cell>CCV</cell></row><row><cell>Donahue et al. [9]</cell><cell>82.9</cell><cell cols="2">Lai et al. [29]</cell><cell>43.6</cell></row><row><cell cols="2">Srivastava et al. [48] 84.3</cell><cell cols="3">Jiang et al. [25] 59.5</cell></row><row><cell>Wang et al. [57]</cell><cell>85.9</cell><cell cols="2">Xu et al. [66]</cell><cell>60.3</cell></row><row><cell>Tran et al. [53]</cell><cell>86.7</cell><cell cols="2">Ma et al. [34]</cell><cell>63.4</cell></row><row><cell cols="2">Simonyan et al. [45] 88.0</cell><cell cols="2">Jhuo et al. [21]</cell><cell>64.0</cell></row><row><cell>Ng et al. [39]</cell><cell>88.6</cell><cell cols="2">Ye et al. [68]</cell><cell>64.0</cell></row><row><cell>Lan et al. [30]</cell><cell>89.1</cell><cell cols="2">Liu et al. [32]</cell><cell>68.2</cell></row><row><cell>Zha et al. [70]</cell><cell>89.6</cell><cell cols="2">Wu et al. [64]</cell><cell>83.5</cell></row><row><cell>Wang et al. [59]</cell><cell>91.5</cell><cell cols="3">Nagel et al. [35] 71.7</cell></row><row><cell>Wang et al. [60]</cell><cell>92.4</cell><cell></cell><cell></cell></row><row><cell>Ours (-A)</cell><cell>92.2</cell><cell cols="2">Ours (-A)</cell><cell>84.0</cell></row><row><cell>Ours</cell><cell>92.6</cell><cell></cell><cell>Ours</cell><cell>84.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison with state-of-the-art results.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported by a China's National 863 Program (#2014AA015101), and two grants from National Natural Science Foundation of China (#61572138 and #U1509206).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Exploring convolutional neural network structures and optimization techniques for speech recognition</title>
		<author>
			<persName><forename type="first">O</forename><surname>Abdel-Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>INTERSPEECH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video classification using semantic concept co-occurrences</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Assari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convex optimization with sparsity-inducing norms</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimization for Machine Learning</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Using web co-occurrence statistics for improving image categorization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Webly supervised learning of convolutional networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Gated feedback recurrent neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ç</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large-scale object classification using label relation graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adapting to unknown smoothness via wavelet shrinkage</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Johnstone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the american statistical association</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On feature combination for multiclass object classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICASSP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The thumos challenge on action recognition for videos</title>
		<author>
			<persName><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06182</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">University of amsterdam at thumos</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR THUMOS Workshop</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Discovering joint audio-visual codewords for video event detection</title>
		<author>
			<persName><forename type="first">I.-H</forename><surname>Jhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Machine Vision and Applications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Domain adaptive semantic diffusion for large scale context-based video annotation</title>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Consumer video understanding: A benchmark database and an evaluation of human and machine performance</title>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Loui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM ICMR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Lp-norm multiple kernel learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Brefeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sonnenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Video event detection by inferring temporal instance labels</title>
		<author>
			<persName><forename type="first">K.-T</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Beyond gaussian pyramid: Multi-skip feature stacking for action recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sample-specific late fusion for visual category recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-T</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reduced analytic dependency modeling: Robust fusion for visual recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Event fisher vectors: Robust encoding visual diversity of visual streams</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Likelihood ratio-based biometric score fusion</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nandakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Dass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multimodal feature fusion for robust event detection in web videos</title>
		<author>
			<persName><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vitaladevuni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsakalidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Moddrop: adaptive multi-modal gesture recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nebout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Action and event recognition with fisher vectors on a compact feature set</title>
		<author>
			<persName><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Objects in context</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wiewiora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning temporal embeddings for complex video analysis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Image classification with the fisher vector: Theory and practice</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Action recognition using visual attention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using LSTMs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Action recognition via local descriptors and holistic features</title>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Going Deeper with Convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning latent temporal structure for complex event detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<title level="m">Generic features for video analysis</title>
		<imprint>
			<publisher>CoRR</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep content-based music recommendation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Differential recurrent neural networks for action recognition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Veeriah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Lear-inria submission for the thumos workshop</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV THUMOS Workshop</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Evaluation of local spatio-temporal features for action recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Actions transformations</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Max-margin hidden conditional random fields for human action recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Harnessing object and scene semantics for large-scale video understanding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Exploring inter-feature and inter-class relationships with deep neural networks for video classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Modeling spatial-temporal clues in a hybrid deep learning framework for video classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A discriminative cnn video representation for event detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Feature weighting via optimal thresholding for video analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Robust late fusion with rank minimization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I.-H</forename><surname>Jhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Informedia@ trecvid 2014 med and mer</title>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIST TRECVID Video Retrieval Evaluation Workshop</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Exploiting image-trained cnn architectures for unconstrained video classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Luisier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Play and rewind: Optimizing binary representations of videos by self-supervised temporal hashing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
