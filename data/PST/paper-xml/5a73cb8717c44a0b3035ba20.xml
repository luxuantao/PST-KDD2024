<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Animal Recognition and Identification with Deep Convolutional Neural Networks for Automated Wildlife Monitoring</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hung</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Pattern Recognition and Data Analytics</orgName>
								<orgName type="institution">Deakin University</orgName>
								<address>
									<settlement>Geelong</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sarah</forename><forename type="middle">J</forename><surname>Maclagan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Centre for Integrative Ecology</orgName>
								<orgName type="institution">Deakin University</orgName>
								<address>
									<settlement>Burwood</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tu</forename><forename type="middle">Dinh</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Pattern Recognition and Data Analytics</orgName>
								<orgName type="institution">Deakin University</orgName>
								<address>
									<settlement>Geelong</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thin</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Pattern Recognition and Data Analytics</orgName>
								<orgName type="institution">Deakin University</orgName>
								<address>
									<settlement>Geelong</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><surname>Flemons</surname></persName>
							<email>paul.flemons@austmus.gov.au</email>
							<affiliation key="aff2">
								<orgName type="institution">Australian Museum Research Institute</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kylie</forename><surname>Andrews</surname></persName>
							<email>andrews.kylie@abc.net.au</email>
							<affiliation key="aff3">
								<orgName type="institution">ABC Radio National</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Euan</forename><forename type="middle">G</forename><surname>Ritchie</surname></persName>
							<email>e.ritchie@deakin.edu.au</email>
							<affiliation key="aff1">
								<orgName type="department">Centre for Integrative Ecology</orgName>
								<orgName type="institution">Deakin University</orgName>
								<address>
									<settlement>Burwood</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dinh</forename><surname>Phung</surname></persName>
							<email>dinh.phung@deakin.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Pattern Recognition and Data Analytics</orgName>
								<orgName type="institution">Deakin University</orgName>
								<address>
									<settlement>Geelong</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Animal Recognition and Identification with Deep Convolutional Neural Networks for Automated Wildlife Monitoring</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DB4CE3B318AB91541565A95D99573E14</idno>
					<idno type="DOI">10.1109/DSAA.2017.31</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>deep learning</term>
					<term>convolutional neural networks</term>
					<term>large scale image classification</term>
					<term>animal recognition</term>
					<term>wildlife monitoring</term>
					<term>citizen science</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Efficient and reliable monitoring of wild animals in their natural habitats is essential to inform conservation and management decisions. Automatic covert cameras or "camera traps" are being an increasingly popular tool for wildlife monitoring due to their effectiveness and reliability in collecting data of wildlife unobtrusively, continuously and in large volume. However, processing such a large volume of images and videos captured from camera traps manually is extremely expensive, time-consuming and also monotonous. This presents a major obstacle to scientists and ecologists to monitor wildlife in an open environment. Leveraging on recent advances in deep learning techniques in computer vision, we propose in this paper a framework to build automated animal recognition in the wild, aiming at an automated wildlife monitoring system. In particular, we use a single-labeled dataset from Wildlife Spotter project, done by citizen scientists, and the state-of-the-art deep convolutional neural network architectures, to train a computational system capable of filtering animal images and identifying species automatically. Our experimental results achieved an accuracy at 96.6% for the task of detecting images containing animal, and 90.4% for identifying the three most common species among the set of images of wild animals taken in South-central Victoria, Australia, demonstrating the feasibility of building fully automated wildlife observation. This, in turn, can therefore speed up research findings, construct more efficient citizen sciencebased monitoring systems and subsequent management decisions, having the potential to make significant impacts to the world of ecology and trap camera images analysis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Observing wild animals in their natural environments is a central task in ecology. The fast growth of human population and the endless pursuit of economic development are making over-exploitation of natural resources, causing rapid, novel and substantial changes to Earth's ecosystems. An increasing area of land surface has been transformed by human action, altering wildlife population, habitat and behavior. More seriously, many wild species on Earth have been driven to extinction, and many species are introduced into new areas where they can disrupt both natural and human systems <ref type="bibr" target="#b0">[1]</ref>. Monitoring wild animals, therefore, is essential as it provides researchers evidences to inform conservation and management decisions to maintain diverse, balanced and sustainable ecosystems in the face of those changes. Various modern technologies have been developed for wild animal monitoring, including radio tracking <ref type="bibr" target="#b1">[2]</ref>, wireless sensor network tracking <ref type="bibr" target="#b2">[3]</ref>, satellite and global positioning system (GPS) tracking <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, and monitoring by motionsensitive camera traps <ref type="bibr" target="#b5">[6]</ref>. Motion-triggered remote cameras or "camera traps" are an increasingly popular tool for wildlife monitoring, due to their novel features equipped, wider commercial availability, and the ease of deployment and operation. For instance, a typical covert camera model (Figure <ref type="figure" target="#fig_0">1</ref>) is capable of not only capturing high definition images in both day and night, but also collecting information of time, temperature and moon phase integrated in image data. In addition, generous and flexible camera settings allow tracking animals secretly and continuously. Once being fully charged, a camera can snap thousands of consecutive images, providing a large volume of data. These specifications make camera traps a powerful tool for ecologists as they can document every aspect of wildlife <ref type="bibr" target="#b6">[7]</ref>.</p><p>Visual data, if can be captured, is a rich source of information that provide scientists evidences to answer ecology-related scientific questions such as: what are the spatial distributions of rare animals, which species are being threatened and need protection such as bandicoot, which cohort of pest species, such as red fox and rabbit, need to be controlled; these are examples of key questions to understand wild animals' populations, ecological relationships and population dynamics <ref type="bibr" target="#b6">[7]</ref>. To this end, a recently widely-used approach by ecologists is to set up several camera traps in the wild to collect image data of wild animals in their natural habitats <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>.</p><p>Camera trapping is rapidly being adopted for wildlife monitoring thanks to advances in digital technology that produce more modern camera traps with automation of system components but lower cost of purchase; the task of analyzing huge collections of camera trap images, however, has been conducted manually. Despite the fact that human visual system can process images effortlessly and rapidly <ref type="bibr" target="#b8">[9]</ref>, processing such an enormous number of images manually is much expensive. For example, to date, the Snapshot Serengeti project<ref type="foot" target="#foot_1">1</ref> gathered 3.2 million images through 225 camera traps across the Serengeti National Park, Tanzania from 2010-2013 <ref type="bibr" target="#b7">[8]</ref>. Another similar project, Wildlife Spotter<ref type="foot" target="#foot_2">2</ref> , collected millions photos of wildlife captured in tropical rainforests and dry rangelands of Australia. Unfortunately, due to automatic trap camera snapping mechanism, the vast majority of captured images are challenging to process, even for human. Only a limited number of collected images are in favorable condition as in Figure <ref type="figure" target="#fig_1">2a</ref>. Many images contain only partial body of animal objects (Figure <ref type="figure" target="#fig_1">2d</ref>), in others the animal objects are captured in the whole body but too far from camera (Figure <ref type="figure" target="#fig_1">2b</ref>), in varied views or deformations (Figure <ref type="figure" target="#fig_1">2g</ref>), or occlusion (Figure <ref type="figure" target="#fig_1">2f</ref>). Further more, numerous images are in grayscale as they were captured at night with infrared flash support (Figure <ref type="figure" target="#fig_1">2e</ref>), and a large number of images contains no animal as Figure <ref type="figure" target="#fig_1">2h</ref> (75% of the Snapshot Serengeti <ref type="bibr" target="#b7">[8]</ref> and 32.26% of Wildlife Spotter labeled images were classified as "no animal"), while in others might appear several objects belonging to different species. Overwhelming amounts of data and limited image quality, therefore, remarkably slow down the image analyzing process.</p><p>To share scientists' workload, in large wildlife monitoring projects such as Snapshot Serengeti or Wildlife Spotter, volunteers were invited as "citizen scientists" to join the image analyzing process remotely via Web-based image classification systems. A large number of volunteers engaged in these projects and the species identifying accuracy of 96.6% obtained on the Snapshot Serengeti dataset <ref type="bibr" target="#b7">[8]</ref>, which was validated by experts, demonstrates the success of citizen science projects. However, the huge collections of images and the limitation of imperfect image quality notably influence human classification speed, and sometimes accuracy, even for experts <ref type="bibr" target="#b7">[8]</ref>. In particular, some images in the Snapshot Serengeti dataset were annotated by experts as "impossible to identify", over 9,600 images in the Wildlife Spotter dataset of Southcentral Victoria were tagged as "something else" or "image problem", thousands of photos were inconsistently labeled (e.g., the same image was classified as different species by different volunteers). In addition, even though many volunteers were enthusiastic about joining citizen science projects, it would take a long time to complete analyzing millions of images manually. For example, in the Snapshot Serengeti project, it took more than two months to annotate a 6month batch of images by a group of 28,000 registered and 40,000 unregistered volunteers <ref type="bibr" target="#b7">[8]</ref>. The demand of wild animal identifying automation, therefore, arises from these obstacles.</p><p>To our best of knowledge, there are currently very limited existing works have attempted to build automated system to process and analyze videos and images captured in the wild for environmental monitoring task. The overwhelming amounts of data from camera traps highlight the need for image processing automation. From data analysis and machine learning point of views, there are some immediate techniques to make wildlife identification automated such as applying linear support vector machine (SVM) classifier with manual object bounding on hand-crafted features <ref type="bibr" target="#b9">[10]</ref>, convolutional neural network (CNN) model with automatic object detection <ref type="bibr" target="#b10">[11]</ref>, or fine-tuning CNN models inheriting model weights pretrained on a very large scale dataset such as the ImageNet <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. These approaches addressed the problem of wildlife monitoring automation and demonstrated promisingly empirical results. However, two primary challenges, which inhibit the feasibility of an automated wildlife monitoring application in practice, are still remaining. The first obstacle is that, to obtain applicable image classification accuracy, an enormous amount of manual preprocessing is still required to input images for detecting and bounding animal objects <ref type="bibr" target="#b9">[10]</ref>. The second limitation is poor performance obtained by wildlife monitoring system, in spite of complete automation, requiring much more improvements for practical application <ref type="bibr" target="#b10">[11]</ref>.</p><p>In this paper, we design a framework for animal recognition in the wild, aiming at a fully automatic wildlife spotting system. Our work is motivated by the state-of-the-art power of recent deep CNN models for image classification, in particular the recent evidence that automated recognition can surpass human at certain object recognition tasks in the ImageNet competition <ref type="bibr" target="#b13">[14]</ref>. We carry out experiments on datasets of Wildlife Spotter project, containing a large number of images taken by trap cameras set up by Australian scientists. More specifically, since the Wildlife Spotter dataset includes both animal and non-animal images, we divide the wild animal identifying automation into two subsequent tasks: (1) Wildlife detection, which is actually a binary classifier capable of classifying input images into two classes: "animal" or "no animal" based on the prediction of animal presence in images; and (2) Wildlife identification, a multiclass classifier to label each input image with animal presence by a specified species. The core of each task is essentially a deep CNN-based classifier, trained from prepared datasets manually labeled by volunteers. Several selected deep CNN architectures are employed to the framework for comparisons. The success of Task 1 will have a significant impact in improving the efficiency of citizen science-based projects (e.g., Wildlife Spotter) by automatically filtering out a large portion of non-animal images where citizen annotators are currently wasting their time on. Our experimental results on the Wildlife Spotter datasets show that this approach is feasible, and can save considerable time and expense. Hence, the key contribution of this work is that, with sufficient data and computing infrastructure, deep learning could be employed to build a fully automatic image classification system at large scale, liberating scientists from the burden of manual processing of millions of images, which is considered by the project managers "It's a job that computers just can't do" <ref type="foot" target="#foot_3">3</ref> . In addition, our proposed framework can be combined with the existing citizen science project, forming a "hybrid" image classifier whose automated component works as a recommendation system, providing volunteers remarkable suggestions to speed up their classifying decisions.</p><p>The rest of the paper is organized as follows. In Section II we briefly outline fundamentals of CNN and its application to image classification. In this section we also summarize related work for the topic of automated wildlife classification and an existing citizen science-based wild animal classification project: the Wildlife Spotter. We describe the proposed animal recognition framework, data, and experimental set-up in Section III. Empirical results and discussion are presented in Section IV. Finally we conclude and state future work in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section we first briefly describe the CNN and its application to image classification. We then summarize various CNN architectures that have demonstrated the state-of-the-art performance in recent ImageNet Challenges <ref type="bibr" target="#b13">[14]</ref>. Finally we discuss existing approaches to a particular problem: animal classification in natural scenes from camera trap images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Convolutional Neural Networks for Image Classification</head><p>Visual recognition is a relatively trivial task for human, but still challenging for automated image recognition systems due to complicated and varied properties of images <ref type="bibr" target="#b14">[15]</ref>. Each object of interest can alter an infinite number of different images, generated by variations in position, scale, view, background, or illumination. Challenges become more serious in real-world problems such as wild animal classification from automatic trap cameras, where most captured images are in imperfect quality as described previously in Section I. Therefore, for the task of image classifying automation, it is important to build models that are capable of being invariant to certain transformations of the inputs, while keeping sensitivity with inter-class objects <ref type="bibr" target="#b15">[16]</ref>.</p><p>Firstly proposed by LeCun et al. <ref type="bibr" target="#b16">[17]</ref>, CNNs have been showing great practical performance and been widely used in machine learning in the past recent years, especially in the areas of image classification <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, speech recognition <ref type="bibr" target="#b21">[22]</ref>, and natural language processing <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. These models have made the state-of-the-art results that even outperformed human in image recognition task <ref type="bibr" target="#b24">[25]</ref>, due to recent improvements in neural networks, namely deep CNNs, and computing power, especially the successful implementations of parallel computing on graphical processing units (GPUs), and heterogeneous distributed systems for learning deep models in large scale such as TensorFlow <ref type="bibr" target="#b25">[26]</ref>. CNNs are basically neural network-based learning models specifically designed to take advance the spatial structure of input images, which are usually in 3-dimensional volume: width, height, and depth (the number of color channels). As illustrated in Figure <ref type="figure" target="#fig_2">3</ref>, a CNN is essentially a sequence of layers which can be divided into groups each comprising of convolutional layer plus non-linear activation function, usually the Rectifier Linear Unit (ReLU) <ref type="bibr" target="#b19">[20]</ref>, and pooling layer, mostly max pooling; ended by several fully-connected layers where the last one is the output layer with predictions. In the standard neural networks, each neuron is fully connected to all neurons in the previous layer and the neurons in each layer are completely independent. When applied to high dimensional data such as natural images, the total number of parameters can reach millions, leading to serious overfitting problem and impractical to be trained. In CNNs, by contrast, each neuron is connected only to a small region of the preceding layer, forming local connectivity. The convolution layer computes the outputs of its neurons connected to local regions in the previous layer, the spatial extent of this connection is specified by a filter size. In addition, another important property of CNNs, namely parameter sharing, dramatically reduces the number of parameters and so does computing complexity. Thus, compared to regular neural networks with similar size of layers, CNNs have much fewer connections and parameters, making them easier to train while their performance is slightly degraded <ref type="bibr" target="#b19">[20]</ref>. These three main characteristics -spatial structure, local connectivity and parameter sharing -allow CNNs converting input image into layers of abstraction; the lower layers present detail features of images such as edges, curves and corners, while the higher layers exhibit more abstract features of object.</p><p>Apart from using more powerful models and better techniques for preventing overfitting, the performance of data-driven machine learning approaches depends strictly on the size and quality of collected training datasets. Real-life objects exhibit considerable variability, requiring much larger training sets to learn recognizing them <ref type="bibr" target="#b19">[20]</ref>. The ImageNet, one of the world's largest public image datasets to date, contains over 14 million color, high-resolution, human-labeled images of 22,000 categories. A reduced version of ImageNet dataset including 1,000 categories, each contains roughly 1,000 images, was released in 2010 by the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC), has recently been the standard benchmark for evaluation of large scale image classification models <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b19">[20]</ref>.</p><p>The ILSVRC aims at three main tasks: image classification (from 2010), single-object localization (from 2011), and object detection (from 2013) from the ImageNet dataset <ref type="bibr" target="#b13">[14]</ref>. There were many efforts from research groups over the world participating the challenge and the reported performance was improved significantly over time. The winner of ILSVRC-2010 was AlexNet <ref type="bibr" target="#b19">[20]</ref>, a CNN-based architecture comprising of 8 layers with 5 convolutional layers and 3 fully-connected layers. A variant of the AlexNet model also achieved over 10% top-5 test error rate better than the second-best entry <ref type="bibr" target="#b19">[20]</ref>. GoogLeNet <ref type="bibr" target="#b18">[19]</ref>, the winner of ILSVRC-2014, developed an Inception Module that dramatically reduces the number of parameters. Further more, the GoogLeNet replaced fullyconnected layers at the top of the CNN by average pooling, removing a large number parameters which do not affect performance of the network. The VGG Nets <ref type="bibr" target="#b17">[18]</ref>, which are analogous to AlexNet but the network depth was increased up to 19 layers, with smaller convolutional filters, outperformed other models in the ILSVRC-2014 except the GoogLeNet. Not only show great performance on the ImageNet dataset, the VGG models also generalize well and achieve the best results on other datasets <ref type="bibr" target="#b17">[18]</ref>. The most recently published state-of-the-art architecture is the ResNet, a residual learning framework with a depth of up to 152 layers but still having lower complexity than the VGG Nets. Similar to the VGG Nets, the ResNet also shows good generalization performance on new datasets other than the ImageNet <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Wildlife Classification</head><p>Monitoring wildlife through camera traps is an effective and reliable method in natural observation as it can collect a large volume of visual data naturally and inexpensively. The wildlife data, which can be fully automatic captured and collected from camera traps, however, is a burden for biologists to analyze to detect whether there exist animal in each image, or identify which species the objects belong to. Making this costly, timeconsuming manual analyzing process automated thus could dramatically reduce a large amount of human resource and quickly provide research findings. There were few attempts to build an automatic wildlife classification system. In <ref type="bibr" target="#b9">[10]</ref>, Yu et al. employed improved sparse coding spatial pyramid matching (ScSPM) for image classification <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. Animal objects are first manually detected and croppedout of the background with the whole body, then image features are extracted based on the ScSPM to convert an image or a bounding box to a single vector, finally a linear multi-class SVM is applied for classification. The average classification accuracy was at 82% on their own dataset of 7,196 images of 18 species. Chen et al. proposed a CNN-based model with automatic image segmentation preprocessing <ref type="bibr" target="#b10">[11]</ref>. The network comprises of three convolutional layers with filter size of 9 × 9, each followed by a max pooling layer with kernel size of 2 × 2, ended by a fully-connected layer and a softmax layer. In addition, different to <ref type="bibr" target="#b9">[10]</ref>, in <ref type="bibr" target="#b10">[11]</ref> the animal object cropping process was carried out automatically by applying an automatic segmentation method, namely Ensemble Video Object Cut (EVOC) <ref type="bibr" target="#b28">[29]</ref>. Although Chen's proposed framework is fully automatic and outperformed a traditional Bag-of-visual-words model based image classification algorithm <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, the recognition results obtained on their own dataset were only around 38.32%, inapplicable to practice. Motivated by the success of deep CNN-based models in recent ILSVRC contests, Gomez et al. <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b11">[12]</ref> employed deep CNN models, which have shown the state-ofthe-art performances on the ImageNet dataset, to deal with the problem of large scale wild animal identification on a new open dataset, the Snapshot Serengeti <ref type="bibr" target="#b7">[8]</ref>. More specifically, in <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b11">[12]</ref> all CNN models were pre-trained with the ImageNet dataset, then re-trained on top layers of new dataset, namely fine-tuning technique. This comes from the assumption that in data-driven approaches, a network pre-trained on a large dataset such as the ImageNet would have already learned features well for most image classification problems, resulting in better performance than training on smaller datasets <ref type="bibr" target="#b11">[12]</ref>. The method achieved their best results at 88.9% and 98.1% for top-1 and top-5 accuracy, respectively <ref type="bibr" target="#b11">[12]</ref>. This approach arises a question that, will the CNNs gain better accuracy when training on new large dataset from scratch compared to those inheriting available pre-trained models as <ref type="bibr" target="#b11">[12]</ref> did? Additionally, Gomez's approach did not solve the task of automatic animal detection to filter out non-animal images, which should be considered first on datasets containing a large number of images without animal presence.</p><p>Inspired by the great success of deep CNN-based models, in this work we apply deep CNN models for wild animal classification, similar with <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b11">[12]</ref>, on the Wildlife Spotter dataset. Different to <ref type="bibr" target="#b11">[12]</ref>, we solve the task of animal image filtering prior to the task of animal identification, as the Wildlife Spotter dataset contains a large amount of blank images (i.e. images without animal presence). Further more, for the task of animal identification, we investigate two training scenarios for comparisons: training models from scratch on the Wildlife Spotter dataset, and training with available ImageNet pre-trained models (i.e. fine-tuning).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Citizen Science</head><p>Citizen science plays an important role in many research areas, particularly in ecology and environmental sciences <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. A citizen scientist is a volunteer who contributes to science by collecting and/or processing data as part of a scientific enquiry. Significant development in digital technique, especially the Internet and mobile computing, is one of key factors responsible for the great explosion of recent citizen science projects <ref type="bibr" target="#b32">[33]</ref>. Volunteers are now able to, remotely, take part to a project by using designated applications on their mobile phones or computers to collect data or process introduced data, and then enter them online into centralized, relational databases <ref type="bibr" target="#b35">[36]</ref>. Citizen scientists now participate in many projects on a range of areas, including climate change, invasive species and monitoring of all kinds <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b35">[36]</ref>. In addition, the engagement of public significantly facilitates the area of machine learning. Supervised machine learning algorithms require large amounts of labeled data to train automated models, thus human-labeled datasets, such as Snapshot Serengeti or Wildlife Spotter, are valuable resources. Many Internet based applications, such as Google Search, Facebook or Amazon, are leveraging machine learning techniques through data collected from public user activities to enhance their business management.</p><p>Apart from valuable contributions the citizen science brings about, several challenges arise when working with citizen science data <ref type="bibr" target="#b35">[36]</ref>. Two technical principles, therefore, should be considered. First, data collected by citizen scientists must be properly validated. Second, it is important to design standard methods and tools for data collection and processing <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Wildlife Spotter Project</head><p>Wildlife Spotter is an online citizen science project undertaken by several Australian organizations and universities, taking crowd-sourcing approach to science by asking volunteers to help scientists classifying animals from millions of images collected from automatic trap cameras. These cameras, located in the nation wide: tropical rainforests, dry rangelands, and around the cities, set up to automatically snap color, high definition images day and night. To date, over 3 million images were completed. To deal with the enormous volume of images, the project invites volunteers playing as "citizen scientists" to join image analyzing. The main goal of the project is, through analyzing captured images, to assist researchers study Australian wildlife populations, behaviors and habitats to save threatened species and preserve balanced, diverse, and sustainable ecosystems <ref type="foot" target="#foot_4">4</ref> .</p><p>The Wildlife Spotter project is divided into six sub-projects, specializing on separated natural areas of Australia: Tasmanian nature reserves, Far north Queensland, South-central Victoria, Northern Territory arid zone, New South Wales coastal forests, and Central mallee lands of New South Wales <ref type="foot" target="#foot_5">5</ref> . Volunteers participate the project by registering online accounts, logging in the Web-based image classification system and manually labeling the displayed images, one by one. User assigns an introduced image to a specific species by clicking the appropriate category from a given list of animals. In case of uncertainty, blank image or image problem, user labels image as "Something else", "There is no animal in view" or "There is a problem with this image", respectively. In order to obtain reliable classification accuracy, image in the dataset is repeatedly introduced to a number of different users to label. For instance, most classified images in the Southcentral Victoria dataset each was annotated by five citizen scientists. As we described in Section I, the image datasets collected from camera traps are usually in large volume and in imperfect quality, which critically prolong processing time and probably lead to misclassification or inconsistent labeling. In this work, we aim at building a practical, fully automatic animal recognition framework for Wildlife Spotter project, freeing scientists from the burden of manual labeling, while dramatically reducing processing time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DEEP CNN FOR ANIMAL RECOGNITION FRAMEWORK</head><p>In this section, we present our proposed image classification framework and its application to the Wildlife Spotter datasets. First we describe the datasets. Then we introduce a CNNbased framework for wildlife identification. Finally we characterize selected CNN architectures employed in our experiments and implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Wildlife Spotter Dataset</head><p>We take particular interest in the Wildlife Spotter dataset of South-central Victoria, including 125,621 single-labeled images to date. The images are collected from many different scenes using 30 Reconyx HC600 Hyperfire covert cameras; and are captured at daylight without flash and at night with infrared flash in both color and grayscale settings at 1920×1080 or 2048 × 1536 resolutions. From this dataset, we take a list of 108,944 labeled images, each annotated by, approximately, 5 different citizen scientists. A citizen scientist was trained to annotate an observed animal as a category among 15 wildlife species in South-central Victoria, Australia (bandicoot, wombat, rat, brushtail possum, mouse, cat, rabbit, wallaby, ringtail possum, echidna, dog, fox, koala, kangaroo, and deer) and 3 groups of species (mammal, bird, and reptile). Image without appearance of animal is labeled as "no animal". If the user is not confident with his/her assessment due to bad quality images or occlusions, the image is labeled as "something else" or "image problem", respectively.</p><p>To preprocess the data, we eliminate the samples that are: duplicated or inconsistently labeled (i.e. the same image but being tagged differently by different citizen scientists, e.g., images are labeled as "something else" or "image problem", or having the tags "animal" and "no animal" at the same time). In the end, we obtain a set of 107,022 single-labeled images containing 34,524 non-animal samples and the rest 72,498 samples of 18 species as listed in Table <ref type="table" target="#tab_1">II</ref>, this accounts for more than 85% of the whole original dataset.</p><p>Next, we construct two settings to apply our proposed framework on two tasks: Wildlife detection and Wildlife identification. For the former, we consider a binary classification problem and experiment with both balanced and imbalanced classes. Firstly we investigate the typical situation in training machine learning algorithms: the balanced dataset; each training class has equivalently 25,000 samples for training and 8,500 for validation. The balanced dataset preparation is done by under-sampling the superior classes down to the size of the minority class. Data imbalance is a popular phenomenon in real-life problems when some classes are superior to others, and the Wildlife Spotter project is no exception. In particular, the largest population is bird with 22,145 samples while the least species, deer, appears in only 16 images. This highly imbalance probably leads to misclassification since classifier tends to be biased to superior classes. In case of imbalanced dataset we use 107,000 labeled images for training, divided into two sub-sets: training set and validation set. The training set consists of 80,000 images including 55,000 labeled as "animal" and the remaining 25,000 labeled as "no animal"; the validation set includes 18,500 and 8,500 images labeled as "animal" and "no animal", respectively.</p><p>For the later case of Wildlife identification or animal recognition, due to a large number of animals with different number of observations (cf. Table <ref type="table" target="#tab_1">II</ref>), we resort to two experimental cases: identifying the three most common species and the six most common species respectively. In case of identifying the three most common species (bird, bandicoot, and rat), we first investigate the case of balanced dataset, where each class consists of 8,000 images for training and 2,000 for validation. We then carry out training and testing on all samples of these three classes from the dataset, i.e. the imbalanced dataset case. The list of species and corresponding number of images used for the imbalanced case is listed in Table <ref type="table" target="#tab_1">II</ref>. For the most complicated task: identifying the six most common species (bird, rat, bandicoot, rabbit, wallaby, and mammal), we investigate only the case of imbalanced dataset; all samples from the six species will be used for training, 80% for training and the rest 20% for validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Recognition Framework for Animal Monitoring</head><p>As described above in this section, the Wildlife Spotter labeled dataset contains both animal and non-animal images with proportions of 67.74% and 32.26%, respectively. This fact arises two tasks of Wildlife Spotting system: (1) Wildlife detection to specify whether there exist animal in an image, and (2) Wildlife identification to identify which species the animal objects belong to. It has been shown that CNNs outperform other approaches in the topic of image classification; thus in this work we focus on adopting recent state-of-the-art CNN architectures for both those two tasks -detection and recognition.</p><p>As depicted in Figure <ref type="figure">4</ref>, our proposed recognition system consists of two CNN-based image classification models corresponding to the two addressed tasks. First a CNN-based model is designed to train a binary classifier, namely Wildlife detector; then another CNN-based model is created to train a multi-class classifier, namely Wildlife identifier.</p><p>1) CNN Architectures: Three CNN architectures with different depths are employed to our proposed framework, namely Lite AlexNet, VGG-16 <ref type="bibr" target="#b17">[18]</ref>, and ResNet-50 <ref type="bibr" target="#b20">[21]</ref>. We use a simplified version of AlexNet <ref type="bibr" target="#b19">[20]</ref> and call it Lite AlexNet, with less hidden layers and feature maps at each layer. In par-Figure <ref type="figure">4</ref>: Key steps in the proposed framework for automated wild animal identification. ticular, the Lite AlexNet comprises of three 2-D convolutional layers with ReLU activations and MaxPooling, followed by two fully-connected layers: one with ReLU nonlinear activation plus Dropout for reducing overfitting, the output layer with sigmoid activation for binary classification in detecting task and softmax activation for multiclass classification in recognizing task. All convolutional layers have small filter size of 3 × 3, while all max-pooling layers have window size of 2 × 2 pixels. VGG-16 and ResNet-50 are two representatives of the state-of-the-art CNN architectures that not only showed excellent performances on the ILSVRC <ref type="bibr" target="#b19">[20]</ref>, but also generalized well to other datasets <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b20">[21]</ref>. The input to all CNN architectures is a fixed-size 224 × 224 image in RGB color.</p><p>2) Image Processing: The Wildlife Spotter dataset contains high resolution images of 1920 × 1080 and 2048 × 1536 pixels, while the input of CNN models must be in fixed dimension. Therefore, in our experiments all original images were downscaled to 224 × 224 pixels for training. In <ref type="bibr" target="#b19">[20]</ref> this process was carried out by firstly rescaling the shorter side of image to the fixed length, then applying center cropping the image with the same length. In this work, for simplicity, we rescale both image width and height simultaneously, which may result in image distortion. Pixel intensities are normalized into the range of [0, 1]. Data quality, which can be enhanced by augmentation techniques, is a key to data-driven machine learning models; however in this work a few data augmentation processes, shearing and zooming, were applied to training images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Training Deep Networks:</head><p>Our implementation is in Keras <ref type="bibr" target="#b36">[37]</ref>, a high-level neural networks API, with TensorFlow backend <ref type="bibr" target="#b25">[26]</ref>. Adam optimizer, the first-order gradient-based optimization based on adaptive estimates of lower-order moments, was employed for training all networks <ref type="bibr" target="#b37">[38]</ref>. A small minibatch size of 16 was set to all experiments. We train our models on four NVIDIA Titan X GPUs, each network takes three to five days to finish training.</p><p>For each task we train CNN models in two scenarios: imbalanced and balanced datasets. We compute classification accuracy for both cases. In case of dataset imbalance, Fmeasure is employed in addition to accuracy, to test the robustness of the proposed system. Accuracy on the validation set is used as performance metric. To evaluate transfer learning, we carry out training Task 2 -Wildlife identification, in two scenarios: training model from scratch and fine-tuning with available ImageNet pre-trained models. Fine-tuning techniques leverage a network pre-trained on a large dataset, in this case is the ImageNet, based on the assumption that such network would have already learned useful features for most computer vision problems, thus could reach better accuracy than a model trained on a smaller dataset. Our fine-tuning process follows three steps: firstly the convolutional blocks are instantiated, then the model will be trained once on new training and validation data, finally the fully-connected model with fewer specified classes will be trained on top of the stored features <ref type="bibr" target="#b36">[37]</ref>. The data are imbalanced; the training set contains 55,000 animal images and 25,000 non-animal images, the validation set contains 18,500 animal images and 8,500 non-animal images. F-measure was used, in addition to accuracy, to evaluate the system's robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS AND DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Results for Recognizing Animal vs. Non-animal Images</head><p>Figure <ref type="figure" target="#fig_3">5</ref> shows the performance of Task 1 with three different CNN architectures on Wildlife Spotter imbalanced dataset. Overall, all models achieved very high results. In particular, the best accuracy stands at 96.6% (with VGG-16 architecture), ResNet-50 comes next at 95.96%, which is only marginally lower. These results, once again, confirm that VGG and ResNet models generalize well to other datasets as observed in <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b20">[21]</ref>. In addition, Lite AlexNet, the simplest architecture with only five learnable layers, also showed excellent result with this binary classification task. Similarly, high performance achieved with F-measure metrics indicates that these models are robust against imbalanced data.</p><p>To further test whether imblanced data is a notable issue for our animal recognition problem, Table <ref type="table" target="#tab_2">III</ref> shows the performance on balanced dataset described earlier. We keep three CNN architectures the same as in the case of data imbalance. As can be observed, for all models, the performances were only marginally degraded, which might due to undersampling process, where the samples of superior classes were considerably reduced to obtain a balanced dataset. This is again, confirming the promise of the method where it is robust with very high accuracy in detecting images with animals. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Animal Identification Results</head><p>1) Identifying the three most common species: For training the model for identifying the three most common species (bird, rat, and bandicoot) in case of imbalanced dataset, all samples from these species are used for training. The training set contains 80% images of each class (35,629 images), the validation set is the rest 20% (8,907 images). As shown in Figure <ref type="figure">6</ref>, the animal identifying task achieved very high performance for all CNN architectures with accuracy ranging from 89.16% to 90.4%. The simplest model, Lite AlexNet, demonstrates the lowest performance. The deepest model, ResNet-50, shows the best results, however the first runnerup, VGG-16, shows very close performance.</p><p>The experimental results of identifying the three most common species, in case of balanced dataset, are showed in Table <ref type="table" target="#tab_3">IV</ref>. Having trained from scratch, all three CNN models show comparable performances with classification accuracy ranging from 87.80% to 88.03%. In this case, VGG-16 outperformed the others although the gap is very small. In comparison to performance of Task 1, the Task 2 demonstrates worse results for all models. The possible reason for performance deterioration is two folds, more complicated problem caused by an increased number of classes, while a smaller number of training samples generated by under-sampling process, making the models more difficult to fit the datasets.</p><p>Fine-tuning technique was applied only to VGG-16 and ResNet-50 since these models have pre-trained weights available on the ImageNet, and our experimental results show opposite trends. While VGG-16 model achieved a little accuracy improvement of 0.2% on new dataset compared to training from the scratch, the ResNet-50 shows a serious performance drop, from 87.97% down to 76.43% in accuracy, indicating that overfitting might occurred. The most important contribution the fine-tuning technique brought to the framework is the cost of computing. In particular, to run each VGG-16 model 2) Identifying the six most common species: Towards a fully automated wild animal recognition system, we investigate further with the case of identifying the six most common classes from the Wildlife Spotter dataset (bird, rat, bandicoot, rabbit, wallaby, and mammal). In this case we only consider the case of imbalanced dataset, all samples from these six classes are used as listed in Table <ref type="table" target="#tab_1">II</ref>, each class is split into two sets, 80% images for training and 20% for validation. Figure <ref type="figure">7</ref> shows the wildlife identification results obtained by the three CNN architectures. Again, the system achieves reasonably good results for up to six different types of animal classes. In this case the ResNet-50 appears to be the best model with accuracy at 84.39%, despite the small gap, indicating that deeper CNN architectures would produce better performance in complex recognition problems. In addition, similar performances achieved with F-measure metrics indicates the robustness of the proposed system against imbalanced data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Discussion</head><p>The experimental results have shown that high accuracy can be achieved in detecting images that contain animals with more than 96% accuracy. For citizen science-based projects and systems, human annotators' time are valuable and this framework has the promise to radically improve the efficiency in these systems. For example, for the current Wildlife Spotter system up to the time we collected this dataset, more than 32% of images presented to human annotators does not contain any animal to be annotated.</p><p>The animal identification results have shown a good performance in identifying the most common three and six species of animals. While this performance may not yet be sufficient to build a fully automatic recognition, it still adds an enormous value in improving the system by automatically providing initial animal labels for human annotators. We anticipate that with more data collected over time and with a rapid growing capacity of deep learning techniques in computer vision, this performance could be improved significantly in near future.</p><p>In addition, we also believe that there are different ways that we could use the current dataset from the Wildlife Spotter Figure <ref type="figure">7</ref>: Animal identification accuracy on Wildlife Spotter dataset of the six most common species. The dataset is imbalanced as listed in Table <ref type="table" target="#tab_1">II</ref>. From each class 80% images are used for training and the 20% for validation. system to improve the performance. For example, the training dataset could further be prepared and confirmed by ecology experts, similar to the Golden Standard set in the Snapshot Serengeti project, to enhance the quality of the training data. Besides, data enhancement techniques can be applied to obtain better results; in this work only few data augmentation processes were applied as described in Section III. These will be part of our future work and report.</p><p>V. CONCLUSION Efficient and reliable monitoring of wild animals in their natural habitats is essential to inform conservation and management decisions. In this paper, using the Wildlife Spotter dataset, which contains a large number of images taken by trap cameras in South-central Victoria, Australia, we proposed and demonstrated the feasibility of a deep learning approach towards constructing scalable automated wildlife monitoring system. Our models achieved more than 96% in recognizing images with animals and close to 90% in identifying three most common animals (bird, rat and bandicoot). Furthermore, with different experimental settings for balanced and imbalanced, the system has shown to be robust, stable and suitable for dealing with images captured from the wild.</p><p>We are working on alternative ways to improve the system's performance by enhancing the dataset, applying deeper CNN models and exploiting specific properties of camera trap images. Towards a fully automated wild animal recognition system, we would investigate transfer learning to deal with problem of highly imbalanced data. In the near future, we focus on developing a "hybrid" wild animal classification framework whose automated module working as a recommendation system for the existing citizen science-based Wildlife Spotter project.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of camera trap setting in the open space (source: http://reconyx.com.au/gallery.php, 08 June 2017).</figDesc><graphic coords="1,313.75,317.65,119.72,94.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples from Wildlife Spotter image dataset under various scenarios. Original images are in resolutions of 1920 × 1080 or 2048 × 1536 pixels. All images are resized for illustration.</figDesc><graphic coords="2,314.49,365.51,119.46,67.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of a typical convolutional neural network architecture setup.</figDesc><graphic coords="4,57.79,71.33,428.89,120.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Animal vs. Non-animal image detection accuracy on Wildlife Spotter dataset of South-central Victoria, Australia.The data are imbalanced; the training set contains 55,000 animal images and 25,000 non-animal images, the validation set contains 18,500 animal images and 8,500 non-animal images. F-measure was used, in addition to accuracy, to evaluate the system's robustness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table I :</head><label>I</label><figDesc>The most common and successful CNN architectures for image classification.</figDesc><table><row><cell>Model</cell><cell>Trainable layers</cell><cell>Main specifications</cell></row><row><cell>AlexNet</cell><cell>8</cell><cell>5 convolutional layers and 3 fully-connected layers. [20]</cell></row><row><cell>VGG-16</cell><cell>16</cell><cell>13 convolutional layers with 3x3 filters, and 3 fully-connected layers. [18]</cell></row><row><cell></cell><cell></cell><cell>Developed an Inception Module that</cell></row><row><cell></cell><cell></cell><cell>dramatically reduces the number of</cell></row><row><cell>GoogLeNet</cell><cell>22</cell><cell>parameters while achieving high accuracy. Average pooling is used at top</cell></row><row><cell></cell><cell></cell><cell>of CNN instead of fully-connected</cell></row><row><cell></cell><cell></cell><cell>layers. [19]</cell></row><row><cell></cell><cell></cell><cell>A deep residual learning framework, skip</cell></row><row><cell></cell><cell></cell><cell>connections and batch normalization.</cell></row><row><cell>ResNet-50</cell><cell>50</cell><cell>Much deeper than VGG-16 (50</cell></row><row><cell></cell><cell></cell><cell>compared to 16) but having lower</cell></row><row><cell></cell><cell></cell><cell>complexity and higher performance. [21]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table II :</head><label>II</label><figDesc>Representative of animals with label data from Wildlife Spotter dataset of South-central Victoria, Australia (sorted in descending order of the number of images).</figDesc><table><row><cell>Species</cell><cell>Samples</cell><cell>Species</cell><cell>Samples</cell></row><row><cell>Bird</cell><cell>22145</cell><cell>Brushtail Possum</cell><cell>1072</cell></row><row><cell>Rat</cell><cell>11884</cell><cell>Wombat</cell><cell>616</cell></row><row><cell>Bandicoot</cell><cell>10507</cell><cell>Kangaroo</cell><cell>322</cell></row><row><cell>Rabbit</cell><cell>7965</cell><cell>Echidna</cell><cell>307</cell></row><row><cell>Wallaby</cell><cell>5370</cell><cell>Ringtail Possum</cell><cell>274</cell></row><row><cell>Mammal</cell><cell>4982</cell><cell>Dog</cell><cell>204</cell></row><row><cell>Mouse</cell><cell>3901</cell><cell>Reptile</cell><cell>158</cell></row><row><cell>Cat</cell><cell>1531</cell><cell>Koala</cell><cell>27</cell></row><row><cell>Fox</cell><cell>1217</cell><cell>Deer</cell><cell>16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table III :</head><label>III</label><figDesc>Animal vs. Non-animal image detection accuracy on Wildlife Spotter dataset of South-central Victoria, Australia. The data are balanced, each class contains 25,000 images for training and 8,500 for validation.</figDesc><table><row><cell></cell><cell>Figure 6: Animal identification accuracy on Wildlife Spotter</cell></row><row><cell></cell><cell>dataset of the three most common species (bird, rat, and</cell></row><row><cell></cell><cell>bandicoot). The training set is imbalanced as listed in Table</cell></row><row><cell></cell><cell>II, 80% images of each class are used for training, 20% for</cell></row><row><cell></cell><cell>validation.</cell></row><row><cell>Model</cell><cell>Accuracy (%)</cell></row><row><cell>Lite AlexNet</cell><cell>92.68</cell></row><row><cell>VGG-16</cell><cell>95.88</cell></row><row><cell>ResNet-50</cell><cell>95.65</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table IV :</head><label>IV</label><figDesc>Animal identification accuracy on 3 most common species of Wildlife Spotter dataset (bird, rat, and bandicoot) with different CNN architectures. The dataset is balanced. The models are trained in two scenarios:<ref type="bibr" target="#b0">(1)</ref> training from scratch, and (2) training with ImageNet pre-trained weights inheritance (i.e. fine-tuning). Note that fine-tuning was not applied to Lite AlexNet due to the unavailability of pre-trained network.</figDesc><table><row><cell></cell><cell cols="2">Accuracy (%)</cell></row><row><cell>Model</cell><cell>Training from scratch</cell><cell>Fine-tuning</cell></row><row><cell>Lite AlexNet</cell><cell>87.80</cell><cell>-</cell></row><row><cell>VGG-16</cell><cell>88.03</cell><cell>88.23</cell></row><row><cell>ResNet-50</cell><cell>87.97</cell><cell>76.43</cell></row><row><cell cols="3">training epoch on our learning system, training from scratch</cell></row><row><cell cols="3">took more than 4,000 seconds in general to complete, while</cell></row><row><cell cols="3">fine-tuning did only in around 65 seconds, 61 times faster.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>978-1-5090-5004-8/17 $31.00 © 2017 IEEE DOI 10.1109/DSAA.2017.31</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>https://www.snapshotserengeti.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>https://wildlifespotter.net.au</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>https://wildlifespotter.net.au/classify</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>https://wildlifespotter.net.au/about</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5"><p>https://wildlifespotter.net.au/projects</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work is partially supported by the Telstra-Deakin Centre of Excellence in Big Data and Machine Learning.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human domination of Earth&apos;s ecosystems</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Vitousek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lubchenco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Melillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">277</biblScope>
			<biblScope unit="issue">5325</biblScope>
			<biblScope unit="page" from="494" to="499" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Analysis of wildlife radio-tracking data</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Garrott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An analysis of a large scale habitat monitoring application</title>
		<author>
			<persName><forename type="first">R</forename><surname>Szewczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mainwaring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Polastre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Culler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Embedded Networked Sensor Systems</title>
		<meeting>the 2nd International Conference on Embedded Networked Sensor Systems</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="214" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Satellite tracking of sea turtles: Where have we been and where do we go next?</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Godley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blumenthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Broderick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Coyne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hawkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Witt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Endangered Species Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="3" to="22" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The accuracy of GPS for wildlife telemetry and habitat mapping</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Hulbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>French</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Ecology</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="869" to="878" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Monitoring wild animal communities with arrays of motion sensitive camera traps</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tilak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kranstauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Carbone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Rowcliffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fountain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eggert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1009.5718</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>O'connell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">U</forename><surname>Karanth</surname></persName>
		</author>
		<title level="m">Camera traps in animal ecology: Methods and Analyses</title>
		<imprint>
			<publisher>Springer Science &amp; Business Media</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Snapshot Serengeti, high-frequency annotated camera trap images of 40 mammalian species in an African savanna</title>
		<author>
			<persName><forename type="first">A</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kosmala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lintott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Packer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scientific Data</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">150026</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Speed of processing in the human visual system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fize</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Marlot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="issue">6582</biblScope>
			<biblScope unit="page">520</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automated identification of animal species in camera trap images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network based species recognition for wild animal monitoring</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Forrester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="858" to="862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Towards automatic wild animal monitoring: Identification of animal species in camera-trap images using very deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vargas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06169</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Why is real-world visual object recognition hard?</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">e27</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pattern recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="1" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning (ICML)</title>
		<meeting>the 25th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02344</idno>
		<title level="m">A Convolutional Encoder Model for Neural Machine Translation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Convolutional Sequence to Sequence Learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ciregan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1794" to="1801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Localityconstrained linear coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3360" to="3367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ensemble video object cut in highly dynamic scenes</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1947" to="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Bayesian hierarchical model for learning natural scene categories</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="524" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Animal identification in low quality camera-trap images using very deep convolutional neural networks and confidence thresholds</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Diaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Visual Computing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="747" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A new dawn for citizen science</title>
		<author>
			<persName><forename type="first">J</forename><surname>Silvertown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Ecology &amp; Evolution</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="467" to="471" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Citizen science: A developing tool for expanding science knowledge and scientific literacy</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bonney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kelling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">V</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shirk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioScience</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="977" to="984" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Citizen science: A study of people, expertise and sustainable development</title>
		<author>
			<persName><forename type="first">A</forename><surname>Irwin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Psychology Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Citizen science as an ecological research tool: Challenges and benefits</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zuckerberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Bonter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Ecology, Evolution, and Systematics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="149" to="172" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Keras</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
