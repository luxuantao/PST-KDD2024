<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sparse ReRAM Engine: Joint Exploration of Activation and Weight Sparsity in Compressed Neural Networks</title>
				<funder ref="#_xZnGGnv">
					<orgName type="full">National Taiwan University</orgName>
				</funder>
				<funder ref="#_xXXQZSM">
					<orgName type="full">Macronix Inc.</orgName>
				</funder>
				<funder ref="#_2r482Mg #_9VyJZHS">
					<orgName type="full">Ministry of Science and Technology of Taiwan</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tzu-Hsien</forename><surname>Yang</surname></persName>
							<email>yangc@csie.ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University, ? Academia Sinica</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hsiang-Yun</forename><surname>Cheng</surname></persName>
							<email>hycheng@citi.sinica.edu.tw</email>
						</author>
						<author>
							<persName><forename type="first">Chia-Lin</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University, ? Academia Sinica</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">I-Ching</forename><surname>Tseng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University, ? Academia Sinica</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Han-Wen</forename><surname>Hu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Macronix International Co</orgName>
								<address>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hung-Sheng</forename><surname>Chang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Macronix International Co</orgName>
								<address>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hsiang-Pang</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Macronix International Co</orgName>
								<address>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">ISCA &apos;19</orgName>
								<address>
									<addrLine>June 22-26</addrLine>
									<postCode>2019</postCode>
									<settlement>Phoenix</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sparse ReRAM Engine: Joint Exploration of Activation and Weight Sparsity in Compressed Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3307650.3322271</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Neural network</term>
					<term>sparsity</term>
					<term>ReRAM</term>
					<term>accelerator architecture</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Exploiting model sparsity to reduce ineffectual computation is a commonly used approach to achieve energy efficiency for DNN inference accelerators. However, due to the tightly coupled crossbar structure, exploiting sparsity for ReRAM-based NN accelerator is a less explored area. Existing architectural studies on ReRAMbased NN accelerators assume that an entire crossbar array can be activated in a single cycle. However, due to inference accuracy considerations, matrix-vector computation must be conducted in a smaller granularity in practice, called Operation Unit (OU). An OU-based architecture creates a new opportunity to exploit DNN sparsity. In this paper, we propose the first practical Sparse ReRAM Engine that exploits both weight and activation sparsity. Our evaluation shows that the proposed method is effective in eliminating ineffectual computation, and delivers significant performance improvement and energy savings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Computer systems organization ? Neural networks; Special purpose systems; ? Hardware ? Memory and dense storage.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>computing and memory demands introduce performance and energy efficiency challenges to the underlying processing hardware.</p><p>Memristor-based neural network accelerators have been shown to be a promising solution to meet the performance and energy efficiency challenges for DNN inference. In contrast to the conventional von Neumann architecture, where computation and data storage are separate, emerging memristor devices such as resistive random access memory (ReRAM) <ref type="bibr" target="#b21">[22]</ref> are able to perform arithmetic operations beyond data storage. Recent works have demonstrated that ReRAM crossbar arrays can be used to efficiently perform matrixvector multiplication in convolution and fully-connected layers of DNNs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b39">40]</ref>. By storing filter weights as the conductance of ReRAM cells and converting input feature maps into input voltage signals, we have the dot-product results (output feature maps) at the end of the bitlines in ReRAM crossbar arrays. With such computing-in-memory capability, memristor-based DNN accelerators can reduce data movement and provide significant energy savings compared to CPU and GPU based DNN acceleration platforms <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>Despite this promising potential, the development of ReRAMbased DNN accelerators is still in its early stage and there remain challenges to overcome. One primary concern is how to efficiently exploit sparsity, which is commonly done in digital CMOS-based accelerators to improve energy efficiency <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b48">49]</ref>. Many studies have shown that common neural networks have significant redundancy in filter weights and can be pruned dramatically during training without substantially affecting accuracy <ref type="bibr" target="#b17">[18]</ref>. In addition to weight sparsity, a massive amount of input activations are zeros in typical neural network models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32]</ref>, as many neural networks employ the ReLU function which clamps all negative activation values to zero as their nonlinear operator. Furthermore, in ReRAM-based DNN accelerators, there exists a finer-level of sparsity granularity to be exploited, bit-level sparsity, due to the cell bit-density and limited wordline driver resolution. Eliminating zero values in filter weights and input activations are important for both performance and energy. However, due to the tightly coupled crossbar structure, it is difficult to exploit sparsity efficiently in ReRAM-based DNN accelerators.</p><p>In a ReRAM crossbar architecture, weights stored in the same wordline need to multiply to the same input, and accumulated currents flowing through the same bitline contribute to the same output. Sparsity can only be exploited when the entire wordline or bitline cells contain zeros. Similarly, the sparsity of feature maps can be leveraged when the input bits to the crossbar array are all zeros in the same cycle. A structural pruning algorithm <ref type="bibr" target="#b44">[45]</ref> has been proposed to regularize the distribution of zero weights, so that more all-zero rows and columns can be found. SNrram <ref type="bibr" target="#b43">[44]</ref> seeks to enable fine-grained column compression at the cost of high output indexing overhead.</p><p>Existing sparsity solutions <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b43">44]</ref> are based on an over-idealized ReRAM crossbar architecture <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">40]</ref>. Most existing ReRAM-based DNN accelerators published in the architectural community overlook the accumulated effect of per-cell current deviation on inference accuracy, as well as the overhead from ADC. Thus they assume that an entire 128?128 or 256?256 crossbar array can be activated in a single cycle. However, in practice, to achieve satisfactory inference accuracy, matrix-vector multiplication in ReRAM-based DNN accelerators must proceed at a smaller granularity, called an Operation Unit (OU) <ref type="bibr" target="#b30">[31]</ref>. For example, only nine wordlines and eight bitlines are turned on concurrently within a 512?256 crossbar array in a state-of-the-art ReRAM macro designed for DNN acceleration <ref type="bibr" target="#b5">[6]</ref>. Therefore, a practical OU-based ReRAM accelerator is very likely to deliver lower performance compared to an over-idealized design like ISAAC <ref type="bibr" target="#b39">[40]</ref> or PRIME <ref type="bibr" target="#b8">[9]</ref> since less computation is done in one cycle. However, unlike the over-idealized design, in which the entire crossbar array operates in one cycle, each OU in a crossbar is activated independently. This opens up a new design opportunity for sparsity exploration in a ReRAM-based DNN accelerator.</p><p>In this paper, we propose the first practical Sparse ReRAM Engine (SRE) that takes advantage of fine-grained OU-based computations to jointly exploit weight and activation sparsity. Weight compression can be done at the OU level in either the row or column dimension. Row-based compression requires input indexing to fetch the correct inputs for compressed weights, while column-based compression requires output indexing. For activation compression, a naive approach for OU-based ReRAM architecture is to skip an OU computation when inputs to all the wordlines of the OU are zeros. We exploit activation sparsity further with Dynamic OU Formation, a novel method which activates non-contiguous wordlines with non-zero input values in the same cycle to form an OU unit at run-time. This method works perfectly with row-wise weight compression to allow for joint exploration of weight and activation sparsity for ReRAM-based DNN accelerators. Evaluation results show that, for neural networks that use the structural pruning algorithm <ref type="bibr" target="#b44">[45]</ref> to regularize weight sparsity during training, SRE provides up to 42.3x performance speedups (average 13.1x) and up to 95.4% energy savings (average 85.3%) over a baseline that does not exploit sparsity.</p><p>In summary, this paper offers the following contributions:</p><p>? We study the design challenges of sparsity exploration on ReRAM-based DNN accelerators, and show that a practical OUbased design enables new opportunities to effectively exploit DNN sparsity. To our best knowledge, this is the first sparsity exploration work that targets a practical hardware design for ReRAM-based accelerators instead of an over-idealized architecture.</p><p>? We propose a sparse ReRAM engine (SRE) to jointly exploit weight and activation sparsity, with only minimal indexing overhead. Our design takes advantage of fine-grained OU-based computations and combines row-wise weight compression with dynamic wordline activation to provide significant performance speedup and energy savings.</p><p>? We also compare SRE with the over-idealized ReRAM architecture, which overlooks the inference accuracy loss caused by the accumulated effect of per-cell current deviation. We show that SRE successfully enables a practical ReRAM-based DNN accelerator design , which achieves satisfactory inference accuracy considering the limitation of ReRAM cell reliability while delivering comparable performance and energy efficiency with the over-idealized ReRAM DNN accelerator.</p><p>The rest of the paper is organized as follows. Section 2 provides background on the architecture of ReRAM-based DNN accelerator and its challenges in exploiting DNN sparsity. The practical OUbased ReRAM accelerator design is introduced in Section 3, and the new opportunities to exploit weight and activation sparsity in such OU-based design is discussed in Section 4. The proposed Sparse ReRAM Engine is explained in details in Section 5. Section 6 and Section 7 describe evaluation methodologies and results, followed by a summary of related work in Section 8. Finally, we conclude the paper in Section 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND 2.1 ReRAM-based DNN Accelerator Architecture</head><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the generic ReRAM-based DNN accelerator architecture assumed in a few works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b39">40]</ref>. The ReRAM-based deep learning accelerator is composed of multiple processing engines (PEs) connected with on-chip interconnects. Each PE consists of multiple computation units (CUs), each of which has multiple crossbar arrays, which are responsible for the acceleration of matrixvector multiplications in convolution and fully-connected layers. By storing filter weights as the conductance of ReRAM cells and converting input feature maps into input voltage signals, we can obtain the dot-product results (output feature maps) by reading out the accumulated currents on the bitlines. A wordline driver (WLD) such as a digital-to-analog converter (DAC) or an inverter <ref type="bibr" target="#b39">[40]</ref> is connected to each wordline of the ReRAM crossbar array to convert the input feature map data to input voltages. The accumulated currents on the bitlines (sum-of-products results) are read out by sample-and-hold (S&amp;H) circuits and fed to the shared analog-todigital converters (ADCs). In addition to the crossbar arrays, there is one on-chip eDRAM buffer in each PE for temporarily storing input and output feature maps. A non-linear function unit and a pooling unit are also included in the PE to support the implementation of the non-linear function and pooling layer in the neural network.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> shows a high-level view of how to map filter weights to a crossbar array for a convolution layer with a 4?4?2 feature map and four 2?2?2 filters. The weights of one filter are mapped to the cells of one bit line. In an ideal ReRAM-based DNN accelerator design <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b39">40]</ref>, all the wordlines in the crossbar array can be activated concurrently in a single cycle. At every cycle, a 2?2?2 input vector of the input feature map is converted to the input voltages of the crossbar array via the WLD. The input sliding window (the red rectangular box in the figure) then shifts right (or down) and the corresponding input vector is fed into the crossbar array in the next cycle. Due to the limited WLD resolution and ReRAM cell  density, in practice, the input vector is decomposed and fed into wordlines using multiple clock cycles, and each data in the original filter weight is also decomposed and mapped onto different bitlines as illustrated in Figure <ref type="figure" target="#fig_2">3</ref>. For simplicity, we show the mapping result of the weights only in the first channel of the filters. In this example, suppose that four filters are mapped to the 4?8 crossbar array: the WLD resolution is one-bit, each cell can store two bits, the precision of each feature map data is 2-bit, and the precision of each filter weight is 4-bit. So each 4-bit filter weight is decomposed into two concatenated 2-bit values and thus the weights of each filter span two bit-lines. Similarly, due to limited WLD resolution, the 2-bit input feature map is separated into LSB and MSB groups and fed into wordlines sequentially. In this example, to compute the output neuron for the first input sliding window <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b0">1]</ref>, the data we feed into the input register after decomposition are [1, 0, 1, 1] and [0, 1, 1, 0]; it costs two cycles to get the LSB and MSB part of the output neuron. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Challenges in Exploiting DNN Sparsity in ReRAM-based DNN Accelerator</head><p>Recent studies show that most neural network models have significant amounts of zeros in filter weights and input activations <ref type="bibr" target="#b2">[3]</ref>; pruning zero weights and skipping zero activations can help to reduce resource consumption without accuracy loss. In typical deep learning models, about 50% to 70% of input activations are zeros <ref type="bibr" target="#b35">[36]</ref>, as many neural networks employ the ReLU function, which clamps all negative activation values to zero as their non-linear operator. To further create weight sparsity, algorithmic techniques such as quantization <ref type="bibr" target="#b16">[17]</ref>, low-rank matrix factorization <ref type="bibr" target="#b11">[12]</ref>, and 1 -norm regularization <ref type="bibr" target="#b27">[28]</ref> have been proposed to prune the network during training. Such high-degree sparsity in input activations and filter weights provides great potential for the underlying deep learning hardware platforms to achieve better performance and energy efficiency. In addition to the sparsity presented in NN models, there exists a finer-level of sparsity granularity to be exploited in ReRAM-based DNN accelerators, bit-level sparsity. As explained in Section 2.1, since a ReRAM cell can store only a limited number of bits, filter weights are decomposed and mapped to multiple bitlines. Similarly, the input vector is decomposed and fed into wordlines using multiple clock cycles, as the WLD has limited resolution. This introduces more opportunities to exploit sparsity for ReRAM-based DNN accelerators.  However, even though there are inherently more ineffectual computations for ReRAM-based DNN accelerators to exploit than digital DNN accelerator designs, the coupled crossbar structure makes it difficult to efficiently exploit sparsity in ReRAM-based DNN accelerators. As we observe from the example in Figure <ref type="figure" target="#fig_2">3</ref>, the cell at the 1 st wordline and 2 nd bitline is zero, but we cannot shift up the rest of the cells of the corresponding bitline (2 nd bitline), since the shifted weights would then be multiplied by a wrong input value. Hence for ReRAM-based accelerators, we must find all-zero rows/columns of a crossbar array for compression. To increase the possibility of all-zero rows/columns, Ji et al. <ref type="bibr" target="#b23">[24]</ref> propose ReCom, which uses SSL <ref type="bibr" target="#b44">[45]</ref> to regularize the distribution of zero weights, so that more all-zero rows and columns can be found. However, with ReCom, there are still many zero weights left in the compressed model. SNrram <ref type="bibr" target="#b43">[44]</ref> compresses the model at a finer level, i.e., allzero filters in structurally compressed neural networks. As the size of a filter is usually smaller than a column, SNrram better exploits sparsity than ReCom <ref type="bibr" target="#b23">[24]</ref>. However, this finer granularity comes at the expense of an output indexing module which incurs significant storage overhead.</p><p>To exploit activation sparsity, ReRAM-based accelerators also face a new challenge. In contrast to digital accelerators in which the arithmetic unit sequentially computes each individual input value, ReRAM's crossbar array handles the computation of multiple inputs in parallel, as shown in Figure <ref type="figure" target="#fig_2">3</ref>. Even though a zero-valued wordline such as the 2 nd wordline at the first cycle in Figure <ref type="figure" target="#fig_2">3</ref> does not consume power, a single zero-valued wordline cannot be exploited to reduce execution time. Activation sparsity can be exploited for performance improvement only when all the decomposed input bits fed into the crossbar array in a single cycle are all zeros.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A PRACTICAL ReRAM ACCELERATOR ARCHITECTURE</head><p>In a practical ReRAM-based DNN accelerator, only a limited number of wordlines and bitlines in a crossbar array can be activated in a single cycle <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b46">47]</ref>. The maximum number of wordlines that can be turned on concurrently depends on accuracy limitations, while the number of bitlines that can be concurrently activated is constrained by the number of ADCs connected to a crossbar array and the throughput of each ADC. For example, only nine wordlines and eight bitlines are turned on concurrently within a 512?256 crossbar array in a state-of-the-art ReRAM macro designed for DNN acceleration <ref type="bibr" target="#b5">[6]</ref>.</p><p>Turning on a massive number of wordlines concurrently makes it difficult for the ADC to accurately read out the sum-of-product values accumulated on the bitline <ref type="bibr" target="#b30">[31]</ref>. As ReRAM cells are nonideal <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b30">31]</ref>, the per-cell current deviation accumulates on the bitline and leads to overlap with neighboring states (i.e., sum-ofproduct values) in the accumulated current distribution. The overlap with neighboring states makes it difficult for the ADC with its limited sensing margin to differentiate between different states <ref type="bibr" target="#b5">[6]</ref>. Thus, if all of the wordlines in a crossbar array are turned on simultaneously, an incorrect sum-of-products result can be produced, even though none of the cell stores an error value <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b30">31]</ref>. When too many wordlines are activated concurrently, the sum-of-products errors per bitline degrade the inference accuracy of neural networks.</p><p>We use DL-RSIM <ref type="bibr" target="#b30">[31]</ref> to analyze the inference accuracy of different neural networks when various numbers of wordlines are activated concurrently 1 , as shown in Figure <ref type="figure" target="#fig_5">5</ref>. Since oxide-based ReRAM has good electronic properties (high density, low switching energy, and high endurance) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b45">46]</ref> and thus is commonly deployed in latest ReRAM-based DNN accelerator studies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b46">47]</ref>, we use one of the oxide-based ReRAM, WOx ReRAM <ref type="bibr" target="#b21">[22]</ref>, for our evaluation. We choose the R-ratio and resistance-deviation (? ) of WOx ReRAM <ref type="bibr" target="#b21">[22]</ref> as the baseline setting (R b and ? b ), and analyze the inference accuracy for three different ReRAM cells: the cells with</p><formula xml:id="formula_0">(R-ratio, ? ) = (R b , ? b ), (2 ? R b , ? b /2), and (3 ? R b , ? b /3).</formula><p>As shown in the figure, the inference accuracy decreases when the number of concurrently activated wordlines increases. Turning on all of the wordlines in a 128?128 crossbar array degrades the inference accuracy to an unacceptable level. Increasing R-ratio and reducing ? can help to improve the inference accuracy by shrinking the overlap with neighboring states in the accumulated current distribution to reduce ADC sensing errors. Nevertheless, even if advances in technology enable the R-ratio/? to increase/shrink by 1 The evaluated NN models are described in Section 6. 3x of the WOx ReRAM <ref type="bibr" target="#b21">[22]</ref>, the inference accuracy drops considerably, especially at CaffeNet, when more than 16 wordlines are activated concurrently.</p><p>In addition to the limitation on the number of concurrently activated wordlines, the number of bitlines that can be turned on simultaneously is constrained by the number of ADCs connected to a crossbar array and ADC's throughput. Since an ADC consumes a significant amount of power and chip area <ref type="bibr" target="#b39">[40]</ref>, in practical designs multiple bitlines share an ADC <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b41">42]</ref>. In addition, when considering the area and power constraints of an accelerator chip, it is impractical to deploy higher-frequency ADCs. Thus, in the state-of-the-art ReRAM macro designed for DNN acceleration <ref type="bibr" target="#b5">[6]</ref>, only a limited number of bitlines can be operated in a single cycle.</p><p>Considering the constraints on the number of concurrently activated wordlines and bitlines, a practical ReRAM accelerator can only perform a portion of dot-product computation in a convolution or fully-connected layer within a cycle. We define the maximum amount of dot-product computations that can be performed within a single cycle in a crossbar array as an Operation Unit (OU), with at most S W L wordlines and S BL bitlines activated concurrently. Note that an OU-based architecture does not physically split a large crossbar into smaller ones. It activates a smaller section (OU) of a crossbar array within a single cycle. As shown in Figure <ref type="figure" target="#fig_6">6</ref>, an additional wordline activation vector is used to indicate which wordlines should be turned on. Assuming the OU size is 2?2, then only two entries in the wordline activation vector are set to 1. A 2-to-1 multiplexer is connected to each DAC to determine the on/off state of the wordline. When operating the crossbar array in normal memory mode to store synaptic weights in ReRAM cells after offline training, the NN_mode signal is set to 0 and the on/off state of each wordline is specified by the row decoder. During the inference process, the NN_mode signal is set to 1 and the on/off state of each wordline is specified by the wordline activation vector. The multiplexers and the wordline activation vector induce only minimal peripheral overhead. Figure <ref type="figure" target="#fig_7">7</ref> shows an example of the OU-based dot-product computation for a crossbar array with a 2?2 OU size. Assuming we perform the dot-product computation in the order OU1, OU2, OU3, and OU4, the corresponding wordlines are activated in eight different cycles, as marked by C1 to C8 in the figure. The dot-product results of different OUs that share the same set of bitlines, such as OU1 and OU2, are added together before the shift-and-add circuit assembles the final result based on the bit position of the input and  synaptic weight. For example, at the first cycle, the matrix-vector multiplication of OU1 and the LSB of the first two values in the input sliding window <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">0]</ref> is performed to obtain the output <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">0]</ref>. The matrix-vector multiplication of OU2 and the LSB of the rest of values in the input sliding window <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b0">1]</ref> is then performed at the second cycle to yield the output <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. The output of OU1 and OU2 are added together, resulting in the summed output <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b3">4]</ref> before being assembled by the shift-and-add circuit.</p><p>For the example shown in Figure <ref type="figure" target="#fig_7">7</ref>, it takes two cycles for an over-idealized design described in Section 2.1 to complete the dotproduct computation, while the OU-based design requires eight cycles. However, since fewer wordlines are activated in one cycle, lower-resolution ADCs can be deployed in the OU-based design. Thus, as the sensing speed of an ADC is proportional to the ADC's bit-resolution <ref type="bibr" target="#b37">[38]</ref>, the ADC sensing time in an OU-based architecture is shorter. Compared to the trivial RC delay (around 10ps for a 100?100 crossbar array <ref type="bibr" target="#b34">[35]</ref>), ADC sensing time brings a much larger impact on the operating speed of a ReRAM crossbar array <ref type="bibr" target="#b46">[47]</ref>. Hence, the cycle time of a ReRAM-based DNN accelerator is dictated by the slowest ADC sensing stage and the OU-based architecture could achieve shorter cycle time compared with the over-idealized counterpart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">NEW OPPORTUNITY FOR EXPLOITING SPARSITY IN OU-BASED ReRAM ACCELERATOR</head><p>The OU-based ReRAM accelerator creates new opportunities to exploit DNN sparsity. Unlike the over-idealized design where an entire crossbar array operates in a single cycle, each OU in a crossbar is activated independently. This naturally enables us to exploit a finer granularity of weight and activation compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Weight Compression</head><p>There are two ways to exploit weight sparsity: OU-row compression and OU-col compression. In Figure <ref type="figure" target="#fig_8">8</ref>, we use an example with a 4?4 crossbar size and a 2?2 OU size to illustrate these two different types of weight compression, assuming 1-bit WLD resolution and two bits of storage for each ReRAM cell. Figure <ref type="figure" target="#fig_8">8</ref>(a) shows the original weight mapping and computation sequence before row compression. The first four weights (marked in green) of Filters 1 and 2 are mapped into the crossbar. The decomposed value of the first four feature map elements (marked in green) in the first input sliding window of the convolution are fetched to the Input Register and sequentially fed into wordlines. It takes four cycles to complete the LSB part of inputs and another four cycles to complete the MSB part, in order to get the dot-product results of the first four output channels (O1 to O4) in this window of convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OU-Row Compression</head><p>Row-based compression at the OU level eliminates zero row vectors and shifts the remaining row vectors up. In Figure <ref type="figure" target="#fig_8">8</ref>(a), we observe that all the weights of the 2 nd row vector in OU1, the 1 st row vector in OU2, the 1 st row vector in OU3, and the 2 nd row vector in OU4 are zeros. As no crossbar rows are completely composed of zeros, the dot-product computation of these zero weights cannot be skipped in an over-idealized design where the entire crossbar array operates concurrently. With OU-row compression, we can remove these zero rows within each OU and shift the remaining rows up, as shown in Figure <ref type="figure" target="#fig_8">8(b)</ref>. From this illustration, we see that the input order is different from the original, as some rows of weights are skipped. For OU1 and OU2, the 1 st , 4 th , 5 th , and 6 th element (with indexes 0, 3, 4, and 5) of the feature map's input sliding window are to be fetched to the Input Register, while for OU3 and OU4, the 2 nd , 3 rd , 5 th , and 6 th element (with indexes 1, 2, 4, and 5) of the feature map's input sliding window are to be fetched. Hence, to support OU-row compression, we need an input index buffer to store input indexes and an input indexing unit to fetch the correct inputs for compressed weights. For every column-wise OU group (e.g., OU1 and OU2 belong to the same column-wise OU group), an eDRAM access is required to fetch correct inputs to the Input Register based on the input indexes. Thus, for this example, two eDRAM accesses are required to support OU-row compression, as there are two column-wise OU groups and every column-wise OU group has its own input indexes. Note that as OU-row compression does not change the output channel mapped to each bitline, no output indexing is needed. In this example, the convolution of more feature map elements (6 elements) can be done in 8 cycles when OU-row compression is applied; the skipped computations with zero weights reduce energy consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OU-Col Compression</head><p>In a similar fashion, OU-level column-based compression eliminates zero column vectors and shifts the remaining column vectors left. In Figure <ref type="figure" target="#fig_8">8</ref>(a), we see that all the weights of the 2 nd column vector in OU1 and the 2 nd column vector in OU4 are zeros. As no crossbar columns are completely composed of zeros, the dotproduct computation of these zero weights cannot be skipped in an over-idealized design. With OU-col compression, we can remove these zero columns and shift the remaining columns left, as shown in Figure <ref type="figure" target="#fig_8">8(c)</ref>. From this illustration, we see that the output order is different from the original, as some of the columns of the weights within an OU are skipped. In the original scheme, the mapping between the output current of each bitline and the output channel (Oi) is fixed. When OU-col compression is applied, for OU1, the output current of the 2 nd bitline should be mapped to the 3 rd output channel. For OU3, the output current of the 1 st /2 nd bitline should be mapped to the 4 th /5 th output channel, while for OU4, the output current of the 2 nd bitline should be mapped to the 5 th output channel. Note that OU-col compression does not change the input order. In this example, in addition to Filters 1 and 2, the convolution with the MSB part of the first four elements of Filter 3 can also be done within 8 cycles when OU-col compression is applied; the skipped computations with zero weights reduce energy consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Activation Compression</head><p>One naive way to exploit activation sparsity for the OU-based ReRAM architecture is to skip an OU computation when the inputs concurrently feed into all the wordlines of the OU are zeros. To exploit activation sparsity further, we propose a method called Dynamic OU Formation (DOF). In the example shown in Figure <ref type="figure" target="#fig_8">8</ref>(a), we observe that the input bits feed into the 2 nd and 3 rd wordlines are zeros when we first compute the LSB part of inputs. The computation associated with these two wordlines cannot be skipped in the aforementioned naive method, as these two wordlines belong to different OUs. The idea of DOF is to activate the 1 st and 4 th wordlines together in the same cycle for computing the LSB part of inputs, as shown in Figure <ref type="figure" target="#fig_9">9</ref>. That is, a virtual OU execution unit is formed dynamically. With DOF, we can skip the 2 nd and 3 rd wordlines for the computation of LSB part of inputs, and also the 1 st and 4 th wordlines for the MSB part of inputs. As a result, we need only 4 cycles instead of 8 cycles to complete the convolution in this example; the skipped computations save on energy use. Note that DOF does not change the output channel associated with each bitline; every activated wordline within a dynamically formed OU must follow the same output indexing to guarantee correctness. To jointly exploit DOF and weight compression, we must adopt row-wise compression for weights, as row-wise compression does not change the bitline-to-output-channel mapping. With column compression, the same bitline shared by different OU blocks may be mapped to different output channels. For the example shown in Figure <ref type="figure" target="#fig_8">8</ref>(c), after column compression, the output current of the 2 nd  bitline in OU1 and OU2 is mapped to the 3 rd and 2 nd output channel respectively, and the output current of the 1 st bitline in OU3 and OU4 is mapped to the 4 th and 3 rd output channel respectively. If we attempt to apply DOF, as shown in Figure <ref type="figure" target="#fig_10">10</ref>, the partial sum of the dynamically formed OU may accumulate the currents associated with the convolution of different filters and thus the value of the output channel may be wrong. For example, in Figure <ref type="figure" target="#fig_10">10</ref>, the 2 nd bitline of the dynamically formed OU1 wrongly accumulates the output currents that should be mapped to the 2 nd and 3 rd output channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SPARSE ReRAM ENGINE</head><p>In this section, we present the Sparse ReRAM Engine (SRE), in which we use the techniques described in Section 4 (OU-Row Compression + Dynamic OU Formation) for exploiting both weight and activation sparsity. Figure <ref type="figure" target="#fig_11">11</ref> shows the architecture and dataflow of the proposed Sparse ReRAM Engine inside a PE. The weight matrices are first compressed offline using OU-based row compression (ORC) and the corresponding input indexing information is also generated. In each CU, the Input Index Buffer is used to store the input indexing information for the filter weights mapped to this CU. To reduce the storage overhead for input indexes, we store the index difference instead of the absolute values, similar to the approach in <ref type="bibr" target="#b48">[49]</ref>. The Wordline Vector Generator produces the wordline activation vector at each cycle to support Dynamic OU Formation. Below we describe the Index Decoder, the Wordline Vector Generator, and the SRE pipeline in details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Index Decoder</head><p>To reduce the index storage overhead, we store index differences instead of absolute values. Figure <ref type="figure" target="#fig_12">12</ref> shows an example: in (a), we see that the non-zero row vectors correspond to index values 1, 3 and 9, and so on. Thus, instead of storing 1, 3, and 9 in the Input Index Buffer, we store the index differences: 1, 2 and 6. Hence one encoded index of an input address requires log 2 max_dist bits, where max_dist is the maximal difference between two non-zero row vectors in the target model. Thus the index storage overhead varies for different models. To bound the storage overhead, we adopt zero-padding <ref type="bibr" target="#b16">[17]</ref>, in which filler zeros are added if the index difference exceeds the largest unsigned number that can be represented with the target number of index bits. In the example shown in Figure <ref type="figure" target="#fig_12">12</ref>, if we limit the number of index bits to 2, the third encoded index value is 6, which exceeds the bound. In this case, as shown in Figure <ref type="figure" target="#fig_12">12</ref>(b), we insert a zero row at index 7 in the compressed weight matrices. As doing so reduces the index storage overhead but also affects the weight compression ratio, the index length (target number of index bits) should be chosen carefully, considering this tradeoff for each model.</p><p>The decoding procedure for encoded index values uses prefix sum operations, as illustrated in Figure <ref type="figure" target="#fig_13">13</ref>. The Index Decoder is an implementation of Hillis and Steele's algorithm <ref type="bibr" target="#b20">[21]</ref> for parallel prefix sum, as shown in Figure <ref type="figure" target="#fig_14">14</ref>. The decoder width depends on the required decoding throughput. In Section 5.3, we discuss this issue further. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dynamic OU Formation</head><p>Figure <ref type="figure" target="#fig_15">15</ref> shows the required hardware support for Dynamic OU Formation. The Wordline Vector Generator module decides which wordlines should be activated within a cycle to dynamically form a virtual OU execution unit. The example illustrated in Figure <ref type="figure" target="#fig_15">15</ref> assumes 1-bit DAC resolution, 8 wordlines in a crossbar, and a 2?2 OU size. The decomposed input values (1-bit values in this example) are passed to the mask vector to mark non-zero inputs<ref type="foot" target="#foot_0">2</ref> . Then a prefix sum operation is performed on this mask vector. Thus the value in the i-th element of the prefix-sum vector indicates the number of non-zero inputs between wordline 0 and wordline i. In this example, we need to find two wordlines with non-zero inputs at every cycle and mark the entries of these two wordlines in the wordline activation vector. To achieve this goal, we use the prefixsum vector, the mask vector, and a set of comparators to perform a condition check. At cycle c, the wordlines that need to be activated are wordline j, which satisfies</p><formula xml:id="formula_1">(1 + (c -1)S W L ? Prefix_sum[j] &lt; 1 + c ? S W L ) &amp; mask_vector[j],</formula><p>where S W L is the number of wordlines in an OU (S W L =2 in this example). To implement this condition check, counters L and H are set initially to 1 and 1+S W L , and incremented by S W L every cycle. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">SRE Pipeline</head><p>Figure <ref type="figure" target="#fig_16">16</ref> shows the pipeline diagram of the SRE engine. Each OU computation is completed in one cycle and the outputs of the OU computation are latched in the S&amp;H circuit. In the next cycle, these outputs are fed to the ADC unit. The result of the ADC is then fed to the shift-and-add unit (S+A), where the result is assembled with the data stored in the output register (OR) based on the bit position of the inputs and synaptic weights. These three stages (OU, ADC, and S+A OR Wr) are executed in a pipelined manner to produce the convolution result of each output neuron.</p><p>Before each OU computation, we must fetch the associated inputs and activate the corresponding wordlines. To support ORC and DOF, for a crossbar array with n wordlines, we must fetch n inputs (termed a batch) at a time from the on-chip eDRAM buffer to the input register (IR), based on the indexes decoded by the Index Decoder. The Wordline Vector Generator (WL Vec Gen) then generates one wordline activation vector from the batch at each cycle to specify which wordlines should be concurrently activated in the  next cycle to perform the OU computation. Except for the first batch of input data, index decoding (Index Decod) and input data fetching (On-chip Buffer Rd + IR) can be performed concurrently with the OU computation, as shown in Figure <ref type="figure" target="#fig_16">16</ref>. The number of cycles required to process a batch of inputs depends on the results of the Dynamic OU Formation. In the extreme case, no OU computation is required for one input batch if all of the input values in the batch are zeros. Thus, to minimize pipeline stalls, index decoding must be completed within one cycle; likewise for input data fetching.</p><p>The cycle time of the SRE pipeline is dictated by the slowest stage, which is the sensing of the OU computation result (ADC stage). In the 65nm ReRAM macro <ref type="bibr" target="#b5">[6]</ref> with 3-bit sensing resolution, the clock cycle time is 15.6 ns. Assume a 128?128 crossbar with a 16?16 OU, 2-bit ReRAM cells, and 16-bit feature map values: we need a 6-bit ADC to read out the OU computation result, with a cycle time of approximately 30 ns, as the sensing speed of an ADC is proportional to the ADC's bit-resolution <ref type="bibr" target="#b37">[38]</ref>. To minimize pipeline stalls, the Index Decoder must decode 128 inputs in 30 ns. Similarly, the Wordline Vector Generator must also generate one wordline activation vector every 30 ns. We implement the Index Decoder and the Wordline Vector Generator in Verilog and use Synopsys Design Compiler to synthesize latency, area, and power. Based on our synthesis results, the width (parallelism degree) of the Index Decoder and the Wordline Vector Generator are both set to eight, to meet throughput requirements while minimizing the area and power consumption. Note that it is also possible to design the eDRAM buffer (8 banks and 512 bits bus width) based on the access latency modeled by CACTI <ref type="bibr" target="#b33">[34]</ref> to ensure that fetching 128 16-bit inputs for each crossbar array can be completed in one cycle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION METHODOLOGY</head><p>We implement a custom cycle-accurate simulator written in Python to evaluate the performance and energy consumption of the Sparse ReRAM Engine. Our simulator models the mapping and execution flow of sparse neural networks on ReRAM crossbar arrays. Table <ref type="table" target="#tab_1">1</ref> shows the hardware configuration of each PE for our simulation. Except for the OU-related components (ADC and eDRAM buffer), we use the hardware configuration from ISAAC <ref type="bibr" target="#b39">[40]</ref>, a state-ofthe-art over-idealized design. The power consumption of all the memories, including the eDRAM buffer, IR, and OR, is modeled using CACTI <ref type="bibr" target="#b33">[34]</ref> at 32nm process assumed in ISAAC <ref type="bibr" target="#b39">[40]</ref>. Each PE has a 64KB on-chip eDRAM buffer to store intermediate input and output feature maps. The eDRAM buffer is configured to ensure that fetching a batch of input data could be completed in one cycle. There are 12 CUs within each PE; each CU has 8 crossbar arrays. The crossbar array size is set to 128?128, and each ReRAM cell can store two bits. Anticipating future improvements in cell reliability, we set the OU size to 16?16 3 . Since a 6-bit ADC is sufficient for a 16?16 OU, we use the same style of ADC as in ISAAC <ref type="bibr" target="#b39">[40]</ref> but follow the equation in <ref type="bibr" target="#b37">[38]</ref> to scale the ADC power consumption for the lower bit resolution. For the analysis of indexing overhead, we implement the Index Decoder and Wordline Vector Generator in Verilog and synthesize using the Synopsys Design Compiler under TSMC 28nm process 4 . The obtained power and area are scaled up to 32nm process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Workloads</head><p>We evaluate the proposed Sparse ReRAM Engine on three datasets: MNIST <ref type="bibr" target="#b26">[27]</ref>, CIFAR-10 <ref type="bibr" target="#b24">[25]</ref>, and ImageNet <ref type="bibr" target="#b10">[11]</ref>. MNIST and CIFAR-10 are small-scale datasets, whereas ImageNet is a largescale dataset. In our evaluation, we use the ILSVRC 2012, a subset of ImageNet with approximately 1000 categories, each of which includes 1000 images. The neural networks used in our evaluation are LeNet <ref type="bibr" target="#b26">[27]</ref> on MNIST, a CNN with three convolution layers and two fully-connected layers on CIFAR-10, and four large-scale CNNs (CaffeNet <ref type="bibr" target="#b25">[26]</ref>, VGG-16 <ref type="bibr" target="#b40">[41]</ref>, GoogLeNet <ref type="bibr" target="#b42">[43]</ref>, and ResNet-50 <ref type="bibr" target="#b18">[19]</ref>) on ImageNet. Table <ref type="table" target="#tab_2">2</ref> lists the network topology of these evaluated NN models. These models are all trained with the SSL pruning algorithm <ref type="bibr" target="#b44">[45]</ref>. CaffeNet and VGG-16 are released by <ref type="bibr" target="#b44">[45]</ref> and thereby were well trained for structural sparsity. Thus, in the results shown in Section 7, CaffeNet and VGG-16 show higher gains from OU-based row compression than the other models, which are trained by ourselves and are not well tuned for structural sparsity as CaffeNet and VGG-16. These NN models cover the test cases with a broad range of weight and activation sparsity. Note that the weight sparsity and activation sparsity listed in Table <ref type="table" target="#tab_2">2</ref> only represents the fraction of zero values in the synaptic weights and feature maps of the target NN model before bit-level decomposition. The amount of sparsity that can be exploited to improve performance and energy efficiency depends on ReRAM bits-per-cell, DAC resolution, and the applied sparsity exploration techniques. 3 Our motivation experiment in Figure <ref type="figure" target="#fig_5">5</ref> shows that for large-scale NN models, inference accuracy drops if more than 16 wordlines are activated concurrently, even when technology improvements enable the R-ratio/resistance-deviation to increase/shrink by 3x of the WOx ReRAM <ref type="bibr" target="#b21">[22]</ref>. 4 We synthesize the circuits under 28nm process as we are only authorized to access TSMC 28nm standard cell library. </p><formula xml:id="formula_2">-conv3x64-pool-conv3x128-conv3x128 -pool-conv3x256-conv3x256-conv3x256-pool -conv3x512-conv3x512-conv3x512-pool-conv3x512 -conv3x512-conv3x512-pool-4096-4096-1000 GoogLeNet 79% 37% conv7x64-pool-conv3x192-pool-inception(3a) -inception(4c)-inception(4d)-inception(4e) -pool-inception(5a)-inception(5b)-pool-1000 ResNet-50 81% 46% conv7x64-pool-[conv1x64-conv3x64-conv1x256]x3 -[conv1x128-conv3x128-conv1x512]x4-[conv1x256 -conv3x256-conv1x1024]x6-[conv1x512-conv3x512 -conv1x2048]x3-pool-1000</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison Baselines</head><p>As the baseline, we use an OU-based ReRAM accelerator that does not exploit any sparsity. In addition, we compare the performance and energy efficiency of the proposed SRE with naive crossbar row-based compression. We also compare it with ReCom <ref type="bibr" target="#b23">[24]</ref>, a weight matrix row-based compression method designed for neural networks pruned by SSL <ref type="bibr" target="#b44">[45]</ref>. When ReCom is applied, if the same pixel of each filter in the same convolution/fully-connected layer is all zeros, the corresponding OU rows are removed to reduce unnecessary computations. Note that we does not compare with SNrram <ref type="bibr" target="#b43">[44]</ref>, as SNrram uses model-based compression and its crossbar architecture is highly model-dependent. A quantitative performance comparison with SNrram would be difficult because we use a different baseline design from SNrram, and the SNrram paper provides no cycle time information.</p><p>For our SRE, we evaluate three different modes: ORC, DOF, and ORC+DOF. ORC adopts only OU-based row compression to exploit weight sparsity. DOF exploits only activation sparsity by dynamically skipping the activation of wordlines with zero input signals.</p><p>ORC+DOF combines ORC and DOF to jointly exploit both weight and activation sparsity. To bound the storage overhead of input indexing, we adopt zero-padding <ref type="bibr" target="#b16">[17]</ref> and choose the minimum number of index bits that ensured less than 10% loss in weight compression ratio compared to when zero-padding is not used. Based on this principle, the length of index bits for MNIST, CIFAR-10, CaffeNet, VGG-16, GoogLeNet, and ResNet-50 is set to 5, 5, 5, 5, 3, and 3 bits, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EXPERIMENTAL RESULTS</head><p>In this section, we first evaluate the performance and energy efficiency of the proposed Sparse ReRAM Engine (SRE for short). We then analyze the indexing overhead of the scheme, followed by sensitivity studies on the OU size and ReRAM bits-per-cell. We also evaluate the performance of SRE when running non-SSL sparse neural networks. Finally, we compare SRE with an over-idealized design <ref type="bibr" target="#b39">[40]</ref> to show that jointly exploiting weight and activation sparsity enables a practical ReRAM-based DNN accelerator to achieve comparable performance with substantial energy savings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Performance and Energy</head><p>Figure <ref type="figure" target="#fig_17">17</ref> shows the performance speedup of different designs against the baseline OU-based ReRAM accelerator that does not exploit any sparsity. With ORC, the performance speedup ranges from 1.3? to 6.8?. As mentioned earlier, CaffeNet and VGG-16 are well tuned for structural sparsity, so they obtain higher benefit from ORC than the other models, 2.6? and 6.8?, respectively. We expect to see similar benefits for the other models if they could be trained well for structural sparsity as CaffeNet and VGG-16. On the other hand, DOF delivers significant performance gains, ranging from 4.1 ? to 16.0?, for all the models. ResNet-50 obtains the largest gain from DOF as there are many batch norm layers in this model. The joint use of ORC and DOF provides accumulated benefit (average 13.1?). Since zero bits are usually randomly distributed in feature maps, applying ORC rarely degrades the fraction of activation sparsity that can be explored. For example, VGG-16 sees 6.8?, 7.5? and 42.3? speedup with ORC, DOF and ORC+DOF, respectively. Compared to SRE, both ReCom and naive provide only small performance speedups over the baseline, as ReCom and naive explore weight sparsity only in coarser-grained weight-matrix-row and crossbar-row granularity. The speedup for naive is slightly higher than that for ReCom, since a weight-matrix-row can span multiple crossbar arrays and ReCom cannot remove an all-zero crossbar-row if the row is just part of a non-all-zero weight-matrix-row.</p><p>Figure <ref type="figure" target="#fig_18">18</ref> shows the energy consumption of different sparsityexploration designs normalized to the baseline. ReCom and naive help to conserve a small amount of energy (average 12.5% and 21.3%) by skipping computations with all-zero crossbar-rows. Compared to ReCom and naive, ORC saves more energy by exploiting a finer granularity of weight sparsity to skip computations with all-zero OU rows (50.6% on average). The energy savings come from the reduced number of accesses to peripheral circuits/buffers (ADC, IR, and OR) associated with each OU computation. DOF shows high effectiveness for energy reduction across all NN models (between 70% to 90%). Combining ORC and DOF reduces energy further for CaffeNet and VGG-16 but not for the other four models. We can see from Figure <ref type="figure" target="#fig_18">18</ref>, ORC+DOF consumes significantly higher eDRAM energy than DOF, which outweighs the additional computationrelated energy savings from ORC. As we discuss in Section 4, since row-wise compression changes the input order, each column-wise OU group must fetch correct inputs from the eDRAM buffer based on its own input indexes. Hence, compared to DOF that does not change the input order, additional eDRAM accesses are required when row-wise compression is applied. As mentioned earlier, Caf-feNet and VGG-16 are well trained for structural sparsity so the additional reduction on computation-related energy from ORC can compensate the loss due to extra eDRAM accesses. We expect that ORC+DOF in general should achieve the largest energy savings if NN models could be fine-tuned for structural pruning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Indexing Overhead Analysis</head><p>To support OU-based row compression in SRE, we must store the input indexes of each column-wise OU group. To bound the storage overhead, we store the index difference between non-zero OU rows instead of the absolute values and adopt zero-padding <ref type="bibr" target="#b16">[17]</ref>. Figure <ref type="figure" target="#fig_19">19</ref> shows the storage overhead for input indexing. The overhead varies for different NN models, depending on the size of the model. For large-scale NN models with many convolution layers such as ResNet-50, the storage overhead is larger. The OU size also affects the amount of storage overhead. With a smaller OU size, we can remove more all-zero OU rows. Thus, for each column-wise OU group, fewer input indexes need to be stored. Nevertheless, as the OU size decreases, the number of column-wise OU groups increases. As a result, more sets of input indexes must be stored for smaller OUs, leading to greater storage overhead. From our evaluation, we find that the storage overhead increases only slightly when the OU size decreases for most of the evaluated neural networks, except for ResNet-50. Even though the storage overhead of ResNet-50 is higher (778KB) than other NN models, storing the index difference between non-zero OU rows still leads to far lower storage overhead than directly storing the absolute index of each non-zero OU row (about 4MB).</p><p>In SRE, to support ORC and DOF, we add the Index Decoder and Wordline Vector Generator components. As the crossbar array has 128 wordlines, the Index Decoder must decode 128 input indexes in one cycle to minimize pipeline stalls. Based on our synthesis, an Index Decoder that can decode 8 indexes at a time in a parallel fashion (width = 8) is required to provide sufficient throughput. The Index Decoder is composed of seven 5-bit adders, six 6-bit adders, four 7-bit adders, eight 13-bit adders, eight 6-bit latches, eight 7-bit latches, eight 8-bit latches, and one 13-bit latch, inducing only a small area (0.001 mm 2 ) and power (1.24 mW) overhead. Note that the overhead is independent of the OU size, as the Index Decoder must decode the input indexes for all the wordlines in the crossbar array at a time. To support DOF without seriously stalling the pipeline, the Wordline Vector Generator must be able to generate a wordline activation vector within one cycle. Our synthesis result shows that a Wordline Vector Generator that can generate 8 elements of the wordline activation vector at a time in a parallel fashion (width = 8) can provide sufficient throughput. The Wordline Vector Generator is composed of a circuit implementation of parallel prefix sum (with four 1-bit adders, four 2-bit adders, four 3-bit adders, and eight 8-bit adders) and thirty-two 4-bit comparators. The circuit is simple and induces only a 0.001 mm 2 area and 0.86 mW power overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Sensitivity Studies</head><p>We evaluate SRE with different configurations of crossbar architecture, including the OU size and ReRAM bits-per-cell, to analyze the impact of these architectural parameters on the weight compression ratio, performance, and energy consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OU Size</head><p>In SRE, the OU size impacts the weight compression ratio and the amount of computation reduction opportunities. Figure <ref type="figure" target="#fig_20">20</ref> shows that as the OU size decreases, the weight compression ratio increases, since it is easier to find all-zero OU rows when the OU is smaller. With a smaller OU, the weight compression ratio is comparable to the ideal case which assumes that all ReRAM cells with   zero weights can be removed. In Figure <ref type="figure" target="#fig_20">20</ref>, we also use arrows to indicate the weight compression ratio that can be obtained from SNrram <ref type="bibr" target="#b43">[44]</ref>. As SNrram uses model-based compression (i.e., finegrained column-based compression that removes all-zero column vectors with size equals to filter height ? filter width) to exploit weight sparsity, its weight compression ratio varies across different neural network models. Although SNrram yields a high compression ratio for neural network models with small filter sizes such as GoogLeNet and ResNet-50, it is not a practical design as the output indexing overhead is large and it requires different sizes of crossbar arrays for layers with different filter sizes. With proper OU size settings, the proposed SRE achieves a weight compression ratio similar to that of SNrram.</p><p>The energy consumption of SRE also varies for different OU sizes. In the baseline OU-based ReRAM accelerator that does not exploit any sparsity, lower-resolution ADCs with less energy consumption can be utilized when the OU is small. Nevertheless, with a smaller OU, more OU computations are needed to complete the same amount of matrix-vector multiplication, resulting in a higher number of peripheral circuit/buffer (ADC, IR, and OR) accesses. Thus, energy consumption dramatically increases when the OU size decreases in the baseline, as shown in Figure <ref type="figure" target="#fig_21">21</ref>(a). After applying ORC+DOF to exploit sparsity, smaller OUs do not necessarily result in higher energy consumption, as shown in Figure <ref type="figure" target="#fig_21">21(b)</ref>. With a smaller OU size, it is easier to find all-zero OU rows and save unnecessary computations. As a result, for most of the evaluated neural networks, the energy consumption decreases when the OU size decreases from 128?128 to 32?32. In this paper, we set the OU size to 16?16 considering the impact on inference accuracy. Results in Figure <ref type="figure" target="#fig_21">21</ref>(b) indicates that even if advances in technology make it possible to achieve sustainable accuracy when operating the entire crossbar array in a single cycle, it is more energy-efficient to jointly exploit weight and activation sparsity on OU-based architecture with appropriate OU sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ReRAM Bits-per-cell</head><p>In the practical ReRAM crossbar array, filter weights are decomposed and mapped to multiple bitlines, as each ReRAM cell stores only a limited number of bits. When the ReRAM bits-per-cell decreases, more cycles are needed to complete the same amount of matrix-vector multiplication if weight sparsity is not explored. Nevertheless, due to the the weight decomposition process, when ReRAM bits-per-cell decreases, bit-level weight sparsity increases, and it is easier to find all-zero OU rows. As a result, the SRE performance speedup is greater when each ReRAM cell stores a fewer number of bits, as shown in Figure <ref type="figure" target="#fig_22">22</ref>. Although advances in technology may enable a ReRAM cell to store higher number of bits with sustainable reliability in the next few years, SRE still provides 11.4? performance speedups on average if the storage capacity of each ReRAM cell is 8 bits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Non-SSL Sparse Neural Networks</head><p>Even though the structural pruning algorithm (SSL) <ref type="bibr" target="#b44">[45]</ref> helps to regularize the distribution of zero weights, the proposed SRE provides performance speedup not only for SSL-trained neural networks but also for non-SSL sparse neural network models, as shown in Figure <ref type="figure" target="#fig_23">23(a)</ref>. For such non-SSL sparse neural network models, we use the NN models released by SkimCaffe <ref type="bibr" target="#b36">[37]</ref>. SkimCaffe adopts  Guided Sparsity Learning (GSL), a generic algorithm that supports different regularization methods and prunes neural networks by adjusting sparsity targets precisely at different layers. When SSL is not applied during training, weight sparsity may decrease and it can be harder to find all-zero OU rows. Thus, ORC yields only small performance speedups. For example, for VGG-16, ORC provides a 6.8? performance speedup when SSL is applied (Figure <ref type="figure" target="#fig_17">17</ref>) but only a 1.1? speedup when a non-SSL pruning algorithm is applied (Figure <ref type="figure" target="#fig_23">23(a)</ref>). In contrast, activation sparsity is less relevant to the weight pruning algorithm. Therefore, DOF still provides significant performance speedups for non-SSL sparse neural network models. By jointly exploiting weight and activation sparsity, the proposed SRE provides a 9.7? performance speedup on average for non-SSL sparse neural networks.</p><p>SRE can also provide significant energy savings for non-SSL sparse neural networks, as shown in Figure <ref type="figure" target="#fig_23">23(b)</ref>. Although ORC provides fewer energy savings for non-SSL sparse neural networks (average 26.7%) compared to SSL neural networks (average 50.6% as shown in Figure <ref type="figure" target="#fig_18">18</ref>), DOF still helps to greatly reduce the energy consumption. As fewer weight sparsity could be explored by the non-SSL pruning algorithm, the energy consumption of ORC+DOF is slightly higher than that of DOF for neural networks such as VGG-16. For VGG-16 trained by non-SSL pruning algorithm, ORC+DOF consumes significantly higher eDRAM energy than DOF, outweighing the additional computation-related energy savings from ORC. On average, for non-SLL sparse neural networks, ORC+DOF still yields 78.7% energy savings over the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Comparison with Over-Idealized Design</head><p>ISAAC <ref type="bibr" target="#b39">[40]</ref> is a state-of-the-art but over-idealized design, which overlooks the inference accuracy loss caused by the accumulated effect of per-cell current deviation and assumes that 128 wordlines in a crossbar array can be activated concurrently. As discussed in Section 3, a practical OU-based deign completes less computation in a cycle but has the advantage of shorter cycle time. For the first-order comparison of a 16?16 OU-based design (15-ns cycle in this subsection, we demonstrate that with the proposed joint weight and activation sparsity method, the practical OUbased architecture, which achieves satisfactory inference accuracy the limitation of ReRAM cell reliability, could actually deliver comparable performance and energy efficiency with the over-idealized design, ISAAC.</p><p>Figure <ref type="figure" target="#fig_24">24</ref>(a) shows the execution time of our SRE normalized to ISAAC's execution time. For fair comparison, we apply ReCom <ref type="bibr" target="#b23">[24]</ref> to exploit weight sparsity for ISAAC. We can observe that SRE achieves better performance than the over-idealized ISAAC for 3 out of 6 neural network models. For neural networks with considerable increase in weight compression ratio on OU-based architecture than on ISAAC, SRE delivers higher performance speedup. On average, SRE provides 15.8% performance improvement over ISAAC. In the aspect of energy efficiency, for all of the evaluated neural network models, SRE is more energy efficient than ISAAC, as shown in Figure <ref type="figure" target="#fig_24">24(b)</ref>. Without exploiting sparsity, the OU-based architecture consumes roughly 2.5? energy than ISAAC due to the combined effect of a higher number of accesses to peripheral circuits/buffers (ADC, IR, and OR) and the lower-resolution (6-bit) ADC (vs. 8-bit ADC). With the proposed scheme, SRE provides 67.0% energy savings over ISAAC on average. The energy savings are considerable especially for neural network models with a large amount of sparsity such as VGG-16 (87.6%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">RELATED WORK Pruning Algorithms</head><p>Various algorithmic techniques have been studied to remove the redundancy inside neural network models. For instance, quantization <ref type="bibr" target="#b16">[17]</ref>, low-rank matrix factorization <ref type="bibr" target="#b11">[12]</ref>, and 1 -norm regularization <ref type="bibr" target="#b27">[28]</ref> have been proposed to prune networks during training. Han et al. <ref type="bibr" target="#b17">[18]</ref> propose another approach to directly remove lowvalue weights and retrain the network, but most of the computation reduction is attained in fully-connected layers. As the majority of computations are at convolution layers, Molchanov et al. <ref type="bibr" target="#b32">[33]</ref> develop a new formulation based on Taylor expansion to iteratively remove the least important parameters in convolution layers. Several studies focus on adapting the sparse network structures to make it hardware-friendly in an algorithmic way <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b47">48]</ref>. Yu et al. <ref type="bibr" target="#b47">[48]</ref> propose a method to customize DNN pruning for different hardware platforms based on each platform's data-parallelism. Wei el al. <ref type="bibr" target="#b44">[45]</ref> regularize the structure of DNN and prune weights in a systematic way to derive a hardware-friendly compressed neural network. Liang et al. <ref type="bibr" target="#b28">[29]</ref> propose a crossbar-grain pruning algorithm to remove an entire crossbar whose partial sum contributes less to its output. These weight pruning algorithms can be utilized to increase the speedup/energy-savings of DNN inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sparse DNN Acceleration in Digital ASICs and GPUs</head><p>Many prior studies have designed different mechanisms to exploit weight sparsity <ref type="bibr" target="#b48">[49]</ref>, activation sparsity <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref>, or both <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b35">36]</ref> in CMOS-based digital accelerators in order to improve the energy efficiency of DNN inference. To exploit weight sparsity, Zhang et al. <ref type="bibr" target="#b48">[49]</ref> propose Cambricon-X, which retains only nonzero weights in its internal buffers and uses an indexing module to efficiently fetch needed inputs for computation. However, it still wastes times computing multiplications for zero-valued activations. To exploit activation sparsity, Chen et al. <ref type="bibr" target="#b7">[8]</ref> propose Eyeriss to gate the multiplier when an input activation is zero to save energy. Another sparse DNN accelerator, Cnvlutin <ref type="bibr" target="#b0">[1]</ref>, selects only non-zero activation values for delivery as multiplier operands. Nevertheless, neither Eyeriss nor Cnvlutin skips computations with zero weights. To jointly exploit weight and activation sparsity, EIE <ref type="bibr" target="#b15">[16]</ref> uses a compressed representation for both activations and weights, and only delivers non-zero operands to the multipliers. However, EIE is designed only for fully connected layers. SCNN <ref type="bibr" target="#b35">[36]</ref> also jointly exploits weight and activation sparsity. It targets the convolution layers, where the majority of computations take place, and uses a sparse planar-tiled input-stationary Cartesian product dataflow to enable efficient storage, delivery, and processing of the sparse weights and activations.</p><p>Efficiently executing sparse DNN inferences on GPU is challenging, as the irregular DNN topology and non-contiguous data structure increase the amount of branch divergences and uncoalesced memory accesses. To improve performance, Hill et al. <ref type="bibr" target="#b19">[20]</ref> propose a synapse vector elimination technique to drop non-contributing synapses in the neural network and maintain computational regularity when pruning the network model. Their proposed technique reduces under-utilization of GPU resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ReRAM-based Sparse DNN Accelerators</head><p>The tightly coupled crossbar structure in the ReRAM-based DNN accelerator makes it difficult to effectively skip irregular zero weights and activations in DNNs. Directly applying sparsity exploration techniques designed for CMOS-based digital accelerators and GPUs on ReRAM-based DNN accelerators is not practical, as zero weights/activations are tightly coupled with other non-zero data in the same row/column and cannot be easily skipped. Re-Com <ref type="bibr" target="#b23">[24]</ref> is the first ReRAM-based DNN accelerator that exploits neural network sparsity. They use SSL <ref type="bibr" target="#b44">[45]</ref> to structurally compress neural network models and remove all-zero rows for resource savings. They also exploit activation sparsity to reduce off-chip memory accesses. However, with ReCom's coarse-grain compression, only all-zero rows can be removed; many zero weights still remain in the compressed model. SNrram <ref type="bibr" target="#b43">[44]</ref> is another ReRAMbased sparse DNN accelerator that compresses the model at a finer level, i.e., filter-sized all-zero columns in structurally compressed neural networks. As the size of a filter is usually smaller than a row, SNrram better exploits sparsity than ReCom. However, the scheme requires the use of an output indexing module with significant storage overhead. SNrram also exploits activation sparsity, but activation compression is only performed for deconvolution-layers in generative adversarial networks (GANs). Lin et al. <ref type="bibr" target="#b29">[30]</ref> propose a different sparse mapping scheme based on k-means clustering, and use a crossbar-grained pruning algorithm to remove crossbars with low utilization. These prior sparsity solutions assume an over-idealized ReRAM crossbar architecture that activates an entire crossbar array in a single cycle. In contrast to these studies, our SRE takes advantage of practical fine-grained OU-based computations to jointly exploit weight and activation sparsity. Chen et al. <ref type="bibr" target="#b6">[7]</ref> exploit the output sparsity introduced by the ReLU function to reduce computation. They propose an adaptive estimation method to detect negative output activations and terminate unnecessary bit-level convolutions earlier. Their early-termination technique is orthogonal to our SRE and can be combined with our approach to further improve performance and save energy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>In this paper, we study the design challenges of sparsity exploration on ReRAM-based DNN accelerators, and demonstrate that a practical OU-based ReRAM accelerator opens up new opportunities to effectively exploit DNN sparsity. We propose the first practical sparse ReRAM engine (SRE) that takes advantage of fine-grained OU-based computations to jointly exploit weight and activation sparsity. Our evaluation across a broad range of neural network models shows that SRE provides significant performance speedups (up to 42.3x) and energy savings (up to 95.4%) over a baseline that does not exploit sparsity. In addition, SRE successfully enables a practical ReRAM-based DNN accelerator design, which achieves satisfactory inference accuracy considering the limitation of ReRAM cell reliability while delivering comparable performance and energy efficiency with the over-idealized counterpart.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: ReRAM-based DNN accelerator architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Mapping of filter weights and feqture maps to a crossbar array.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Mapping of filter weights and feature maps to a crossbar array after value decomposition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 (</head><label>4</label><figDesc>a)(b) shows how the weight (input) sparsity increases as the bits-per-cell (DAC resolution) decreases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) Weight density (fraction of non-zero ReRAM cells) and (b) input density (fraction of non-zero input voltages) of VGG-16 after decomposition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Inference accuracy of (a) MNIST, (b) CIFAR-10, and (c) CaffeNet when various number of wordlines are activated concurrently, with three different types of ReRAM cells. R b and ? b are the R-ratio and resistance-deviation of WOx ReRAM [22].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Wordline driver in OU-based ReRAM accelerator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: OU-based dot-product computation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Example of weight compression. (a) Original OUbased computation without zero reduction, (b) OU-row compression, and (c) OU-col compression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Dynamic OU Formation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Wrong output when combining DOF with OU-col compression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Sparse ReRAM Engine.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Input index encoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Input index decoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Circuit implementation of parallel prefix sum.</figDesc><graphic url="image-1.png" coords="8,106.19,88.37,110.30,109.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Wordline Vector Generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Pipelined execution in SRE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Performance speedup of different sparsityexploration approaches over the baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Energy consumption of different sparsity-exploration approaches normalized to the baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: Storage overhead of input indexes for SRE with different OU sizes (from 128?128 to 16?16).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Weight compression ratio of SRE with different OU sizes (from 128?128 to 2?2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: Energy consumption of (a) baseline and (b) SRE for different OU sizes, normalized to 128?128 OU size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 22 :</head><label>22</label><figDesc>Figure 22: Performance speedup of SRE over baseline for various ReRAM bits-per-cell settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 23 :</head><label>23</label><figDesc>Figure 23: (a) Performance speedup and (b) energy consumption of SRE normalized to the baseline for non-SSL sparse neural networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 24 :</head><label>24</label><figDesc>Figure 24: (a) Execution time and (b) energy consumption of SRE normalized to ISAAC.time5 ) with ISAAC assuming 128?128 crossbar arrays (100-ns cycle time), the OU-based design is 9.6? slower (64? cycles and 6.6? faster cycle time). However, as what we have presented in this paper, the OU-based design enables a new opportunity to exploit sparsity for further performance and energy improvements. in this subsection, we demonstrate that with the proposed joint weight and activation sparsity method, the practical OUbased architecture, which achieves satisfactory inference accuracy the limitation of ReRAM cell reliability, could actually deliver comparable performance and energy efficiency with the over-idealized design, ISAAC.Figure24(a) shows the execution time of our SRE normalized to ISAAC's execution time. For fair comparison, we apply ReCom<ref type="bibr" target="#b23">[24]</ref> to exploit weight sparsity for ISAAC. We can observe that SRE achieves better performance than the over-idealized ISAAC for 3 out of 6 neural network models. For neural networks with considerable increase in weight compression ratio on OU-based architecture than on ISAAC, SRE delivers higher performance speedup. On average, SRE provides 15.8% performance improvement over ISAAC. In the aspect of energy efficiency, for all of the evaluated neural network models, SRE is more energy efficient than ISAAC, as shown in Figure24(b). Without exploiting sparsity, the OU-based architecture consumes roughly 2.5? energy than ISAAC due to the combined effect of a higher number of accesses to peripheral circuits/buffers (ADC, IR, and OR) and the lower-resolution (6-bit) ADC (vs. 8-bit ADC). With the proposed scheme, SRE provides 67.0% energy savings over ISAAC on average. The energy savings are considerable especially for neural network models with a large amount of sparsity such as VGG-16 (87.6%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Hardware configuration.</figDesc><table><row><cell cols="3">PE configuration (1.2 GHz, 32nm process, 168 PEs per chip)</cell></row><row><cell>Component</cell><cell>Spec</cell><cell>Power</cell></row><row><cell>eDRAM Buffer</cell><cell>size: 64KB; banks: 2 bus width: 512 bits</cell><cell>29 mW (leakage: 0.38 mW)</cell></row><row><cell>eDRAM-to-CU bus</cell><cell>number of wires: 384</cell><cell>7 mW</cell></row><row><cell>Router</cell><cell>flit size: 32; number of ports: 8 (shared by 4 PEs)</cell><cell>42 mW</cell></row><row><cell>Sigmoid</cell><cell>number: 2</cell><cell>0.52 mW</cell></row><row><cell>S+A</cell><cell>number: 1</cell><cell>0.05 mW</cell></row><row><cell>MaxPool</cell><cell>number: 1</cell><cell>0.4 mW</cell></row><row><cell>OR</cell><cell>size: 3KB</cell><cell>1.68 mW (leakage: 0.21 mW)</cell></row><row><cell></cell><cell cols="2">CU configuration (12 CUs per PE)</cell></row><row><cell>Component</cell><cell>Spec</cell><cell>Power</cell></row><row><cell>ADC</cell><cell>number: 8; resolution: 6 bits frequency: 1.2GSps</cell><cell>5.14 mW</cell></row><row><cell>DAC</cell><cell>number: 8 x 128; resolution: 1 bit</cell><cell>4 mW</cell></row><row><cell>S+H</cell><cell>number: 8 x 128</cell><cell>10 ?W</cell></row><row><cell>Memristor Array</cell><cell>number: 8; size: 128 ? 128 bits-per-cell: 2; OU size: 16 ? 16</cell><cell>2.4 mW (4.7 ?W per OU)</cell></row><row><cell>S+A</cell><cell>number: 4</cell><cell>0.2 mW</cell></row><row><cell>IR</cell><cell>size: 2KB</cell><cell>1.24 mW (leakage: 0.42 mW)</cell></row><row><cell>OR</cell><cell>size: 256B</cell><cell>0.23 mW (leakage: 0.05 mW)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 : NN topology of evaluated benchmarks.</head><label>2</label><figDesc></figDesc><table><row><cell>Name</cell><cell>Weight Sparsity</cell><cell>Activation Sparsity</cell><cell>Structure of networks</cell></row><row><cell>MNIST</cell><cell>42%</cell><cell>28%</cell><cell>conv5x20-pool-conv5x50-pool-500-10</cell></row><row><cell>CIFAR-10</cell><cell>34%</cell><cell>22%</cell><cell>conv5x32-pool-conv5x32-pool-conv5x64-pool -64-10</cell></row><row><cell>CaffeNet</cell><cell>91%</cell><cell>21%</cell><cell>conv11x96-conv5x256-conv3x384-conv3x384 -conv3x256-4096-4096-1000</cell></row><row><cell></cell><cell></cell><cell></cell><cell>conv3x64</cell></row><row><cell>VGG-16</cell><cell>95%</cell><cell>41%</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>If DAC resolution is larger than 1 bit, the decomposed input values are passed through logic circuits that are able to mark non-zero inputs to form the mask vector.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>As described in Section 5, the cycle time of SRE is 30ns at</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>65nm technology. For an apples-to-apples comparison with ISAAC, the cycle time of SRE is scaled down from 65nm to 32nm, and is approximately 15 ns.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We would like to thank <rs type="person">Bing-Chen Wu</rs> and <rs type="person">Tsung-Te Liu</rs> for their help on circuit synthesis, and <rs type="person">Meng-Fan Chang</rs> and <rs type="person">Chin-Fu Nien</rs> for their suggestions on circuit designs. We also appreciate the anonymous reviewers for their valuable comments and suggestions. This work was supported in part by research grants from the <rs type="funder">Ministry of Science and Technology of Taiwan</rs> (<rs type="grantNumber">MOST-107-2221-E-002-043-MY2</rs>, <rs type="grantNumber">MOST-107-2218-E-001-004-MY3</rs>), <rs type="funder">National Taiwan University</rs> (<rs type="grantNumber">NTU-108L891903</rs>), and sponsored by <rs type="funder">Macronix Inc.</rs>, Hsin-chu, Taiwan (<rs type="grantNumber">107-S-C38</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_2r482Mg">
					<idno type="grant-number">MOST-107-2221-E-002-043-MY2</idno>
				</org>
				<org type="funding" xml:id="_9VyJZHS">
					<idno type="grant-number">MOST-107-2218-E-001-004-MY3</idno>
				</org>
				<org type="funding" xml:id="_xZnGGnv">
					<idno type="grant-number">NTU-108L891903</idno>
				</org>
				<org type="funding" xml:id="_xXXQZSM">
					<idno type="grant-number">107-S-C38</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cnvlutin: Ineffectual-Neuron-Free Deep Neural Network Computing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Albericio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hetherington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aamodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Jerger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Symposium on Computer Architecture (ISCA)</title>
		<meeting>International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fused-layer CNN Accelerators</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Symposium on Microarchitecture (MICRO)</title>
		<meeting>International Symposium on Microarchitecture (MICRO)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<idno>arXiv</idno>
		<title level="m">The Power of Sparsity in Convolutional Neural Networks</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Small-footprint Keyword Spotting using Deep Neural Networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Parada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">NeuroSim: A Circuit-Level Macro Model for Benchmarking Neuro-Inspired Architectures in Online Learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="3067" to="3080" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A 65nm 1Mb Nonvolatile Computing-in-memory ReRAM Macro with Sub-16ns Multiply-andaccumulate for Binary DNN AI Edge Processors</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Solid-State Circuits Conference (ISSCC)</title>
		<meeting>International Solid-State Circuits Conference (ISSCC)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">CompRRAE: RRAM-based Convolutional Neural Network Accelerator with Reduced Computations Through a Runtime Activation Estimation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tsui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Asia and South Pacific Design Automation Conference</title>
		<meeting>Asia and South Pacific Design Automation Conference</meeting>
		<imprint>
			<publisher>ASPDAC)</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="127" to="138" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">PRIME: A Novel Processing-in-Memory Architecture for Neural Network Computation in ReRAM-Based Main Memory</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Symposium on Computer Architecture (ISCA)</title>
		<meeting>International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Handwritten Digit Recognition: Applications of Neural Network Chips and Automatic Learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Magazine</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="1989">1989. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ImageNet: A Largescale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predicting Parameters in Deep Learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shakibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
		<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Making Memristive Neural Network Accelerators Reliable</title>
		<author>
			<persName><forename type="first">B</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ipek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Symposium on High Performance Computer Architecture (HPCA)</title>
		<meeting>International Symposium on High Performance Computer Architecture (HPCA)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">HfO2-Based OxRAM Devices as Synapses for Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vianello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bichler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Rafhay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gamrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ghibaudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Desalvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Perniola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Electron Devices</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="2494" to="2501" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep Sparse Rectifier Neural Networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Artificial Intelligence and Statistics (AIStats)</title>
		<meeting>International Conference on Artificial Intelligence and Statistics (AIStats)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">EIE: Efficient Inference Engine on Compressed Deep Neural Network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pedram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Symposium on Computer Architecture (ISCA)</title>
		<meeting>International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning both Weights and Connections for Efficient Neural Networks</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Residual Learning for Image Recognition</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DeftNN: Addressing Bottlenecks for DNN Execution on GPUs via Synapse Vector Elimination and Near-compute Data Fission</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zamirai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Laurenzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahlke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Symposium on Microarchitecture</title>
		<meeting>International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>MICRO</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Data Parallel Algorithms</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Hillis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Steele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1170" to="1183" />
			<date type="published" when="1986">1986. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Study of Array Resistance Distribution and a Novel Operation Algorithm for WOx ReRAM Memory</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Lung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Solid State Devices and Materials (SSDM)</title>
		<meeting>International Conference on Solid State Devices and Materials (SSDM)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dot-product Engine for Neuromorphic Computing: Programming 1T1M Crossbar to Accelerate Matrix-vector Multiplication</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Strachan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Grafals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Davila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Design Automation Conference (DAC)</title>
		<meeting>Design Automation Conference (DAC)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ReCom: An Efficient Resistive Accelerator for Compressed Deep Neural Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Design, Automation Test in Europe (DATE)</title>
		<meeting>Design, Automation Test in Europe (DATE)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech Report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
		<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gradient-based Learning Applied to Document Recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998. 1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T P</forename><surname>Tang</surname></persName>
		</author>
		<title level="m">Enabling Sparse Winograd Convolution by Native Pruning</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<title level="m">Crossbaraware Neural Network Pruning</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning the Sparsity for ReRAM: Mapping and Pruning Sparse Neural Network for ReRAM Based Accelerator</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Asia and South Pacific Design Automation Conference</title>
		<meeting>Asia and South Pacific Design Automation Conference</meeting>
		<imprint>
			<publisher>ASPDAC)</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">DL-RSIM: A Simulation Framework to Enable Reliable ReRAM-based Accelerators for Deep Learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer-Aided Design (ICCAD)</title>
		<meeting>the International Conference on Computer-Aided Design (ICCAD)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rectifier Nonlinearities Improve Neural Network Acoustic Models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno>arXiv</idno>
		<title level="m">Pruning Convolutional Neural Networks for Resource Efficient Transfer Learning</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<title level="m">CACTI 6.0: A Tool to Model Large Caches</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>HP Lab</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Technological Exploration of RRAM Crossbar Array for Matrix-vector Multiplication</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Asia and South Pacific Design Automation Conference</title>
		<meeting>Asia and South Pacific Design Automation Conference</meeting>
		<imprint>
			<publisher>ASPDAC)</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Puglielli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Symposium on Computer Architecture (ISCA)</title>
		<meeting>International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dubey</surname></persName>
		</author>
		<idno>arXiv</idno>
		<title level="m">Holistic SparseCNN: Forging the Trident of Accuracy, Speed, and Size</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Analysis of Power Consumption and Linearity in Capacitive Digital-to-Analog Converters Used in Successive Approximation ADCs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saberi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lotfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mafinezhad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Serdijn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems I: Regular Papers</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="1736" to="1748" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated Recognition, Localization and Detection using Convolutional Networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">ISAAC: A Convolutional Neural Network Accelerator with In-Situ Analog Arithmetic in Crossbars</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shafiee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Strachan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Srikumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Symposium on Computer Architecture (ISCA)</title>
		<meeting>International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">A 462GOPs/J RRAM-based Nonvolatile Intelligent Processor for Energy Harvesting IoE System Featuring Nonvolatile Logics and Processing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Proceedings of International Symposium on VLSI Technology, Systems and Applications(VLSI-TSA)</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Going Deeper with Convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">SNrram: An Efficient Sparse Neural Network Computation Architecture Based on Resistive Randomaccess Memory</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Design Automation Conference (DAC)</title>
		<meeting>Design Automation Conference (DAC)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning Structured Sparsity in Deep Neural Networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
		<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Overcoming the Challenges of Crossbar Resistive Memory Architectures</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Symposium on High Performance Computer Architecture (HPCA)</title>
		<meeting>International Symposium on High Performance Computer Architecture (HPCA)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A 1Mb Multibit ReRAM Computing-In-Memory Macro with 14.6ns Parallel MAC Computing Time for CNN Based AI Edge Processors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Solid-State Circuits Conference (ISSCC)</title>
		<meeting>International Solid-State Circuits Conference (ISSCC)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Scalpel: Customizing DNN Pruning to the Underlying Hardware Parallelism</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lukefahr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Palframan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dasika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahlke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Symposium on Computer Architecture (ISCA)</title>
		<meeting>International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cambricon-X: An Accelerator for Sparse Neural Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Symposium on Microarchitecture</title>
		<meeting>International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>MICRO</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
