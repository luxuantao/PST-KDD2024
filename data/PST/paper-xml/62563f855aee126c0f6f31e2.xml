<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FederatedScope-GNN: Towards a Unified, Comprehensive and Efficient Package for Federated Graph Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-04-13">13 Apr 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Weirui</forename><surname>Kuang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuexiang</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Liuyi</forename><surname>Yao</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
							<email>yaliang.li@alibaba-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alibaba</forename><surname>Group</surname></persName>
						</author>
						<title level="a" type="main">FederatedScope-GNN: Towards a Unified, Comprehensive and Efficient Package for Federated Graph Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-13">13 Apr 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2204.05562v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The incredible development of federated learning (FL) has benefited various tasks in the domains of computer vision and natural language processing, and the existing frameworks such as TFF and FATE has made the deployment easy in real-world applications. However, federated graph learning (FGL), even though graph data are prevalent, has not been well supported due to its unique characteristics and requirements. The lack of FGL-related framework increases the efforts for accomplishing reproducible research and deploying in real-world applications. Motivated by such strong demand, in this paper, we first discuss the challenges in creating an easy-to-use FGL package and accordingly present our implemented package FederatedScope-GNN (FS-G), which provides (1) a unified view for modularizing and expressing FGL algorithms; (2) comprehensive DataZoo and ModelZoo for out-ofthe-box FGL capability; (3) an efficient model auto-tuning component; and (4) off-the-shelf privacy attack and defense abilities. We validate the effectiveness of FS-G by conducting extensive experiments, which simultaneously gains many valuable insights about FGL for the community. Moreover, we employ FS-G to serve the FGL application in real-world E-commerce scenarios, where the attained improvements indicate great potential business benefits. We publicly release FS-G, as submodules of FederatedScope, at https://github.com/alibaba/FederatedScope to promote FGL's research and enable broad applications that would otherwise be infeasible due to the lack of a dedicated package.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Along with the rising concerns about privacy, federated learning (FL) <ref type="bibr" target="#b27">[28]</ref>, a paradigm for collaboratively learning models without access to dispersed data, has attracted more and more attention from both industry and academia. Its successful applications include keyboard prediction <ref type="bibr" target="#b11">[12]</ref>, object detection <ref type="bibr" target="#b25">[26]</ref>, speech recognition <ref type="bibr" target="#b28">[29]</ref>, the list goes on. This fantastic progress benefits from the FL frameworks, e.g., TFF <ref type="bibr" target="#b4">[5]</ref> and FATE <ref type="bibr" target="#b39">[40]</ref>, which save practitioners from the implementation details and facilitate the transfer from research prototype to deployed service.</p><p>However, such helpful supports have mainly focused on tasks in vision and language domains. Yet, the graph data, ubiquitous in realworld applications, e.g., recommender systems <ref type="bibr" target="#b35">[36]</ref>, healthcare <ref type="bibr" target="#b41">[42]</ref>, and anti-money laundering <ref type="bibr" target="#b32">[33]</ref>, have not been well supported. As a piece of evidence, most existing FL frameworks, including TFF, FATE, and PySyft <ref type="bibr" target="#b42">[43]</ref>, have not provided off-the-shelf federated graph learning (FGL) capacities, not to mention the lack of FGL benchmarks on a par with LEAF <ref type="bibr" target="#b5">[6]</ref> for vision and language tasks.</p><p>As a result, FL optimization algorithms, including FedAvg <ref type="bibr" target="#b27">[28]</ref>, FedProx <ref type="bibr" target="#b22">[23]</ref>, and FedOPT <ref type="bibr" target="#b0">[1]</ref>, are mainly evaluated on vision and language tasks. When applied to optimize graph neural network (GNN) models, their characteristics are unclear to the community. Another consequence of the lack of dedicated framework support is that many recent FGL works (e.g., FedSage+ <ref type="bibr" target="#b41">[42]</ref> and GCFL <ref type="bibr" target="#b36">[37]</ref>) have to implement their methods from scratch and conduct experiments on respective testbeds.</p><p>We notice that such a lack of widely-adopted benchmarks and unified implementations of related works have become obstacles to developing novel FGL methods and the deployment in real-world applications. It increases engineering effort and, more seriously, introduces the risk of making unfair comparisons. Therefore, it is much in demand to create an FGL package that can save the effort of practitioners and provide a testbed for accomplishing reproducible research. To this end, we pinpoint what features prior frameworks lack for FGL and the challenges to satisfy these requirements:</p><p>(1) Unified View for Modularized and Flexible Programming. In each round of an FL course, a general FL algorithm (e.g., FedAvg) exchanges homogeneous data (here model parameters) for one pass. In contrast, FGL algorithms <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42]</ref> often require several heterogeneous data (e.g., gradients, node embeddings, encrypted adjacency lists, etc.) exchanges across participants. Meantime, besides ordinary local updates/global aggregation, participants of FGL have rich kinds of subroutines to handle those heterogeneous data. FL algorithms are often expressed in most existing frameworks by declaring a static computational graph, which pushes developers to care about coordinating the participants for these data exchanges. Thus, an FGL package should provide a unified view for developers to express such heterogeneous data exchanges and the various subroutines effortlessly, allowing flexible modularization of the rich behaviors so that FGL algorithms can be implemented conveniently.</p><p>(2) Unified and Comprehensive Benchmarks. Due to the privacy issue, real-world FGL datasets are rare. Most prior FGL works are evaluated by splitting a standalone graph dataset. Without a unified splitting mechanism, they essentially use their respective datasets. Meantime, their GNN implementations have not been aligned and integrated into the same FL framework. All these increase the risk of inconsistent comparisons of related works, urging an FGL package to set up configurable, unified, and comprehensive benchmarks.</p><p>(3) Efficient and Automated Model Tuning. Most federated optimization algorithms have not been extensively studied with GNN models. Hence, practitioners often lack proper prior for tuning their GNN models under the FL setting, making it inevitable to conduct hyper-parameter optimization (HPO). Moreover, directly integrating a general HPO toolkit into an FL framework cannot satisfy the efficiency requirements due to the massive cost of executing an entire FL course <ref type="bibr" target="#b17">[18]</ref>. Even a single model is tuned perfectly, the prevalent non-i.i.d.ness in federated graph data might still lead to unsatisfactory performances. In this situation, monitoring the FL procedure to get aware of the non-i.i.d.ness and personalizing (hyper-)parameters are helpful for further tuning the GNN models.</p><p>(4) Privacy Attacks and Defence. Performing privacy attacks on the FL algorithm is a direct and effective way to examine whether the FL procedure has the risk of privacy leakage. However, none of the existing FL framework contains this. Moreover, compared with general FL framework, except sharing the gradients of global model, FGL may also share additional graph related information among clients, such as node embeddings <ref type="bibr" target="#b35">[36]</ref> and neighbor generator <ref type="bibr" target="#b41">[42]</ref>. Without verifying the security of sharing those information, it prevents the applications of FGL in the real-world scenarios.</p><p>Motivated by these, in this paper, we develop an FGL package FS-G to satisfy these challenging requirements:</p><p>(1) We choose to build FS-G upon a message-oriented FL framework FederatedScope <ref type="bibr" target="#b37">[38]</ref>, which abstracts the exchanged data into messages and characterizes the behavior of each participant by defining the message handlers. Users who need to develop FGL algorithms can simply define the (heterogeneous) messages and handlers, eliminating the engineering for coordinating participants. Meantime, different handlers can be implemented with respective graph learning backends (e.g., torch_geometric and tf_geometric).</p><p>(2) For the ease of benchmarking related FGL methods, FS-G provides a GraphDataZoo that integrates a rich collection of splitting mechanisms applicable to most existing graph datasets and a GN-NModelZoo that integrates many state-of-the-art FGL algorithms. Thus, users can reproduce the results as related works effortlessly. It is worth mentioning that we identify a unique covariate shift of graph data that comes from the graph structures, and we design a federal random graph model for the corresponding further study.</p><p>(3) FS-G also provide a component for tuning the FGL methods. On the one hand, it provides fundamental functionalities to achieve low-fidelity HPO, empowering users of FS-G to generalize existing HPO algorithms to the FL settings. On the other hand, when a single model is inadequate to handle the non-i.i.d. graph data, our modeltuning component provides rich metrics to monitor the dissimilarity among clients and a parameter grouping mechanism for describing various personalization algorithms in a unified way.</p><p>(4) Considering the additional heterogeneous data exchanged in FGL, demonstrating the level of privacy leakage under various attacks and providing effective defense strategies are indispensable. FS-G includes a dedicated component to provide various off-theshelf privacy attack and defence abilities, which are encapsulated as plug-in functions for the FGL procedure.</p><p>We utilize FS-G to conduct extensive experimental studies to validate the implementation correctness, verify its efficacy, and better understanding the characteristics of FGL. Furthermore, we employ FS-G to serve three real-world E-commerce scenarios, and the collaboratively learned GNN outperforms their locally learned counterparts, which confirms the business value of FS-G. We plan to open-source FS-G for the community, which we believe can ease the innovation of FGL algorithms, promote their applications, and benefit more real-world business.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Federated Learning</head><p>Generally, the goal of FL is to solve: min ? ? (? )</p><formula xml:id="formula_0">= ? ?=1 ? ? ? ? (? ) = E[? ? (? )],</formula><p>where ? is the number of clients (a.k.a. devices or parties), ? ? (?) is the local objective of the ?-th client, ? ? &gt; 0, and ? ? ? ? = 1.</p><p>As a special case of distributed learning, the essential research topic for FL is its optimization approaches. Concerning the communication cost, FedAvg <ref type="bibr" target="#b27">[28]</ref> allows clients to make more than one local update at each round. The following works include FedProx <ref type="bibr" target="#b22">[23]</ref>, Fe-dOPT <ref type="bibr" target="#b0">[1]</ref>, FedNOVA <ref type="bibr" target="#b34">[35]</ref>, SCAFFOLD <ref type="bibr" target="#b15">[16]</ref>, etc. These methods have been extensively studied on vision and language tasks, but when applied to optimize GNNs, their characteristics are less understood to the community. We refer readers to the survey papers <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Federated Graph Learning</head><p>When handling graph-related tasks under the FL setting, several unique algorithmic challenges emerge, e.g., how to complete the cross-client edges <ref type="bibr" target="#b41">[42]</ref>, handle the heterogeneous graph-level tasks <ref type="bibr" target="#b36">[37]</ref>, augment each client's subgraph <ref type="bibr" target="#b35">[36]</ref>, and align the entities across clients <ref type="bibr" target="#b29">[30]</ref>. Many recent FGL works have attempted to resolve such challenges, which usually require exchanging heterogeneous data across the participants, and the behaviors of the participants become richer than ordinary FL methods. These characteristics of FGL algorithms lead to the unique requirements (see Sec. 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">FL Software</head><p>With the need for FL increasing, many FL frameworks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43]</ref> have sprung up. Most of them are designed like a conventional distributed machine learning framework, where a computational graph is declared and split for participants. Then each specific part is executed by the corresponding participant. Users often have to implement their FL algorithms with declarative programming (i.e., describing the computational graph), which raises the bar for developers. This usability issue is exacerbated in satisfying the unique requirements of FGL methods. Consequently, most existing FL frameworks have no dedicated support for FGL, and practitioners cannot effortlessly build FGL methods upon them. An exception is FedML <ref type="bibr" target="#b13">[14]</ref>, on which a FGL package FedGraphNN <ref type="bibr" target="#b12">[13]</ref> is built. However, they still focus on the FGL algorithms that have simple and canonical behaviors. Many impactful FGL works have not been integrated, including those discussed above. Besides, they have ignored the requirements of efficiently tuning GNN models and conducting privacy attacks&amp;defence for FGL algorithms, which are crucial in both practice and research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">INFRASTRUCTURE</head><p>We present the overview of FS-G in Fig. <ref type="figure" target="#fig_0">1</ref>. At the core of FS-G is a message-oriented FL framework FederatedScope <ref type="bibr" target="#b37">[38]</ref>, which provides fundamental utilities (i.e., framing the FL procedure) and is compatible with various graph learning backends. Thus, we can build our GNNModelZoo and GraphDataZoo upon FederatedScope with maximum flexibility for expressing the learning procedure and minimum care for the federal staff (e.g., coordination participants).</p><p>With such ease of development, we have integrated many state-ofthe-art FGL algorithms into our GNNModelZoo. Then we design the Runner class as an interface to access FGL executions, which unifies the simulation and distributed modes. Meanwhile, a modeltuning component and a component for privacy attack and defense purposes are supplied concerning the performances and privacy preservation of FGL.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Requirements of Federated Graph Learning</head><p>We first review the existing FGL algorithms and summarize their uniqueness against general FL. As shown in Table <ref type="table" target="#tab_1">1</ref>, the three very recent FGL works, targeting different tasks, need to exchange heterogeneous data across the participants. In contrast, in each round of a general FL procedure (e.g., using FedAvg), only homogeneous data are exchanged for one pass from server to clients and one pass back. As a result of this difference, the participant of an FGL course often executes rich kinds of subroutines to handle the received data and prepare what to send. In contrast, the participant of a general FL course has canonical behaviors, i.e., local updates or aggregation. Thus, there is a demand for a unified view to express these multiple passes of heterogeneous data exchanges and the accompanying subroutines. In this way, developers can be agnostic about the communication, better modularize the FGL procedure, and choose the graph learning backend flexibly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Development based on FederatedScope</head><p>To satisfy the unique requirements of FGL discussed above, we develop FS-G based on a message-oriented FL framework named FederatedScope, which abstracts the data exchange in an FL procedure as message passing. With the help of FederatedScope, implementing FGL methods can be summarized in two folds: <ref type="bibr" target="#b0">(1)</ref> Initiate: send model para. defining what kinds of messages should be exchanged; (2) describing the behaviors of the server/client to handle these messages.</p><formula xml:id="formula_1">Handle model para. ? Callback B ? Aggregate &amp; Update Handle model para.? Callback A ? Local update ? Send model para.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Server Client</head><p>From such a point of view, a standard FL procedure is shown in Fig. <ref type="figure" target="#fig_1">2</ref>, where server and client pass homogeneous messages (i.e., the model parameters). When receiving the messages, they conduct aggregation and local updates, respectively. As for FGL algorithms, we take FedSage+ as an example. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, we extract the heterogeneous exchanged messages according to FedSage+, including model parameters, gradients, and node embeddings. Then we frame and transform the operations defined in training steps to callback functions as subroutines to handle different types of received messages. For example, when receiving the request, the client employs the corresponding callback function to send node embeddings and "NeighGen" (i.e., a neighbor generation model) back to the server. The goal of FS-G includes both convenient usage for the existing FGL methods and flexible extension for new FGL approaches. Benefited from FederatedScope, the heterogeneous exchanged data and various subroutines can be conveniently expressed as messages and handlers, which supports us to implement many state-of-the-art FGL methods, including FedSage+, FedGNN, and GCFL+, by providing different kinds of message (e.g., model parameters, node embeddings, auxiliary model, adjacent list, etc) and participants' behavior (e.g., broadcast, cluster, etc). The modularization of a whole FGL procedure into messages and handlers makes it flexible for developers to express various operations defined in customized FGL methods separately without considering coordinating the participants in a static computational graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GRAPHDATAZOO</head><p>A comprehensive GraphDataZoo is indispensable to provide a unified testbed for FGL. To satisfy the various experiment purposes, we allow users to constitute an FL dataset by configuring the choices of Dataset, Splitter, Transform, and Dataloader. Conventionally, the Transform classes are responsible for mapping each graph into another, e.g., augmenting node degree as a node attribute. The Dataloader classes are designed for traversing a collection of graphs or sampling subgraphs from a graph. We will elaborate on the Splitter and the Dataset classes in this section.</p><p>Tasks defined on graph data are usually categorized as follow:</p><p>? Node-level task: Each instance is a node which is associated with its label. To make prediction for a node, its ?-hop neighborhood is often considered as the input to a GNN. ? Link-level task: The goal is to predict whether any given node pair is connected or the label of each given link (e.g., the rating a user assigns to an item). ? Graph-level task: Each instance is an individual graph which is associated with its label.  For the link/node-level tasks, transductive setting is prevalent, where both the labeled and unlabeled links/nodes appear in the same graph. As for the graph-level task, a standalone dataset often consists of a collection of graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Splitting Standalone Datasets</head><p>Existing graph datasets are a valuable source to satisfy the need for more FL datasets <ref type="bibr" target="#b14">[15]</ref>. Under the federated learning setting, the dataset is decentralized. To simulate federated graph datasets by existing standalone ones, our GraphDataZoo integrates a rich collection of splitters. These splitters are responsible for dispersing a given standalone graph dataset into multiple clients, with configurable statistical heterogeneity among them. For the node/link-level tasks, each client should hold a subgraph, while for the graph-level tasks, each client should hold a subset of all the graphs. We aim to enable related works to compare on unified, configurable, and comprehensive federated graph datasets, and thus provide many off-the-shelf splitters. Some splitters split a given dataset by specific meta data or the node attribute value, expecting to simulate realistic FL scenarios. Some other splitters are designed to provide various non-i.i.d.ness, including covariate shift, concept drift, and prior probability shift <ref type="bibr" target="#b14">[15]</ref>. Details about the provided splitters and the FL datasets constructed by applying them can be found in the Appendix A.1 and Appendix A.3, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">New Federated Learning Datasets</head><p>In addition to the the strategy of splitting existing standalone datasets, we also construct three federated graph datasets from other real-world data sources or federal random graph model:</p><p>(1) FedDBLP: We create this dataset from the latest DBLP dump, where each node corresponds to a published paper, and each edge corresponds to a citation. We use the bag-of-words of each paper's abstract as its node attributes and regard the theme of paper as its label. To simulate the scenario that a venue or an organizer forbids others to cite its papers, FS-G allows users to split this dataset by each node's venue or the organizer of that venue.</p><p>(2) Cross-scenario recommendation (CSR): We create this dataset from the user-item interactions collected from an E-commerce platform. FS-G allows users to split the graph by each item's category or by which scenario an interaction happens.</p><p>(3) FedcSBM: Graph data consist of attributive and structural patterns, but prior federated graph datasets have not decoupled the covariate shifts of these two aspects. Hence, we propose a federal random graph model FedcSBM based on cSBM <ref type="bibr" target="#b9">[10]</ref>. FedcSBM can produce the dataset where the node attributes of different clients obey the same distribution. Meantime, the homophilic degrees can be different among the clients, so that covariate shift comes from the structural aspect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">GNNMODELZOO AND MODEL-TUNING COMPONENT</head><p>As an FGL package, FS-G provides a GNNModelZoo. As discussed in Sec. 4, models designated for tasks of different levels will encounter heterogeneous input and/or output, thus requiring different architectures. Hence, in the NN module of FS-G, we modularize a general neural network model into four categories of building bricks:</p><p>? Encoder: embeds the raw node attributes or edge attributes, e.g., atom encoder and bond encoder. With a rich collection of choices in each category, users can build various kinds of neural network models out-of-the-box. Particularly, in addition to the vanilla GNNs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39]</ref>, our GNNModelZoo also includes the GNNs that decouples feature transformation and propagation, e.g., GPR-GNN <ref type="bibr" target="#b7">[8]</ref>. Such a kind of GNNs has been ignored by FedGraphNN <ref type="bibr" target="#b12">[13]</ref>. However, we identify their unique advantages in FGL-handling covariate shift among the client-wise graphs that comes from graph structures.</p><p>For the convenience of developers, FS-G integrates the GN-NTrainer class, which encapsulates the local training procedure. It can be easily configured to adjust for different levels of tasks and full-batch/graph-sampling settings. Thus, developer can focus on the FGL algorithms without caring for the procedure of local updates. Then we implement many state-of-the-art FGL algorithms and integrate them into our GNNModelZoo.</p><p>With our GraphDataZoo and GNNModelZoo, users are empowered with FGL capacities out-of-the-box. However, the performances of FGL algorithms are often sensitive to their hyper-parameters. It is indispensable to make hyper-parameter optimization (HPO) to create reproducible research. To this ends, FS-G incorporates a model-tuning component, which provides the functionalities for (1) making efficient HPO under the FL setting; (2) monitoring the FL procedure and making personalization to better handle non-i.i.d. data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Federated Hyper-parameter Optimization</head><p>In general, HPO is a trial-and-error procedure, where, in each attempt, a specific hyper-parameter configuration is proposed and evaluated. Methods in this line mainly differ from each other in exploiting the feedback of each attempt and exploring the possible configurations based on previous attempts. Whatever method, one of the most affecting issues is the cost of making an exact evaluation, which corresponds to an entire training course and then evaluation. This issue becomes severer under the FGL setting since an entire training course often consists of hundreds of communication rounds, and FGL methods, in each round, often exchange additional information across participants, where even a single training course is extremely costly.</p><p>A general and prosperous strategy to reduce the evaluation cost is making low-fidelity HPO <ref type="bibr" target="#b24">[25]</ref>. Benefiting from FederatedScope, FS-G allows users to achieve this by (1) making a limited number of FL rounds instead of an entire FL course for each attempt of a specific configuration; (2) sampling a small fraction of clients, e.g., ? (? ? ? ), in each round.</p><p>Let us illustrate how it improves the efficiency of an HPO method by the federated version of the Successive Halving Algorithm (SHA) <ref type="bibr" target="#b19">[20]</ref>. As Fig. <ref type="figure" target="#fig_4">4</ref> shows, a set of candidate configurations are maintained, each of which will be evaluated in each stage of SHA. When performing low-fidelity HPO, each specific configuration can be evaluated by restoring an FL course from its corresponding checkpoint (if it exists), making only a few FL rounds to update the model, and evaluating the model to acquire its performance. Then these configurations are sorted w.r.t. their performances and only the top half of them are reserved for the next stage of SHA. This procedure continues until only one configuration remains, regarded as the optimal one. We identify the requirement of functionalities to save and restore an FGL training course from this algorithmic example. These functionalities are also indispensable to achieving a reliable failover mechanism. Thus, we first show which factors determine the state of an FL course: (1) Server-side: Basically, the current round number and the global model parameters must be saved. When the aggregator is stateful, e.g., considering momentum, its maintained state, e.g., the moving average of a certain quantity, needs to be kept.</p><p>(2) Client-side: When the local updates are made with mini-batch training, the client-specific data-loader is usually stateful, whose index and order might need to be held. When personalization is utilized, the client-specific model parameters need to be saved. With sufficient factors saved as a checkpoint, FS-G can restore an FGL training course from it and proceed.</p><p>Meanwhile, we follow the design of FederatedScope and make the entry interface of FS-G as a callable FGL runner, which receives a configuration and returns a collection of metrics for the conducted FGL training course (entire or not). Consequently, each HPO algorithm incorporated in our model-tuning component can be abstracted as an agent, repeatedly calling the FGL runner to collect the feedback. Benefiting from this interface design and the capacity to save and restore an FGL training course, any one-shot HPO method can be effortlessly generalized to the FGL setting upon FS-G. It is worth mentioning that what to return by the FGL runner is configurable, where efficiency-related metrics, e.g., the average latency of each round, can be included. Therefore, optimizing hyper-parameters from the system perspective <ref type="bibr" target="#b40">[41]</ref> is also supported by FS-G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Monitoring and Personalization</head><p>Practitioners often monitor the learning procedure by visualizing the curves of training loss and validation performance to understand whether the learning has converged and the GNN model has overfitted to the training data. When it comes to FGL, we consider both the client-wise metrics, e.g., the training loss of local update, and some metrics to be calculated at the server-side. Specifically, we have implemented several metrics, including B-local dissimilarity <ref type="bibr" target="#b22">[23]</ref> and the covariance matrix of gradients, calculated from the aggregated messages to reflect the statistical heterogeneity among clients. The larger these metrics are, the more different client-wise graphs are. As shown in Fig. <ref type="figure" target="#fig_7">5a</ref>, the B-local dissimilarity on noni.i.d. data is larger than that on i.i.d. data at almost all stages of the training course, which becomes particularly noticeable at the end.</p><p>Then we build the monitoring functionality upon related toolkits (e.g., WandB and TensorBoard) to log and visualize the metrics. To use the out-off-shelf metrics, users only need to specify them in the configuration. Meantime, FS-G has provided the API for users to register any quantity calculated/estimated during the local update/aggregation, which would be monitored in the execution.   Once some monitored metrics indicate the existence of noni.i.d.ness, users can further tune their GNN by personalizing the model parameters and even the hyper-parameters. We present an example of personalization in Fig. <ref type="figure" target="#fig_7">5b</ref>. Each client has its specific encoder and decoder as the tasks among the clients come from different domains with different node attributes and node classes.</p><p>In practice, a more fine-grained personalization might be preferred, where only some layers or even some variables are client-specific.</p><p>To satisfy such purposes, FS-G first allows users to instantiate the model of each participant individually. Then we build a flexible parameter grouping mechanism upon the naming systems of underlying machine learning engines. Specifically, this mechanism allows users to easily declare each part (with flexible granularity) of the model as client-specific or shared. Only the shared parts will be exchanged. Meantime, FS-G allows users to employ the latest personalized FL algorithms (e.g., FedBN <ref type="bibr" target="#b23">[24]</ref> and Ditto <ref type="bibr" target="#b20">[21]</ref>) that have been provided by FederatedScope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">OFF-THE-SHELF ATTACK AND DEFENCE ABILITIES</head><p>The privacy attack&amp;defence component of FederatedScope has integrated various off-the-shelf passive privacy attack methods, including class representative inference attack, membership inference attack, property inference attack, and training inputs and labels inference attack. These methods have been encapsulated as optional hooks for our GNNTrainer. Once the user has selected a specific hook, the GNN model and some needed information about the target data would automatically be fed into the hook during the training procedure. Besides the previous passive attack setting where the adversaries are honest-but-curious, FS-G also supports the malicious adversary setting. The attackers can deviate from the FL protocol by modifying the messages. To defend the passive privacy attacks, FS-G can leverage FederatedScope's plug-in defense strategies, including differential privacy, MPC, and data compression. Meanwhile, FederatedScope provides the information checking mechanism to effectively detect anomaly messages and defend the malicious attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EXPERIMENTS</head><p>We utilize FS-G to conduct extensive experiments, with the aim to validate the implementation correctness of FS-G, set up benchmarks for FGL that have long been demanded, and gain more insights about FGL. Furthermore, we deploy FS-G in real-world E-commerce scenarios to evaluate its business value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">An Extensive Study about Federated Graph Learning</head><p>In this study, we consider three different settings: (1) Local: Each client trains a GNN model with its data. ( <ref type="formula">2</ref>) FGL: FedAvg <ref type="bibr" target="#b27">[28]</ref>,</p><p>FedOpt <ref type="bibr" target="#b0">[1]</ref>, and FedProx <ref type="bibr" target="#b22">[23]</ref> are applied to collaboratively train a GNN model on the dispersed data, respectively. (3) Global: One GNN model is trained on the completed dataset. By comparing these settings with various GNN architectures and on diverse tasks, we intend to set up comprehensive and solid benchmarks for FGL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Node-level Tasks.</head><p>Protocol. For the purposes of training and validation, the data of interest are subgraphs deduced from the original graph. Yet, the so-called global evaluation is considered for testing, which means the models are evaluated on the original graph. In other words, for the local setting, we train and validate each client's model on its incomplete subgraph and test the model on the complete global graph. As for FGL, we train each model on their incomplete subgraph federally and evaluate each model on the complete global graph. For the global setting, we train and evaluate each model on the complete global graph. For each setting, we consider popular GNN architectures: GCN, GraphSage, GAT, and GPR-GNN.</p><p>Particularly, we consider one of the recently proposed FGL algorithms, FedSAGE+ <ref type="bibr" target="#b41">[42]</ref>, which is highlighted by simultaneously training generative models for predicting the missing links so that its GraphSAGE models can be trained on the mended graphs. To conduct the experiments uniformly and fairly, we split the nodes into train/valid/test sets, where the ratio is 60% : 20% : 20% for citation networks and 50% : 20% : 30% for FedDBLP. We randomly generate five splits for each dataset. Each model is trained and evaluated with these five splits, and we report the averaged metric and the standard deviation. To compare the performance of each model, we choose accuracy as the metric for all node-level tasks. In addition, we perform hyper-parameter optimization (HPO) for all methods with the learning rate ? {0.01, 0.05, 0.25} in all settings, and the local update steps ? {1, 4, 16} in FGL.  Results and Analysis. We present the results on three citation networks in Table <ref type="table" target="#tab_3">2</ref> and Table <ref type="table" target="#tab_4">3</ref>, with random_splitter and commu-nity_splitter, respectively. Overall, FGL can improve performance regarding those individually trained on local data, as the experience in vision and language domains suggests. In most cases on the randomly split datasets, the GNN trained on the global (original) data performs better than that of FGL. With the splitting, the union of all client-wise graphs still has fewer edges than the original graph, which may explain this performance gap. In contrast, the FGL setting often performs better than those trained on the global graph when our community_splitter constructs the federated graph datasets. At first glance, this phenomenon is counterintuitive, as the splitter also removes some of the original edges. However, these citation graphs exhibit homophily, saying that nodes are more likely to connect to nodes with the same label than those from other classes. When split by the community detection-based algorithm, we identify the changes that removed edges often connect nodes with different labels, which further improves the homophilic degree of the graphs. As a result, the resulting federated graph dataset becomes easier to handle by most GNN architecture. For FedSage+, we show the results in Table <ref type="table" target="#tab_5">4</ref>, where FedSage+ significantly outperforms its baseline (i.e., GraphSage) on most of the datasets. It benefits from the jointly learned generative models, which enable each client to reconstruct the missing cross-client links under federated setting. We present the results on FedDBLP in Table <ref type="table">8</ref>, where the fraction of removed edges reaches 60% and 40% when split by venue and organizer, respectively. Besides, there are 20 and 8 clients under the two splitting settings, respectively, larger than the previous experiments. Since the client-specific graphs are tiny, w.r.t. the original one, the available training examples are limited for the local setting. All these factors make the performances of different GNNs unsatisfactory under the local setting. As for FGL, since FedAvg aggregates the clients' updates, it somehow exploits all the training examples and thus achieves comparable performances against the global setting. Considering that FedDBLP has simulated the data interruption in real life, these results confirm the effectiveness of FGL to handle this emerging challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Link-level Tasks.</head><p>Protocol. We largely follow the experimental setup in Sec. 7.1.1. Specifically, we split the links into train/valid/test sets, where the ratio is 80% : 10% : 10% for recommendation dataset Ciao. We use the official train/valid/test split for the knowledge graphs WN18 and FB15k-237. In addition, the link predictor for each model is a two-layer MLP with 64 as the hidden layer dimension, where the input is the element-wise multiplication of the node embeddings of the node pair. We report the mean accuracy for the recommendation dataset and Hits at 1, 5, and 10 for the knowledge graphs. Results and Analysis. We present the results on Ciao in Table <ref type="table">9</ref> and the results on the knowledge graphs in Table <ref type="table" target="#tab_6">5</ref>. Overall, the performances of the FGL setting are better than those of the local setting, which is consistent with the conclusion drawn from nodelevel tasks and further confirms the effectiveness of FGL. Notably, the splitting strategy results in unbalanced relation distributions among the clients on knowledge graphs, making the triplets of each client insufficient to characterize the entities. Thus, the performance gaps between the local and the FGL settings are broad. Meantime, the performances of the FGL setting become comparable to those of the global setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.3">Graph-level Tasks.</head><p>Protocol. We largely follow the experimental setup in Sec. 7.1.2. Specifically, a pre-linear layer is added to encode the categorical attributes preceding GNN. And a two-layer MLP is added to adapt GNN to multi-task classification as a classifier following GNN. Notably, for multi-task FGL in multi-domain datasets, only the parameters of GNN are shared. In addition, we consider the recently proposed FGL algorithm GCFL+ <ref type="bibr" target="#b36">[37]</ref>, which clusters clients and performs FedAvg in a cluster-wise manner so that clients with similar data distributions share a common GIN model. Results and Analysis. We present the main results in Table <ref type="table" target="#tab_7">6</ref>, where there is no global setting on Multi-task dataset, as the same model cannot handle the graphs of different tasks. Overall, the relationships between different settings have been preserved compared to those on node-level tasks. One exception is on the Multi-task dataset, where the non-i.i.d.ness is severe, e.g., the average number of nodes of the graphs on a client deviates from other clients a lot. Although we have personalized the models by FS-G, the gradients/updates collected at the server-side might still have many interferences. Without dedicated techniques to handle these, e.g., GCFL+, FedAvg cannot surpass the individually trained models. But with federated optimization algorithms, such as FedOpt and FedProx, some GNN architectures surpass the individually trained models, which proves the effectiveness of these federated optimization algorithms. In addition, we implement personalized FL algorithms (here FedBN <ref type="bibr" target="#b23">[24]</ref> and Ditto <ref type="bibr" target="#b20">[21]</ref>) and apply them for GIN on the multi-task dataset. FedBN and Ditto lead to performances (i.e., mean accuracy with standard deviation) 72.90?1.33 (%) and 63.35?0.6 (%), respectively. Due to the label unbalance, all kinds of GNNs result in the same accuracy on the HIV dataset. Hence, we have to solely report their ROC-AUC in Table <ref type="table" target="#tab_1">10</ref> of Appendix A.2, from which we can draw similar conclusions. Meantime, GIN surpasses GCN with FedAvg on almost all datasets, which implies the capability of distinguishing isomorphism graphs is still critical for federated graph-level tasks. And in Table <ref type="table" target="#tab_8">7</ref>, GCFL+ outperforms its baseline (i.e., GIN federally learned by FedAvg) on both the IMDB and Multi-task datasets, and achieves comparable performance on PROTEINS. GCFL+ clusters the clients according to their sequences of gradients, where clients belonging to the same cluster share the same model parameters. Its advantages are likely to come from this smart mechanism, which handles the non-i.i.d.ness among clients better. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Study about Hyper-parameter Optimization</head><p>In this experiment, we conduct HPO for federated learned GCN and GPR-GNN, intending to verify the effectiveness of our modeltuning component. Moreover, we can set up an HPO benchmark for the FGL setting and attain some insights, which have long been missed but strongly demanded.</p><p>Protocol. We take SHA (see Sec. 5.1) to optimize the hyper-parameters for GCN/GPR-GNN learned by FedAvg. We largely follow the experimental setup in the Sec. 7.1.1 while extend the search space of the hyper-parameters, where hidden dim ? {32, 64}, dropout ? {0.0, 0.5}, and weight decay in ? {0, 0.0005}. As PubMed has much more nodes and edges than the other two citation networks, we target the node classification task on PubMed to draw statistically reliable conclusions. In order to simulate the FL setting, we apply our community_splitter to divide the PubMed into five parts for five clients. As exact evaluation is infeasible in practice, we study low-fidelity HPO, where the number of FL rounds for each evaluation ? {1, 2, 4, 8}, and the client sampling rate for each round of FedAvg ? {20%, 40%, 80%, 100%}. For each choice of fidelity, we repeat the SHA five times with different seeds, and we report the average ranking of its searched hyper-parameter configuration.</p><p>Results and Analysis. There are in total 72 possible configurations, with each of which we conduct the FGL procedure and thus acquire the ground-truth performances. These results become a lookup table, making comparing HPO methods efficient and convenient. We present the results of our HPO experiment in Fig. <ref type="figure" target="#fig_10">6</ref>, where higher fidelity leads to better configuration for both kinds of GNNs, as expected. At first, we want to remind our readers that the left-upper region in each grid table corresponds to extremely lowfidelity HPO. Although their performances cannot be comparable    to those in the other regions, they have successfully eliminated a considerable fraction of poor configurations. Meanwhile, increasing fidelity through the two aspects, i.e., client sampling rate and the number of training rounds, reveal comparable efficiency in improving the quality of searched configurations. This property provides valuable flexibility for practitioners to keep a fixed fidelity while trade-off between these two aspects according to their system status (e.g., network latency and how the dragger behaves). All these observations suggest the application of low-fidelity HPO to FGL, as well as the effectiveness of FS-G's model-tuning component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Study about Non-I.I.D.ness and Personalization</head><p>We aim to study the unique covariate shift of graph data-node attributes of different clients obey the identical distributions, but their graph structures are non-identical. Meantime, we evaluate whether the personalization provided by FS-G's model-tuning component can handle such non-i.i.d.ness. Protocol. We choose FedcSBM (see Sec. We apply FedAvg to optimize the parameters for feature transformation while the spectral coefficients (i.e., that for propagation) are client-specific. We repeat such a procedure five times and report the mean test accuracy and the standard deviation. More details can be found in Appendix A.2.</p><p>Results and Analysis. We illustrate the results in Fig. <ref type="figure" target="#fig_11">7</ref>, where each level of homophilic degree corresponds to a client. Overall, the personalization setting outperforms others. We attribute this advantage to making appropriate personalization, as the personalization setting consistently performs better across different clients, i.e., on graphs with different levels of homophilic degrees. On the other hand, the ordinary FL exhibits comparable performances with the local setting, implying the collaboration among clients fails to introduce any advantage. FedAvg might fail to converge due to the dissimilarity among received parameters, as the B-local dissimilarities shown in Fig. <ref type="figure" target="#fig_7">5a</ref> indicate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Deployment in Real-world E-commerce Scenarios</head><p>We deploy FS-G to serve three E-commerce scenarios operated by different companies/departments. These scenarios differ from each other-one is a search engine, but the others are recommender systems before and after the purchase behavior. Since they have the same goal to predict whether a given user will click a specific item, and their users and items have massive intersections, sharing the user-item interaction logs to train a model is promising and has long been their solution. However, such data sharing raises the risk of privacy leakage and cannot respect the newly established regulations, e.g., the European General Data Protection Regulation (GDPR) and Cybersecurity Law of China (CLA). Without data sharing, each party has to train its model on its useritem bipartite graph, missing the opportunity to borrow strength from other graphs. Considering the scales of their graphs are pretty different, e.g., one scenario has only 184,419 interactions while another has 2,860,074, this might severely hurt the performance on "small" scenarios. With our FGL service, these parties can collaboratively train a GraphSAGE model for predicting the link between each user-item pair. Evaluation on their held-out logs shows that the ROC-AUC of individually trained GraphSAGE models is 0.6209?0.0024 while that of FGL is 0.6278?0.0040, which is a significant improvement. It is worth noting that a small improvement (at 0.001-level) in offline ROC-AUC evaluation means a substantial difference in real-world click-through rate prediction for search/recommendation/advertisements. Therefore, the improvement confirms the business value of FS-G.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of FS-G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Implement a standard FL course based on Feder-atedScope.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Implement FedSage+ based on the message-oriented framework FederatedScope.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>? GNN : learns discriminative representations for the nodes from their original representations (raw or encoded) and the graph structures.? Decoder: recovers these hidden representations back into original node attributes or adjacency relationships. ? Readout: aggregates node representations into a graph representation, e.g., the mean pooling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An example of HPO for FGL: FS-G allows each training course to be restored from a given checkpoint, proceed for any specified number of rounds, and be saved for continual training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( a )</head><label>a</label><figDesc>Monitoring the FGL course on i.i.d. and non-i.i.d. datasets constructed by FedcSBM. An example of personalizing GNN: Each client has its dedicated encoder and decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Examples of monitoring and personalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: SHA with various fidelity to optimize GCN/GPR-GNN: We report the average ranking of searched hyperparameter configuration (the small, the better).</figDesc><graphic url="image-1.png" coords="8,351.50,97.99,63.39,63.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Accuracy by levels of homophily and methods.</figDesc><graphic url="image-3.png" coords="8,336.08,240.06,201.73,108.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>4 . 2 )</head><label>42</label><figDesc>as the dataset. Specifically, we randomly generate a sample from our FedcSBM, containing eight graphs with different homophilic degrees. Then we choose GPR-GNN as the GNN model and consider three settings: (1) Ordinary FL: We apply FedAvg to optimize the model where the clients collaboratively optimize all model parameters; (2) Local: We allow each client to optimize its model on its graph; (3) Personalization:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>A summary of three representative FGL algorithms, where we only list the behaviors other than ordinary local updates and aggregation.</figDesc><table><row><cell>Method</cell><cell>FedSage+ [42]</cell><cell>FedGNN [36]</cell><cell>GCFL+ [37]</cell></row><row><cell>Task</cell><cell>Node classification</cell><cell>Link prediction</cell><cell>Graph classification</cell></row><row><cell></cell><cell>Model param.</cell><cell>Model param.</cell><cell>Model param.</cell></row><row><cell>Exchange</cell><cell>Node emb. NeighGen param.</cell><cell>Node emb. Adj. list</cell><cell>Model grad.</cell></row><row><cell></cell><cell>NeighGen grad.</cell><cell></cell><cell></cell></row><row><cell>Server</cell><cell>Broadcast emb.</cell><cell>Node clustering</cell><cell>Grad. clustering</cell></row><row><cell>behavior</cell><cell>Broadcast grad.</cell><cell>Broadcast emb.</cell><cell>Param. deriving</cell></row><row><cell>Client</cell><cell cols="2">Send emb. and NeighGen Generate pseudo edges</cell><cell>None</cell></row><row><cell>behavior</cell><cell>Apply cross-client grad.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results on representative node classification datasets with random_splitter: Mean accuracy ? standard deviation. 63?1.35 86.11?1.29 86.60?1.59 86.89?1.82 74.29?1.35 76.48?1.52 77.43?0.90 77.29?1.20 77.42?1.15 85.25?0.73 85.29?0.95 84.39?1.53 85.21?1.17 85.38?0.33 GraphSAGE 75.12?1.54 85.42?1.80 84.73?1.58 84.83?1.66 86.86?2.15 73.30?1.30 76.86?1.38 75.99?1.96 78.05?0.81 77.48?1.27 84.58?0.41 86.45?0.43 85.67?0.45 86.51?0.37 86.23?0.58 GAT 78.86?2.25 85.35?2.29 84.40?2.70 84.50?2.74 85.78?2.43 73.85?1.00 76.37?1.11 76.96?1.75 77.15?1.54 76.91?1.02 83.81?0.69 84.66?0.74 83.78?1.11 83.79?0.87 84.89?0.34</figDesc><table><row><cell></cell><cell></cell><cell>Cora</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CiteSeer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PubMed</cell><cell></cell><cell></cell></row><row><cell>Local</cell><cell>FedAvg</cell><cell>FedOpt</cell><cell>FedProx</cell><cell>Global</cell><cell>Local</cell><cell>FedAvg</cell><cell>FedOpt</cell><cell>FedProx</cell><cell>Global</cell><cell>Local</cell><cell>FedAvg</cell><cell>FedOpt</cell><cell>FedProx</cell><cell>Global</cell></row><row><cell cols="15">GCN 80.95?1.49 86.GPR-GNN 84.90?1.13 89.00?0.66 87.62?1.20 88.44?0.75 88.54?1.58 74.81?1.43 79.67?1.41 77.99?1.25 79.35?1.11 79.67?1.42 86.85?0.39 85.88?1.24 84.57?0.68 86.92?1.25 85.15?0.76</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results on representative node classification datasets with community_splitter: Mean accuracy ? standard deviation.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Cora</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CiteSeer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PubMed</cell><cell></cell></row><row><cell></cell><cell>Local</cell><cell>FedAvg</cell><cell>FedOpt</cell><cell>FedProx</cell><cell>Global</cell><cell>Local</cell><cell>FedAvg</cell><cell>FedOpt</cell><cell>FedProx</cell><cell>Global</cell><cell>Local</cell><cell>FedAvg</cell><cell>FedOpt</cell><cell>FedProx</cell><cell>Global</cell></row><row><cell>GCN</cell><cell cols="15">65.08?2.39 87.32?1.49 87.29?1.65 87?16?1.51 86.89?1.82 67.53?1.87 77.56?1.45 77.80?0.99 77.62?1.42 77.42?1.15 77.01?3.37 85.24?0.69 84.11?0.87 85.14, 0.88 85.38?0.33</cell></row><row><cell cols="16">GraphSAGE 61.29?3.05 87.19?1.28 87.13?1.47 87.09?1.46 86.86?2.15 66.17?1.50 77.80?1.03 78.54?1.05 77.70?1.09 77.48?1.27 78.35?2.15 86.87?0.53 85.72?0.58 86.65?0.60 86.23?0.58</cell></row><row><cell>GAT</cell><cell cols="15">61.53?2.81 86.08?2.52 85.65?2.36 85.68?2.68 85.78?2.43 66.17?1.31 77.21?0.97 77.34?1.33 77.26?1.02 76.91?1.02 75.97?3.32 84.38?0.82 83.34?0.87 84.34?0.63 84.89?0.34</cell></row><row><cell>GPR-GNN</cell><cell cols="15">69.32?2.07 88.93?1.64 88.37?2.12 88.80?1.29 88.54?1.58 71.30?1.65 80.27?1.28 78.32?1.45 79.73?1.52 79.67?1.42 78.52?3.61 85.06?0.82 84.30?1.57 86.77?1.16 85.15?0.76</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparisons between FedSage+ and GraphSAGE (with FedAvg) on representative node classification datasets: Mean accuracy (%) ? standard deviation.</figDesc><table><row><cell></cell><cell></cell><cell>Cora</cell><cell cols="2">CiteSeer</cell><cell cols="2">PubMed</cell></row><row><cell></cell><cell>random</cell><cell>community</cell><cell>random</cell><cell>community</cell><cell>random</cell><cell>community</cell></row><row><cell cols="3">GraphSAGE 85.42?1.80 87.19?1.28</cell><cell cols="2">76.86?1.38 77.80?1.03</cell><cell cols="2">86.45?0.43 86.87?0.53</cell></row><row><cell>FedSage+</cell><cell cols="2">85.07?1.20 87.68?1.55</cell><cell cols="2">78.04?0.91 77.98?1.23</cell><cell cols="2">88.19?0.32 87.94?0.27</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Results on representative link prediction datasets with label_space_splitter: Hits@?. .34 73.85 30.00 79.72 96.67 22.13 78.96 94.07 27.32 83.01 96.38 29.67 86.73 97.05 6.07 20.29 30.35 9.86 34.27 48.02 4.12 18.07 31.79 4.66 28.74 41.67 7.80 32.46 44.64 GraphSAGE 21.06 54.12 79.88 23.14 78.85 93.70 22.82 79.86 93.12 23.14 78.52 93.67 24.24 79.86 93.84 3.95 14.64 24.47 7.13 23.38 36.60 2.20 19.21 27.64 5.85 24.05 36.33 6.19 23.57 35.98 GAT 20.89 49.42 72.48 23.14 77.62 93.49 23.14 74.64 93.52 23.53 78.40 93.00 24.24 80.18 93.76 3.44 15.02 25.14 6.06 25.76 39.04 2.71 18.89 32.76 6.19 25.09 38.00 6.94 24.43 37.87 GPR-GNN 22.86 60.45 80.73 26.67 82.35 96.18 24.46 73.33 87.18 27.62 81.87 95.68 29.19 82.34 96.24 4.45 13.26 21.24 9.62 32.76 45.97 2.01 9.81 16.65 3.72 15.62 27.79 10.62 33.87 47.45</figDesc><table><row><cell>WN18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Results on representative graph classification datasets: Mean accuracy (%) ? standard deviation.</figDesc><table><row><cell></cell><cell></cell><cell>PROTEINS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>IMDB</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Multi-task</cell><cell></cell><cell></cell></row><row><cell>Local</cell><cell>FedAvg</cell><cell>FedOpt</cell><cell>FedProx</cell><cell>Global</cell><cell>Local</cell><cell>FedAvg</cell><cell>FedOpt</cell><cell>FedProx</cell><cell>Global</cell><cell>Local</cell><cell>FedAvg</cell><cell>FedOpt</cell><cell>FedProx</cell><cell>Global</cell></row><row><cell cols="6">GCN 71.10?4.65 73.54?4.48 71.24?4.17 73.36?4.49 71.77?3.62 50.76?1.14</cell><cell>53.24?6.04</cell><cell cols="7">50.49?8.32 48.72?6.73 53.24?6.04 66.37?1.78 65.99?1.18 69.10?1.58 68.59?1.99</cell><cell>-</cell></row><row><cell cols="14">GIN 69.06?3.47 73.74?5.71 60.14?1.22 73.18?5.66 72.47?5.53 55.82?7.56 64.79?10.55 51.87?6.82 70.65?8.35 72.61?2.44 75.05?1.81 63.40?2.22 63.33?1.18 63.01?0.44</cell><cell>-</cell></row><row><cell cols="6">GAT 70.75?3.33 71.95?4.45 71.07?3.45 72.13?4.68 72.48?4.32 53.12?5.81</cell><cell>53.24?6.04</cell><cell cols="7">47.94?6.53 53.82?5.69 53.24?6.04 67.72?3.48 66.75?2.97 69.58?1.21 69.65?1.14</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Comparisons between GCFL+ and GIN (with Fe-dAvg) on graph classification datasets: Mean accuracy (%) ? standard deviation.</figDesc><table><row><cell></cell><cell>PROTEINS</cell><cell>IMDB</cell><cell>Multi-task</cell></row><row><cell>GIN</cell><cell cols="3">73.74?5.71 64.79?10.55 63.40?2.22</cell></row><row><cell cols="2">GCFL+ 73.00?5.72</cell><cell>69.47?8.71</cell><cell>65.14?1.23</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">CONCLUSION</head><p>In this paper, we implemented an FGL package, FS-G, to facilitate both the research and application of FGL. Utilizing FS-G, FGL algorithms can be expressed in a unified manner, validated against comprehensive and unified benchmarks, and further tuned efficiently. Meanwhile, FS-G provides rich plug-in attack and defence utilities to assess the level of privacy leakage for the FGL algorithm of interest. Besides extensive studies on benchmarks, we deploy FS-G in real-world E-commerce scenarios and gain business benefits. We will release FS-G to create greater business value from the ubiquitous graph data while preserving privacy.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 Details of Off-the-shelf Splitters</head><p>To this end, we have implemented mainly six classes of splitters:</p><p>(1) community_splitter: This is often adopted in node-level tasks to simulate the locality-based federated graph data <ref type="bibr" target="#b41">[42]</ref>, where nodes in the same client are densely connected while cross-client edges are unavailable. Specifically, community detection algorithms (e.g., Louvain <ref type="bibr" target="#b3">[4]</ref> and METIS <ref type="bibr" target="#b16">[17]</ref>) are at first applied to partition a graph into several clusters. Then these clusters are assigned to the clients, optionally with the objective of balancing the number of nodes in each client.</p><p>(2) random_splitter: Random split is often adopted in node-level tasks, e.g., FedGL <ref type="bibr" target="#b6">[7]</ref>. Specifically, the node set of the original graph is randomly split into ? subsets with or without intersections. Then, the subgraph of each client is deduced from the nodes assigned to that client. Optionally, a specified fraction of edges is randomly selected to be removed.</p><p>(3) meta_splitter: In many cases, there are meta data or at least interpretable edge/node attributes that allow users to simulate a real FL setting via splitting the graph based on the meta data or the values of those attributes. In citation networks, papers published in different conferences or organizations usually focus on different themes. Splitting by conference/organization naturally leads to node (i.e., paper) classification tasks with non-identical label distributions (i.e., prior probability shift <ref type="bibr" target="#b14">[15]</ref>). Meanwhile, in recommender systems, the same user often has different tendencies to items in different domains/scenarios, where splitting by domain/scenario can provide concept shift among clients.</p><p>(4) instance_space_splitter: It is responsible for creating feature distribution skew (i.e., covariate shift). To realize this, we sort the graphs based on their values of a certain aspect, e.g., for Molhiv, molecules are sorted by their scaffold, and then each client is assigned with a segment of the sorted list.</p><p>(5) label_space_splitter: It is designed to provide label distribution skew. For classification tasks, e.g., relation prediction for knowledge graph completion, the existing triplets are split into the clients by latent dirichlet allocation (LDA) <ref type="bibr" target="#b2">[3]</ref>. For regression tasks, e.g., PCQM4M, FS-G can discretize the label before conducting LDA.</p><p>(6) multi_task_splitter: This is mainly designed for multi-task learning or personalized learning. Sometimes different clients have different tasks, e.g., in the domain of the molecule, some clients have the task of determining the toxicity, while some clients have the task of predicting the HOMO-LUMO gap. A more challenging case <ref type="bibr" target="#b36">[37]</ref> is that the graphs come from the different domains, e.g., molecules, proteins, and social networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Details about Our Experiments</head><p>Experimental settings. In node-level tasks, the detailed hyperparameters in our experiments are as follows: the number of training rounds is 400, the early stopping is 100, the GNN layers is 2 (in GPR-GNN, K is 10), the hidden layer dimension is 64 on citation networks and 1024 on FedDBLP, the dropout is 0.5, weight decay is 0.0005, the number of clients is five on citation networks and the optimizer is SGD. More Results. We provide more experimental results on the Fed-DBLP, Ciao, and HIV in the Table. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HIV (ROC-AUC) Local</head><p>FGL Global GCN 0.6193?0.0319 0.6263?0.0332 0.6939?0.0165 GIN 0.6925?0.0354 0.7774?0.0195 0.7958?0.0200 GAT 0.6192?0.0101 0.6287?0.0197 0.7034?0.0201 Non-I.I.D.ness and Personalization Study. In order to generate different level of homophily graphs, we set ? ? {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8} for cSBM model. We repeat our experiment with GPR-GNN for three-time with a different seed. In FGL setting, each client shares the parameters of linear layers, while the parameters of the label propagation layer are personalized. Real-world Deployment. We provide more statistical information about the real-world E-commerce scenarios dataset. Search engine scenario contains 106,222 users and 464,904 items. In the other two scenarios, the first contains 12,588 users and 78,996 items, and the second contains 107,589 users and 559,796, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Datasets description</head><p>As in Table <ref type="table">11</ref>, we provide a detailed description of datasets of FS-G with datasets and suggested Splitter accordingly. The datasets are collected from different domains, and the nodes and edges represent different meanings. We will support more datasets and provide more benchmarks in the future. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">FedOpt: Towards Communication Efficiency and Privacy Preservation in Federated Learning</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Asad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Moustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takayuki</forename><surname>Ito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taner</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhil</forename><surname>Topal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchi</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Titouan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">Pb</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">D</forename><surname>De Gusm?o</surname></persName>
		</author>
		<author>
			<persName><surname>Lane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.14390</idno>
		<title level="m">A friendly federated learning research framework</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>David M Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01">2003. Jan (2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast unfolding of communities in large networks</title>
		<author>
			<persName><forename type="first">Jean-Loup</forename><surname>Vincent D Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renaud</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Lambiotte</surname></persName>
		</author>
		<author>
			<persName><surname>Lefebvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Mechanics: Theory and Experiment</title>
		<imprint>
			<biblScope unit="page">10008</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards Federated Learning at Scale: System Design</title>
		<author>
			<persName><forename type="first">Keith</forename><surname>Bonawitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Grieskamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Huba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ingerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chlo?</forename><surname>Kiddon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Kone?n?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Mazzocchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timon</forename><surname>Van Overveldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Petrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Roselander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="374" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Caldas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Meher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Duddu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Kone?n?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01097</idno>
		<title level="m">Leaf: A benchmark for federated settings</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">FedGL: Federated Graph Learning Framework with Global Self-Supervision</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weibo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyue</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zibin</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03170</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adaptive Universal Generalized PageRank Graph Neural Network</title>
		<author>
			<persName><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="3844" to="3852" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Contextual Stochastic Block Models</title>
		<author>
			<persName><forename type="first">Yash</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhabrata</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elchanan</forename><surname>Mossel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Hard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajiv</forename><surname>Mathews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fran?oise</forename><surname>Beaufays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Eichner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03604</idno>
		<title level="m">Federated learning for mobile keyboard prediction</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Chlo? Kiddon, and Daniel Ramage</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Chaoyang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keshav</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emir</forename><surname>Ceyani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murali</forename><surname>Annavaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Avestimehr</surname></persName>
		</author>
		<title level="m">FedGraphNN: A Federated Learning System and Benchmark for Graph Neural Networks</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Chaoyang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songze</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhyun</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praneeth</forename><surname>Vepakomma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murali</forename><surname>Annavaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Avestimehr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13518</idno>
		<title level="m">FedML: A Research Library and Benchmark for Federated Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Peter</forename><surname>Kairouz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Avent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aur?lien</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Bennis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Arjun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kallista</forename><surname>Bhagoji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Bonawitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><surname>Cummings</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04977</idno>
		<title level="m">Advances and open problems in federated learning</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SCAFFOLD: Stochastic Controlled Averaging for Federated Learning</title>
		<author>
			<persName><forename type="first">Praneeth</forename><surname>Sai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satyen</forename><surname>Karimireddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sashank</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananda</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Theertha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5132" to="5143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multilevel k-way hypergraph partitioning</title>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLSI design</title>
		<imprint>
			<biblScope unit="page" from="285" to="300" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Federated Hyperparameter Tuning: Challenges, Baselines, and Connections to Weight-Sharing</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renbo</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nina</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hyperband: A novel bandit-based approach to hyperparameter optimization</title>
		<author>
			<persName><forename type="first">Lisha</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giulia</forename><surname>Desalvo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="6765" to="6816" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note>Afshin Rostamizadeh, and Ameet Talwalkar</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ditto: Fair and Robust Federated Learning Through Personalization</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Beirami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6357" to="6368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Federated learning: Challenges, methods, and future directions</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anit</forename><surname>Kumar Sahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="50" to="60" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Federated optimization in heterogeneous networks</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anit</forename><surname>Kumar Sahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maziar</forename><surname>Sanjabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="429" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">FedBN: Federated Learning on Non-IID Features via Local Batch Normalization</title>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Meirui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automl: From methodology to application</title>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexiang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4853" to="4856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fedvision: An online visual object detection platform powered by federated learning</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anbu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youzhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lican</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="13172" to="13179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathalie</forename><surname>Baracaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gegi</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashank</forename><surname>Rajamoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuya</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayaram</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Sinn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10987</idno>
		<title level="m">Ibm federated learning: an enterprise framework white paper v0. 1</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Communication-Efficient Learning of Deep Networks from Decentralized Data</title>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eider</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Hampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaise</forename><surname>Aguera Y Arcas</surname></persName>
		</author>
		<idno>AISTATS. 1273-1282</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Paulik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Seigel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominic</forename><surname>Telaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joris</forename><surname>Kluivers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><forename type="middle">Wai</forename><surname>Rogier Van Dalen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><surname>Granqvist</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08503</idno>
		<title level="m">Chris Vandevelde, et al. 2021. Federated Evaluation and Tuning for On-Device Personalization: System Design &amp; Applications</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Differentially Private Federated Knowledge Graphs Embedding</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1416" to="1425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Federated Learning and Differential Privacy: Software tools analysis, the Sherpa. ai FL framework and methodological guidelines for preserving data privacy</title>
		<author>
			<persName><forename type="first">Nuria</forename><surname>Rodr?guez-Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Stipcich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Jim?nez-L?pez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jos?</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Ruiz-Mill?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugenio</forename><surname>Mart?nez-C?mara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerardo</forename><surname>Gonz?lez-Seco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Victoria Luz?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><forename type="middle">Angel</forename><surname>Veganzones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="page" from="270" to="292" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Pyvertical: A vertical federated learning framework for multi-headed splitnn</title>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Romanini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">James</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavlos</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Titcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abbas</forename><surname>Ismail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tudor</forename><surname>Cebere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Sandmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Roehm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Hoeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00489</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Towards federated graph learning for collaborative financial crimes detection</title>
		<author>
			<persName><forename type="first">Toyotaro</forename><surname>Suzumura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natahalie</forename><surname>Baracaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangnan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Houck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryo</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><forename type="middle">Larise</forename><surname>Stavarache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Loyola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12946</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gauri</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vincent Poor</surname></persName>
		</author>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7611" to="7623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Chuhan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04925</idno>
		<title level="m">Fedgnn: Federated graph neural network for privacy-preserving recommendation</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Federated graph classification over non-iid graphs</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Yuexiang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daoyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liuyi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weirui</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.05011</idno>
		<title level="m">FederatedScope: A Comprehensive and Flexible Federated Learning Platform via Message Passing</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">How Powerful are Graph Neural Networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Federated learning</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="207" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Automatic Tuning of Federated Learning Hyper-Parameters from System Perspective</title>
		<author>
			<persName><forename type="first">Huanle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasant</forename><surname>Mohapatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Delucia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.03061</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Subgraph federated learning with missing neighbor generation</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siu</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiu</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">PySyft: A Library for Easy Federated Learning</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ziller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Trask</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Lopardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Szymkow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bobby</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Bluemke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Mickael</forename><surname>Nounahon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Passerat-Palmbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kritika</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Federated Learning Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="111" to="139" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
