<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DEEP GRAPH INFOMAX</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
							<email>petar.velickovic@cst.cam.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
						</author>
						<author>
							<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
							<email>pietro.lio@cst.cam.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
							<email>yoshua.bengio@mila.quebec</email>
						</author>
						<author>
							<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
							<email>devon.hjelm@microsoft.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Technology University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Mila -Québec Artificial Intelligence Institute Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Mila -Québec Artificial Intelligence Institute McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Technology University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Mila -Québec Artificial Intelligence Institute</orgName>
								<orgName type="institution">Université de Montréal</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Microsoft Research Mila -Québec Artificial Intelligence Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DEEP GRAPH INFOMAX</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs-both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Generalizing neural networks to graph-structured inputs is one of the current major challenges of machine learning <ref type="bibr" target="#b4">(Bronstein et al., 2017;</ref><ref type="bibr" target="#b16">Hamilton et al., 2017b;</ref><ref type="bibr" target="#b0">Battaglia et al., 2018)</ref>. While significant strides have recently been made, notably with graph convolutional networks <ref type="bibr" target="#b23">(Kipf &amp; Welling, 2016a;</ref><ref type="bibr" target="#b10">Gilmer et al., 2017;</ref><ref type="bibr" target="#b38">Veličković et al., 2018)</ref>, most successful methods use supervised learning, which is often not possible as most graph data in the wild is unlabeled. In addition, it is often desirable to discover novel or interesting structure from large-scale graphs, and as such, unsupervised graph learning is essential for many important tasks.</p><p>Currently, the dominant algorithms for unsupervised representation learning with graph-structured data rely on random walk-based objectives <ref type="bibr" target="#b13">(Grover &amp; Leskovec, 2016;</ref><ref type="bibr" target="#b31">Perozzi et al., 2014;</ref><ref type="bibr" target="#b37">Tang et al., 2015;</ref><ref type="bibr" target="#b15">Hamilton et al., 2017a)</ref>, sometimes further simplified to reconstruct adjacency information <ref type="bibr" target="#b24">(Kipf &amp; Welling, 2016b;</ref><ref type="bibr" target="#b9">Duran &amp; Niepert, 2017)</ref>. The underlying intuition is to train an encoder network so that nodes that are "close" in the input graph are also "close" in the representation space.</p><p>While powerful-and related to traditional metrics such as the personalized PageRank score <ref type="bibr" target="#b21">(Jeh &amp; Widom, 2003)</ref>-random walk methods suffer from known limitations. Most prominently, the random-walk objective is known to over-emphasize proximity information at the expense of structural information <ref type="bibr" target="#b32">(Ribeiro et al., 2017)</ref>, and performance is highly dependent on hyperparameter choice <ref type="bibr" target="#b13">(Grover &amp; Leskovec, 2016;</ref><ref type="bibr" target="#b31">Perozzi et al., 2014)</ref>. Moreover, with the introduction of stronger encoder models based on graph convolutions <ref type="bibr" target="#b10">(Gilmer et al., 2017)</ref>, it is unclear whether randomwalk objectives actually provide any useful signal, as these encoders already enforce an inductive bias that neighboring nodes have similar representations.</p><p>In this work, we propose an alternative objective for unsupervised graph learning that is based upon mutual information, rather than random walks. Recently, scalable estimation of mutual information was made both possible and practical through Mutual Information Neural Estimation <ref type="bibr">(MINE, Belghazi et al., 2018)</ref>, which relies on training a statistics network as a classifier of samples coming from the joint distribution of two random variables and their product of marginals. Following on MINE, <ref type="bibr" target="#b19">Hjelm et al. (2018)</ref> introduced Deep InfoMax (DIM) for learning representations of highdimensional data. DIM trains an encoder model to maximize the mutual information between a high-level "global" representation and "local" parts of the input (such as patches of an image). This encourages the encoder to carry the type of information that is present in all locations (and thus are globally relevant), such as would be the case of a class label. DIM relies heavily on convolutional neural network structure in the context of image data, and to our knowledge, no work has applied mutual information maximization to graph-structured inputs. Here, we adapt ideas from DIM to the graph domain, which can be thought of as having a more general type of structure than the ones captured by convolutional neural networks. In the following sections, we introduce our method called Deep Graph Infomax (DGI). We demonstrate that the representation learned by DGI is consistently competitive on both transductive and inductive classification tasks, often outperforming both supervised and unsupervised strong baselines in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Contrastive methods. An important approach for unsupervised learning of representations is to train an encoder to be contrastive between representations that capture statistical dependencies of interest and those that do not. For example, a contrastive approach may employ a scoring function, training the encoder to increase the score on "real" input (a.k.a, positive examples) and decrease the score on "fake" input (a.k.a., negative samples). Contrastive methods are central to many popular word-embedding methods <ref type="bibr" target="#b6">(Collobert &amp; Weston, 2008;</ref><ref type="bibr" target="#b26">Mnih &amp; Kavukcuoglu, 2013;</ref><ref type="bibr" target="#b25">Mikolov et al., 2013)</ref>, but they are found in many unsupervised algorithms for learning representations of graphstructured input as well. There are many ways to score a representation, but in the graph literature the most common techniques use classification <ref type="bibr" target="#b31">(Perozzi et al., 2014;</ref><ref type="bibr" target="#b13">Grover &amp; Leskovec, 2016;</ref><ref type="bibr" target="#b24">Kipf &amp; Welling, 2016b;</ref><ref type="bibr" target="#b16">Hamilton et al., 2017b)</ref>, though other scoring functions are used <ref type="bibr" target="#b9">(Duran &amp; Niepert, 2017;</ref><ref type="bibr" target="#b2">Bojchevski &amp; Günnemann, 2018)</ref>. DGI is also contrastive in this respect, as our objective is based on classifying local-global pairs and negative-sampled counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sampling strategies.</head><p>A key implementation detail to contrastive methods is how to draw positive and negative samples. The prior work above on unsupervised graph representation learning relies on a local contrastive loss (enforcing proximal nodes to have similar embeddings). Positive samples typically correspond to pairs of nodes that appear together within short random walks in the graph-from a language modelling perspective, effectively treating nodes as words and random walks as sentences. Recent work by <ref type="bibr" target="#b2">Bojchevski &amp; Günnemann (2018)</ref> uses node-anchored sampling as an alternative. The negative sampling for these methods is primarily based on sampling of random pairs, with recent work adapting this approach to use a curriculum-based negative sampling scheme (with progressively "closer" negative examples; <ref type="bibr" target="#b44">Ying et al., 2018a)</ref> or introducing an adversary to select the negative examples <ref type="bibr" target="#b3">(Bose et al., 2018)</ref>.</p><p>Predictive coding. Contrastive predictive coding <ref type="bibr">(CPC, Oord et al., 2018)</ref> is another method for learning deep representations based on mutual information maximization. Like the models above, CPC is also contrastive, in this case using an estimate of the conditional density (in the form of noise contrastive estimation, <ref type="bibr" target="#b14">Gutmann &amp; Hyvärinen, 2010)</ref> as the scoring function. However, unlike our approach, CPC and the graph methods above are all predictive: the contrastive objective effectively trains a predictor between structurally-specified parts of the input (e.g., between neighboring node pairs or between a node and its neighborhood). Our approach differs in that we contrast global / local parts of a graph simultaneously, where the global variable is computed from all local variables.</p><p>To the best of our knowledge, the sole prior works that instead focuses on contrasting "global" and "local" representations on graphs do so via (auto-)encoding objectives on the adjacency matrix <ref type="bibr" target="#b40">(Wang et al., 2016)</ref> and incorporation of community-level constraints into node embeddings <ref type="bibr" target="#b41">(Wang et al., 2017)</ref>. Both methods rely on matrix factorization-style losses and are thus not scalable to larger graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DGI METHODOLOGY</head><p>In this section, we will present the Deep Graph Infomax method in a top-down fashion: starting with an abstract overview of our specific unsupervised learning setup, followed by an exposition of the objective function optimized by our method, and concluding by enumerating all the steps of our procedure in a single-graph setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">GRAPH-BASED UNSUPERVISED LEARNING</head><p>We assume a generic graph-based unsupervised machine learning setup: we are provided with a set of node features, X = { x 1 , x 2 , . . . , x N }, where N is the number of nodes in the graph and x i ∈ R F represents the features of node i. We are also provided with relational information between these nodes in the form of an adjacency matrix, A ∈ R N ×N . While A may consist of arbitrary real numbers (or even arbitrary edge features), in all our experiments we will assume the graphs to be unweighted, i.e. A ij = 1 if there exists an edge i → j in the graph and A ij = 0 otherwise.</p><p>Our objective is to learn an encoder, E :</p><formula xml:id="formula_0">R N ×F × R N ×N → R N ×F , such that E(X, A) = H = { h 1 , h 2 , . . . , h N } represents high-level representations h i ∈ R F</formula><p>for each node i. These representations may then be retrieved and used for downstream tasks, such as node classification.</p><p>Here we will focus on graph convolutional encoders-a flexible class of node embedding architectures, which generate node representations by repeated aggregation over local node neighborhoods <ref type="bibr" target="#b10">(Gilmer et al., 2017)</ref>. A key consequence is that the produced node embeddings, h i , summarize a patch of the graph centered around node i rather than just the node itself. In what follows, we will often refer to h i as patch representations to emphasize this point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LOCAL-GLOBAL MUTUAL INFORMATION MAXIMIZATION</head><p>Our approach to learning the encoder relies on maximizing local mutual information-that is, we seek to obtain node (i.e., local) representations that capture the global information content of the entire graph, represented by a summary vector, s.</p><p>In order to obtain the graph-level summary vectors, s, we leverage a readout function, R : R N ×F → R F , and use it to summarize the obtained patch representations into a graph-level representation; i.e., s = R(E(X, A)).</p><p>As a proxy for maximizing the local mutual information, we employ a discriminator, D : R F × R F → R, such that D( h i , s) represents the probability scores assigned to this patch-summary pair (should be higher for patches contained within the summary).</p><p>Negative samples for D are provided by pairing the summary s from (X, A) with patch representations h j of an alternative graph, ( X, A). In a multi-graph setting, such graphs may be obtained as other elements of a training set. However, for a single graph, an explicit (stochastic) corruption function, C : R N ×F × R N ×N → R M ×F × R M ×M is required to obtain a negative example from the original graph, i.e. ( X, A) = C(X, A). The choice of the negative sampling procedure will govern the specific kinds of structural information that is desirable to be captured as a byproduct of this maximization.</p><p>For the objective, we follow the intuitions from Deep InfoMax (DIM, <ref type="bibr" target="#b19">Hjelm et al., 2018)</ref> and use a noise-contrastive type objective with a standard binary cross-entropy (BCE) loss between the samples from the joint (positive examples) and the product of marginals (negative examples). Following their work, we use the following objective<ref type="foot" target="#foot_0">1</ref> :</p><formula xml:id="formula_1">L = 1 N + M   N i=1 E (X,A) log D h i , s + M j=1 E ( X, A) log 1 − D h j , s   (1)</formula><p>This approach effectively maximizes mutual information between h i and s, based on the Jensen-Shannon divergence<ref type="foot" target="#foot_1">2</ref> between the joint and the product of marginals.</p><p>As all of the derived patch representations are driven to preserve mutual information with the global graph summary, this allows for discovering and preserving similarities on the patch-level-for example, distant nodes with similar structural roles (which are known to be a strong predictor for many node classification tasks; <ref type="bibr" target="#b8">Donnat et al., 2018)</ref>. Note that this is a "reversed" version of the argument given by <ref type="bibr" target="#b19">Hjelm et al. (2018)</ref>: for node classification, our aim is for the patches to establish links to similar patches across the graph, rather than enforcing the summary to contain all of these similarities (however, both of these effects should in principle occur simultaneously).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">THEORETICAL MOTIVATION</head><p>We now provide some intuition that connects the classification error of our discriminator to mutual information maximization on graph representations.</p><p>Lemma 1. Let {X (k) } |X| k=1 be a set of node representations drawn from an empirical probability distribution of graphs, p(X), with finite number of elements, |X|, such that p(X (k) ) = p(X (k ) ) ∀k, k . Let R(•) be a deterministic readout function on graphs and s (k) = R(X (k) ) be the summary vector of the k-th graph, with marginal distribution p( s). The optimal classifier between the joint distribution p(X, s) and the product of marginals p(X)p( s), assuming class balance, has an error rate upper bounded by</p><formula xml:id="formula_2">Err * = 1 2 |X| k=1 p( s (k) ) 2 . This upper bound is achieved if R is injective.</formula><p>Proof. Denote by Q (k) the set of all graphs in the input set that are mapped to s (k) by R, i.e. k) , s (k) ) are drawn from the product of marginals with probability p( s (k) )p(X (k) ), which decomposes into: X ) . As, by definition, X (k) ∈ Q (k) , it holds that ρ (k) ≤ 1. This probability ratio is maximized at 1 when Q (k) = {X (k) }, i.e. when R is injective for X (k) . The probability of drawing any sample of the joint from the product of marginals is then bounded above by</p><formula xml:id="formula_3">Q (k) = {X (j) | R(X (j) ) = s (k) }. As R(•) is deterministic, samples from the joint, (X (</formula><formula xml:id="formula_4">p( s (k) ) s p(X (k) , s) = p( s (k) )p(X (k) | s (k) )p( s (k) ) = p(X (k) ) X ∈Q (k) p(X ) p( s (k) ) 2 (2) For convenience, let ρ (k) = p(X (k) ) X ∈Q (k) p(</formula><formula xml:id="formula_5">|X| k=1 p( s (k) ) 2 . As the probability of drawing (X (k) , s (k) ) from the joint is ρ (k) p( s (k) ) ≥ ρ (k) p( s (k)</formula><p>) 2 , we know that classifying these samples as coming from the joint has a lower error than classifying them as coming from the product of marginals. The error rate of such a classifier is then the probability of drawing a sample from the joint as a sample from product of marginals under the mixture probability, which we can bound by Err ≤ 1 2 |X| k=1 p( s (k) ) 2 , with the upper bound achieved, as above, when R(•) is injective for all elements of {X (k) }.</p><p>It may be useful to note that 1 2|X| ≤ Err * ≤ 1 2 . The first result is obtained via a trivial application of Jensen's inequality, while the other extreme is reached only in the edge case of a constant readout function (when every example from the joint is also an example from the product of marginals, so no classifier performs better than chance). Corollary 1. From now on, assume that the readout function used, R, is injective. Assume the number of allowable states in the space of s, | s|, is greater than or equal to |X|. Then, for s , the optimal summary under the classification error of an optimal classifier between the joint and the product of marginals, it holds that | s | = |X|.</p><p>Proof. By injectivity of R, we know that s = argmin s Err * . As the upper error bound, Err * , is a simple geometric sum, we know that this is minimized when p( s (k) ) is uniform. As R(•) is deterministic, this implies that each potential summary state would need to be used at least once. Combined with the condition | s| ≥ |X|, we conclude that the optimum has | s | = |X|. Theorem 1. s = argmax s MI(X; s), where MI is mutual information.</p><p>Proof. This follows from the fact that the mutual information is invariant under invertible transforms. As | s | = |X| and R is injective, it has an inverse function, R −1 . It follows then that, for any s, MI(X; s) ≤ H(X) = MI(X; X) = MI(X; R(X)) = MI(X; s ), where H is entropy.</p><p>Theorem 1 shows that for finite input sets and suitable deterministic functions, minimizing the classification error in the discriminator can be used to maximize the mutual information between the input and output. However, as was shown in <ref type="bibr" target="#b19">Hjelm et al. (2018)</ref>, this objective alone is not enough to learn useful representations. As in their work, we discriminate between the global summary vector and local high-level representations.</p><formula xml:id="formula_6">Theorem 2. Let X (k) i = { x j } j∈n(X (k) ,i</formula><p>) be the neighborhood of the node i in the k-th graph that collectively maps to its high-level features,</p><formula xml:id="formula_7">h i = E(X (k) i ),</formula><p>where n is the neighborhood function that returns the set of neighborhood indices of node i for graph X (k) , and E is a deterministic encoder function. Let us assume that</p><formula xml:id="formula_8">|X i | = |X| = | s| ≥ | h i |.</formula><p>Then, the h i that minimizes the classification error between p( h i , s) and p( h i )p( s) also maximizes MI(X</p><formula xml:id="formula_9">(k) i ; h i ).</formula><p>Proof. Given our assumption of |X i | = | s|, there exists an inverse X i = R −1 ( s), and therefore h i = E(R −1 ( s)), i.e. there exists a deterministic function (E • R −1 ) mapping s to h i . The optimal classifier between the joint p( h i , s) and the product of marginals p( h i )p( s) then has (by Lemma 1) an error rate upper bound of</p><formula xml:id="formula_10">Err * = 1 2 |X| k=1 p( h (k) i ) 2</formula><p>. Therefore (as in Corollary 1), for the optimal h i , | h i | = |X i |, which by the same arguments as in Theorem 1 maximizes the mutual information between the neighborhood and high-level features, MI(X</p><formula xml:id="formula_11">(k) i ; h i ).</formula><p>This motivates our use of a classifier between samples from the joint and the product of marginals, and using the binary cross-entropy (BCE) loss to optimize this classifier is well-understood in the context of neural network optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">OVERVIEW OF DGI</head><p>Assuming the single-graph setup (i.e., (X, A) provided as input), we will now summarize the steps of the Deep Graph Infomax procedure:</p><p>1. Sample a negative example by using the corruption function: ( X, A) ∼ C(X, A).</p><p>2. Obtain patch representations, h i for the input graph by passing it through the encoder:</p><formula xml:id="formula_12">H = E(X, A) = { h 1 , h 2 , . . . , h N }.</formula><p>3. Obtain patch representations, h j for the negative example by passing it through the encoder:</p><formula xml:id="formula_13">H = E( X, A) = { h 1 , h 2 , .</formula><p>. . , h M }. 4. Summarize the input graph by passing its patch representations through the readout function: s = R(H).</p><p>5. Update parameters of E, R and D by applying gradient descent to maximize Equation <ref type="formula">1</ref>.</p><p>This algorithm is fully summarized by Figure <ref type="figure">1</ref>. </p><formula xml:id="formula_14">x i x j (X, A) ( X, A) h i h j (H, A) ( H, A) E C E s R D D + − Figure 1: A high-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CLASSIFICATION PERFORMANCE</head><p>We have assessed the benefits of the representation learnt by the DGI encoder on a variety of node classification tasks (transductive as well as inductive), obtaining competitive results. In each case, DGI was used to learn patch representations in a fully unsupervised manner, followed by evaluating the node-level classification utility of these representations. This was performed by directly using these representations to train and test a simple linear (logistic regression) classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DATASETS</head><p>We follow the experimental setup described in Kipf &amp; Welling (2016a) and <ref type="bibr" target="#b15">Hamilton et al. (2017a)</ref> on the following benchmark tasks: (1) classifying research papers into topics on the Cora, Citeseer and Pubmed citation networks <ref type="bibr" target="#b34">(Sen et al., 2008)</ref>; (2) predicting the community structure of a social network modeled with Reddit posts; and (3) classifying protein roles within protein-protein interaction (PPI) networks <ref type="bibr" target="#b48">(Zitnik &amp; Leskovec, 2017)</ref>, requiring generalisation to unseen networks.</p><p>Further information on the datasets may be found in Table <ref type="table" target="#tab_0">1</ref> and Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">EXPERIMENTAL SETUP</head><p>For each of three experimental settings (transductive learning, inductive learning on large graphs, and multiple graphs), we employed distinct encoders and corruption functions appropriate to that setting (described below).</p><p>Transductive learning. For the transductive learning tasks (Cora, Citeseer and Pubmed), our encoder is a one-layer Graph Convolutional Network (GCN) model (Kipf &amp; Welling, 2016a), with the following propagation rule:</p><formula xml:id="formula_15">E(X, A) = σ D− 1 2 Â D− 1 2 XΘ<label>(3)</label></formula><p>where Â = A + I N is the adjacency matrix with inserted self-loops and D is its corresponding degree matrix; i.e. Dii = j Âij . For the nonlinearity, σ, we have applied the parametric ReLU</p><formula xml:id="formula_16">h 1 h 2 h 3 s Figure 2:</formula><p>The DGI setup on large graphs (such as Reddit). Summary vectors, s, are obtained by combining several subsampled patch representations, h i (here obtained by sampling three and two neighbors in the first and second level, respectively).</p><p>(PReLU) function <ref type="bibr" target="#b17">(He et al., 2015)</ref>, and Θ ∈ R F ×F is a learnable linear transformation applied to every node, with F = 512 features being computed (specially, F = 256 on Pubmed due to memory limitations).</p><p>The corruption function used in this setting is designed to encourage the representations to properly encode structural similarities of different nodes in the graph; for this purpose, C preserves the original adjacency matrix ( A = A), whereas the corrupted features, X, are obtained by row-wise shuffling of X. That is, the corrupted graph consists of exactly the same nodes as the original graph, but they are located in different places in the graph, and will therefore receive different patch representations. We demonstrate DGI is stable to other choices of corruption functions in Appendix C, but we find those that preserve the graph structure result in the strongest features.</p><p>Inductive learning on large graphs. For inductive learning, we may no longer use the GCN update rule in our encoder (as the learned filters rely on a fixed and known adjacency matrix); instead, we apply the mean-pooling propagation rule, as used by GraphSAGE-GCN <ref type="bibr" target="#b15">(Hamilton et al., 2017a)</ref>:</p><formula xml:id="formula_17">MP(X, A) = D−1 ÂXΘ<label>(4)</label></formula><p>with parameters defined as in Equation <ref type="formula" target="#formula_15">3</ref>. Note that multiplying by D−1 actually performs a normalized sum (hence the mean-pooling). While Equation 4 explicitly specifies the adjacency and degree matrices, they are not needed: identical inductive behaviour may be observed by a constant attention mechanism across the node's neighbors, as used by the Const-GAT model <ref type="bibr" target="#b38">(Veličković et al., 2018)</ref>.</p><p>For Reddit, our encoder is a three-layer mean-pooling model with skip connections <ref type="bibr" target="#b18">(He et al., 2016)</ref>:</p><formula xml:id="formula_18">MP(X, A) = σ (XΘ MP(X, A)) E(X, A) = MP 3 ( MP 2 ( MP 1 (X, A), A), A)<label>(5)</label></formula><p>where is featurewise concatenation (i.e. the central node and its neighborhood are handled separately). We compute F = 512 features in each MP layer, with the PReLU activation for σ.</p><p>Given the large scale of the dataset, it will not fit into GPU memory entirely. Therefore, we use the subsampling approach of <ref type="bibr" target="#b15">Hamilton et al. (2017a)</ref>, where a minibatch of nodes is first selected, and then a subgraph centered around each of them is obtained by sampling node neighborhoods with replacement. Specifically, we sample 10, 10 and 25 neighbors at the first, second and third level, respectively-thus, each subsampled patch has 1 + 10 + 100 + 2500 = 2611 nodes. Only the computations necessary for deriving the central node i's patch representation, h i , are performed. These representations are then used to derive the summary vector, s, for the minibatch (Figure <ref type="figure">2</ref>). We used minibatches of 256 nodes throughout training.</p><p>To define our corruption function in this setting, we use a similar approach as in the transductive tasks, but treat each subsampled patch as a separate graph to be corrupted (i.e., we row-wise shuffle the feature matrices within a subsampled patch). Note that this may very likely cause the central node's features to be swapped out for a sampled neighbor's features, further encouraging diversity in the negative samples. The patch representation obtained in the central node is then submitted to the discriminator.</p><p>Inductive learning on multiple graphs. For the PPI dataset, inspired by previous successful supervised architectures <ref type="bibr" target="#b38">(Veličković et al., 2018)</ref>, our encoder is a three-layer mean-pooling model with dense skip connections <ref type="bibr" target="#b18">(He et al., 2016;</ref><ref type="bibr" target="#b20">Huang et al., 2017)</ref>:</p><formula xml:id="formula_19">H 1 = σ (MP 1 (X, A))<label>(6)</label></formula><formula xml:id="formula_20">H 2 = σ (MP 2 (H 1 + XW skip , A)) (7) E(X, A) = σ (MP 3 (H 2 + H 1 + XW skip , A)) (8)</formula><p>where W skip is a learnable projection matrix, and MP is as defined in Equation <ref type="formula" target="#formula_17">4</ref>. We compute F = 512 features in each MP layer, using the PReLU activation for σ.</p><p>In this multiple-graph setting, we opted to use randomly sampled training graphs as negative examples (i.e., our corruption function simply samples a different graph from the training set). We found this method to be the most stable, considering that over 40% of the nodes have all-zero features in this dataset. To further expand the pool of negative examples, we also apply dropout <ref type="bibr" target="#b35">(Srivastava et al., 2014)</ref> to the input features of the sampled graph. We found it beneficial to standardize the learnt embeddings across the training set prior to providing them to the logistic regression model.</p><p>Readout, discriminator, and additional training details. Across all three experimental settings, we employed identical readout functions and discriminator architectures.</p><p>For the readout function, we a simple averaging of all the nodes' features:</p><formula xml:id="formula_21">R(H) = σ 1 N N i=1 h i (9)</formula><p>where σ is the logistic sigmoid nonlinearity. While we have found this readout to perform the best across all our experiments, we assume that its power will diminish with the increase in graph size, and in those cases, more sophisticated readout architectures such as set2vec <ref type="bibr" target="#b39">(Vinyals et al., 2015)</ref> or DiffPool <ref type="bibr" target="#b45">(Ying et al., 2018b</ref>) are likely to be more appropriate.</p><p>The discriminator scores summary-patch representation pairs by applying a simple bilinear scoring function (similar to the scoring used by Oord et al. ( <ref type="formula">2018</ref>)):</p><formula xml:id="formula_22">D( h i , s) = σ h T i W s (10)</formula><p>Here, W is a learnable scoring matrix and σ is the logistic sigmoid nonlinearity, used to convert scores into probabilities of ( h i , s) being a positive example.</p><p>All models are initialized using Glorot initialization <ref type="bibr" target="#b11">(Glorot &amp; Bengio, 2010)</ref> and trained to maximize the mutual information provided in Equation 1 on the available nodes (all nodes for the transductive, and training nodes only in the inductive setup) using the Adam SGD optimizer <ref type="bibr" target="#b22">(Kingma &amp; Ba, 2014)</ref> with an initial learning rate of 0.001 (specially, 10 −5 on Reddit). On the transductive datasets, we use an early stopping strategy on the observed training loss, with a patience of 20 epochs<ref type="foot" target="#foot_2">3</ref> . On the inductive datasets we train for a fixed number of epochs (150 on Reddit, 20 on PPI).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">RESULTS</head><p>The results of our comparative evaluation experiments are summarized in Table <ref type="table" target="#tab_1">2</ref>.</p><p>For the transductive tasks, we report the mean classification accuracy (with standard deviation) on the test nodes of our method after 50 runs of training (followed by logistic regression), and reuse the metrics already reported in <ref type="bibr" target="#b23">Kipf &amp; Welling (2016a)</ref> for the performance of DeepWalk and GCN, as well as Label Propagation (LP) <ref type="bibr" target="#b47">(Zhu et al., 2003)</ref> and Planetoid <ref type="bibr" target="#b43">(Yang et al., 2016</ref>)-a representative supervised random walk method. Specially, we provide results for training the logistic regression on raw input features, as well as DeepWalk with the input features concatenated. For the inductive tasks, we report the micro-averaged F 1 score on the (unseen) test nodes, averaged after 50 runs of training, and reuse the metrics already reported in <ref type="bibr" target="#b15">Hamilton et al. (2017a)</ref> for the other techniques. Specifically, as our setup is unsupervised, we compare against the unsupervised GraphSAGE approaches. We also provide supervised results for two related architectures-FastGCN <ref type="bibr" target="#b5">(Chen et al., 2018)</ref> and Avg. pooling <ref type="bibr" target="#b46">(Zhang et al., 2018)</ref>.</p><p>Our results demonstrate strong performance being achieved across all five datasets. We particularly note that the DGI approach is competitive with the results reported for the GCN model with the supervised loss, even exceeding its performance on the Cora and Citeseer datasets. We assume that these benefits stem from the fact that, indirectly, the DGI approach allows for every node to have access to structural properties of the entire graph, whereas the supervised GCN is limited to only two-layer neighborhoods (by the extreme sparsity of the training signal and the corresponding threat of overfitting). It should be noted that, while we are capable of outperforming equivalent supervised encoder architectures, our performance still does not surpass the current supervised transductive state of the art (which is held by methods such as GraphSGAN <ref type="bibr" target="#b7">(Ding et al., 2018)</ref>). We further observe that the DGI method successfully outperformed all the competing unsupervised GraphSAGE approaches on the Reddit and PPI datasets-thus verifying the potential of methods based on local mutual information maximization in the inductive node classification domain. Our Reddit results are competitive with the supervised state of the art, whereas on PPI the gap is still large-we believe this can be attributed to the extreme sparsity of available node features (over 40% of the nodes having all-zero features), that our encoder heavily relies on.</p><p>We note that a randomly initialized graph convolutional network may already extract highly useful features and represents a strong baseline-a well-known fact, considering its links to the Weisfeiler- Lehman graph isomorphism test <ref type="bibr" target="#b42">(Weisfeiler &amp; Lehman, 1968)</ref>, that have already been highlighted and analyzed by Kipf &amp; Welling (2016a) and <ref type="bibr" target="#b15">Hamilton et al. (2017a)</ref>. As such, we also provide, as Random-Init, the logistic regression performance on embeddings obtained from a randomly initialized encoder. Besides demonstrating that DGI is able to further improve on this strong baseline, it particularly reveals that, on the inductive datasets, previous random walk-based negative sampling methods may have been ineffective for learning appropriate features for the classification task.</p><p>Lastly, it should be noted that deeper encoders correspond to more pronounced mixing between recovered patch representations, reducing the effective variability of our positive/negative examples' pool. We believe that this is the reason why shallower architectures performed better on some of the datasets. While we cannot say that these trends will hold in general, with the DGI loss function we generally found benefits from employing wider, rather than deeper models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">QUALITATIVE ANALYSIS</head><p>We performed a diverse set of analyses on the embeddings learnt by the DGI algorithm in order to better understand the properties of DGI. We focus our analysis exclusively on the Cora dataset (as it has the smallest number of nodes, significantly aiding clarity).</p><p>A standard set of "evolving" t-SNE plots <ref type="bibr">(Maaten &amp; Hinton, 2008)</ref> of the embeddings is given in Figure <ref type="figure" target="#fig_0">3</ref>. As expected given the quantitative results, the learnt embeddings' 2D projections exhibit discernible clustering in the 2D projected space (especially compared to the raw features and Random-Init), which respects the seven topic classes of Cora. The projection obtains a Silhouette score (Rousseeuw, 1987) of 0.234, which compares favorably with the previous reported score of 0.158 for Embedding Propagation <ref type="bibr" target="#b9">(Duran &amp; Niepert, 2017)</ref>.</p><p>We ran further analyses, revealing insights into DGI's mechanism of learning, isolating biased embedding dimensions for pushing the negative example scores down and using the remainder to encode useful information about positive examples. We leverage these insights to retain competitive performance to the supervised GCN even after half the dimensions are removed from the patch representations provided by the encoder. These-and several other-qualitative and ablation studies can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>We have presented Deep Graph Infomax (DGI), a new approach for learning unsupervised representations on graph-structured data. By leveraging local mutual information maximization across the graph's patch representations, obtained by powerful graph convolutional architectures, we are able to obtain node embeddings that are mindful of the global structural properties of the graph. This enables competitive performance across a variety of both transductive and inductive classification tasks, at times even outperforming relevant supervised architectures. A FURTHER DATASET DETAILS Transductive learning. We utilize three standard citation network benchmark datasets-Cora, Citeseer and Pubmed <ref type="bibr" target="#b34">(Sen et al., 2008)</ref>-and closely follow the transductive experimental setup of <ref type="bibr" target="#b43">Yang et al. (2016)</ref>. In all of these datasets, nodes correspond to documents and edges to (undirected) citations. Node features correspond to elements of a bag-of-words representation of a document. Each node has a class label. We allow for only 20 nodes per class to be used for training-however, honouring the transductive setup, the unsupervised learning algorithm has access to all of the nodes' feature vectors. The predictive power of the learned representations is evaluated on 1000 test nodes.</p><p>Inductive learning on large graphs. We use a large graph dataset (231,443 nodes and 11,606,919 edges) of Reddit posts created during September 2014 (derived and preprocessed as in <ref type="bibr" target="#b15">Hamilton et al. (2017a)</ref>). The objective is to predict the posts' community ("subreddit"), based on the GloVe embeddings of their content and comments <ref type="bibr" target="#b30">(Pennington et al., 2014)</ref>, as well as metrics such as score or number of comments. Posts are linked together in the graph if the same user has commented on both. Reusing the inductive setup of <ref type="bibr" target="#b15">Hamilton et al. (2017a)</ref>, posts made in the first 20 days of the month are used for training, while the remaining posts are used for validation or testing and are invisible to the training algorithm.</p><p>Inductive learning on multiple graphs. We make use of a protein-protein interaction (PPI) dataset that consists of graphs corresponding to different human tissues <ref type="bibr" target="#b48">(Zitnik &amp; Leskovec, 2017)</ref>. The dataset contains 20 graphs for training, 2 for validation and 2 for testing. Critically, testing graphs remain completely unobserved during training. To construct the graphs, we used the preprocessed data provided by <ref type="bibr" target="#b15">Hamilton et al. (2017a)</ref>. Each node has 50 features that are composed of positional gene sets, motif gene sets and immunological signatures. There are 121 labels for each node set from gene ontology, collected from the Molecular Signatures Database <ref type="bibr" target="#b36">(Subramanian et al., 2005)</ref>, and a node can possess several labels simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B FURTHER QUALITATIVE ANALYSIS</head><p>Visualizing discriminator scores. After obtaining the t-SNE visualizations, we turned our attention to the discriminator-and visualized the scores it attached to various nodes, for both the positive and a (randomly sampled) negative example (Figure <ref type="figure" target="#fig_1">4</ref>). From here we can make an interesting observation-within the "clusters" of the learnt embeddings on the positive Cora graph, only a handful of "hot" nodes are selected to receive high discriminator scores. This suggests that there may be a clear distinction between embedding dimensions used for discrimination and classification, which we more thoroughly investigate in the next paragraph. In addition, we may observe that, as expected, the model is unable to find any strong structure within a negative example. Lastly, a few negative examples achieve high discriminator scores-a phenomenon caused by the existence of low-degree nodes in Cora (making the probability of a node ending up in an identical context it had in the positive graph non-negligible).</p><p>Impact and role of embedding dimensions. Guided by the previous result, we have visualized the embeddings for the top-scoring positive and negative examples (Figure <ref type="figure">5</ref>). The analysis revealed existence of distinct dimensions in which both the positive and negative examples are strongly biased. We hypothesize that, given the random shuffling, the average expected activation of a negative example is zero, and therefore strong biases are required to "push" the example down in the discriminator. The positive examples may then use the remaining dimensions to both counteract this bias and encode patch similarity. To substantiate this claim, we order the 512 dimensions based on how distinguishable the positive and negative examples are in them (using p-values obtained from a t-test as a proxy). We then remove these dimensions from the embedding, respecting this order-either starting from the most distinguishable (p ↑) or least distinguishable dimensions (p ↓)-monitoring how this affects both classification and discriminator performance (Figure <ref type="figure">6</ref>). The observed trends largely support our hypothesis: if we start by removing the biased dimensions first (p ↓), the classification performance holds up for much longer (allowing us to remove over half of the embedding dimensions while remaining competitive to the supervised GCN), and the positive examples mostly remain correctly discriminated until well over half the dimensions are removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ROBUSTNESS TO CHOICE OF CORRUPTION FUNCTION</head><p>Here, we consider alternatives to our corruption function, C, used to produce negative graphs. We generally find that, for the node classification task, DGI is stable and robust to different strategies. However, for learning graph features towards other kinds of tasks, the design of appropriate corruption strategies remains an area of open research.</p><p>Our corruption function described in Section 4.2 preserves the original adjacency matrix ( A = A) but corrupts the features, X, via row-wise shuffling of X. In this case, the negative graph is constrained to be isomorphic to the positive graph, which should not have to be mandatory. We can instead produce a negative graph by directly corrupting the adjacency matrix. Therefore, we first consider an alternative corruption function C which preserves the features ( X = X) but instead adds or removes edges from the adjacency matrix ( A = A). This is done by sampling, i.i.d., a switch parameter Σ ij , which determines whether to corrupt the adjacency matrix at position (i, j). Assuming a given corruption rate, ρ, we may define C as performing the following operations:</p><formula xml:id="formula_23">Σ ij ∼ Bernoulli(ρ)<label>(11)</label></formula><formula xml:id="formula_24">A = A ⊕ Σ (<label>12</label></formula><formula xml:id="formula_25">)</formula><p>where ⊕ is the XOR (exclusive OR) operation.</p><p>This alternative strategy produces a negative graph with the same features, but different connectivity.</p><p>Here, the corruption rate of ρ = 0 corresponds to an unchanged adjacency matrix (i.e. the positive and negative graphs are identical in this case). In this regime, learning is impossible for the discriminator, and the performance of DGI is in line with a randomly initialized DGI model. At higher rates of noise, however, DGI produces competitive embeddings.</p><p>We also consider simultaneous feature shuffling ( X = X) and adjacency matrix perturbation ( A = A), both as described before. We find that DGI still learns useful features under this compound corruption strategy-as expected, given that feature shuffling is already equivalent to an (isomorphic) adjacency matrix perturbation.</p><p>From both studies, we may observe that a certain lower bound on the positive graph perturbation rate is required to obtain competitive node embeddings for the classification task on Cora. Furthermore, the features learned for downstream node classification tasks are most powerful when the negative graph has similar levels of connectivity to the positive graph.</p><p>The classification performance peaks when the graph is perturbed to a reasonably high level, but remains sparse; i.e. the mixing between the separate 1-step patches is not substantial, and therefore the pool of negative examples is still diverse enough. Classification performance is impacted only marginally at higher rates of corruption-corresponding to dense negative graphs, and thus a less rich negative example pool-but still considerably outperforming the unsupervised baselines we have considered. This could be seen as further motivation for relying solely on feature shuffling, without adjacency perturbations-given that feature shuffling is a trivial way to guarantee a diverse set of negative examples, without incurring significant computational costs per epoch.</p><p>The results of this study are visualized in Figures <ref type="figure">7 and 8</ref>. Classification accuracy for (X, A) corruption X = X GCN Figure <ref type="figure">8</ref>: DGI is stable and robust under a corruption function that modifies both feature matrix (X = X) and the adjacency matrix ( A = A) on the Cora dataset. Corruption functions that preserve sparsity (ρ ≈ 1 N ) perform the best. However, DGI still performs well even with large disruptions (where edges are added or removed with probabilities approaching 1). N.B. log scale used for ρ.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: t-SNE embeddings of the nodes in the Cora dataset from the raw features (left), features from a randomly initialized DGI model (middle), and a learned DGI model (right). The clusters of the learned DGI model's embeddings are clearly defined, with a Silhouette score of 0.234.</figDesc><graphic url="image-1.png" coords="10,109.98,81.86,130.68,104.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Discriminator scores, D h i , s , attributed to each node in the Cora dataset shown over a t-SNE of the DGI algorithm. Shown for both the original graph (left) and a negative sample (right).</figDesc><graphic url="image-4.png" coords="14,108.00,81.86,198.00,148.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: The learnt embeddings of the highest-scored positive examples (upper half ), and the lowest-scored negative examples (lower half ).</figDesc><graphic url="image-6.png" coords="15,193.20,105.63,212.75,176.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>10 − 6 Figure 7 :</head><label>67</label><figDesc>Figure 7: DGI also works under a corruption function that modifies only the adjacency matrix ( A = A) on the Cora dataset. The left range (ρ → 0) corresponds to no modifications of the adjacency matrix-therein, performance approaches that of the randomly initialized DGI model. As ρ increases, DGI produces more useful features, but ultimately fails to outperform the featureshuffling corruption function. N.B. log scale used for ρ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>level overview of Deep Graph Infomax. Refer to Section 3.4 for more details. Summary of the datasets used in our experiments.</figDesc><table><row><cell>Dataset</cell><cell>Task</cell><cell>Nodes</cell><cell>Edges</cell><cell>Features</cell><cell>Classes</cell><cell>Train/Val/Test Nodes</cell></row><row><cell cols="2">Cora Citeseer Transductive Transductive Pubmed Transductive Reddit Inductive PPI Inductive</cell><cell>2,708 3,327 19,717 231,443 56,944 (24 graphs)</cell><cell>5,429 4,732 44,338 11,606,919 818,716</cell><cell>1,433 3,703 500 602 50</cell><cell>7 6 3 41 121 (multilbl.)</cell><cell>140/500/1,000 120/500/1,000 60/500/1,000 151,708/23,699/55,334 44,906/6,514/5,524 (20/2/2 graphs)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Summary of results in terms of classification accuracies (on transductive tasks) or microaveraged F 1 scores (on inductive tasks). In the first column, we highlight the kind of data available to each method during training (X: features, A: adjacency matrix, Y: labels). "GCN" corresponds to a two-layer DGI encoder trained in a supervised manner.</figDesc><table><row><cell></cell><cell cols="2">Transductive</cell><cell></cell></row><row><cell cols="2">Available data Method</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell></row><row><cell>X A, Y A X, A X, A X, A X, A, Y X, A, Y</cell><cell cols="4">Raw features LP (Zhu et al., 2003) DeepWalk (Perozzi et al., 2014) 67.2% 47.9 ± 0.4% 49.3 ± 0.2% 69.1 ± 0.3% 68.0% 45.3% 63.0% 43.2% 65.3% DeepWalk + features 70.7 ± 0.6% 51.4 ± 0.5% 74.3 ± 0.9% Random-Init (ours) 69.3 ± 1.4% 61.9 ± 1.6% 69.6 ± 1.9% DGI (ours) 82.3 ± 0.6% 71.8 ± 0.7% 76.8 ± 0.6% GCN (Kipf &amp; Welling, 2016a) 81.5% 70.3% 79.0% Planetoid (Yang et al., 2016) 75.7% 64.7% 77.2%</cell></row><row><cell></cell><cell cols="2">Inductive</cell><cell></cell></row><row><cell cols="2">Available data Method</cell><cell></cell><cell>Reddit</cell><cell>PPI</cell></row><row><cell>X A X, A</cell><cell>Raw features DeepWalk (Perozzi et al., 2014) DeepWalk + features</cell><cell></cell><cell>0.585 0.324 0.691</cell><cell>0.422 --</cell></row><row><cell>X, A X, A X, A X, A</cell><cell cols="3">GraphSAGE-GCN (Hamilton et al., 2017a) GraphSAGE-mean (Hamilton et al., 2017a) GraphSAGE-LSTM (Hamilton et al., 2017a) 0.907 0.908 0.897 GraphSAGE-pool (Hamilton et al., 2017a) 0.892</cell><cell>0.465 0.486 0.482 0.502</cell></row><row><cell>X, A X, A X, A, Y X, A, Y</cell><cell>Random-Init (ours) DGI (ours) FastGCN (Chen et al., 2018) Avg. pooling (Zhang et al., 2018)</cell><cell></cell><cell cols="2">0.933 ± 0.001 0.626 ± 0.002 0.940 ± 0.001 0.638 ± 0.002 0.937 -0.958 ± 0.001 0.969 ± 0.002</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Note that<ref type="bibr" target="#b19">Hjelm et al. (2018)</ref> use a softplus version of the binary cross-entropy.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">The "GAN" distance defined here-as per<ref type="bibr" target="#b12">Goodfellow et al. (2014)</ref> and<ref type="bibr" target="#b27">Nowozin et al. (2016)</ref>-and Jensen-Shannon divergence can be related by DGAN = 2DJS − log 4. Therefore, any parameters that optimize one also optimize the other.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">A reference DGI implementation may be found at https://github.com/PetarV-/DGI.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank the developers of PyTorch <ref type="bibr" target="#b29">(Paszke et al., 2017)</ref>. PV and PL have received funding from the European Union's Horizon 2020 research and innovation programme PROPAG-AGEING under grant agreement No 634821. We specially thank Hugo Larochelle and Jian Tang for the extremely useful discussions, and Andreea Deac, Arantxa Casanova, Ben Poole, Graham Taylor, Guillem Cucurull, Justin Gilmer, Nithium Thain and Zhaocheng Zhu for reviewing the paper prior to submission.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Peter W Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Ishmael</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04062</idno>
		<title level="m">Mine: mutual information neural estimation</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1ZdKJ-0W" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adversarial contrastive estimation</title>
		<author>
			<persName><forename type="first">Avishek</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanshuai</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10247</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
				<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semi-supervised learning on graphs with generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00130</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning structural node embeddings via diffusion wavelets</title>
		<author>
			<persName><forename type="first">Claire</forename><surname>Donnat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Hallac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International ACM Conference on Knowledge Discovery and Data Mining</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning graph representations with embedding propagation</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duran</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5119" to="5130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01212</idno>
		<title level="m">Neural message passing for quantum chemistry</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
				<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
				<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017a</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<title level="m">Representation learning on graphs: Methods and applications</title>
				<imprint>
			<date type="published" when="2017">2017b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scaling personalized web search</title>
		<author>
			<persName><forename type="first">Glen</forename><surname>Jeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on the World Wide Web</title>
				<meeting>the 12th International Conference on the World Wide Web</meeting>
		<imprint>
			<publisher>Acm</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2016b. Nov. 2008</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Variational graph auto-encoders</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2265" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryota</forename><surname>Tomioka. F-Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing</title>
				<meeting>the 2014 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning node representations from structural identity</title>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">Hp</forename><surname>Leonardo Fr Ribeiro</surname></persName>
		</author>
		<author>
			<persName><surname>Saverese</surname></persName>
		</author>
		<author>
			<persName><surname>Daniel R Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Silhouettes: a graphical aid to the interpretation and validation of cluster analysis</title>
		<author>
			<persName><forename type="first">Rousseeuw</forename><surname>Peter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="53" to="65" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles</title>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Tamayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sayan</forename><surname>Vamsi K Mootha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">L</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Gillette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">L</forename><surname>Paulovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><forename type="middle">R</forename><surname>Pomeroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">S</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><surname>Lander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">43</biblScope>
			<biblScope unit="page" from="15545" to="15550" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Line: Largescale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
				<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJXMpikCZ" />
		<title level="m">Graph Attention Networks. International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06391</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Community preserving network embedding</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="203" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A reduction of a graph to a canonical form and an algebra arising during this reduction</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName><surname>Lehman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nauchno-Technicheskaya Informatsia</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="12" to="16" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08861</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01973</idno>
		<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08804</idno>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07294</idno>
		<title level="m">Gaan: Gated attention networks for learning on large and spatiotemporal graphs</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International conference on Machine learning (ICML-03)</title>
				<meeting>the 20th International conference on Machine learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Predicting multicellular function through multi-layer tissue networks</title>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="190" to="198" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
