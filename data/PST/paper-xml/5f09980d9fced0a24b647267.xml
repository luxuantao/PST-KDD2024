<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">he rapid development of wearable and mobile devices has facilitated the application of Internet of Things (IoT)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Xiaokang</forename><surname>Zhou</surname></persName>
							<email>zhou@biwako.shiga-u.ac.jp</email>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Laurence</forename><forename type="middle">T</forename><surname>Yang</surname></persName>
							<email>ltyang@ieee.org</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Wei</forename><surname>Liang</surname></persName>
							<email>weiliang@csu.edu.cn</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Kevin I-Kai</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Hao</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Data Science</orgName>
								<orgName type="institution">Shiga University</orgName>
								<address>
									<settlement>Hikone</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">RIKEN Center for Advanced Intelligence Project</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">and Business School</orgName>
								<orgName type="laboratory">Key Laboratory of Hunan Province for New Retail Virtual Reality Technology</orgName>
								<orgName type="institution" key="instit1">Hunan University of Technology and Business</orgName>
								<orgName type="institution" key="instit2">Central South University</orgName>
								<address>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Electrical, Computer and Software Engineering</orgName>
								<orgName type="institution">The University of Auckland</orgName>
								<address>
									<settlement>Auckland</settlement>
									<country key="NZ">New Zealand</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Norwegian University of Science &amp; Technology</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Xavier University</orgName>
								<address>
									<settlement>St, Antigonish</settlement>
									<region>Francis</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Faculty of Human Sciences</orgName>
								<orgName type="institution">Waseda University</orgName>
								<address>
									<settlement>Tokorozawa</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department" key="dep1">Department of Human Informatics and Cognitive Sciences</orgName>
								<orgName type="department" key="dep2">Faculty of Human Sciences</orgName>
								<orgName type="institution">Waseda University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">he rapid development of wearable and mobile devices has facilitated the application of Internet of Things (IoT)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8CB3490F6DBBFE687F90CCC50828555C</idno>
					<idno type="DOI">10.1109/JIOT.2020.2985082</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/JIOT.2020.2985082, IEEE Internet of Things Journal</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Human Activity Recognition</term>
					<term>Deep Learning</term>
					<term>Reinforcement Learning</term>
					<term>Internet of Things</term>
					<term>Weakly Labeled Data</term>
					<term>Smart Healthcare</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Along with the advancement of several emerging computing paradigms and technologies, such as cloud computing, mobile computing, artificial intelligence, and big data, Internet of Things (IoT) technologies have been applied in a variety of fields. In particular, the Internet of Healthcare Things (IoHT) is becoming increasingly important in Human Activity Recognition (HAR), due to the rapid development of wearable and mobile devices. In this study, we focus on the deep learning enhanced HAR in IoHT environments. A semi-supervised deep learning framework is designed and built for more accurate HAR, which efficiently use and analyze the weakly labeled sensor data to train the classifier learning model. To better solve the problem of inadequately labeled sample, an intelligent auto-labeling scheme based on Deep Q-Network (DQN) is developed with a newly designed distance-based reward rule, which can improve the learning efficiency in IoT environments. A multi-sensor based data fusion mechanism is then developed to seamlessly integrate the onbody sensor data, context sensor data, and personal profile data together, and a Long Short Term Memory (LSTM)-based classification method is proposed to identify fine-grained patterns according to the high-level features contextually extracted from sequential motion data. Finally, experiments and evaluations are conducted to demonstrate the usefulness and effectiveness of the proposed method using real world data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>technologies in healthcare field. Monitoring real time human activities, especially Activities of Daily Living (ADL) of elder people, is an essential issue in smart healthcare, which can evidently enhance the medical rehabilitation and elderly care using wearable and mobile sensors. Accordingly, Human Activity Recognition (HAR) in ubiquitous computing environments, has become a hotly discussed topic in better understanding people's daily behaviors and interactions with their living environments, which is studied extensively for the so-called Internet of Healthcare Things (IoHT) <ref type="bibr" target="#b0">[1]</ref>. Different kinds of wearable devices (e.g., body-worn inertial sensors, and smartphone) can be placed on different on-body locations (e.g., head, chest, upper arm, forearm, shin, etc.) to collect and transfer real time posture data (e.g., accelerometer and gyroscope) using wireless sensor networks <ref type="bibr" target="#b1">[2]</ref>. These sensor technologies provide us opportunities to improve the robustness of multi-modal data sensing and fusion in HAR, which may support the development of human-centric applications and services in cyber-physical-social systems based on the enrichment of sensed information from real-time big data environments <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Since the motion data based on people's physical activities can be easily obtained via built-in inertial sensors of wearable and mobile devices, activity recognition and classification using smart devices are widely used to monitor, analyze, and understand one person's status in different scenes across a variety of applications and systems <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. However, there are still several challenges in HAR via sensor data. For example, the effectiveness of traditional machine learning based methods mainly rely on the availability of training data, which means feature extraction mostly depends on a well-designed dataset with adequate labeling <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. However, the ADL data is continuously generated by the built-in sensors of smart devices, regardless of whether people are performing sensible actions, which results in a labor intensive process of annotating and recording well-labeled data. This kind of wearable sensor data, namely, massive amount of unlabeled data combined with small portion of labeled data, can be referred to as weakly labeled motion data <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. It is necessary to build a new semisupervised learning or weakly supervised learning framework, to efficiently deal with such kind of situation.</p><p>Actually, it is obvious that activity recognition in real world is depended on the subjects and where the wearable device is worn, and signals obtained by on-body sensors is up to the 2 positions of them. Different positions of on-body sensors can generate different motion patterns even for the same activity performed by one person. Existing works are able to recognize coarse-grained behaviors from repetitive movements (e.g., walking, running), static actions (e.g., sitting, standing), or simple transitional activities (e.g., stand-sit, sit-lie) with relatively high accuracy, but face challenges to identify some complex patterns with single sensor data <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. Sensor data collected from multiple wearable devices, including accelerometer, GPS, light, video, etc., needs to be synchronized and integrated together through a unified data fusion strategy, to capture more complex human activity patterns from a multimodal and multi-positional view. In addition, with the same onbody sensors, different subjects may produce different motion patterns for the same type of activity. For instance, the falling pattern of an elder person and a child can be entirely different. Moreover, traditional approaches on feature extraction for HAR may only figure out low-level features, which could be enough to recognize basic physical or postural activities. But without considerations of some context-aware or location-aware issues, it would be a challenge to detect more meaningful actions with semantic information, such as jogging, cooking, etc. Therefore, it is essential to find an efficient way to extract the high-level features within a certain context, which may facilitate pattern recognitions in terms of the fine-grained human actions, gestures, and expressions.</p><p>In this study, we focus on the deep learning enhanced activity recognition, in order to detect the fine-grained motion patterns through better utilization of weakly labeled sensor data. Considering the multiple sensor data obtained by various wearable devices, a semi-supervised learning framework is introduced to incorporate massive amount of unlabeled data with small portion of labeled data to enhance the accuracy of HAR in IoHT environments. Specifically, comparing with the existed related researches, contributions of our study can be concluded as follows.</p><p>i) A semi-supervised deep learning framework is designed and constructed with an auto-labeling module and a Long Short Term Memory (LSTM)-based classification module, which can efficiently utilize the massive amount of weakly labeled data to train the classifier, and improve the accuracy of HAR in IoHT environments. ii) An intelligent auto-labeling scheme based on Deep Q-Network (DQN) is developed with a novel distancebased reward rule, which can better solve the problem of inadequately labeled sample, and improve the learning efficiency. iii) An LSTM-based classifier is built with a multi-sensor based data fusion mechanism, which can be used to deal with the sequential motion data and detect fine-grained patterns according to the extracted high-level features. The rest of this article is organized as follows. Section II presents an overview of related works. The framework of semisupervised deep learning model is presented in Section III. Algorithm and mechanism for auto-labeling from weakly labeled data and fine-grained activity recognition are discussed in Section IV. In Section V, we discuss the experiment and evaluation results using two real world datasets. In Section VI, we conclude this research and give a promising perspective on future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The study of HAR is an important research direction in the field of IoHT. Research works in this direction are applied in various practical applications such as healthcare, gym physical activity recognition, fall detection, etc. Several issues relating to this topic are discussed in this section. Foremost, studies on HAR, issues of analysis based on multiple motion data, and researches on semi-supervised learning for HAR, are addressed respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Studies on HAR</head><p>HAR can be viewed as one kind of artificial intelligent technology, which analyzes and recognizes human activities and behavior patterns automatically through a series of observations of wearable device data. Generally, Turaga et al. <ref type="bibr" target="#b6">[7]</ref> divided HAR into three levels as: the movement recognition, action recognition and activity recognition, which were referred to as the low-level vision <ref type="bibr" target="#b12">[13]</ref>, middle-level vision <ref type="bibr" target="#b13">[14]</ref>, and high-level vision respectively <ref type="bibr" target="#b14">[15]</ref>. Different machine learning and deep learning based schemes were explored to handle issues in HAR and achieved effective performance <ref type="bibr" target="#b15">[16]</ref>.</p><p>Recently, studies focusing on HAR could be classified into two important categories as ambient sensor based and wearable sensor based approaches. Ambient sensor based approaches usually deployed surveillance camera, sound, temperature and other indoor sensors to capture environment-related context signals, and recognize people's daily activities in the fixed space (e.g., smart home <ref type="bibr" target="#b16">[17]</ref>, recovery center <ref type="bibr" target="#b17">[18]</ref>). The requirement of fixed environments made them not applicable for analyzing normal outdoor activities. On the other hand, wearable sensor based approach utilized wearable devices or smartphones to monitor and obtain on-body physiological signals with accelerometer, magnetometer and gyroscope sensors <ref type="bibr" target="#b18">[19]</ref>. For example, Lee and Mase <ref type="bibr" target="#b19">[20]</ref> used the acceleration and angular velocity sensor data measured by inexpensive wearable devices, to determine the users' location information and identify their sitting, standing and walking behaviors. Mantyjarvi et al. <ref type="bibr" target="#b20">[21]</ref> utilized the independent component analysis and principal component analysis schemes to recognize one person's walking posture based on the acceleration data collected from the buttocks. These results demonstrated that activity recognition techniques based on wearable sensors could work effectively for the low-level vision, but failed to handle high-level recognition tasks for complex activity recognition <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Analysis Based on Multiple Motion Data</head><p>Typically, lots of studies focused on human activity recognition with the accelerometer sensor data. Bao and Intille <ref type="bibr" target="#b22">[23]</ref> developed algorithms to detect physical activities from subjects' daily tasks using multiple accelerometers. Kern et al. <ref type="bibr" target="#b23">[24]</ref> placed a three-axis accelerometer on different parts of the user's body, where each accelerometer could reflect different directions and motions of relevant parts. However, these approaches were not easy to achieve a higher performance because of the constraint of single data source without enough context information. Lukowicz et al. <ref type="bibr" target="#b24">[25]</ref> used gyroscopes and accelerometers to measure the acceleration and angular velocity. But the recognition accuracy was affected by the limited experimental conditions, especially by the noisy data and sensor variations. Patterson et. al. <ref type="bibr" target="#b25">[26]</ref> presented a Bayesian model with traveler's moving data drawn from the GPS sensor stream, to learn travelers' transportation mode and their most likely route in an unsupervised manner. They demonstrated that the recognition accuracy could be improved by adding external context knowledge about bus routes and bus stops. Liao et al. <ref type="bibr" target="#b26">[27]</ref> introduced a hierarchical Markov model to infer one user's daily movements by integrating raw GPS sensor measurements with high-level information (e.g., user's destination, mode of transportation), which was applied to help people use the public transportation more safely. In particular, several existing HAR methods explored hybrid classifiers to improve recognition performance <ref type="bibr" target="#b27">[28]</ref>, which were usually trained and combined multiple classifiers from different sensor data according to their corresponding feature patterns, in IoT and network computing environments <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>. However, these methods had limited considerations of unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Semi-Supervised Learning for HAR</head><p>Supervised learning model was widely implemented for HAR, in which the collected labeled data was used to train the model. However, most data collected by wearable devices was unlabeled, which led to a tedious and costly work of annotating those unlabeled data. This has become a difficult issue for HAR in IoHT environment <ref type="bibr" target="#b30">[31]</ref>.</p><p>Semi-supervised learning is a learning approach that combines supervised learning with unsupervised learning techniques, which is studied for human activity recognition extensively. It can incorporate the large portion of unlabeled data with labeled data to solve the problem of inadequate annotation for human activities. Motivated by the classification problem of web pages, Blum et al. <ref type="bibr" target="#b31">[32]</ref> proposed a co-training framework to leverage the unlabeled data to enlarge the training set, and improve the performance of the recognition algorithm. Zhou et al. <ref type="bibr" target="#b32">[33]</ref> proposed a so-called disagreement-based semisupervised learning paradigm, in which multiple classifiers were trained from real-world tasks, and disagreements among the classifiers were exploited to guide the semi-supervised learning process. Zhu et al. <ref type="bibr" target="#b8">[9]</ref> proposed a semi-supervised deep learning approach, in which the temporal ensemble of deep LSTM was developed to recognize human activities with labeled and unlabeled smartphone inertial sensor data. The unsupervised losses were leveraged and combined together with the supervised losses for the accurate HAR. To investigate unobtrusive and context-aware activity recognition using onbody wearable sensors, Stikic et al. <ref type="bibr" target="#b33">[34]</ref> proposed an annotation strategy which leveraged sparsely labeled data together with more easily obtainable unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. FRAMEWORK OF SEMI-SUPERVISED DEEP LEARNING</head><p>In this section, after introducing the formal description of HAR problem in IoHT environments, we present the design of a semi-supervised deep learning framework with its core function modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Definition</head><p>HAR for daily living in the IoHT environment can be viewed as a time-dependent task since the data is generated continuously via wearable devices. The main challenge in designing the classifier is how to reasonably segment this time stream data, which is also an essential step to extract features for each activity. Recurrent Neural Network (RNN) <ref type="bibr" target="#b34">[35]</ref> is one kind of classic supervised learning model, which can utilize the internal state memory to record the temporal information between layers of a neural network. This allows to process the unsegmented and temporal sequence data for applicable tasks in real world, such as nature language process, handwriting recognition, speech recognition, and HAR, etc. In particular, LSTM model is a special kind of recurrent network that incorporates some gates and memory cells into the network, in order to capture the long-term dependences of the sequence data. In this study, we utilize the LSTM model in the constructed deep learning framework, to detect the temporal features of segmented motion activities.</p><p>As we discussed earlier, labeling for the accelerometer and gyroscope data generated by wearable devices is a costly and labor-intensive task, which indicates the importance of using weakly labeled data to design a practical classifier for HAR. However, it may result in unfavorable results if we simply apply the traditional LSTM model to deal with such kind of weakly labeled data in HAR, because it contains massive unlabeled data mixed with only a small portion of labeled data. Therefore, a semi-supervised deep learning framework is designed to handle this situation.</p><p>Given the HAR problem in IoHT environments with a time series of unlabeled and labeled data uniformly indicated as ğ‘‹ ğ‘¡ with the timestamp t, obtained by different kinds of on-body wearable devices, the definition of labeled data can be expressed as follows.</p><p>ğ¿ğµ(ğ‘‹_ğ‘™ğ‘ ğ‘¡ , ğ‘Œ_ğ‘™ğ‘ ğ‘¡ , ğ‘†, ğ·) (1) Where ğ‘‹_ğ‘™ğ‘ ğ‘¡ = {ğ‘¥_ğ‘™ğ‘ ğ‘¡ ğ‘› |ğ‘› âˆˆ {1,2, â€¦ , ğ‘}} denotes the labeled dataset, ğ‘‹_ğ‘™ğ‘ ğ‘¡ âŠ‚ ğ‘‹ ğ‘¡ , and ğ‘Œ_ğ‘™ğ‘ ğ‘¡ = {ğ‘¦_ğ‘™ğ‘ ğ‘¡ ğ‘› |ğ‘› âˆˆ {1,2, â€¦ , ğ‘}} denotes their corresponding label set, ğ‘ is the number of the labeled data. ğ‘† = {ğ‘  1 , ğ‘  2 , â€¦ , ğ‘  â„ } denotes the subject set, and ğ· = {ğ‘‘ 1 , ğ‘‘ 2 , â€¦ , ğ‘‘ ğ‘“ } stands for the set of wearable devices used by the subjects, in which each device ğ‘‘ ğ‘˜ stands for a typical onbody devices used by a subject ğ‘  ğ‘– .</p><p>Likewise, the definitions of unlabeled data can be expressed as follows.</p><p>ğ‘ˆğ¿ğµ(ğ‘‹_ğ‘¢ğ‘™ğ‘ ğ‘¡ , ğ‘†, ğ·)</p><p>(2) where ğ‘‹_ğ‘¢ğ‘™ğ‘ ğ‘¡ = {ğ‘¥_ğ‘¢ğ‘™ğ‘ ğ‘¡ ğ‘š |ğ‘š âˆˆ {1,2, â€¦ , ğ‘€}} denotes the unlabeled dataset, ğ‘‹_ğ‘¢ğ‘™ğ‘ ğ‘¡ âŠ‚ ğ‘‹ ğ‘¡ . Note that we assume ğ‘ â‰ª ğ‘€ in this study.</p><p>Thus, given a set of test data ğ‘‹ ğ‘¡ â€² âŠ‚ ğ‘‹ ğ‘¡ in a certain temporal sequence, the problem studied here is to detect the corresponding activities implicated in ğ‘‹ ğ‘¡ â€² , i.e., recognize the activity annotation ğ‘Œ ğ‘¡ â€² for ğ‘‹ ğ‘¡ â€² based on the fine-grained feature extraction and classification in a context-aware way. Furthermore, to address the detailed attributes of each subject, a four-dimension tuple is defined to describe their profile as follows.</p><p>ğ‘ƒğ‘Ÿğ‘œ ğ‘– = (ğ‘”ğ‘‘ ğ‘– , ğ‘ğ‘” ğ‘– , â„ğ‘¡ ğ‘– , ğ‘¤ğ‘¡ ğ‘– )</p><p>(3) where ğ‘”ğ‘‘ ğ‘– , ğ‘ğ‘” ğ‘– , â„ğ‘¡ ğ‘– , and ğ‘¤ğ‘¡ ğ‘– denote a specific subject's gender, age, height, and weight respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Framework Overview</head><p>Following definitions we introduced above, a semisupervised deep learning framework is designed and constructed to build the fine-grained classifier for HAR in IoHT environments, which is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, the whole framework is mainly composed of two basic modules, namely, the auto-labeling module and the LSTM-based classification module. More precisely, multiple types of data, including the on-body sensor data, context sensor data, and personal profile data, are selected as the input to the auto labeling and classification module. The data cleaning process is performed firstly to handle the incomplete, incorrect, or irrelevant parts of the input data, after which the pre-processed data is sent to the auto-labeling module.</p><p>In the auto-labeling module, a reinforcement learning based auto-segmentation and labeling process is applied before sending the weakly labeled input data to the LSTM neural network. Given ğ‘‹ ğ‘¡ , the goal of the auto labeling module is to convert all the samples in ğ‘ˆğ¿ğµ to ğ¿ğµ . Specifically, assuming the state of an input sample data ğ‘¥ ğ‘¡ at timestamp ğ‘¡ is ğ‘ ğ‘¡ ğ‘¡ , an agent is designed to conduct a possible action ğ‘ğ‘ğ‘¡ ğ‘¡ to assign a label ğ‘¦ ğ‘¡ for ğ‘¥ ğ‘¡ . The core idea of the auto-labeling is to assign rewards for actions conducted by the agent. In this study, a distance-based reward rule is designed to enable the agent to choose a correct action with more confidence. Each time when the agent succeeded in predicting a correct label, it will receive a positive reward 1 * ğ›¾. In contrast, if the agent fails to predict the correct label, it will receive a penalty of -1 * ğ›¾, where ğ›¾ is the distance-based weight parameter.</p><p>In the classification module, a multi-sensor based data fusion technique is developed to integrate multiple on-body sensor data together to handle complex activity recognition tasks. Considering the case that detecting falling patterns of an elder and a child based on their motions respectively, fusion of multiple sensor data captured from different body positions may provide enriched motion information to distinguish this similar pattern recognition. Furthermore, a ğ¾ layer LSTM based deep learning network is adopted to extract the high-level features based on the fused multi-sensor data. Given a set of weakly labeled input data ğ‘‹ ğ‘¡ , the core components of LSTM can be expressed as follows.</p><formula xml:id="formula_0">ğ‘“ ğ‘¡ = ğœ(ğ‘Š ğ‘“ â€¢ [â„ ğ‘¡-1 , ğ‘‹ ğ‘¡ ] + ğ‘ ğ‘“ ) ğ‘– ğ‘¡ = ğœ(ğ‘Š ğ‘– â€¢ [â„ ğ‘¡-1 , ğ‘‹ ğ‘¡ ] + ğ‘ ğ‘– ) ğ¶ Ìƒğ‘¡ = tanh(ğ‘Š ğ¶ â€¢ [â„ ğ‘¡-1 , ğ‘‹ ğ‘¡ ] + ğ‘ ğ¶ ) ğ¶ ğ‘¡ = ğ‘“ ğ‘¡ * ğ¶ ğ‘¡-1 + ğ‘– ğ‘¡ * ğ¶ Ìƒğ‘¡ ğ‘œ ğ‘¡ = ğœ(ğ‘Š ğ‘œ â€¢ [â„ ğ‘¡-1 , ğ‘‹ ğ‘¡ ] + ğ‘ ğ‘œ ) â„ ğ‘¡ = ğ‘œ ğ‘¡ â€¢ tanh (ğ¶ ğ‘¡ )<label>(4</label></formula><p>) where ğ‘“ ğ‘¡ , ğ‘– ğ‘¡ , ğ‘œ ğ‘¡ , ğ¶ Ìƒğ‘¡, ğ¶ ğ‘¡ , â„ ğ‘¡ stand for the forget gate, input gate, output gate, two memory cells for state persistence, and the hidden layer respectively. ğœ( * ) is a sigmoid activation function to introduce nonlinear variations to the network. ğ‘Š ğ‘“ , ğ‘Š ğ‘– , ğ‘Š ğ¶ , ğ‘Š ğ‘œ are the weights for each component, and ğ‘ ğ‘“ , ğ‘ ğ‘– , ğ‘ ğ¶ , ğ‘ ğ‘œ are the biases.</p><p>In addition, to avoid the overfitting issue and enhance the generalization for the deep learning network, the Gaussian noise is included in the classification module, and the dropout component with probabilities ğ‘ ğ‘‘ğ‘Ÿğ‘œğ‘ is applied in the LSTM network. The SoftMax function is utilized to handle the multiclassification situation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. MECHANISM FOR SEMI-SUPERVISED HUMAN ACTIVITY RECOGNITION</head><p>In this section, we discuss the auto-labeling propagation process incorporating the reinforcement learning model to resolve the HAR problem with weakly labeled data. A DQNbased auto-labeling scheme is introduced, a multi-sensor based data fusion mechanism, and a LSTM-based fine-grained classifier for HAR are developed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Reinforcement Learning Based Auto-Labeling Scheme</head><p>Considering the challenge of HAR based on weakly labeled data in IoHT environments, we develop an intelligent autolabeling scheme based on reinforcement learning model to find an optimal strategy that can maximize the reward during the labeling process. DQN, which is a combination of Q-learning and deep neural network, is capable of solving the complex problem with uncertainty.</p><p>Basically, the auto-labeling scheme based on reinforcement learning, can be described according to an agent and its triple (ğ‘ ğ‘¡ ğ‘¡ , ğ‘ğ‘ğ‘¡ ğ‘¡ , ğ‘Ÿğ‘¤ğ‘‘ ğ‘¡ ) at timestamp ğ‘¡ , in which ğ‘ ğ‘¡ ğ‘¡ denotes the current agent's state at ğ‘¡, ğ‘ğ‘ğ‘¡ ğ‘¡ denotes a current action adopted by the learning model, and ğ‘Ÿğ‘¤ğ‘‘ ğ‘¡ denotes the reward determined by ğ‘ ğ‘¡ ğ‘¡ and ğ‘ğ‘ğ‘¡ ğ‘¡ . Additionally, to describe the autolabeling process, a reward function ğ‘…(ğ‘ ğ‘¡ ğ‘¡ , ğ‘ğ‘ğ‘¡ ğ‘¡ , ğ›¾) is defined to evaluate the mapping from state to action, which can be expressed as Eq. ( <ref type="formula">5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ğ‘…( ğ‘ ğ‘¡ ğ‘¡ , ğ‘ğ‘ğ‘¡ ğ‘¡ , ğ›¾) = ğ¸[ğ‘Ÿğ‘¤ğ‘‘ ğ‘¡+1 + ğ›¾ ğ‘„ ğœ‹ ( ğ‘ ğ‘¡ ğ‘¡ , ğ‘ğ‘ğ‘¡ ğ‘¡ , ğœƒ)| ğ‘ ğ‘¡ ğ‘¡ , ğ‘ğ‘ğ‘¡ ğ‘¡ ] (5)</head><p>The Q-function ğ‘„ ğœ‹ ( ğ‘ ğ‘¡ ğ‘¡ , ğ‘ğ‘ğ‘¡ ğ‘¡ , ğœƒ) is introduced to evaluate the cumulative reward expectation gained by the agent according to ğ‘ ğ‘¡ ğ‘¡ and ğ‘ğ‘ğ‘¡ ğ‘¡ , and can be described as Eq. ( <ref type="formula">6</ref>).</p><p>ğ‘„ ğœ‹ ( ğ‘ ğ‘¡ ğ‘¡ , ğ‘ğ‘ğ‘¡ ğ‘¡ , ğœƒ) = ğ¸ ğœ‹ {âˆ‘ ğ›¾ ğ‘˜ ğ‘Ÿğ‘¤ğ‘‘ ğ‘¡+ğ‘˜ | âˆ ğ‘˜=0</p><p>ğ‘ ğ‘¡ ğ‘¡ , ğ‘ğ‘ğ‘¡ ğ‘¡ } (6) where ğœƒ is the DQN parameter, ğ›¾ âˆˆ [0,1] is the discount parameter.</p><p>The key issue of auto-labeling is to design a reasonable reward rule, i.e., the reward function defined in Eq. ( <ref type="formula">5</ref>), to propagate labels for massive unlabeled data with high accuracy. Specifically, the point is how to design the discount parameter ğ›¾ for the reward function. Normally, the data with identical label usually may have similar attributes, we thus try to deduce ğ›¾ for each propagation action by evaluating the distance of an unlabeled input data against the nearest labeled neighbor, which means we cluster all the data by the unsupervised clustering method firstly, and then measure the distance of each unlabeled data to the nearest cluster with labeled data. Concretely, given a set of input data ğ‘‹ ğ‘¡ = ğ‘‹_ğ‘™ğ‘ ğ‘¡ âˆª ğ‘‹_ğ‘¢ğ‘™ğ‘ ğ‘¡ at timestamp ğ‘¡ , the dataset can be firstly clustered into k clusters as ğ¶ = {ğ‘ 1 , ğ‘ 2 , â€¦ , ğ‘ ğ‘˜ }. Then, for each unlabeled data ğ‘¥ ğ‘¡ ğ‘š âˆˆ ğ‘‹_ğ‘¢ğ‘™ğ‘ ğ‘¡ , ğ‘š âˆˆ {1,2, â€¦ , ğ‘€}, assuming its nearest cluster containing labeled data is ğ‘ ğ‘˜ , thus the weight of its reward/penalty is calculated by Euclidean Distance between ğ‘¥ ğ‘¡ ğ‘¢ to the corresponding cluster center of ğ‘ ğ‘˜ . The detailed calculation of the propagation entropy of the labeling can be expressed as follows.</p><p>ğ›¾ ğ‘™ğ‘˜ = -ln (ğ‘‘ğ‘–ğ‘ (ğ‘¥ ğ‘™ , ğ‘ ğ‘˜ )) (7) Based on these, if the propagation entropy value is more than a given threshold, ğ‘¥ ğ‘¡ ğ‘š will be assigned with the label from this cluster ğ‘ ğ‘˜ . Finally, the goal of the auto-labeling is to find an optimal action ğ‘ğ‘ğ‘¡ ğ‘¡ * at each step by estimating the state ğ‘ ğ‘¡ ğ‘¡ at ğ‘¡. The concrete auto-labeling scheme is illustrated in Fig <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input: The weakly labeled data ğ¿ğµ âˆª ğ‘ˆğ¿ğµ;</head><p>The clustering set ğ¶ = {ğ‘ 1 , ğ‘ 2 , â€¦ , ğ‘ ğ‘˜ } Output: A set of labeling actions ğ´ğ‘ğ‘¡ Initialize sequence ğ‘†ğ‘¡ = {ğ‘ ğ‘¡ 1 }, and preprocess Î¦ 1 = ğ‘ ğ‘¡ 1 6:</p><p>for t = 1 to T do 7:</p><p>Select a random action ğ‘ğ‘ğ‘¡ ğ‘¡ , ğ´ğ‘ğ‘¡ = ğ´ğ‘ğ‘¡ âˆª ğ‘ğ‘ğ‘¡ ğ‘¡ 8:</p><p>Execute action ğ‘ğ‘ğ‘¡ ğ‘¡ and observe the responding reward ğ‘Ÿğ‘¤ğ‘‘ ğ‘¡ (ğ‘ ğ‘¡ ğ‘¡ , ğ‘ğ‘ğ‘¡ ğ‘¡ , ğ›¾) 9:</p><p>Set ğ‘ ğ‘¡ ğ‘¡+1 = ğ‘ ğ‘¡ ğ‘¡ , ğ‘ğ‘ğ‘¡ ğ‘¡ and preprocess Î¦ ğ‘¡+1 = Î¦(ğ‘ ğ‘¡ ğ‘¡+1 ) 10:</p><p>Sample random minibatch of transitions (Î¦ ğ‘— , ğ‘ğ‘ğ‘¡ ğ‘— , ğ‘Ÿğ‘¤ğ‘‘ ğ‘— , Î¦ ğ‘—+1 ) 11:</p><p>Set ğ‘¦ ğ‘— = { ğ‘Ÿğ‘¤ğ‘‘ ğ‘— ğ‘–ğ‘“ ğ‘’ğ‘ğ‘–ğ‘ ğ‘œğ‘‘ğ‘’ ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘–ğ‘›ğ‘ğ‘¡ğ‘’ğ‘  ğ‘ğ‘¡ ğ‘ ğ‘¡ğ‘’ğ‘ ğ‘— + 1 ğ‘…(Î¦ ğ‘— , ğ‘ğ‘ğ‘¡, ğ›¾) by Eq.( <ref type="formula">5</ref>) ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>Perform a gradient descent step on (ğ‘¦ ğ‘— -ğ‘„(Î¦ ğ‘— , ğ‘ğ‘ğ‘¡ ğ‘— ; ğœƒ)) 2 with respect to weight ğœƒ 13:</p><p>Every 100 steps reset ğ‘„ Ì‚= ğ‘„ 14:</p><p>end for 15: end for 16: Return ğ´ğ‘ğ‘¡ Fig. <ref type="figure">2</ref>. Scheme for auto-labeling based on DQN</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-Sensor Based Data Fusion Mechanism</head><p>The on-body sensor data, context sensor data, and personal profile data are integrated together to figure out different motion patterns in IoHT environments.</p><p>We utilize on-body devices in six different positions to capture subjects' accelerometer data simultaneously. The onbody positions of the sensors are listed in Table <ref type="table">1</ref>. Given a subject ğ‘  ğ‘– , we build the tensor for on-body accelerometer sensor data, and integrate the above six sensors as in Eq. ( <ref type="formula">8</ref>).</p><p>ğ‘‘ğ‘ğ‘¡ğ‘ ğ‘– ğ‘ğ‘ğ‘ (ğ‘‘ğ‘ğ‘¡ğ‘ ğ‘–,ğ‘‘ 1 , ğ‘‘ğ‘ğ‘¡ğ‘ ğ‘–,ğ‘‘ 2 , â€¦ , ğ‘‘ğ‘ğ‘¡ğ‘ ğ‘–,ğ‘‘ 6 ) (8) Three kinds of modalities of context sensor data, i.e., GPS, orientation, light, are adopted to enable the context-aware HAR in the fine-grained classifier. Given a subject ğ‘  ğ‘– , we build the tensor for context sensor data as in Eq. ( <ref type="formula">9</ref>).</p><p>ğ‘‘ğ‘ğ‘¡ğ‘ ğ‘– ğ‘ğ‘œğ‘¡ (ğ‘”ğ‘ğ‘  ğ‘– , ğ‘œğ‘Ÿğ‘¡ ğ‘– , ğ‘™ğ‘–ğ‘¡ ğ‘– ) (9) Accordingly, the final tensor for data fusion representation is integrated and constructed as follows.</p><p>ğ‘§  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. LSTM-Based Fine-Grained Activity Recognition in IoHT</head><p>We apply the LSTM recurrent network to capture temporal features hidden in the fused multi-sensor data. As illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, we send a sequence of fused multi-sensor data constructed as Eq. <ref type="bibr" target="#b9">(10)</ref> with their corresponding labels into the LSTM-based deep neural network, in order to train the finegrained classification model. The concrete training process is illustrated in Fig. <ref type="figure">3</ref>.</p><formula xml:id="formula_1">Input: A sequence of fused input data ğ‘ = {ğ‘§ 1 , ğ‘§ 2 , â€¦ , ğ‘§ ğ‘› }</formula><p>A set of labels ğ‘Œ corresponding to ğ‘ Output: A trained human activity classification model 1: Initialize parameter ğ¼ğ‘¡ğ‘’ğ‘Ÿ, ğµğ‘ğ‘¡ğ‘â„, ğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ ğ‘™ğ‘œğ‘ ğ‘  2: for iteration = 1 to ğ¼ğ‘¡ğ‘’ğ‘Ÿ do 3:</p><p>for each minibatch {ğ‘§ ğ‘˜ } ğ‘˜=1 ğ‘›/ğµğ‘ğ‘¡ğ‘â„ do 4: Filter input data ğ‘” ğ‘˜ = G(ğ‘§ ğ‘˜ ) with Gaussian Noise layer 5:</p><p>Compute high level feature â„ğ‘“ ğ‘˜ = LSTM(ğ‘” ğ‘˜ ) with LSTM recurrent network 6:</p><p>Conduct the variation computation with dropout 0.5: ğ‘‘ğ‘ ğ‘˜ = dropout 0.5 (ğ‘” ğ‘˜ ) 7:</p><p>Predict the activity label ğ‘¦ Ìƒğ‘˜ = SoftMax(ğ‘‘ğ‘ ğ‘˜ ) 8:</p><p>Compute the cost function about the loss by ğ‘¦ Ìƒğ‘˜ and ğ‘¦ ğ‘˜ âˆˆ ğ‘Œ 9:</p><p>if loss &lt; ğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ ğ‘™ğ‘œğ‘ ğ‘  : break 10:</p><p>Run the Back-Propagation process to update the parameters for the model 11:</p><p>end for 12: end for Fig.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Fine-grained classification training based on LSTM</head><p>As shown in Fig. <ref type="figure">3</ref>, with a sequence of fused input data ğ‘ = {ğ‘§ 1 , ğ‘§ 2 , â€¦ , ğ‘§ ğ‘› } and a set of corresponding labels (Note that some of these labels are propagated through the proposed autolabeling scheme), we start the training process upon the presetting parameters, i.e., the max iterations ğ‘€, batchsize ğµğ‘ğ‘¡ğ‘â„, and the threshold for cost function ğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ ğ‘™ğ‘œğ‘ ğ‘  . Each batch of the fused input data runs through the Gaussian noise, LSTM recurrent neural network, Dropout and SoftMax output layers in the forward-propagation process. Then, the training loss is calculated by the cost function and applied in the backpropagation process to update the model. When the loss is lower than the threshold, the model is prepared and ready for HAR in IoHT environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENT AND ANALYSIS</head><p>To verify the effectiveness of our method to the fine-grained human activity recognition in IoHT, comparison experiments are conducted with multiple sensor data collected in real world. All experiments have run on a server of Intel i7-4790 @3.6GHz CPU, 32GB RAM, NVidia GeForce GTX 970 GPU, Linux, Python 3.5, TensorFlow r2.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Set</head><p>To investigate the proposed method on HAR in IoHT environments, two free datasets <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b35">36]</ref>, which were collected using smartphones and on-body wearable devices (see Table <ref type="table" target="#tab_0">I</ref>), were downloaded for the comparison evaluation. In the IoHT environment, the on-body wearable devices were communicated and synchronized with the network provider.</p><p>The first dataset was a pure acceleration data, which included 11,771 activities performed by 30 subjects of ages ranging from 18 to 60 <ref type="bibr" target="#b7">[8]</ref>. Specifically, this dataset covered 9 types of activities in daily living and 8 types of falls. Another dataset was collected by a set of on-body wearable devices, which also included several context information (e.g., GPS, magnetic field, sound level, and light) <ref type="bibr" target="#b35">[36]</ref>. This dataset contained the acceleration data covering 8 types of activities from fifteen subjects (seven females and eight males, age 31.9 Â± 12.4, height 173.1 Â± 6.9, weight 74.1 Â± 13.8). For each activity, the on-body positions: chest, forearm, head, shin, thigh, upper arm, and waist were simultaneously recorded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiment Design and Evaluation Metrics</head><p>The whole dataset was separated into three parts: 60% as training subset, 20% as validation subset, and 20% as testing subset. Both the accelerometer and the corresponding context information in the train set were utilized to construct the Semi-Supervised learning model, and train the parameters in DQN and LSTM. To avoid the overfitting problem, the validation subset was applied to adjust the structure and parameters of the model.</p><p>We used the metrics: Accuracy, Precision-Recall, F1-score to evaluate the performance of the proposed method. We set a set of different portions of unlabeled data ranging from 20% to 100%. The DNN, RF, and SVM-based classifier are chosen for performance comparison for HAR in IoHT environments.</p><p>In addition, we used the Q-Learning and full connection deep neural network in DQN, the Gradient Descent Optimizer and SoftMax regression were used after the full connection layer to generate the classification results. A dropout scheme (dropout rate =0.4) was applied to avoid the overfitting problem. The learning rate was set to 0.05 and batch size was 25. We reset the Q function value by every 100 steps within an episode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Effectiveness Evaluation for Auto-Labeling</head><p>We evaluated the effectiveness of the proposed auto-labeling method based on the weakly labeled data in IoHT environments. We simulated the scenario that lacking of well labeled activities in daily living by moving out some of the labels from the dataset. Evaluations were conducted with the portions of unlabeled data ranging from 20% to 100%, to demonstrate the performance of our proposed method with the other three baseline methods under different degrees of weakly labeled data. Comparison results according to Precision, Recall, and F1score are shown in Fig. <ref type="figure" target="#fig_2">4</ref> (a)-(c) respectively.</p><p>As shown in Fig. <ref type="figure" target="#fig_2">4</ref>, generally, all the methods perform well in the fully supervised situation (100% portion of labeled data). However, results of all the methods becomes worse along with the increasing portion of unlabeled data. Since the proposed semi-supervised framework is benefited by our auto-labeling scheme, it achieves an average F1-score of 0.79 under different portion settings of unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Performance Evaluation for HAR</head><p>We investigated eight types of activities in daily living, to evaluate the different performances for activity recognition, which are listed in Table <ref type="table" target="#tab_3">III</ref>. First, we investigated how the proposed method performed with different kinds of activities. Fig. <ref type="figure" target="#fig_3">5</ref> demonstrates the recognition performance of the proposed method on different categories of activities (classes). We utilize the ROC curve to demonstrate the performance for different activities. The eight solid lines with different colors represent the Area Under Curve (AUC) values for activities listed in Table <ref type="table" target="#tab_3">III</ref>. The dot line summarizes the macro-average performance of our proposed method among all the activities. The average AUC value is 0.95. In particular, recognition based on Climbing down (ğ´ 1 ) obtain the best performance, while recognition based on Standing (ğ´ 5 ) has the worst performance. This is mainly because the Climbing down activity may result in some distinctive features, while the Standing activity has fewer number of explicit features against the other activities.</p><p>We further compared the average recognition in terms of the eight activities among all the four methods. We utilize the ROC curve to demonstrate performances for different activities among our method and other three baseline methods. The comparison result is shown in Fig. <ref type="figure" target="#fig_4">6</ref>.</p><p>As shown in Fig. <ref type="figure" target="#fig_4">6</ref>, all the ROC curves are located upon the diagonal line in the upper left corner, which demonstrates the general positive effects for all the methods in the test scenario. It is obvious that the proposed method achieves a substantial improvement in terms of the AUC value comparing with the other three methods. Specifically, our method achieves the AUC value of 0.95, and the DNN, SVM, RF methods can only get the value of 0.87, 0.74, and 0.90 respectively. In addition, the body position where the sensor located on would have different influence on the results of activity recognition. To investigate the influence in terms of different sensor positions to the recognition performance, considering results based on the above activity category evaluation, we conducted the evaluation by six different on-body positions (ğ‘‘ 1 -ğ‘‘ 6 ) for the activity Jumping (ğ´ 3 ). The result illustrated in Table <ref type="table" target="#tab_4">IV</ref> depicts the fact that there is no absolute optimal onbody position for activity Jumping when considering all the classifiers. However, according to ğ´ 3 , the accuracy results of ğ‘‘ 1 , ğ‘‘ 2 , ğ‘‘ 6 are relatively higher than the others for all the classifiers. It is because these on-body positions can generate more exclusive data to improve the recognition accuracy than from other positions. Moreover, to evaluate how recognition performances of the methods are influenced by the proposed data fusion mechanism, we conducted the experiment by combining different types of sensors data together in terms of Eq. <ref type="bibr" target="#b9">(10)</ref>. Totally 42 features have been extracted from the subject's profile, accelerometer sensor data, and context sensor data for the evaluation. To investigate how these features affect the methods' recognition performance, we conducted evaluations for all the methods by setting the fused features number from 3 to 42. The experimental results are illustrated in Fig. <ref type="figure" target="#fig_6">7</ref>   The following observations and discussions based on these evaluation results can be noted.</p><p>i) The Precision results for the methods are shown in Fig. <ref type="figure" target="#fig_6">7</ref> (a). As shown in the figure, all four precision curves represent general upward trend along with the increasing number of features. Worth to be noted that all the curves arise largely when the number of the features reaches to 33 and become stable at 35, which indicates that the three features (no.33 to no.35) can enhance the performance significantly about 18% and provide important support for HAR in IoHT environments. In addition, although all the methods achieved their best precision results when all the features were included in the evaluation, the proposed method outperforms the others in terms of its maximum precision value and the general performance. ii) Fig. <ref type="figure" target="#fig_6">7</ref> (b) demonstrates the Recall results for the four methods. Obviously, the neural network based methods (both the DNN and the proposed method) perform better than the conventional machine learning methods (RF and SVM), which suggests that the neural network based methods are relatively suitable to handle this problem. Moreover, the result that our method outperforms the DNN method indicates that the auto-labeling scheme based on semi-supervised learning can benefit the Recall performance. It can partially resolve the inadequately labeled sample situation especially when considering the Recall metric by taking the advantage of leveraging the unlabeled data to enlarge the training set. iii) At last, we analyze the F1-score metric for the methods as shown in Fig. <ref type="figure" target="#fig_6">7 (c</ref>). We observe that the proposed method achieves a substantial improvement in terms of F1-score than the other three methods. The overall F1-score of the proposed method is higher than the others especially when all the features are considered. It indicates that the data fusion upon multi-sensor data can enhance the performance of HAR in IoHT environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this study, we proposed a deep learning enhanced HAR method using weakly labeled sensor data in IoHT environments.</p><p>Specifically, a semi-supervised deep learning framework was designed to improve the accuracy of HAR in IoHT environments, which consisted of two key function modules as auto-labeling module and LSTM-based classification module. An intelligent auto-labeling scheme was developed based on the DQN technique, which could better solve the problem of inadequately labeled motion data in daily living based on a newly designed distance-based reward rule. A multi-sensor based data fusion mechanism was then developed to integrate the on-body sensor data, context sensor data, and personal profile data together in a seamless way. A LSTM-based neural network was constructed to deal with a series of sequential motion data, and a classification mechanism was improved to identify the fine-grained motion pattern based on the extracted high-level features. Two sensor data sets collected using smartphones and on-body wearable devices were utilized to conduct the experiment. Evaluation results demonstrated the practicability and usefulness of our proposed model and method, comparing with other three baseline methods.</p><p>In the future studies, we will go further to study more efficient deep learning techniques for meaningful pattern detection from the weakly labeled data. More evaluations in different situations will be conducted to improve the algorithm with better accuracy and efficiency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Framework of Semi-supervised deep learning for HAR with weakly labeled input data</figDesc><graphic coords="4,46.80,122.97,252.00,147.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 :</head><label>1</label><figDesc>Initialize action-value function ğ‘„ with random weight ğœƒ 2: Initialize target action-value function ğ‘„ Ì‚ with ğœƒ 3: Initialize parameters: EP, T 4: for episode = 1 to EP do 5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Efficiency evaluation on auto-labeling tasks.</figDesc><graphic coords="6,52.20,477.30,170.02,127.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Comparison result for all activities</figDesc><graphic coords="7,46.80,298.84,252.00,189.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Comparison result for all Activities</figDesc><graphic coords="7,313.20,98.70,252.00,189.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a)-(c) respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Comparison results by different number of features. (a) Precision. (b) Recall. (c) F1-score.</figDesc><graphic coords="8,59.25,62.47,170.02,127.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">DESCRIPTION OF ON-BODY DEVICES</cell></row><row><cell>Devices</cell><cell>On-body position</cell><cell>Devices</cell><cell>On-body position</cell></row><row><cell>ğ‘‘ 1</cell><cell>head</cell><cell>ğ‘‘ 4</cell><cell>waist</cell></row><row><cell>ğ‘‘ 2</cell><cell>chest</cell><cell>ğ‘‘ 5</cell><cell>thigh</cell></row><row><cell>ğ‘‘ 3</cell><cell>upper arm</cell><cell>ğ‘‘ 6</cell><cell>shin</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>sample, including the personal profile ğ‘ƒğ‘Ÿğ‘œ ğ‘– = (ğ‘”ğ‘‘ ğ‘– , ğ‘ğ‘” ğ‘– , â„ğ‘¡ ğ‘– , ğ‘¤ğ‘¡ ğ‘– ) , accelerometer data ğ‘‘ğ‘ğ‘¡ğ‘ ğ‘– ğ‘ğ‘ğ‘ , and context data ğ‘‘ğ‘ğ‘¡ğ‘ ğ‘– ğ‘ğ‘œğ‘¡ , obtained by devices {ğ‘‘ ğ‘˜ } from subjects {ğ‘  ğ‘– }, can be exemplified in Table II with two subjects ğ‘  1 , ğ‘  2 and three onbody devices ğ‘‘ 1 , ğ‘‘ 2 , ğ‘‘ 3 considering the location context.</figDesc><table><row><cell>ğ‘– (ğ‘  ğ‘– , ğ‘‘ğ‘ğ‘¡ğ‘ ğ‘– ğ‘ğ‘ğ‘ , ğ‘‘ğ‘ğ‘¡ğ‘ ğ‘– ğ‘ğ‘œğ‘¡ )</cell><cell>(10)</cell></row><row><cell>A simple data</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III TYPICAL</head><label>III</label><figDesc>CATEGORIES OF ACTIVITIES TESTED IN THE EXPERIMENT</figDesc><table><row><cell>ID of activities</cell><cell>Class</cell><cell>Activity description</cell></row><row><cell>ğ´ 1</cell><cell>class 0</cell><cell>Climbing down</cell></row><row><cell>ğ´ 2</cell><cell>class 1</cell><cell>Climbing up</cell></row><row><cell>ğ´ 3</cell><cell>class 2</cell><cell>Jumping</cell></row><row><cell>ğ´ 4</cell><cell>class 3</cell><cell>Sitting</cell></row><row><cell>ğ´ 5</cell><cell>class 4</cell><cell>Standing</cell></row><row><cell>ğ´ 6</cell><cell>class 5</cell><cell>Lying</cell></row><row><cell>ğ´ 7</cell><cell>class 6</cell><cell>Walking</cell></row><row><cell>ğ´ 8</cell><cell>class 7</cell><cell>Jogging</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV PERFORMANCE</head><label>IV</label><figDesc>OF THE CLASSIFICATION OF DEVICES UNDER DIFFERENT ON-BODY POSITIONS</figDesc><table><row><cell>On-body devices</cell><cell>Our method</cell><cell>DNN</cell><cell>RF</cell><cell>SVM</cell></row><row><cell>ğ‘‘ 1</cell><cell>0.96</cell><cell>0.95</cell><cell>0.94</cell><cell>0.89</cell></row><row><cell>ğ‘‘ 2</cell><cell>0.97</cell><cell>0.95</cell><cell>0.92</cell><cell>0.87</cell></row><row><cell>ğ‘‘ 3</cell><cell>0.96</cell><cell>0.94</cell><cell>0.92</cell><cell>0.85</cell></row><row><cell>ğ‘‘ 4</cell><cell>0.95</cell><cell>0.91</cell><cell>0.92</cell><cell>0.78</cell></row><row><cell>ğ‘‘ 5</cell><cell>0.95</cell><cell>0.92</cell><cell>0.92</cell><cell>0.81</cell></row><row><cell>ğ‘‘ 6</cell><cell>0.97</cell><cell>0.97</cell><cell>0.93</cell><cell>0.83</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Authorized licensed use limited to: University of Wollongong. Downloaded on April 05,2020 at 06:17:22 UTC from IEEE Xplore. Restrictions apply.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>&gt; REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) &lt;</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>, IEEE Internet of Things Journal Xiaokang Zhou (M'12) is currently an associate professor with the Faculty of Data Science, Shiga University, Japan. He received the Ph.D. degree in human sciences from Waseda University, Japan, in 2014. From 2012 to 2015, he was a research associate with the</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Data fusion and multiple classifier systems for human activity detection and health monitoring: Review and open research directions</title>
		<author>
			<persName><forename type="first">Henry</forename><surname>Friday Nweke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><forename type="middle">Wah</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ghulam</forename><surname>Mujtaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al-Garadi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="147" to="170" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Communication-Efficient Federated Learning for Wireless Edge Intelligence in IoT</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Min</surname></persName>
		</author>
		<idno type="DOI">10.1109/JIOT.2019.2956615</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Big Data-as-a-Service Framework: State-of-the-Art and Perspectives</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Deen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="325" to="340" />
			<date type="published" when="2018">1 Sept. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Big Data Real-Time Processing Based on Storm</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th IEEE International Conference on Trust, Security and Privacy in Computing and Communications</title>
		<meeting><address><addrLine>Melbourne, VIC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="1784" to="1787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey on human activity recognition using wearable sensors</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">D</forename><surname>Lara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Labrador</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Surveys Tuts</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1192" to="1209" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>3rd Quart.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An Efficient Deep Learning Model to Predict Cloud Workload for Industry Informatics</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3170" to="3178" />
			<date type="published" when="2018-07">July 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Machine recognition of human activities: A survey</title>
		<author>
			<persName><forename type="first">P</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1473" to="1488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Dataset for Human Activity Recognition Using Acceleration Data from Smartphones</title>
		<author>
			<persName><forename type="first">M</forename><surname>Daniela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Paolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shar</forename><surname>Unimib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1101" to="1113" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Novel Semi-Supervised Deep Learning Method for Human Activity Recognition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Yeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3821" to="3830" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weakly Supervised Human Activity Recognition From Wearable Sensors by Recurrent Attention Learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sensors Journal</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2287" to="2297" />
			<date type="published" when="2019">15 March15, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">HuMAn: Complex Activity Recognition with Multi-Modal Multi-Positional Body Sensing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chellappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Mobile Computing</title>
		<imprint>
			<date type="published" when="2019-04-01">1 April 2019</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="857" to="870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Hybrid Hierarchical Framework for Gym Physical Activity Recognition and Measurement Using Wearable Sensors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hanneghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1384" to="1393" />
			<date type="published" when="2019-04">April 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On Spatial Diversity in WiFi-Based Human Activity Recognition: A Deep Learning-Based Approach</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2035" to="2047" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Wearable Computing for Internet of Things: A Discriminant Approach for Human Activity Recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yuting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2749" to="2759" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Human Action Recognition by Semilatent Topic Models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1762" to="1774" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards Accurate Prediction for High-Dimensional and Highly-Variable Cloud Workloads with Deep Learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zomaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>El-Ghazawi</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPDS.2019.2953745</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Real time human activity recognition using tri-axial accelerometers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Colbry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Juillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Sensors, Signals Inf. Process. Workshop</title>
		<meeting>Sensors, Signals Inf. ess. Workshop</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="3337" to="3340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feature selection and activity recognition system using a single tri-axial accelerometer</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dallas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1780" to="1786" />
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Activity recognition from accelerometer data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dandekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mysore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th Conf</title>
		<title level="s">Innovative Appl. Artif. Intell</title>
		<meeting>17th Conf</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1541" to="1546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Activity and location recognition using wearable sensors</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Pervasive Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="24" to="32" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recognizing human motion with multiple acceleration sensors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mantyjarvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Himberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Seppanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Syst., Man, Cybern</title>
		<meeting>IEEE Int. Conf. Syst., Man, Cybern</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="747" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A survey of depth and inertial sensor fusion for human action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kehtarnavaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Multimed. Tools Appl.</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="4405" to="4425" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Activity recognition from user-annotated acceleration data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Intille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Pervasive</title>
		<meeting>Pervasive</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3001</biblScope>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Wearable sensing to annotate meeting recordings</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Junker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lukowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Troster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pers. Ubiquitous Comput</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="263" to="274" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recognizing workshop activity using body worn microphones and accelerometers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lukowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Junker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Starner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Pervasive Comput</title>
		<meeting>Pervasive Comput</meeting>
		<imprint>
			<date type="published" when="2004-04">Apr. 2004</date>
			<biblScope unit="page" from="18" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Inferring high-level behavior from low-level sensors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Conf. Ubiquitous Comput</title>
		<meeting>5th Conf. Ubiquitous Comput</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="73" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning and inferring transporation routines</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="311" to="331" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multimodal physical activity recognition by fusing temporal and cepstral information</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Syst. Reha-bil. Eng</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="369" to="380" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Blockchain-based Nonrepudiation Network Computing Service Scheme for Industrial IoT</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3632" to="3641" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Blockchain Empowered Arbitrable Data Auditing Scheme for Network Storage as a Service</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSC.2019.2953033</idno>
		<ptr target="https://doi.org/10.1109/TSC.2019.2953033" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Services Computing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schcolkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Conf. Comput. Learn. Theory</title>
		<meeting>Annu. Conf. Comput. Learn. Theory</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by dis-agreement</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="415" to="439" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Weakly supervised recognition of daily life activities with wearable sensors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stikic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intel</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2521" to="2537" />
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">EESEN: End-to-end speech using deep RNN models and WFST-based decoding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Automatic Speech Recognition &amp; Understanding</title>
		<meeting>Automatic Speech Recognition &amp; Understanding</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Position-aware activity recognition with wearable devices</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sztyler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stuckenschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Petrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pervasive and Mobile Computing</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="281" to="295" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
