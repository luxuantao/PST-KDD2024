<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Soft fuzzy rough sets for robust feature evaluation and selection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qinghua</forename><surname>Hu</surname></persName>
							<email>huqinghua@hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>150001</postCode>
									<settlement>Harbin</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuang</forename><surname>An</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>150001</postCode>
									<settlement>Harbin</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daren</forename><surname>Yu</surname></persName>
							<email>yudaren@hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>150001</postCode>
									<settlement>Harbin</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Soft fuzzy rough sets for robust feature evaluation and selection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">17893EDE9EDD3008CE8D79750607C85B</idno>
					<idno type="DOI">10.1016/j.ins.2010.07.010</idno>
					<note type="submission">Received 29 June 2009 Received in revised form 1 June 2010 Accepted 19 July 2010</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Fuzzy rough sets Feature evaluation Robust Noise</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The fuzzy dependency function proposed in the fuzzy rough set model is widely employed in feature evaluation and attribute reduction. It is shown that this function is not robust to noisy information in this paper. As datasets in real-world applications are usually contaminated by noise, robustness of data analysis models is very important in practice. In this work, we develop a new model of fuzzy rough sets, called soft fuzzy rough sets, which can reduce the influence of noise. We discuss the properties of the model and construct a new dependence function from the model. Then we use the function to evaluate and select features. The presented experimental results show the effectiveness of the new model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In classification learning, data are usually described with a great number of features. Typically, part of them are irrelevant or redundant with the classification task. These irrelevant features might confuse learning algorithms and deteriorate learning performance. Hence, it is useful to select relevant and indispensable features for designing classification systems.</p><p>So far, a number of algorithms have been developed for feature reduction <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31]</ref>. Generally speaking, there are two key issues in constructing a feature selection algorithm: feature evaluation and search strategies. Feature evaluation is used to measure the quality of the candidate features. Obviously, evaluation functions have great influence on outputs of algorithms. A great number of functions were designed, such as dependency <ref type="bibr" target="#b44">[45]</ref>, neighborhood dependency <ref type="bibr" target="#b14">[15]</ref> and fuzzy dependency in the rough set theory <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39]</ref>; mutual information and symmetric uncertainty in information theory <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31]</ref>; sample margin <ref type="bibr" target="#b56">[57]</ref> and hypothesis margin <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b42">43]</ref> in statistical learning theory, and so on. As to the search strategy, it can be roughly divided into two categories. One guarantees to find the optimal subset of features in terms of the used evaluation function, such as the exhaustive search <ref type="bibr" target="#b24">[25]</ref> and the branch-and-bound algorithm <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b39">40]</ref>. And the other is to find a suboptimal solution for efficiency, including sequential forward selection <ref type="bibr" target="#b20">[21]</ref>, sequential backward elimination <ref type="bibr" target="#b24">[25]</ref>, floating search <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b40">41]</ref>, mRMR <ref type="bibr" target="#b30">[31]</ref>, etc.</p><p>The rough set theory provides a mathematical tool to handle uncertainty in data analysis <ref type="bibr" target="#b29">[30]</ref>. It has been successfully used in attribute reduction and rule learning <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b44">45]</ref>. Moreover, this theory also provides practical solutions to many data analysis tasks, such as data mining <ref type="bibr" target="#b28">[29]</ref> and rule discovery <ref type="bibr" target="#b31">[32]</ref>. The classic rough set model is defined with equivalence relations, which leads to the limitation in handling data with numerical or fuzzy attributes, some generalized models were proposed, such as fuzzy rough sets <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b58">59</ref>] and neighborhood rough sets <ref type="bibr" target="#b14">[15]</ref>.</p><p>It is well known that datasets in real-world application are usually corrupted by noise <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b60">61]</ref>. The noisy samples may have great influence on outputs of the models. Accordingly, the performance of classification systems would be reduced. So robust models and algorithms are highly desirable in practice. 0020-0255/$ -see front matter Ó 2010 Elsevier Inc. All rights reserved. doi:10.1016/j.ins.2010.07.010</p><p>In the framework of rough sets, dependency functions, defined as the ratio of the consistent samples over the universe, are used to compute the quality of features. This function plays the central role in rough set based learning algorithms. However, it is observed that the dependency function defined in Pawlak rough set model is not robust. This property is passed down to neighborhood rough sets and fuzzy rough sets <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b61">62]</ref>, which limits the applications of these models.</p><p>In order to deal with this problem, some extended models were developed. First, Yao, Wong et al. proposed the decisiontheoretic rough set model (DTRS) in 1990 <ref type="bibr" target="#b54">[55]</ref> and applied this model to attribute reduction in 2008 <ref type="bibr" target="#b55">[56]</ref>. This model considers the statistic information in data. In 1993, Ziarko developed the variable precision rough set model (VPRS) to tolerate noisy samples <ref type="bibr" target="#b61">[62]</ref>, where several mislabeled samples in an equivalence class are overlooked in computing lower and upper approximations. However, given a learning task, it is a big problem to set how many samples should be overlooked. In addition, information theory was also introduced to compute the significance of features <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b53">54]</ref>. These models are indeed more robust than rough sets, however, the granular structures are lost in these models. In <ref type="bibr" target="#b48">[49]</ref>, a comparative study between Pawlak's rough sets based reduction and the information-theoretic based reduction was conducted. Besides, Rolka et al. and Zhao, Tsang et al. showed the definitions of variable precision fuzzy rough sets <ref type="bibr" target="#b36">[37]</ref> and fuzzy variable precision rough sets <ref type="bibr" target="#b57">[58]</ref> to enhance robustness of fuzzy rough sets, respectively. Unfortunately, we find that the model in <ref type="bibr" target="#b57">[58]</ref> is still sensitive to mislabeled samples. Although there are some models to deal with noise in datasets, it seems that handling noise is still an open problem in the rough set theory.</p><p>In intelligent data analysis, there are two ways to deal with noisy information. One is to remove noise in the step of data preprocessing, such as outlier detection <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42]</ref>, data cleaner <ref type="bibr" target="#b52">[53]</ref> and impact-sensitive ranking <ref type="bibr" target="#b59">[60]</ref>. And the other is to design robust algorithms, such as noise-tolerance feature selection <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20]</ref>, weighted k-Nearest Neighbor <ref type="bibr" target="#b43">[44]</ref>, Maxi-Min Margin Machine <ref type="bibr" target="#b17">[18]</ref>, robust minimax approach <ref type="bibr" target="#b23">[24]</ref>, Nearest Subclass Classifier <ref type="bibr" target="#b47">[48]</ref>, Cost-Sensitive Classification <ref type="bibr" target="#b60">[61]</ref>, Error-Aware Classification <ref type="bibr" target="#b51">[52]</ref>, robust clustering <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref> and soft-margin SVM <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>.</p><p>In recent years, soft-margin SVM becomes a popular and robust learning algorithm for classification modeling. As to hardmargin SVM, all the samples should be correctly classified with a margin, while soft-margin SVM allows some samples to be misclassified for obtaining a large-margin classifier by making tradeoff between margin and classification error. By this way, soft-margin SVM reduces the impact of noisy information on the final classifier.</p><p>In this work, we follow the idea of soft-margin SVM and introduce a robust rough set model, called soft fuzzy rough set. The classic fuzzy rough set model computes the membership of an object to a class with the nearest sample from different classes. However, this leads to the sensitivity to noisy samples. Our model improves the computation of approximations, where the membership is not calculated with the nearest sample from different classes, but the k 0 th sample, where k is determined by tradeoff between the number of misclassified samples and the augmentation of membership. By this way, the proposed model is robust to the noisy samples. Some numerical experiments are conducted to test the robustness of the model in feature evaluation and selection.</p><p>The rest of the paper is organized as follows. Section 2 gives the basic notations of rough sets and analyzes the robustness of these models. Section 3 introduces the definition of soft fuzzy rough sets and discusses the properties of the model. Next, we define the soft fuzzy dependency and design a feature selection algorithm based on soft dependency in Section 4. And then we introduce some measures for evaluating robustness of algorithms in Section 5. Numerical experiments are presented in Section 6. Finally, the conclusions are given in Section 7.</p><p>2. Basic notations of rough sets and robustness analysis IS = hU, Ci is called an information table, where U is a finite and nonempty set of objects and C is a set of features used to characterize the objects. " B # C, a B-indiscernibility relation is defined as</p><formula xml:id="formula_0">INDðBÞ ¼ fðx; yÞ 2 U 2 j8a 2 B; aðxÞ ¼ aðyÞg:<label>ð1Þ</label></formula><p>Then the partition of U generated by</p><formula xml:id="formula_1">IND(B) is denoted by U/IND(B) (or U/B). The equivalence class of x induced by B-indis- cernible relation is denoted by [x] B .</formula><p>Given an arbitrary X # U, R is an equivalence relation on U induced by a set of attributes. The lower and upper approximations of X with respect to R are defined as</p><formula xml:id="formula_2">RX ¼ fx 2 Uj½x R # Xg; RX ¼ fx 2 Uj½x R \ X -/g: (<label>ð2Þ</label></formula><formula xml:id="formula_3">BN R ðXÞ ¼ RX À RX is called R-boundary region of X and NEG R ðXÞ ¼ U À RX is the R-negative region of X. The lower approxi- mation is also called R-positive region of X, denoted by POS R (X).</formula><p>Given a decision table DS = hU, C [ D i, D is the decision attribute. For " B # C, the positive region of decision D on B, denoted by POS B (D), is defined as</p><formula xml:id="formula_4">POS B ðDÞ ¼ [ X2U=D BX;<label>ð3Þ</label></formula><p>where U/D is the set of the equivalence classes generated by D. The dependency of decision D on B is defined as</p><formula xml:id="formula_5">c B ðDÞ ¼ jPOS B ðDÞj jUj :<label>ð4Þ</label></formula><p>Dependency is the ratio of the samples in the lower approximation over the universe. As the lower approximation is the set of objects with consistent decisions, dependency is used to measure the classification performance of attributes. It is expected that all the decisions of objects are consistent with respect to the given attributes. In practice, inconsistency widely exists in data.</p><p>The previous research shows that the lower and upper approximations in Pawlak's rough sets were sensitive to noise. According to the definition of lower approximations, the sample is grouped into lower approximation if all samples in its equivalence class consistently belong to a decision class. While the sample belongs to the upper approximation if one of the samples in its equivalence class comes from the decision class. Thus if there is one noisy sample, the whole equivalence class is grouped into the classification boundary. This leads to the sensitivity of dependency to noisy samples.</p><p>As to data with numerical-valued features, neighborhood relations and neighborhood rough sets are introduced <ref type="bibr" target="#b14">[15]</ref>. Given a decision table hU, C [ Di, U is divided into N decision classes: X 1 , X 2 , . . ., X N . " B 2 C, the neighborhood of sample x is defined as d(x) = {yjD B (x, y) 6 d, y 2 U}, where D B is a distance function defined in a feature space B. If sample y is contained by the neighborhood of x, we say y and x satisfy neighborhood relation N B . We can see that neighborhood relation relaxes the equivalence relation to a similarity relation, and the similarity degree is characterized by distance functions. The lower and upper approximations of D in the neighborhood induced granular space are</p><formula xml:id="formula_6">N B D ¼ fN B X 1 ; N B X 2 ; . . . ; N B X N g; N B D ¼ fN B X 1 ; N B X 2 ; . . . ; N B X N g; (<label>ð5Þ</label></formula><formula xml:id="formula_7">where N B X = {x i jd B (x i ) # X, x i 2 U} and N B X ¼ fx i jd B ðx i Þ \ X -/; x i 2 Ug. The neighborhood dependency of D on B is defined as c B ðDÞ ¼ jN B Dj jUj :<label>ð6Þ</label></formula><p>Just like dependency in Pawlak's rough sets, neighborhood dependency is also sensitive to noisy samples. We can see if there is one sample with a different decision in the neighborhood of x i , x i should be grouped as classification boundary. In this sense, the lower approximation of neighborhood rough sets is sensitive to noise, which make neighborhood dependency is not robust to noise.</p><p>As to fuzzy cases, fuzzy rough sets were developed. Given a nonempty universe U, R is a fuzzy binary relation on U. If R satisfies:</p><p>(1) reflexivity: R(x, x) = 1, (2) symmetry: R(x, y) = R(y, x), (3) sup-min transitivity: R(x, y) P supmin z2U {R(x, z), R(z, y)}.</p><p>We say R is a fuzzy similarity relation. Fuzzy similarity relations are used to measure the similarity of the objects characterized with continuous features. The fuzzy similarity class [x] R associated with x and R is a fuzzy set, where [x] R (y) = R(x, y) for all y 2 U. Fuzzy rough sets were first introduced by Dubois and Prade in <ref type="bibr" target="#b11">[12]</ref> based on fuzzy similarity relations. Definition 1. Let U be a nonempty universe, R be a fuzzy similarity relation on U and F(U) be the fuzzy power set of U. Given a fuzzy set A 2 F(U), the lower and upper approximations are defined as RAðxÞ ¼ inf y2U maxf1 À Rðx; yÞ; AðyÞg; RAðxÞ ¼ sup y2U minfRðx; yÞ; AðyÞg:</p><formula xml:id="formula_8">8 &gt; &lt; &gt; :<label>ð7Þ</label></formula><p>The approximation operators in <ref type="bibr" target="#b6">(7)</ref> were studied in detail from the constructive and axiomatic approaches in <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref>. In 1998, Morsi and Yakout replaced fuzzy equivalence relation with a T-equivalence relation and built an axiom system of the model <ref type="bibr" target="#b26">[27]</ref>. In 2002, based on the negator operator d and implicator operator h, Radzikowska and Kerre defined fuzzy lower and upper approximations <ref type="bibr" target="#b33">[34]</ref>.</p><p>If A is a crisp set, then</p><formula xml:id="formula_9">AðyÞ ¼ 1; y 2 A; 0; y R A:<label>ð8Þ</label></formula><p>The fuzzy lower and upper approximations in <ref type="bibr" target="#b6">(7)</ref> degenerate into the following formulae</p><formula xml:id="formula_10">RAðxÞ ¼ inf y2UÀA f1 À Rðx; yÞg; RAðxÞ ¼ sup<label>y2A</label></formula><p>Rðx; yÞ:</p><formula xml:id="formula_11">8 &gt; &lt; &gt; :<label>ð9Þ</label></formula><p>Considering the above definitions, we see that the membership of a sample x 2 U to the fuzzy lower approximation of A is the dissimilarity between x and the nearest sample y R A and the membership of a sample x 2 U to the fuzzy upper approximation of A is the similarity between x and the nearest sample y 2 A. If we take Rðx; yÞ</p><formula xml:id="formula_12">¼ exp Àkx À yk 2 d !<label>ð10Þ</label></formula><p>as a similarity function, then 1 À R(x, y) can be considered as a general distance function d(x, y) between x and y. Then formula ( <ref type="formula" target="#formula_11">9</ref>) can be expressed as</p><formula xml:id="formula_13">RAðxÞ ¼ inf y2UÀA fdðx; yÞg; RAðxÞ ¼ sup y2A f1 À dðx; yÞg ¼ 1 À inf y2A fdðx; yÞg: 8 &gt; &lt; &gt; :<label>ð11Þ</label></formula><p>Fig. <ref type="figure">1</ref> shows a toy example. According to the above analysis, in Fig. <ref type="figure">1</ref>, the membership of x to the fuzzy lower approximation of the class marked by squares is the distance between x and y 1 . Unfortunately, y 1 is a noisy sample. If y 1 does not exist, the membership of x to the fuzzy lower approximation of the class equals to the distance between x and y 2 . The membership of x to the fuzzy lower approximation of the class increases significantly in this case. However, if y 1 does exist, the memberships of the lower approximation of all samples marked by squares change. One noisy sample completely alters the lower approximation of a class. Correspondingly, the fuzzy dependency of D on feature subset B, defined as c B ðDÞ ¼</p><formula xml:id="formula_14">P x2U POS B ðDÞðxÞ jUj ¼ P x2U ð sup X2U=D BðXÞðxÞÞ jUj<label>ð12Þ</label></formula><p>is sensitive to noise as well. Zhao et al. <ref type="bibr" target="#b57">[58]</ref> discussed the robustness of several rough set models, including VPRS <ref type="bibr" target="#b61">[62]</ref> and VPFRS <ref type="bibr" target="#b36">[37]</ref>, generalized with a threshold from Pawlak rough sets, and they pointed out that all of them were sensitive to noise. Moreover, Zhao also referred to that it was difficult for VQRS <ref type="bibr" target="#b6">[7]</ref> to design an attribute reduction method since the important property that monotonicity of approximation quality with features does not hold in this model. Then a robust model, called fuzzy variable precision rough sets (FVPRS), was developed <ref type="bibr" target="#b57">[58]</ref>. For understandability, we describe the lower and upper approximations of FVPRS as R a AðxÞ ¼ inf </p><formula xml:id="formula_15">8 &gt; &lt; &gt; :<label>ð13Þ</label></formula><p>In computing R a A(x), if A(y) 6 a (y 2 U), then A(y) = a. In other words, the samples with A(y) &lt; a are overlooked. In computing R a AðxÞ, if A(y) P 1 À a (y 2 U), A(y) = 1 À a i.e. the samples with A(y) &gt; 1 À a are overlooked. From the formulae of the lower and upper approximations we conclude that " x 2 U, R a A(x) P a by neglecting some samples that satisfy A(y) 6 a. Similarly, " x 2 U; R a AðxÞ 6 1 À a by neglecting some samples that satisfy A(y) P 1 À a. Compared with fuzzy rough sets, R a A(x) P RA(x) and R a AðxÞ 6 RAðxÞ.</p><p>If A is an arbitrary crisp subset of U, the lower and upper approximations of FVPRS of A degenerate into the following formulae.</p><formula xml:id="formula_16">" x 2 U R a AðxÞ ¼ inf AðyÞ¼0 maxf1 À Rðx; yÞ; ag; R a AðxÞ ¼ sup AðyÞ¼1 minfRðx; yÞ; 1 À ag: 8 &gt; &lt; &gt; :<label>ð14Þ</label></formula><p>R a A(x) P a and R a AðxÞ 6 1 À a still hold.</p><p>Fig. <ref type="figure">1</ref>. The influence of noise on the membership of x to the fuzzy lower approximation of the class.</p><p>However, the lower approximation of FVPRS is not robust to outliers, as shown in Fig. <ref type="figure" target="#fig_2">2</ref>. x i (i = 1, 2) belongs to class 1 marked with balls and y i (i = 1, 2, 3, 4, 5) comes from class 2 marked with squares, where y 3 and y 5 are imaginary samples.</p><p>Here, we consider x 2 and y 1 as outliers. Suppose</p><formula xml:id="formula_17">kx 1 À y 3 k ¼ kx 2 À y 5 k ¼ a. As to x 1 , Rclass 1 ðx 1 Þ ¼ 1 À Rðx 1 ; y 1 Þ ¼ kx 1 À y 1 k; R a class 1 ðx 1 Þ ¼ 1 À Rðx 1 ; y 3 Þ ¼ kx 1 À y 3 k ¼ a: Rclass 1 (x 1 ) &lt; R a class 1 (x 1 )</formula><p>. It seems that the lower approximation of FVPRS is more robust than fuzzy lower approximation. However, y 1 is a mislabeled sample. If we neglect y 1 , the membership of x 1 to the fuzzy lower approximation of class 1 should be hu kx 1 À y 2 k &gt; a.</p><p>As to x 2 ,</p><formula xml:id="formula_18">Rclass 1 ðx 2 Þ ¼ 1 À Rðx 2 ; y 4 Þ ¼ kx 2 À y 4 k; R a class 1 ðx 2 Þ ¼ 1 À Rðx 2 ; y 5 Þ ¼ kx 2 À y 5 k ¼ a: Rclass 1 (x 2 ) &lt; R a class 1 (x 2 )</formula><p>. That is to say we have to neglect some samples around x 2 to make R a class 1 (x 2 ) P a. In fact the samples around x 2 should not be overlooked.</p><p>According to the above analysis, we see that the lower approximation of FVPRS is sensitive to mislabeled samples as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Soft fuzzy rough sets</head><p>Inspired by the idea of soft-margin SVM <ref type="bibr" target="#b8">[9]</ref>, we introduce a robust model of rough sets, named soft fuzzy rough sets. Softmargin SVM is more robust than hard-margin SVM in classification. Hard-margin SVM finds the optimal classification hyperplane to make all the samples classified correctly with a margin. It is not applicable in many real-world problems where the data usually contain noise. And soft-margin SVM is to find an optimal classification hyperplane to make most samples classified correctly with a margin by neglecting a few samples. Soft-margin SVM is to find tradeoff between the size of margin and the classification error, which prevents the classifier overfitting noise.</p><p>First we introduce the definitions of hard distance and soft distance. We explain the soft distance with Fig. <ref type="figure">3</ref>. Sample x comes from class1 and the other samples are from class2, denoted by Y. Here, we suppose d 1 &lt; d 2 &lt; d 3 &lt; d 4 . We can see that HD(x, Y) is d 1 . y 1 is a noisy sample. HD(x, Y) may not exactly reflect the distance between x and Y. In this case soft distance can be used. If we take y 1 as a noisy sample and neglect it, SD(x, Y) should be d 2 ; if y 2 is also taken as a noisy sample, SD(x, Y) should be d 3 . How many samples should be taken as noisy samples in this case? We here add a penalty term to the distance to solve the problem. If we overlook one noisy sample, d(x, y) will be reduced by b.</p><formula xml:id="formula_19">If dðx; y 0 Þ À bm 0 Y (y 0 2 Y) is the largest of d(x, y) À bm Y (" y 2 Y), this distance d(x, y 0</formula><p>) is taken as the soft distance between x and Y.</p><p>Moreover, if b is larger than a certain value, the soft distance degenerates to the hard distance; and if b is smaller than a certain value, many samples would be overlooked. In other words, the larger b is, the less the noisy samples are neglected.</p><p>Next, we use an example to explain the definition of the soft distance.</p><p>Example. Given a set of objects Y = {y </p><p>bis a penalty factor, m Y L is the number of the samples overlooked in computing R S (A)(x) and n Y U is the number of the samples overlooked in computing R S ðAÞðxÞ.</p><p>The essence of Definition 4 is to select two proper samples in U to compute R S A(x) and R S AðxÞ, where the two samples satisfy A(y) 6 A(y L ) and A(y) P A(y U ), respectively. Fig. <ref type="figure">4</ref> illustrates this proposition. In the left figure, the two curves are 1 À R(x, y) and A(y). According to the definition of fuzzy rough sets, RA(x) = 1 À R(x, y L ) = A(y L ). If y L is a noisy sample, we should use a sample that is farther away from x than y L to compute R S A(x). And such samples satisfy A(y) 6 A(y L ). Similarly, if y U is a noisy sample, we should use a sample satisfying A(y) P A(y U ) to compute R S AðxÞ (the right figure in Fig. <ref type="figure">4</ref>).</p><p>Suppose A is a crisp set. The membership of x to the soft fuzzy lower approximation of A is R S ðAÞðxÞ ¼ </p><p>Here R S ðAÞðxÞ can be considered as the soft similarity between x and A.</p><p>Since the soft distance is more robust than the hard distance, soft fuzzy rough sets are more robust to noise than fuzzy rough sets.</p><p>Compared with Zhao's model, the advantage of our model is that it can automatically find optimal samples to compute the soft fuzzy memberships of the lower and upper approximations. In Fig. <ref type="figure" target="#fig_2">2</ref>, FVPRS model lets R a class 1 (x 1 ) = d(x 1 , y 3 ) = a because y 1 is a noisy sample, where a is subjectively set. And a = d(x 1 , y 3 ) is much less than the real value d(x 1 , y 2 ). While our model can automatically find a balance between the memberships and the number of overlooked samples. If the enlargement d(x 1 , y 1 ) À d(x 1 , y 2 ) of R S class 1 (x 1 ) is larger than the cost of misclassifying the sample y 1 , the membership will be d(x 1 , y 2 ); otherwise, R S class 1 (x 1 ) = d(x 1 , y 1 ). Moreover, it is proven that soft fuzzy lower and upper approximations have the following properties.</p><p>Proposition 1. For " A, B 2 F(U), the following statements hold.</p><p>ðP11Þ R S ðAÞ \ R S ðBÞ ¼ R S ðA \ BÞ;</p><formula xml:id="formula_22">R S ðAÞ [ R S ðBÞ ¼ R S ðA [ BÞ:<label>ð23ÞðP12Þ</label></formula><formula xml:id="formula_23">Proof (P11) " x 2 U,<label>ð24Þ</label></formula><p>R S ðAÞðxÞ ^RS ðBÞðxÞ ¼ ð1 À Rðx; arg y 1 sup</p><formula xml:id="formula_24">Aðy 1 Þ6Aðy L A Þ f1 À Rðx; y 1 Þ À bm 1 Y L gÞÞ ^ð1 À Rðx; arg y 2 sup Bðy 2 Þ6Bðy L B Þ f1 À Rðx; y 2 Þ À bm 2 Y L gÞÞ ¼ 1 À Rðx; arg y sup ðA\BÞðyÞ6Aðy L A Þ^Bðy L B Þ¼ðA\BÞðy L ðA\BÞ Þ f1 À Rðx; yÞ À bm Y L gÞ ¼ R S ðA \ BÞðxÞ: Then R S (A) \ R S (B) = R S (A \ B). (P12) " x 2 U, R S ðAÞðxÞ _ R S ðBÞðxÞ ¼ Rðx; arg y 1 inf Aðy 1 ÞPAðy U A Þ fRðx; y 1 Þ þ bn 1 Y U gÞ _ Rðx; arg y 2 inf Bðy 2 ÞPBðy U B Þ fRðx; y 2 Þ þ bn 2 Y U gÞ ¼ Rðx; arg y inf ðA[BÞðyÞPAðy U A Þ_Bðy U B Þ¼ðA[BÞðy U A[B Þ fRðx; yÞ þ bn Y U gÞ ¼ R S ðA [ BÞðxÞ:</formula><p>Then R S ðAÞ [ R S ðBÞ ¼ R S ðA [ BÞ.</p><p>Fig. <ref type="figure" target="#fig_4">5</ref> illustrates (P11). A and B are two fuzzy sets. In terms of the definition of fuzzy rough sets,</p><formula xml:id="formula_25">RAðxÞ ¼ 1 À Rðx; y L A Þ ¼ Aðy L A Þ; RBðxÞ ¼ 1 À Rðx; y LB Þ ¼ Aðy LB Þ and RðA T BÞðxÞ ¼ ð1 À Rðx; y L A ÞÞ T ð1 À Rðx; y LB ÞÞ ¼ Aðy L A Þ. If y L A is a noisy sample, a sample y satisfying ðA T BÞðyÞ &lt; ðA T BÞðy L A Þ will be used to compute R S (A T B)(x).</formula><p>And the sample must be the sample that is used to compute the smaller one of R S A(x) or R S B(x). h Proposition 2. For "A 2 F (U), the following statements hold.</p><formula xml:id="formula_26">ðP21Þ ðR S ðAÞÞ c ¼ R S ðA c Þ;<label>ð25Þ</label></formula><formula xml:id="formula_27">ðP22Þ ðR S ðAÞÞ c ¼ R S ðA c Þ:<label>ð26Þ</label></formula><formula xml:id="formula_28">Proof (P21) " x 2 U, ðR S ðAÞðxÞÞ c ¼ 1 À R S ðAÞðxÞ ¼ Rðx; arg y sup AðyÞ6Aðy L Þ f1 À Rðx; yÞ À bm Y L gÞ;</formula><p>where arg y sup </p><formula xml:id="formula_29">AðyÞ6Aðy L Þ f1 À Rðx; yÞ À bm Y L g ¼ arg y inf AðyÞ6Aðy L Þ fRðx; yÞ þ bm Y L g ¼ arg y inf A c ðyÞPA c ðy L Þ fRðx; yÞ þ bm Y L g: Then ðR S ðAÞðxÞÞ c ¼ Rðx; arg y inf A c ðyÞPA c ðy L Þ fRðx; yÞ þ bm Y L gÞ ¼ R S ðA c ÞðxÞ: (P22) " x 2 U, ðR S ðAÞðxÞÞ c ¼ 1 À R S<label>ðAÞðxÞ</label></formula><p>Soft fuzzy dependency (SFD) can also be used to evaluate features. Section 3 illustrates that soft fuzzy lower approximation is robust to the mislabeled samples. We consider that soft fuzzy dependency is also robust to the mislabeled samples in feature evaluation. Based on the soft fuzzy dependency we design a feature selection algorithm, shown in Table <ref type="table">1</ref>. The algorithm employs SFD as the feature evaluation function and the sequential forward selection as the search strategy. The output of the algorithm is a feature ranking F 0 ¼ ff 0 1 ; f 0 2 ; . . . ; f 0 jF 0 j g. Given the set F 0 kÀ1 with k À 1 features selected, the k 0 th feature is determined by max</p><formula xml:id="formula_31">f 2FÀF 0 kÀ1 fSFD ðF 0 kÀ1 [ff gÞ ðDÞg:<label>ð29Þ</label></formula><p>With the ranking, we can get n feature subsets F 0 1 ¼ ff 0 1 g; F 0 2 ¼ ff 0 1 ; f 0 2 g; . . . ; F 0 jF 0 j ¼ ff 0 1 ; f 0 2 ; . . . ; f 0 jF 0 j g. Next, we use KNN classifier to cross-validate the classification accuracy of the data with these feature subsets. The feature subset with the highest classification accuracy is the final feature subset. In this work, we use this algorithm to validate the robustness of soft fuzzy dependency in Section 6.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Robustness evaluation</head><p>We wish that the feature quality computed with an evaluation function evaluation does not vary much if the samples are corrupted by a little noise. We take the robustness of measures as the similarity between the evaluation results computed with raw data and noisy data. Intuitively, the larger the similarity is, the more robust the evaluation function is.</p><p>In this work, we generate some noisy datasets from the given sets. The noisy datasets are generated as follows. Take the raw data containing n samples and m features as an example. Firstly, we randomly draw 3i% (i = 1,. . . , k) samples, and then give them labels that are distinct from their original labels. We get i-level noisy datasets for the raw dataset.</p><p>Assume W = {w 1 , w 2 , . . . , w m } and W 0 ¼ fw 0 1 ; w 0 2 ; . . . ; w m 0g are the significance vectors of features computed with the raw data and the noisy data, respectively, where w i (i = 1,2,. . . , m) and w 0 i (i = 1,2,. . . , m) are the significance values of the i 0 th feature with the raw data and the noisy data. To compute similarity between W and W 0 , we use Pearson's correlation coefficient</p><formula xml:id="formula_32">S w ðW; W 0 Þ ¼ P m i¼1 ðw i À WÞðw 0 i À W 0 Þ P m i¼1 ðw i À WÞ 2 P m i¼1 ðw 0 i À W 0 Þ 2 h i 1=2 ;<label>ð30Þ</label></formula><p>where S w (W, W 0 ) takes values in [-1, 1]. The larger the value of S w (W, W 0 ) is, the larger the similarity is. S w (W, W 0 ) = 1 means that W and W 0 are perfectly linearly correlated. S w (W, W 0 ) = 0 means there is no linear correlation between W and W 0 . S w (W, W 0 ) = À1 means W and W 0 are inverse-correlated.</p><p>As there are k evaluation results we compute the similarity between a pair of evaluations, and then we get a similarity matrix</p><formula xml:id="formula_33">S ¼ s 11 s 12 Á Á Á s 1k s 21 s 22 Á Á Á s 2k . . . . . . . . . . . . s k1 s k2 Á Á Á s kk 0 B B B B @ 1 C C C C A ;<label>ð31Þ</label></formula><p>where s ij is the similarity between the i 0 th and j 0 th evaluation results.</p><p>In order to measure the similarity of all the evaluation results, in <ref type="bibr" target="#b16">[17]</ref>, the authors computed the similarity matrix with</p><formula xml:id="formula_34">TS ¼ À 1 k X k j¼1 log X k i¼1 s ij k ;<label>ð32Þ</label></formula><p>where TS 2 [0, logk]. If " i, j, s ij = 1, which means the k evaluation results are the same, then TS = 0. In this case, the feature evaluation measure is robust. If " ij, s ij = 0, S is an identity matrix, then TS = logk. We consider the similarity matrix is not stable and the measure is not robust. In Section 6.1, we use TS to measure the robustness of feature evaluation measures.</p><p>Table <ref type="table">1</ref> Feature selection algorithm.</p><p>Input: X, F X is a sample set and F is a feature set. Output:</p><formula xml:id="formula_35">F 0 F 0 is a feature ranking. Begin Initialize F 0 = / while F -/ Find f ¼ arg f max f 2F fSFD ðF 0 [ff gÞ ðDÞg, F 0 = F 0 [ {f}, F = F À {f} End Return F 0 End</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental analysis</head><p>In this section, we first discuss the role of parameter b with an experiment. And then the robustness on soft fuzzy dependency is validated from two aspects. One is to validate the robustness on soft fuzzy dependency in evaluating a single feature using the methods described in Section 5, and the other one is to validate the robustness of soft fuzzy dependency in feature selection. The experiments are performed on nine data sets collected from UCI <ref type="bibr" target="#b2">[3]</ref>. The summaries of the data sets are given in Table <ref type="table" target="#tab_3">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Parameter b</head><p>The parameter b in the definition of the soft distance is used to make tradeoff in computing the soft distance. If b is too small, more samples would be overlooked in computing soft distance. And if b is too large, the soft distance will degenerate to the hard distance. We give an experiment to show the relationship between b and soft distance. Data wine is used here.</p><p>The experiment is to compute the average soft distance of all the samples and the corresponding number of the overlooked samples in computing the soft distance. The results are shown in Fig. <ref type="figure" target="#fig_5">6</ref>.</p><p>The first figure illustrates the relationship between the average soft distance of a sample and the value of b. It shows the average soft distance decreases if the value of b increases and it converges to a certain value. The second plot illustrates the relationship between the average number of the overlooked samples and the value of b. We can see that the average number of the overlooked samples also decreases along with the increasing of the value of b and converges to zero.</p><p>As we do not expect the soft distance just increases a little after many samples are ignored, we set that one sample is not ignored until the increment of soft distance is not less than b. For example, b = 0.1 means if the soft distance increases 0.1, we overlook one sample at most. In this work, we set b = 0.1. The subsequent experiments show that 0.1 is a good choice for b.  6.2. Robustness on soft fuzzy dependency for evaluating a single feature</p><p>We test the robustness of the soft fuzzy dependency in evaluating a single feature in this section. Here, we call the data sets downloaded from UCI raw datasets. With a raw dataset we can generate k noisy data sets using the method referred to in Section 5. In this work, k = 10 and the maximal noise level is 30%.</p><p>Firstly, we compute the soft fuzzy dependency of decision on each feature with the raw data sets and ten noisy data sets. Here, we take formula <ref type="bibr" target="#b9">(10)</ref> as similarity function (d = 0.15 and k Á k is 2-norm.) and let b = 0.1. Then we get a raw soft fuzzy dependency value (computed with a raw data set) and ten noisy soft fuzzy dependency values (computed with ten noisy data sets) of decision on each feature. Similarly, we also can compute a raw fuzzy dependency value and ten noisy fuzzy dependency values of decision on each feature. Fig. <ref type="figure" target="#fig_6">7</ref> shows the eleven evaluation results of decision on each feature.</p><p>For each subgraph, x axis is the feature index and y axis is the values of soft fuzzy dependency or fuzzy dependency. Eleven curves show eleven dependency values computed with a raw data set and ten noisy data sets. As to glass, we can see that the eleven curves of soft fuzzy dependency (SFD) are more similar than that of fuzzy dependency (FD). Notice that the dependency values are unitary. As Section 5 refers to, the larger the similarity is, the more robust the feature evaluation measure is. Thereby, soft fuzzy dependency function is more robust than fuzzy dependency on glass. As to musk, yeast and ionosphere, we can get the same conclusion. Next, we use Pearson's correlation coefficient to compute the similarity between a raw dependency and a noisy dependency, and compare the robustness of SFD with FD and neighborhood dependency (ND) by the values. Here, the raw dependency denotes a vector composed of the dependency values of decision on each feature, where the values are computed with a raw data set. And the values of the noisy dependency are computed with a noisy data set. The results, calculated with formula <ref type="bibr" target="#b29">(30)</ref>, are shown in Tables <ref type="table" target="#tab_4">3</ref><ref type="table" target="#tab_5">4</ref><ref type="table" target="#tab_6">5</ref>, where 6%, 12%, 18%, 24% and 30% are noise levels.</p><p>Table <ref type="table" target="#tab_4">3</ref> shows the similarity between raw fuzzy dependency and noisy fuzzy dependency, Table <ref type="table" target="#tab_5">4</ref> shows the similarity between raw neighborhood dependency and noisy neighborhood dependency and Table <ref type="table" target="#tab_6">5</ref> shows the similarity between raw soft fuzzy dependency and noisy soft fuzzy dependency. The three tables show that the average similarity values of soft fuzzy dependency are the largest i.e. soft fuzzy dependency is more robust than other two dependency functions.</p><p>Moreover, we use TS to measure the robustness of FD, ND and SFD. The results are shown in Table <ref type="table" target="#tab_7">6</ref>. The average evaluation values of FD and ND are 0.58, while the average value of SFD is 0.20. As we know, the smaller the value is, the more robust the measure is. Consequently, soft fuzzy dependency is more robust than fuzzy dependency and neighborhood dependency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Robustness of soft fuzzy dependency in feature selection</head><p>Next, we test the robustness on soft fuzzy dependency for feature selection with the algorithm in Table <ref type="table">1</ref>.  <ref type="table">1</ref> with a real-world data set. Next, we use KNN (k = 3) classifier to cross-validate the classification accuracy of the data set with the feature subsets F 0 m (m = 1,2,. . . , jF 0 j) composed of the first m features in the ranking. The feature subset with the highest classification accuracy is the final feature subset. Then we replace SFD with FD and ND, respectively, and select features with the same method.</p><p>The number of features selected and classification accuracies are shown in Table <ref type="table" target="#tab_8">7</ref>, where n is the number of features selected and RawAcc is classification accuracy. It is shown that, with SFD as the feature evaluation, the feature subsets selected can produce higher classification accuracies. In this work, we use the classification accuracies of feature subsets to evaluate the robustness of measures. The higher the classification accuracy is, the stronger the robustness of the measure is. Therefore, SFD is more robust than FD and ND.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2.">Synthetic data</head><p>We conduct the proposed feature selection algorithm on the noisy synthetic data. We generate a set of data with 100 samples and 13 features. In addition, ten noisy data sets are generated from the synthetic data, where the noise levels are i% (i = 1,2,. . . , 10), respectively. With these set of data we perform select features using SFD, FD and ND as feature evaluation functions, respectively. The best four features are shown in Table <ref type="table" target="#tab_9">8</ref>. It is shown that, with FD or ND as feature evaluation functions, the best four features are different from each other if the noise levels are different, while those with SFD are almost the same. Take the data containing 4% noise as an example. With the noisy data the best four features are {13, 8, 11, 5}, {13, 8, 9, 10} and {7, 12, 11, 13} using FD, ND and SFD, respectively. With the raw data the feature subsets are {7, 12, 11, 8}, {7, 12, 11, 6} and {7, 12, 11, 13}, respectively. Fig. <ref type="figure" target="#fig_7">8</ref> gives the comparison of classification performance.</p><p>For ''FD (raw data)", ''ND (raw data)" and ''SFD (raw data)", the sixteen subfigures illustrate the two-dimensional distribution of the synthetic data with the first four features selected, where the features are selected with raw data and the feature evaluation are fuzzy dependency, neighborhood dependency and soft fuzzy dependency, respectively. While, for ''FD (noisy data)", ''ND (noisy data)" and ''SFD (noisy data)", the features are selected with noisy data. We can see the classification performance of ''SFD (noisy data)" is as well as ''SFD (noisy data)" while "FD (noisy data)" and ''ND (noisy data)" are worse than ''FD (raw data)" and ''ND (raw data)", respectively. That is to say noise has a great influence on the algorithms taking FD and ND as the feature evaluation. But the algorithm using SFD is more robust to noise.</p><p>The numbers of selected features and classification accuracies are shown in Table <ref type="table" target="#tab_10">9</ref>, where n is the number of features selected, Raw is the classification accuracy of raw data (synthetic data) (That is to say the features, selected with raw data, is used to classify the raw data) and Noisy is the classification accuracy of noisy data (That is to say the features, selected with noisy data, is used to classify the noisy data). We can see that if we use FD and ND the numbers of selected features are influenced by noise. But the numbers do not vary if SFD is used. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3.">Noisy data created from real-world data</head><p>In this section, the classification accuracies are computed as follows. First, we select features with the noisy datasets and obtain feature ranking. Next, we use KNN (k = 3) classifier to compute the classification accuracy of the raw data with the selected feature subsets composed of the first m (m = 1,2,. . . , jF 0 j) features in the ranking. The feature subset with the highest classification accuracy is output as the final feature subset. Take wine and sonar data as examples. In this work, we suppose there is no noise in the raw datasets of wine and sonar.</p><p>The numbers of the selected features and the corresponding performance are displayed in Tables <ref type="table" target="#tab_11">10</ref> and<ref type="table" target="#tab_12">11</ref>, where n is the number of features selected and RawAcc is classification accuracy on raw data (wine and sonar). It is clear that the feature subsets selected, where SFD is taken as the measure, can produce higher classification accuracies. It shows that soft fuzzy dependency is more robust than fuzzy dependency and neighborhood dependency. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>Feature selection plays an important role in pattern classification systems. Feature evaluation functions used to compute the quality of features is a key issue in feature selection. In the rough set theory, dependency and fuzzy dependency has been successfully used to evaluate features. However, we find these function are not robust. In practice, data are usually corrupted by noise. So it is desirable to design robust models of rough sets.</p><p>Inspired by the idea of soft-margin SVM, we introduce a robust model of rough sets called soft fuzzy rough sets. The new model can reduce the influence of noise on the computation of soft fuzzy lower and upper approximations by overlooking some samples which are considered as noisy samples. In computing the membership of a sample to the soft fuzzy approximations, we make a tradeoff between the memberships and the number of the samples overlooked. Moreover, we discuss the properties of the new model. Then with the soft fuzzy lower approximation we give the definition of the soft fuzzy dependency and use it to evaluate features in feature selection. Finally, we test the robustness on soft fuzzy dependency function for feature evaluation and selection. The experimental results show that the soft fuzzy dependency function is effective in dealing with noisy data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>AðyÞ6amaxð1À</head><label></label><figDesc>Rðx; yÞ; aÞ ^inf AðyÞ&gt;a maxð1 À Rðx; yÞ; AðyÞÞ; R a AðxÞ ¼ sup AðyÞP1Àa minðRðx; yÞ; 1 À aÞ _ sup AðyÞ&lt;1Àa minðRðx; yÞ; AðyÞÞ:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Definition 2 .Definition 3 .</head><label>23</label><figDesc>Given an object x and a set of objects Y, the hard distance between x and Y is defined as HDðx; YÞ ¼ min y2Y dðx; yÞ; ð15Þ where d is a distance function. As we all know, the statistical minimum is sensitive to noise and not robust. We introduce a new definition of distance. Given an object x and a set of objects Y, the soft distance between x and Y is defined as SDðx; YÞ ¼ arg dðx;yÞ sup y2Y fdðx; yÞ À bm Y g; ð16Þ where d is a distance function, b is a penalty factor and m Y = j{y i jd(x, y i ) &lt; d(x, y)}j.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The influence of noise on R a class 1 (x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Soft distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. R S (A) \ R S (B) = R S (A \ B).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Variation of soft distance and number of overlooked samples with b.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Comparison of fuzzy dependency and soft fuzzy dependency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>FDFig. 8 .</head><label>8</label><figDesc>Fig. 8. Classification performance comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1 À Rðx; y AL Þ; ð19Þ Rðx; yÞ À bm Y L g ¼ arg y sup AðyÞ¼0 fdðx; yÞ À bm Y L g ¼ arg y SDðx; U À AÞ: ð20Þ So R S (A)(x) can be viewed as the soft distance from x to U À A. Similarly, the membership of x to the soft fuzzy upper approximation of A is Rðx; yÞ À bn Y U g ¼ arg y sup AðyÞ¼1 fdðx; yÞ À bn Y U g ¼ arg y SDðx; AÞ:</figDesc><table><row><cell>R S ðAÞðxÞ ¼ Rðx; y AU Þ;</cell><cell>ð21Þ</cell></row></table><note><p>where y AL ¼ arg y sup AðyÞ¼0 f1 À where y AU ¼ arg y inf AðyÞ¼1 fRðx; yÞ þ bn Y U g ¼ arg y sup AðyÞ¼1 f1 À</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Given a decision table hU, C [ Di, U is a nonempty universe, C is the set of attributes and D is the decision attribute. " B 2 C, the membership of an object x 2 U belonging to the soft positive region of D on B is defined as</figDesc><table><row><cell cols="3">POS S B ðDÞðxÞ ¼ sup X2U=D</cell><cell cols="2">B S ðXÞðxÞ:</cell><cell>ð27Þ</cell></row><row><cell cols="5">The soft fuzzy dependency of decision D on B is defined as</cell></row><row><cell>c S B ðDÞ ¼</cell><cell>P</cell><cell cols="2">x2U POS S B ðDÞðxÞ jUj</cell><cell>:</cell></row></table><note><p><p><p><p>¼ 1 À Rðx; arg y inf AðyÞPAðy U Þ fRðx; yÞ þ bn Y U gÞ; where arg y inf</p>AðyÞPAðy U Þ fRðx; yÞ þ bn Y U g ¼ arg y sup AðyÞPAðy U Þ f1 À Rðx; yÞ À bn Y U g ¼ arg y sup A c ðyÞ6A c ðy U Þ f1 À Rðx; yÞ À bn Y U g: Then ðR S ðAÞðxÞÞ c ¼ 1 À Rðx; arg y sup A c ðyÞ6A c ðy U Þ f1 À Rðx; yÞ À bn Y U gÞ ¼ R S ðA c ÞðxÞ Therefore, ðR S ðAÞÞ c ¼ R S ðA c Þ and ðR S ðAÞÞ c ¼ R S ðA c Þ hold.</p>h</p>4. Soft fuzzy dependency based feature selection Definition 5.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>Summaries of data sets.</figDesc><table><row><cell>Data</cell><cell></cell><cell></cell><cell></cell><cell>Samples</cell><cell></cell><cell>Features</cell><cell></cell><cell>Classes</cell></row><row><cell>Wine</cell><cell></cell><cell></cell><cell></cell><cell>178</cell><cell></cell><cell>13</cell><cell></cell><cell>3</cell></row><row><cell>WDBC</cell><cell></cell><cell></cell><cell></cell><cell>569</cell><cell></cell><cell>30</cell><cell></cell><cell>2</cell></row><row><cell>Sonar</cell><cell></cell><cell></cell><cell></cell><cell>208</cell><cell></cell><cell>60</cell><cell></cell><cell>2</cell></row><row><cell>Ionosphere</cell><cell></cell><cell></cell><cell></cell><cell>351</cell><cell></cell><cell>34</cell><cell></cell><cell>2</cell></row><row><cell>Glass</cell><cell></cell><cell></cell><cell></cell><cell>210</cell><cell></cell><cell>13</cell><cell></cell><cell>6</cell></row><row><cell>Musk</cell><cell></cell><cell></cell><cell></cell><cell>467</cell><cell></cell><cell>166</cell><cell></cell><cell>2</cell></row><row><cell cols="3">Pima-indians-diabetes</cell><cell></cell><cell>768</cell><cell></cell><cell>8</cell><cell></cell><cell>2</cell></row><row><cell>Shutter-trn</cell><cell></cell><cell></cell><cell></cell><cell>43500</cell><cell></cell><cell>10</cell><cell></cell><cell>5</cell></row><row><cell>Yeast</cell><cell></cell><cell></cell><cell></cell><cell>1484</cell><cell></cell><cell>7</cell><cell></cell><cell>10</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>soft distance</cell><cell>0.7 0.8 0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">soft distance</cell></row><row><cell></cell><cell></cell><cell>0.05</cell><cell>0.1</cell><cell>0.15</cell><cell>0.2</cell><cell>0.25</cell><cell>0.3</cell><cell>0.35</cell></row><row><cell>number of the noisy samples</cell><cell>0 5 10 15</cell><cell>0.05</cell><cell>0.1</cell><cell>0.15</cell><cell>0.2</cell><cell cols="2">0.25 noisy samples 0.3</cell><cell>0.35</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>Similarity between raw fuzzy dependency and noisy fuzzy dependency.</figDesc><table><row><cell>Data</cell><cell>6%</cell><cell>12%</cell><cell>18%</cell><cell>24%</cell><cell>30%</cell></row><row><cell>Wine</cell><cell>0.98</cell><cell>0.74</cell><cell>0.39</cell><cell>0.05</cell><cell>0.34</cell></row><row><cell>WDBC</cell><cell>0.89</cell><cell>0.67</cell><cell>0.69</cell><cell>À0.30</cell><cell>À0.33</cell></row><row><cell>Sonar</cell><cell>0.79</cell><cell>0.77</cell><cell>0.55</cell><cell>0.57</cell><cell>0.37</cell></row><row><cell>Ionosphere</cell><cell>0.32</cell><cell>0.46</cell><cell>0.22</cell><cell>0.31</cell><cell>0.42</cell></row><row><cell>Glass</cell><cell>0.80</cell><cell>0.67</cell><cell>0.42</cell><cell>0.52</cell><cell>0.48</cell></row><row><cell>Musk</cell><cell>0.97</cell><cell>0.87</cell><cell>0.77</cell><cell>0.30</cell><cell>0.28</cell></row><row><cell>Pima-indians-diabetes</cell><cell>0.97</cell><cell>0.98</cell><cell>0.97</cell><cell>0.86</cell><cell>0.98</cell></row><row><cell>Shutter-trn</cell><cell>À0.15</cell><cell>0.02</cell><cell>0.08</cell><cell>0.99</cell><cell>0.81</cell></row><row><cell>Yeast</cell><cell>À0.26</cell><cell>0.72</cell><cell>0.72</cell><cell>À0.27</cell><cell>À0.26</cell></row><row><cell>Average</cell><cell>0.59</cell><cell>0.65</cell><cell>0.53</cell><cell>0.33</cell><cell>0.34</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>Similarity between raw neighborhood dependency and noisy neighborhood dependency.</figDesc><table><row><cell>Data</cell><cell>6%</cell><cell>12%</cell><cell>18%</cell><cell>24%</cell><cell>30%</cell></row><row><cell>Wine</cell><cell>0.92</cell><cell>0.02</cell><cell>0.40</cell><cell>À0.17</cell><cell>0.00</cell></row><row><cell>WDBC</cell><cell>0.84</cell><cell>À0.11</cell><cell>0.63</cell><cell>À0.48</cell><cell>À0.26</cell></row><row><cell>Sonar</cell><cell>0.98</cell><cell>0.84</cell><cell>0.61</cell><cell>0.55</cell><cell>0.36</cell></row><row><cell>Ionosphere</cell><cell>0.30</cell><cell>0.37</cell><cell>0.00</cell><cell>0.40</cell><cell>0.00</cell></row><row><cell>Glass</cell><cell>0.20</cell><cell></cell><cell>0.04</cell><cell>0.22</cell><cell>0.16</cell></row><row><cell>Musk</cell><cell>0.85</cell><cell>0.27</cell><cell>0.21</cell><cell>0.05</cell><cell>0.07</cell></row><row><cell>Pima-indians-diabetes</cell><cell>0.77</cell><cell>0.77</cell><cell>0.77</cell><cell>0.00</cell><cell>0.77</cell></row><row><cell>Shutter-trn</cell><cell>0.98</cell><cell>À0.33</cell><cell>À0.33</cell><cell>À0.33</cell><cell>À0.33</cell></row><row><cell>Yeast</cell><cell>0.63</cell><cell>1.00</cell><cell>0.63</cell><cell>0.13</cell><cell>0.13</cell></row><row><cell>Average</cell><cell>0.71</cell><cell>0.32</cell><cell>0.32</cell><cell>0.04</cell><cell>0.10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc>Similarity between raw soft fuzzy dependency and noisy soft fuzzy dependency.</figDesc><table><row><cell>Data</cell><cell>6%</cell><cell>12%</cell><cell>18%</cell><cell>24%</cell><cell>30%</cell></row><row><cell>Wine</cell><cell>0.89</cell><cell>0.81</cell><cell>0.47</cell><cell>0.77</cell><cell>0.71</cell></row><row><cell>WDBC</cell><cell>0.69</cell><cell>0.73</cell><cell>0.80</cell><cell>0.74</cell><cell>0.69</cell></row><row><cell>Sonar</cell><cell>0.87</cell><cell>0.96</cell><cell>0.49</cell><cell>0.68</cell><cell>0.50</cell></row><row><cell>Ionosphere</cell><cell>0.96</cell><cell>0.96</cell><cell>0.95</cell><cell>0.93</cell><cell>0.93</cell></row><row><cell>Glass</cell><cell>0.99</cell><cell>0.99</cell><cell>0.99</cell><cell>0.98</cell><cell>0.98</cell></row><row><cell>Musk</cell><cell>0.96</cell><cell>0.95</cell><cell>0.98</cell><cell>0.95</cell><cell>0.88</cell></row><row><cell>Pima-indians-diabetes</cell><cell>0.99</cell><cell>0.99</cell><cell>0.96</cell><cell>0.96</cell><cell>0.95</cell></row><row><cell>Shutter-trn</cell><cell>0.99</cell><cell>0.96</cell><cell>0.88</cell><cell>0.69</cell><cell>À0.03</cell></row><row><cell>Yeast</cell><cell>0.98</cell><cell>0.98</cell><cell>0.99</cell><cell>0.98</cell><cell>0.98</cell></row><row><cell>Average</cell><cell>0.92</cell><cell>0.92</cell><cell>0.83</cell><cell>0.85</cell><cell>0.73</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6</head><label>6</label><figDesc>Robustness comparison of soft fuzzy dependency, fuzzy dependency and neighborhood dependency.</figDesc><table><row><cell>Data</cell><cell>FD</cell><cell>ND</cell><cell>SFD</cell></row><row><cell>Wine</cell><cell>0.56</cell><cell>0.67</cell><cell>0.07</cell></row><row><cell>WDBC</cell><cell>0.74</cell><cell>0.69</cell><cell>0.07</cell></row><row><cell>Sonar</cell><cell>0.77</cell><cell>0.88</cell><cell>0.71</cell></row><row><cell>Ionosphere</cell><cell>0.71</cell><cell>0.81</cell><cell>0.40</cell></row><row><cell>Glass</cell><cell>0.22</cell><cell>0.56</cell><cell>0.01</cell></row><row><cell>Musk</cell><cell>0.56</cell><cell>0.80</cell><cell>0.00</cell></row><row><cell>Pima-indians-diabetes</cell><cell>0.56</cell><cell>0.24</cell><cell>0.02</cell></row><row><cell>Shutter-trn</cell><cell>0.50</cell><cell>0.23</cell><cell>0.33</cell></row><row><cell>Yeast</cell><cell>0.61</cell><cell>0.34</cell><cell>0.00</cell></row><row><cell>Synthetic</cell><cell>100</cell><cell>13</cell><cell>2</cell></row><row><cell>Average</cell><cell>0.58</cell><cell>0.58</cell><cell>0.20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7</head><label>7</label><figDesc>Numbers of features selected and classification accuracies (%) on real-world data.</figDesc><table><row><cell>Data</cell><cell>FD</cell><cell></cell><cell>ND</cell><cell></cell><cell>SFD</cell><cell></cell></row><row><cell></cell><cell>n</cell><cell>RawAcc</cell><cell>n</cell><cell>RawAcc</cell><cell>n</cell><cell>RawAcc</cell></row><row><cell>Wine</cell><cell>7</cell><cell>95.9 ± 4.2</cell><cell>6</cell><cell>95.4 ± 5.3</cell><cell>6</cell><cell>97.1 ± 4.6</cell></row><row><cell>WDBC</cell><cell>7</cell><cell>89.9 ± 5.2</cell><cell>8</cell><cell>93.7 ± 2.6</cell><cell>8</cell><cell>94.7 ± 2.0</cell></row><row><cell>Sonar</cell><cell>8</cell><cell>81.2 ± 8.8</cell><cell>7</cell><cell>77.0 ± 9.5</cell><cell>10</cell><cell>84.9 ± 6.6</cell></row><row><cell>Ionosphere</cell><cell>10</cell><cell>94.2 ± 2.4</cell><cell>6</cell><cell>90.3 ± 5.6</cell><cell>9</cell><cell>91.0 ± 3.8</cell></row><row><cell>Glass</cell><cell>9</cell><cell>66.3 ± 8.0</cell><cell>6</cell><cell>68.5 ± 8.1</cell><cell>6</cell><cell>69.3 ± 7.7</cell></row><row><cell>Pima-indians-diabetes</cell><cell>8</cell><cell>70.8 ± 3.7</cell><cell>8</cell><cell>70.8 ± 3.7</cell><cell>4</cell><cell>71.6 ± 3.5</cell></row><row><cell>Average</cell><cell>8</cell><cell>83.1</cell><cell>7</cell><cell>82.6</cell><cell>7</cell><cell>84.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8</head><label>8</label><figDesc>First four features of feature rankings.</figDesc><table><row><cell>Noise levels (%)</cell><cell>FD</cell><cell>ND</cell><cell>SFD</cell></row><row><cell>0</cell><cell>7, 12, 11, 8</cell><cell>7, 12, 11, 6</cell><cell>7, 12, 11, 13</cell></row><row><cell>1</cell><cell>7, 5, 3, 13</cell><cell>7, 5, 1, 3</cell><cell>7, 12, 11, 13</cell></row><row><cell>2</cell><cell>13, 9, 7, 8</cell><cell>13, 9, 6, 2</cell><cell>7, 12, 11, 13</cell></row><row><cell>3</cell><cell>11, 2, 12, 5</cell><cell>11, 2, 8, 1</cell><cell>7, 12, 11, 13</cell></row><row><cell>4</cell><cell>13, 8, 11, 5</cell><cell>13, 8, 9, 10</cell><cell>7, 12, 11, 13</cell></row><row><cell>5</cell><cell>13, 1, 12, 5</cell><cell>7, 1, 3, 10</cell><cell>7, 12, 11, 13</cell></row><row><cell>6</cell><cell>13, 2, 12, 3</cell><cell>13, 2, 8, 4</cell><cell>7, 12, 13, 11</cell></row><row><cell>7</cell><cell>12, 3, 13, 1</cell><cell>7, 3, 1, 5</cell><cell>7, 12, 11, 13</cell></row><row><cell>8</cell><cell>7, 3, 1, 5</cell><cell>7, 1, 8, 5</cell><cell>7, 12, 11, 8</cell></row><row><cell>9</cell><cell>7, 5, 1, 8</cell><cell>8, 6, 5, 13</cell><cell>7, 12, 11, 13</cell></row><row><cell>10</cell><cell>13, 9, 2, 10</cell><cell>13, 9, 10, 8</cell><cell>7, 2, 12, 10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9</head><label>9</label><figDesc>Numbers of features selected and classification accuracies (%).</figDesc><table><row><cell>Noise levels (%)</cell><cell>FD</cell><cell></cell><cell></cell><cell></cell><cell>ND</cell><cell></cell><cell></cell><cell></cell><cell>SFD</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Raw</cell><cell></cell><cell>Noisy</cell><cell></cell><cell>Raw</cell><cell></cell><cell>Noisy</cell><cell></cell><cell>Raw</cell><cell></cell><cell>Noisy</cell><cell></cell></row><row><cell></cell><cell>n</cell><cell>Acc</cell><cell>n</cell><cell>Acc</cell><cell>n</cell><cell>Acc</cell><cell>n</cell><cell>Acc</cell><cell>n</cell><cell>Acc</cell><cell>n</cell><cell>Acc</cell></row><row><cell>0</cell><cell>1</cell><cell>100</cell><cell>1</cell><cell>100</cell><cell>1</cell><cell>100</cell><cell>1</cell><cell>100</cell><cell>1</cell><cell>100</cell><cell>1</cell><cell>100</cell></row><row><cell>1</cell><cell>1</cell><cell>100</cell><cell>1</cell><cell>99</cell><cell>1</cell><cell>100</cell><cell>1</cell><cell>99</cell><cell>1</cell><cell>100</cell><cell>1</cell><cell>99</cell></row><row><cell>2</cell><cell>3</cell><cell>100</cell><cell>3</cell><cell>98</cell><cell>11</cell><cell>100</cell><cell>11</cell><cell>98</cell><cell>1</cell><cell>100</cell><cell>1</cell><cell>98</cell></row><row><cell>3</cell><cell>3</cell><cell>100</cell><cell>3</cell><cell>97</cell><cell>9</cell><cell>100</cell><cell>9</cell><cell>97</cell><cell>1</cell><cell>100</cell><cell>1</cell><cell>97</cell></row><row><cell>4</cell><cell>6</cell><cell>100</cell><cell>6</cell><cell>96</cell><cell>11</cell><cell>100</cell><cell>11</cell><cell>96</cell><cell>1</cell><cell>100</cell><cell>1</cell><cell>96</cell></row><row><cell>5</cell><cell>3</cell><cell>100</cell><cell>3</cell><cell>95</cell><cell>1</cell><cell>100</cell><cell>1</cell><cell>95</cell><cell>1</cell><cell>100</cell><cell>1</cell><cell>95</cell></row><row><cell>6</cell><cell>5</cell><cell>100</cell><cell>3</cell><cell>94</cell><cell>9</cell><cell>100</cell><cell>9</cell><cell>95</cell><cell>1</cell><cell>100</cell><cell>1</cell><cell>95</cell></row><row><cell>7</cell><cell>3</cell><cell>100</cell><cell>3</cell><cell>94</cell><cell>1</cell><cell>100</cell><cell>1</cell><cell>94</cell><cell>1</cell><cell>100</cell><cell>1</cell><cell>94</cell></row><row><cell>8</cell><cell>1</cell><cell>100</cell><cell>1</cell><cell>93</cell><cell>1</cell><cell>100</cell><cell>1</cell><cell>93</cell><cell>1</cell><cell>100</cell><cell>1</cell><cell>93</cell></row><row><cell>9</cell><cell>1</cell><cell>100</cell><cell>1</cell><cell>92</cell><cell>1</cell><cell>100</cell><cell>1</cell><cell>92</cell><cell>1</cell><cell>100</cell><cell>1</cell><cell>92</cell></row><row><cell>10</cell><cell>10</cell><cell>100</cell><cell>4</cell><cell>91</cell><cell>11</cell><cell>100</cell><cell>3</cell><cell>91</cell><cell>1</cell><cell>100</cell><cell>1</cell><cell>91</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10</head><label>10</label><figDesc>Numbers of selected features and classification accuracies (%) on wine.</figDesc><table><row><cell>Noise levels (%)</cell><cell>FD</cell><cell></cell><cell>ND</cell><cell></cell><cell>SFD</cell><cell></cell></row><row><cell></cell><cell>n</cell><cell>RawAcc</cell><cell>n</cell><cell>RawAcc</cell><cell>n</cell><cell>RawAcc</cell></row><row><cell>3</cell><cell>3</cell><cell>95.0 ± 4.0</cell><cell>6</cell><cell>97.7 ± 4.2</cell><cell>5</cell><cell>97.2 ± 3.2</cell></row><row><cell>6</cell><cell>3</cell><cell>93.2 ± 5.3</cell><cell>6</cell><cell>93.8 ± 7.8</cell><cell>5</cell><cell>97.2 ± 3.2</cell></row><row><cell>9</cell><cell>6</cell><cell>97.1 ± 4.3</cell><cell>7</cell><cell>94.1 ± 4.1</cell><cell>6</cell><cell>97.7 ± 4.4</cell></row><row><cell>12</cell><cell>5</cell><cell>96.5 ± 4.2</cell><cell>5</cell><cell>92.7 ± 5.3</cell><cell>7</cell><cell>96.5 ± 3.8</cell></row><row><cell>15</cell><cell>9</cell><cell>97.2 ± 3.0</cell><cell>7</cell><cell>96.6 ± 5.2</cell><cell>6</cell><cell>96.6 ± 4.3</cell></row><row><cell>18</cell><cell>8</cell><cell>96.5 ± 5.6</cell><cell>8</cell><cell>96.6 ± 6.2</cell><cell>7</cell><cell>98.3 ± 3.0</cell></row><row><cell>21</cell><cell>7</cell><cell>95.4 ± 4.6</cell><cell>6</cell><cell>93.8 ± 6.8</cell><cell>6</cell><cell>96.6 ± 4.1</cell></row><row><cell>24</cell><cell>10</cell><cell>96.6 ± 3.0</cell><cell>4</cell><cell>91.6 ± 5.2</cell><cell>7</cell><cell>97.7 ± 4.7</cell></row><row><cell>27</cell><cell>9</cell><cell>97.1 ± 3.0</cell><cell>7</cell><cell>96.6 ± 5.0</cell><cell>6</cell><cell>93.4 ± 6.7</cell></row><row><cell>30</cell><cell>10</cell><cell>95.9 ± 5.8</cell><cell>8</cell><cell>97.1 ± 6.6</cell><cell>8</cell><cell>97.6 ± 6.1</cell></row><row><cell>Average</cell><cell>7</cell><cell>96.3</cell><cell>6</cell><cell>95.4</cell><cell>6</cell><cell>97.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11</head><label>11</label><figDesc>Numbers of selected features and classification accuracies (%) on sonar.</figDesc><table><row><cell>Noise levels (%)</cell><cell>FD</cell><cell></cell><cell>ND</cell><cell></cell><cell>SFD</cell><cell></cell></row><row><cell></cell><cell>n</cell><cell>RawAcc</cell><cell>n</cell><cell>RawAcc</cell><cell>n</cell><cell>RawAcc</cell></row><row><cell>3</cell><cell>7</cell><cell>83.7 ± 4.5</cell><cell>6</cell><cell>80.3 ± 12.2</cell><cell>5</cell><cell>85.6 ± 8.1</cell></row><row><cell>6</cell><cell>7</cell><cell>78.4 ± 8.1</cell><cell>7</cell><cell>78.4 ± 10.3</cell><cell>15</cell><cell>85.6 ± 8.1</cell></row><row><cell>9</cell><cell>3</cell><cell>85.1 ± 8.9</cell><cell>6</cell><cell>80.7 ± 8.3</cell><cell>5</cell><cell>83.6 ± 6.0</cell></row><row><cell>12</cell><cell>9</cell><cell>77.4 ± 7.9</cell><cell>7</cell><cell>75.5 ± 6.9</cell><cell>17</cell><cell>87.5 ± 5.9</cell></row><row><cell>15</cell><cell>10</cell><cell>83.0 ± 9.8</cell><cell>7</cell><cell>81.1 ± 8.3</cell><cell>8</cell><cell>85.1 ± 4.9</cell></row><row><cell>18</cell><cell>11</cell><cell>85.6 ± 5.1</cell><cell>7</cell><cell>78.4 ± 10.1</cell><cell>9</cell><cell>84.6 ± 6.9</cell></row><row><cell>21</cell><cell>6</cell><cell>82.7 ± 11.2</cell><cell>6</cell><cell>76.4 ± 8.9</cell><cell>19</cell><cell>84.1 ± 4.8</cell></row><row><cell>24</cell><cell>12</cell><cell>77.4 ± 11.3</cell><cell>6</cell><cell>68.3 ± 7.5</cell><cell>11</cell><cell>83.1 ± 5.2</cell></row><row><cell>27</cell><cell>11</cell><cell>75.0 ± 11.6</cell><cell>7</cell><cell>66.8 ± 9.1</cell><cell>9</cell><cell>85.2 ± 10.2</cell></row><row><cell>30</cell><cell>6</cell><cell>80.6 ± 11.4</cell><cell>7</cell><cell>79.8 ± 10.8</cell><cell>5</cell><cell>82.2 ± 9.5</cell></row><row><cell>Average</cell><cell>8</cell><cell>81.2</cell><cell>7</cell><cell>77.0</cell><cell>10</cell><cell>84.9</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Q. Hu et al. / Information Sciences 180 (2010) 4384-4400</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Q. Hu et al. / Information Sciences 180 (2010) 4384-4400</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to express their gratitude to the anonymous reviewers and Prof. Witold Pedrycz for their valuable comments. This work is partly supported by National Natural Science Foundation of China under Grants 60703013, 10978011 and The Hong Kong Polytechnic University (G-YX3B). Prof. Yu is supported by National Science Fund for Distinguished Young Scholars under Grant 50925625.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast outlier detection in high dimensional spaces</title>
		<author>
			<persName><forename type="first">F</forename><surname>Angiulli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pizzuti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth European Conference on the Principles of Data Mining and Knowledge Discovery</title>
		<meeting>the Sixth European Conference on the Principles of Data Mining and Knowledge Discovery</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Using mutual information for selecting features in supervised neural net learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Battiti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="531" to="549" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">UCI Repository of Machine Learning Databases</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Merz</surname></persName>
		</author>
		<ptr target="&lt;http://www.ics.uci.edu/mlearn/MLRepository.html&gt;" />
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Factor analysis latent subspace modeling and robust fuzzy clustering using t-distributions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chatzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Varvarigou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="505" to="516" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Support vector machine soft margin classifiers: error analysis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">W</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1143" to="1175" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Outlier detection with the kernelized spatial depth function</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Bart</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="288" to="305" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vaguely quantified rough sets, in: Rough Sets, Fuzzy Sets, Data Mining and Granular Computing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cornelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Cock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Radzikowska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">4482</biblScope>
			<biblScope unit="page" from="87" to="94" />
			<date type="published" when="2007">2007</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A noise-tolerant approach to fuzzy-rough feature selection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cornelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Fuzzy Systems</title>
		<meeting>the 17th International Conference on Fuzzy Systems</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1598" to="1605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust fuzzy clustering of relational data</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="713" to="727" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A formalism for relevance and its application in feature subset selection</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="175" to="195" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rough fuzzy sets and fuzzy rough sets</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Prade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">General Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="191" to="209" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Second International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Correlation-based feature selection for discrete and numeric class machine learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Machine Learning</title>
		<meeting>the 17th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="359" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neighborhood rough set based heterogeneous feature subset selection</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="page" from="3577" to="3594" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hybrid attribute reduction based on a novel fuzzy-rough model and information granulation</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="3509" to="3521" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stability analysis on rough set based feature evaluation</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rough Sets and Knowledge Technology</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5009</biblScope>
			<biblScope unit="page" from="88" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Max-min margin machine: learning large margin classifiers locally and globally</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="260" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fuzzy-rough sets for descriptive dimensionality reduction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Fuzzy Systems</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="29" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">New approaches to fuzzy-rough feature selection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="824" to="838" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An efficient ant colony optimization approach to attribute reduction in rough set theory</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">R</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">G</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1351" to="1357" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Distance-based outliers: algorithms and applications, Very Large Databases</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Knorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tucakov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="237" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Input feature selection by mutual information based on Parzen window</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1667" to="1671" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A robust minimax approach to classification</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="555" to="582" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Feature Selection for Knowledge Discovery and Data Mining</title>
		<editor>H. Liu, H. Motoda</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generalized fuzzy rough sets determined by a triangular norm</title>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="page" from="3203" to="3213" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Axiomatics for fuzzy rough sets</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Morsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Yakout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fuzzy Sets and Systems</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="327" to="342" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A branch and bound algorithm for feature subset selection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Narendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="917" to="922" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Soft data mining, computational theory of perceptions, and rough-fuzzy approach</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="5" to="12" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rough sets</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Pawlak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer and Information Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="341" to="356" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Feature selection based on mutual information: criteria of max-dependency, max-relevance, and min-redundancy</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1226" to="1238" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m">Rough Sets in Knowledge Discovery: Applications, Case Studies, and Software Systems</title>
		<editor>
			<persName><forename type="first">Soft</forename><surname>Fuzziness</surname></persName>
		</editor>
		<editor>
			<persName><surname>Computingl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Polkowski</surname></persName>
		</editor>
		<editor>
			<persName><surname>Skowron</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg, New York</addrLine></address></meeting>
		<imprint>
			<publisher>Physica-Verlag</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Floating search methods in feature selection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pudil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Novovicova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1119" to="1125" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A comparative study off fuzzy rough sets</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Radzikowskaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Kerreb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fuzzy Sets and Systems</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="137" to="155" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Margin based feature selection-theory and algorithms</title>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Naftali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference Proceeding Series, Proceedings of the 21st International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">69</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient algorithms for mining outliers from large data sets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kyuseok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGMOD International Conference on Management of Data</title>
		<meeting>ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="427" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Variable precision fuzzy rough sets</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rolka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rolka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Rough Sets I</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>James</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Andrzej</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3100</biblScope>
			<biblScope unit="page" from="144" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Wavecluster: a multi-resolution clustering approach for very large spatial databases</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sheikholeslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Very Large Databases</title>
		<meeting>International Conference on Very Large Databases</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="428" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Selecting informative features with fuzzy-rough sets and its application for complex systems monitoring</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1351" to="1363" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fast branch and bound algorithms for optimal feature selection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Somol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pudil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="900" to="912" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adaptive floating search methods in feature selection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Somol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pudil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Novoviova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Paclik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1157" to="1163" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mining distance-based outliers in near linear time with randomization and a simple pruning rule</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Discovery and Data Mining, Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Iterative RELIEF for feature weighting: algorithms, theories, and applications</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1035" to="1051" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Weighted k-nearest leader classifier for large data sets</title>
		<author>
			<persName><forename type="first">V</forename><surname>Suresh Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Viswanath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition and Machine Intelligence</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4815</biblScope>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rough set methods in feature selection and recognition</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Swiniarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Skowron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="833" to="849" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On the generalization of soft margin algorithms</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2721" to="2735" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley and Sons</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The nearest subclass classifier: a compromise between the nearest mean and nearest neighbor classifier</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Veenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J T</forename><surname>Reinders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1417" to="1429" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A comparative study of algebra viewpoint and information viewpoint in attribute reduction</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fundamenta Informaticae</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="289" to="301" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Constructive and axiomatic approaches of fuzzy approximation operators</title>
		<author>
			<persName><forename type="first">W.-Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="233" to="254" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Generalized fuzzy rough sets</title>
		<author>
			<persName><forename type="first">W.-Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="263" to="282" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Mining with noise knowledge: error-aware data mining</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Q</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="917" to="931" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Enhancing data analysis with noise removal</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="304" to="319" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">An approach for fuzzy-rough sets attribute reduction via mutual information</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Fuzzy Systems and Knowledge Discovery</title>
		<meeting>the Fourth International Conference on Fuzzy Systems and Knowledge Discovery</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="107" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A decision-theoretic rough set model</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K M</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lingras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Methodologies for Intelligent Systems</title>
		<editor>
			<persName><forename type="first">Z</forename><forename type="middle">W</forename><surname>Ras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Zemankova</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Emrich</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Attribute reduction in decision-theoretic rough set models</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="page" from="3356" to="3373" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Feature selection based on loss-margin of nearest neighbor classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1914" to="1921" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The model of fuzzy variable precision rough sets</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C C</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="451" to="467" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">On characterization of intuitionistic fuzzy rough sets based on intuitionistic fuzzy implicators</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="page" from="883" to="898" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Error detection and impact-sensitive instance ranking in noisy datasets</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aaai Conference on Artificial Intelligence archive Proceedings of the 19th national conference on Artificial intelligence</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="378" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Class noise handling for effective cost-sensitive learning by cost-guided iterative classification filtering</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">D</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1435" to="1440" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Variable precision rough set model</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ziarko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="39" to="59" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
