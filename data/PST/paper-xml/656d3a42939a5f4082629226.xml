<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-11-30">30 Nov 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-11-30">30 Nov 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2312.00589v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ego car should be cautious and prepared to stop or slow down when necessary to avoid colliding with the pedestrian. The ego car's correct course of action is to be patient and wait for the pedestrian to cross the street before proceeding.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Here we showcase several main capabilities of Merlin. Notably, in the dialogue, the words marked with colors correspond to the trajectory outputs of the targets in the image. To save space, we highlight them using the same colors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Humans possess the remarkable ability to foresee the future to a certain extent based on present observations, a skill we term as "foresight minds". However, this capability remains largely under-explored within existing Multimodal Large Language Models (MLLMs), hindering their capacity to learn the fundamental principles of how things operate and the intentions behind the observed subjects. To address this issue, we introduce the integration of "future model-ing" into the existing learning frameworks of MLLMs. By utilizing the subject's trajectory -a highly structured representation of a consecutive frame sequence -as a learning objective, we aim to bridge the gap between the past and the future. We propose two innovative methods to empower MLLMs with "foresight minds": Foresight Pre-Training (FPT) and Foresight Instruction-Tuning (FIT), which are inspired by the modern learning paradigm of LLMs. Specifically, FPT jointly training various tasks centered on trajectories, enabling MLLMs to learn how to attend and predict</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human beings can predict future events or outcomes based on current observations, known in neuroscience theory as predictive processing <ref type="bibr" target="#b19">[20]</ref>. In this paper, we refer to this ability as foresight minds, which involves the use of past experiences, knowledge, sensory information, and probabilistic reasoning to generate expectations about future events. In the artificial intelligence (AI) domain, the capability to predict future events is an important topic towards the realization of artificial general intelligence (AGI).</p><p>Recent advancements in Multimodal Large Language Models (MLLMs), such as GPT-4V <ref type="bibr" target="#b48">[49]</ref> and Bard <ref type="bibr" target="#b1">[2]</ref>, have shown significant potential in image understanding and logical reasoning. Despite these achievements, these models struggle to foresee future events based on current image observations. Even provided with additional observations, like sequences of multiple frames, the current MLLM models still struggle to adequately analyze and infer specific target behaviors, such as predicting object movements or interactions (shown in Figure <ref type="figure" target="#fig_2">2</ref>). On the contrary, human can reason the future to some extent based on the observed current state <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b54">55]</ref>, which shows powerful foresight minds.</p><p>To mitigate this existing deficiency in MLLMs, we start from dividing human's process of foreseeing the future into a two-stage system <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b54">55]</ref>: <ref type="bibr" target="#b0">(1)</ref> observing the dynamic clues of the subject and then <ref type="bibr" target="#b1">(2)</ref> analyzing the behavior pattern and reasoning what might happen according to the observation. For instance, while watching a basketball game, people will first observe the moving players on the court, and then forecast the specific player's forthcoming actions, e.g., shooting, slam-dunking, or passing, by analyzing the current states and movement patterns of the players. Compare this system to current MLLMs, we find that MLLMs can complete the second stage well, thanks to the powerful logical reasoning ability of LLM <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b70">71]</ref>. Therefore the key challenge is the first stage. That is, how to make MLLM acquire correctly dynamic information from the multi-image observation?</p><p>Explicitly modeling next frames (e.g., reconstructing next frames <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b73">74]</ref>) can be a straightforward way. However, it can be hard to directly extract dynamic clues from the redundant visual information <ref type="bibr" target="#b25">[26]</ref>, especially from</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Failure case of GPT-4V</head><p>What is this player in the first frame going to do next?Analyse based on the trajectory.</p><p>Based on the player's position within the court, their body language suggest they might be considering taking a shot.</p><p>Is there any player trying to disturb this shot?</p><p>In the provided frames, there does not appear to be a defender trying to contest the shot.  video sequences. It is necessary to construct a suitable learning objective to assist MLLM in obtaining dynamic clues about the specific subjects. To this end, we point out that trajectory, as a highly structured representation, is a good learning objective which can link the temporal contexts between the past and the future.</p><p>Based on this insight, we propose to model the future to empower existing MLLMs with "foresight minds". Following the modern learning paradigm of LLMs, our future learning process includes two stages: (1) Foresight Pre-Training (FPT), a paradigm that causally models the temporal trajectories, which interleave with multi-frame images. The model starts with the initial observation of one or multiple subjects in the first frame as the query and then is required to predict the whole trajectory. Notably, we introduce various tasks containing richly labeled data <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b77">78]</ref>, including object detection, object tracking, etc., to perform multitask learning. And samples from these tasks are properly formatted to ensure coordinated pre-training. (2) Future Instruction-Tuning (FIT), then, considers the trajectory modeling bestowed by FPT as a bridge in the logical chain of future reasoning. Simply put, when querying an MLLM, it must articulate its reasoning in conjunction with the trajectory for each object referenced. This method, as a form of Trajectory Chainof-Thought, effectively narrows the gap between trajectory perception and predictive future reasoning, thereby fully unleashing model's foresight minds.</p><p>Aided by the above future modeling technologies, we provide Merlin 1 , a novel and unified MLLM capable of handling inputs and outputs of spatial coordinates or track-lets from single image or multiple frames. Moreover, Merlin is adept at performing inductive reasoning about future events based on current observational results. To demonstrate this, we provide several real dialogues between users and Merlin, as displayed in the Figure <ref type="figure" target="#fig_0">1</ref>. Unlike the previous MLLMs <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b84">85]</ref> which only supported interaction with a single image, Merlin not only provides a richer multi-image interaction, but also on this basis, is capable of executing unique and powerful future reasoning.</p><p>We construct a new future reason benchmark to evaluate Merlin's logical reasoning and future prediction abilities. The results, which significantly surpass previous baselines <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b74">75]</ref>, demonstrate Merlin's stunning performance in future reasoning. We further reveal Merlin's exceptional performance in general visual understanding. Through analysis in scenarios such as VQA (Visual Question Answering) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b28">29]</ref>, comprehensive understanding <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b76">77]</ref>, and hallucination <ref type="bibr" target="#b38">[39]</ref>, we unexpectedly discovered that our proposed novel paradigm of future learning aids MLLMs in gaining a deeper understanding of images. We believe this brings new insights for the training of future MLLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Large Language Models</head><p>Large Language Models (LLMs) have gained significant attention due to their capabilities in language generation and logical reasoning. Pioneering models like BERT <ref type="bibr" target="#b14">[15]</ref>, GPT-2 <ref type="bibr" target="#b51">[52]</ref>, and T5 <ref type="bibr" target="#b53">[54]</ref> laid the groundwork, but GPT-3 <ref type="bibr" target="#b7">[8]</ref>, the first model with a 175 billion parameter size, made notable strides, demonstrating strong zero-shot performance. An emergent ability, wherein model size scaling results in significant language capability improvements, was also observed in LLMs. This was further facilitated by Instruct-GPT <ref type="bibr" target="#b49">[50]</ref> and ChatGPT <ref type="bibr" target="#b47">[48]</ref> using Reinforcement Learning with Human Feedback (RLHF) on GPT-3. These advancements led to what's called LLMs' "iPhone moment". Following GPT's success, several open-source LLMs, including OPT <ref type="bibr" target="#b79">[80]</ref>, LLaMA <ref type="bibr" target="#b66">[67]</ref>, and GLM <ref type="bibr" target="#b78">[79]</ref>, have been proposed, showing similar performance to GPT-3. Models like Alpaca <ref type="bibr" target="#b65">[66]</ref> and Vicuna <ref type="bibr" target="#b9">[10]</ref> illustrate the application of these LLMs, using a self-instruct framework to construct excellent dialogue models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multimodal Large Language Models</head><p>The advancements in LLMs <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b67">68]</ref> have projected a promising path towards artificial general intelligence (AGI). This has incited interest in developing multi-modal versions of these models. Current Multi-modal Large Language Models (MLLMs) harness the potential of crossmodal transfer technologies. These models consolidate multiple modalities into a unified language semantic space, and then employ autoregressive language models as decoders for language outputs. Models like Flamingo <ref type="bibr" target="#b0">[1]</ref> have adopted a gated cross-attention mechanism, trained on billions of image-text pairs, to align visual and linguistic modalities, showing impressive performance on few-shot learning tasks. Similarly, BLIP-2 <ref type="bibr" target="#b36">[37]</ref> introduced the Q-Former to align visual features more effectively with language space. The LLaVA series <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref> further enhanced this process by using simply a MLP in place of the Q-Former and designing a two-stage instruction-tuning procedure. Apart from creating general MLLMs, techniques have also been developed for visual-interactive multimodal comprehension, involving the precise tuning of referring instructions <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b82">83]</ref>. Furthermore, another interesting direction in MLLM research involves integrating MLLMs for cross-modal generation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33]</ref> by using text-to-image models such as Stable Diffusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Metholodgy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall Architecture</head><p>Merlin is designed to unlock the foresight minds based on observations from single images and multi-frame videos. In order to accomplish this, images and videos are comprehensively represented through a series of visual tokens, which are then integrated into the language sequence that can be comprehended by Large Language Models (LLMs) in a unified framework. Specifically, Merlin consists of an image encoder, a decoder-only LLM, and a modality alignment block as illustrated in Figure <ref type="figure" target="#fig_4">3</ref>. Following prevalent practice <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b84">85]</ref>, we opt for the pre-trained CLIP <ref type="bibr" target="#b52">[53]</ref> ViT-L/14 <ref type="bibr" target="#b17">[18]</ref> as the visual encoder and Vicuna-7B v1.5 <ref type="bibr" target="#b9">[10]</ref> as the large language decoder. For more details, please refer to our supplementary materials.</p><p>To provide enough visual information and details, the input images are resized to a resolution of 448 ? 448. At this juncture, the visual encoder iteratively attends to (448/14) 2 uniformly divided image patches, yielding 1024 encoded tokens. Considering the limited context length of LLMs and addressing the substantial computational challenges posed by high resolution and multi-frame context modeling, we simply utilize a 2D convolution to achieve both dimension projection and token aggregation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b55">56]</ref>.</p><p>We choose 2D convolution over 1D linear layers <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref> or cross-attention layers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b84">85]</ref> as connector for the following reasons: (1) 2D convolution clusters local visual tokens on a spatial scale <ref type="bibr" target="#b22">[23]</ref>, effectively achieving a onestep transformation from spatial to channel information; <ref type="bibr" target="#b1">(2)</ref> The good convergence properties <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b61">62]</ref> of 2D convolution compared with cross-attention lay a solid foundation for foresight learning in a two-step training approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User Prompt</head><p>What will happen to this player? Judging by his respective trajectories and movements: Track1&lt;Id1&gt;Frame1: <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr">115,</ref><ref type="bibr">191]</ref>;Frame2: <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr">124,</ref><ref type="bibr">205]</ref>;Frame3: <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr">136,</ref><ref type="bibr">214]</ref>. This player will be tackled by another player in a white jersey:Track2&lt;Id2&gt;Frame1: <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr">119,</ref><ref type="bibr">203]</ref>;Frame2: <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr">113,</ref><ref type="bibr">200]</ref>;Frame3: <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr">101,</ref><ref type="bibr">205</ref>]&lt;/Id2&gt;. And both players will fall to the ground.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Response:</head><p>Trajectory Visualization </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Foresight Pre-Training</head><p>Generative Pre-Training (GPT) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49]</ref> serves as the cornerstone of this generation's Language Models (LLMs). Through learning to predict next token, the model efficiently condenses data, thereby yielding emergent forms of intelligence <ref type="bibr" target="#b69">[70]</ref>. In this context, a very natural approach to enhance the model's perception of the dynamic clues across multiple frames is to explicitly model the next frame (or image). However, due to the high redundancy in multi-frame visual information, the truly next-frame prediction remains a significant challenge to date. A better approach at this juncture is to implicitly model high semantic information in the label space (such as categories, bounding boxes) on a frame-by-frame basis. Temporally, this label information forms a trajectory, a highly structured representation. Causally modeling the trajectory in conjunction with each frame of image helps to connect the past and present in time, thus enabling the model to perceive the future.</p><p>To this end, we propose the Foresight Pre-Training, a way of causally modeling the trajectories interleaved with multi-frame images, to empower the MLLM with the capacity of perceive the dynamic clues, and ultimately achieving future reasoning. Specifically, given a video clip including several frames, we first give the model the observation of the first frame, then we require the model to predict the whole trajectory of the concerned subject in this video. Notably, the observation of the first frame can be the description or simple position of the concerned object. Formally,</p><formula xml:id="formula_0">P (Y |X) ? P (Y |{X 1 , X 2 , ...}, O f irst ),<label>(1)</label></formula><p>where X i denotes the i th frame and O f irst is the first frame observation, Y refers to the trajectory of the subject in O f irst within the frame sequence. The observation and the raw frames will be regarded as the condition to prompt MLLM to predict the trajectory. Data Construction. We first aggregate all valuable multimodal information from diverse data resources and then properly organize them for multi-task foresight pre-training. Specifically, for each sample instance I, we first collect its multimodal information including consecutive multi-frame images {X 1 , X 2 , ...}, subject observations from the first frame O f irst , and subject trajectory Y constructed from all frames. Formally,</p><formula xml:id="formula_1">I = {{X 1 , X 2 , ...}, O f irst , Y }.<label>(2)</label></formula><p>We categorize observations of one subject of the first frame into three types: location description, appearance description and action description. Then we randomly select one of these observations of a particular subject in the first frame as the query object.</p><p>To better unleash the powerful generative modeling capacity of LLM, we construct this query process as a type of conversation. Here is an example of the constructed data shown in Figure <ref type="figure" target="#fig_4">3</ref>. In this case, we want to query the subject -the panda on the right -with the randomly select observation, and expect the answer with the movement trajectory of this panda across multiple frames. To model this process, we convert the query to question and trajectory to answer with proper natural language for embellishment.</p><p>Overall, the aforementioned process of dialogization roughly follows these three principles: (1) Precise definition of task prompts and answer formats. In particular, we use a task prompt to tell MLLM what specific task to do (detect or track), and also specified the answer for-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>An Example of FPT dialogue</head><p>Given a video clip including: frame1:</p><p>\n, frame2: \n,frame3: \n</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interleaved Multiple Images: Initial Observation:</head><p>Appearance: A panda on the right side.</p><p>Action: A panda is lying on the ground.  mat with accurate descriptions in each question. In this way, different types of tasks can be flexibly organized together without compromising the general language ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Randomly choice</head><p>(2) Clear indication of multimodal information. Concretely, for each group of image tokens, we add a special frame indicator in front of then, i.e., frame1:&lt;image&gt; and frame2:&lt;image&gt;, so as to help MLLM better focus on the corresponding image. (3) Interleaving of frames and observations. For the same identity, we interleave the frames in which it appears with its positional observations, and enclose them with two ID tokens (i.e. &lt;Idi&gt; and &lt;/Idi&gt;) to construct a trajectory. We believe that this interleaved organization helps in generatively training to model causality within the trajectory, while the ID tokens ensures that the model can distinguish among different identity objects.</p><p>Training Details. Previous practices <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref> typically conducting a separate modality alignment training phase following a multi-task pre-training stage, which however, complicates the training process and data construction. In this paper, we directly incorporate both of them into one stage, and unfreeze all modules during pre-training. This is because that we believe the MLLMs are sufficiently powerful to concurrently handle the learning of general multimodal capabilities and multi-task specific abilities under proper guidance. Furthermore, we mix a large amount of image-text pairs and rich-annotated conversation data (formatted according to the above method) from diverse data sources <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b77">78]</ref> to conduct multitask learning. In doing so, not only endows the model with foresight minds but also ensures its multimodal alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Foresight Instruction Tuning</head><p>Althought Foresight Pre-Training equips the model with the ability to observe dynamic clues across multiple frames, it still falls short of true foresight minds. This is because models typically struggle to effectively transform such observations into successful future reasoning <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b83">84]</ref>.</p><p>Recent work <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b83">84]</ref> has highlighted that Chain-of-Thought (CoT) <ref type="bibr" target="#b70">[71]</ref> is crucial in bridge the gap between the observations and actions of MLLMs with theory of mind <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b68">69]</ref>. Meanwhile, several prior studies <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b81">82]</ref> have also demonstrated that prompts indicating position (such as bounding boxes or points) -a principle analogous to CoT -can concentrate an MLLM's attention on the relevant area, leading to more accurate dialogues and reducing the likelihood of visual hallucination. Drawing inspiration from these findings, we conduct the Foresight Instruction Training (FIT) building upon the foundation of FPT to further enhance the model's future reasoning capability. In specific, building on the trajectory generating powered by FPT, we further union the trajectories to generatively rationalize the forthcoming events. Mathematically,</p><formula xml:id="formula_2">P (Z|X, Y ) ? P (Z|{X 1 , X 2 , ...}, O f irst , Y ),<label>(3)</label></formula><p>where Z refers to the future observation which is deduced from observations in each frame. It can be actions, events, trends, or simply likelihoods. In this context, multi-frame images, in conjunction with the first subject observation, and the trajectory of the same subject across all frames, serve as the union condition to prompt MLLM to causally predict the future. This way, akin to a Trajectory Chainof-Thought, effectively bridges the gap between trajectory perception and predictive future reasoning, thereby fully unleashing model's foresight minds. Data Construction. The specific data construction method is similar to FPT, but on this basis, we also deduce a future observation Z from the information across multiple frames and append it after the trajectory in the answer. Formally,</p><formula xml:id="formula_3">I = {{X 1 , X 2 , ...}, O f irst , Y, Z}.<label>(4)</label></formula><p>Practically, in this paper, we constitute future observations based on multi-frame, multi-target action descriptions combined with human priors, and further process them with GPT-4 <ref type="bibr" target="#b48">[49]</ref> to ultimately form reasonable future inferences. More details are provided in the supplementary materials.</p><p>Figure <ref type="figure" target="#fig_5">4</ref> provides an illustrative example of FIT, when a user questions Merlin about the future of a player in red attire, Merlin initially presents the observed trajectory of the concerned player, followed by the trajectory of another player in white. Using these trajectories, Merlin deduces that the player in white is likely to tackle the one in red, resulting in both players falling to the ground. Training Details. We freeze the vision encoder and keep the convolutional projector and the LLM unfreezed in this stage. On this basis, we primarily adopt the open-source instruction tuning datasets, e.g. LLaVA-665K <ref type="bibr" target="#b40">[41]</ref> for building the basic ability for multi-round visual-language conversation. For further unleash the foresight minds of model, we first uniformly sample a certain number of multitask dialogues in FPT, in order to maintain the model's capacity of modeling the dynamic clues across multi-frame images.</p><p>In addiction, we also sample data from three specific scenario datasets <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b71">72]</ref> and construct around 60K FIT conversations based on the aforementioned data construction process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>Datasets. For the foresight pre-training (FPT) stage, we first use 10M image-text pairs sampled from LAION400M <ref type="bibr" target="#b57">[58]</ref> to ensure multimodal alignment. On this basis, we gather various open-source datasets with rich annotations to conduct multi-task learning, including (1) object detection datasets: Object365 <ref type="bibr" target="#b60">[61]</ref> and OpenImage <ref type="bibr" target="#b33">[34]</ref>; (2) tracking datasets: LaSOT <ref type="bibr" target="#b18">[19]</ref>, GOT10K <ref type="bibr" target="#b26">[27]</ref>, MOT17 <ref type="bibr" target="#b46">[47]</ref>, DanceTrack <ref type="bibr" target="#b64">[65]</ref> and SOMPT22 <ref type="bibr" target="#b62">[63]</ref>; <ref type="bibr" target="#b2">(3)</ref> grounding dataset: Ref-COCO <ref type="bibr" target="#b31">[32]</ref>; (4) object relation dataset: VCR <ref type="bibr" target="#b77">[78]</ref>. For these data, as described in Section 3.2), we apply strict task definitions and format specifications, and re-organize them in the form of interleaved frames and observations. Ultimately, we obtain approximately 5M question-answer data, which are mixed with 10M paired data for foresight pre-training.</p><p>For the foresight instruction-tuning (FIT) stage, we mix approximately 730K conversation data, including <ref type="bibr" target="#b0">(1)</ref> opensource instruction-tuning data LLaVA-665K <ref type="bibr" target="#b40">[41]</ref>, which integrates a series of VQA datasets <ref type="bibr" target="#b63">[64]</ref> and multi-round conversation datasets <ref type="bibr" target="#b41">[42]</ref>; (2) around 30K FIT multi-frame conversations constructed from three specific scenarios including MultiSports <ref type="bibr" target="#b37">[38]</ref>, TITAN <ref type="bibr" target="#b45">[46]</ref> and STAR <ref type="bibr" target="#b71">[72]</ref> based on the data construction method described in Section 3.3;</p><p>(3) nearly 40K randomly sampled FPT multi-task data. For more details of the datasets, please refer to the supplementary materials. Implementation Details. During the foresight pre-training, we optimize all parameters of the model, setting the learning rate to 5e -5 and training for one epoch. In the instruction tuning stage, we freeze the visual encoder and fine-tune the parameters of the projector and LLM. In both stages, we train Merlin using the AdamW <ref type="bibr" target="#b44">[45]</ref> optimizer and a cosine annealing scheduler <ref type="bibr" target="#b43">[44]</ref> as the learning rate scheduler. Additional implementation details can be found in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Properties Evaluation of Foresight Minds</head><p>In this section, we mainly verify the foresight minds of Merlin from two aspects, i.e., future reasoning and identity association ability, where the former focuses on forecasting future events or behavior based on image observation, and the latter focuses on the model's ability to establish subject identity associations across multiple frames to obtain dynamic clues. Future Reasoning. To evaluate this ability, we probe this For VQA tasks, we mainly choose GQA <ref type="bibr" target="#b28">[29]</ref> and VisWiz <ref type="bibr" target="#b24">[25]</ref> to evaluate the model; For general evaluation, we mainly choose MMBench <ref type="bibr" target="#b42">[43]</ref> and MM-Vet <ref type="bibr" target="#b76">[77]</ref>. ? Includes using in-house data that is not publicly accessible.</p><p>ability based on the several sub-tasks of MMBench <ref type="bibr" target="#b42">[43]</ref>. MMBench provides a comprehensive evaluation system to assess various capabilities of MLLM, with some metrics focusing on the model's prediction and reasoning capabilities. To this end, we pick out these metrics to establish this new future reasoning benchmark and compare Merlin with the existing SOTA models. As shown in Table <ref type="table" target="#tab_1">1</ref>, Merlin achieves the best overall performance (64.4 average score on the development set and 66.5 average score on the test set). Moreover, it obtains the best in 8/10 indicators and ranks second in all other indicators, which favorably demonstrates Merlin's strong future reasoning ability. Identity Association. We examine this ability by evaluating the performance of object-tracking tasks, which can comprehensively demonstrate object association and prediction capabilities. To this end, we evaluate Merlin in existing mainstream tracking benchmarks, i.e., LaSOT <ref type="bibr" target="#b18">[19]</ref> and GOT10K <ref type="bibr" target="#b26">[27]</ref>. It is worth noting that Merlin is the first MLLM that can also carry out tracking tasks. As shown in Table <ref type="table" target="#tab_2">2</ref>, Merlin achieves comparable performance with expert models and even outperforms on some metrics. Notably, we only sample a small amount of tracking data to train Merlin instead of the full amount of data, which means LLM exhibits significant potential in handling temporal tasks, possibly because tracking, as a temporal task, can be viewed as a casually frame-level autoregressive task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Object Hallucination</head><p>Hallucination presents a significant challenge in existing MLLMs. This term describes the phenomenon where the generated textual content exhibits inconsistencies when compared to its corresponding image content. In this section, we present the experiments from the Polling-Based Object Probing Evaluation (POPE <ref type="bibr" target="#b38">[39]</ref>). As demonstrated in Table <ref type="table" target="#tab_3">3</ref>, Merlin surpasses recent SOTA methods with clear margins. More specifically, Merlin achieves optimal performance in all metrics across three scenarios: Random, Popular and Adversarial, with improvements of up to 5 points compared to the highly competitive baseline Shikra <ref type="bibr" target="#b8">[9]</ref>. Surprisingly, in multiple scenarios, the 'yes' rate of Merlin is quietly close to 50%, demonstrating its extraordinary visual perception capabilities. We analyze this success largely owing to the proposed foresight learning (FPT and FIT). By enabling the model to learn the dynamic correspondence between trajectories across multiple images, the model has gained a more precise ability to attend to relevant object (trajectories) contexts in the image, which helps to better avoid misidentification and misalignment of irrelevant targets. We believe that this result will provide new thinking about addressing the issue of hallucinations in MLLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">General Comprehension</head><p>In order to showcase the general multi-modal ability, we further benchmark Merlin on various VQA benchmarks and recent benchmarks proposed for evaluating the comprehensive capabilities of MLLMs. Visual Question Answering (VQA). We first evaluate Merlin on several mainstream VQA benchmarks to reflect the perceptual abilities of MLLMs in understanding image content. As shown in Table <ref type="table" target="#tab_4">4</ref>, Merlin achieves competitive performance compared with existing advanced MLLMs in the selected VQA benchmarks (VQA). The results indicate that Merlin possesses strong image understanding and question-answering capabilities.   Synthetica MLLM Benchmarks. Recently, several benchmarks have been proposed to evaluate the comprehensive performance of MLLMs, encompassing diverse finergrained scenarios including visual perception, object recognition, optical character recognition (OCR), future reasoning, and so on. In this part, we select several mainstream MLLM benchmarks to evaluate Merlin. As shown in Table 4, We present performance in accuracy on benchmarks including MM-Vet <ref type="bibr" target="#b76">[77]</ref> and MMBench <ref type="bibr" target="#b42">[43]</ref>. On MM-Bench, we report results on the both development and test sets. The results show that Merlin significantly outperforms comparative methods, even though many methods utilized a substantial amount of in-house data for pre-training, or employed several times more parameters. This implies that, while introducing foresight minds into MLLMs, we not only preserved their original visual capabilities but even further enhanced their overall level of visual perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablative Analysis of FPT &amp; FIT</head><p>As introduced in Section 3.2 and Section 3.3, FPT serves as the pre-training strategy to enable MLLM to encapsulate dynamic information across frames by predicting the trajectory of the next frame. FIT is designed to activate the ability of foresight minds in a way of trajectory CoT during instruction fine-tuning. To further explore the effect of FPT and FIT, we conduct an ablation study based on the established future reasoning benchmark and tracking dataset GOT10K <ref type="bibr" target="#b26">[27]</ref>. As shown in Table <ref type="table" target="#tab_5">5</ref>, we mainly report the average overlap (AO) of GOT10K and the average score of future reasoning in the development set.</p><p>The results show that both FPT and FIT training strategies contribute to the improvement of the metrics. Combining both FPT and FIT, Merlin achieves the best performance which proves the effectiveness of the proposed strategies. Furthermore, we can also observe that the lack of imagetext pair data during the pre-training stage considerably hampers the model's general ability. This phenomenon supports our perspective that, during the comprehensive pretraining phase, the integration of image-text pair data is essential for maintaining modality alignment and preventing a decline in combined capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Visualization Analysis</head><p>In this subsection, we visualize the attention map of Merlin to further substantiate the effectiveness of utilizing the proposed strategies. As shown in Figure <ref type="figure" target="#fig_7">5</ref>, we select the output attention map of the middle-level layers of LLM for visualization. We can observe that the word embedding of the output trajectory coordinates can attend to the corresponding object from different frames correctly. This visualization results further prove that the trajectory representation is a good interface to enable MLLM to establish the alignment between the language description and the multi-images dynamic visual contexts. Furthermore, this effectively explains why Merlin possesses a more powerful comprehensive visual capability and a greatly lower level of hallucination compared to previous baselines. Indeed, the trajectorydriven foresight learning allows the large language model to read images more profoundly!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitation and Conclusion</head><p>This study highlighted an obvious deficiency in Multimodal Large-Language Models (MLLMs), specifically their ability to predict future events or outcomes based on current observations, referred as "foresight minds". To address this, we serve as the first to point out that trajectory, as a highly structured representation, is a good learning objective to assist MLLM in obtaining dynamic information from the image observations. Based on this insight, we introduced a unique training method including Foresight Pre-Training (FPT) and Foresight Instruction-Tuning (FIT). By synergizing FPT and FIT, we created Merlin, a unified MLLM that effectively understands and outputs spatial coordinates or tracklets from single images or multiple frames. Merlin excels at a range of traditional vision-language tasks while demonstrating powerful future reasoning capacities. Despite the substantial advancements made by Merlin, there still are some limitations, particularly in processing long sequential videos and more comprehensive feature reasoning evaluation. We aspire for Merlin to guide the enhancement of more advanced MLLMs in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>In this appendix, we offer additional information about Merlin due to the paper's page limit of 9 pages. Specifically, Section B provides in-depth insights into the dataset we constructed, including its data sources and how it was created. Section C delves into the data formats of the proposed FPT and FIT, as explained in the main manuscript. Section D offers a more detailed explanation of the training approach. Section E expands on the discussion of the proposed method, including its limitations and future directions. Finally, Section F offers additional experimental results and demo visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dataset Details</head><p>In Section 3 of the manuscript, we explained how we created the Foresight Pre-Training (FPT) and Foresight Instruction-Tuning (FIT) datasets. Now, in this section, we go into greater detail about how we collected and built the dataset. To start, we provide an overview of our collected data in Table <ref type="table" target="#tab_7">6</ref>, and then dive into the step-by-step process of how it was constructed. Image-text pairs. We mainly collect image-text pairs dataset from the open-sourced dataset, LAION-400M <ref type="bibr" target="#b57">[58]</ref>. LAION-400M is a high-quality dataset with CLIP-filtered 400 million image-text pairs. In this paper, we random sample 10M image-text pairs of LAION-400M for the Foresight Pre-Training. Detection datasets. Detection datasets are crucial for improving the model's ability to understand space during its initial training phase. In our research, we used datasets from various publicly available sources like Object365 <ref type="bibr" target="#b59">[60]</ref>, OpenImage <ref type="bibr" target="#b33">[34]</ref>, and CrowdHuman <ref type="bibr" target="#b58">[59]</ref>. We processed these datasets in the following ways:</p><p>(1) Extract all objects in each image along with their categories and bounding boxes.</p><p>(2) Remove too small objects (smaller than 1/32 of the image size).</p><p>(3) Randomly select 15 categories if image has more categories than a set limit <ref type="bibr" target="#b14">(15)</ref>.  (4) Compose the detection data format as the following cat1:&lt;box&gt;,&lt;box&gt;,&lt;box&gt;;cat2:&lt;box&gt;;....</p><p>As shown in Table <ref type="table" target="#tab_7">6</ref>, we also created special output prompts that guide the model to respond in a precise format, as detailed in our study. Tracking datasets. Building tracking data is a key part of our Foresight Pre-Training (FPT) method, as we've discussed in our main paper. FPT is designed to causally modeling the trajectory in conjunction with each frame of the image to empower the model to acquire foresight minds. The tracking data naturally includes trajectory information about movement paths, making it ideal for our FPT task. We gathered a variety of open-source tracking data, including Single Object Tracking (SOT) and Multi-Object Tracking (MOT) datasets like GOT10K <ref type="bibr" target="#b26">[27]</ref>, La-SOT <ref type="bibr" target="#b18">[19]</ref>, MOT17 <ref type="bibr" target="#b46">[47]</ref>, Sompt <ref type="bibr" target="#b62">[63]</ref>, DanceTrack <ref type="bibr" target="#b64">[65]</ref>, SportsMOT <ref type="bibr" target="#b11">[12]</ref>, BDD100K <ref type="bibr" target="#b75">[76]</ref>, MeViS <ref type="bibr" target="#b15">[16]</ref>), etc. We follow the steps below to pre-process the tracking data.</p><p>(1) Random sample 3 to 5 frames at a certain frame interval (gap = 3,4 or 5) to form a video clip each time.</p><p>(2) Extract all trajectories with their category, identity and bounding boxes in each video clip.</p><p>(3) Remove the trajectory containing too small objects (smaller than 1/32 of the image size). (4) Select the initial observation (location, appearance or action in the first frame) as the trajectory query. <ref type="bibr" target="#b4">(5)</ref> Compose the tracking data format as the following query,cat1&lt;Idi&gt;Frame1:&lt;box&gt;;Frame2:&lt;box&gt;; ...&lt;/Idi&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Examples of Data Formats</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image-Text Pairs General Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open-Vocabulary Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Referring Expression Comprehension</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Referring Expression Generation General Tracking</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Referring Tracking</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual Reasoning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Merlin-Chat</head><p>In this video clip, a captivating basketball game is visible, where athletes are fully engrossed in a highly intense contest. This competition, marked by skillful plays and fervent energy, is actively unfolding right before our eyes.  In our training, we use datasets that involve multiple tasks. To illustrate how these datasets are structured, we've chosen an example from each one. It's important to mention that all information about boxes has been adjusted to a standard range of 1000. In the examples, questions are shown in black text, answers in blue, and negative samples in red.</p><p>Similar to constructing detection data, we also adopt output format prompts to guide the model to provide answers in the desired trajectory format. Visual Reasoning datasets. Visual reasoning dataset is constructed to enhance the common sense reasoning ability of the model. In this work, we mainly collect the VCR dataset and adopt the same processing method as Shikra <ref type="bibr" target="#b8">[9]</ref>. Referring datasets. For referring dataset, we mainly collect from RefCOCO <ref type="bibr" target="#b31">[32]</ref> and MeViS <ref type="bibr" target="#b15">[16]</ref> to construct image referring and video referring datasets, respectively. For image referring, we simply extract the description and the corresponding bounding box from each image. For video referring, we primarily extract pairs of trajectories and trajectory descriptions from MeViS. Dialogue datasets. To facilitate the model's ability to achieve long conversations and follow user instructions, we utilized the open-sourced LLaVA-665K instruction tuning dataset <ref type="bibr" target="#b40">[41]</ref>. Additionally, we created a smaller instruction tuning dataset (30K entries) using our Trajectory Chain-of-Thought (T-CoT) method within Foresight Instruction Tuning. This dataset focuses on three specific contexts: Multi-Sports <ref type="bibr" target="#b37">[38]</ref>, TITAN <ref type="bibr" target="#b45">[46]</ref>, and STAR <ref type="bibr" target="#b71">[72]</ref>. MultiSports catalogs multi-person sports actions with spatial and temporal details. TITAN encompasses street scenes with varied labels like vehicle states, pedestrian age groups, and specific pedestrian actions. STAR offers real-world video data with symbolic descriptions and logic-based analysis.</p><p>To create the T-CoT conversation data, we leveraged GPT-4 to generate rich feature reasoning dialogues using the trajectories and actions from these datasets. We guided GPT-4 to interpret video clips through these trajectories and their associated descriptions or actions, then instructed it to formulate Q&amp;A pairs. These questions were designed to derive answers from available information. This process resulted in 30K QA pairs, incorporating trajectory coordinates in both questions and answers. We plan to expand this dataset, which we refer to as Merlin-chat, in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Data Format Details</head><p>To assist readers in comprehending the specific nature of the data we've constructed, this section includes visualizations of the training data format. As illustrated in Figure <ref type="figure" target="#fig_9">6</ref>, we present an exhaustive list of all data formats used, encompassing both pretraining and instruction tuning phases. Notably, we have incorporated negative samples (highlighted in bold red) into our question-answer pairs. This addition is designed to teach the model the skill of responding with a negative answer when appropriate, thereby bolstering its ability to resist producing hallucinatory responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training Details</head><p>In this section, we report the detailed training hyperparameter settings of Merlin in Table <ref type="table" target="#tab_8">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. More Discussions</head><p>Limitations and Future Work. While Merlin demonstrates exceptional foresight capabilities, it is not without its limitations in its current form. A significant constraint is the inability to support long-range video sequences (exceeding 8 frames), which hinders its capacity to model extended motion information. This is primarily due to Merlin's reliance on an image encoder rather than a video encoder, resulting in an excessive number of vision tokens for the LLM to process. Addressing this, the development of a more efficient tokenizer for long-range videos emerges as a crucial area for future research. Moreover, this paper introduces a novel benchmark for future reasoning, building upon the existing MMBench framework. Currently, there is no comprehensive and accurate benchmark to evaluate future reasoning abilities thoroughly. Therefore, exploring the creation of a more robust and comprehensive future reasoning benchmark represents another significant avenue for future investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. More Experiments</head><p>In this section, we aim to provide more experimental results including ablation studies (shown in Table <ref type="table" target="#tab_9">8</ref>) and qualitative analysis (shown in Figure <ref type="figure">7</ref>, 8 and 9).</p><p>Effect of Precise Task Description. In the method section of main paper, we emphasize the importance of precise task descriptions, which can prevent conflicts between multiple task learning and damage to general language abilities. The performance corresponding to the models with and without the precise task description is reported in Table <ref type="table" target="#tab_9">8</ref>. We can observe that the model without the precise task description behaves significantly poorer, especically the tracking performance (51.4% -28.4%). The results further prove that precise task description is an important option for multitask learning in existing MLLM.</p><p>Qualitative analysis. We further provide more conversation results between use and Merlin as shown in Figure <ref type="figure">7</ref>, Figure <ref type="figure" target="#fig_11">8</ref> and Figure <ref type="figure" target="#fig_13">9</ref>. It can be seen that Merlin is not only capable of successfully completing tasks that existing MLLMs can do (such as REC, REG, etc.), but also can undertake some brand new tasks (like tracking, video referring, etc.), while demonstrating powerful future reasoning abilities. These results further showcase Merlin's strong instruction-following ability and foresight minds. The Hulkbuster&lt;Id1&gt;Frame1:[000,001,682,999];Frame2:[500, 275, 999, 999];Frame3:[167, 001, 698, 999]&lt;/Id1&gt; is a robotic suit that is part of the Marvel Cinematic Universe. In the video clip, the Hulkbuster is engaged in a fight with another character, possibly the Hulk or another opponent. The suit is designed to be a powerful and versatile weapon, capable of dealing significant damage to its enemies. The Hulkbuster's appearance and actions in the clip suggest that it is in the midst of a battle, using its advanced technology and strength to take on its opponent.  If there is an ego car on the road, this picture is the one seen inside the ego car. How should the ego car have?</p><p>If there is an ego car on the road. The ego car should be prepared to stop or slow down if necessary, and be cautious of the cyclist's movements. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Demo cases presentation of Merlin.Here we showcase several main capabilities of Merlin. Notably, in the dialogue, the words marked with colors correspond to the trajectory outputs of the targets in the image. To save space, we highlight them using the same colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>?</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Failure case of GPT-4V about future reasoning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>First frameFigure 3 .</head><label>3</label><figDesc>Figure 3. Overall pipeline of Merlin. The architecture of Merlin consists of three main components: (1) an image encoder, (2) a large language model, and (3) a modality-align projector. Bottom: The diverse input format that supports multiple-image contexts, initial observation and the specific user prompt. Top: The model response including the predicted trajectory and the feature reasoning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. One example to illustrate the multi-modality pretraining dataset. The top block shows the provided contexts including the multiple images contexts and initial observation (box, appearance and action) about the subject to prompt the LLM. The bottom block shows the dialogue including question and answer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>This player &lt;Id1&gt; Frame1: [461, 278, 651, 976]; Frame2: [458, 254, 609, 968]; Frame3: [442, 254, 607, 918] &lt;/Id1&gt; is dribbling the ball while being pursued by the opponents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Attention map visualization. To facilitate the observation, we map the attention between the box responses and the visual tokens of each frame for visualization.</figDesc><graphic url="image-54.png" coords="8,51.14,72.22,161.76,66.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Could you tell me what this player&lt;Id1&gt;Frame1:[401, 506, 479, 662]&lt;/Id1&gt;, might be contemplating as their next move?Maintain consistency in your answer by using the object&lt;Idi&gt;Frame t:[xmin,ymin,xmax,ymax]&lt;/Idi&gt; format for trajectories. Given the trajectory and action of this player&lt;Id1&gt;Frame1:[401, 506, 479, 662];Frame2:[374, 495, 457, 669];Frame3:[353, 493, 414, 680];Frame4:[316, 484, 405, 675]&lt;/Id1&gt;, it is evident he is readying to make a basketball pick-and-roll defensive next. Which player is he planning to defense? This player is planning to avoid the dribble of the player&lt;Id2&gt;Frame1:[265, 491, 316, 675];Frame2:[257, 480, 312, 659]&lt;/Id2&gt;.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Data format visualization. In our training, we use datasets that involve multiple tasks. To illustrate how these datasets are structured, we've chosen an example from each one. It's important to mention that all information about boxes has been adjusted to a standard range of 1000. In the examples, questions are shown in black text, answers in blue, and negative samples in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>&lt;image&gt;,frame2:&lt;image&gt;, frame3:&lt;image&gt;,track person&lt;Id1&gt;Frame1:[329, 238, 420, 633]&lt;/Id1&gt; and person&lt;Id2&gt; Frame1:[491, 235, 570, 628]&lt;/Id2&gt;.To respond correctly, utilize the specified class&lt;Idi&gt;Frame t:[xmin,ymin,xmax,ymax]&lt;/Idi&gt; format. person&lt;Id1&gt;Frame1:[329, 238, 420, 633];Frame2:[310, 212, 400, 645];Frame3:[271, 202, 378, 648]&lt;/Id1&gt;, person&lt;Id2&gt;Frame1:[491, 235, 570, 628];Frame2:[495, 247, 572, 637];Frame3:[481, 258, 553, 635]&lt;/Id2&gt;. Video REC Given frame1:&lt;image&gt;,frame2:&lt;image&gt; and frame3:&lt;image&gt;,track the black cat in this video clip.Use the specified Tracki&lt;Idi&gt;Frame t:[xmin,ymin,xmax,ymax]&lt;/Idi&gt; format for all trajectories in your reply. There is one black cat.Track1&lt;Id1&gt;frame:1:[098, 181, 600, 662];frame:2:[196, 000, 642, 800];frame:3:[068, 115, 623, 786]&lt;/Id1&gt;. Video REG Given frame1:&lt;image&gt;,frame2:&lt;image&gt; and frame3:&lt;image&gt;,what is the hulkbuster&lt;Id1&gt;Frame1:[000,001,682,999];Frame2:[500, 275, 999, 999];Frame3:[167, 001, 698, 999]&lt;/Id1&gt; doing in this video clip?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. More conversation visualization using Merlin in video-level tasks. We showcase additional examples where Merlin adeptly handles various video-level tasks including general tracking, video referring expression comprehension (Video REC) and video referring expression generation (Video REG).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>, frame2&lt;image&gt; and frame3&lt;image&gt;, track person and human-used objects present in the video along with a caption about what the person is doing. When detailing person and human-used objects' trajectories in your response, adhere to the class&lt;Idi&gt;Frame t:[xmin,ymin,xmax,ymax]&lt;/Idi&gt; format. There is one person&lt;Id1&gt;frame:1:[392, 001, 827, 998];frame:2:[392, 001, 827, 998];frame:3:[342, 001, 742, 998]&lt;/Id1&gt; and one trash can&lt;Id2&gt;frame:1:[332, 608, 485, 983];frame:2:[332, 608, 485, 983];frame:3:[228, 163, 419, 536]&lt;/Id2&gt;. The person is holding a trash can and is standing in a room. What might this person do next immediately? The man is standing in a room and appears to be emptying the trash can. The man might next put the trash can on the floor or a designated trash receptacle. According to frame1&lt;image&gt;, frame2&lt;image&gt; and frame3&lt;image&gt;, Explain Cyclist&lt;Id1&gt;Frame1:[280, 436, 346, 721]&lt;/Id1&gt;'s route and include a short caption. For the trajectories included in the answer, please use the format class&lt;Idi&gt;Frame t:[xmin,ymin,xmax, ymax]&lt;/Idi&gt;. There is one Cyclist&lt;Id1&gt;Frame1:[280, 436, 346, 721];Frame2:[183, 425, 300, 771];Frame3:[071, 235, 829]&lt;/Id1&gt;. Cyclist's route: From the left side of the road, the cyclist is riding a bicycle down the road.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. More future reasoning conversation visualization using Merlin.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>The Effectiveness of Future Reasoning. We mainly select 5 metrics from MMBench develop and test set, respectively, including OL: Object localization, PPR: Physical property reasoning, FR: Function reasoning, IR: Identity reasoning, and FP: Future prediction. The best and second-best performances are shown in bold font and underlined respectively.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="2">LLM Size</cell><cell></cell><cell cols="3">Future Reasoning (Dev.)</cell><cell></cell><cell></cell><cell cols="2">Future Reasoning (Test)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Average OL PPR FR</cell><cell>IR</cell><cell>FP</cell><cell cols="3">Average OL PPR FR</cell><cell>IR</cell><cell>FP</cell></row><row><cell cols="2">InstructBLIP [13]</cell><cell>13B</cell><cell></cell><cell>42.0</cell><cell cols="4">14.8 30.7 56.8 88.9 19.0</cell><cell>44.4</cell><cell>5.7</cell><cell>24.0 67.3 92.7 32.4</cell></row><row><cell cols="2">MiniGPT-4 [85]</cell><cell>13B</cell><cell></cell><cell>43.3</cell><cell cols="4">28.4 30.7 49.4 86.7 21.4</cell><cell>48.9</cell><cell cols="2">21.0 35.0 67.3 90.2 31.1</cell></row><row><cell cols="2">OpenFlamingo [3]</cell><cell>7B</cell><cell></cell><cell>5.28</cell><cell>2.5</cell><cell>10.7 8.6</cell><cell>2.2</cell><cell>2.4</cell><cell>11.5</cell><cell>2.9</cell><cell>14.0 9.3</cell><cell>11.0 20.3</cell></row><row><cell cols="2">MMGPT [43]</cell><cell>7B</cell><cell></cell><cell>19.5</cell><cell>1.2</cell><cell>24.0 9.9</cell><cell cols="2">60.0 2.4</cell><cell>16.8</cell><cell>3.8</cell><cell>13.0 12.1 52.4 2.7</cell></row><row><cell cols="2">MiniGPT-4 [85]</cell><cell>7B</cell><cell></cell><cell>26.8</cell><cell>7.4</cell><cell cols="3">14.7 19.8 80.0 11.9</cell><cell>27.9</cell><cell>8.6</cell><cell>13.0 29.9 61.0 27.0</cell></row><row><cell cols="2">InstructBLIP [13]</cell><cell>7B</cell><cell></cell><cell>34.8</cell><cell>6.2</cell><cell cols="3">17.3 51.9 84.4 14.3</cell><cell>39.0</cell><cell>2.9</cell><cell>17.0 52.3 78.0 44.6</cell></row><row><cell cols="2">LLaVA [42]</cell><cell>7B</cell><cell></cell><cell>38.7</cell><cell>8.6</cell><cell cols="3">25.3 53.1 77.8 28.6</cell><cell>39.7</cell><cell cols="2">13.3 35.0 48.6 82.9 18.9</cell></row><row><cell cols="2">mPLUG-Owl [75]</cell><cell>7B</cell><cell></cell><cell>41.0</cell><cell cols="4">18.5 18.7 66.7 86.7 14.3</cell><cell>45.9</cell><cell cols="2">16.2 23.0 59.8 91.5 39.2</cell></row><row><cell cols="2">Shikra [9]</cell><cell>7B</cell><cell></cell><cell>51.5</cell><cell cols="4">32.1 30.7 63.0 88.9 42.9</cell><cell>60.0</cell><cell cols="2">27.6 50.0 70.1 92.7 59.5</cell></row><row><cell cols="2">Kosmos-2 [28]</cell><cell>1.6B</cell><cell></cell><cell>54.4</cell><cell cols="4">38.3 33.3 56.8 91.1 52.4</cell><cell>58.2</cell><cell cols="2">40.4 30.0 65.4 89.0 66.2</cell></row><row><cell cols="2">LLaVA-1.5 [41]</cell><cell>7B</cell><cell></cell><cell>59.6</cell><cell cols="4">43.2 52.0 71.6 93.3 38.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Merlin (Ours)</cell><cell>7B</cell><cell></cell><cell>64.4</cell><cell cols="4">42.0 54.7 72.8 97.8 54.8</cell><cell>66.5</cell><cell cols="2">41.3 51.0 83.0 97.6 59.7</cell></row><row><cell></cell><cell cols="2">LaSOT</cell><cell></cell><cell></cell><cell cols="2">GOT10k</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Success P norm</cell><cell>P</cell><cell cols="3">AO SR 0.5 SR 0.75</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Specialist Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SiamFC [6]</cell><cell>33.6</cell><cell cols="2">42.0 33.9</cell><cell cols="2">34.8 35.3</cell><cell>9.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ATOM [14]</cell><cell>51.5</cell><cell>-</cell><cell>-</cell><cell cols="2">55.6 63.4</cell><cell>40.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SiamRPN++ [36]</cell><cell>49.6</cell><cell cols="2">56.9 49.1</cell><cell cols="2">51.8 61.8</cell><cell>32.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SiamFC++ [73]</cell><cell>54.4</cell><cell cols="2">62.3 54.7</cell><cell cols="2">59.5 69.5</cell><cell>47.9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Generalist Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Merlin (Ours)</cell><cell>39.8</cell><cell cols="2">40.2 38.1</cell><cell cols="2">51.4 55.9</cell><cell>42.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of Merlin with available tracking method. Notably, Merlin is only trained on a small sampled data from the relevant tracking datasets instead of the full one.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Zero-shot object hallucination evaluation on the COCO validation set. "Yes" represents the proportion of positive answers that the model outputs.</figDesc><table><row><cell>Method</cell><cell></cell><cell>LLM Size</cell><cell></cell><cell>Random</cell><cell></cell><cell></cell><cell></cell><cell>Popular</cell><cell></cell><cell></cell><cell>Adversarial</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Accuracy F1-Score</cell><cell>Yes</cell><cell cols="2">Accuracy F1-Score</cell><cell>Yes</cell><cell cols="2">Accuracy F1-Score</cell><cell>Yes</cell></row><row><cell>LLaVA [42]</cell><cell></cell><cell>13B</cell><cell>64.12</cell><cell>73.38</cell><cell></cell><cell>83.26</cell><cell>63.90</cell><cell>72.63</cell><cell>81.93</cell><cell>58.91</cell><cell>69.95</cell><cell>86.76</cell></row><row><cell>MiniGPT-4 [85]</cell><cell></cell><cell>13B</cell><cell>79.67</cell><cell>80.17</cell><cell></cell><cell>52.53</cell><cell>69.73</cell><cell>73.02</cell><cell>62.20</cell><cell>65.17</cell><cell>70.42</cell><cell>67.77</cell></row><row><cell>InstructBLIP [13]</cell><cell></cell><cell>13B</cell><cell>88.57</cell><cell>89.27</cell><cell></cell><cell>56.57</cell><cell>82.77</cell><cell>84.66</cell><cell>62.37</cell><cell>72.10</cell><cell>77.32</cell><cell>73.03</cell></row><row><cell>Shikra [9]</cell><cell></cell><cell>13B</cell><cell>86.90</cell><cell>86.19</cell><cell></cell><cell>43.26</cell><cell>83.97</cell><cell>83.16</cell><cell>45.23</cell><cell>83.10</cell><cell>82.49</cell><cell>46.50</cell></row><row><cell cols="2">MultiModal-GPT [22]</cell><cell>7B</cell><cell>50.10</cell><cell>66.71</cell><cell></cell><cell>99.90</cell><cell>50.00</cell><cell>66.67</cell><cell>100.00</cell><cell>50.00</cell><cell>66.67</cell><cell>100.00</cell></row><row><cell>mPLUG-Owl [75]</cell><cell></cell><cell>7B</cell><cell>53.97</cell><cell>68.39</cell><cell></cell><cell>95.63</cell><cell>50.90</cell><cell>66.94</cell><cell>98.57</cell><cell>50.67</cell><cell>66.82</cell><cell>98.67</cell></row><row><cell>LLaVA [42]</cell><cell></cell><cell>7B</cell><cell>72.16</cell><cell>78.22</cell><cell></cell><cell>76.29</cell><cell>61.37</cell><cell>71.52</cell><cell>85.63</cell><cell>58.67</cell><cell>70.12</cell><cell>88.33</cell></row><row><cell>Merlin</cell><cell></cell><cell>7B</cell><cell>91.58</cell><cell>91.66</cell><cell></cell><cell>49.38</cell><cell>89.53</cell><cell>89.56</cell><cell>50.27</cell><cell>84.10</cell><cell>84.95</cell><cell>55.63</cell></row><row><cell></cell><cell cols="2">VQA Task</cell><cell></cell><cell>Generalist</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="6">GQA VisWiz MMB d MMB t MM-Vet</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BLIP-2 [37]</cell><cell>41.0</cell><cell>19.6</cell><cell>-</cell><cell>-</cell><cell>22.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>InstructBLIP [13]</cell><cell>49.2</cell><cell>34.5</cell><cell>36.0</cell><cell>33.9</cell><cell>26.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Shikra [9]</cell><cell>-</cell><cell>-</cell><cell>58.8</cell><cell>60.2</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IDEFICS-9B [35]</cell><cell>38.4</cell><cell>35.5</cell><cell>48.2</cell><cell>45.3</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IDEFICS-80B [35]</cell><cell>45.2</cell><cell>36.0</cell><cell>54.5</cell><cell>54.6</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Qwen-VL  ? [4]</cell><cell>59.3</cell><cell>35.2</cell><cell>38.2</cell><cell>32.2</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Qwen-VL-Chat  ? [4] 57.5</cell><cell>38.9</cell><cell>60.6</cell><cell>61.8</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LLaVA-1.5 [41]</cell><cell>62.0</cell><cell>50.0</cell><cell>64.3</cell><cell>59.5</cell><cell>30.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Merlin (Ours)</cell><cell>60.5</cell><cell>50.4</cell><cell>66.2</cell><cell>65.5</cell><cell>34.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparison with SOTA methods on main MLLM benchmarks.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Ablation study of the proposed strategies in Merlin. (ITP: Image-text pair data, ITD: instruction-tuning data). We mainly report the AO score of GOT10k and the average score of future reasoning.</figDesc><table><row><cell>ITP FPT-Data</cell><cell>ITD FIT-Data</cell><cell>AO</cell><cell>Average dev</cell></row><row><cell>?</cell><cell>?</cell><cell>-</cell><cell>59.5</cell></row><row><cell>?</cell><cell></cell><cell>-</cell><cell>60.7</cell></row><row><cell>?</cell><cell></cell><cell>15.5</cell><cell>52.8</cell></row><row><cell></cell><cell>?</cell><cell>51.4</cell><cell>61.2</cell></row><row><cell></cell><cell></cell><cell>51.4</cell><cell>64.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>All training data. Cap.:Captioning, Ref.:Referring (including REC, REG and Referring Tracking), Det.: Detection, Track:Tracking (including single object tracking (SOT) and multiple object tracking (MOT)), Rea.:Reasoning, Dia.:Dialogue. * means that the data is only used in the SFT stage.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Training hyperparameters of Merlin. The hyperparameter placed in the middle indicates that this hyperparameter is used in both stages.</figDesc><table><row><cell></cell><cell>Setting</cell><cell cols="2">M etrics</cell></row><row><cell></cell><cell cols="3">Precise Task description GOT10K Future Rea</cell></row><row><cell>1</cell><cell>w</cell><cell>51.4</cell><cell>64.4</cell></row><row><cell>2</cell><cell>w/o</cell><cell>28.4</cell><cell>62.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>Comparisions between Merlin with (w) and without (w/o) the precise task description. We minaly report the AO score of GOT10K and the average score of Future Reasoning.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Merlin is a legendary character in the tales of King Arthur, renowned as a powerful wizard and a wise counselor in the Arthurian legends. He is frequently depicted as having the power to foresee future events and has a deep understanding of fate and destiny.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Flamingo: a visual language model for few-shot learning</title>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yana</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serkan</forename><surname>Cabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sina</forename><surname>Samangooei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marianne</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><forename type="middle">L</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aida</forename><surname>Nematzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahand</forename><surname>Sharifzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikolaj</forename><surname>Binkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Barreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kar?n</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuel</forename><surname>Taropa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paige</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.10403</idno>
		<title level="m">Palm 2 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Openflamingo: An opensource framework for training large autoregressive visionlanguage models</title>
		<author>
			<persName><forename type="first">Anas</forename><surname>Awadalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irena</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuf</forename><surname>Hanafy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanrong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Kalyani Marathe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samir</forename><surname>Bitton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiori</forename><surname>Gadre</surname></persName>
		</author>
		<author>
			<persName><surname>Sagawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.01390</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Jinze</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.12966</idno>
		<title level="m">Qwen-vl: frontier large vision-language model with versatile abilities</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Humans predict liquid dynamics using probabilistic simulation</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilker</forename><surname>Yildirim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CogSci</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="850" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Token merging: Your vit but faster</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR. OpenReview.net</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Shikra: Unleashing multimodal llm&apos;s referential dialogue magic</title>
		<author>
			<persName><forename type="first">Keqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weili</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.15195</idno>
		<imprint>
			<date type="published" when="2023">2023. 3, 5, 6, 7, 10</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<ptr target="https://lmsys.org/blog/2023-03-30-vicuna/,2023.3" />
		<title level="m">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Transformers predicting the future. applying attention in next-frame and time series forecasting</title>
		<author>
			<persName><forename type="first">Radostin</forename><surname>Cholakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Kolev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.08224</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenkai</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.05170</idno>
		<title level="m">Sportsmot: A large multi-object tracking dataset in multiple sports scenes</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Instructblip: Towards generalpurpose vision-language models with instruction tuning</title>
		<author>
			<persName><forename type="first">Wenliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huat</forename><surname>Tiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weisheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.06500</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Atom: Accurate tracking by overlap maximization</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4660" to="4669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mevis: A large-scale benchmark for video segmentation with motion expressions</title>
		<author>
			<persName><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuting</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Runpei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunrui</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zekun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinrong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianjian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.11499</idno>
		<title level="m">Synergistic multimodal comprehension and creation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno>ICLR. OpenReview.net, 2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lasot: A high-quality benchmark for large-scale single object tracking</title>
		<author>
			<persName><forename type="first">Liting</forename><surname>Heng Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hexin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2009">2019. 2, 5, 6, 7, 9</date>
			<biblScope unit="page" from="5374" to="5383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The free-energy principle: a unified brain theory?</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature reviews neuroscience</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="138" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Planting a seed of vision in large language model</title>
		<author>
			<persName><forename type="first">Yuying</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.08041</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Tao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yudong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuikun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.04790</idno>
		<title level="m">Multimodal-gpt: A vision and language model for dialogue with humans</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Inductive biases for deep learning of higher-level cognition</title>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR, abs/2011.15091</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Vizwiz grand challenge: Answering visual questions from blind people</title>
		<author>
			<persName><forename type="first">Danna</forename><surname>Gurari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abigale</forename><forename type="middle">J</forename><surname>Stangl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">P</forename><surname>Bigham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3608" to="3617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Vizwiz grand challenge: Answering visual questions from blind people</title>
		<author>
			<persName><forename type="first">Danna</forename><surname>Gurari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abigale</forename><forename type="middle">J</forename><surname>Stangl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">P</forename><surname>Bigham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3608" to="3617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16000" to="16009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Got-10k: A large high-diversity benchmark for generic object tracking in the wild</title>
		<author>
			<persName><forename type="first">Lianghua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1562" to="1577" />
			<date type="published" when="2009">2019. 2, 5, 6, 7, 8, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Language is not all you need: Aligning perception with language models</title>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaru</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saksham</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengchao</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owais</forename><surname>Khan Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.14045</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gqa: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Identifying natural images from human brain activity</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kendrick N Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">J</forename><surname>Naselaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">L</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName><surname>Gallant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">452</biblScope>
			<biblScope unit="issue">7185</biblScope>
			<biblScope unit="page" from="352" to="355" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName><forename type="first">Sahar</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2005">2014. 2, 5</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yu Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.17216</idno>
		<title level="m">Generating images with multimodal language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International</title>
		<author>
			<persName><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Obelics: An open web-scale filtered dataset of interleaved image-text documents</title>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Hugo Laurenc ?on</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L?o</forename><surname>Saulnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stas</forename><surname>Tronchon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Bekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lozhkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Karamcheti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">SiamRPN++: Evolution of siamese visual tracking with very deep networks</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4282" to="4291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.12597</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multisports: A multi-person video dataset of spatio-temporally localized sports actions</title>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinpeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.10355</idno>
		<title level="m">Evaluating object hallucination in large vision-language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.03744</idno>
		<title level="m">Improved baselines with visual instruction tuning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.03744</idno>
		<title level="m">Improved baselines with visual instruction tuning</title>
		<imprint>
			<date type="published" when="2006">2023. 3, 6</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Visual instruction tuning</title>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2023. 3, 5, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haodong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yike</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conghui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.06281</idno>
		<title level="m">Is your multi-modal model an all-around player? arXiv preprint</title>
		<imprint>
			<date type="published" when="2008">2023. 3, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Titan: Future forecast using action priors</title>
		<author>
			<persName><forename type="first">Srikanth</forename><surname>Malla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behzad</forename><surname>Dariush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiho</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<author>
			<persName><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00831</idno>
	</analytic>
	<monogr>
		<title level="m">A benchmark for multi-object tracking</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<author>
			<persName><surname>Chatgpt</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/chatgpt/,2023.3" />
		<imprint>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">OpenAI. Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2005">2023. 2, 4, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fraser</forename><surname>Kelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maddie</forename><surname>Simens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">F</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhe</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanze</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Detgpt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14167</idno>
		<title level="m">Detect what you need via reasoning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno>PMLR, 2021. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5485" to="5551" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A system in the human brain for predicting the actions of others</title>
		<author>
			<persName><forename type="first">Narender</forename><surname>Ramnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miall</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="85" to="90" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Tokenlearner: What can 8 learned tokens do for images and videos?</title>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
		<idno>CoRR, abs/2106.11297, 2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Neural theory-of-mind? on the limits of social intelligence in large lms</title>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3762" to="3780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Laion-400m: Open dataset of clip-filtered 400 million image-text pairs</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Vencu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Kaczmarczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clayton</forename><surname>Mullis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aarush</forename><surname>Katta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Coombes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenia</forename><surname>Jitsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aran</forename><surname>Komatsuzaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.02114</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Crowdhuman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00123</idno>
		<title level="m">A benchmark for detecting human in a crowd</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Objects365: A large-scale, high-quality dataset for object detection</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Objects365: A large-scale, high-quality dataset for object detection</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Sompt22: A surveillance oriented multi-pedestrian tracking dataset</title>
		<author>
			<persName><forename type="first">Emre</forename><surname>Fatih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cevahir</forename><surname>Simsek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Cigla</surname></persName>
		</author>
		<author>
			<persName><surname>Kayabol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Towards vqa models that can read</title>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meet</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8317" to="8326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Dancetrack: Multi-object tracking in uniform appearance and diverse motion</title>
		<author>
			<persName><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinkun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>2022. 2, 5</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<ptr target="https://github.com/tatsu-lab/stanford_alpaca,2023.3" />
		<title level="m">Stanford alpaca: An instruction-following llama model</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timoth?e</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozi?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Llama: Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Llama 2: Open foundation and fine-tuned chat models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Large language models fail on trivial alterations to theory-of-mind tasks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tomer</surname></persName>
		</author>
		<author>
			<persName><surname>Ullman</surname></persName>
		</author>
		<idno>CoRR, abs/2302.08399</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Emergent abilities of large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Star: A benchmark for situated reasoning in real-world videos</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoubin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenfang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Siamfc++: Towards robust and accurate visual tracking with target estimation guidelines</title>
		<author>
			<persName><forename type="first">Yinda</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="140" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Videogpt: Video generation using vq-vae and transformers</title>
		<author>
			<persName><forename type="first">Wilson</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10157</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<author>
			<persName><forename type="first">Qinghao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiabo</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anwen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaya</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.14178</idno>
		<title level="m">mplug-owl: Modularization empowers large language models with multimodality</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Bdd100k: A diverse driving dataset for heterogeneous multitask learning</title>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haofeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vashisht</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2636" to="2645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.02490</idno>
		<title level="m">Mm-vet: Evaluating large multimodal models for integrated capabilities</title>
		<imprint>
			<date type="published" when="2008">2023. 3, 7, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">From recognition to cognition: Visual commonsense reasoning</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006">2019. 2, 5, 6</date>
			<biblScope unit="page" from="6720" to="6731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<author>
			<persName><forename type="first">Aohan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyu</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02414</idno>
		<title level="m">Glm-130b: An open bilingual pre-trained model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Opt: Open pre-trained transformer language models</title>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01068</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<author>
			<persName><forename type="first">Shilong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoufa</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.03601</idno>
		<title level="m">Gpt4roi: Instruction tuning large language model on region-of-interest</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">En</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinrong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianjian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runpei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunrui</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09474</idno>
		<title level="m">Bootstrapping multimodal llms via precise referring instruction tuning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">En</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinrong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianjian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runpei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunrui</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09474</idno>
		<title level="m">Bootstrapping multimodal llms via precise referring instruction tuning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">How far are large language models from agents with theory-of-mind?</title>
		<author>
			<persName><forename type="first">Pei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranavi</forename><surname>Srividya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Potharaju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Mckee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Pujara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aida</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyam</forename><surname>Nematzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manaal</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><surname>Faruqui</surname></persName>
		</author>
		<idno>CoRR, abs/2310.03051</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Minigpt-4: Enhancing vision-language understanding with advanced large language models</title>
		<author>
			<persName><forename type="first">Deyao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.10592</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
