<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking Label Smoothing on Multi-hop Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhangyue</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<address>
									<country>Fudan University</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuxin</forename><surname>Wang</surname></persName>
							<email>wangyuxin21@m.fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<address>
									<country>Fudan University</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yiguang</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<address>
									<country>Fudan University</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hang</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<address>
									<country>Fudan University</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiannian</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<address>
									<country>Fudan University</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
							<email>zhangxinyu35@huawei.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Huawei Poisson Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhao</forename><surname>Cao</surname></persName>
							<email>caozhao1@huawei.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Huawei Poisson Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<email>xjhuang@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<address>
									<country>Fudan University</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
							<email>xpqiu@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<address>
									<country>Fudan University</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking Label Smoothing on Multi-hop Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Label smoothing is a regularization technique widely used in supervised learning to improve the generalization of models on various tasks, such as image classification and machine translation. However, the effectiveness of label smoothing in multi-hop question answering (MHQA) has yet to be well studied. In this paper, we systematically analyze the role of label smoothing on various modules of MHQA and propose F1 smoothing, a novel label smoothing technique specifically designed for machine reading comprehension (MRC) tasks. We evaluate our method on the HotpotQA dataset and demonstrate its superiority over several strong baselines, including models that utilize complex attention mechanisms. Our results suggest that label smoothing can be effective in MHQA, but the choice of smoothing strategy can significantly affect performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Label smoothing is a regularization technique that has been widely used in supervised learning to improve the generalization of models on various tasks, such as image classification <ref type="bibr">(He et al., 2020b)</ref> and machine translation <ref type="bibr" target="#b11">(Gao et al., 2020;</ref><ref type="bibr">Lukasik et al., 2020b)</ref>. The basic idea of label smoothing is to smooth the distribution of true labels by replacing the one-hot encoding of the labels with a softened version <ref type="bibr" target="#b30">(Szegedy et al., 2016)</ref>. This encourages the model to be less confident in its predictions and to consider a wider range of possibilities, which can reduce overfitting and improve generalization <ref type="bibr" target="#b23">(M?ller et al., 2019;</ref><ref type="bibr">Lukasik et al., 2020a)</ref>.</p><p>Multi-hop question answering (MHQA) is a task that involves answering complex questions by aggregating information from multiple sources. These tasks require a model to perform multiple * Equal contribution.</p><p>? Corresponding author.</p><p>reasoning steps and to handle varied structures of information. Mainstream QA models for MHQA often consist of a complex pipeline, including a document retriever, a supporting evidence selector, and a module for multi-hop reasoning <ref type="bibr" target="#b31">(Tu et al., 2020;</ref><ref type="bibr">Wu et al., 2021a;</ref><ref type="bibr">Li et al., 2022b)</ref>. These components work together to accurately retrieve and integrate relevant information from multiple sources in order to provide a correct answer to the given question. Despite the widespread use of label smoothing in other tasks, the effectiveness of this technique in MHQA has not to be thoroughly investigated. In this paper, we aim to fill this research gap by systematically analyzing the role of label smoothing on various modules of MHQA. We will conduct experiments using various label smoothing strategies and multiple label smoothing techniques, including F1 smoothing, a novel method we propose for machine reading comprehension (MRC) tasks, and evaluate its performance on the Hot-potQA dataset <ref type="bibr">(Yang et al., 2018a)</ref>.</p><p>To the best of our knowledge, we are the first to systematically study the effect of label smoothing on MHQA. Our experiments demonstrate that carefully designing label smoothing for each module of MHQA and using the appropriate label smoothing strategy can significantly improve the performance of each module and lead to overall improvements in the model. The code for our approach is available on GitHub 1 .</p><p>Our main contributions are as follows:</p><p>? We conduct a systematic analysis of the impact of label smoothing on various modules of MHQA, including document retrieval, supporting evidence prediction, answer type selection and answer span extraction. smoothing method that is specifically tailored for MRC tasks.</p><p>? We evaluate the proposed method on the Hot-potQA dataset and find that it outperforms several strong baseline models and achieves the best results. This demonstrates the effectiveness of our smart label smoothing design in MHQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Label Smoothing Label smoothing is a regularization technique that was introduced in computer vision to improve classification accuracy on Ima-geNet <ref type="bibr" target="#b30">(Szegedy et al., 2016)</ref>. The idea behind label smoothing is to prevent the model from becoming too confident in its predictions by slightly modifying the ground truth labels during training. This helps to improve the generalization of the model. Label smoothing has been widely adopted in a variety of natural language processing tasks, including speech recognition (Chorowski and Jaitly, 2017), document retrieval <ref type="bibr" target="#b25">(Penha and Hauff, 2021)</ref>, dialogue generation <ref type="bibr" target="#b28">(Saha et al., 2021)</ref> and neural machine translation <ref type="bibr" target="#b11">(Gao et al., 2020;</ref><ref type="bibr">Lukasik et al., 2020b;</ref><ref type="bibr" target="#b12">Gra?a et al., 2019)</ref>. In recent years, label smoothing has also been applied to span-related tasks such as machine reading comprehension and named entity recognition <ref type="bibr">(Zhao et al., 2020a;</ref><ref type="bibr" target="#b41">Zhu and Li, 2022)</ref>.</p><p>Multi-hop Question Answering Multi-hop reading comprehension (MHRC) is a challenging task in the field of machine reading comprehension (MRC) that closely resembles the human thought process in real-world scenarios. As a result, it has become a popular topic in the field of natural language understanding in recent years. To facilitate research in this area, several datasets have been developed, including HotpotQA <ref type="bibr">(Yang et al., 2018a)</ref>, WikiHop <ref type="bibr" target="#b32">(Welbl et al., 2018)</ref>, and Nar-rativeQA <ref type="bibr" target="#b16">(Ko?isk? et al., 2018)</ref>. Among these, HotpotQA is particularly representative and challenging, as it not only requires the model to extract the correct answer span from the context but also requires a series of supporting evidence as proof of MHRC. In this paper, we focus on HotpotQA as our primary dataset for studying label smoothing in the context of MHRC.</p><p>Recent advances in MHRC have led to the development of several graph-free models, such as QUARK <ref type="bibr" target="#b13">(Groeneveld et al., 2020)</ref>, C2FReader <ref type="bibr" target="#b29">(Shao et al., 2020)</ref>, and S2G <ref type="bibr">(Wu et al., 2021b)</ref>, which have challenged the dominance of previous graph-based approaches like DFGN <ref type="bibr" target="#b26">(Qiu et al., 2019)</ref>, SAE <ref type="bibr" target="#b31">(Tu et al., 2020)</ref>, and HGN <ref type="bibr" target="#b10">(Fang et al., 2020)</ref>. C2FReader <ref type="bibr" target="#b29">(Shao et al., 2020)</ref> suggests that the performance difference between graphical attention and self-attention is minimal, while S2G's <ref type="bibr">(Wu et al., 2021b</ref>) strong performance demonstrates the potential of not using graphical modeling in MHRC. FE2H <ref type="bibr">(Li et al., 2022a)</ref>, which uses a two-stage selector and a multitask reader, currently achieves the best performance on the MHRC task, indicating that pre-trained language models alone may be sufficient for modeling multi-hop reasoning. Motivated by the design of S2G <ref type="bibr">(Wu et al., 2021b)</ref> and FE2H <ref type="bibr">(Li et al., 2022a)</ref>, we propose a simpler model called C2FM that does not include an additional attention module. Our aim is to examine the impact of label smoothing on individual models of MHQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Framework</head><p>We first introduce our multi-hop question answering architecture to facilitate our description of label smoothing. Our framework uses a Coarse-to-Fine retriever and a Multi-task prediction reader (C2FM) for answer extraction and supporting evidences prediction. Compared to the complex structural design of S2G <ref type="bibr">(Wu et al., 2021a)</ref>, the simpler design of C2FM is more conducive for us to investigate the role of label smoothing in each component of the framework.</p><p>Figure <ref type="figure">1</ref> illustrates the overall framework of C2FM. In the document retrieval module, we use a coarse-to-fine retrieval approach to identify the most relevant documents for a given question. In this example, Doc1 and Doc4 are marked as relevant documents. In the coarse stage, the top k documents (which we set to 3 in this paper) are retrieved, resulting in Doc1, Doc3, and Doc4 as the most likely to contain the answer. In the finegrained stage, we combine documents two by two and retrieve them based on their relationship to each other, which is crucial for multi-hop QA. Doc1 and Doc4 are the correct combinations that we want to pass on to the downstream multi-task reading module, hence the label 1. This two-stage retrieval process allows our model to capture not only the relationship between the question and the candidate documents, but also the relationship between the most promising candidate documents.</p><p>In the reading comprehension module, we use  a multi-task learning approach to simultaneously optimize three goals: answer type selection, answer span extraction, and evidence sentence prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Coarse-to-Fine Retrieval</head><p>Coarse Retrieval Module In the formal MHRC task, each question Q is typically provided with a set of m documents {D 1 , D 2 . . . , D m }, only a few of which (two in HotpotQA) are truly relevant to Q. In coarse retrieval, we optimize the classification of each combination of question and document using the Cross-Entropy loss. Specifically, we treat every document D i as a two-class classification problem.</p><formula xml:id="formula_0">L coarse = E[- 1 M M i=1 (y coarse i ? log(? coarse i ) +(1 -y coarse i ) ? log(1 -?coarse i ))]<label>(1)</label></formula><p>where ?coarse i is the probability predicted by the model and y coarse i is the one-hot encoded groundtruth distribution.</p><formula xml:id="formula_1">y coarse i = 1 D i is a related document. 0 D i is an unrelated document.</formula><p>(2) Fine Retrieval Module In fine-grained retrieval, we select the top three relevant articles from the previous step and combine them in pairs to create a set of document pairs {C 1 , C 2 , C 3 } with C 2 3 total combinations. We then focus on the interactions between these document pairs, which are essential for multi-hop question answering, and optimize using cross-entropy loss.</p><formula xml:id="formula_2">L f ine = E[- 3 i=1 y f ine i log(? f ine i )]</formula><p>(3)</p><p>We use ?fine i to represent the document pair probability predicted by our model and yi f ine to represent the one-hot encoded ground-truth distribution.</p><formula xml:id="formula_3">y f ine i = 1 C i consists of two related documents. 0 otherwise.</formula><p>(4) Thus, when the coarse retrieval fails to retrieve two related documents, y f ine i is all zero, indicating that it does not contribute to the model's performance until the coarse retrieval is sufficient. We use a single pre-trained language model as the encoder for both the coarse and fine retrieval steps, and combine the retrieval losses using a weighted sum. In our paper, both ? 1 and ? 2 are set to 1, indicating that the fine retrieval contributes equally to the model's performance as the coarse retrieval</p><formula xml:id="formula_4">L retrieval = ? 1 L coarse + ? 2 L f ine .</formula><p>(5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-Task Reader</head><p>In the reading comprehension module, we use multi-task learning to simultaneously predict Evidence Sentences and extract answer span. In order to better evaluate the role of label smoothing, we do not include an additional attention module in our model. Instead, we focus on the effects of label smoothing on the performance of the main reading comprehension module. In addition, Hot-potQA contains samples with yes/no answers. The practice of splicing "yes" and "no" tokens at the beginning of the sequence <ref type="bibr">(Li et al., 2022a</ref>) could corrupt the original text's semantic information. To avoid the impact of additional information on label smoothing analysis, we introduce an answer type selection header trained with a cross-entropy loss function.</p><formula xml:id="formula_5">L type = E[- 3 i=1 y type i log(? type i )]<label>(6)</label></formula><p>where ?i f ine denotes the predicted probability distribution over answer types generated by our model, and yi f ine represents the corresponding one-hot encoded ground-truth distribution.</p><formula xml:id="formula_6">y type i = ? ? ? 0 answer is no 1 answer is yes 2 answer is a span (7)</formula><p>To extract the span of answers, we use a linear prediction layer on the contextual representation to identify the start and end positions of answers, and employ cross-entropy as the loss function. The corresponding loss terms are denoted as L start and L end , respectively. Similar to previous work in the field, such as S2G <ref type="bibr">(Wu et al., 2021a)</ref> and FE2H <ref type="bibr">(Li et al., 2022a)</ref>, we also inject a special placeholder token "&lt;/e&gt;" and use a linear binary classifier on the output of "&lt;/e&gt;" to determine whether a sentence is a supporting fact. The classification loss of the supporting facts is denoted as L sup , and we jointly optimize all of these objectives in our model.</p><formula xml:id="formula_7">L reading = ? 3 L type +? 4 (L start +L end )+? 5 L sup</formula><p>(8) Similarly, we set ? 3 and ? 4 and ? 5 all to 1, giving equal importance to each module for multitask learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Label Smoothing</head><p>Label smoothing is a regularization technique that aims to reduce over-fitting in a classifier by modifying the ground truth labels of the training data. In the one-hot setting, the probability of the correct category q(y|x) for a training sample (x, y) is typically defined as 1, while the probabilities of all other categories q( y|x) are defined as 0. The cross-entropy loss function used in this setting is typically defined as follows:</p><formula xml:id="formula_8">L = - K k=1 q(k|x) log(p(k|x))<label>(9)</label></formula><p>where p(k|x) is the probability of the model's prediction for the k-th class. Specifically, label smoothing mixes q(k|x) with a uniform distribution u(k), independent of the training samples, to produce a new distribution q (k|x).</p><formula xml:id="formula_9">q (k|x) = (1 -)q(k|x) + u(k)<label>(10)</label></formula><p>We denote as the weight that controls the importance of q(k|x) and u(k) in the resulting distribution. u(k) is construed as 1 K of the uniform distribution, where K is the total number of categories. Next, we will explore the role of Label Smoothing in each of the modules in C2FM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Label Smoothing in Retrieval Module</head><p>Coarse Retrieval We apply Eq.1 to Eq.10. Additionally, SAE <ref type="bibr" target="#b31">(Tu et al., 2020)</ref> and S2G <ref type="bibr">(Wu et al., 2021a)</ref> both prioritize documents containing answer spans, named gold documents. Therefore, we introduce a answer aware distribution y gold and obtain a new hybird distribution y coarse i .</p><formula xml:id="formula_10">y coarse i = (1 -)y coarse i + u(x) + y gold i (11)</formula><p>where u(x) is a uniform distribution and y gold is defined as follows.</p><formula xml:id="formula_11">y gold i = 1</formula><p>D i contains answer 0 D i does not contain answer (12) Fine Retrieval In section 3.1, we described how the loss of fine retrieval is 0 when coarse retrieval fails to retrieve two relevant documents. In addition, we use a pre-trained model to learn both retrieval processes simultaneously. However, when fine retrieval starts training, the model already has some document retrieval capabilities. In this case, applying label smoothing during the fine retrieval phase could obscure the goal of model training and potentially hinder the model's performance. Therefore, we choose not to use label smoothing in the fine retrieval phase. For the sake of completeness, we include the results of experiments using label smoothing in the fine retrieval phase in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Label Smoothing in Reading Module</head><p>In multi-task prediction, there are three types of loss: L type , L sentence , and L span . L type and L sentence are losses for simple classification tasks, which can be smoothed using normal methods. L span is the loss for answer extraction, which involves identifying the start and end positions of a span. Due to the specific nature of this task, a different smoothing method may be required to achieve optimal results. Previous research <ref type="bibr">(Zhao et al., 2020a)</ref> has explored various label smoothing methods for machine reading comprehension, including normal label smoothing and word overlap smoothing. Motivated by the concept of word overlapping, we propose a more mathematically consistent extension of label smoothing to tasks with F1 scores, named F1 Smoothing.</p><p>Consider a sample x that contains a context S and an answer a gold . The total length of the context is denoted by L. We use q s (t|x) to denote the F1 score between a span of arbitrary length starting at position t in S and the ground truth answer a gold . Similarly, q e (t|x) denotes the F1 score between a span of arbitrary length ending at position t in S and a gold .</p><formula xml:id="formula_12">q s (t|x) = L-1 ?=t F 1 ((t, ?), a gold )<label>(13)</label></formula><formula xml:id="formula_13">q e (t|x) = t ?=0 F 1 ((?, t), a gold )<label>(14)</label></formula><p>The normalized distributions are noted as q s (t|x) and q e (t|x), respectively. q s (t|x) = exp(q s (t|x))</p><p>L-1 i=0 exp(q s (i|x))</p><p>q e (t|x) = exp(q e (t|x))</p><p>L-1 i=0 exp(q e (i|x))</p><p>In order to reduce the computational overhead of F1 Smoothing, we present a fast version of the computation in Appendix B. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, F1 Smoothing provides a more precise labeling of tokens compared to Word Overlapping, while also decreasing the probability of irrelevant tokens and preventing incorrect guidance of the model during training. This makes F1 Smoothing an effective method for multi-hop question answering tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset</head><p>We evaluate our approach on the distractor setting of HotpotQA <ref type="bibr">(Yang et al., 2018b)</ref>, a multihop question-answer dataset with 90k training samples, 7.4k validation samples, and 7.4k test samples. Each question in this dataset is provided with several candidate documents, two of which are relevant to the question, while the others are irrelevant. In addition to this, HotpotQA also provides supporting evidence for each question, encouraging the model to explain the inference path of the multihop question-answer. We use the Exact Match (EM) and F1 scores (F1) to evaluate the performance of our approach in terms of related document retrieval, supporting evidence prediction, and answer extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Details</head><p>Our model is built using the Pre-trained language models (PLMs) provided by HuggingFace's Transformers library <ref type="bibr" target="#b33">(Wolf et al., 2020)</ref>.</p><p>Coarse-to-Fine Retriever We employed the large version of RoBERTa <ref type="bibr" target="#b20">(Liu et al., 2019)</ref> and ELECTRA <ref type="bibr" target="#b9">(Clark et al., 2020)</ref> as our PLMs and conducted the ablation study on RoBERTalarge <ref type="bibr" target="#b20">(Liu et al., 2019)</ref>. We used a single RTX3090 GPU, set the number of epochs to 8, and the batch size to 16. For optimizer, we used the AdamW optimizer with a learning rate of 6e-6 and a weight decay of 1e-2.</p><p>Multi-Task Reader We utilized the large version of RoBERTa <ref type="bibr" target="#b20">(Liu et al., 2019)</ref> and the XXLarge version of DeBERTa <ref type="bibr">(He et al., 2020a)</ref> as our PLMs and conducted ablation studies on RoBERTa-large <ref type="bibr" target="#b20">(Liu et al., 2019)</ref>. For the RoBERTa-large model, we employed an RTX3090 GPU and set the number of epochs to 16 and the batch size to 16. For the DeBERTa-v2-xxlarge model, due to the larger number of parameters, we used an A100 GPU and set the number of epochs to 8 and the batch size to 16. We also utilized the AdamW optimizer with a learning rate of 4e-6 and a weight decay of 1e-2 for optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Results</head><p>We employ ELECTRA <ref type="bibr" target="#b9">(Clark et al., 2020)</ref> as the PLM for the retrieval module and DeBERTa-v2xxlarge as the PLM for the reading comprehension module. Our model, dubbed C2FM with multiple label smoothing, is tested on the HotpotQA test set with the distractor setting. As shown in Table <ref type="table">1</ref>, C2FM with multiple label smoothing achieves an improvement of 0.8% and 0.77% in EM and F1 for answer, and 0.19% and 0.57% in EM and F1 for supporting evidence compared to the C2FM model. Among the label smoothing techniques we experimented with, F1 smoothing yielded the most significant performance improvement and thus we named our model C2FM with F1 Smoothing, or C2FM-F1 for short.</p><p>We compare the performance of our proposed Coarse-to-Fine Retrieval method, which utilizes ELECTRA as a backbone for training, with three advanced works: SAE, S2G, and FE2H. These methods also employ elaborate selectors to retrieve relevant documents. We evaluate the performance of the document retrieval using the EM and F1 metrics. As shown in Table <ref type="table" target="#tab_3">2</ref>, our Coarse-to-Fine retrieval method outperforms these three strong baselines. Moreover, the performance can be further improved through the use of label smoothing.</p><p>In Table <ref type="table" target="#tab_4">3</ref>, we compare the performance of a Multi-task Reader trained with DeBERTa-v2xxlarge <ref type="bibr">(He et al., 2020a)</ref> as the backbone on the documents retrieved by the Coarse-to-Fine Retriever. Our results show that the C2FM model outperforms the strong baseline SAE <ref type="bibr" target="#b31">(Tu et al., 2020)</ref> and S2G <ref type="bibr">(Wu et al., 2021b)</ref>, and the use of label smoothing techniques can further improve model performance. Overall, these results demonstrate the value of the C2FM approach and the potential for further performance improvements through the use of label smoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Smoothing Analysis</head><p>In our study of the role of label smoothing, we used RoBERTa-large <ref type="bibr" target="#b20">(Liu et al., 2019)</ref> as the backbone for our model. To ensure the reliability of our experimental results, we conducted multiple runs with different random number seeds <ref type="bibr">(41, 42, 43, and 44</ref>) to ensure stability.</p><p>Label smoothing contains a hyperparameter to modify the target label probabilities of a model during training. In order to improve the effectiveness of label smoothing, <ref type="bibr" target="#b36">(Xu et al., 2020)</ref> proposed TSLA, a two-stage learning approach that applies label smoothing in the first stage and trains the model in the normal way in the second stage. We propose that Epsilon can also be decayed linearly, similar to the way the learning rate is often decayed. In our experiments, we compared three label smoothing strategies: Constant, TSLA, and Linear Decay. The initial value of Epsilon in our experiments was 0.1, and in the first stage of TSLA, the number of epochs was set to 4. For each epoch in the linear decay strategy, Epsilon was decreased by 0.01.</p><p>Coarse Retrieval In our analysis presented in Table <ref type="table" target="#tab_5">4</ref>, we observed that introducing y gold did not Table 1: In the distractor setting of the HotpotQA test set, our comparison showed that the C2FM model with various label smoothing methods significantly outperforms the original model. Among the label smoothing methods we evaluated, C2FM with F1 smoothing achieved the best results, surpassing the state-of-the-art performance in the literature. These findings demonstrate the effectiveness of introducing label smoothing in multi-hop question answering tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>EM F1 SAE large <ref type="bibr" target="#b31">(Tu et al., 2020)</ref> 91.98 95.76 S2G large <ref type="bibr">(Wu et al., 2021b)</ref> 95.77 97.82 FE2H large <ref type="bibr">(Li et al., 2022a)</ref>   significantly improve the performance of the coarse retrieval module. One possible explanation for this result is that the inclusion of y gold may have exacerbated the overconfidence of the model. Our findings on the Constant label smoothing strategy were consistent with <ref type="bibr" target="#b25">(Penha and Hauff, 2021)</ref>, which showed that it did not significantly improve the retrieval module's performance. However, we found that using the TSLA and Linear Decay strategies effectively stimulated the potential of label smoothing, resulting in improved generalization for the retrieval model.</p><p>Supporting Evidence Prediction We evaluated the impact of using different label smoothing strategies on the performance of our model on supporting evidence prediction. As shown in Table <ref type="table" target="#tab_6">5</ref>, regular label smoothing had a negligible effect on the model's performance. On the other hand, the TSLA strategy resulted in an average improvement of 0.22% in EM and 0.48% in F1. cay strategy also yielded positive results, with an average gain of 0.35% in EM and 0.69% in F1. These results suggest that both TSLA and Linear Decay may be effective strategies for improving the performance of supporting evidence prediction through label smoothing.</p><p>Answer Span Extraction Table <ref type="table" target="#tab_7">6</ref> demonstrates the effect of applying different label smoothing techniques on the performance of answer span extraction. Our findings are consistent with previous research <ref type="bibr">(Zhao et al., 2020b)</ref>, which showed that label smoothing can improve model performance. Specifically, we found that Word Overlapping resulted in an average F1 improvement of 0.3% and an average EM improvement of 0.12%, while F1 Smoothing resulted in an average F1 improvement of 0.63% and an average EM improvement of 0.49%. These results suggest that F1 Smoothing is an effective technique for improving the performance of the reading comprehension module.</p><p>Answer Type Selection We implemented label smoothing on the answer type selection task. We evaluated the classification performance using accuracy and the results are shown in Table <ref type="table" target="#tab_8">7</ref>. Despite the relatively simple nature of this task, we achieved a very high accuracy rate. However, we found that the use of label smoothing did not significantly improve the performance of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Error Analysis</head><p>To better understand the role of label smoothing for the overall architecture, we conducted an error analysis following the approach of S2G <ref type="bibr">(Wu et al., 2021b)</ref> on our C2FM and C2FM-F1 model. Ta- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we present C2FM, a simple architecture for the HotpotQA dataset, and systematically analyze the effect of label smoothing on various modules of multi-hop question answering (MHQA). We also propose F1 smoothing, a novel label smoothing technique specifically designed for machine reading comprehension (MRC) tasks. Our experiments on the HotpotQA dataset demonstrate that C2FM with label smoothing outperforms several strong baselines, highlighting the effectiveness of label smoothing in MHQA. However, our results also show that the choice of smoothing strategy is critical for achieving optimal performance.</p><p>Overall, our work contributes to a deeper understanding of the role of label smoothing in MHQA and introduces a new label smoothing technique that can be applied to improve the performance of MRC models. We believe our findings will be valuable for researchers and practitioners working on MRC and MHQA tasks and hope they will inspire further research in this area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix A</head><p>Table <ref type="table" target="#tab_10">9</ref> presents the results of our experiments on label smoothing for the fine retrieval module. Our findings indicate that the use of label smoothing, regardless of the strategy employed, negatively impacts model performance. Based on these results, we do not recommend implementing label smoothing in this module. This conclusion aligns with our analysis in Section 4.1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Appendix B</head><p>We use L a = e * -s * + 1 and L p = e -s + 1 to denote respectively the length of gold answer and predicted answer. As mentioned in 4.2, q s (t|x) = 2(e -s * + 1) L p + L a .</p><p>(21) In equation 20 and 21, L p = i -s + 1.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization of original distribution and different label smoothing distributions, including Label Smoothing, Word Overlapping, and F1 Smoothing. The first row shows the distribution of the start token, and the second row shows the distribution of the end token. The gold start and end tokens are highlighted in red.</figDesc><graphic url="image-6.png" coords="5,293.39,156.04,119.54,99.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Coarse Retrieval Module (c) Multi-task Prediction Reader (b) Fine Retrieval Module</head><label></label><figDesc>Figure 1: Overview of our C2FM model, which consists of three main modules: Coarse Retrieval, Fine Retrieval, and the Multi-Task Reader.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Label</cell><cell></cell><cell cols="2">?? ????????????</cell><cell>? ?? ????????????</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Label</cell><cell>?? ???????? ? ?? ????????</cell></row><row><cell>??</cell><cell>?? ??</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>?? ?? ?? ??</cell><cell cols="12">?? ?? ?? ?? ?? ?? ?? ???? (a) ?? ?? ?? ?? ?? ?? ? ?? ?? ?? 1</cell><cell>?? ?? ?? ?? ?? ??</cell><cell>1 1 1 0 0 1</cell><cell>Cross-Entropy 1 0.6 0.3 0 0 0.1</cell></row><row><cell cols="2">Answer Type</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Supporting Evidence</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Answer Span</cell></row><row><cell></cell><cell>[CLS]</cell><cell>?? ??</cell><cell>?</cell><cell>?? ??</cell><cell>&lt;/d&gt;</cell><cell>&lt;/e&gt;</cell><cell>?? ????</cell><cell>?? ????</cell><cell>?</cell><cell>&lt;/e&gt;</cell><cell>? &lt;/d&gt;</cell><cell>&lt;/e&gt;</cell><cell cols="2">?? ???? ?? ????</cell><cell>?</cell><cell>&lt;/e&gt; ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Question and Context Embedding</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Pretrained Model</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>[CLS]</cell><cell cols="3">????????????????</cell><cell>&lt;/d&gt;</cell><cell>&lt;/e&gt;</cell><cell cols="4">???????????????? ??</cell><cell cols="2">&lt;/d&gt; &lt;/e&gt;</cell><cell></cell><cell cols="2">???????????????? ??</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of our coarse-to-fine retriever with previous baselines on HotpotQA dev set. Label smoothing can further enhance model performance.</figDesc><table><row><cell>Model</cell><cell cols="2">Answer EM F1</cell><cell cols="2">Supporting EM F1</cell></row><row><cell>SAE</cell><cell cols="4">67.70 80.75 63.30 87.38</cell></row><row><cell>S2G</cell><cell>70.80</cell><cell>-</cell><cell>65.70</cell><cell>-</cell></row><row><cell>C2FM</cell><cell cols="4">71.39 83.84 66.32 89.54</cell></row><row><cell cols="5">C2FM-F1 71.89 84.65 66.75 90.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Performances of cascade results with label smoothing on the dev set of HotpotQA in the distractor setting.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation and strategy analysis on Coarse Retrieval Module with Label Smoothing.</figDesc><table><row><cell>Setting</cell><cell>F1</cell><cell>EM</cell></row><row><cell>Constant</cell><cell cols="2">97.94?.04 96.06?.11</cell></row><row><cell>w/o u(x)</cell><cell cols="2">97.91?.09 95.93?.05</cell></row><row><cell>w/o y gold</cell><cell cols="2">97.88?.08 95.89?.07</cell></row><row><cell>TSLA</cell><cell cols="2">98.05?.05 96.21?.01</cell></row><row><cell cols="3">Linear Decay 98.18?.04 96.57?.05</cell></row><row><cell>Setting</cell><cell>F1</cell><cell>EM</cell></row><row><cell>Constant</cell><cell cols="2">90.53?.02 66.88?.02</cell></row><row><cell>w/o u(x)</cell><cell cols="2">90.50?.02 66.94?.05</cell></row><row><cell>TSLA</cell><cell cols="2">90.72?.05 67.42?.05</cell></row><row><cell cols="3">Linear Decay 90.85?.03 67.63?.04</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>An analysis of the effectiveness of label smoothing for Supporting Evidence Prediction through ablation and strategy evaluation.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>The Linear De-Analysis of different label smoothing methods for Answer Span Extraction.</figDesc><table><row><cell>Methods</cell><cell>F1</cell><cell>EM</cell></row><row><cell>RoBERTa large</cell><cell cols="2">69.11?.02 82.21?.03</cell></row><row><cell>w. Label Smoothing</cell><cell cols="2">69.30?.02 82.56?.09</cell></row><row><cell cols="3">w. Word Overlapping 69.60?.09 82.68?.13</cell></row><row><cell>w. F1 Smoothing</cell><cell cols="2">69.93?.07 83.05?.10</cell></row><row><cell>Setting</cell><cell>Accuracy</cell><cell></cell></row><row><cell>Constant</cell><cell>99.43?.01</cell><cell></cell></row><row><cell>w/o u(x)</cell><cell>99.41?.01</cell><cell></cell></row><row><cell>TSLA</cell><cell>99.45?.04</cell><cell></cell></row><row><cell cols="2">Linear Decay 99.44?.02</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Answer Type Selection results with different smoothing strategies.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Error analysis on three types of errors: Incompleteness, Superfluity, and Multi-hop Reasoning errors.</figDesc><table><row><cell>Model</cell><cell cols="3">Incomplete Superfluous Multi-hop RC</cell></row><row><cell>C2FM</cell><cell>729</cell><cell>644</cell><cell>828</cell></row><row><cell>C2FM-F1</cell><cell>669</cell><cell>581</cell><cell>738</cell></row><row><cell cols="4">ble 8 shows three types of errors in our model's</cell></row><row><cell cols="4">predictions: incompleteness errors, superfluity er-</cell></row><row><cell cols="4">rors, and multi-hop reasoning errors. An incom-</cell></row><row><cell cols="4">pleteness error occurs when the predicted answer</cell></row><row><cell cols="4">span is smaller than the labeled answer span. A</cell></row><row><cell cols="4">superfluity error occurs when the predicted answer</cell></row><row><cell cols="4">span exceeds the labeled answer span. A multi-hop</cell></row><row><cell cols="4">reasoning error occurs when the predicted answer</cell></row><row><cell cols="4">span is offset from the labeled answer span due</cell></row><row><cell cols="4">to errors in the model's multi-hop reasoning. The</cell></row><row><cell cols="4">experimental results indicate that label smoothing</cell></row><row><cell cols="4">was effective in reducing incompleteness, super-</cell></row><row><cell cols="4">fluity, and multi-hop reasoning errors by 8.23%,</cell></row><row><cell cols="4">9.78%, and 10.87%, respectively. Among the three</cell></row><row><cell cols="4">types of errors examined, label smoothing had the</cell></row><row><cell cols="4">greatest impact on reducing Multi-hop Reasoning</cell></row><row><cell cols="4">errors, which decreased by 10.87%. Overall, these</cell></row><row><cell cols="4">results suggest that label smoothing is a effective</cell></row><row><cell cols="4">technique to consider when training a multi-hop</cell></row><row><cell cols="2">question answering model.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>An analysis of label smoothing on Fine Retrieval Module.</figDesc><table><row><cell></cell><cell>F1</cell><cell>EM</cell></row><row><cell>Constant</cell><cell cols="2">97.77?.06 95.91?.08</cell></row><row><cell>w/o u(x)</cell><cell cols="2">97.91?.09 95.93?.05</cell></row><row><cell>TSLA</cell><cell cols="2">97.79?.04 95.89?.09</cell></row><row><cell cols="3">Linear Decay 97.78?.02 95.90?.06</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>? t ? e * , we have the following distribution q s (t|x) =In equation 18 and 19, L p = e -i + 1.We can get q e (t|x) similarly. If t &gt; e * , q e (t|x) = ? t ? e * , q e (t|x) =</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>e  *  ?=s  *</cell><cell>2(e  *  -? + 1) L p + L a</cell><cell>+</cell><cell>s  *  -1 ?=0</cell><cell>2L a L p + L a</cell><cell>,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(20)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>else if s  e ?=s  *</cell><cell>2L p L p + L a</cell><cell>+</cell><cell>s  *  -1 ?=0</cell></row><row><cell></cell><cell></cell><cell>L-1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">F1 ((t, ?), a gold )</cell><cell>(17)</cell></row><row><cell></cell><cell></cell><cell>?=t</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">If t &lt; s  *  , the distribution is</cell><cell></cell><cell></cell></row><row><cell>q s (t|x) =</cell><cell>e  *  ?=s  *</cell><cell cols="3">2(? -s  *  + 1) L p + L a</cell><cell>+</cell><cell>L-1 ?=e  *  +1</cell><cell>2L a L p + L a</cell><cell>,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(18)</cell></row><row><cell cols="2">else if s  e  *  ?=s</cell><cell>2L p L p + L a</cell><cell>+</cell><cell cols="4">L-1 ?=e  (19)</cell></row></table><note><p>* * +1 2(e * -s + 1) L p + L a . *</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>There are three main limitations to our study: the limited scope of our model architecture, the assumptions we made in proposing F1 Smoothing, and the computational complexity of our experiments.</p><p>Limited scope Due to the specificity of our model architecture design, our study is only applicable to the classical multi-hop question answering (MHQA) dataset HotpotQA. Future work will involve designing a more general MHQA model architecture and studying a broader range of MHQA datasets.</p><p>Assumptions We proposed F1 Smoothing under the assumption that the span of answers is not unique. However, this assumption may not hold for all datasets, such as SQuAD <ref type="bibr" target="#b27">(Rajpurkar et al., 2018)</ref>, which may require the development of new smoothing methods.</p><p>Computational complexity Finally, our experiments on large versions of pre-trained language models (PLMs) to investigate the effectiveness of our smoothing method are computationally complex. As mentioned in Section 5.2, reproducing our results may require enough computational resources."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>In conducting this study, we considered the ethical implications of our work and ensured that this work complied with ACL's ethical policies. We used the publicly available HotpotQA dataset for our experiments, which does not raise potential ethical issues.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Yang</forename></persName>
		</author>
		<idno>45.60 59.02 20.32 64.49</idno>
	</analytic>
	<monogr>
		<title level="j">Model Answer Supporting EM F1 EM F1 Baseline Model</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Qfe (nishida</surname></persName>
		</author>
		<idno>86 68.06 57.75 84.49</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">53</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>Dfgn (qiu</surname></persName>
		</author>
		<idno>56.31 69.69 51.50 81.62</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><surname>Sae-Large (tu</surname></persName>
		</author>
		<idno>66.92 79.62 61.53 86.86</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">(</forename><surname>Reader</surname></persName>
		</author>
		<author>
			<persName><surname>Shao</surname></persName>
		</author>
		<idno>67.98 81.24 60.81 87.63</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><surname>Hgn-Large (fang</surname></persName>
		</author>
		<idno>69.22 82.19 62.76 88.47</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Electra (</forename><surname>Fe2h On</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno>70.53 83.37 63.57 88.83</idno>
		<imprint>
			<date type="published" when="2021">2022. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><surname>S2g+ega (wu</surname></persName>
		</author>
		<idno>71.89 84.44 64.98 89.14 C2FM (ours) 71.27 83.57 65.25 88.98 C2FM with F1 Smoothing (ours) 72.07 84.34 65.44 89.55</idno>
		<title level="m">FE2H on ALBERT</title>
		<imprint>
			<date type="published" when="2021">2021. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Towards better decoding and language model integration in sequence to sequence models</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><surname>Jaitly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In INTERSPEECH</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ELECTRA: pretraining text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hierarchical graph network for multi-hop question answering</title>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.710</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8823" to="8838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards a better understanding of label smoothing in neural machine translation</title>
		<author>
			<persName><forename type="first">Yingbo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Herold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</title>
		<meeting>the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing<address><addrLine>Suzhou, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="212" to="223" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generalizing back-translation in neural machine translation</title>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Gra?a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schamper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahram</forename><surname>Khadivi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-5205</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation</title>
		<meeting>the Fourth Conference on Machine Translation<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A simple yet strong pipeline for HotpotQA</title>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName><surname>Sabharwal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.711</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8839" to="8845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">2020a. Deberta: Decoding-enhanced bert with disentangled attention</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/2006.03654</idno>
		<imprint/>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Retinal optical coherence tomography image classification with label smoothing generative adversarial network</title>
		<author>
			<persName><forename type="first">Xingxin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Rabbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhimin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">405</biblScope>
			<biblScope unit="page" from="37" to="47" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The NarrativeQA reading comprehension challenge</title>
		<author>
			<persName><forename type="first">Tom??</forename><surname>Ko?isk?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G?bor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00023</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="317" to="328" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Asynchronous multi-grained graph network for interpretable multi-hop reading comprehension</title>
		<author>
			<persName><forename type="first">Ronghan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zejun</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3857" to="3863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">From easy to hard: Two-stage selector and reader for multi-hop question answering</title>
		<author>
			<persName><forename type="first">Xin-Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jun</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Bin</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/2205.11729</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">From easy to hard: Two-stage selector and reader for multi-hop question answering</title>
		<author>
			<persName><forename type="first">Xin-Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixian</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubin</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/2205.11729</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized bert pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Does label smoothing mitigate label noise?</title>
		<author>
			<persName><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Krishna Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
		<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<date type="published" when="2020-07-18">2020. 18 July 2020</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="6448" to="6458" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic label smoothing for sequence to sequence problems</title>
		<author>
			<persName><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.405</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4992" to="4998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">When does label smoothing help?</title>
		<author>
			<persName><forename type="first">Rafael</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. 2019. December 8-14, 2019</date>
			<biblScope unit="page" from="4696" to="4705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Answering while summarizing: Multi-task learning for multi-hop QA with evidence extraction</title>
		<author>
			<persName><forename type="first">Kosuke</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyosuke</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atsushi</forename><surname>Otsuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itsumi</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hisako</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junji</forename><surname>Tomita</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1225</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2335" to="2345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Weakly supervised label smoothing</title>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Penha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Hauff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="334" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dynamically fused graph network for multi-hop reasoning</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunxuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1617</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6140" to="6150" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Know what you don&apos;t know: Unanswerable questions for SQuAD</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-2124</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Similarity based label smoothing for dialogue generation</title>
		<author>
			<persName><forename type="first">Sougata</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Souvik</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohini</forename><surname>Srihari</surname></persName>
		</author>
		<idno>abs/2107.11481</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Is Graph Structure Necessary for Multi-hop Question Answering?</title>
		<author>
			<persName><forename type="first">Nan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.583</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7187" to="7192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.308</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas</title>
		<meeting><address><addrLine>NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27">2016. June 27-30, 2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Select, answer and explain: Interpretable multi-hop reading comprehension over multiple documents</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Constructing datasets for multi-hop reading comprehension across documents</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00021</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="287" to="302" />
		</imprint>
	</monogr>
	<note>Transactions of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">2021a. Graph-free multi-hop reading comprehension: A select-to-guide strategy</title>
		<author>
			<persName><forename type="first">Bohong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">2021b. Graph-free multi-hop reading comprehension: A select-to-guide strategy</title>
		<author>
			<persName><forename type="first">Bohong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<idno>abs/2107.11823</idno>
		<imprint/>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Towards understanding label smoothing</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
		<idno>abs/2006.11653</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">HotpotQA: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1259</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2369" to="2380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">HotpotQA: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1259</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2369" to="2380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust machine reading comprehension by learning soft labels</title>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangzhi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muyun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kehai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.248</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2754" to="2759" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Robust machine reading comprehension by learning soft labels</title>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangzhi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muyun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kehai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.248</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2754" to="2759" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Boundary smoothing for named entity recognition</title>
		<author>
			<persName><forename type="first">Enwei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinpeng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
