<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SE-GSL: A General and Effective Graph Structure Learning Framework through Structural Entropy Optimization</title>
				<funder>
					<orgName type="full">Xiaomi Young Scholar Funds for Beihang University</orgName>
				</funder>
				<funder ref="#_S65f5wn">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder>
					<orgName type="full">CCF-DiDi GAIA Collaborative Research Funds for Young Scholars</orgName>
				</funder>
				<funder ref="#_hfYRdbe">
					<orgName type="full">S&amp;T Program of Hebei</orgName>
				</funder>
				<funder ref="#_ePjfgmu">
					<orgName type="full">NSFC</orgName>
				</funder>
				<funder>
					<orgName type="full">Fundamental Research Funds for the Central Universities</orgName>
				</funder>
				<funder ref="#_DjsbrP4 #_wqYk53E #_3Sw6cf7">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_vbEM8cB">
					<orgName type="full">Natural Science Foundation of Beijing Municipality</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-03-17">17 Mar 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dongcheng</forename><surname>Zou</surname></persName>
							<email>zoudongcheng@buaa.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
							<email>penghao@buaa.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Huang</surname></persName>
							<email>huang.xiang@buaa.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Renyu</forename><surname>Yang</surname></persName>
							<email>renyu.yang@buaa.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jianxin</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jia</forename><surname>Wu</surname></persName>
							<email>jia.wu@mq.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Chunyang</forename><surname>Liu</surname></persName>
							<email>liuchunyang@didiglobal.com</email>
						</author>
						<author>
							<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
							<email>psyu@uic.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Macquarie University Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Didi Chuxing</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">University of Illinois Chicago Chicago</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SE-GSL: A General and Effective Graph Structure Learning Framework through Structural Entropy Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-03-17">17 Mar 2023</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3543507.3583453</idno>
					<idno type="arXiv">arXiv:2303.09778v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph structure learning</term>
					<term>structural entropy</term>
					<term>graph neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) are de facto solutions to structural data learning. However, it is susceptible to low-quality and unreliable structure, which has been a norm rather than an exception in real-world graphs. Existing graph structure learning (GSL) frameworks still lack robustness and interpretability. This paper proposes a general GSL framework, SE-GSL, through structural entropy and the graph hierarchy abstracted in the encoding tree. Particularly, we exploit the one-dimensional structural entropy to maximize embedded information content when auxiliary neighbourhood attributes is fused to enhance the original graph. A new scheme of constructing optimal encoding trees is proposed to minimize the uncertainty and noises in the graph whilst assuring proper community partition in hierarchical abstraction. We present a novel sample-based mechanism for restoring the graph structure via node structural entropy distribution. It increases the connectivity among nodes with larger uncertainty in lower-level communities. SE-GSL is compatible with various GNN models and enhances the robustness towards noisy and heterophily structures. Extensive experiments show significant improvements in the effectiveness and robustness of structure learning and node representation learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph Neural Networks (GNNs) <ref type="bibr" target="#b50">[49,</ref><ref type="bibr" target="#b60">59]</ref> have become the cornerstone and de facto solution of structural representation learning. Most of the state-of-the-art GNN models employ message passing <ref type="bibr" target="#b13">[12]</ref> and recursive information aggregation from local neighborhoods <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b32">31,</ref><ref type="bibr" target="#b42">41,</ref><ref type="bibr" target="#b53">52]</ref> to learn node representation. These models have been advancing a variety of tasks, including node classification <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b51">50]</ref>, node clustering <ref type="bibr" target="#b3">[2,</ref><ref type="bibr" target="#b33">32]</ref>, graph classification <ref type="bibr" target="#b31">[30,</ref><ref type="bibr" target="#b55">54]</ref>, and graph generation <ref type="bibr" target="#b56">[55]</ref>, etc.</p><p>GNNs are extremely sensitive to the quality of given graphs and thus require resilient and high-quality graph structures. However, it is increasingly difficult to meet such a requirement in real-world graphs. Their structures tend to be noisy, incomplete, adversarial, and heterophily (i.e., the edges with a higher tendency to connect nodes of different types), which can drastically weaken the representation capability of GNNs <ref type="bibr" target="#b7">[6,</ref><ref type="bibr" target="#b26">25,</ref><ref type="bibr" target="#b29">28]</ref>. Recent studies also reveal that even a minor perturbation in the graph structure can lead to inferior prediction quality <ref type="bibr">[3,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b58">57]</ref>. Additionally, GNNs are vulnerable to attacks since the raw graph topology is decoupled from node features, and attackers can easily fabricate links between entirely different nodes <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b58">57]</ref>.</p><p>To this end, Graph Structure Learning (GSL) <ref type="bibr" target="#b6">[5,</ref><ref type="bibr" target="#b17">16,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b45">44,</ref><ref type="bibr" target="#b62">61,</ref><ref type="bibr" target="#b63">62]</ref> becomes the recent driving force for learning superior taskrelevant graph topology and for enhancing the resilience and robustness of node representation. The existing works focus on jointly optimizing GNN whilst imposing regularization on refined graph  <ref type="formula" target="#formula_0">1</ref>) Vertices and edges represent the people and their interconnectivity (e.g., common locations, interests, occupations). There are different abstraction levels, and each community can be divided into sub-communities in a finer-grained manner (e.g., students are placed in different classrooms while teachers are allocated different offices). The lowest abstraction will come down to the individuals with own attributes, and the highest abstraction is the social network system. (b) An encoding tree is a natural form to represent and interpret such a multi-level hierarchy.</p><p>structures. Typical methods include metric-based <ref type="bibr" target="#b6">[5,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" target="#b45">44]</ref>, probabilistic sampling <ref type="bibr" target="#b9">[8,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b59">58]</ref>, and learnable structure approach <ref type="bibr" target="#b17">[16]</ref>, etc. While promising, GNNs and GSL still have the following issues. i) robustness to system noises and heterophily graphs. While many GSL models strive to fuse node features and topological features through edge reconstruction (e.g., add, prune, or reweight) <ref type="bibr" target="#b45">[44,</ref><ref type="bibr" target="#b58">57,</ref><ref type="bibr" target="#b63">62]</ref>, additional noises and disassortative connections will be inevitably involved in the fused structure due to the unreliable priori topology and node embeddings, which would further degrade the GNNs representation capability <ref type="bibr" target="#b23">[22]</ref>. ii) model interpretability. Fully parameterizing the adjacency matrix will incur a non-negligible cost of parameter storage and updating and is liable to low model interpretability <ref type="bibr" target="#b14">[13]</ref>. Although some studies on the improved GNN interpretability <ref type="bibr" target="#b16">[15,</ref><ref type="bibr" target="#b54">53]</ref>, few works can effectively explain the topology evolution during graph structure learning. Therefore, fusing the node and topological features in a noisy system environment to obtain GNN-friendly graphs by exploiting inherent graph structures is still an underexplored problem <ref type="bibr" target="#b49">[48]</ref>.</p><p>In this paper, we present SE-GSL, a general and effective graph structure learning framework that can adaptively optimize the topological graph structure in a learning-free manner and can achieve superior node representations, widely applicable to the mainstream GNN models. This study is among the first attempts to marry the structural entropy and encoding tree theory <ref type="bibr" target="#b19">[18]</ref> with GSL, which offers an effective measure of the information embedded in an arbitrary graph and structural diversity. The multi-level semantics of a graph can be abstracted and characterized through an encoding tree. Encoding tree <ref type="bibr" target="#b19">[18,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b57">56]</ref> represents a multi-grained division of graphs into hierarchical communities and sub-communities, thus providing a pathway to better interpretability. Fig. <ref type="figure" target="#fig_0">1</ref> showcases how such graph semantics are hierarchically abstracted. Specifically, we first enhance the original graph topology by incorporating the vertex similarities and auxiliary neighborhood information via the ?-Nearest Neighbors (?-NN) approach, so that noise can be better identified and diminished. This procedure is guided by the ?-selector that maximizes the amount of embedded information in the graph structure. We then propose a scheme to establish an optimal encoding tree to minimize the graph uncertainty and edge noises whilst maximizing the knowledge in the encoding tree. To restore the entire graph structure that can be further fed into GNN encoders, we recover edge connectivity between related vertices from the encoding tree taking into account the structural entropy distribution among vertices. The core idea is to weaken the association between vertices in high-level communities whilst establishing dense and extensive connections between vertices in low-level communities. The steps above will be iteratively conducted to co-optimize both graph structure and node embedding learning. SE-GSL<ref type="foot" target="#foot_0">1</ref> is an interpretable GSL framework that effectively exploits the substantive structure of the graph. We conduct extensive experiments and demonstrate significant and consistent improvements in the effectiveness of node representation learning and the robustness of edge perturbations.</p><p>Contribution highlights: i) SE-GSL provides a generic GSL solution to improve both the effectiveness and robustness of the mainstream GNN approaches. ii) SE-GSL offers a new perspective of navigating the complexity of attribute noise and edge noise by leveraging structural entropy as an effective measure and encoding tree as the graph hierarchical abstraction. iii) SE-GSL presents a series of optimizations on the encoding tree and graph reconstruction that can not only explicitly interpret the graph hierarchical meanings but also reduce the negative impact of unreliable fusion of node features and structure topology on the performance of GNNs. iv) We present a visualization study to reveal improved interpretability when the graph structure is evolutionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>This section formally reviews the basic concepts of Graph, Graph Neural Networks (GNNs), Graph Structure Learning (GSL), and Structural Entropy. Important notations are given in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph and Graph Structure Learning</head><p>Graph and Community. Let ? = {? , ?, ? } denote a graph, where ? is the set of ? vertices<ref type="foot" target="#foot_1">2</ref> , ? ? ? ? ? is the edge set, and ? ? R ??? refers to the vertex attribute set. A ? R ??? denotes the adjacency matrix of ?, where A ? ? is referred to as the weight of the edge between vertex ? and vertex ? in ?. Particularly, if ? is unweighted, A ? {0, 1} ??? and A ? ? only indicate the existence of the edges. In our work, we only consider the undirected graph, where A ? ? = A ?? . For any vertex ? ? , the degree of ? ? is defined as ? (? ? ) = ? A ? ? , and ? = diag(? (? 1 ), ? (? 2 ), . . . , ? (? ? )) refers to the degree matrix.</p><p>Suppose that P = {? 1 , ? 2 , . . . , ? ? } is a partition of ? . Each ? ? is called a community (aka. module or cluster), representing a group of vertices with commonality. Due to the grouping nature of a realworld network, each community of the graph can be hierarchically split into multi-level sub-communities. Such hierarchical community partition (i.e., hierarchical semantic) of a graph can be intrinsically abstracted as the encoding tree <ref type="bibr" target="#b19">[18,</ref><ref type="bibr" target="#b21">20]</ref>, and each tree node represents a specific community. Take Fig. <ref type="figure" target="#fig_0">1</ref> as an example: at a high abstraction (semantic) level, the entire graph can be categorized as two coarse-grained communities, i.e., teachers (T) and students (S). Students can be identified as sub-communities like S.1 and S.2, as per the class placement scheme. Graph Structure Learning (GSL). For any given graph ?, the goal of GSL <ref type="bibr" target="#b62">[61]</ref> is to simultaneously learn an optimal graph structure ? * optimized for a specific downstream task and the corresponding graph representation ? . In general, the objective of GSL can be summarized as L ??? = L ???? (?, ? ) + ? L ??? (?, ? * , ?), where L ???? refers to a task-specific objective with respect to the learned representation ? and the ground truth ? . L ??? imposes constraints on the learned graph structure and representations, and ? is a hyper-parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Structural Entropy</head><p>Different from information entropy (aka. Shannon entropy) that measures the uncertainty of probability distribution in information theory <ref type="bibr" target="#b38">[37]</ref>, structural entropy <ref type="bibr" target="#b19">[18]</ref> measures the structural system diversity, e.g., the uncertainty embedded in a graph. Encoding Tree. Formally, the encoding tree T of graph ? = (? , ?) holds the following properties: (1) The root node ? in T has a label ? ? = ? , ? represents the set of all vertices in ?. (2) Each non-root node ? has a label ? ? ? ? . Furthermore, if ? is a leaf node, ? ? is a singleton with one vertex in ? . (3) For each non-root node ?, its parent node in ? is denoted as ? -. (4) For each non-leaf node ?, its ?-th children node is denoted as ? ?? ? ordered from left to right as ? increases. (5) For each non-leaf node ?, assuming the number of children ? is ? , all vertex subset ? ? ??? form a partition of ? ? , written as ? ? = ? ?=1 ? ? ??? and ? ?=1 ? ? ??? = ?. If the encoding tree's height is restricted to ?, we call it ?-level encoding tree. Entropy measures can be conducted on different encoding trees. One-dimensional Structural Entropy. In a single-level encoding tree T , its structural entropy degenerates to the unstructured Shannon entropy, which is formulated as:</p><formula xml:id="formula_0">? 1 (?) = - ?? ? ?? ? ? ??? (?) log 2 ? ? ??? (?) ,<label>(1)</label></formula><p>where ? ? is the degree of vertex ?, and ??? (?) is the sum of the degrees of all vertices in ?. According to the fundamental research <ref type="bibr" target="#b19">[18]</ref>, one-dimensional structural entropy ? 1 (?) measures the uncertainty of vertex set ? in ?, which is also the upper bound on the amount of information embedded in ?.</p><p>High-dimensional Structural Entropy. For the encoding tree T , we define high-dimensional structural entropy of ? as:</p><formula xml:id="formula_1">? ? (?) = min ?T:?????? ( T) ?? {? T (?)},<label>(2)</label></formula><formula xml:id="formula_2">? T (?) = ?? ? ? T,??? ? T (?; ?) = - ?? ? ? T,??? ? ? ??? (?) log 2 V ? V ? - ,<label>(3)</label></formula><p>where ? ? is the sum weights of the cut edge set [? ? ,? ? /? ? ], i.e., all edges connecting vertices inside ? ? with vertices outside ? ? . V ? is the sum of degrees of all vertices in ? ? . ? T (?; ?) is the structural entropy of node ? and ? T (?) is the structural entropy of T . ? ? (?) is the ?-dimensional structural entropy, with the optimal encoding tree of ?-level .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR APPROACH</head><p>This section presents the architecture of SE-GSL, then elaborate on how we enhance the graph structure learning by structural entropy-based optimization of the hierarchical encoding tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview of SE-GSL</head><p>Fig. <ref type="figure">2</ref> depicts the overall pipeline. At the core of SE-GSL is the structure optimization procedure that transforms and enhances the graph structure. More specifically, it encompasses multi-stages: graph structure enhancement, hierarchical encoding tree generation, and sampling-based structure reconstruction before an iterative representation optimization.</p><p>First, the original topological information is integrated with vertex attributes and the neighborhood in close proximity. Specifically, we devise a similarity-based edge reweighting mechanism and incorporate ?-NN graph structuralization to provide auxiliary edge information. The most suitable ? is selected under the guidance of the one-dimensional structural entropy maximization strategy ( ? 3.2). Upon the enhanced graph, we present a hierarchical abstraction mechanism to further suppress the edge noise and reveal the high-level hierarchical community structure (encoding tree) ( ? 3.3). A novel sampling-based approach is designed to build new graph topology from the encoding tree, particularly by restoring the edge connectivity from the tree hierarchy ( ? 3.4). The core idea is to weaken the association between high-level communities whilst establishing dense and extensive connections within low-level communities. To this end, we transform the node structural entropy into probability, rejecting the deterministic threshold. Through multi-iterative stochastic sampling, it is more likely to find favorable graph structures for GNNs. Afterward, the rebuilt graph will be fed into the downstream generic GNN encoders. To constantly improve both the node representation and the graph structure, the optimization pipeline is iterated for multiple epochs.</p><p>The training procedure of SE-GSL is summarized in Appendix A.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph Structure Enhancement</head><p>To fully incorporate vertex attributes and neighborhood information in the graph structure, we perform feature fusion and edge reweighting so that the topological structure, together with the informative vertex adjacent similarity, can be passed on to the encoding tree generator. To begin with, we calculate the pair-wise similarity matrix ? ? R |? |?|? | among vertices in graph ?. To better depict the linear correlation between two vertex attributes, we take the Pearson correlation coefficient (PCC) as the similarity measure in the experiments, i.e.,</p><formula xml:id="formula_3">? ? ? = PCC(? ? , ? ? ) = ? ((? ? -? ? )(? ? -? ? )) ? ? ? ? ,<label>(4)</label></formula><p>where ? ? and ? ? ? R 1?? are the attribute vectors of vertices ? and ?, respectively. ? ? and ? ? denote the mean value and variance of ? ? , and ? (?) is the dot product function. Based on ?, we can intrinsically construct the ?-NN graph ? ??? = {? , ? ??? } where each edge in ? ??? represents a vertex and its ? nearest neighbors (e.g., the edges in red in <ref type="bibr">Fig 2)</ref>. We fuse ? ??? and the original ? to Encoding tree </p><formula xml:id="formula_4">? ? = {? , ? ? = ? ? ? ??? }.</formula><formula xml:id="formula_5">?? ? ?? ? Optimized graph Optimized graph ?? ? ?? ? Vertex attributes ?? ? ?? ?</formula><formula xml:id="formula_6">G f G Node pairs ( , )<label>( , ) ( , )</label></formula><formula xml:id="formula_7">? ? ? ( , )<label>( , ) ( , ) ( , )</label></formula><formula xml:id="formula_8">? ? ? ( , ) ' G ?? ? ?? ? Optimized attributes ?? ? ?? ? Optimized attributes ij w S ( ) k knn G 1 ( ) ( ) k f G H Figure 2:</formula><p>The overall architecture of SE-GSL.</p><p>A key follow-up step is pinpointing the most suitable number ? of nearest neighbors. An excessive value of ? would make ? ? overnoisy and computationally inefficient, while a small ? would result in insufficient information and difficulties in hierarchy extraction. As outlined in ? 2.2, a larger one-dimensional structural entropy indicates more information that ? ? can potentially hold. Hence, we aim to maximize the one-dimensional structural entropy ? 1 (? ? ) to guide the selection of ? for larger encoding information. In practice, we gradually increase the integer parameter ?, generate the corresponding ? (?) ? and compute ? 1 (? (?) ? ). Observably, when ? reaches a threshold ? ? , ? 1 (? (?) ? ) comes into a plateau without noticeable increase. This motivates us to regard this critical point ? ? as the target parameter. The ?-selector algorithm is depicted in Appendix A.5. In addition, the edge ? ? ? between ? ? and ? ? is reweighted as:</p><formula xml:id="formula_9">? ? ? = ? ? ? + ?, ? = 1 2|? | ? 1 |?| ?? 1&lt;?,? &lt;? ? ? ? , (<label>5</label></formula><formula xml:id="formula_10">)</formula><p>where ? is a modification factor that amplifies the trivial edge weights and thus makes the ?-selector more sensitive to noises.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hierarchical Encoding Tree Generation</head><p>Our methodology of abstracting the fused graph into a hierarchy is inspired by the structural entropy theory <ref type="bibr" target="#b19">[18,</ref><ref type="bibr" target="#b21">20]</ref>. We intend to minimize the graph uncertainty (i.e., edge noises) and maximize the knowledge embedded (e.g., optimal partition) in the highdimensional hierarchy. Correspondingly, the objective of structural entropy minimization is to find out an encoding tree T that minimizes ? T (? ? ) defined in Eq. 3. Due to the difficulty in graph semantic complexity quantification, we restrict the optimization objective to the ?-level tree with a hyperparameter ?. The optimal ?-dimensional encoding tree is represented as:</p><formula xml:id="formula_11">T * = arg min ?T:?????? ( T) ?? (? T (?)).<label>(6)</label></formula><p>To address this optimization problem, we design a greedy-based heuristic algorithm to approximate ? ? (?). To assist the greedy heuristic, we define two basic operators: Definition 1. Combining operator: Given an encoding tree T for ? = (? , ?), let ? and ? be two nodes in T sharing the same parent ?. The combining operator CB T (?, ?) updates the encoding tree as: ? ? ? -; ? ? ? -; ? ? ? -. A new node ? is inserted between ? and its children ?, ?. Definition 2. Lifting operator: Given an encoding tree T for ? = (? , ?), let ?, ? and ? be the nodes in T , satisfying ? -= ? and ? -= ?. The lifting operator LF T (?, ?) updates the encoding tree as: ? ? ? -; IF :? ? = ?, THEN :drop(?). The subtree rooted at ? is lifted by placing itself as ?'s child. If no more children exist after lifting, ? will be deleted from T .</p><p>In light of the high-dimensional structural entropy minimization principle <ref type="bibr" target="#b21">[20]</ref>, we first build a full-height binary encoding tree by greedily performing the combining operations. Two children of the root are combined to form a new partition iteratively until the structural entropy is no longer reduced. To satisfy the height restriction, we further squeeze the encoding tree by lifting subtrees to higher levels. To do so, we select and conduct lifting operations between a non-root node and its parent node that can reduce the structural entropy to the maximum. This will be repeated until the encoding tree height is less than ? and the structural entropy can no longer be decreased. Eventually, we obtain an encoding tree with a specific height ? with minimal structural entropy. The pseudo-code is detailed in Appendix A.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Sample-based Graph Reconstruction</head><p>The subsequent step is to restore the topological structure from the hierarchy whilst guaranteeing the established hierarchical semantics in optimal encoding tree T * . The key to graph reconstruction is determining which edges to augment or weaken. Intuitively, the nodes in real-life graphs in different communities tend to have different labels. The work <ref type="bibr" target="#b64">[63]</ref> has demonstrated the effectiveness of strengthening intra-cluster edges and reducing inter-cluster edges in a cluster-awareness approach to refine the graph topology. However, for hierarchical communities, simply removing crosscommunity edges will undermine the integrity of the higher-level community. Adding edges within communities could also incur additional edge noises to lower-level partitioning.</p><p>We optimize the graph structure with community preservation by investigating the structural entropy of deduction between two interrelated nodes as the criterion of edge reconstruction: Definition 3. Structural entropy of deduction: Let T be an encoding tree of ?. We define the structural entropy of the deduction from non-leaf node ? to its descendant ? as:</p><formula xml:id="formula_12">? T (?; (?, ?]) = ?? ?,? ? ?? ? ?? ? ? T (?; ?).<label>(7)</label></formula><p>This node structure entropy definition exploits the hierarchical structure of the encoding tree and offers a generic measure of topdown deduction to determine a community or vertex in the graph.</p><p>From the viewpoint of message passing, vertices with higher structural entropy of deduction produce more diversity and uncertainty and thus require more supervisory information. Therefore, such vertices need expanded connection fields during the graph reconstruction to aggregate more information via extensive edges. To achieve this, we propose an analog sampling-based graph reconstruction method. The idea is to explore the node pairs at the leaf node level (the lowest semantic level) and stochastically generate an edge for a given pair of nodes with a certain probability associated with the deduction structural entropy.</p><p>Specifically, for a given T , assume the node ? has a set of child nodes {? ?1? , ? ?2? , . . . , ? ??? }. The probability of the child ? ?? ? is defined as: ? (? ?? ? ) = ? ? (? T (? ? ; (?, ? ?? ? ])), where ? is the root of T and ? ? (?) represents a distribution function. Take softmax function as an example, the probability of ? ?? ? can be calculated as:</p><formula xml:id="formula_13">? (? ?? ? ) = exp(? T (? ? ; (?, ? ?? ? ])) ? ?=1 exp(? T (? ? ; (?, ? ?? ? ])) . (<label>8</label></formula><formula xml:id="formula_14">)</formula><p>The probability of a non-root node can be acquired recursively. To generate new edges, we sample leaf node pairs by a top-down approach with a single sampling flow as follows:</p><p>(1) For the encoding tree (or subtree) with root node ?, two different child nodes ? ?? ? and ? ?? ? are selected by sampling according to ? (? ?? ? ) and ? (? ?? ? ). Let ? 1 ? ? ?? ? and ? 2 ? ? ?? ? (2) If ? 1 is a non-leaf node, we perform sampling once on the subtree rooted at ? 1 to get</p><formula xml:id="formula_15">? ?? ? 1 , then update ? 1 ? ? ?? ? 1 .</formula><p>The same is operated on ? 2 . (3) After recursively performing step (2), we sample two leaf nodes ? 1 and ? 2 , while adding the edge connecting vertex ? 1 = ? ? 1 and ?2 = ? ? 2 into the edge set ? ? of graph ? ? . To establish extensive connections at all levels, multiple samplings are performed on all encoding subtrees. For each subtree rooted at ?, we conduct independent samplings for ? ? ? times, where ? is the number of ?'s children, and ? is a hyperparameter that positively correlated with the density of reconstructed graph. For simplicity, we adopt a uniform ? for all subtrees. Separately setting and tuning ? of each semantic level for precise control is also feasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Time Complexity of SE-GSL</head><p>The overall time complexity is ? (? 2 + ? + ? log 2 ?), in which ? is the number of nodes. Separately, in ? 3.2, the time complexity of calculating similarity matrix is ? (? 2 ) and of ?-selector is ? (?).</p><p>According to <ref type="bibr" target="#b19">[18]</ref>, the optimization of a high-dimensional encoding tree in ? 3.3 costs the time complexity of ? (? log 2 ?). As for the sampling process in ? 3.4, the time complexity can be proved as ? (2?). We report the time cost of SE-GSL in Appendix A.3.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS AND ANALYSIS</head><p>In this section, we demonstrate the efficacy of SE-GSL on semisupervised node classification ( ? 5.1, followed by micro-benchmarks that investigate the detailed effect of the submodules on the overall performance and validate the robustness of SE-GSL when tackling random perturbations ( ? 5.2). For better interpretation, we visualize the change of structural entropy and graph topology ( ? 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Node Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Comparison with baselines.</head><p>We compare the node classification performance of SE-GSL with ten baseline methods on nine benchmark datasets. Table <ref type="table" target="#tab_0">1</ref> shows the average accuracy and the standard deviation. Note that the results of H 2 GCN (except PT and TW) and Geom-GCN are from the reported value in original papers ( -for not reported), while the rest are obtained based on the execution of the source code provided by their authors under our experimental settings. Our observations are three-fold:</p><p>(1) SE-GSL achieves optimal results on 5 datasets, runner-up results on 8 datasets, and advanced results on all datasets. The accuracy can be improved up to 3.41% on Pubmed, 3.00% on Cora, and 2.92% on Citeseer compared to the baselines. This indicates that our design can effectively capture the inherent and deep structure of the graph and hence the classification improvement.</p><p>(2) SE-GSL shows significant improvement on the datasets with heteropily graphs, e.g., up to 37.97% and 27.13% improvement against Wisconsin and Texas datasets, respectively. This demonstrates the importance of the graph structure enhancement that can contribute to a more informative and robust node representation.</p><p>(3) While all GNN methods can achieve satisfactory results on citation networks, the graph learning/high-order neighborhood awareness frameworks substantially outperform others on the We-bKB datasets and the actor co-occurrence networks, which is highly disassortative. This is because these methods optimize local neighborhoods for better information aggregation. Our method is one of the top performers among them due to the explicit exploitation of the global structure information in the graph hierarchical semantics. node representation and graph structure. We also notice that despite the lower improvement, SE-GSL variants based on GraphSAGE and APPNP perform relatively better compared to those on GCN and GAT. This is most likely due to the backbone model itself being more adapted to handle disassortative settings on graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Comparison base on different backbones.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Micro-benchmarking</head><p>5.2.1 Effectiveness of ?-selector. This subsection evaluate how the one-dimensional structural entropy guides the ?-selector in ? 3.2. Table <ref type="table" target="#tab_3">3</ref> showcases the selected parameter ? in each iteration with SE-GSL ??? . Noticeably, as the iterative optimization proceeds, the optimal parameter ? converges to a certain range, indicating the gradual stabilization of the graph structure and node representation. The disparity of parameter ? among different datasets also demonstrates the necessity of customizing ? in different cases rather than using ? as a static hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.2.2</head><p>Impact of the encoding tree's height ?. We evaluate all four variants of SE-GSL on the website network datasets, and the encoding tree height ? involved in ? 3.3 varies from 2 to 4. As shown in Fig. <ref type="figure" target="#fig_3">3</ref>, there is a huge variation in the optimal tree heights among different datasets. For example, in the variants based on GAT, GCN, and APPNP, the best results can be targeted at ? = 3 in Texas and at ? = 4 in Cornell and Wisconsin. By contrast, in SE-GSL ???? , ? = 2 can enable the best accuracy of 86.27%. This weak correlation between the best ? and the model performance is worth investigating further, which will be left as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Sensitivity to perturbations.</head><p>We introduce random edge noises into Cora and Citeseer, with perturbation rates of 0.2, 0.4, 0.6, 0.8, and 1. As shown in Fig. <ref type="figure" target="#fig_4">4</ref>(a), SE-GSL outperforms baselines in both GCN and GAT cases under most noise settings. For instance, SE-GSL ??? achieves up to 8.09% improvement against the native GCN when the perturbation rate is 0.8; by contrast, improvements by GCN-Jaccard and GCN-DropEdge are merely 6.99% and 5.77%, respectively. A similar phenomenon is observed for most cases in the Citeseer dataset (Fig. <ref type="figure" target="#fig_4">4</ref>(b)), despite an exception when compared against GCN-Jaccard. Nevertheless, our approach is still competitive and even better than GCN-Jaccard at a high perturbation rate. For comparison, we normalize the structural entropy by</p><formula xml:id="formula_16">? T (?)</formula><p>? 1 (?) . As shown in Fig. <ref type="figure" target="#fig_5">5</ref>(a)-(c), as the accuracy goes up, the normalized structural entropy constantly decreases during the iterative graph reconstruction, reaching the minimums of 0.7408 in Texas, 0.7245 in Cornell, and 0.7344 in Wisconsin. This means the increasing determinism of the overall graph structure and the reduced amount of information required to determine a vertex. Interestingly, if our graph reconstruction mechanism is disabled (as shown in Fig. <ref type="figure" target="#fig_5">5(d)</ref>), the normalized structural entropy keeps rising from 0.7878, compared with Fig. <ref type="figure" target="#fig_5">5(c</ref>). Accordingly, the final accuracy will even converge to 55.34%, a much lower level.</p><p>Such a comparison also provides a feasible explanation for the rising trend of the normalized structural entropy within every single iteration. This stems from the smoothing effect during the GNN training. As the node representation tends to be homogenized, the graph structure will be gradually smoothed, leading to a decrease in the one-dimensional structural entropy thus the normalized structural entropy increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Visualization. Fig. 6 visualizes the topology of the original</head><p>Cora and Citeseer graph and of the 2nd and 5th iterations. The vertex color indicates the class it belongs to, and the layout denotes connecting relations. Edges are hidden for clarity. As the iteration continues, much clearer clustering manifests -few outliers and more concentrated clusters. Vertices with the same label are more tightly connected due to the iterative graph reconstruction scheme. This improvement hugely facilitates the interpretability of the GSL and the node representation models. pan2021information</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Graph structure learning and neighborhood optimization. The performance of GNNs is heavily dependent on task-relevant links and neighborhoods. Graph structure learning explicitly learns   and adjusts the graph topology, and our SE-GSL is one of them. GDC <ref type="bibr" target="#b11">[10]</ref> reconnects graphs through graph diffusion to aggregate from multi-hop neighborhoods. Dropedge <ref type="bibr" target="#b35">[34]</ref>, Neuralsparse <ref type="bibr" target="#b59">[58]</ref> contribute to graph denoising via edge-dropping strategy while failing to renew overall structures. LDS-GNN <ref type="bibr" target="#b9">[8]</ref> models edges by sampling graphs from the Bernoulli distribution. Meanwhile, we consider linking the structural entropy, which is more expressive of graph topology, to the sampling probability. GEN <ref type="bibr" target="#b44">[43]</ref>, IDGL <ref type="bibr" target="#b6">[5]</ref> explore the structure from the node attribute space by the ?-NN method. Differently, instead of directly using attribute similarity, we regenerate edges from the hierarchical abstraction of graphs to avoid inappropriate metrics. Besides adjusting the graph structure, methods to optimize aggregation are proposed with results on heterophily graphs. MixHop <ref type="bibr" target="#b2">[1]</ref> learns the aggregation parameters for neighborhoods of different hops through a mixing network, while H 2 GCN <ref type="bibr" target="#b61">[60]</ref> identifies higher-order neighbor-embedding separation and intermediate representation combination, for adapting to heterophily graphs. Geom-GCN <ref type="bibr" target="#b29">[28]</ref> aggregates messages over both the original graph and latent space by a designed geometric scheme. Structural entropy with neural networks. Structural information principles <ref type="bibr" target="#b19">[18]</ref>, defining encoding trees and structural entropy, were first used in bioinformatic structure analysis <ref type="bibr" target="#b20">[19,</ref><ref type="bibr" target="#b21">20]</ref>. Existing work mainly focuses on network analysis, node clustering and community detection <ref type="bibr" target="#b22">[21,</ref><ref type="bibr" target="#b25">24,</ref><ref type="bibr" target="#b28">27]</ref>. As an advanced theory on graphs and hierarchical structure, structural information theory has great potential in combination with neural networks. SR-MARL <ref type="bibr" target="#b57">[56]</ref> applies structural information principles to hierarchical role discovery in multi-agent reinforcement learning, thereby boosting agent network optimization. SEP <ref type="bibr" target="#b48">[47]</ref> provides a graph pooling scheme based on optimal encoding trees to address local structure damage and suboptimal problem. It essentially uses structural entropy minimization for a multiple-layer coarsened graph. MinGE <ref type="bibr" target="#b27">[26]</ref> and MEDE <ref type="bibr" target="#b53">[52]</ref> estimate the node embedding dimension of GNNs via structural entropy, which introduces both attribute entropy and structure entropy as objective. Although these works exploit structural entropy to mine the latent settings of neural networks and GNNs, how to incorporate this theory in the optimization process is still understudied, and we are among the first attempts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>To cope with edge perturbations in graphs with heterophily, this paper proposes a novel graph structure learning framework SE-GSL that considers the structural entropy theory in graph structure optimization. We design a structure enhancement module guided by the one-dimensional structural entropy maximization strategy to extend the information embedded in the topology. To capture the hierarchical semantics of graphs, high-dimensional structural entropy minimization is performed for optimal encoding trees. We propose a node sampling technique on the encoding tree to restore the most appropriate edge connections at different community levels, taking into account the deduction structural entropy distribution. In the future, we plan to combine delicate loss functions with structural entropy so that the knowledge in encoding trees can be converted into gradient information, which will further allow for end-to-end structure optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 Glossary of notations</head><p>In Table <ref type="table" target="#tab_4">4</ref>, we summarize the notations used in our work. The weight of edge ? ? ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>??? (?)</head><p>The volume of graph ?, i.e., degree sum in ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? (?) ???</head><p>The ?-NN graph with parameter ?.</p><p>? ? Fusion graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? (?) ?</head><p>The fusion graph with parameter ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T</head><p>Encoding tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T *</head><p>The optimal encoding tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>The root node of an encoding tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>A non-root node of an encoding tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? -</head><p>The parent node of ?. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Dataset and Time Costs of SE-GSL</head><p>Our framework SE-GSL is evaluated on nine graph datasets. the statistics of these datasets are shown in Table <ref type="table" target="#tab_6">5</ref>. The time costs of SE-GSL on all datasets are shown in Table <ref type="table" target="#tab_7">6</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Baselines</head><p>Baselines are briefly described as follows 3 :</p><p>? GCN <ref type="bibr" target="#b46">[45]</ref> is the most popular GNN, which defines the first-order approximation of a localized spectral filter on graphs. ? GAT <ref type="bibr" target="#b42">[41]</ref> introduces a self-attention mechanism to important scores for different neighbor nodes. ? GraphSAGE <ref type="bibr" target="#b15">[14]</ref> is an inductive framework that leverages node features to generate embeddings by sampling and aggregating features from the local neighborhood. ? APPNP <ref type="bibr" target="#b10">[9]</ref> combines GCN with personalized PageRank. ? GCNII 4 <ref type="bibr">[4]</ref> employs residual connection and identity mapping.</p><p>? Grand 5 <ref type="bibr" target="#b8">[7]</ref> purposes a random propagation strategy for data augmentation, and uses consistency regularization to optimize. ? Mixhop 6 <ref type="bibr" target="#b2">[1]</ref> aggregates mixing neighborhood information.</p><p>? Geom-GCN 7 <ref type="bibr" target="#b29">[28]</ref> exploits geometric relationships to capture long-range dependencies within structural neighborhoods. Three variant of Geom-GCN is used for comparison. ? GDC 8 <ref type="bibr" target="#b11">[10]</ref> refines graph structure based on diffusion kernels. ? GEN 9 [43] estimates underlying meaningful graph structures.</p><p>? H 2 GCN 10 [60] combine multi-hop neighbor-embeddings for adapting to both heterophily and homophily graph settings. ? DropEdge 11 <ref type="bibr" target="#b35">[34]</ref> randomly removes edges from the input graph for over-fitting prevention. ? Jaccard 12 <ref type="bibr" target="#b47">[46]</ref> prunes the edges connecting nodes with small Jaccard similarity. A.6 Algorithm of high-dimensional structural entropy minimization</p><p>The pseudo-code of the high-dimensional structural entropy minimization algorithm is shown in Algorithm 3.  LF(?, ?) according to Definition 2;</p><p>14 Return T * ? T ;</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustrative example of the hierarchical community (semantics) in a simple social network. (1) Vertices and edges represent the people and their interconnectivity (e.g., common locations, interests, occupations). There are different abstraction levels, and each community can be divided into sub-communities in a finer-grained manner (e.g., students are placed in different classrooms while teachers are allocated different offices). The lowest abstraction will come down to the individuals with own attributes, and the highest abstraction is the social network system. (b) An encoding tree is a natural form to represent and interpret such a multi-level hierarchy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results of SE-GSL with different encoding tree heights.</figDesc><graphic url="image-1.png" coords="6,57.49,317.48,123.11,87.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Robustness of SE-GSL against random noises.</figDesc><graphic url="image-5.png" coords="7,321.57,103.44,119.21,85.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The normalized structural entropy changes during the training of SE-GSL ??? with 2-dimensional structural entropy on (a) Texas, (b) Cornell, and (c) Wisconsin. The structure is iterated every 200 epochs. By comparison, (d) shows the entropy changes on Wisconsin without the graph reconstruction strategy.</figDesc><graphic url="image-7.png" coords="8,56.32,83.69,499.37,109.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The visualized evolution of the graph structure on Cora (a,b,c) and Citeseer (d,e,f). The corresponding Structural Entropy (SE) is also shown.</figDesc><graphic url="image-12.png" coords="8,145.72,342.97,76.84,72.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Algorithm 3 : 8 Select</head><label>38</label><figDesc>K-dimensional structural entropy minimization Input: a graph ? = (? , ?), the height of encoding tree ? &gt; 1 Output: Optimal high-dimensional encoding tree T * 1 //Initialize an encoding tree T with height 1 and root ? 2 Create root node ?; 3 for ? ? ? ? do 4 Create node ? ? . Let ? ? ? = ? ? ; 5 ? - ? = ?; 6 //Generation of binary encoding tree 7 while ? has more than 2 children do ? ? and ? ? in T , condition on ? - ? = ? - ? = ? and arg max ? ? ,? ? (? T (?) -? T CB(? ? ,? ? ) (?)); 9 CB(? ? , ? ? ) according to Definition 1; 10 //Squeezing of encoding tree 11 while height(T ) &gt; ? do 12 Select non-root node ? and ? in T , condition on ? -= ? and arg max ?,? (? T (?) -? T LF(?,?) (?));</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>13</head><label>13</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Classification Accuracy (%) comparison, with improvement range of SE-GSL against the baselines. The best results are bolded and the second-best are underlined. Green denotes the outperformance percentage, while yellow denotes underperformance. ?0.63 76.22 ?0.71 87.46 ?0.12 67.62 ?0.21 62.46 ?1.94 27.65 ?0.55 49.19 ?1.80 57.30 ?2.86 48.57 ?4.08 GAT 87.52 ?0.69 76.04 ?0.78 86.61 ?0.15 68.76 ?1.01 61.68 ?1.20 27.77 ?0.59 57.09 ?6.32 58.10 ?4.14 51.34 ?4.78 GCNII 87.57 ?0.87 75.47 ?1.01 88.64 ?0.23 68.93 ?0.93 65.17 ?0.47 30.66 ?0.66 58.76 ?7.11 55.36 ?6.45 51.96 ?4.36 Grand 87.93 ?0.71 77.59 ?0.85 86.14 ?0.98 69.80 ?0.75 66.79 ?0.22 29.80 ?0.60 57.21 ?2.48 56.56 ?1.53 52.94 ?3.36 Mixhop 85.71 ?0.85 75.94 ?1.00 87.31 ?0.44 69.48 ?0.30 66.34 ?0.22 33.72 ?0.76 64.47 ?4.78 63.16 ?6.28 72.12 ?3.34 Dropedge 86.32 ?1.09 76.12 ?1.32 87.58 ?0.34 68.49 ?0.91 65.24 ?1.45 30.10 ?0.71 58.94 ?5.95 59.20 ?5.43 60.45 ?4.48 ?0.36 76.13 ?0.53 88.08 ?0.16 66.14 ?0.54 64.14 ?1.40 28.74 ?0.76 59.46 ?4.35 56.42 ?3.99 48.30 ?4.29 GEN 87.84 ?0.69 78.77 ?0.88 86.13 ?0.41 71.62 ?0.78 65.16 ?0.77 36.69 ?1.02 65.57 ?6.74 73.38 ?6.65 54.90 ?4.73 H 2 GCN-2 87.81 ?1.35 76.88 ?1.77 89.59 ?0.33 68.15 ?0.30 63.33 ?0.77 35.62 ?1.30 82.16 ?6.00 82.16 ?5.28 85.88 ?4.22 SE-GSL 87.93 ?1.24 77.63 ?1.65 88.16 ?0.76 71.91 ?0.66 66.99 ?0.93 36.34 ?2.07 75.21 ?5.54 82.49 ?4.80 86.27 ?4.32</figDesc><table><row><cell>Method</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell>PT</cell><cell>TW</cell><cell>Actor</cell><cell>Cornell</cell><cell>Texas</cell><cell>Wisconsin</cell></row><row><cell cols="2">GCN 87.26 Geom-GCN-P 84.93</cell><cell>75.14</cell><cell>88.09</cell><cell>-</cell><cell>-</cell><cell>31.63</cell><cell>60.81</cell><cell>67.57</cell><cell>64.12</cell></row><row><cell>Geom-GCN-S</cell><cell>85.27</cell><cell>74.71</cell><cell>84.75</cell><cell>-</cell><cell>-</cell><cell>30.30</cell><cell>55.68</cell><cell>59.73</cell><cell>56.67</cell></row><row><cell>GDC</cell><cell>87.17</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>Improvement 0.00?3.00 -1.14?2.92 -1.43?3.41 0.29?5.77 0.20?5.31 -0.35?8.69 -6.95?26.02 0.33?27.13 0.39?37.97</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table /><note><p>shows the mean classification accuracy of SE-GSL with different backbone encoders. Observably, SE-GSL upon GCN and GAT overwhelmingly outperforms its backbone model, with an accuracy improvement of up to 31.04% and 27.48%, respectively. This indicates the iterative mechanism in the SE-GSL pipeline can alternately optimize the</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracy(%) of SE-GSL and corresponding backbones. Wisc. is short for Wisconsin. Method Actor TW Texas Wisc. Improvement SE-GSL ??? 35.03 66.88 75.68 79.61 ? 5.20?31.04 SE-GSL ???? 36.20 66.92 82.49 86.27 ? 0.25?6.79 SE-GSL ??? 32.46 63.57 74.59 78.82 ? 4.69?27.48 SE-GSL ??? ? ? 36.34 66.99 81.28 83.14 ? 2.01?12.16</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The ? selection for each iteration in structural optimization. Bolds represent the ? selection when the accuracy reaches maximum.</figDesc><table><row><cell>Iteration</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell></row><row><cell>Cora</cell><cell cols="9">22 22 19 22 21 22 20 21 20</cell></row><row><cell>Actor</cell><cell cols="9">23 15 15 15 14 15 14 14 15</cell></row><row><cell>TW</cell><cell cols="9">50 16 16 17 15 17 27 16 16</cell></row><row><cell cols="10">Wisconsin 21 16 11 16 14 13 16 13 11</cell></row><row><cell>Texas</cell><cell cols="9">21 13 13 13 13 10 14 10 14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Glossary of Notations. The degree matrix; The degree of vertex ? ? .? ? ?The edge between ? ? and ? ? ? ? ?</figDesc><table><row><cell>Notation</cell><cell>Description</cell></row><row><cell>?; ?; ?</cell><cell>Graph; Adjacency matrix; Similarity matrix.</cell></row><row><cell>?; ?; ?</cell><cell>Vertex; Edge; Vertex attribute.</cell></row><row><cell>? ; ?; ?</cell><cell>Vertex set; Edge set; Attribute set.</cell></row><row><cell>|? |; |?|</cell><cell>The number of vertices and edges.</cell></row><row><cell>P; ? ?</cell><cell>The partition of ? ; A community.</cell></row><row><cell>?; ? (? ? )</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>? ?? ? the ?-th child of ?. ? ? The label of ?, i.e., node set ? . ? ? The label of ?, i.e., a subset of ? . V ? Volume of graph ?. ? ? the sum weights of cut edge set [? ? ,? ? /? ? ]. Structural entropy of node ? in T . ? T (?; (?, ?]) Structural entropy of a deduction from ? to ?.</figDesc><table><row><cell>? (T )</cell><cell>The number of non-root node in T .</cell></row><row><cell>? T (?)</cell><cell>Structural entropy of ? under T .</cell></row><row><cell>? ? (?)</cell><cell>?-dimensional structural entropy.</cell></row><row><cell>? 1 (?)</cell><cell>One-dimensional structural entropy.</cell></row><row><cell>? T (?; ?)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Statistics of benchmark datasets. Citation networks<ref type="bibr" target="#b46">[45,</ref><ref type="bibr" target="#b52">51]</ref>. Cora, Citeseer, and Pubmed are benchmark datasets of citation networks. Nodes represent paper, and edges represent citation relationships in these networks. The features are bag-of-words representations of papers, and labels denote their academic fields.? Social networks<ref type="bibr" target="#b36">[35]</ref>. TW and PT are two subsets of Twitch Gamers dataset<ref type="bibr" target="#b37">[36]</ref>, designed for binary node classification tasks, where nodes correspond to users and links to mutual friendships. The features are liked games, location, and streaming habits of the users. The labels denote whether a streamer uses explicit language (Taiwanese and Portuguese). ? WebKB networks<ref type="bibr" target="#b12">[11]</ref>. Cornell, Texas, and Wisconsin are three subsets of WebKB, where nodes are web pages, and edges are hyperlinks. The features are the bag-of-words representation of pages. The labels denote categories of pages, including student, project, course, staff, and faculty. ? Actor co-occurrence network<ref type="bibr" target="#b41">[40]</ref>. This dataset is a subgraph of the film-director-actor-writer network, in which nodes represent actors, edges represent co-occurrence relation, node features are keywords of the actor, and labels are the types of actors.</figDesc><table><row><cell>Dataset</cell><cell cols="5">Nodes Edges Classes Features homophily</cell></row><row><cell>Cora</cell><cell>2708</cell><cell>5429</cell><cell>7</cell><cell>1433</cell><cell>0.83</cell></row><row><cell>Citeseer</cell><cell>3327</cell><cell>4732</cell><cell>6</cell><cell>3703</cell><cell>0.71</cell></row><row><cell>Pubmed</cell><cell cols="2">19717 44338</cell><cell>3</cell><cell>500</cell><cell>0.79</cell></row><row><cell>PT</cell><cell cols="2">1912 31299</cell><cell>2</cell><cell>3169</cell><cell>0.59</cell></row><row><cell>TW</cell><cell cols="2">2772 63462</cell><cell>2</cell><cell>3169</cell><cell>0.55</cell></row><row><cell>Actor</cell><cell cols="2">7600 33544</cell><cell>5</cell><cell>931</cell><cell>0.24</cell></row><row><cell>Cornell</cell><cell>183</cell><cell>295</cell><cell>5</cell><cell>1703</cell><cell>0.30</cell></row><row><cell>Texas</cell><cell>183</cell><cell>309</cell><cell>5</cell><cell>1703</cell><cell>0.11</cell></row><row><cell>Wisconsin</cell><cell>251</cell><cell>499</cell><cell>5</cell><cell>1703</cell><cell>0.21</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Comparison of training time(hr.) of achieving the best performance based on GPU.The overall algorithm of SE-GSL is shown in Algorithm 1. Note that, if choose to retain the connection from the previous iteration, to ensure that the number of edges remains stable during the training, a percentage of edges in the reconstructed graph with low similarity will be dropped in each iteration. Algorithm 1: Model training for SE-GSL Input: a graph ? = (? , ?), features ? , labels ? ? , mode ? ? ???, ????? iterations ?, encoding tree height ?, hyperparameter ? Output: optimized graph ? ? = (? , ? ? ), prediction ? ? , GNN parameters ? 1 Initialize ?; 2 for ? = 1 to ? do 3 Update ? by classification loss L ??? (? ? , ? ? ); Create fusion map ? ? according to Algorithm 2; Create ?-dimensional encoding tree T * according to Algorithm 3; Calculate ? T * (? ? ; (?, ?]) through Eq. 7; Sample a node pair (? ? , ? ? ) according to ? 3.4; Adding edge ? ?? to ? ? ; Let ? ? = ? ? ? ? , where ? ? and ? are the edge set of ? ? and ?, respectively; Get prediction ? ? ; 21 Return ? ? , ? ? and ?; A.5 Algorithm of one-dimensional structural entropy guided graph enhancement The ?-selector is designed for choosing an optimal ? for ?-NN structuralization under the guidance of one-dimensional structural entropy maximization. The algorithm of ?-selector and fusion graph construction is shown in Algorithm 2. Algorithm 2: ?-selector and fusion graph construction Input: a graph ? = (? , ?), node representation ? Output: fusion graph ? ? 1 Calculate ? ? R |? |?|? | via Eq. 4; 2 for ? = 2 to |? | -1 do 3 Generate ? ??? by ?; {? , ? ? = ? ? ? ??? };</figDesc><table><row><cell>Method</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell>PT</cell><cell>TW</cell><cell>Actor</cell><cell>Cornell</cell><cell>Texas</cell><cell>Wisconsin</cell></row><row><cell>SE-GSL ???</cell><cell>0.071</cell><cell>0.213</cell><cell>4.574</cell><cell>0.178</cell><cell>0.374</cell><cell>1.482</cell><cell>0.006</cell><cell>0.008</cell><cell>0.009</cell></row><row><cell>SE-GSL ????</cell><cell>0.074</cell><cell>0.076</cell><cell>4.852</cell><cell>0.169</cell><cell>0.214</cell><cell>0.817</cell><cell>0.006</cell><cell>0.007</cell><cell>0.009</cell></row><row><cell>SE-GSL ???</cell><cell>0.071</cell><cell>0.180</cell><cell>4.602</cell><cell>0.172</cell><cell>0.329</cell><cell>1.273</cell><cell>0.006</cell><cell>0.008</cell><cell>0.009</cell></row><row><cell>SE-GSL ??? ? ?</cell><cell>0.073</cell><cell>0.215</cell><cell>4.854</cell><cell>0.138</cell><cell>0.379</cell><cell>1.367</cell><cell>0.010</cell><cell>0.011</cell><cell>0.013</cell></row><row><cell cols="3">A.4 Overall algorithm of SE-GSL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p>4</p>Getting node representation ? ? = GNN(? ); 5 Initialize ? = 1 for ?-NN structuralization; 6 7 8 for each non-root node ? in T * do 9 10 Assign probability ? (?) to ? through Eq. 8; 11 for each subtree rooted at ? in T * do 12 Assuming ? has ? children, set ? = ? ? ?; 13 for ? = 1 to ? do 14 18 Drop a percentage of edges in ? ? ; 19 Update graph structure ? ? ? ? ; Update node representation: ? ? ? ? ; 20 6 Calculate ? 1 (? (?) ? ) via Eq. 1; 7 if ? 1 (? (?) ? ) reaches the maximal optima then 8 ? ? ? ? (?) ? ; 9 Return ? ? ;</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>code is available at: https://github.com/RingBDStack/SE-GSL</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>A vertex is defined in the graph and a node in the tree.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This paper was supported by the <rs type="funder">National Key R&amp;D Program of China</rs> through grant <rs type="grantNumber">2021YFB1714800</rs>, <rs type="funder">NSFC</rs> through grant <rs type="grantNumber">62002007</rs>, <rs type="funder">S&amp;T Program of Hebei</rs> through grant <rs type="grantNumber">20310101D</rs>, <rs type="funder">Natural Science Foundation of Beijing Municipality</rs> through grant <rs type="grantNumber">4222030</rs>, <rs type="funder">CCF-DiDi GAIA Collaborative Research Funds for Young Scholars</rs>, the <rs type="funder">Fundamental Research Funds for the Central Universities</rs>, <rs type="funder">Xiaomi Young Scholar Funds for Beihang University</rs>, and in part by <rs type="funder">NSF</rs> under grants <rs type="grantNumber">III-1763325</rs>, <rs type="grantNumber">III-1909323</rs>, <rs type="grantNumber">III-2106758</rs>, and SaTC-1930941.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_S65f5wn">
					<idno type="grant-number">2021YFB1714800</idno>
				</org>
				<org type="funding" xml:id="_ePjfgmu">
					<idno type="grant-number">62002007</idno>
				</org>
				<org type="funding" xml:id="_hfYRdbe">
					<idno type="grant-number">20310101D</idno>
				</org>
				<org type="funding" xml:id="_vbEM8cB">
					<idno type="grant-number">4222030</idno>
				</org>
				<org type="funding" xml:id="_DjsbrP4">
					<idno type="grant-number">III-1763325</idno>
				</org>
				<org type="funding" xml:id="_wqYk53E">
					<idno type="grant-number">III-1909323</idno>
				</org>
				<org type="funding" xml:id="_3Sw6cf7">
					<idno type="grant-number">III-2106758</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">All experiments are conducted on a Linux server with GPU (NVIDIA RTX A6000) and CPU (Intel i9-10980XE), using PyTorch 1.12 and Python 3.9. Datasets. We experiment on nine open graph benchmark datasets, including three citation networks</title>
	</analytic>
	<monogr>
		<title level="m">Cora, Citeseer, and Pubmed), two social networks (i.e., PT and TW), three website networks from WebKB (i.e., Cornell, Texas, and Wisconsin)</title>
		<imprint/>
	</monogr>
	<note>and a co-occurrence network. Their statistics are summarized in Appendix A.2. Baseline and backbone models. We compare SE-GSL with baselines including general GNNs. i.e., GCN, GAT, GCNII, Grand) and graph learning/high-order neighborhood awareness methods (i.e.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Details are in Appendix A.3. Parameter settings. For SE-GSL with various backbones, we uniformly adopt two-layer GNN encoders. To avoid over-fitting, We adopt ReLU (ELU for GAT) as the activation function and apply a dropout layer with a dropout rate of 0.5. The training iteration is set to 10. The embedding dimension ? is chosen from {8</title>
		<author>
			<persName><forename type="first">Dropedge</forename><surname>Mixhop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geom-Gcn</forename><surname>Gdc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gen</forename><surname>Gcn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gat</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Graphsage</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Four classic GNNs</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
	<note>10, 30}. We adopt the scheme of data split in Geom-GCN [28] and H 2 GCN [60] for all experiments -nodes are randomly divided into the train, validation, and test sets, which take up 48%, 32%, 20%, respectively. In each iteration, the GNN encoder optimization is carried out for 200 epochs, using the Adam optimizer. with an initial learning rate of 0.01 and a weight decay of 5? -4. The model with the highest accuracy on validation sets is used for further testing and reporting. REFERENCES</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral clustering with graph neural networks for graph pooling</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Filippo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cesare</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName><surname>Alippi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="874" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Certifiable robustness to graph perturbations</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Iterative deep graph learning for graph neural networks: Better and robust node embeddings</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="19314" to="19326" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nrgnn: Learning a label noise resistant graph neural network on sparsely and noisily labeled graphs</title>
		<author>
			<persName><forename type="first">Enyan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charu</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="227" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Graph random neural networks for semi-supervised learning on graphs</title>
		<author>
			<persName><forename type="first">Wenzheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Kharlamov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22092" to="22103" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning discrete structures for graph neural networks</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>He</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1972" to="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Predict then Propagate: Graph Neural Networks meet Personalized PageRank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Gasteiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Diffusion improves graph learning</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Gasteiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Wei?enberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the 33rd International Conference on Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13366" to="13378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Link-based classification</title>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced methods for knowledge discovery from complex data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="189" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Explaining explanations: An overview of interpretability of machine learning</title>
		<author>
			<persName><forename type="first">David</forename><surname>Leilani H Gilpin</surname></persName>
		</author>
		<author>
			<persName><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayesha</forename><surname>Ben Z Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lalana</forename><surname>Specter</surname></persName>
		</author>
		<author>
			<persName><surname>Kagal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 5th International Conference on data science and advanced analytics (DSAA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="80" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Graphlime: Local interpretable model explanations for graph neural networks</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph structure learning for robust graph neural networks</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="66" to="74" />
		</imprint>
	</monogr>
	<note>Xianfeng Tang, Suhang Wang, and Jiliang Tang</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Keyulu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hu</forename><surname>Weihua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leskovec</forename><surname>Jure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Structural information and dynamical complexity of networks</title>
		<author>
			<persName><forename type="first">Angsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="3290" to="3339" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Three-dimensional gene map of cancer cell types: Structural entropy minimisation principle for defining tumour subtypes</title>
		<author>
			<persName><forename type="first">Angsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianchen</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Decoding topologically associating domains with ultra-low resolution Hi-C data by graph structural entropy</title>
		<author>
			<persName><forename type="first">Angsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianchen</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingxiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Resistance maximization principle for defending networks against virus attack</title>
		<author>
			<persName><forename type="first">Angsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">466</biblScope>
			<biblScope unit="page" from="211" to="223" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reliable Representations Make A Stronger Defender: Unsupervised Structure Refinement for Robust GNN</title>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghua</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="925" to="935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">Ruoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiyun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">REM: From structural entropy to community structure deception</title>
		<author>
			<persName><forename type="first">Yiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiamou</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liehuang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to drop: Robust graph neural network via topological denoising</title>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenchao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingchao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM international conference on web search and data mining</title>
		<meeting>the 14th ACM international conference on web search and data mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="779" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graph entropy guided node embedding dimension selection for graph neural networks</title>
		<author>
			<persName><forename type="first">Gongxu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2767" to="2774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingchen</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.06036</idno>
		<title level="m">An Information-theoretic Perspective of Hierarchical Clustering</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Geom-GCN: Geometric Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fine-Grained Event Categorization with Heterogeneous Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiran</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanxing</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunfeng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3238" to="3245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Motif-Matching Based Subgraph-Level Attentional Convolutional Network for Graph Classification</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiran</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Senzhang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 34th AAAI Conference on Artificial Intelligence</title>
		<meeting>34th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Yuanxin Ning, and Lifang He</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Reinforced Neighborhood Selection Guided Multi-Relational Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruitong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingtong</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reinforced, Incremental and Cross-Lingual Event Detection From Social Messages</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruitong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoning</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="980" to="998" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph Structure Learning with Variational Information Bottleneck</title>
		<author>
			<persName><forename type="first">Sun</forename><surname>Qingyun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Jianxin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fu</forename><surname>Xingcheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 36th AAAI Conference on Artificial Intelligence</title>
		<meeting>36th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">DropEdge: Towards Deep Graph Convolutional Networks on Node Classification</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-scale attributed node embedding</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Benedek Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rik</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complex Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Twitch gamers: a dataset for evaluating proximity preserving and structural role-based node embeddings</title>
		<author>
			<persName><forename type="first">Benedek</forename><surname>Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rik</forename><surname>Sarkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03091</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName><forename type="first">Claude</forename><surname>Elwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shannon</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Bell system technical journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948">1948. 1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adversarial Attack and Defense on Graph Data: A Survey</title>
		<author>
			<persName><forename type="first">Lichao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingtong</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Position-Aware Structure Learning for Graph Topology-Imbalance by Relieving Under-Reaching and Over-Squashing</title>
		<author>
			<persName><forename type="first">Qingyun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingcheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 31st ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1848" to="1857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Social influence analysis in large-scale networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="807" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deep graph library: A graphcentric, highly-performant package for graph neural networks</title>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph structure estimation neural networks</title>
		<author>
			<persName><forename type="first">Ruijia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanpeng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="342" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Amgcn: Adaptive multi-channel graph convolutional networks</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meiqi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1243" to="1253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Adversarial examples for graph data: deep insights into attack and defense</title>
		<author>
			<persName><forename type="first">Huijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuriy</forename><surname>Tyshetskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Docherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liming</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4816" to="4823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Structural entropy guided graph hierarchical pooling</title>
		<author>
			<persName><forename type="first">Junran</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangzhe</forename><surname>Li</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="24017" to="24030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Graph information bottleneck</title>
		<author>
			<persName><forename type="first">Tailin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="20437" to="20448" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Revisiting semisupervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Minimum Entropy Principle Guided Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Zhengyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Angsheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><forename type="middle">Z</forename><surname>Sheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International WSDM Conference</title>
		<meeting>the ACM International WSDM Conference</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Gnnexplainer: Generating explanations for graph neural networks</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Graphrnn: Generating realistic graphs with deep auto-regressive models</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5708" to="5717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Effective and Stable Role-based Multi-Agent Collaboration by Structural Information Principles</title>
		<author>
			<persName><forename type="first">Xianghua</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 37th AAAI Conference on Artificial Intelligence</title>
		<meeting>37th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Gnnguard: Defending graph neural networks against adversarial attacks</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9263" to="9275" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Robust graph representation learning via neural sparsification</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingchao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenchao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11458" to="11468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Graph neural networks: A review of methods and applications</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Open</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="57" to="81" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7793" to="7804" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03036</idno>
		<title level="m">Deep graph structure learning for robust representations: A survey</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with adaptive augmentation</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2069" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Cagnn: Cluster-aware graph neural networks for unsupervised graph representation learning</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.01674</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">we adopt implementation from DGL library</title>
		<author>
			<persName><forename type="first">Gcn</forename><surname>For</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gat</forename><surname>Graphsage</surname></persName>
		</author>
		<author>
			<persName><surname>Layers</surname></persName>
		</author>
		<ptr target="https://github.com/graphdml-uiuc-jlu/geom-gcn8https://github.com/gasteigerjo/gdc" />
		<imprint>
			<biblScope unit="volume">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
