<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Classification of Hyperspectral Image Based on Double-Branch Dual-Attention Mechanism Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-02-10">10 February 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
							<idno type="ORCID">0000-0001-7858-3160</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Remote Sensing and Information Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shunyi</forename><surname>Zheng</surname></persName>
							<email>syzheng@whu.edu.cn</email>
							<idno type="ORCID">0000-0001-7858-3160</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Remote Sensing and Information Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chenxi</forename><surname>Duan</surname></persName>
							<email>chenxiduan@whu.edu.cn</email>
							<idno type="ORCID">0000-0003-0056-3295</idno>
							<affiliation key="aff1">
								<orgName type="department">State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
							<email>yangyang001@whu.edu.cn</email>
							<idno type="ORCID">0000-0003-0056-3295</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Remote Sensing and Information Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiqi</forename><surname>Wang</surname></persName>
							<email>wangxiqi@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Remote Sensing and Information Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Classification of Hyperspectral Image Based on Double-Branch Dual-Attention Mechanism Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-02-10">10 February 2020</date>
						</imprint>
					</monogr>
					<idno type="MD5">499DCEAFF1DE8111FA27BBB482C15693</idno>
					<idno type="DOI">10.3390/rs12030582</idno>
					<note type="submission">Received: 11 December 2019; Accepted: 8 February 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>hyperspectral image classification</term>
					<term>deep learning</term>
					<term>channel-wise attention mechanism</term>
					<term>spatial-wise attention mechanism</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, researchers have paid increasing attention on hyperspectral image (HSI) classification using deep learning methods. To improve the accuracy and reduce the training samples, we propose a double-branch dual-attention mechanism network (DBDA) for HSI classification in this paper. Two branches are designed in DBDA to capture plenty of spectral and spatial features contained in HSI. Furthermore, a channel attention block and a spatial attention block are applied to these two branches respectively, which enables DBDA to refine and optimize the extracted feature maps. A series of experiments on four hyperspectral datasets show that the proposed framework has superior performance to the state-of-the-art algorithm, especially when the training samples are signally lacking.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Remote sensing images can be categorized by their spatial, spectral, and temporal resolutions <ref type="bibr" target="#b0">[1]</ref>, and has been generally researched for many areas such as land-cover mapping <ref type="bibr" target="#b1">[2]</ref>, water monitoring <ref type="bibr" target="#b2">[3]</ref>, and anomaly detection <ref type="bibr" target="#b3">[4]</ref>. As a particular type of remote sensing images with high spectral resolution, hyperspectral image (HSI) contains plentiful information both in the spectral and spatial dimension <ref type="bibr" target="#b4">[5]</ref>. HSI has been used in many fields including vegetation cover monitoring <ref type="bibr" target="#b5">[6]</ref>, atmospheric environmental research <ref type="bibr" target="#b6">[7]</ref>, and change area detection <ref type="bibr" target="#b7">[8]</ref>, among others. Supervised classification is an essential task of HSI, and is the common technology used in the above applications. However, the over-redundancy of spectral band information and limited training samples account for a huge challenge to HSI classification.</p><p>Early spectral-based attempts including support vector machines (SVM) <ref type="bibr">[9]</ref>, multinomial logistic regression (MLR) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, and random or dynamic subspace <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, focus on the spectral characteristics of HSI. Nevertheless, another useful piece of information is that the adjacent pixels are possibly of the same category, but the spectral-based methods ignore the high spatial correlation and local consistency of HSI. Therefore, the increasing number of classification frameworks based on spectral-spatial features have been presented. Two types of low-level features, morphological profiles [14] and Gabor feature <ref type="bibr" target="#b14">[15]</ref>, were designed to represent the spatial information. Based on SVM, the morphological kernel <ref type="bibr" target="#b15">[16]</ref> and the composite kernel <ref type="bibr" target="#b16">[17]</ref> methods were also proposed to exploit spectral-spatial information. Although above attempts improve the accuracy of the classifier, these methods highly depend on the hand-crafted descriptors.</p><p>Deep learning (DL) has shown powerful capabilities in automatically extracting nonlinear and hierarchical features. A great surge of computer vision tasks have benefited from DL and made significant breakthroughs, such as objection detection <ref type="bibr" target="#b17">[18]</ref>, natural language processing <ref type="bibr" target="#b18">[19]</ref>, and image classification <ref type="bibr" target="#b19">[20]</ref>. As a typical classification tasks, HSI classification has been deeply influenced by DL and has obtained excellent improvements.</p><p>In <ref type="bibr" target="#b20">[21]</ref>, Chen introduced stacked autoencoders (SAE) for extracting useful features. Similarly, Tao <ref type="bibr" target="#b21">[22]</ref> used two sparse SAEs to capture spectral and spatial information separately. Ma et al. <ref type="bibr" target="#b22">[23]</ref> proposed an updated deep auto-encoder (DAE) to extract spectral-spatial features, and designed a novel synergic representation to handle the small-scale training set. Zhang et al. <ref type="bibr" target="#b23">[24]</ref> used a recursive autoencoder (RAE) to extracted high-level features from the neighborhoods of the target pixel and used a new weighting scheme to fuse the spatial information. In <ref type="bibr" target="#b24">[25]</ref>, Chen et al. proposed a classification method based on deep belief network (DBN) and restricted Boltzmann machine (RBM).</p><p>However, in the above-mentioned methods, the input is one-dimensional. Although the spatial information is utilized, the initial structure is destroyed. Since convolutional neural networks (CNN) could exploit spatial feature while retaining the original structure, some novel solutions have been introduced with the advent of CNN. Zhao et al. <ref type="bibr" target="#b25">[26]</ref> adopted CNN as a feature extractor in their framework. Lee et al. <ref type="bibr" target="#b26">[27]</ref> proposed a contextual deep CNN (CDCNN) with deeper and wider networks. In <ref type="bibr" target="#b27">[28]</ref>, Chen et al. designed 3D-CNN-based feature extractor model integrated with regularization.</p><p>Although DL has brought promising improvements in HSI classification, the demand of DL for training samples is enormous, while the cost of manual annotation is rather expensive for HSI. Generally, deeper networks can capture finer features, but it will be harder to train deeper networks. The emergence of the residual network (ResNet) <ref type="bibr">[29]</ref> and the dense convolutional network (DenseNet) <ref type="bibr" target="#b29">[30]</ref> eases the difficulty of training of deeper networks. Inspired by the ResNet, Zhong et al. <ref type="bibr" target="#b30">[31]</ref> proposed a spectral-spatial residual network (SSRN), which is more effective with limited training samples. <ref type="bibr">Wang et al. [32]</ref> introduced DenseNet to their fast dense spectral-spatial convolution (FDSSC) algorithm.</p><p>To optimize the discrimination of extracted features, the attention mechanism was adopted to refine the feature maps. Fang et al. <ref type="bibr" target="#b32">[33]</ref> designed a 3-D dense convolutional network with spectral-wise attention mechanism (MSDN-SA) based on DenseNet and attention mechanism. Ma et al. <ref type="bibr" target="#b33">[34]</ref> proposed a double-branch multi-attention mechanism network (DBMA) motivated by the convolutional block attention module (CBAM) <ref type="bibr" target="#b34">[35]</ref>, and obtained the best classification results.</p><p>Inspired by the latest development of DL fields, some new methods could be observed in the literature. Mou et al. <ref type="bibr" target="#b35">[36]</ref> proposed a recurrent neural networks (RNN) framework for HSI classification in which hyperspectral pixels were analyzed via the sequential perspective. Because of the severe absence of labelled samples in HSI, semi-supervised learning (SSL) <ref type="bibr" target="#b36">[37]</ref>, generative adversarial network (GAN) <ref type="bibr" target="#b37">[38]</ref>, and active learning (AL) <ref type="bibr" target="#b38">[39]</ref> were introduced to alleviate this problem. In <ref type="bibr" target="#b39">[40]</ref>, spectral-spatial capsule networks (CapsNets) were designed to weaken the complexity of the network and enhance the accuracy of the classification. Furthermore, self-pace learning <ref type="bibr" target="#b40">[41]</ref>, self-taught learning <ref type="bibr" target="#b41">[42]</ref>, and superpixel-based methods <ref type="bibr" target="#b42">[43]</ref> are also worth noting.</p><p>In this paper, inspired by the state-of-the-art DBMA algorithm and an adaptive self-attention mechanism dual attention network (DANet) <ref type="bibr" target="#b43">[44]</ref>, we design the double-branch dual-attention mechanism network (DBDA) for HSI classification. The proposed framework contains two branches named the spectral branch and spatial branch, which capture spectral and spatial features separately. The channel-wise attention mechanism and spatial-wise attention mechanism are adopted to refine the feature maps. By concatenating the output of the two branches, we obtain syncretic spectral-spatial features. Finally, the classification results are determined using a softmax function. The three significant contributions of this paper could be listed as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Based on DenseNet and 3D-CNN, we propose an end-to-end framework double-branch dual-attention mechanism network (DBDA). The spectral branch and spatial branch of the proposed framework can exploit features respectively without any feature engineering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>A flexible and adaptive self-attention mechanism is introduced to both the spectral and spatial dimensions. The channel-wise attention block is designed to focus on the information-rich spectral bands, and the spatial-wise attention block is built to concentrate on the information-rich pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The DBDA obtains the state-of-the-art classification accuracy in four datasets with limited training data. Furthermore, the time consumption of our proposed network is less than the two compared deep-learning algorithms.</p><p>The rest of this paper is arranged as follows: In Section 2, we illustrate the related work briefly. The detailed structure of DBDA is given in Section 3. In Sections 4 and 5, we provide and analyze the experimental results. Finally, a conclusion of the entire paper with a direction for future work is presented in Section 6.</p><p>All of our code is available publicly at https://github.com/lironui/Double-Branch-Dual-Attention-Mechanism-Network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we are going to make a brief introduction to the basic modules used in DBDA, including the 3D-cube-based HSI classification framework, 3D-CNN with batch normalization, ResNet and DenseNet, the channel-wise attention mechanism, and the spatial-wise attention mechanism. Since both the number of the HSI spectrums and convolutional kernels could be referred to as channels, we call the number of the HSI spectrums bands, and named the number of the convolutional kernels channels to avoid confusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">HSI Classification Framework Based on 3D-Cube</head><p>Unlike traditional pixel-based methods that only use spectral features, 3D-cube-based methods like SSRN <ref type="bibr" target="#b30">[31]</ref>, FDSSC <ref type="bibr">[32]</ref>, DBMA <ref type="bibr" target="#b33">[34]</ref>, and our proposed framework exploit both spectral and spatial information. The pixel-based methods use the pixel individually to train the network, but the 3D-cube-based methods take the target pixel and its adjacent pixels as input. Certainly, the labels of adjacent central pixels are not fed into the network, and we only explore the abundant spatial information around the target pixel. Generally, the difference between pixel-based methods and 3D-cube-based methods is the input size of the former is 1 × 1 × b, while that of the latter is p × p × b, where p × p represents the number of neighboring pixels and b denotes the number of spectral bands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">3D-CNN with Batch Normalization</head><p>3D-CNN with batch normalization (BN) <ref type="bibr" target="#b44">[45]</ref> is a common element in 3D-cube-based deep learning models. Inputting abundant labelled images, deep learning models with multiple nonlinear layers can learn hierarchical representations, and the multilevel convolutional layers empower CNN to learn characteristics under sparsity constraint more discriminatively. 1D-CNN and 2D-CNN only use spectral features or capture local spatial features of the pixels. When classifying HSI that contains plenty of both spatial and spectral information, 3D-CNN should be adopted to get reasonable results. Therefore, we use 3D-CNN as the basic structure of the DBDA. Moreover, we add a BN layer in each 3D-CNN layer to improve the numerical stability.</p><p>As shown in Figure <ref type="figure" target="#fig_1">1</ref>, with n m input feature maps at the size of p m × p m × b m , a 3D-CNN layer contains k m+1 channels in the size of α m+1 × α m+1 × d m+1 , which generates the n m+1 output feature maps of size p m+1 × p m+1 × b m+1 . The ith output of the (m + 1)th 3D-CNN layer with BN could be calculated as:</p><formula xml:id="formula_0">X m+1 i = R n m j=1 X m j * H m+1 i + b m+1 i<label>(1)</label></formula><formula xml:id="formula_1">X m = X m -E(X m ) Var(X m )<label>(2)</label></formula><p>in which X m j ∈ R p×p×b is the jth input feature map of the (m + 1)th layer, and X m is the output after the BN in the mth layer.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">ResNet and DenseNet</head><p>Normally, the more convolutional layers, the better a network will perform. However, too many layers may make the problems of vanishing and exploding gradients worse. <ref type="bibr">ResNet [29]</ref> and DenseNet <ref type="bibr" target="#b29">[30]</ref> are valid and efficient methods to escape this dilemma.</p><p>Generally, a skip connection is added to the conventional CNN model in ResNet. As indicated in Figure <ref type="figure" target="#fig_7">2a</ref>, H denotes hidden block, which is a module containing convolutional layers, activation layers, and BN layers. The skip connection, which could be regarded as an identity mapping, enables the input data to pass directly through the network. The residual block is the basic unit in ResNet, and the output of the th residual block can be calculated as:</p><formula xml:id="formula_2">= ( ) +<label>(3)</label></formula><p>Based on ResNet, DenseNet connects all layers directly to ensure maximum information flow between each layer of the network. Instead of combining features through summation like ResNet, DenseNet combines features via concatenating them in the channel dimension. The dense block is the basic unit in DenseNet, and the output of the th dense block can be computed as:</p><formula xml:id="formula_3">= [ , , … ,<label>(4)</label></formula><p>in which is a module including convolution layers, activation layers, and BN layers, and , , … , denote the feature maps generated by the preceding dense blocks. As shown in Figure <ref type="figure" target="#fig_7">2b</ref>, more connections ensure more information flow in the DenseNet. Specifically, DenseNet with L layers owns ( + 1) 2 ⁄ , while traditional convolutional networks with equal layers only have L direct connections. The structure of the dense connection block used in our framework can be seen in Figure <ref type="figure" target="#fig_8">3</ref>. The Mish in Figure <ref type="figure" target="#fig_8">3</ref> means the activation function adopted in our framework, and the details about Mish </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">ResNet and DenseNet</head><p>Normally, the more convolutional layers, the better a network will perform. However, too many layers may make the problems of vanishing and exploding gradients worse. <ref type="bibr">ResNet [29]</ref> and DenseNet <ref type="bibr" target="#b29">[30]</ref> are valid and efficient methods to escape this dilemma.</p><p>Generally, a skip connection is added to the conventional CNN model in ResNet. As indicated in Figure <ref type="figure" target="#fig_7">2a</ref>, H denotes hidden block, which is a module containing convolutional layers, activation layers, and BN layers. The skip connection, which could be regarded as an identity mapping, enables the input data to pass directly through the network. The residual block is the basic unit in ResNet, and the output of the lth residual block can be calculated as:</p><formula xml:id="formula_4">x l = H l (x l-1 ) + x l-1<label>(3)</label></formula><p>Based on ResNet, DenseNet connects all layers directly to ensure maximum information flow between each layer of the network. Instead of combining features through summation like ResNet, DenseNet combines features via concatenating them in the channel dimension. The dense block is the basic unit in DenseNet, and the output of the lth dense block can be computed as:</p><formula xml:id="formula_5">x l = H l [x 0 , x 1 , . . . , x l-1 ]<label>(4)</label></formula><p>in which H l is a module including convolution layers, activation layers, and BN layers, and x 0 , x 1 , . . . , x l-1 denote the feature maps generated by the preceding dense blocks. As shown in Figure <ref type="figure" target="#fig_7">2b</ref>, more connections ensure more information flow in the DenseNet. Specifically, DenseNet with L layers owns L(L + 1)/2, while traditional convolutional networks with equal layers only have L direct connections.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">ResNet and DenseNet</head><p>Normally, the more convolutional layers, the better a network will perform. However, too many layers may make the problems of vanishing and exploding gradients worse. ResNet [29] and DenseNet <ref type="bibr" target="#b29">[30]</ref> are valid and efficient methods to escape this dilemma.</p><p>Generally, a skip connection is added to the conventional CNN model in ResNet. As indicated in Figure <ref type="figure" target="#fig_7">2a</ref>, H denotes hidden block, which is a module containing convolutional layers, activation layers, and BN layers. The skip connection, which could be regarded as an identity mapping, enables the input data to pass directly through the network. The residual block is the basic unit in ResNet, and the output of the th residual block can be calculated as:</p><formula xml:id="formula_6">= ( ) +<label>(3)</label></formula><p>Based on ResNet, DenseNet connects all layers directly to ensure maximum information flow between each layer of the network. Instead of combining features through summation like ResNet, DenseNet combines features via concatenating them in the channel dimension. The dense block is the basic unit in DenseNet, and the output of the th dense block can be computed as:</p><formula xml:id="formula_7">= [ , , … ,<label>(4)</label></formula><p>in which is a module including convolution layers, activation layers, and BN layers, and , , … , denote the feature maps generated by the preceding dense blocks. As shown in Figure <ref type="figure" target="#fig_7">2b</ref>, more connections ensure more information flow in the DenseNet. Specifically, DenseNet with L layers owns ( + 1) 2 ⁄ , while traditional convolutional networks with equal layers only have L direct connections. The structure of the dense connection block used in our framework can be seen in Figure <ref type="figure" target="#fig_8">3</ref>. The Mish in Figure <ref type="figure" target="#fig_8">3</ref> means the activation function adopted in our framework, and the details about Mish The structure of the dense connection block used in our framework can be seen in Figure <ref type="figure" target="#fig_8">3</ref>. The Mish in Figure <ref type="figure" target="#fig_8">3</ref> means the activation function adopted in our framework, and the details about Mish can be seen in Section 3.2.1. Supposing that the shape of the input feature maps is p × p × b with n Remote Sens. 2020, 12, 582 5 of 25 channels, and that each convolution layer is composed of k kernels in the shape of 1 × 1 × d, then each layer generates feature maps in the shape of p × p × b with k channels. However, a dense connection concatenates feature maps at the channel dimension, so there is a linear relationship between the number of channels and the number of convolution layers. The output with k m channels generated by an m-layers dense block can be formulated as:</p><formula xml:id="formula_8">k m = b + (m -1) × k (5)</formula><p>where b represents the channel's number in the input feature maps.</p><p>Remote Sens. 2020, 12, 582 5 of 25 can be seen in Section 3.2.1. Supposing that the shape of the input feature maps is × × with n channels, and that each convolution layer is composed of k kernels in the shape of 1 × 1 × , then each layer generates feature maps in the shape of × × with k channels. However, a dense connection concatenates feature maps at the channel dimension, so there is a linear relationship between the number of channels and the number of convolution layers. The output with channels generated by an m-layers dense block can be formulated as:</p><formula xml:id="formula_9">= + ( -1) × (<label>5</label></formula><formula xml:id="formula_10">)</formula><p>where b represents the channel's number in the input feature maps. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Attention Mechanism</head><p>A shortcoming of the 3D-CNN is that all the spatial pixels and spectral bands own the equivalent weights in the spatial and spectral domains. Obviously, different spectral bands and spatial pixels make different contributions to extracting features. The attention mechanism is a powerful technique to deal with this problem. Motivated by the human visual perception process <ref type="bibr" target="#b45">[46]</ref>, the attention mechanism is designed to focus more on the informative areas and takes less account of non-essential areas. The attention mechanism has been used for image categorization <ref type="bibr" target="#b46">[47]</ref> and was later proved to be outstanding in other areas including image caption <ref type="bibr" target="#b47">[48]</ref>, text to image synthesis <ref type="bibr" target="#b48">[49]</ref> and scene segmentation <ref type="bibr" target="#b43">[44]</ref>, etc. In DANet <ref type="bibr" target="#b43">[44]</ref>, the channel attention block and spatial attention block can be adopted to increase the weight of compelling channels and pixels. The two blocks will be introduced in detail as the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1.">Spectral Attention Block</head><p>As illustrated in Figure <ref type="figure" target="#fig_11">4a</ref>, the channel attention map ℝ × is directly computed from the initial input ℝ × × , where × is the patch size of the input, and c denotes the number of the input channels. Concretely, a matrix multiplication between A and is operated, and to obtain the channel attention map ℝ × , a softmax layer is connected as:</p><formula xml:id="formula_11">= × ∑ ×<label>(6)</label></formula><p>in which means the ith channel's influence on the jth channel. Then, the results of matrix multiplication between and A are reshaped into ℝ × × . Finally, the reshaped results are weighted by a parameter of scale and added input A to acquire the final spectral attention map </p><formula xml:id="formula_12">ℝ × × : = ( ) +<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Attention Mechanism</head><p>A shortcoming of the 3D-CNN is that all the spatial pixels and spectral bands own the equivalent weights in the spatial and spectral domains. Obviously, different spectral bands and spatial pixels make different contributions to extracting features. The attention mechanism is a powerful technique to deal with this problem. Motivated by the human visual perception process <ref type="bibr" target="#b45">[46]</ref>, the attention mechanism is designed to focus more on the informative areas and takes less account of non-essential areas. The attention mechanism has been used for image categorization <ref type="bibr" target="#b46">[47]</ref> and was later proved to be outstanding in other areas including image caption <ref type="bibr" target="#b47">[48]</ref>, text to image synthesis <ref type="bibr" target="#b48">[49]</ref> and scene segmentation <ref type="bibr" target="#b43">[44]</ref>, etc. In DANet <ref type="bibr" target="#b43">[44]</ref>, the channel attention block and spatial attention block can be adopted to increase the weight of compelling channels and pixels. The two blocks will be introduced in detail as the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1.">Spectral Attention Block</head><p>As illustrated in Figure <ref type="figure" target="#fig_11">4a</ref>, the channel attention map X ∈ R c×c is directly computed from the initial input A ∈ R c×p×p , where p × p is the patch size of the input, and c denotes the number of the input channels. Concretely, a matrix multiplication between A and A T is operated, and to obtain the channel attention map X ∈ R c×c , a softmax layer is connected as:</p><formula xml:id="formula_13">x ji = exp A i × A j C i=1 exp A i × A j<label>(6)</label></formula><p>Remote Sens. 2020, 12, 582 6 of 25 in which x ji means the ith channel's influence on the jth channel. Then, the results of matrix multiplication between X T and A are reshaped into R c×p×p . Finally, the reshaped results are weighted by a parameter of scale α and added input A to acquire the final spectral attention map E ∈ R c×p×p :</p><formula xml:id="formula_14">E j = α C i=1 (x ji A j ) + A j (<label>7</label></formula><formula xml:id="formula_15">)</formula><p>where α is initialized as zero and can be learned gradually. The final map E encompasses the weighted summations of all channels' features, which can describe long-range dependencies and boost the discriminability about features.</p><p>executed between B and C, and a softmax layer is attached subsequently to calculate the spatial attention feature maps ℝ × :</p><formula xml:id="formula_16">= × ∑ ×<label>(8)</label></formula><p>where measures the impact of ith pixel to the jth pixel. The closer feature representations of the two pixels signify a stronger correlation between them.</p><p>The initial input feature A is simultaneously fed into a convolution layer to obtain a new feature map ℝ × × which is reshaped into ℝ × subsequently. Then a multiplication of matrices is performed between D and , and the result is reshaped into ℝ × × as:</p><formula xml:id="formula_17">= ∑ ( ) + (9)</formula><p>where with a zero initial value can be learned to assign more weight gradually. By Equation ( <ref type="formula">9</ref>), it can be inferred that all positions and original features are added with a certain weight to get the final feature ℝ × × . Therefore, long-range contextual information in the spatial dimension is modeled as E.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2.">Spatial Attention Block</head><p>As illustrated in Figure <ref type="figure" target="#fig_11">4b</ref>, given an input feature map A ∈ R c×p×p , two convolution layers are adopted to generate new feature maps B and C respectively, where {B, C} ∈R c×p×p . Next, B and C are reshaped into R c×n , where n = p × p is the number of pixels. Then a multiplication of matrices is executed between B and C, and a softmax layer is attached subsequently to calculate the spatial attention feature maps S ∈ R n×n :</p><formula xml:id="formula_18">s ji = exp(B i + C j ) N i=1 exp(B i + C j )<label>(8)</label></formula><p>where s ji measures the impact of ith pixel to the jth pixel. The closer feature representations of the two pixels signify a stronger correlation between them.</p><p>The initial input feature A is simultaneously fed into a convolution layer to obtain a new feature map D ∈ R c×p×p which is reshaped into R c×n subsequently. Then a multiplication of matrices is performed between D and S T , and the result is reshaped into R c×p×p as:</p><formula xml:id="formula_19">E j = β N i=1 (s ji D j ) + A j (9)</formula><p>where β with a zero initial value can be learned to assign more weight gradually. By Equation (9), it can be inferred that all positions and original features are added with a certain weight to get the final feature E ∈ R c×p×p . Therefore, long-range contextual information in the spatial dimension is modeled as E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>The procedure of the DBDA framework contains three steps: dataset generation, training and validation, and prediction. Figure <ref type="figure" target="#fig_13">5</ref> illustrates the whole framework of our method.</p><p>An HSI dataset X is supposed to be composed of N labelled pixels , , … , ℝ × × , where b represents the bands, and the corresponding category label set is = , , … , ℝ × × , where c denotes the numbers of land cover classes.</p><p>In the dataset generation step, × neighboring pixels of the center pixel is selected from the original data to generate the 3D-cubes set , , … , ℝ × × . If the target pixel is on the edge of the image, the values of missing adjacent pixels are set as zero. The , i.e., patch size, is set as 9 in our framework. Then, the 3D-cubes set is randomly divided into training set , validation set , and testing set . Accordingly, their corresponding label vectors are divided into , , and . Certainly, the labels of neighboring pixels are not visible to the network, we use the spatial information around target pixel only.</p><p>In the training and validation steps, the training set is used to update the parameters for many epochs, while the validation set is adopted to monitor the performance of models and to select the best-trained model.</p><p>In the prediction step, the test set is chosen to verify the effectiveness of the trained model. The commonly used quantitative indexes for HSI classification to measure the difference between predicted results and real values is the cross-entropy loss function, which is defined as  </p><formula xml:id="formula_20">( , ) = -<label>(10</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Framework of the DBDA Network</head><p>The whole structure of the DBDA network can be seen in Figure <ref type="figure" target="#fig_15">6</ref>. For convenience, we call the top branch Spectral Branch and name the bottom branch Spatial Branch. The input is fed into spectral branch and spatial branch respectively to get the spectral feature maps and spatial feature maps. Then the fusion operation between spectral and spatial feature maps are adopted to get the classification results.</p><p>The following parts introduce the spectral branch, Spatial Branch and spectral and spatial fusion operation taking the Indian Pines (IP) dataset as an example; the patch size is assigned as 9 × 9 × 200. To facilitate the understanding for the matrices mentioned below such as (9 × 9 × 97, 24) , the 9 × 9 × 97 represent the height, width, and depth of the 3D-cube, and 24 represents the number of 3D-cubes generated by 3D-CNN. An HSI dataset X is supposed to be composed of N labelled pixels {x 1 , x 2 , . . . , x n } ∈ R 1×1×b , where b represents the bands, and the corresponding category label set is Y = y 1 , y 2 , . . . , y n ∈ R 1×1×c , where c denotes the numbers of land cover classes.</p><p>In the dataset generation step, p × p neighboring pixels of the center pixel x i is selected from the original data to generate the 3D-cubes set {z 1 , z 2 , . . . , z n } ∈ R p×p×b . If the target pixel is on the edge of the image, the values of missing adjacent pixels are set as zero. The p, i.e., patch size, is set as 9 in our framework. Then, the 3D-cubes set is randomly divided into training set Z train , validation set Z val , and testing set Z test . Accordingly, their corresponding label vectors are divided into Y train , Y val , and Y test . Certainly, the labels of neighboring pixels are not visible to the network, we use the spatial information around target pixel only.</p><p>In the training and validation steps, the training set is used to update the parameters for many epochs, while the validation set is adopted to monitor the performance of models and to select the best-trained model.</p><p>In the prediction step, the test set is chosen to verify the effectiveness of the trained model. The commonly used quantitative indexes for HSI classification to measure the difference between predicted results and real values is the cross-entropy loss function, which is defined as C( y, y) = L m=1 y m log L n=1 e y ny m <ref type="bibr" target="#b9">(10)</ref> where y = [ y 1 , y 2 , . . . , y L ] means the label vector predicted by the model and y = [y 1 , y 2 , . . . y L ] represents the ground-truth label vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Framework of the DBDA Network</head><p>The whole structure of the DBDA network can be seen in Figure <ref type="figure" target="#fig_15">6</ref>. For convenience, we call the top branch Spectral Branch and name the bottom branch Spatial Branch. The input is fed into spectral branch and spatial branch respectively to get the spectral feature maps and spatial feature maps. Then the fusion operation between spectral and spatial feature maps are adopted to get the classification results.</p><p>The following parts introduce the spectral branch, Spatial Branch and spectral and spatial fusion operation taking the Indian Pines (IP) dataset as an example; the patch size is assigned as 9 × 9 × 200. To facilitate the understanding for the matrices mentioned below such as (9 × 9 × 97, 24), the 9 × 9 × 97 represent the height, width, and depth of the 3D-cube, and 24 represents the number of 3D-cubes generated by 3D-CNN.</p><p>The IP dataset contains 145 × 145 pixels with 200 spectral bands, that is, the size of IP is 145 × 145 × 200. The details of IP can be seen in Table <ref type="table" target="#tab_2">3</ref>. There are only 10, 249 pixels have corresponding labels, and the other pixels are background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Spectral Branch with the Channel Attention Block</head><p>First, a 3D-CNN layer with a 1 × 1 × 7 kernel size is used. The down sampling stride is set to (1, 1, 2), which could reduce the number of bands. Then, feature maps in the shape of (9 × 9 × 97, 24) are captured. After that, the dense spectral block combined by 3D-CNN with BN is attached. Each 3D-CNN of the dense spectral block has 12 channels with a 1 × 1 × 7 kernel size. After attaching the dense spectral block, the channels of feature maps increase to 60 calculated by Equation <ref type="bibr" target="#b4">(5)</ref>. Therefore, we obtain feature maps with size of (9 × 9 × 97, 60). Next, after the last 3D-CNN with kernel size of 1 × 1 × 97, a (9 × 9 × 1, 60) feature map is generated. However, the 60 channels make different contributions to the classification. To refine the spectral features, the channel attention block illustrated in Figure <ref type="figure" target="#fig_11">4a</ref> and explained in Section 2.4.1 is adopted. The channel attention block reinforces the informative channels and whittles the information-lacking channels. After obtaining the weighted spectral feature maps by channel attention, a BN layer and a dropout layer are applied to enhance the numerical stability and vanquish the overfitting. Finally, via a global average pooling layer, the feature maps in the shape of 1 × 60 are obtained. The implementation of the spectral branch is available in Table <ref type="table" target="#tab_0">1</ref>. phase. Moreover, the p is selected as 0.5 in our framework. The existence of dropout makes the presence of other units unreliable, which prevents co-adaptation between units.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Spatial Branch with the Spatial Attention Block</head><p>Meanwhile, the input data in the shape of 9 × 9 × 200 are delivered to the spatial branch, and the initial 3D-CNN layer's size is set to 1 × 1 × 200, which can compress spectral bands into one dimension. After that, feature maps in the shape of (9 × 9 × 1, 24) are obtained. Then, the dense spatial block combined by 3D-CNN with BN is attached. Each 3D-CNN in the dense spectral block has 12 channels with a 3 × 3 × 1 kernel size. Next, the extracted feature maps in the shape of (9 × 9 × 1, 60) are fed into the spatial attention block, as illustrated in Figure <ref type="figure" target="#fig_11">4b</ref> and expounded in Section 2.4.2. With the attention block, the coefficient of each pixel is weighted to get a more discriminative spatial feature.</p><p>After capturing the weighted spatial feature maps, a BN layer with a dropout layer is applied. Finally, the spatial feature maps in the shape of 1 × 60 are obtained via a global average pooling layer. The implementation of the spatial branch is given in Table <ref type="table" target="#tab_1">2</ref>. With the spectral branch and spatial branch, several spectral feature maps and spatial feature maps are obtained. Then, we perform a concatenation between two features for classification. Moreover, the reason why the concatenation operation is applied instead of add operation is that the spectral and spatial features are in the irrelevant domains, and the concatenate operation could keep them independent while the add operation would mix them together. In the end, the classification result is obtained via the fully connected layer and the softmax activation function.</p><p>For other datasets, network implementations are the same, and the only difference is the number of spectral bands. The whole methodology flowchart of DBDA is shown in Figure <ref type="figure" target="#fig_17">7</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Measures Taken to Prevent Overfitting</head><p>Numerous training parameters and limited training samples cause the network to be prone overfitting. Thus, we take some measures to prevent overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">A Strong and Appropriate Activation Function</head><p>The activation function brings the concept of nonlinearity to a neural network. An appropriate activation function can accelerate the speed of the counter-propagation and convergence of the network. The activation function we adopted is Mish <ref type="bibr" target="#b49">[50]</ref>, a self-regularized non-monotone activation function, instead of the conventional ReLU(x) = max(0, x) <ref type="bibr" target="#b50">[51]</ref>. The formula for the Mish is:</p><formula xml:id="formula_21">mish(x) = x × tanh(so f tplus(x)) = x i × tanh(ln(1 + e x ))<label>(11)</label></formula><p>where x represents the input of the activation. The comparison of Mish and ReLU can be seen in Figure <ref type="figure" target="#fig_20">8</ref>. Mish is upper unbounded, and lower bounded with a scope of [≈ -0.31, ∞). The differential coefficient definition of Mish is:</p><formula xml:id="formula_22">f (x) = e x ω δ 2 (<label>12</label></formula><formula xml:id="formula_23">)</formula><p>where ω = 4(x + 1) + 4e x + e 3x + e x (4x + 6) and δ = 2e x + e 2x + 2.</p><p>Remote Sens. 2020, 12, 582 11 of 25  In addition, two training skills, the early stopping strategy, and the dynamic learning rate adjustment method are also introduced to our model. Early stopping signifies if the loss function is no longer decreasing for a certain number of epochs (the number is 20 in our model), then we would stop the training process early to prevent overfitting and reduce the training time.</p><p>The learning rate is a crucial hyper parameter to train a network, and dynamic learning rate can help a network avoid some local minima. The cosine annealing <ref type="bibr" target="#b52">[53]</ref> method is adopted to adjust the learning rate dynamically as the following equation:</p><formula xml:id="formula_24">= + 1 2 - 1 + cos<label>(13)</label></formula><p>where is the learning rate within the ith run and , is the range of the learning rate. accounts for the count of epochs that have been executed, and controls the count of epochs that will be executed in a cycle of adjustment. ReLU is a piecewise linear function that prunes all the negative inputs. Thus, if the input is nonpositive, then the neuron is going to "die" and cannot be activated anymore, even though negative inputs might contain useful information. On the contrary, negative inputs are preserved as negative outputs by Mish, which trades the input information and the network sparsity better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Dropout Layer, Early Stopping Strategy and Dynamic Learning Rate Adjustment</head><p>A dropout layer <ref type="bibr" target="#b51">[52]</ref> is adopted between the last BN layer and the global average pooling layer in the spatial branch and spectral branch separately. Dropout is a simple but effective method to prevent overfitting by dropping out units (hidden or visible) on a given percentage p at the training phase. Moreover, the p is selected as 0.5 in our framework. The existence of dropout makes the presence of other units unreliable, which prevents co-adaptation between units.</p><p>In addition, two training skills, the early stopping strategy, and the dynamic learning rate adjustment method are also introduced to our model. Early stopping signifies if the loss function is no longer decreasing for a certain number of epochs (the number is 20 in our model), then we would stop the training process early to prevent overfitting and reduce the training time.</p><p>The learning rate is a crucial hyper parameter to train a network, and dynamic learning rate can help a network avoid some local minima. The cosine annealing <ref type="bibr" target="#b52">[53]</ref> method is adopted to adjust the learning rate dynamically as the following equation:</p><formula xml:id="formula_25">η t = η i min + 1 2 η i max -η i min 1 + cos T cur T i π (<label>13</label></formula><formula xml:id="formula_26">)</formula><p>where η t is the learning rate within the ith run and η i min , η i max is the range of the learning rate. T cur accounts for the count of epochs that have been executed, and T i controls the count of epochs that will be executed in a cycle of adjustment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>To verify the accuracy and efficiency of the proposed model, experiments on four datasets are designed to compare and validate the accuracy and efficiency between the proposed network and other methods. The three quantitative metrics of overall accuracy (OA), average accuracy (AA), and Kappa coefficient (K) are used to measure the accuracy of each method. Concretely, OA represents the ratio of the true classifications of the entire pixels. AA means the average accuracy of all categories. The Kappa coefficient reflects the consistency between the ground truth and classification result. The higher the three metric values are, the better the classification result is. Meanwhile, we investigate the running time for each framework to evaluate its efficiency.</p><p>For each dataset, a certain number of training samples and validation samples are randomly selected from the labelled data on a certain percentage, and the rest of the samples are used to test the performance of the model. Since the proposed DBDA can maintain excellent performance when training samples are severely lacking, the amount of training samples and validation samples are set at a minimal level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The Introduction about Datasets</head><p>In this paper, four widely used HSI datasets, the Indian Pines (IP) dataset, the Pavia University (UP) dataset, the Salinas Valley (SV) dataset, and the Botswana dataset (BS), are employed in the experiments.</p><p>Indian Pines (IP): Obtained through airborne visible infrared imaging spectrometer (AVIRIS) sensor in north-western Indiana, the Indian Pines dataset is composed of 200 spectral bands with a wavelength scope of 0.4 um to 2.5 um and 16 land cover classes. IP encompasses 145 × 145 pixels and owns the resolution of 20 m/pixel.</p><p>Pavia University (UP): Gathered by the reflective optics imaging spectrometer (ROSIS-3) sensor at the University of Pavia, northern Italy, the Pavia University dataset is composed of 103 spectral bands with a wavelength scope of 0.43 um to 0.86 um and 9 land cover classes. UP encompasses 610 × 340 pixels and owns the resolution of 1.3 m/pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Salinas Valley (SV):</head><p>Collected by the AVIRIS sensor from Salinas Valley, CA, USA, the Salinas Valley dataset is composed of 204 spectral bands with a wavelength scope of 0.4 um to 2.5 um and 16 land cover classes. SV encompasses 512 × 217 pixels and owns the resolution of 3.7 m/pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Botswana (BS):</head><p>Captured by the NASA EO-1 satellite over the Okavango Delta, Botswana, the Botswana dataset is composed of 145 spectral bands with a wavelength scope of 0.4 um to 2.5 um and 14 land cover classes. BS encompasses 1476 × 256 pixels and owns the resolution of 30 m/pixel.</p><p>Deep learning algorithms are data-driven, which rely on plenty of labelled training samples. The more labelled data are fed into training, the better accuracy is yielded. However, more data mean more time consumption and higher computation complexity. It is worth noting that the proposed DBDA can maintain excellent performance even though the training samples are very lacking. Therefore, the size of training samples and validation samples are set at a minimal level in the experiments. For IP, we select 3% samples for training, and 3% samples for validation. As the samples are enough for each class of UP and SV, we only select 0.5% samples for training, and 0.5% samples for validation. For BS, the proportion of samples for training and validation is set to 1.2%. The reason why a decimal appears is that the number of samples in BS is small, so we set the ratio as 1% with a ceiling operation. Tables <ref type="table" target="#tab_2">3</ref><ref type="table" target="#tab_3">4</ref><ref type="table" target="#tab_4">5</ref><ref type="table" target="#tab_5">6</ref>list the samples of training, validation and testing for the four datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Setting</head><p>To evaluate the effectiveness of DBDA, the deep-learning-based classifiers CDCNN <ref type="bibr" target="#b26">[27]</ref>, SSRN <ref type="bibr" target="#b30">[31]</ref>, FDSSC <ref type="bibr">[32]</ref>, and the state-of-the-art double-branch multi-attention mechanism network (DBMA) <ref type="bibr" target="#b33">[34]</ref> are compared with our proposed framework. Furthermore, the SVM with RBF kernel [9] is also taken into account. The patch size of each classifier is set according to its original paper. To compare the training and testing consumptions of time, all experiments were executed on the same platform configured with 32 GB of memory and an NVIDIA GeForce RTX 2080Ti GPU. All deep-learning-based classifiers were implemented with PyTorch, and SVM was implemented with sklearn. Then, a brief introduction to the above methods will be given separately. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SVM:</head><p>For SVM with a radial basis function (RBF) kernel, all individual pixels with their spectral bands are fed in directly.</p><p>CDCNN: The architecture of the CDCNN is shown in <ref type="bibr" target="#b26">[27]</ref>, which is based on 2D-CNN and ResNet. The size of input is 5 × 5 × b, where b denotes the number of spectral bands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SSRN:</head><p>The architecture of the SSRN is proposed in <ref type="bibr" target="#b30">[31]</ref>, which is based on 3D-CNN and ResNet. The size of the input is 7 × 7 × b.</p><p>FDSSC: The architecture of the FDSSC can be seen in <ref type="bibr">[32]</ref>, which is based on 3D-CNN and DenseNet. The size of the input is 9 × 9 × b.</p><p>DBMA: The architecture of the DBMA is presented in <ref type="bibr" target="#b33">[34]</ref>, which is based on 3D-CNN, DenseNet, and an attention mechanism. 7 × 7 × b is the input patch size.</p><p>For CDCNN, SSRN, FDSSC, DBMA, and the proposed method, the batch size is set as 16, and the optimizer is set to Adam with the 0.0005 learning rate. The upper limit of the early stopping strategy is set to 200 epochs. If the loss in the validation set no longer declines for 20 epochs, then we would terminate the training phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Classification Maps and Categorized Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Classification Maps and Categorized Results for the IP Dataset</head><p>The categorized results using different methods for the IP dataset are demonstrated in Table <ref type="table" target="#tab_6">7</ref> where the best class-specific accuracy is in bold, and classification maps of the different methods and ground truth are shown in Figure <ref type="figure" target="#fig_22">9</ref>.  The categorized results using different methods for the BS dataset are demonstrated in Table <ref type="table" target="#tab_8">10</ref> where the best class-specific accuracy is in bold, and classification maps of the different methods and ground truth are shown in Figure <ref type="figure" target="#fig_44">12</ref>.</p><p>Since the BS dataset is small and only with 3, 248 labelled samples, just 40 samples are selected as the training set and 40 samples are chosen as the validation set. Nonetheless, our method achieves 96.24% OA performance, 2.81% higher than DBMA. One reason is that our method can capture spatial and spectral features more effectively.  The categorized results using different methods for the BS dataset are demonstrated in Table <ref type="table" target="#tab_8">10</ref> where the best class-specific accuracy is in bold, and classification maps of the different methods and ground truth are shown in Figure <ref type="figure" target="#fig_44">12</ref>.</p><p>Since the BS dataset is small and only with 3, 248 labelled samples, just 40 samples are selected as the training set and 40 samples are chosen as the validation set. Nonetheless, our method achieves 96.24% OA performance, 2.81% higher than DBMA. One reason is that our method can capture spatial and spectral features more effectively.  The categorized results using different methods for the BS dataset are demonstrated in Table <ref type="table" target="#tab_8">10</ref> where the best class-specific accuracy is in bold, and classification maps of the different methods and ground truth are shown in Figure <ref type="figure" target="#fig_44">12</ref>.</p><p>Since the BS dataset is small and only with 3, 248 labelled samples, just 40 samples are selected as the training set and 40 samples are chosen as the validation set. Nonetheless, our method achieves 96.24% OA performance, 2.81% higher than DBMA. One reason is that our method can capture spatial and spectral features more effectively.  The categorized results using different methods for the BS dataset are demonstrated in Table <ref type="table" target="#tab_8">10</ref> where the best class-specific accuracy is in bold, and classification maps of the different methods and ground truth are shown in Figure <ref type="figure" target="#fig_44">12</ref>.</p><p>Since the BS dataset is small and only with 3, 248 labelled samples, just 40 samples are selected as the training set and 40 samples are chosen as the validation set. Nonetheless, our method achieves 96.24% OA performance, 2.81% higher than DBMA. One reason is that our method can capture spatial and spectral features more effectively.  The categorized results using different methods for the BS dataset are demonstrated in Table <ref type="table" target="#tab_8">10</ref> where the best class-specific accuracy is in bold, and classification maps of the different methods and ground truth are shown in Figure <ref type="figure" target="#fig_44">12</ref>.</p><p>Since the BS dataset is small and only with 3, 248 labelled samples, just 40 samples are selected as the training set and 40 samples are chosen as the validation set. Nonetheless, our method achieves 96.24% OA performance, 2.81% higher than DBMA. One reason is that our method can capture spatial and spectral features more effectively.   The categorized results using different methods for the BS dataset are demonstrated in Table <ref type="table" target="#tab_8">10</ref> where the best class-specific accuracy is in bold, and classification maps of the different methods and ground truth are shown in Figure <ref type="figure" target="#fig_44">12</ref>.</p><p>Since the BS dataset is small and only with 3, 248 labelled samples, just 40 samples are selected as the training set and 40 samples are chosen as the validation set. Nonetheless, our method achieves 96.24% OA performance, 2.81% higher than DBMA. One reason is that our method can capture spatial and spectral features more effectively.  The categorized results using different methods for the BS dataset are demonstrated in Table <ref type="table" target="#tab_8">10</ref> where the best class-specific accuracy is in bold, and classification maps of the different methods and ground truth are shown in Figure <ref type="figure" target="#fig_44">12</ref>.</p><p>Since the BS dataset is small and only with 3, 248 labelled samples, just 40 samples are selected as the training set and 40 samples are chosen as the validation set. Nonetheless, our method achieves 96.24% OA performance, 2.81% higher than DBMA. One reason is that our method can capture spatial and spectral features more effectively.  The categorized results using different methods for the BS dataset are demonstrated in Table <ref type="table" target="#tab_8">10</ref> where the best class-specific accuracy is in bold, and classification maps of the different methods and ground truth are shown in Figure <ref type="figure" target="#fig_44">12</ref>.</p><p>Since the BS dataset is small and only with 3, 248 labelled samples, just 40 samples are selected as the training set and 40 samples are chosen as the validation set. Nonetheless, our method achieves 96.24% OA performance, 2.81% higher than DBMA. One reason is that our method can capture spatial and spectral features more effectively.  The categorized results using different methods for the BS dataset are demonstrated in Table <ref type="table" target="#tab_8">10</ref> where the best class-specific accuracy is in bold, and classification maps of the different methods and ground truth are shown in Figure <ref type="figure" target="#fig_44">12</ref>.</p><p>Since the BS dataset is small and only with 3, 248 labelled samples, just 40 samples are selected as the training set and 40 samples are chosen as the validation set. Nonetheless, our method achieves 96.24% OA performance, 2.81% higher than DBMA. One reason is that our method can capture spatial and spectral features more effectively.           The categorized results using different methods for the UP dataset are demonstrated in Table <ref type="table" target="#tab_33">8</ref> where the best class-specific accuracy is in bold, and classification maps for the different methods and ground truth are shown in Figure <ref type="figure" target="#fig_29">10</ref>.</p><p>We can see that our proposed method obtains the best results regarding the three indexes fromTable 8. Though our method cannot make every class precision best, the accuracy of each class using our method exceeds 89%, which means our method is able to capture the distinctive features between different classes.</p><p>Since the samples in the UP dataset are sufficient, there are enough samples for each class even if we just choose 0.5% training samples. Thus, DBMA overcomes overfitting and performs better than FDSSC because of its superior architecture. CDCNN with ample samples surpasses the performance of SVM. OA, due to the limited training samples and weak network structure. Although SVM performs better than CDCNN with more than 7% in OA, the salt-and-pepper noise is severe, which can be seen in Figure <ref type="figure" target="#fig_43">9c</ref>. Because SVM uses no spatial neighborhood information. The 3D-CNN based models far exceed SVM and CDCNN, owing to its incorporation of both spatial and spectral information in the classification. FDSSC uses dense connection instead of residual connection, which enhances the performance of the network and obtains more than 5% improvement in OA compared to SSRN. Based on FDSSC, DBMA extracts the spatial and spectral features in two independent branches and brings the attention mechanism in. However, when training samples are very lacking, DBMA might overfit the training data. With our proposed framework DBDA, it can accomplish stable and reliable performance with limited data duo to its flexible and adaptive attention mechanism, the appropriate activation function, and the other measures to prevent overfitting. Taking class 7, which only has three training samples in the IP dataset, as an example, our method performs well and obtains an acceptable consequence of 92.59%, while the results of other methods (SVM: 56.10%, CDCNN: 0.00%, SSRN: 0.00%, FDSSC: 73.53%, and DBMA: 40.00%) are not very satisfactory.</p><p>Overall, the proposed model improves the OA by 2.23%, the AA by 8.80%, and the kappa by 0.0225 compared to DBMA.   Our proposed framework obtains the best results with 95.38% OA, 96.47% AA, and 0.9474 Kappa, which can be seen from Table <ref type="table" target="#tab_6">7</ref>.CDCNN based on 2D-CNN achieves the worst accuracy with 62.32% OA, due to the limited training samples and weak network structure. Although SVM performs better than CDCNN with more than 7% in OA, the salt-and-pepper noise is severe, which can be seen in Figure <ref type="figure" target="#fig_43">9c</ref>. Because SVM uses no spatial neighborhood information. The 3D-CNN based models far exceed SVM and CDCNN, owing to its incorporation of both spatial and spectral information in the classification. FDSSC uses dense connection instead of residual connection, which enhances the performance of the network and obtains more than 5% improvement in OA compared to SSRN. Based on FDSSC, DBMA extracts the spatial and spectral features in two independent branches and brings the attention mechanism in. However, when training samples are very lacking, DBMA might overfit the training data. With our proposed framework DBDA, it can accomplish stable and reliable performance with limited data duo to its flexible and adaptive attention mechanism, the appropriate activation function, and the other measures to prevent overfitting.</p><p>Taking class 7, which only has three training samples in the IP dataset, as an example, our method performs well and obtains an acceptable consequence of 92.59%, while the results of other methods (SVM: 56.10%, CDCNN: 0.00%, SSRN: 0.00%, FDSSC: 73.53%, and DBMA: 40.00%) are not very satisfactory.</p><p>Overall, the proposed model improves the OA by 2.23%, the AA by 8.80%, and the kappa by 0.0225 compared to DBMA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Classification Maps and Categorized Result for the UP Dataset</head><p>The categorized results using different methods for the UP dataset are demonstrated in Table <ref type="table" target="#tab_33">8</ref> where the best class-specific accuracy is in bold, and classification maps for the different methods and ground truth are shown in Figure <ref type="figure" target="#fig_29">10</ref>.</p><p>We can see that our proposed method obtains the best results regarding the three indexes fromTable 8. Though our method cannot make every class precision best, the accuracy of each class using our method exceeds 89%, which means our method is able to capture the distinctive features between different classes.</p><p>Since the samples in the UP dataset are sufficient, there are enough samples for each class even if we just choose 0.5% training samples. Thus, DBMA overcomes overfitting and performs better than FDSSC because of its superior architecture. CDCNN with ample samples surpasses the performance of SVM.      The categorized results using different methods for the where the best class-specific accuracy is in bold, and classific ground truth are shown in Figure <ref type="figure" target="#fig_44">12</ref>.</p><p>Since the BS dataset is small and only with 3, 248 labell as the training set and 40 samples are chosen as the validatio 96.24% OA performance, 2.81% higher than DBMA. One reas and spectral features more effectively.    fromTable 8. Though our method cannot make every class precision best, the accuracy of each class using our method exceeds 89%, which means our method is able to capture the distinctive features between different classes. Since the samples in the UP dataset are sufficient, there are enough samples for each class even if we just choose 0.5% training samples. Thus, DBMA overcomes overfitting and performs better than FDSSC because of its superior architecture. CDCNN with ample samples surpasses the performance of SVM.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Classification Maps and Categorized Results for the SV Dataset</head><p>The categorized results using the different methods for the SV dataset are demonstrated in Table <ref type="table" target="#tab_7">9</ref> where the best class-specific accuracy is in bold, and classification maps of the different methods and ground truth are shown in Figure <ref type="figure" target="#fig_38">11</ref>.  The categorized results using different methods for the BS dataset are demonstrated in Table <ref type="table" target="#tab_8">10</ref> where the best class-specific accuracy is in bold, and classification maps of the different methods and ground truth are shown in Figure <ref type="figure" target="#fig_44">12</ref>.            OA, due to the limited training samples and weak network structure. Although SVM performs better than CDCNN with more than 7% in OA, the salt-and-pepper noise is severe, which can be seen in Figure <ref type="figure" target="#fig_43">9c</ref>. Because SVM uses no spatial neighborhood information. The 3D-CNN based models far exceed SVM and CDCNN, owing to its incorporation of both spatial and spectral information in the classification. FDSSC uses dense connection instead of residual connection, which enhances the performance of the network and obtains more than 5% improvement in OA compared to SSRN. Based on FDSSC, DBMA extracts the spatial and spectral features in two independent branches and brings the attention mechanism in. However, when training samples are very lacking, DBMA might overfit the training data. With our proposed framework DBDA, it can accomplish stable and reliable performance with limited data duo to its flexible and adaptive attention mechanism, the appropriate activation function, and the other measures to prevent overfitting. Taking class 7, which only has three training samples in the IP dataset, as an example, our method performs well and obtains an acceptable consequence of 92.59%, while the results of other methods (SVM: 56.10%, CDCNN: 0.00%, SSRN: 0.00%, FDSSC: 73.53%, and DBMA: 40.00%) are not very satisfactory.</p><p>Overall, the proposed model improves the OA by 2.23%, the AA by 8.80%, and the kappa by 0.0225 compared to DBMA.  OA, due to the limited training samples and weak network structure. Although SVM performs better than CDCNN with more than 7% in OA, the salt-and-pepper noise is severe, which can be seen in Figure <ref type="figure" target="#fig_43">9c</ref>. Because SVM uses no spatial neighborhood information. The 3D-CNN based models far exceed SVM and CDCNN, owing to its incorporation of both spatial and spectral information in the classification. FDSSC uses dense connection instead of residual connection, which enhances the performance of the network and obtains more than 5% improvement in OA compared to SSRN. Based on FDSSC, DBMA extracts the spatial and spectral features in two independent branches and brings the attention mechanism in. However, when training samples are very lacking, DBMA might overfit the training data. With our proposed framework DBDA, it can accomplish stable and reliable performance with limited data duo to its flexible and adaptive attention mechanism, the appropriate activation function, and the other measures to prevent overfitting. Taking class 7, which only has three training samples in the IP dataset, as an example, our method performs well and obtains an acceptable consequence of 92.59%, while the results of other methods (SVM: 56.10%, CDCNN: 0.00%, SSRN: 0.00%, FDSSC: 73.53%, and DBMA: 40.00%) are not very satisfactory.</p><p>Overall, the proposed model improves the OA by 2.23%, the AA by 8.80%, and the kappa by 0.0225 compared to DBMA.  OA, due to the limited training samples and weak network structure. Although SVM performs better than CDCNN with more than 7% in OA, the salt-and-pepper noise is severe, which can be seen in Figure <ref type="figure" target="#fig_43">9c</ref>. Because SVM uses no spatial neighborhood information. The 3D-CNN based models far exceed SVM and CDCNN, owing to its incorporation of both spatial and spectral information in the classification. FDSSC uses dense connection instead of residual connection, which enhances the performance of the network and obtains more than 5% improvement in OA compared to SSRN. Based on FDSSC, DBMA extracts the spatial and spectral features in two independent branches and brings the attention mechanism in. However, when training samples are very lacking, DBMA might overfit the training data. With our proposed framework DBDA, it can accomplish stable and reliable performance with limited data duo to its flexible and adaptive attention mechanism, the appropriate activation function, and the other measures to prevent overfitting. Taking class 7, which only has three training samples in the IP dataset, as an example, our method performs well and obtains an acceptable consequence of 92.59%, while the results of other methods (SVM: 56.10%, CDCNN: 0.00%, SSRN: 0.00%, FDSSC: 73.53%, and DBMA: 40.00%) are not very satisfactory.</p><p>Overall, the proposed model improves the OA by 2.23%, the AA by 8.80%, and the kappa by 0.0225 compared to DBMA.  OA, due to the limited training samples and weak network structure. Although SVM performs better than CDCNN with more than 7% in OA, the salt-and-pepper noise is severe, which can be seen in Figure <ref type="figure" target="#fig_43">9c</ref>. Because SVM uses no spatial neighborhood information. The 3D-CNN based models far exceed SVM and CDCNN, owing to its incorporation of both spatial and spectral information in the classification. FDSSC uses dense connection instead of residual connection, which enhances the performance of the network and obtains more than 5% improvement in OA compared to SSRN. Based on FDSSC, DBMA extracts the spatial and spectral features in two independent branches and brings the attention mechanism in. However, when training samples are very lacking, DBMA might overfit the training data. With our proposed framework DBDA, it can accomplish stable and reliable performance with limited data duo to its flexible and adaptive attention mechanism, the appropriate activation function, and the other measures to prevent overfitting. Taking class 7, which only has three training samples in the IP dataset, as an example, our method performs well and obtains an acceptable consequence of 92.59%, while the results of other methods (SVM: 56.10%, CDCNN: 0.00%, SSRN: 0.00%, FDSSC: 73.53%, and DBMA: 40.00%) are not very satisfactory.</p><p>Overall, the proposed model improves the OA by 2.23%, the AA by 8.80%, and the kappa by 0.0225 compared to DBMA.  OA, due to the limited training samples and weak network structure. Although SVM performs better than CDCNN with more than 7% in OA, the salt-and-pepper noise is severe, which can be seen in Figure <ref type="figure" target="#fig_43">9c</ref>. Because SVM uses no spatial neighborhood information. The 3D-CNN based models far exceed SVM and CDCNN, owing to its incorporation of both spatial and spectral information in the classification. FDSSC uses dense connection instead of residual connection, which enhances the performance of the network and obtains more than 5% improvement in OA compared to SSRN. Based on FDSSC, DBMA extracts the spatial and spectral features in two independent branches and brings the attention mechanism in. However, when training samples are very lacking, DBMA might overfit the training data. With our proposed framework DBDA, it can accomplish stable and reliable performance with limited data duo to its flexible and adaptive attention mechanism, the appropriate activation function, and the other measures to prevent overfitting. Taking class 7, which only has three training samples in the IP dataset, as an example, our method performs well and obtains an acceptable consequence of 92.59%, while the results of other methods (SVM: 56.10%, CDCNN: 0.00%, SSRN: 0.00%, FDSSC: 73.53%, and DBMA: 40.00%) are not very satisfactory.</p><p>Overall, the proposed model improves the OA by 2.23%, the AA by 8.80%, and the kappa by 0.0225 compared to DBMA.  and ground truth are shown in Figure <ref type="figure" target="#fig_38">11</ref>.</p><p>We can see that our proposed method obtains the best results regarding the three indexes from Table <ref type="table" target="#tab_7">9</ref>, and the accuracy of each category classified by our method exceeds 93%.</p><p>Similarly, because of the sufficient samples in the SV dataset, 0.5% training samples are enough. Thus, DBMA once again performs better than FDSSC. However, the SV dataset owns 16 classes while the UP dataset only has 9 classes, so CDCNN obtains a weaker performance than SVM.  We can see that our proposed method obtains the best results regarding the three indexes from Table <ref type="table" target="#tab_7">9</ref>, and the accuracy of each category classified by our method exceeds 93%.</p><p>Similarly, because of the sufficient samples in the SV dataset, 0.5% training samples are enough. Thus, DBMA once again performs better than FDSSC. However, the SV dataset owns 16 classes while the UP dataset only has 9 classes, so CDCNN obtains a weaker performance than SVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4.">Classification Maps and Categorized Result for the BS Dataset</head><p>The categorized results using different methods for the BS dataset are demonstrated in Table <ref type="table" target="#tab_8">10</ref> where the best class-specific accuracy is in bold, and classification maps of the different methods and ground truth are shown in Figure <ref type="figure" target="#fig_44">12</ref>.</p><p>Since the BS dataset is small and only with 3, 248 labelled samples, just 40 samples are selected as the training set and 40 samples are chosen as the validation set. Nonetheless, our method achieves 96.24% OA performance, 2.81% higher than DBMA. One reason is that our method can capture spatial and spectral features more effectively.  The categorized results using different methods for the BS dataset are demonstrated in Table <ref type="table" target="#tab_8">10</ref> where the best class-specific accuracy is in bold, and classification maps of the different methods and ground truth are shown in Figure <ref type="figure" target="#fig_44">12</ref>.</p><p>Since the BS dataset is small and only with 3, 248 labelled samples, just 40 samples are selected as the training set and 40 samples are chosen as the validation set. Nonetheless, our method achieves 96.24% OA performance, 2.81% higher than DBMA. One reason is that our method can capture spatial and spectral features more effectively.  The categorized results using different methods for the BS dataset are demonstrated in Table <ref type="table" target="#tab_8">10</ref> where the best class-specific accuracy is in bold, and classification maps of the different methods and ground truth are shown in Figure <ref type="figure" target="#fig_44">12</ref>.</p><p>Since the BS dataset is small and only with 3, 248 labelled samples, just 40 samples are selected as the training set and 40 samples are chosen as the validation set. Nonetheless, our method achieves 96.24% OA performance, 2.81% higher than DBMA. One reason is that our method can capture spatial and spectral features more effectively.  The categorized results using different methods for the BS dataset are demonstrated in Table <ref type="table" target="#tab_8">10</ref> where the best class-specific accuracy is in bold, and classification maps of the different methods and ground truth are shown in Figure <ref type="figure" target="#fig_44">12</ref>.</p><p>Since the BS dataset is small and only with 3, 248 labelled samples, just 40 samples are selected as the training set and 40 samples are chosen as the validation set. Nonetheless, our method achieves 96.24% OA performance, 2.81% higher than DBMA. One reason is that our method can capture spatial and spectral features more effectively.  The categorized results using different methods for the BS dataset are demonstrated in Table <ref type="table" target="#tab_8">10</ref> where the best class-specific accuracy is in bold, and classification maps of the different methods and ground truth are shown in Figure <ref type="figure" target="#fig_44">12</ref>.</p><p>Since the BS dataset is small and only with 3, 248 labelled samples, just 40 samples are selected as the training set and 40 samples are chosen as the validation set. Nonetheless, our method achieves 96.24% OA performance, 2.81% higher than DBMA. One reason is that our method can capture spatial and spectral features more effectively.  The categorized results using different methods for the BS dataset are demonstrated in Table <ref type="table" target="#tab_8">10</ref> where the best class-specific accuracy is in bold, and classification maps of the different methods and ground truth are shown in Figure <ref type="figure" target="#fig_44">12</ref>.</p><p>Since the BS dataset is small and only with 3, 248 labelled samples, just 40 samples are selected as the training set and 40 samples are chosen as the validation set. Nonetheless, our method achieves 96.24% OA performance, 2.81% higher than DBMA. One reason is that our method can capture spatial and spectral features more effectively.   The categorized results using different methods for the BS dataset are demonstrated in Table <ref type="table" target="#tab_8">10</ref> where the best class-specific accuracy is in bold, and classification maps of the different methods and ground truth are shown in Figure <ref type="figure" target="#fig_44">12</ref>.</p><p>Since the BS dataset is small and only with 3, 248 labelled samples, just 40 samples are selected as the training set and 40 samples are chosen as the validation set. Nonetheless, our method achieves 96.24% OA performance, 2.81% higher than DBMA. One reason is that our method can capture spatial and spectral features more effectively.  The categorized results using different methods for the BS dataset are demonstrated in Table <ref type="table" target="#tab_8">10</ref> where the best class-specific accuracy is in bold, and classification maps of the different methods and ground truth are shown in Figure <ref type="figure" target="#fig_44">12</ref>.</p><p>Since the BS dataset is small and only with 3, 248 labelled samples, just 40 samples are selected as the training set and 40 samples are chosen as the validation set. Nonetheless, our method achieves 96.24% OA performance, 2.81% higher than DBMA. One reason is that our method can capture spatial and spectral features more effectively.  The categorized results using different methods for the BS dataset are demonstrated in Table <ref type="table" target="#tab_8">10</ref> where the best class-specific accuracy is in bold, and classification maps of the different methods and ground truth are shown in Figure <ref type="figure" target="#fig_44">12</ref>.</p><p>Since the BS dataset is small and only with 3, 248 labelled samples, just 40 samples are selected as the training set and 40 samples are chosen as the validation set. Nonetheless, our method achieves 96.24% OA performance, 2.81% higher than DBMA. One reason is that our method can capture spatial and spectral features more effectively.  The categorized results using different methods for the BS dataset are demonstrated in Table <ref type="table" target="#tab_8">10</ref> where the best class-specific accuracy is in bold, and classification maps of the different methods and ground truth are shown in Figure <ref type="figure" target="#fig_44">12</ref>.</p><p>Since the BS dataset is small and only with 3, 248 labelled samples, just 40 samples are selected as the training set and 40 samples are chosen as the validation set. Nonetheless, our method achieves 96.24% OA performance, 2.81% higher than DBMA. One reason is that our method can capture spatial and spectral features more effectively.          </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Investigation of Running Time</head><p>The above experiments prove that our proposed method can achieve a higher degree of accuracy with less data. However, a good method should balance the accuracy and efficiency properly. This part is executed to measure the efficiency of each method. Tables 11-14 list the consumptions of time for the six algorithms on the IP, UP, SV, and BS datasets.</p><p>Since we use SVM as a pixel-based model, it spends less time than 3D-cube-based models in most cases. On account of 2D-CNN containing less parameters to be trained, CDCNN takes less time than 3D-CNN-based models.</p><p>For 3D-CNN-based models, the proposed method consumes less training time compared to FDSSC and DBMA while obtaining better performance because of its higher rate of convergence. Even though SSRN is quicker than our method, the accuracy of our method is superior. That is, our method can balance the accuracy and efficiency better.</p><p>Table <ref type="table" target="#tab_109">11</ref>. Training and testing consumption of support vector machines (SVM), contextual deep convolutional neural networks (CDCNN), spectral-spatial residual network (SSRN), fast dense spectral-spatial convolution (FDSSC), double-branch multi-attention (DBMA), and our method on the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Investigation of Running Time</head><p>The above experiments prove that our proposed method can achieve a higher degree of accuracy with less data. However, a good method should balance the accuracy and efficiency properly. This part is executed to measure the efficiency of each method. Tables 11-14 list the consumptions of time for the six algorithms on the IP, UP, SV, and BS datasets.</p><p>Since we use SVM as a pixel-based model, it spends less time than 3D-cube-based models in most cases. On account of 2D-CNN containing less parameters to be trained, CDCNN takes less time than 3D-CNN-based models. For 3D-CNN-based models, the proposed method consumes less training time compared to FDSSC and DBMA while obtaining better performance because of its higher rate of convergence. Even though SSRN is quicker than our method, the accuracy of our method is superior. That is, our method can balance the accuracy and efficiency better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>In this part, further assessments of DBDA are conducted. First, different proportions of training samples are fed into the network, and the results reflect that our method can maintain effectiveness especially when the training samples are severely limited. Second, the results of ablation experiments confirm the necessity of the attention mechanism. Third, the results of the different activation functions show that Mish is a better choice than ReLU for DBDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Investigation of the Proportion of Training Samples</head><p>As we mentioned, deep learning is a data-driven algorithm that depends on large amounts of high-quality labelled dataset. In this part, we investigate the scenarios for different proportions of training samples.</p><p>Figure <ref type="figure" target="#fig_46">13</ref> demonstrates the experimental results. For the IP and BS datasets, we use 0.5%, 1%, 3%, 5%, and 10% samples as the training sets, respectively. For the UP and SV datasets, we use 0.1%, 0.5%, 1%, 5%, and 10% of samples as the training sets, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Effectiveness of the Attention Mechanism</head><p>To verify the effectiveness of the attention mechanism, we remove the spatial-attention module, spectral-attention module, and both attention modules of the DBDA respectively, and compare the performance between these three "incomplete DBDA" and the "complete DBDA."</p><p>From Figure <ref type="figure" target="#fig_48">14</ref>, we can conclude that the existence of the spatial attention mechanism and the spectral attention mechanism does promote the accuracy on four datasets.</p><p>Averagely, the attention mechanism improves 4.69% OA on four datasets. Furthermore, a single spatial attention mechanism (average 2.18% improvement) performs better than a single spectral attention mechanism (average 0.97% improvement) upon most occasions. As we expected, the accuracy improves with increase in the number of training samples. All 3D-based methods, including SSRN, FDSSC, DBMA, and the proposed framework can obtain near-perfect performances as long as enough samples (about 10% of the whole dataset) are provided. At the same time, the performance gaps between different models are narrowed according to the increases in training samples. Nevertheless, our method outpaces other methods, especially when samples are insufficient. Since it is costly to label the dataset, our proposed method can save labor and cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Effectiveness of the Attention Mechanism</head><p>To verify the effectiveness of the attention mechanism, we remove the spatial-attention module, spectral-attention module, and both attention modules of the DBDA respectively, and compare the performance between these three "incomplete DBDA" and the "complete DBDA".</p><p>From Figure <ref type="figure" target="#fig_48">14</ref>, we can conclude that the existence of the spatial attention mechanism and the spectral attention mechanism does promote the accuracy on four datasets.</p><p>performance between these three "incomplete DBDA" and the "complete DBDA."</p><p>From Figure <ref type="figure" target="#fig_48">14</ref>, we can conclude that the existence of the spatial attention mechanism and the spectral attention mechanism does promote the accuracy on four datasets.</p><p>Averagely, the attention mechanism improves 4.69% OA on four datasets. Furthermore, a single spatial attention mechanism (average 2.18% improvement) performs better than a single spectral attention mechanism (average 0.97% improvement) upon most occasions.  Averagely, the attention mechanism improves 4.69% OA on four datasets. Furthermore, a single spatial attention mechanism (average 2.18% improvement) performs better than a single spectral attention mechanism (average 0.97% improvement) upon most occasions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Effectiveness of the Activation Function</head><p>In Section 3.2.1, we illustrate why we adopted Mish as the activation function rather than the generally used ReLU. Here, we will compare the performance between DBDA based on Mish and DBDA based on ReLU. Figure <ref type="figure" target="#fig_50">15</ref> shows the classification OA of them.</p><p>Remote Sens. 2020, 12, 582 22 of 25</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Effectiveness of the Activation Function</head><p>In Section 3.2.1, we illustrate why we adopted Mish as the activation function rather than the generally used ReLU. Here, we will compare the performance between DBDA based on Mish and DBDA based on ReLU. Figure <ref type="figure" target="#fig_50">15</ref> shows the classification OA of them.</p><p>As shown in Figure <ref type="figure" target="#fig_50">15</ref>, DBDA based on Mish surpasses DBDA based on ReLU. Specifically, there are 2.27%, 2.01%, 4.00% and 1.24% OA improvements on the IP, UP, SV, and BS datasets, respectively. Since Mish can quicken counter-propagation, the difference in performance occurs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we proposed an end-to-end framework double-branch dual-attention mechanism network for HSI classification. The input of the DBDA framework is original 3D pixel data without any cumbersome pre-processing to reduce dimensionality. Based on densely connected 3D-CNN layers with BN, we designed two branches that capture spectral and spatial features respectively. Meanwhile, a flexible and adaptive self-attention mechanism was applied to spectral branch and spatial branch, respectively. Mish was introduced as the activation function to accelerate the counterpropagation and convergence processes. Dynamic learning rates, early stopping, and dropout layers were also adopted to prevent overfitting.</p><p>Extensive experimental results demonstrate that our proposed framework surpasses the stateof-the-art algorithm, especially when training samples are finite and limited. Meanwhile, the consumption of time is also decreased in comparison to FDSSC and DBMA, as the attention blocks and the activation function Mish accelerate the convergent speed of the model. Accordingly, we draw As shown in Figure <ref type="figure" target="#fig_50">15</ref>, DBDA based on Mish surpasses DBDA based on ReLU. Specifically, there are 2.27%, 2.01%, 4.00% and 1.24% OA improvements on the IP, UP, SV, and BS datasets, respectively. Since Mish can quicken counter-propagation, the difference in performance occurs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we proposed an end-to-end framework double-branch dual-attention mechanism network for HSI classification. The input of the DBDA framework is original 3D pixel data without any cumbersome pre-processing to reduce dimensionality. Based on densely connected 3D-CNN layers with BN, we designed two branches that capture spectral and spatial features respectively. Meanwhile, a flexible and adaptive self-attention mechanism was applied to spectral branch and spatial branch, respectively. Mish was introduced as the activation function to accelerate the counter-propagation and convergence processes. Dynamic learning rates, early stopping, and dropout layers were also adopted to prevent overfitting.</p><p>Extensive experimental results demonstrate that our proposed framework surpasses the state-of-the-art algorithm, especially when training samples are finite and limited. Meanwhile, the consumption of time is also decreased in comparison to FDSSC and DBMA, as the attention blocks and the activation function Mish accelerate the convergent speed of the model. Accordingly, we draw a conclusion that the structure of our method is more preferable for HSI classification.</p><p>A future direction of our work is applying our proposed framework to other hyperspectral images, not just process the above-mentioned open-source datasets. Moreover, it is also an attractive challenge to reduce the training time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>E(•) and Var(•) denote the expectation and variance function of the input separately. H m+1 i and b m+1 i represent the weights and biases of the (m + 1)th 3D-CNN layer, * is the 3D convolutional operation, and R(•) denotes the activation function that introduces the nonlinear unit of the network. Remote Sens. 2020, 12, 582 4 of 25 in which ∈ ℝ × × is the th input feature map of the ( + 1)th layer, and is the output after the BN in the th layer. (•) and (•) denote the expectation and variance function of the input separately. and represent the weights and biases of the ( + 1)th 3D-CNN layer, * is the 3D convolutional operation, and (•) denotes the activation function that introduces the nonlinear unit of the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The structure of 3D-convolutional neural networks (CNN) with a batch normalization (BN) layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The architecture of residual network (ResNet) and dense convolutional network (DenseNet).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The structure of 3D-convolutional neural networks (CNN) with a batch normalization (BN) layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>× is the th input feature map of the ( + 1)th layer, and is the output after the BN in the th layer. (•) and (•) denote the expectation and variance function of the input separately. and represent the weights and biases of the ( + 1)th 3D-CNN layer, * is the 3D convolutional operation, and (•) denotes the activation function that introduces the nonlinear unit of the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The structure of 3D-convolutional neural networks (CNN) with a batch normalization (BN) layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The architecture of residual network (ResNet) and dense convolutional network (DenseNet).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The architecture of residual network (ResNet) and dense convolutional network (DenseNet).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The structure of the dense block used in our framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The structure of the dense block used in our framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The details of the spectral attention block and the spatial attention block.Figure 4. The details of the spectral attention block and the spatial attention block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The details of the spectral attention block and the spatial attention block.Figure 4. The details of the spectral attention block and the spatial attention block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>) where = [ , , … , means the label vector predicted by the model and = [ , , … represents the ground-truth label vector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The procedure of our proposed double-branch dual-attention (DBDA) framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The procedure of our proposed double-branch dual-attention (DBDA) framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. The structure of the DBDA network. The upper spectral branch composed of the dense spectral block and channel attention block is designed to capture spectral features. The lower spatial branch constituted by dense spatial block, and spatial attention block is designed to exploit spatial features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. The structure of the DBDA network. The upper spectral branch composed of the dense spectral block and channel attention block is designed to capture spectral features. The lower spatial branch constituted by dense spatial block, and spatial attention block is designed to exploit spatial features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. The flowchart for the DBDA methodology. The 3D-cube is fed into the spectral branch (top) and spatial branch (bottom) respectively. The obtained features are concatenated to classify the target pixel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. The flowchart for the DBDA methodology. The 3D-cube is fed into the spectral branch (top) and spatial branch (bottom) respectively. The obtained features are concatenated to classify the target pixel.</figDesc><graphic coords="10,136.88,757.37,323.04,195.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. The flowchart for the DBDA methodology. The 3D-cube is fed into the spectral branch (top) and spatial branch (bottom) respectively. The obtained features are concatenated to classify the target pixel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. The graph of the activation functions (Mish and ReLU).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. The graph of the activation functions (Mish and ReLU).</figDesc><graphic coords="11,139.83,344.72,323.04,195.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Classification maps for the IP dataset using 3% training samples. (a) False-color image. (b) Ground-truth (GT). (c-h) The classification maps with disparate algorithms.</figDesc><graphic coords="15,86.91,-12.48,376.80,183.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Classification maps for the IP dataset using 3% training samples. (a) False-color image. (b) Ground-truth (GT). (c-h) The classification maps with disparate algorithms.</figDesc><graphic coords="15,87.07,-9.58,376.80,183.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Classification maps for the IP dataset using 3% training samples. (a) False-color image. (b) Ground-truth (GT). (c-h) The classification maps with disparate algorithms.</figDesc><graphic coords="15,86.85,-13.01,376.80,183.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Classification maps for the IP dataset using 3% training samples. (a) False-color image. (b) Ground-truth (GT). (c-h) The classification maps with disparate algorithms.</figDesc><graphic coords="15,86.91,-11.27,376.80,183.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Classification maps for the IP dataset using 3% training samples. (a) False-color image. (b) Ground-truth (GT). (c-h) The classification maps with disparate algorithms.</figDesc><graphic coords="15,86.91,-9.89,376.80,183.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Classification maps for the IP dataset using 3% training samples. (a) False-color image. (b) Ground-truth (GT). (c-h) The classification maps with disparate algorithms.</figDesc><graphic coords="15,108.75,523.23,376.80,183.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Classification maps for the IP dataset using 3% training samples. (a) False-color image. (b) Ground-truth (GT). (c-h) The classification maps with disparate algorithms.</figDesc><graphic coords="15,72.75,697.40,404.70,321.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Classification maps for the UP dataset using 0.5% training samples. (a) False-color image. (b) Ground-truth (GT). (c-h) The classification maps with disparate algorithms.</figDesc><graphic coords="17,114.66,93.29,364.23,289.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Classification maps for the UP dataset using 0.5% training samples. (a) False-color image. (b) Ground-truth (GT). (c-h) The classification maps with disparate algorithms.</figDesc><graphic coords="17,97.39,274.48,357.96,174.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head></head><label></label><figDesc>4. Classification Maps and Categorized Result for the BS DatThe categorized results using different methods for the BS where the best class-specific accuracy is in bold, and classificatio ground truth are shown in Figure12.Since the BS dataset is small and only with 3, 248 labelled s as the training set and 40 samples are chosen as the validation se 96.24% OA performance, 2.81% higher than DBMA. One reason i and spectral features more effectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Classification maps for the IP dataset using 3% training samples. (a) False-color image. (b) Ground-truth (GT). (c-h) The classification maps with disparate algorithms.</figDesc><graphic coords="17,97.45,274.98,357.96,174.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Classification maps for the IP dataset using 3% training samples. (a) False-color image. (b) Ground-truth (GT). (c-h) The classification maps with disparate algorithms.</figDesc><graphic coords="17,97.60,277.74,357.96,174.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Classification maps for the IP dataset using 3% training samples. (a) False-color image. (b) Ground-truth (GT). (c-h) The classification maps with disparate algorithms.</figDesc><graphic coords="17,97.45,277.44,357.96,174.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Classification maps for the IP dataset using 3% training samples. (a) False-color image. (b) Ground-truth (GT). (c-h) The classification maps with disparate algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Classification maps for the IP dataset using 3% training samples. (a) False-color image. (b) Ground-truth (GT). (c-h) The classification maps with disparate algorithms.</figDesc><graphic coords="17,97.45,276.13,357.96,174.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Classification maps for the UP dataset using 0.5% training samples. (a) False-color image. (b) Ground-truth (GT). (c-h) The classification maps with disparate algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_39"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Classification maps for the UP dataset using 0.5% training samples. (a) False-color image. (b) Ground-truth (GT). (c-h) The classification maps with disparate algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_40"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Classification maps for the IP dataset using 3% training samples. (a) False-color image. (b) Ground-truth (GT). (c-h) The classification maps with disparate algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_41"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Classification maps for the IP dataset using 3% training samples. (a) False-color image. (b) Ground-truth (GT). (c-h) The classification maps with disparate algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_42"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Classification maps for the IP dataset using 3% training samples. (a) False-color image. (b) Ground-truth (GT). (c-h) The classification maps with disparate algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_43"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Classification maps for the IP dataset using 3% training samples. (a) False-color image. (b) Ground-truth (GT). (c-h) The classification maps with disparate algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_44"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Classification maps for the BS dataset using 1.2% training samples. (a) False-color image. (b) Ground-truth (GT). (c-h) The classification maps with disparate algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_45"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Classification maps for the BS dataset using 1.2% training samples. (a) False-color image. (b) Ground-truth (GT). (c-h) The classification maps with disparate algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_46"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. The OA results of SVM, CDCNN, CDCNN, SSRN, FDSSC, DBMA and our proposed method with varying proportions of training samples on the (a) IP, (b) UP, (c) SV and (d) BS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_47"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. The OA results of SVM, CDCNN, CDCNN, SSRN, FDSSC, DBMA and our proposed method with varying proportions of training samples on the (a) IP, (b) UP, (c) SV and (d) BS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_48"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. Effectiveness of the attention mechanism (results of different attention mechanisms).Figure 14. Effectiveness of the attention mechanism (results of different attention mechanisms).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_49"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. Effectiveness of the attention mechanism (results of different attention mechanisms).Figure 14. Effectiveness of the attention mechanism (results of different attention mechanisms).</figDesc><graphic coords="22,145.59,124.00,304.08,124.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_50"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. Effectiveness of the activation function (results on different activation functions).</figDesc><graphic coords="22,145.79,471.82,305.34,124.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_51"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. Effectiveness of the activation function (results on different activation functions).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The implementation details of the spectral branch.</figDesc><table><row><cell>Layer Name</cell><cell>Kernel Size</cell><cell>Output Size</cell></row><row><cell>Input</cell><cell>-</cell><cell>(9 × 9 × 200)</cell></row><row><cell>Conv</cell><cell>(1×1 × 7)</cell><cell>(9 × 9 × 97, 24)</cell></row><row><cell>BN-Mish-Conv</cell><cell>(1×1 × 7)</cell><cell>(9 × 9 × 97, 12)</cell></row><row><cell>Concatenate</cell><cell>-</cell><cell>(9 × 9 × 97, 36)</cell></row><row><cell>BN-Mish-Conv</cell><cell>(1×1 × 7)</cell><cell>(9 × 9 × 97, 12)</cell></row><row><cell>Concatenate</cell><cell>-</cell><cell>(9 × 9 × 97, 48)</cell></row><row><cell>BN-Mish-Conv</cell><cell>(1×1 × 7)</cell><cell>(9 × 9 × 97, 12)</cell></row><row><cell>Concatenate</cell><cell>-</cell><cell>(9 × 9 × 97, 60)</cell></row><row><cell>BN-Mish-Conv</cell><cell>(1×1 × 97)</cell><cell>(9 × 9 × 1, 60)</cell></row><row><cell>Channel Attention Block</cell><cell>-</cell><cell>(9 × 9 × 1, 60)</cell></row><row><cell>BN-Dropout-GlobalAveragePooling</cell><cell>-</cell><cell>(1 × 60)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The implementation details of the spatial branch.</figDesc><table><row><cell>Layer Name</cell><cell>Kernel Size</cell><cell>Output Size</cell></row><row><cell>Input</cell><cell>-</cell><cell>(9 × 9 × 200)</cell></row><row><cell>Conv</cell><cell>(1×1 × 200)</cell><cell>(9 × 9 × 1, 24)</cell></row><row><cell>BN-Mish-Conv</cell><cell>(3×3 × 1)</cell><cell>(9 × 9 × 1, 12)</cell></row><row><cell>Concatenate</cell><cell>-</cell><cell>(9 × 9 × 1, 36)</cell></row><row><cell>BN-Mish-Conv</cell><cell>(3×3 × 1)</cell><cell>(9 × 9 × 1, 12)</cell></row><row><cell>Concatenate</cell><cell>-</cell><cell>(9 × 9 × 1, 48)</cell></row><row><cell>BN-Mish-Conv</cell><cell>(3×3 × 1)</cell><cell>(9 × 9 × 1, 12)</cell></row><row><cell>Concatenate</cell><cell>-</cell><cell>(9 × 9 × 1, 60)</cell></row><row><cell>Channel Attention Block</cell><cell>-</cell><cell>(9 × 9 × 1, 60)</cell></row><row><cell>BN-Dropout-GlobalAveragePooling</cell><cell>-</cell><cell>(1 × 60)</cell></row><row><cell cols="2">3.1.3. Spectral and Spatial Fusion for HSI Classification</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The samples for each category of training, validation and testing for the Indian Pines (IP) dataset.</figDesc><table><row><cell>Order</cell><cell>Class</cell><cell>Total Number</cell><cell>Train</cell><cell>Val</cell><cell>Test</cell></row><row><cell>1</cell><cell>Alfalfa</cell><cell>46</cell><cell>3</cell><cell>3</cell><cell>40</cell></row><row><cell>2</cell><cell>Corn-notill</cell><cell>1428</cell><cell>42</cell><cell>42</cell><cell>1344</cell></row><row><cell>3</cell><cell>Corn-mintill</cell><cell>830</cell><cell>24</cell><cell>24</cell><cell>782</cell></row><row><cell>4</cell><cell>Corn</cell><cell>237</cell><cell>7</cell><cell>7</cell><cell>223</cell></row><row><cell>5</cell><cell>Grass-pasture</cell><cell>483</cell><cell>14</cell><cell>14</cell><cell>455</cell></row><row><cell>6</cell><cell>Grass-trees</cell><cell>730</cell><cell>21</cell><cell>21</cell><cell>688</cell></row><row><cell>7</cell><cell>Grass-pasture-mowed</cell><cell>28</cell><cell>3</cell><cell>3</cell><cell>22</cell></row><row><cell>8</cell><cell>Hay-windrowed</cell><cell>478</cell><cell>14</cell><cell>14</cell><cell>450</cell></row><row><cell>9</cell><cell>Oats</cell><cell>20</cell><cell>3</cell><cell>3</cell><cell>14</cell></row><row><cell>10</cell><cell>Soybean-notill</cell><cell>972</cell><cell>29</cell><cell>29</cell><cell>914</cell></row><row><cell>11</cell><cell>Soybean-mintill</cell><cell>2455</cell><cell>73</cell><cell>73</cell><cell>2309</cell></row><row><cell>12</cell><cell>Soybean-clean</cell><cell>593</cell><cell>17</cell><cell>17</cell><cell>559</cell></row><row><cell>13</cell><cell>Wheat</cell><cell>205</cell><cell>6</cell><cell>6</cell><cell>193</cell></row><row><cell>14</cell><cell>Woods</cell><cell>1265</cell><cell>37</cell><cell>37</cell><cell>1191</cell></row><row><cell>15</cell><cell>Buildings-Grass-Trees-Drives</cell><cell>386</cell><cell>11</cell><cell>11</cell><cell>364</cell></row><row><cell>16</cell><cell>Stone-Steel-Towers</cell><cell>93</cell><cell>3</cell><cell>3</cell><cell>87</cell></row><row><cell></cell><cell>Total</cell><cell>10,249</cell><cell>307</cell><cell>307</cell><cell>9635</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>The samples for each category of training, validation and testing for the Pavia University (UP) dataset.</figDesc><table><row><cell>Order</cell><cell>Class</cell><cell>Total Number</cell><cell>Train</cell><cell>Val</cell><cell>Test</cell></row><row><cell>1</cell><cell>Asphalt</cell><cell>6631</cell><cell>33</cell><cell>33</cell><cell>6565</cell></row><row><cell>2</cell><cell>Meadows</cell><cell>18,649</cell><cell>93</cell><cell>93</cell><cell>18,463</cell></row><row><cell>3</cell><cell>Gravel</cell><cell>2099</cell><cell>10</cell><cell>10</cell><cell>2079</cell></row><row><cell>4</cell><cell>Trees</cell><cell>3064</cell><cell>15</cell><cell>15</cell><cell>3034</cell></row><row><cell>5</cell><cell>Painted metal sheets</cell><cell>1345</cell><cell>6</cell><cell>6</cell><cell>1333</cell></row><row><cell>6</cell><cell>Bare Soil</cell><cell>5029</cell><cell>25</cell><cell>25</cell><cell>4979</cell></row><row><cell>7</cell><cell>Bitumen</cell><cell>1330</cell><cell>6</cell><cell>6</cell><cell>1318</cell></row><row><cell>8</cell><cell>Self-Blocking Bricks</cell><cell>3682</cell><cell>18</cell><cell>18</cell><cell>3646</cell></row><row><cell>9</cell><cell>Shadows</cell><cell>947</cell><cell>4</cell><cell>4</cell><cell>939</cell></row><row><cell></cell><cell>Total</cell><cell>42,776</cell><cell>210</cell><cell>210</cell><cell>42,356</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>The samples for each category of training, validation and testing for the Salinas Valley (SV) dataset.</figDesc><table><row><cell>Order</cell><cell>Class</cell><cell>Total Number</cell><cell>Train</cell><cell>Val</cell><cell>Test</cell></row><row><cell>1</cell><cell>Brocoli-green-weeds-1</cell><cell>2009</cell><cell>10</cell><cell>10</cell><cell>1989</cell></row><row><cell>2</cell><cell>Brocoli-green-weeds-2</cell><cell>3726</cell><cell>18</cell><cell>18</cell><cell>3690</cell></row><row><cell>3</cell><cell>Fallow</cell><cell>1976</cell><cell>9</cell><cell>9</cell><cell>1958</cell></row><row><cell>4</cell><cell>Fallow-rough-plow</cell><cell>1394</cell><cell>6</cell><cell>6</cell><cell>1382</cell></row><row><cell>5</cell><cell>Fallow-smooth</cell><cell>2678</cell><cell>13</cell><cell>13</cell><cell>2652</cell></row><row><cell>6</cell><cell>Stubble</cell><cell>3959</cell><cell>19</cell><cell>19</cell><cell>3921</cell></row><row><cell>7</cell><cell>Celery</cell><cell>3579</cell><cell>17</cell><cell>17</cell><cell>3545</cell></row><row><cell>8</cell><cell>Grapes-untrained</cell><cell>11,271</cell><cell>56</cell><cell>56</cell><cell>11,159</cell></row><row><cell>9</cell><cell>Soil-vinyard-develop</cell><cell>6203</cell><cell>31</cell><cell>31</cell><cell>6141</cell></row><row><cell>10</cell><cell>Corn-senesced-green-weeds</cell><cell>3278</cell><cell>16</cell><cell>16</cell><cell>3246</cell></row><row><cell>11</cell><cell>Lettuce-romaine-4wk</cell><cell>1068</cell><cell>5</cell><cell>5</cell><cell>1058</cell></row><row><cell>12</cell><cell>Lettuce-romaine-5wk</cell><cell>1927</cell><cell>9</cell><cell>94</cell><cell>1824</cell></row><row><cell>13</cell><cell>Lettuce-romaine-6wk</cell><cell>916</cell><cell>4</cell><cell>4</cell><cell>908</cell></row><row><cell>14</cell><cell>Lettuce-romaine-7wk</cell><cell>1070</cell><cell>5</cell><cell>5</cell><cell>1060</cell></row><row><cell>15</cell><cell>Vinyard-untrained</cell><cell>7268</cell><cell>36</cell><cell>36</cell><cell>7196</cell></row><row><cell>16</cell><cell>Vinyard-vertical-trellis</cell><cell>1807</cell><cell>9</cell><cell>9</cell><cell>1789</cell></row><row><cell></cell><cell>Total</cell><cell>54,129</cell><cell>263</cell><cell>348</cell><cell>53,603</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>The samples for each category of training, validation and testing for the Botswana dataset (BS) dataset.</figDesc><table><row><cell>Order</cell><cell>Class</cell><cell>Total Number</cell><cell>Train</cell><cell>Val</cell><cell>Test</cell></row><row><cell>1</cell><cell>Water</cell><cell>270</cell><cell>3</cell><cell>3</cell><cell>264</cell></row><row><cell>2</cell><cell>Hippo grass</cell><cell>101</cell><cell>2</cell><cell>2</cell><cell>97</cell></row><row><cell>3</cell><cell>Floodplain grasses1</cell><cell>251</cell><cell>3</cell><cell>3</cell><cell>245</cell></row><row><cell>4</cell><cell>Floodplain grasses2</cell><cell>215</cell><cell>3</cell><cell>3</cell><cell>209</cell></row><row><cell>5</cell><cell>Reeds1</cell><cell>269</cell><cell>3</cell><cell>3</cell><cell>263</cell></row><row><cell>6</cell><cell>Riparian</cell><cell>269</cell><cell>3</cell><cell>3</cell><cell>263</cell></row><row><cell>7</cell><cell>Fierscar2</cell><cell>259</cell><cell>3</cell><cell>3</cell><cell>253</cell></row><row><cell>8</cell><cell>Island interior</cell><cell>203</cell><cell>3</cell><cell>3</cell><cell>197</cell></row><row><cell>9</cell><cell>Acacia woodlands</cell><cell>314</cell><cell>4</cell><cell>4</cell><cell>306</cell></row><row><cell>10</cell><cell>Acacia shrublands</cell><cell>248</cell><cell>3</cell><cell>3</cell><cell>242</cell></row><row><cell>11</cell><cell>Acacia grasslands</cell><cell>305</cell><cell>4</cell><cell>4</cell><cell>297</cell></row><row><cell>12</cell><cell>Short mopane</cell><cell>181</cell><cell>2</cell><cell>2</cell><cell>177</cell></row><row><cell>13</cell><cell>Mixed mopane</cell><cell>268</cell><cell>3</cell><cell>3</cell><cell>262</cell></row><row><cell>14</cell><cell>Exposed soils</cell><cell>95</cell><cell>1</cell><cell>1</cell><cell>93</cell></row><row><cell></cell><cell>Total</cell><cell>3248</cell><cell>40</cell><cell>40</cell><cell>3168</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>The categorized results for the IP dataset with 3% training samples.</figDesc><table><row><cell>Class</cell><cell>Color</cell><cell>SVM</cell><cell>CDCNN</cell><cell>SSRN</cell><cell>FDSSC</cell><cell>DBMA</cell><cell>Proposed</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>Remote Sens. 2020, 12, 582 18 of 25</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table><row><cell cols="7">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell>kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 .</head><label>10</label><figDesc>The categorized results for the BS dataset with 1.2% training samples.</figDesc><table><row><cell cols="7">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell>1</cell><cell>100.0</cell><cell>94.60</cell><cell>94.95</cell><cell>97.41</cell><cell>97.77</cell><cell>95.64</cell></row><row><cell>2</cell><cell>97.56</cell><cell>68.64</cell><cell>100.0</cell><cell>98.95</cell><cell>88.89</cell><cell>98.99</cell></row><row><cell>3</cell><cell>86.35</cell><cell>81.11</cell><cell>91.42</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>4</cell><cell>63.51</cell><cell>65.45</cell><cell>97.34</cell><cell>93.03</cell><cell>92.51</cell><cell>91.30</cell></row><row><cell>5</cell><cell>84.33</cell><cell>89.10</cell><cell>92.42</cell><cell>80.74</cell><cell>93.51</cell><cell>95.58</cell></row><row><cell>6</cell><cell>61.27</cell><cell>69.28</cell><cell>66.39</cell><cell>84.93</cell><cell>68.94</cell><cell>82.23</cell></row><row><cell>7</cell><cell>82.09</cell><cell>80.07</cell><cell>100.0</cell><cell>84.62</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>8</cell><cell>63.46</cell><cell>89.36</cell><cell>100.0</cell><cell>93.36</cell><cell>96.10</cell><cell>95.63</cell></row><row><cell>9</cell><cell>63.53</cell><cell>55.53</cell><cell>90.75</cell><cell>88.44</cell><cell>85.15</cell><cell>96.50</cell></row><row><cell>10</cell><cell>65.74</cell><cell>81.69</cell><cell>86.83</cell><cell>99.59</cell><cell>97.60</cell><cell>98.79</cell></row><row><cell>11</cell><cell>93.91</cell><cell>92.48</cell><cell>100.0</cell><cell>99.67</cell><cell>99.66</cell><cell>99.67</cell></row></table><note><p>Remote Sens. 2020, 12, 582 18 of 25</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table><row><cell cols="7">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell>kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 .</head><label>10</label><figDesc>The categorized results for the BS dataset with 1.2% training samples.</figDesc><table><row><cell cols="7">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell>1</cell><cell>100.0</cell><cell>94.60</cell><cell>94.95</cell><cell>97.41</cell><cell>97.77</cell><cell>95.64</cell></row><row><cell>2</cell><cell>97.56</cell><cell>68.64</cell><cell>100.0</cell><cell>98.95</cell><cell>88.89</cell><cell>98.99</cell></row><row><cell>3</cell><cell>86.35</cell><cell>81.11</cell><cell>91.42</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>4</cell><cell>63.51</cell><cell>65.45</cell><cell>97.34</cell><cell>93.03</cell><cell>92.51</cell><cell>91.30</cell></row><row><cell>5</cell><cell>84.33</cell><cell>89.10</cell><cell>92.42</cell><cell>80.74</cell><cell>93.51</cell><cell>95.58</cell></row><row><cell>6</cell><cell>61.27</cell><cell>69.28</cell><cell>66.39</cell><cell>84.93</cell><cell>68.94</cell><cell>82.23</cell></row><row><cell>7</cell><cell>82.09</cell><cell>80.07</cell><cell>100.0</cell><cell>84.62</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>8</cell><cell>63.46</cell><cell>89.36</cell><cell>100.0</cell><cell>93.36</cell><cell>96.10</cell><cell>95.63</cell></row><row><cell>9</cell><cell>63.53</cell><cell>55.53</cell><cell>90.75</cell><cell>88.44</cell><cell>85.15</cell><cell>96.50</cell></row><row><cell>10</cell><cell>65.74</cell><cell>81.69</cell><cell>86.83</cell><cell>99.59</cell><cell>97.60</cell><cell>98.79</cell></row><row><cell>11</cell><cell>93.91</cell><cell>92.48</cell><cell>100.0</cell><cell>99.67</cell><cell>99.66</cell><cell>99.67</cell></row></table><note><p>Remote Sens. 2020, 12, 582 18 of 25</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table><row><cell cols="7">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell>kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 .</head><label>10</label><figDesc>The categorized results for the BS dataset with 1.2% training samples.</figDesc><table><row><cell cols="7">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell>1</cell><cell>100.0</cell><cell>94.60</cell><cell>94.95</cell><cell>97.41</cell><cell>97.77</cell><cell>95.64</cell></row><row><cell>2</cell><cell>97.56</cell><cell>68.64</cell><cell>100.0</cell><cell>98.95</cell><cell>88.89</cell><cell>98.99</cell></row><row><cell>3</cell><cell>86.35</cell><cell>81.11</cell><cell>91.42</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>4</cell><cell>63.51</cell><cell>65.45</cell><cell>97.34</cell><cell>93.03</cell><cell>92.51</cell><cell>91.30</cell></row><row><cell>5</cell><cell>84.33</cell><cell>89.10</cell><cell>92.42</cell><cell>80.74</cell><cell>93.51</cell><cell>95.58</cell></row><row><cell>6</cell><cell>61.27</cell><cell>69.28</cell><cell>66.39</cell><cell>84.93</cell><cell>68.94</cell><cell>82.23</cell></row><row><cell>7</cell><cell>82.09</cell><cell>80.07</cell><cell>100.0</cell><cell>84.62</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>8</cell><cell>63.46</cell><cell>89.36</cell><cell>100.0</cell><cell>93.36</cell><cell>96.10</cell><cell>95.63</cell></row><row><cell>9</cell><cell>63.53</cell><cell>55.53</cell><cell>90.75</cell><cell>88.44</cell><cell>85.15</cell><cell>96.50</cell></row><row><cell>10</cell><cell>65.74</cell><cell>81.69</cell><cell>86.83</cell><cell>99.59</cell><cell>97.60</cell><cell>98.79</cell></row><row><cell>11</cell><cell>93.91</cell><cell>92.48</cell><cell>100.0</cell><cell>99.67</cell><cell>99.66</cell><cell>99.67</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table><row><cell cols="7">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell>kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 .</head><label>10</label><figDesc>The categorized results for the BS dataset with 1.2% training samples.</figDesc><table><row><cell cols="7">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell>1</cell><cell>100.0</cell><cell>94.60</cell><cell>94.95</cell><cell>97.41</cell><cell>97.77</cell><cell>95.64</cell></row><row><cell>2</cell><cell>97.56</cell><cell>68.64</cell><cell>100.0</cell><cell>98.95</cell><cell>88.89</cell><cell>98.99</cell></row><row><cell>3</cell><cell>86.35</cell><cell>81.11</cell><cell>91.42</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>4</cell><cell>63.51</cell><cell>65.45</cell><cell>97.34</cell><cell>93.03</cell><cell>92.51</cell><cell>91.30</cell></row><row><cell>5</cell><cell>84.33</cell><cell>89.10</cell><cell>92.42</cell><cell>80.74</cell><cell>93.51</cell><cell>95.58</cell></row><row><cell>6</cell><cell>61.27</cell><cell>69.28</cell><cell>66.39</cell><cell>84.93</cell><cell>68.94</cell><cell>82.23</cell></row><row><cell>7</cell><cell>82.09</cell><cell>80.07</cell><cell>100.0</cell><cell>84.62</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>8</cell><cell>63.46</cell><cell>89.36</cell><cell>100.0</cell><cell>93.36</cell><cell>96.10</cell><cell>95.63</cell></row><row><cell>9</cell><cell>63.53</cell><cell>55.53</cell><cell>90.75</cell><cell>88.44</cell><cell>85.15</cell><cell>96.50</cell></row><row><cell>10</cell><cell>65.74</cell><cell>81.69</cell><cell>86.83</cell><cell>99.59</cell><cell>97.60</cell><cell>98.79</cell></row><row><cell>11</cell><cell>93.91</cell><cell>92.48</cell><cell>100.0</cell><cell>99.67</cell><cell>99.66</cell><cell>99.67</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table><row><cell cols="7">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell>kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 .</head><label>10</label><figDesc>The categorized results for the BS dataset with 1.2% training samples.</figDesc><table><row><cell cols="7">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell>1</cell><cell>100.0</cell><cell>94.60</cell><cell>94.95</cell><cell>97.41</cell><cell>97.77</cell><cell>95.64</cell></row><row><cell>2</cell><cell>97.56</cell><cell>68.64</cell><cell>100.0</cell><cell>98.95</cell><cell>88.89</cell><cell>98.99</cell></row><row><cell>3</cell><cell>86.35</cell><cell>81.11</cell><cell>91.42</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>4</cell><cell>63.51</cell><cell>65.45</cell><cell>97.34</cell><cell>93.03</cell><cell>92.51</cell><cell>91.30</cell></row><row><cell>5</cell><cell>84.33</cell><cell>89.10</cell><cell>92.42</cell><cell>80.74</cell><cell>93.51</cell><cell>95.58</cell></row><row><cell>6</cell><cell>61.27</cell><cell>69.28</cell><cell>66.39</cell><cell>84.93</cell><cell>68.94</cell><cell>82.23</cell></row><row><cell>7</cell><cell>82.09</cell><cell>80.07</cell><cell>100.0</cell><cell>84.62</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>8</cell><cell>63.46</cell><cell>89.36</cell><cell>100.0</cell><cell>93.36</cell><cell>96.10</cell><cell>95.63</cell></row><row><cell>9</cell><cell>63.53</cell><cell>55.53</cell><cell>90.75</cell><cell>88.44</cell><cell>85.15</cell><cell>96.50</cell></row><row><cell>10</cell><cell>65.74</cell><cell>81.69</cell><cell>86.83</cell><cell>99.59</cell><cell>97.60</cell><cell>98.79</cell></row><row><cell>11</cell><cell>93.91</cell><cell>92.48</cell><cell>100.0</cell><cell>99.67</cell><cell>99.66</cell><cell>99.67</cell></row></table><note><p><p>Remote Sens. 2020,</p><ref type="bibr" target="#b11">12,</ref> 582    </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset Classification Maps and Categorized Result for the BS The categorized results using different methods for the where the best class-specific accuracy is in bold, and classific ground truth are shown in Figure12.Since the BS dataset is small and only with 3, 248 labell as the training set and 40 samples are chosen as the validatio 96.24% OA performance, 2.81% higher than DBMA. One reas and spectral features more effectively.</figDesc><table><row><cell cols="2">99.39 99.95 99.75 88.60 98.48 98.81 93.30 99.95 100.0 97.86 89.96 100.0 94.72 96.66 0.9412 0 9 9 9 9 9 9 9 9 1 9 7 1 9 9</cell><cell cols="2">93.56 7 99.89 91.33 74.73 97.69 90.01 75.92 95.19 94.87 89.26 75.86 99.03 88.09 91.45 0.8671 Remote Sens. 2020, 12, 582 98.24 96.51 95.98 88.23 99.26 67.39 72.03 75.49 95.71 94.92 51.88 99.62 77.79 79.86 0.7547</cell><cell>5 6 7 8 9 10 11 12 13 14 15 16 OA AA kappa 4.3.4. 84.02 79.63</cell><cell>98.15</cell><cell>100.0</cell><cell>95.66</cell><cell>97.18 18 of 25</cell></row><row><cell>9</cell><cell>94.85</cell><cell>82.99</cell><cell>97.30</cell><cell>4</cell><cell></cell><cell></cell></row><row><cell>9</cell><cell>89.72</cell><cell>94.69</cell><cell>89.88</cell><cell>3</cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>100.0</cell><cell>64.82</cell><cell>98.95</cell><cell>2</cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>100.0</cell><cell>0.00</cell><cell>99.85</cell><cell>1</cell><cell></cell><cell></cell></row><row><cell cols="5">Class Color SVM CDCNN SSRN FD</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table><row><cell cols="7">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell>kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 10 .</head><label>10</label><figDesc>The categorized results for the BS dataset with 1.2% training samples.</figDesc><table><row><cell cols="7">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell>1</cell><cell>100.0</cell><cell>94.60</cell><cell>94.95</cell><cell>97.41</cell><cell>97.77</cell><cell>95.64</cell></row><row><cell>2</cell><cell>97.56</cell><cell>68.64</cell><cell>100.0</cell><cell>98.95</cell><cell>88.89</cell><cell>98.99</cell></row><row><cell>3</cell><cell>86.35</cell><cell>81.11</cell><cell>91.42</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>4</cell><cell>63.51</cell><cell>65.45</cell><cell>97.34</cell><cell>93.03</cell><cell>92.51</cell><cell>91.30</cell></row><row><cell>5</cell><cell>84.33</cell><cell>89.10</cell><cell>92.42</cell><cell>80.74</cell><cell>93.51</cell><cell>95.58</cell></row><row><cell>6</cell><cell>61.27</cell><cell>69.28</cell><cell>66.39</cell><cell>84.93</cell><cell>68.94</cell><cell>82.23</cell></row><row><cell>7</cell><cell>82.09</cell><cell>80.07</cell><cell>100.0</cell><cell>84.62</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>8</cell><cell>63.46</cell><cell>89.36</cell><cell>100.0</cell><cell>93.36</cell><cell>96.10</cell><cell>95.63</cell></row><row><cell>9</cell><cell>63.53</cell><cell>55.53</cell><cell>90.75</cell><cell>88.44</cell><cell>85.15</cell><cell>96.50</cell></row><row><cell>10</cell><cell>65.74</cell><cell>81.69</cell><cell>86.83</cell><cell>99.59</cell><cell>97.60</cell><cell>98.79</cell></row><row><cell>11</cell><cell>93.91</cell><cell>92.48</cell><cell>100.0</cell><cell>99.67</cell><cell>99.66</cell><cell>99.67</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table><row><cell cols="7">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell>kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 10 .</head><label>10</label><figDesc>The categorized results for the BS dataset with 1.2% training samples.</figDesc><table><row><cell cols="7">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell>1</cell><cell>100.0</cell><cell>94.60</cell><cell>94.95</cell><cell>97.41</cell><cell>97.77</cell><cell>95.64</cell></row><row><cell>2</cell><cell>97.56</cell><cell>68.64</cell><cell>100.0</cell><cell>98.95</cell><cell>88.89</cell><cell>98.99</cell></row><row><cell>3</cell><cell>86.35</cell><cell>81.11</cell><cell>91.42</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>4</cell><cell>63.51</cell><cell>65.45</cell><cell>97.34</cell><cell>93.03</cell><cell>92.51</cell><cell>91.30</cell></row><row><cell>5</cell><cell>84.33</cell><cell>89.10</cell><cell>92.42</cell><cell>80.74</cell><cell>93.51</cell><cell>95.58</cell></row><row><cell>6</cell><cell>61.27</cell><cell>69.28</cell><cell>66.39</cell><cell>84.93</cell><cell>68.94</cell><cell>82.23</cell></row><row><cell>7</cell><cell>82.09</cell><cell>80.07</cell><cell>100.0</cell><cell>84.62</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>8</cell><cell>63.46</cell><cell>89.36</cell><cell>100.0</cell><cell>93.36</cell><cell>96.10</cell><cell>95.63</cell></row><row><cell>9</cell><cell>63.53</cell><cell>55.53</cell><cell>90.75</cell><cell>88.44</cell><cell>85.15</cell><cell>96.50</cell></row><row><cell>10</cell><cell>65.74</cell><cell>81.69</cell><cell>86.83</cell><cell>99.59</cell><cell>97.60</cell><cell>98.79</cell></row><row><cell>11</cell><cell>93.91</cell><cell>92.48</cell><cell>100.0</cell><cell>99.67</cell><cell>99.66</cell><cell>99.67</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table><row><cell cols="7">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell>kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 10 .</head><label>10</label><figDesc>The categorized results for the BS dataset with 1.2% training samples.</figDesc><table><row><cell cols="7">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell>1</cell><cell>100.0</cell><cell>94.60</cell><cell>94.95</cell><cell>97.41</cell><cell>97.77</cell><cell>95.64</cell></row><row><cell>2</cell><cell>97.56</cell><cell>68.64</cell><cell>100.0</cell><cell>98.95</cell><cell>88.89</cell><cell>98.99</cell></row><row><cell>3</cell><cell>86.35</cell><cell>81.11</cell><cell>91.42</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>4</cell><cell>63.51</cell><cell>65.45</cell><cell>97.34</cell><cell>93.03</cell><cell>92.51</cell><cell>91.30</cell></row><row><cell>5</cell><cell>84.33</cell><cell>89.10</cell><cell>92.42</cell><cell>80.74</cell><cell>93.51</cell><cell>95.58</cell></row><row><cell>6</cell><cell>61.27</cell><cell>69.28</cell><cell>66.39</cell><cell>84.93</cell><cell>68.94</cell><cell>82.23</cell></row><row><cell>7</cell><cell>82.09</cell><cell>80.07</cell><cell>100.0</cell><cell>84.62</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>8</cell><cell>63.46</cell><cell>89.36</cell><cell>100.0</cell><cell>93.36</cell><cell>96.10</cell><cell>95.63</cell></row><row><cell>9</cell><cell>63.53</cell><cell>55.53</cell><cell>90.75</cell><cell>88.44</cell><cell>85.15</cell><cell>96.50</cell></row><row><cell>10</cell><cell>65.74</cell><cell>81.69</cell><cell>86.83</cell><cell>99.59</cell><cell>97.60</cell><cell>98.79</cell></row><row><cell>11</cell><cell>93.91</cell><cell>92.48</cell><cell>100.0</cell><cell>99.67</cell><cell>99.66</cell><cell>99.67</cell></row><row><cell>12</cell><cell>90.70</cell><cell>90.91</cell><cell>100.0</cell><cell>100.0</cell><cell>97.79</cell><cell>100.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table><row><cell cols="7">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell>kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 10 .</head><label>10</label><figDesc>The categorized results for the BS dataset with 1.2% training samples.</figDesc><table><row><cell cols="7">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell>1</cell><cell>100.0</cell><cell>94.60</cell><cell>94.95</cell><cell>97.41</cell><cell>97.77</cell><cell>95.64</cell></row><row><cell>2</cell><cell>97.56</cell><cell>68.64</cell><cell>100.0</cell><cell>98.95</cell><cell>88.89</cell><cell>98.99</cell></row><row><cell>3</cell><cell>86.35</cell><cell>81.11</cell><cell>91.42</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>4</cell><cell>63.51</cell><cell>65.45</cell><cell>97.34</cell><cell>93.03</cell><cell>92.51</cell><cell>91.30</cell></row><row><cell>5</cell><cell>84.33</cell><cell>89.10</cell><cell>92.42</cell><cell>80.74</cell><cell>93.51</cell><cell>95.58</cell></row><row><cell>6</cell><cell>61.27</cell><cell>69.28</cell><cell>66.39</cell><cell>84.93</cell><cell>68.94</cell><cell>82.23</cell></row><row><cell>7</cell><cell>82.09</cell><cell>80.07</cell><cell>100.0</cell><cell>84.62</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>8</cell><cell>63.46</cell><cell>89.36</cell><cell>100.0</cell><cell>93.36</cell><cell>96.10</cell><cell>95.63</cell></row><row><cell>9</cell><cell>63.53</cell><cell>55.53</cell><cell>90.75</cell><cell>88.44</cell><cell>85.15</cell><cell>96.50</cell></row><row><cell>10</cell><cell>65.74</cell><cell>81.69</cell><cell>86.83</cell><cell>99.59</cell><cell>97.60</cell><cell>98.79</cell></row><row><cell>11</cell><cell>93.91</cell><cell>92.48</cell><cell>100.0</cell><cell>99.67</cell><cell>99.66</cell><cell>99.67</cell></row><row><cell>12</cell><cell>90.70</cell><cell>90.91</cell><cell>100.0</cell><cell>100.0</cell><cell>97.79</cell><cell>100.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 7 .</head><label>7</label><figDesc>The categorized results for the IP dataset with 3% training samples.</figDesc><table><row><cell cols="8">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell></cell><cell>1</cell><cell>24.24</cell><cell>0.00</cell><cell>100.0</cell><cell>85.42</cell><cell>93.48</cell><cell>100.0</cell></row><row><cell></cell><cell>2</cell><cell>58.10</cell><cell>62.36</cell><cell>89.14</cell><cell>97.20</cell><cell>91.15</cell><cell>88.49</cell></row><row><cell></cell><cell>3</cell><cell>64.37</cell><cell>57.00</cell><cell>77.49</cell><cell>94.45</cell><cell>99.58</cell><cell>97.12</cell></row><row><cell></cell><cell>4</cell><cell>37.07</cell><cell>37.50</cell><cell>88.95</cell><cell>100.0</cell><cell>98.57</cell><cell>100.0</cell></row><row><cell></cell><cell>5</cell><cell>87.67</cell><cell>88.16</cell><cell>96.48</cell><cell>100.0</cell><cell>97.45</cell><cell>100.0</cell></row><row><cell></cell><cell>6</cell><cell>84.02</cell><cell>79.63</cell><cell>98.15</cell><cell>100.0</cell><cell>95.66</cell><cell>97.18</cell></row><row><cell></cell><cell>7</cell><cell>56.10</cell><cell>0.00</cell><cell>0.00</cell><cell>73.53</cell><cell>40.00</cell><cell>92.59</cell></row><row><cell></cell><cell>8</cell><cell>89.62</cell><cell>84.02</cell><cell>84.54</cell><cell>99.78</cell><cell>100.0</cell><cell>99.78</cell></row><row><cell></cell><cell>9</cell><cell>21.21</cell><cell>0.00</cell><cell>0.00</cell><cell>100.0</cell><cell>38.10</cell><cell>100.0</cell></row><row><cell></cell><cell>10</cell><cell>65.89</cell><cell>37.50</cell><cell>92.07</cell><cell>89.25</cell><cell>85.98</cell><cell>89.87</cell></row><row><cell></cell><cell>11</cell><cell>62.32 62.32</cell><cell>53.25 53.25</cell><cell>90.89</cell><cell>93.97 90.89</cell><cell>94.39 93.97</cell><cell>99.33 94.39</cell><cell>99.33</cell></row><row><cell>12</cell><cell>12</cell><cell>52.40</cell><cell>42.96</cell><cell>84.19</cell><cell>95.41</cell><cell>89.92</cell><cell>98.50</cell></row><row><cell></cell><cell>13</cell><cell>94.30</cell><cell>49.47</cell><cell>98.47</cell><cell>100.0</cell><cell>99.48</cell><cell>96.02</cell></row><row><cell></cell><cell>14</cell><cell>90.15</cell><cell>76.71</cell><cell>94.56</cell><cell>93.14</cell><cell>92.81</cell><cell>93.22</cell></row><row><cell></cell><cell>15</cell><cell>63.96</cell><cell>62.60</cell><cell>84.11</cell><cell>90.61</cell><cell>89.66</cell><cell>96.99</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 7 .</head><label>7</label><figDesc>The categorized results for the IP dataset with 3% training samples.</figDesc><table><row><cell cols="8">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell></cell><cell>1</cell><cell>24.24</cell><cell>0.00</cell><cell>100.0</cell><cell>85.42</cell><cell>93.48</cell><cell>100.0</cell></row><row><cell></cell><cell>2</cell><cell>58.10</cell><cell>62.36</cell><cell>89.14</cell><cell>97.20</cell><cell>91.15</cell><cell>88.49</cell></row><row><cell></cell><cell>3</cell><cell>64.37</cell><cell>57.00</cell><cell>77.49</cell><cell>94.45</cell><cell>99.58</cell><cell>97.12</cell></row><row><cell></cell><cell>4</cell><cell>37.07</cell><cell>37.50</cell><cell>88.95</cell><cell>100.0</cell><cell>98.57</cell><cell>100.0</cell></row><row><cell></cell><cell>5</cell><cell>87.67</cell><cell>88.16</cell><cell>96.48</cell><cell>100.0</cell><cell>97.45</cell><cell>100.0</cell></row><row><cell></cell><cell>6</cell><cell>84.02</cell><cell>79.63</cell><cell>98.15</cell><cell>100.0</cell><cell>95.66</cell><cell>97.18</cell></row><row><cell></cell><cell>7</cell><cell>56.10</cell><cell>0.00</cell><cell>0.00</cell><cell>73.53</cell><cell>40.00</cell><cell>92.59</cell></row><row><cell></cell><cell>8</cell><cell>89.62</cell><cell>84.02</cell><cell>84.54</cell><cell>99.78</cell><cell>100.0</cell><cell>99.78</cell></row><row><cell></cell><cell>9</cell><cell>21.21</cell><cell>0.00</cell><cell>0.00</cell><cell>100.0</cell><cell>38.10</cell><cell>100.0</cell></row><row><cell></cell><cell>10</cell><cell>65.89</cell><cell>37.50</cell><cell>92.07</cell><cell>89.25</cell><cell>85.98</cell><cell>89.87</cell></row><row><cell></cell><cell>11</cell><cell>62.32</cell><cell>53.25</cell><cell>90.89</cell><cell>93.97</cell><cell>94.39</cell><cell>99.33</cell></row><row><cell></cell><cell>12</cell><cell>52.40 52.40</cell><cell>42.96 42.96</cell><cell>84.19</cell><cell>95.41 84.19</cell><cell>89.92 95.41</cell><cell>98.50 89.92</cell><cell>98.50</cell></row><row><cell>13</cell><cell>13</cell><cell>94.30</cell><cell>49.47</cell><cell>98.47</cell><cell>100.0</cell><cell>99.48</cell><cell>96.02</cell></row><row><cell></cell><cell>14</cell><cell>90.15</cell><cell>76.71</cell><cell>94.56</cell><cell>93.14</cell><cell>92.81</cell><cell>93.22</cell></row><row><cell></cell><cell>15</cell><cell>63.96</cell><cell>62.60</cell><cell>84.11</cell><cell>90.61</cell><cell>89.66</cell><cell>96.99</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 7 .</head><label>7</label><figDesc>The categorized results for the IP dataset with 3% training samples.</figDesc><table><row><cell cols="8">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell></cell><cell>1</cell><cell>24.24</cell><cell>0.00</cell><cell>100.0</cell><cell>85.42</cell><cell>93.48</cell><cell>100.0</cell></row><row><cell></cell><cell>2</cell><cell>58.10</cell><cell>62.36</cell><cell>89.14</cell><cell>97.20</cell><cell>91.15</cell><cell>88.49</cell></row><row><cell></cell><cell>3</cell><cell>64.37</cell><cell>57.00</cell><cell>77.49</cell><cell>94.45</cell><cell>99.58</cell><cell>97.12</cell></row><row><cell></cell><cell>4</cell><cell>37.07</cell><cell>37.50</cell><cell>88.95</cell><cell>100.0</cell><cell>98.57</cell><cell>100.0</cell></row><row><cell></cell><cell>5</cell><cell>87.67</cell><cell>88.16</cell><cell>96.48</cell><cell>100.0</cell><cell>97.45</cell><cell>100.0</cell></row><row><cell></cell><cell>6</cell><cell>84.02</cell><cell>79.63</cell><cell>98.15</cell><cell>100.0</cell><cell>95.66</cell><cell>97.18</cell></row><row><cell></cell><cell>7</cell><cell>56.10</cell><cell>0.00</cell><cell>0.00</cell><cell>73.53</cell><cell>40.00</cell><cell>92.59</cell></row><row><cell></cell><cell>8</cell><cell>89.62</cell><cell>84.02</cell><cell>84.54</cell><cell>99.78</cell><cell>100.0</cell><cell>99.78</cell></row><row><cell></cell><cell>9</cell><cell>21.21</cell><cell>0.00</cell><cell>0.00</cell><cell>100.0</cell><cell>38.10</cell><cell>100.0</cell></row><row><cell></cell><cell>10</cell><cell>65.89</cell><cell>37.50</cell><cell>92.07</cell><cell>89.25</cell><cell>85.98</cell><cell>89.87</cell></row><row><cell></cell><cell>11</cell><cell>62.32</cell><cell>53.25</cell><cell>90.89</cell><cell>93.97</cell><cell>94.39</cell><cell>99.33</cell></row><row><cell></cell><cell>12</cell><cell>52.40</cell><cell>42.96</cell><cell>84.19</cell><cell>95.41</cell><cell>89.92</cell><cell>98.50</cell></row><row><cell></cell><cell>13</cell><cell>94.30 94.30</cell><cell>49.47 49.47</cell><cell>98.47</cell><cell>100.0 98.47</cell><cell>99.48 100.0</cell><cell>96.02 99.48</cell><cell>96.02</cell></row><row><cell>14</cell><cell>14</cell><cell>90.15</cell><cell>76.71</cell><cell>94.56</cell><cell>93.14</cell><cell>92.81</cell><cell>93.22</cell></row><row><cell></cell><cell>15</cell><cell>63.96</cell><cell>62.60</cell><cell>84.11</cell><cell>90.61</cell><cell>89.66</cell><cell>96.99</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Table 7 .</head><label>7</label><figDesc>The categorized results for the IP dataset with 3% training samples.</figDesc><table><row><cell cols="8">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell></cell><cell>1</cell><cell>24.24</cell><cell>0.00</cell><cell>100.0</cell><cell>85.42</cell><cell>93.48</cell><cell>100.0</cell></row><row><cell></cell><cell>2</cell><cell>58.10</cell><cell>62.36</cell><cell>89.14</cell><cell>97.20</cell><cell>91.15</cell><cell>88.49</cell></row><row><cell></cell><cell>3</cell><cell>64.37</cell><cell>57.00</cell><cell>77.49</cell><cell>94.45</cell><cell>99.58</cell><cell>97.12</cell></row><row><cell></cell><cell>4</cell><cell>37.07</cell><cell>37.50</cell><cell>88.95</cell><cell>100.0</cell><cell>98.57</cell><cell>100.0</cell></row><row><cell></cell><cell>5</cell><cell>87.67</cell><cell>88.16</cell><cell>96.48</cell><cell>100.0</cell><cell>97.45</cell><cell>100.0</cell></row><row><cell></cell><cell>6</cell><cell>84.02</cell><cell>79.63</cell><cell>98.15</cell><cell>100.0</cell><cell>95.66</cell><cell>97.18</cell></row><row><cell></cell><cell>7</cell><cell>56.10</cell><cell>0.00</cell><cell>0.00</cell><cell>73.53</cell><cell>40.00</cell><cell>92.59</cell></row><row><cell></cell><cell>8</cell><cell>89.62</cell><cell>84.02</cell><cell>84.54</cell><cell>99.78</cell><cell>100.0</cell><cell>99.78</cell></row><row><cell></cell><cell>9</cell><cell>21.21</cell><cell>0.00</cell><cell>0.00</cell><cell>100.0</cell><cell>38.10</cell><cell>100.0</cell></row><row><cell></cell><cell>10</cell><cell>65.89</cell><cell>37.50</cell><cell>92.07</cell><cell>89.25</cell><cell>85.98</cell><cell>89.87</cell></row><row><cell></cell><cell>11</cell><cell>62.32</cell><cell>53.25</cell><cell>90.89</cell><cell>93.97</cell><cell>94.39</cell><cell>99.33</cell></row><row><cell></cell><cell>12</cell><cell>52.40</cell><cell>42.96</cell><cell>84.19</cell><cell>95.41</cell><cell>89.92</cell><cell>98.50</cell></row><row><cell></cell><cell>13</cell><cell>94.30</cell><cell>49.47</cell><cell>98.47</cell><cell>100.0</cell><cell>99.48</cell><cell>96.02</cell></row><row><cell></cell><cell>14</cell><cell>90.15 90.15</cell><cell>76.71 76.71</cell><cell>94.56</cell><cell>93.14 94.56</cell><cell>92.81 93.14</cell><cell>93.22 92.81</cell><cell>93.22</cell></row><row><cell>15</cell><cell>15</cell><cell>63.96</cell><cell>62.60</cell><cell>84.11</cell><cell>90.61</cell><cell>89.66</cell><cell>96.99</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30"><head>Table 7 .</head><label>7</label><figDesc>The categorized results for the IP dataset with 3% training samples.</figDesc><table><row><cell cols="8">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell></cell><cell>1</cell><cell>24.24</cell><cell>0.00</cell><cell>100.0</cell><cell>85.42</cell><cell>93.48</cell><cell>100.0</cell></row><row><cell></cell><cell>2</cell><cell>58.10</cell><cell>62.36</cell><cell>89.14</cell><cell>97.20</cell><cell>91.15</cell><cell>88.49</cell></row><row><cell></cell><cell>3</cell><cell>64.37</cell><cell>57.00</cell><cell>77.49</cell><cell>94.45</cell><cell>99.58</cell><cell>97.12</cell></row><row><cell></cell><cell>4</cell><cell>37.07</cell><cell>37.50</cell><cell>88.95</cell><cell>100.0</cell><cell>98.57</cell><cell>100.0</cell></row><row><cell></cell><cell>5</cell><cell>87.67</cell><cell>88.16</cell><cell>96.48</cell><cell>100.0</cell><cell>97.45</cell><cell>100.0</cell></row><row><cell></cell><cell>6</cell><cell>84.02</cell><cell>79.63</cell><cell>98.15</cell><cell>100.0</cell><cell>95.66</cell><cell>97.18</cell></row><row><cell></cell><cell>7</cell><cell>56.10</cell><cell>0.00</cell><cell>0.00</cell><cell>73.53</cell><cell>40.00</cell><cell>92.59</cell></row><row><cell></cell><cell>8</cell><cell>89.62</cell><cell>84.02</cell><cell>84.54</cell><cell>99.78</cell><cell>100.0</cell><cell>99.78</cell></row><row><cell></cell><cell>9</cell><cell>21.21</cell><cell>0.00</cell><cell>0.00</cell><cell>100.0</cell><cell>38.10</cell><cell>100.0</cell></row><row><cell></cell><cell>10</cell><cell>65.89</cell><cell>37.50</cell><cell>92.07</cell><cell>89.25</cell><cell>85.98</cell><cell>89.87</cell></row><row><cell></cell><cell>11</cell><cell>62.32</cell><cell>53.25</cell><cell>90.89</cell><cell>93.97</cell><cell>94.39</cell><cell>99.33</cell></row><row><cell></cell><cell>12</cell><cell>52.40</cell><cell>42.96</cell><cell>84.19</cell><cell>95.41</cell><cell>89.92</cell><cell>98.50</cell></row><row><cell></cell><cell>13</cell><cell>94.30</cell><cell>49.47</cell><cell>98.47</cell><cell>100.0</cell><cell>99.48</cell><cell>96.02</cell></row><row><cell cols="2">14 Remote Sens. 2020, 12, 582</cell><cell>90.15</cell><cell>76.71</cell><cell>94.56</cell><cell>93.14</cell><cell>92.81</cell><cell>93.22</cell><cell>16 of 25</cell></row><row><cell></cell><cell>15</cell><cell>63.96 63.96</cell><cell>62.60 62.60</cell><cell>84.11</cell><cell>90.61 84.11</cell><cell>89.66 90.61</cell><cell>96.99 89.66</cell><cell>96.99</cell></row><row><cell>16</cell><cell>16</cell><cell>98.46</cell><cell>83.70</cell><cell>91.40</cell><cell>96.55</cell><cell>96.55</cell><cell>94.38</cell></row><row><cell cols="2">OA</cell><cell>69.41</cell><cell>62.32</cell><cell>89.81</cell><cell>94.87</cell><cell>93.15</cell><cell>95.38</cell></row><row><cell cols="2">AA</cell><cell>65.62</cell><cell>50.93</cell><cell>79.40</cell><cell>94.33</cell><cell>87.67</cell><cell>96.47</cell></row><row><cell cols="2">kappa</cell><cell>0.6472</cell><cell>0.5593</cell><cell cols="2">0.8839 0.9414</cell><cell>0.9219</cell><cell>0.9474</cell></row></table><note><p>4.3.2. Classification Maps and Categorized Result for the UP Dataset</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_32"><head>Table 7 .</head><label>7</label><figDesc>The categorized results for the IP dataset with 3% training samples.</figDesc><table><row><cell>1</cell><cell>24.24</cell><cell>0.00</cell><cell>100.0</cell><cell>85.42</cell><cell>93.48</cell><cell>100.0</cell></row><row><cell>2</cell><cell>58.10</cell><cell>62.36</cell><cell>89.14</cell><cell>97.20</cell><cell>91.15</cell><cell>88.49</cell></row><row><cell>3</cell><cell>64.37</cell><cell>57.00</cell><cell>77.49</cell><cell>94.45</cell><cell>99.58</cell><cell>97.12</cell></row><row><cell>4</cell><cell>37.07</cell><cell>37.50</cell><cell>88.95</cell><cell>100.0</cell><cell>98.57</cell><cell>100.0</cell></row></table><note><p>Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_33"><head>Table 8 .</head><label>8</label><figDesc>The categorized results for the UP dataset with 0.5% training samples.</figDesc><table><row><cell>Remote Sens. 2020, 12, 582</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>18 of 25</cell></row><row><cell>Class</cell><cell>Color</cell><cell>SVM</cell><cell>CDCNN</cell><cell>SSRN</cell><cell>FDSSC</cell><cell>DBMA</cell><cell>Proposed</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_34"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table><row><cell cols="2">Remote Sens. 2020, 12, 582</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>18 of 25</cell></row><row><cell cols="8">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell></cell><cell>1</cell><cell>99.85 82.87</cell><cell>0.00 85.74</cell><cell>100.0</cell><cell>100.0 99.15</cell><cell>100.0 96.88</cell><cell>100.0 94.86</cell><cell>89.03</cell></row><row><cell>2</cell><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell></cell><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell></cell><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell></cell><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell></cell><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell></cell><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell></cell><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell></cell><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell></cell><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell></cell><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell></cell><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell></cell><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell></cell><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell></cell><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell></cell><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell></cell><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell cols="2">kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_35"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table><row><cell cols="2">Remote Sens. 2020, 12, 582</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>18 of 25</cell></row><row><cell cols="8">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell></cell><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>2</cell><cell>98.95 88.07</cell><cell>64.82 94.45</cell><cell>100.0</cell><cell>100.0 98.06</cell><cell>99.51 97.57</cell><cell>99.17 96.57</cell><cell>98.32</cell></row><row><cell>3</cell><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell></cell><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell></cell><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell></cell><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell></cell><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell></cell><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell></cell><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell></cell><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell></cell><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell></cell><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell></cell><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell></cell><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell></cell><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell></cell><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell></cell><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell cols="2">kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_36"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table><row><cell cols="2">Remote Sens. 2020, 12, 582</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>18 of 25</cell></row><row><cell cols="5">Class Color SVM CDCNN SSRN</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell></cell><cell>3</cell><cell>89.88 70.84</cell><cell>94.69 32.59</cell><cell>89.72</cell><cell>99.44 96.64</cell><cell>98.92 89.97</cell><cell>97.74 100.0</cell><cell>98.70</cell></row><row><cell>4</cell><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell></cell><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell></cell><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell></cell><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell></cell><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell></cell><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell></cell><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell></cell><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell></cell><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell></cell><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell></cell><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell></cell><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell></cell><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell></cell><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell cols="2">kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_37"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table><row><cell cols="2">Remote Sens. 2020, 12, 582</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>18 of 25</cell></row><row><cell cols="8">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell></cell><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell></cell><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell></cell><cell>4</cell><cell>97.30 95.61</cell><cell>82.99 97.46</cell><cell>94.85</cell><cell>98.57 99.86</cell><cell>96.39 99.21</cell><cell>95.95 97.44</cell><cell>98.42</cell></row><row><cell>5</cell><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell></cell><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell></cell><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell></cell><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell></cell><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell></cell><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell></cell><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell></cell><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell></cell><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell></cell><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell></cell><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell></cell><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell></cell><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell cols="2">kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_38"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table><row><cell cols="8">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell></cell><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell></cell><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell></cell><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell></cell><cell>5</cell><cell>93.56 92.24</cell><cell>98.24 99.10</cell><cell>99.39</cell><cell>99.87 99.85</cell><cell>96.39 99.55</cell><cell>99.29 95.69</cell><cell>99.78</cell></row><row><cell>6</cell><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell></cell><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell></cell><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell></cell><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell></cell><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell></cell><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell></cell><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell></cell><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell></cell><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell></cell><cell>15</cell><cell cols="2">75.86 Remote Sens. 2020, 12, 582 51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell></cell><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell></cell><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell cols="2">kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_39"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset</figDesc><table><row><cell cols="5">4.3.4. Classification Maps and Categorized Result for the BS</cell></row><row><cell cols="2">0.9412 0</cell><cell>0.7547</cell><cell>0.8671</cell><cell>kappa</cell></row><row><cell>9</cell><cell>96.66</cell><cell>79.86</cell><cell>91.45</cell><cell>AA</cell></row><row><cell>9</cell><cell>94.72</cell><cell>77.79</cell><cell>88.09</cell><cell>OA</cell></row><row><cell>1</cell><cell>100.0</cell><cell>99.62</cell><cell>99.03</cell><cell>16</cell></row><row><cell>7</cell><cell>89.96</cell><cell>51.88</cell><cell>75.86</cell><cell>15</cell></row><row><cell>9</cell><cell>97.86</cell><cell>94.92</cell><cell>89.26</cell><cell>14</cell></row><row><cell>1</cell><cell>100.0</cell><cell>95.71</cell><cell>94.87</cell><cell>13</cell></row><row><cell>9</cell><cell>99.95</cell><cell>75.49</cell><cell>95.19</cell><cell>12</cell></row><row><cell>9</cell><cell>93.30</cell><cell>72.03</cell><cell>75.92</cell><cell>11</cell></row><row><cell>9</cell><cell>98.81</cell><cell>67.39</cell><cell>90.01</cell><cell>10</cell></row><row><cell>9</cell><cell>98.48</cell><cell>99.26</cell><cell>97.69</cell><cell>9</cell></row><row><cell>9</cell><cell>88.60</cell><cell>88.23</cell><cell>74.73</cell><cell>8</cell></row><row><cell>9</cell><cell>99.75</cell><cell>95.98</cell><cell>91.33</cell><cell>7</cell></row><row><cell>9</cell><cell>99.95</cell><cell>96.51</cell><cell>99.89</cell><cell>6</cell></row><row><cell>9</cell><cell>99.39</cell><cell>98.24</cell><cell>93.56</cell><cell>5</cell></row><row><cell>9</cell><cell>94.85</cell><cell>82.99</cell><cell>97.30</cell><cell>4</cell></row><row><cell>9</cell><cell>89.72</cell><cell>94.69</cell><cell>89.88</cell><cell>3</cell></row><row><cell>1</cell><cell>100.0</cell><cell>64.82</cell><cell>98.95</cell><cell>2</cell></row><row><cell>1</cell><cell>100.0</cell><cell>0.00</cell><cell>99.85</cell><cell>1</cell></row><row><cell cols="5">Class Color SVM CDCNN SSRN FD</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_40"><head>Table 10 .</head><label>10</label><figDesc>The categorized results for the BS dataset</figDesc><table><row><cell cols="2">0.9121 0</cell><cell>0.7930</cell><cell>0.7532</cell><cell>kappa</cell></row><row><cell>9</cell><cell>93.92</cell><cell>81.92</cell><cell>79.93</cell><cell>AA</cell></row><row><cell>9</cell><cell>91.89</cell><cell>80.90</cell><cell>77.21</cell><cell>OA</cell></row><row><cell>1</cell><cell>100.0</cell><cell>100.0</cell><cell>92.98</cell><cell>14</cell></row><row><cell>8</cell><cell>94.83</cell><cell>88.59</cell><cell>73.62</cell><cell>13</cell></row><row><cell>1</cell><cell>100.0</cell><cell>90.91</cell><cell>90.70</cell><cell>12</cell></row><row><cell>9</cell><cell>100.0</cell><cell>92.48</cell><cell>93.91</cell><cell>11</cell></row><row><cell>9</cell><cell>86.83</cell><cell>81.69</cell><cell>65.74</cell><cell>10</cell></row><row><cell>8</cell><cell>90.75</cell><cell>55.53</cell><cell>63.53</cell><cell>9</cell></row><row><cell>9</cell><cell>100.0</cell><cell>89.36</cell><cell>63.46</cell><cell>8</cell></row><row><cell>8</cell><cell>100.0</cell><cell>80.07</cell><cell>82.09</cell><cell>7</cell></row><row><cell>8</cell><cell>66.39</cell><cell>69.28</cell><cell>61.27</cell><cell>6</cell></row><row><cell>8</cell><cell>92.42</cell><cell>89.10</cell><cell>84.33</cell><cell>5</cell></row><row><cell>9</cell><cell>97.34</cell><cell>65.45</cell><cell>63.51</cell><cell>4</cell></row><row><cell>1</cell><cell>91.42</cell><cell>81.11</cell><cell>86.35</cell><cell>3</cell></row><row><cell>9</cell><cell>100.0</cell><cell>68.64</cell><cell>97.56</cell><cell>2</cell></row><row><cell>9</cell><cell>94.95</cell><cell>94.60</cell><cell>100.0</cell><cell>1</cell></row><row><cell cols="5">Class Color SVM CDCNN SSRN FD</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_41"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table><row><cell cols="2">Remote Sens. 2020, 12, 582</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>18 of 25</cell></row><row><cell cols="8">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell></cell><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell></cell><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell></cell><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell></cell><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell></cell><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell></cell><cell>7</cell><cell>91.33 68.98</cell><cell>95.98 88.83</cell><cell>99.75</cell><cell>99.75 73.24</cell><cell>96.80 100.0</cell><cell>99.83 95.69</cell><cell>95.84</cell></row><row><cell>8</cell><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell></cell><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell></cell><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell></cell><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell></cell><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell></cell><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell></cell><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell></cell><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell></cell><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell></cell><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell cols="2">kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p><p><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p>The categorized results using different methods for the BS dataset are demonstrated in Table</p>10</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_42"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table><row><cell cols="2">Remote Sens. 2020, 12, 582</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>18 of 25</cell></row><row><cell cols="8">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell></cell><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell></cell><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell></cell><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell></cell><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell></cell><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell></cell><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell></cell><cell>8</cell><cell>74.73 71.14</cell><cell>88.23 66.19</cell><cell>88.60</cell><cell>99.60 82.36</cell><cell>95.60 70.97</cell><cell>95.97 78.93</cell><cell>89.47</cell></row><row><cell>9</cell><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell></cell><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell></cell><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell></cell><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell></cell><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell></cell><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell></cell><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell></cell><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell></cell><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell cols="2">kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p><p><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p>The categorized results using different methods for the BS dataset are demonstrated in Table</p>10</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_43"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table><row><cell cols="8">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell></cell></row><row><cell></cell><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell><cell></cell></row><row><cell></cell><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell><cell></cell></row><row><cell></cell><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell><cell></cell></row><row><cell></cell><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell><cell></cell></row><row><cell></cell><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell><cell></cell></row><row><cell></cell><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell><cell></cell></row><row><cell></cell><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell><cell></cell></row><row><cell></cell><cell>9</cell><cell>97.69 99.89</cell><cell>99.26 96.01</cell><cell>98.48</cell><cell>99.69 100.0</cell><cell>99.22 100.0</cell><cell>99.37 99.55</cell><cell>99.89</cell></row><row><cell>OA</cell><cell>10</cell><cell>90.01 84.29</cell><cell>67.39 87.70</cell><cell>98.81</cell><cell>99.02 95.59</cell><cell>96.20 94.43</cell><cell>96.72 94.72</cell><cell>96.00</cell></row><row><cell>AA</cell><cell>11</cell><cell>75.92 82.96</cell><cell>72.03 82.36</cell><cell>93.30</cell><cell>92.77 94.01</cell><cell>82.29 94.68</cell><cell>93.72 95.49</cell><cell>96.45</cell></row><row><cell cols="2">12 kappa</cell><cell>95.19 0.7883</cell><cell>75.49 0.8359</cell><cell>99.95</cell><cell>99.64 0.9415</cell><cell>99.17 0.9257</cell><cell>100.0 0.9295</cell><cell>0.9467</cell></row><row><cell></cell><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell><cell></cell></row><row><cell></cell><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell><cell></cell></row><row><cell></cell><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell><cell></cell></row><row><cell></cell><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell></cell></row><row><cell cols="2">OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell><cell></cell></row><row><cell cols="2">AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell><cell></cell></row><row><cell cols="2">kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell><cell></cell></row></table><note><p><p><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p>The categorized results using different methods for the BS dataset are demonstrated in Table</p>10</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_44"><head>Table 8 .</head><label>8</label><figDesc>The categorized results for the UP dataset with 0.5% training samples.</figDesc><table><row><cell cols="7">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell>1</cell><cell>82.87</cell><cell>85.74</cell><cell>99.15</cell><cell>96.88</cell><cell>94.86</cell><cell>89.03</cell></row><row><cell>2</cell><cell>88.07</cell><cell>94.45</cell><cell>98.06</cell><cell>97.57</cell><cell>96.57</cell><cell>98.32</cell></row><row><cell>3</cell><cell>70.84</cell><cell>32.59</cell><cell>96.64</cell><cell>89.97</cell><cell>100.0</cell><cell>98.70</cell></row><row><cell>4</cell><cell>95.61</cell><cell>97.46</cell><cell>99.86</cell><cell>99.21</cell><cell>97.44</cell><cell>98.42</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_45"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table><row><cell>Class</cell><cell>Color</cell><cell>SVM</cell><cell>CDCNN</cell><cell>SSRN</cell><cell>FDSSC</cell><cell>DBMA</cell><cell>Proposed</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>Remote Sens. 2020, 12, 582 18 of 25</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_46"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_47"><head>Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</head><label></label><figDesc></figDesc><table><row><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell>kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_48"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_49"><head>Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>2</cell><cell>98.95 98.95</cell><cell>64.82 64.82</cell><cell>100.0</cell><cell>100.0 100.0</cell><cell>99.51 100.0</cell><cell>99.17 99.51</cell><cell>99.17</cell></row><row><cell>3</cell><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell></cell><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell></cell><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell></cell><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell></cell><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell></cell><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell></cell><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell></cell><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell></cell><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell></cell><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell></cell><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell></cell><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell></cell><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell></cell><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell></cell><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell cols="2">kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p><p><p><p><p><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p>The categorized results using different methods for the BS dataset are demonstrated in Table</p>10</p>where the best class-specific accuracy is in bold, and classification maps of the different methods and ground truth are shown in Figure</p>12</p>. Remote Sens. 2020, 12, 582 18 of 25</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_50"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_51"><head>Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell></cell><cell>3</cell><cell>89.88 89.88</cell><cell>94.69 94.69</cell><cell>89.72</cell><cell>99.44 89.72</cell><cell>98.92 99.44</cell><cell>97.74 98.92</cell><cell>97.74</cell></row><row><cell>4</cell><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell></cell><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell></cell><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell></cell><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell></cell><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell></cell><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell></cell><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell></cell><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell></cell><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell></cell><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell></cell><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell></cell><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell></cell><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell></cell><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell cols="2">kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p><p><p><p><p><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p>The categorized results using different methods for the BS dataset are demonstrated in Table</p>10</p>where the best class-specific accuracy is in bold, and classification maps of the different methods and ground truth are shown in Figure</p>12</p>. Remote Sens. 2020, 12, 582 18 of 25</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_52"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_53"><head>Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell></cell><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell></cell><cell>4</cell><cell>97.30 97.30</cell><cell>82.99 82.99</cell><cell>94.85</cell><cell>98.57 94.85</cell><cell>96.39 98.57</cell><cell>95.95 96.39</cell><cell>95.95</cell></row><row><cell>5</cell><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell></cell><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell></cell><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell></cell><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell></cell><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell></cell><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell></cell><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell></cell><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell></cell><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell></cell><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell></cell><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell></cell><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell></cell><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell cols="2">kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p><p><p><p><p><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p>The categorized results using different methods for the BS dataset are demonstrated in Table</p>10</p>where the best class-specific accuracy is in bold, and classification maps of the different methods and ground truth are shown in Figure</p>12</p>. Remote Sens. 2020, 12, 582 18 of 25</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_54"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_55"><head>Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell></cell><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell></cell><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell></cell><cell>5</cell><cell>93.56 93.56</cell><cell>98.24 98.24</cell><cell>99.39</cell><cell>99.87 99.39</cell><cell>96.39 99.87</cell><cell>99.29 96.39</cell><cell>99.29</cell></row><row><cell>6</cell><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell></cell><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell></cell><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell></cell><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell></cell><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell></cell><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell></cell><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell></cell><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell></cell><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell></cell><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell></cell><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell></cell><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell cols="2">kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p><p><p><p><p><p><p><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p>The categorized results using different methods for the BS dataset are demonstrated in Table</p>10</p>where the best class-specific accuracy is in bold, and classification maps of the different methods and ground truth are shown in Figure</p>12</p>.</p>Remote Sens. 2020,</p><ref type="bibr" target="#b11">12,</ref> 582    </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_56"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset wit</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>4.3.</cell></row><row><cell cols="2">0.9412 0.944</cell><cell>0.7547</cell><cell>0.8671</cell><cell>kappa</cell></row><row><cell>97.5</cell><cell>96.66</cell><cell>79.86</cell><cell>91.45</cell><cell>AA</cell></row><row><cell>94.9</cell><cell>94.72</cell><cell>77.79</cell><cell>88.09</cell><cell>OA</cell></row><row><cell>100.</cell><cell>100.0</cell><cell>99.62</cell><cell>99.03</cell><cell>16</cell></row><row><cell>74.5</cell><cell>89.96</cell><cell>51.88</cell><cell>75.86</cell><cell>15</cell></row><row><cell>98.0</cell><cell>97.86</cell><cell>94.92</cell><cell>89.26</cell><cell>14</cell></row><row><cell>100.</cell><cell>100.0</cell><cell>95.71</cell><cell>94.87</cell><cell>13</cell></row><row><cell>99.6</cell><cell>99.95</cell><cell>75.49</cell><cell>95.19</cell><cell>12</cell></row><row><cell>92.7</cell><cell>93.30</cell><cell>72.03</cell><cell>75.92</cell><cell>11</cell></row><row><cell>99.0</cell><cell>98.81</cell><cell>67.39</cell><cell>90.01</cell><cell>10</cell></row><row><cell>99.6</cell><cell>98.48</cell><cell>99.26</cell><cell>97.69</cell><cell>9</cell></row><row><cell>99.6</cell><cell>88.60</cell><cell>88.23</cell><cell>74.73</cell><cell>8</cell></row><row><cell>99.7</cell><cell>99.75</cell><cell>95.98</cell><cell>91.33</cell><cell>7</cell></row><row><cell>99.9</cell><cell>99.95</cell><cell>96.51</cell><cell>99.89</cell><cell>6</cell></row><row><cell>99.8</cell><cell>99.39</cell><cell>98.24</cell><cell>93.56</cell><cell>5</cell></row><row><cell>98.5</cell><cell>94.85</cell><cell>82.99</cell><cell>97.30</cell><cell>4</cell></row><row><cell>99.4</cell><cell>89.72</cell><cell>94.69</cell><cell>89.88</cell><cell>3</cell></row><row><cell>100.</cell><cell>100.0</cell><cell>64.82</cell><cell>98.95</cell><cell>2</cell></row><row><cell>100.</cell><cell>100.0</cell><cell>0.00</cell><cell>99.85</cell><cell>1</cell></row><row><cell cols="5">Class Color SVM CDCNN SSRN FDSS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_57"><head>Table 10 .</head><label>10</label><figDesc>The categorized results for the BS dataset wit</figDesc><table><row><cell cols="2">0.9121 0.908</cell><cell>0.7930</cell><cell>0.7532</cell><cell>kappa</cell></row><row><cell>93.0</cell><cell>93.92</cell><cell>81.92</cell><cell>79.93</cell><cell>AA</cell></row><row><cell>91.5</cell><cell>91.89</cell><cell>80.90</cell><cell>77.21</cell><cell>OA</cell></row><row><cell>100.</cell><cell>100.0</cell><cell>100.0</cell><cell>92.98</cell><cell>14</cell></row><row><cell>81.5</cell><cell>94.83</cell><cell>88.59</cell><cell>73.62</cell><cell>13</cell></row><row><cell>100.</cell><cell>100.0</cell><cell>90.91</cell><cell>90.70</cell><cell>12</cell></row><row><cell>99.6</cell><cell>100.0</cell><cell>92.48</cell><cell>93.91</cell><cell>11</cell></row><row><cell>99.5</cell><cell>86.83</cell><cell>81.69</cell><cell>65.74</cell><cell>10</cell></row><row><cell>88.4</cell><cell>90.75</cell><cell>55.53</cell><cell>63.53</cell><cell>9</cell></row><row><cell>93.3</cell><cell>100.0</cell><cell>89.36</cell><cell>63.46</cell><cell>8</cell></row><row><cell>84.6</cell><cell>100.0</cell><cell>80.07</cell><cell>82.09</cell><cell>7</cell></row><row><cell>84.9</cell><cell>66.39</cell><cell>69.28</cell><cell>61.27</cell><cell>6</cell></row><row><cell>80.7</cell><cell>92.42</cell><cell>89.10</cell><cell>84.33</cell><cell>5</cell></row><row><cell>93.0</cell><cell>97.34</cell><cell>65.45</cell><cell>63.51</cell><cell>4</cell></row><row><cell>100.</cell><cell>91.42</cell><cell>81.11</cell><cell>86.35</cell><cell>3</cell></row><row><cell>98.9</cell><cell>100.0</cell><cell>68.64</cell><cell>97.56</cell><cell>2</cell></row><row><cell>97.4</cell><cell>94.95</cell><cell>94.60</cell><cell>100.0</cell><cell>1</cell></row><row><cell cols="5">Class Color SVM CDCNN SSRN FDSS</cell></row></table><note><p>Remote Sens. 2020, 12, 582 18 of 25</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_58"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_59"><head>Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell></cell><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell></cell><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell></cell><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell></cell><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell></cell><cell>7</cell><cell>91.33 91.33</cell><cell>95.98 95.98</cell><cell>99.75</cell><cell>99.75 99.75</cell><cell>96.80 99.75</cell><cell>99.83 96.80</cell><cell>99.83</cell></row><row><cell>8</cell><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell></cell><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell></cell><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell></cell><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell></cell><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell></cell><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell></cell><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell></cell><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell></cell><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell></cell><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell cols="2">kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p><p><p><p><p><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p>The categorized results using different methods for the BS dataset are demonstrated in Table</p>10</p>where the best class-specific accuracy is in bold, and classification maps of the different methods and ground truth are shown in Figure</p>12</p>. Remote Sens. 2020, 12, 582 18 of 25</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_60"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_61"><head>Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell></cell><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell></cell><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell></cell><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell></cell><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell></cell><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell></cell><cell>8</cell><cell>74.73 74.73</cell><cell>88.23 88.23</cell><cell>88.60</cell><cell>99.60 88.60</cell><cell>95.60 99.60</cell><cell>95.97 95.60</cell><cell>95.97</cell></row><row><cell>9</cell><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell></cell><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell></cell><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell></cell><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell></cell><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell></cell><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell></cell><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell></cell><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell></cell><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell cols="2">kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p><p><p><p><p><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p>The categorized results using different methods for the BS dataset are demonstrated in Table</p>10</p>where the best class-specific accuracy is in bold, and classification maps of the different methods and ground truth are shown in Figure</p>12</p>. Remote Sens. 2020, 12, 582 18 of 25</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_62"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_63"><head>Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell></cell><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell></cell><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell></cell><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell></cell><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell></cell><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell></cell><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell></cell><cell>9</cell><cell>97.69 97.69</cell><cell>99.26 99.26</cell><cell>98.48</cell><cell>99.69 98.48</cell><cell>99.22 99.69</cell><cell>99.37 99.22</cell><cell>99.37</cell></row><row><cell>10</cell><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell></cell><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell></cell><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell></cell><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell></cell><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell></cell><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell></cell><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell></cell><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell cols="2">kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p><p><p><p><p><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p>The categorized results using different methods for the BS dataset are demonstrated in Table</p>10</p>where the best class-specific accuracy is in bold, and classification maps of the different methods and ground truth are shown in Figure</p>12</p>. Remote Sens. 2020, 12, 582 18 of 25</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_64"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_65"><head>Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</head><label></label><figDesc></figDesc><table><row><cell cols="2">Remote Sens. 2020, 12, 582</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>15 of 25</cell></row><row><cell></cell><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell></cell><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell></cell><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell></cell><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell></cell><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell></cell><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell></cell><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell></cell><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell></cell><cell>10</cell><cell>90.01 90.01</cell><cell>67.39 67.39</cell><cell>98.81</cell><cell>99.02 98.81</cell><cell>96.20 99.02</cell><cell>96.72 96.20</cell><cell>96.72</cell></row><row><cell>11</cell><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell></cell><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell></cell><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell></cell><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell></cell><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell></cell><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell></cell><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell cols="2">kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p><p><p><p><p><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p>The categorized results using different methods for the BS dataset are demonstrated in Table</p>10</p>where the best class-specific accuracy is in bold, and classification maps of the different methods and ground truth are shown in Figure</p>12</p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_66"><head>Table 7 .</head><label>7</label><figDesc>The categorized results for the IP dataset with 3% training samples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_67"><head>Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</head><label></label><figDesc></figDesc><table><row><cell cols="2">Remote Sens. 2020, 12, 582</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>15 of 25</cell></row><row><cell></cell><cell>1</cell><cell>24.24</cell><cell>0.00</cell><cell>100.0</cell><cell>85.42</cell><cell>93.48</cell><cell>100.0</cell></row><row><cell></cell><cell>2</cell><cell>58.10</cell><cell>62.36</cell><cell>89.14</cell><cell>97.20</cell><cell>91.15</cell><cell>88.49</cell></row><row><cell></cell><cell>3</cell><cell>64.37</cell><cell>57.00</cell><cell>77.49</cell><cell>94.45</cell><cell>99.58</cell><cell>97.12</cell></row><row><cell></cell><cell>4</cell><cell>37.07</cell><cell>37.50</cell><cell>88.95</cell><cell>100.0</cell><cell>98.57</cell><cell>100.0</cell></row><row><cell></cell><cell>5</cell><cell>87.67</cell><cell>88.16</cell><cell>96.48</cell><cell>100.0</cell><cell>97.45</cell><cell>100.0</cell></row><row><cell></cell><cell>6</cell><cell>84.02</cell><cell>79.63</cell><cell>98.15</cell><cell>100.0</cell><cell>95.66</cell><cell>97.18</cell></row><row><cell></cell><cell>7</cell><cell>56.10</cell><cell>0.00</cell><cell>0.00</cell><cell>73.53</cell><cell>40.00</cell><cell>92.59</cell></row><row><cell></cell><cell>8</cell><cell>89.62</cell><cell>84.02</cell><cell>84.54</cell><cell>99.78</cell><cell>100.0</cell><cell>99.78</cell></row><row><cell></cell><cell>9</cell><cell>21.21</cell><cell>0.00</cell><cell>0.00</cell><cell>100.0</cell><cell>38.10</cell><cell>100.0</cell></row><row><cell></cell><cell>10</cell><cell>65.89</cell><cell>37.50</cell><cell>92.07</cell><cell>89.25</cell><cell>85.98</cell><cell>89.87</cell></row><row><cell></cell><cell>11</cell><cell>62.32 75.92</cell><cell>53.25 72.03</cell><cell>90.89</cell><cell>93.97 93.30</cell><cell>94.39 92.77</cell><cell>99.33 82.29</cell><cell>93.72</cell></row><row><cell>12</cell><cell>12</cell><cell>52.40</cell><cell>42.96</cell><cell>84.19</cell><cell>95.41</cell><cell>89.92</cell><cell>98.50</cell></row><row><cell></cell><cell>13</cell><cell>94.30</cell><cell>49.47</cell><cell>98.47</cell><cell>100.0</cell><cell>99.48</cell><cell>96.02</cell></row><row><cell></cell><cell>14</cell><cell>90.15</cell><cell>76.71</cell><cell>94.56</cell><cell>93.14</cell><cell>92.81</cell><cell>93.22</cell></row><row><cell></cell><cell>15</cell><cell>63.96</cell><cell>62.60</cell><cell>84.11</cell><cell>90.61</cell><cell>89.66</cell><cell>96.99</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_68"><head>Table 7 .</head><label>7</label><figDesc>The categorized results for the IP dataset with 3% training samples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_69"><head>Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</head><label></label><figDesc></figDesc><table><row><cell cols="2">Remote Sens. 2020, 12, 582</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>15 of 25</cell></row><row><cell></cell><cell>1</cell><cell>24.24</cell><cell>0.00</cell><cell>100.0</cell><cell>85.42</cell><cell>93.48</cell><cell>100.0</cell></row><row><cell></cell><cell>2</cell><cell>58.10</cell><cell>62.36</cell><cell>89.14</cell><cell>97.20</cell><cell>91.15</cell><cell>88.49</cell></row><row><cell></cell><cell>3</cell><cell>64.37</cell><cell>57.00</cell><cell>77.49</cell><cell>94.45</cell><cell>99.58</cell><cell>97.12</cell></row><row><cell></cell><cell>4</cell><cell>37.07</cell><cell>37.50</cell><cell>88.95</cell><cell>100.0</cell><cell>98.57</cell><cell>100.0</cell></row><row><cell></cell><cell>5</cell><cell>87.67</cell><cell>88.16</cell><cell>96.48</cell><cell>100.0</cell><cell>97.45</cell><cell>100.0</cell></row><row><cell></cell><cell>6</cell><cell>84.02</cell><cell>79.63</cell><cell>98.15</cell><cell>100.0</cell><cell>95.66</cell><cell>97.18</cell></row><row><cell></cell><cell>7</cell><cell>56.10</cell><cell>0.00</cell><cell>0.00</cell><cell>73.53</cell><cell>40.00</cell><cell>92.59</cell></row><row><cell></cell><cell>8</cell><cell>89.62</cell><cell>84.02</cell><cell>84.54</cell><cell>99.78</cell><cell>100.0</cell><cell>99.78</cell></row><row><cell></cell><cell>9</cell><cell>21.21</cell><cell>0.00</cell><cell>0.00</cell><cell>100.0</cell><cell>38.10</cell><cell>100.0</cell></row><row><cell></cell><cell>10</cell><cell>65.89</cell><cell>37.50</cell><cell>92.07</cell><cell>89.25</cell><cell>85.98</cell><cell>89.87</cell></row><row><cell></cell><cell>11</cell><cell>62.32</cell><cell>53.25</cell><cell>90.89</cell><cell>93.97</cell><cell>94.39</cell><cell>99.33</cell></row><row><cell></cell><cell>12</cell><cell>52.40 95.19</cell><cell>42.96 75.49</cell><cell>84.19</cell><cell>95.41 99.95</cell><cell>89.92 99.64</cell><cell>98.50 99.17</cell><cell>100.0</cell></row><row><cell>13</cell><cell>13</cell><cell>94.30</cell><cell>49.47</cell><cell>98.47</cell><cell>100.0</cell><cell>99.48</cell><cell>96.02</cell></row><row><cell></cell><cell>14</cell><cell>90.15</cell><cell>76.71</cell><cell>94.56</cell><cell>93.14</cell><cell>92.81</cell><cell>93.22</cell></row><row><cell></cell><cell>15</cell><cell>63.96</cell><cell>62.60</cell><cell>84.11</cell><cell>90.61</cell><cell>89.66</cell><cell>96.99</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_70"><head>Table 7 .</head><label>7</label><figDesc>The categorized results for the IP dataset with 3% training samples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_71"><head>Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</head><label></label><figDesc></figDesc><table><row><cell cols="2">Remote Sens. 2020, 12, 582</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>15 of 25</cell></row><row><cell></cell><cell>1</cell><cell>24.24</cell><cell>0.00</cell><cell>100.0</cell><cell>85.42</cell><cell>93.48</cell><cell>100.0</cell></row><row><cell></cell><cell>2</cell><cell>58.10</cell><cell>62.36</cell><cell>89.14</cell><cell>97.20</cell><cell>91.15</cell><cell>88.49</cell></row><row><cell></cell><cell>3</cell><cell>64.37</cell><cell>57.00</cell><cell>77.49</cell><cell>94.45</cell><cell>99.58</cell><cell>97.12</cell></row><row><cell></cell><cell>4</cell><cell>37.07</cell><cell>37.50</cell><cell>88.95</cell><cell>100.0</cell><cell>98.57</cell><cell>100.0</cell></row><row><cell></cell><cell>5</cell><cell>87.67</cell><cell>88.16</cell><cell>96.48</cell><cell>100.0</cell><cell>97.45</cell><cell>100.0</cell></row><row><cell></cell><cell>6</cell><cell>84.02</cell><cell>79.63</cell><cell>98.15</cell><cell>100.0</cell><cell>95.66</cell><cell>97.18</cell></row><row><cell></cell><cell>7</cell><cell>56.10</cell><cell>0.00</cell><cell>0.00</cell><cell>73.53</cell><cell>40.00</cell><cell>92.59</cell></row><row><cell></cell><cell>8</cell><cell>89.62</cell><cell>84.02</cell><cell>84.54</cell><cell>99.78</cell><cell>100.0</cell><cell>99.78</cell></row><row><cell></cell><cell>9</cell><cell>21.21</cell><cell>0.00</cell><cell>0.00</cell><cell>100.0</cell><cell>38.10</cell><cell>100.0</cell></row><row><cell></cell><cell>10</cell><cell>65.89</cell><cell>37.50</cell><cell>92.07</cell><cell>89.25</cell><cell>85.98</cell><cell>89.87</cell></row><row><cell></cell><cell>11</cell><cell>62.32</cell><cell>53.25</cell><cell>90.89</cell><cell>93.97</cell><cell>94.39</cell><cell>99.33</cell></row><row><cell></cell><cell>12</cell><cell>52.40</cell><cell>42.96</cell><cell>84.19</cell><cell>95.41</cell><cell>89.92</cell><cell>98.50</cell></row><row><cell></cell><cell>13</cell><cell>94.30 94.87</cell><cell>49.47 95.71</cell><cell>98.47</cell><cell>100.0 100.0</cell><cell>99.48 100.0</cell><cell>96.02 98.91</cell><cell>100.0</cell></row><row><cell>14</cell><cell>14</cell><cell>90.15</cell><cell>76.71</cell><cell>94.56</cell><cell>93.14</cell><cell>92.81</cell><cell>93.22</cell></row><row><cell></cell><cell>15</cell><cell>63.96</cell><cell>62.60</cell><cell>84.11</cell><cell>90.61</cell><cell>89.66</cell><cell>96.99</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_72"><head>Table 7 .</head><label>7</label><figDesc>The categorized results for the IP dataset with 3% training samples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_73"><head>Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</head><label></label><figDesc></figDesc><table><row><cell cols="2">Remote Sens. 2020, 12, 582</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>15 of 25</cell></row><row><cell></cell><cell>1</cell><cell>24.24</cell><cell>0.00</cell><cell>100.0</cell><cell>85.42</cell><cell>93.48</cell><cell>100.0</cell></row><row><cell></cell><cell>2</cell><cell>58.10</cell><cell>62.36</cell><cell>89.14</cell><cell>97.20</cell><cell>91.15</cell><cell>88.49</cell></row><row><cell></cell><cell>3</cell><cell>64.37</cell><cell>57.00</cell><cell>77.49</cell><cell>94.45</cell><cell>99.58</cell><cell>97.12</cell></row><row><cell></cell><cell>4</cell><cell>37.07</cell><cell>37.50</cell><cell>88.95</cell><cell>100.0</cell><cell>98.57</cell><cell>100.0</cell></row><row><cell></cell><cell>5</cell><cell>87.67</cell><cell>88.16</cell><cell>96.48</cell><cell>100.0</cell><cell>97.45</cell><cell>100.0</cell></row><row><cell></cell><cell>6</cell><cell>84.02</cell><cell>79.63</cell><cell>98.15</cell><cell>100.0</cell><cell>95.66</cell><cell>97.18</cell></row><row><cell></cell><cell>7</cell><cell>56.10</cell><cell>0.00</cell><cell>0.00</cell><cell>73.53</cell><cell>40.00</cell><cell>92.59</cell></row><row><cell></cell><cell>8</cell><cell>89.62</cell><cell>84.02</cell><cell>84.54</cell><cell>99.78</cell><cell>100.0</cell><cell>99.78</cell></row><row><cell></cell><cell>9</cell><cell>21.21</cell><cell>0.00</cell><cell>0.00</cell><cell>100.0</cell><cell>38.10</cell><cell>100.0</cell></row><row><cell></cell><cell>10</cell><cell>65.89</cell><cell>37.50</cell><cell>92.07</cell><cell>89.25</cell><cell>85.98</cell><cell>89.87</cell></row><row><cell></cell><cell>11</cell><cell>62.32</cell><cell>53.25</cell><cell>90.89</cell><cell>93.97</cell><cell>94.39</cell><cell>99.33</cell></row><row><cell></cell><cell>12</cell><cell>52.40</cell><cell>42.96</cell><cell>84.19</cell><cell>95.41</cell><cell>89.92</cell><cell>98.50</cell></row><row><cell></cell><cell>13</cell><cell>94.30</cell><cell>49.47</cell><cell>98.47</cell><cell>100.0</cell><cell>99.48</cell><cell>96.02</cell></row><row><cell></cell><cell>14</cell><cell>90.15 89.26</cell><cell>76.71 94.92</cell><cell>94.56</cell><cell>93.14 97.86</cell><cell>92.81 98.05</cell><cell>93.22 98.22</cell><cell>96.89</cell></row><row><cell>15</cell><cell>15</cell><cell>63.96</cell><cell>62.60</cell><cell>84.11</cell><cell>90.61</cell><cell>89.66</cell><cell>96.99</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_74"><head>Table 7 .</head><label>7</label><figDesc>The categorized results for the IP dataset with 3% training samples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_75"><head>Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</head><label></label><figDesc>where the best class-specific accuracy is in bold, and classification maps for the different methods and ground truth are shown in Figure10.</figDesc><table><row><cell></cell><cell>1</cell><cell>24.24</cell><cell>0.00</cell><cell>100.0</cell><cell>85.42</cell><cell>93.48</cell><cell>100.0</cell><cell></cell></row><row><cell></cell><cell>2</cell><cell>58.10</cell><cell>62.36</cell><cell>89.14</cell><cell>97.20</cell><cell>91.15</cell><cell>88.49</cell><cell></cell></row><row><cell></cell><cell>3</cell><cell>64.37</cell><cell>57.00</cell><cell>77.49</cell><cell>94.45</cell><cell>99.58</cell><cell>97.12</cell><cell></cell></row><row><cell></cell><cell>4</cell><cell>37.07</cell><cell>37.50</cell><cell>88.95</cell><cell>100.0</cell><cell>98.57</cell><cell>100.0</cell><cell></cell></row><row><cell></cell><cell>5</cell><cell>87.67</cell><cell>88.16</cell><cell>96.48</cell><cell>100.0</cell><cell>97.45</cell><cell>100.0</cell><cell></cell></row><row><cell></cell><cell>6</cell><cell>84.02</cell><cell>79.63</cell><cell>98.15</cell><cell>100.0</cell><cell>95.66</cell><cell>97.18</cell><cell></cell></row><row><cell></cell><cell>7</cell><cell>56.10</cell><cell>0.00</cell><cell>0.00</cell><cell>73.53</cell><cell>40.00</cell><cell>92.59</cell><cell></cell></row><row><cell></cell><cell>8</cell><cell>89.62</cell><cell>84.02</cell><cell>84.54</cell><cell>99.78</cell><cell>100.0</cell><cell>99.78</cell><cell></cell></row><row><cell></cell><cell>9</cell><cell>21.21</cell><cell>0.00</cell><cell>0.00</cell><cell>100.0</cell><cell>38.10</cell><cell>100.0</cell><cell></cell></row><row><cell></cell><cell>10</cell><cell>65.89</cell><cell>37.50</cell><cell>92.07</cell><cell>89.25</cell><cell>85.98</cell><cell>89.87</cell><cell></cell></row><row><cell></cell><cell>11</cell><cell>62.32</cell><cell>53.25</cell><cell>90.89</cell><cell>93.97</cell><cell>94.39</cell><cell>99.33</cell><cell></cell></row><row><cell></cell><cell>12</cell><cell>52.40</cell><cell>42.96</cell><cell>84.19</cell><cell>95.41</cell><cell>89.92</cell><cell>98.50</cell><cell></cell></row><row><cell></cell><cell>13</cell><cell>94.30</cell><cell>49.47</cell><cell>98.47</cell><cell>100.0</cell><cell>99.48</cell><cell>96.02</cell><cell></cell></row><row><cell cols="2">14 Remote Sens. 2020, 12, 582</cell><cell>90.15</cell><cell>76.71</cell><cell>94.56</cell><cell>93.14</cell><cell>92.81</cell><cell>93.22</cell><cell>16 of 25</cell></row><row><cell></cell><cell>15</cell><cell>63.96 75.86</cell><cell>62.60 51.88</cell><cell>84.11</cell><cell>90.61 89.96</cell><cell>89.66 74.58</cell><cell>96.99 84.71</cell><cell>93.42</cell></row><row><cell>16</cell><cell>16</cell><cell>98.46 99.03</cell><cell>83.70 99.62</cell><cell>91.40</cell><cell>96.55 100.0</cell><cell>96.55 100.0</cell><cell>94.38 100.0</cell><cell>100.0</cell></row><row><cell cols="2">OA OA</cell><cell>69.41 88.09</cell><cell>62.32 77.79</cell><cell>89.81</cell><cell>94.87 94.72</cell><cell>93.15 94.99</cell><cell>95.38 95.44</cell><cell>97.51</cell></row><row><cell cols="2">AA AA</cell><cell>65.62 91.45</cell><cell>50.93 79.86</cell><cell>79.40</cell><cell>94.33 96.66</cell><cell>87.67 97.56</cell><cell>96.47 96.34</cell><cell>98.00</cell></row><row><cell cols="2">kappa kappa</cell><cell>0.6472 0.8671</cell><cell>0.5593 0.7547</cell><cell cols="2">0.8839 0.9414 0.9412</cell><cell>0.9219 0.9444</cell><cell>0.9474 0.9493</cell><cell>0.9723</cell></row></table><note><p><p><p>4.3.2. Classification Maps and Categorized Result for the UP Dataset</p>The categorized results using different methods for the UP dataset are demonstrated in Table</p>8</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_76"><head>Table 10 .</head><label>10</label><figDesc>The categorized results for the BS dataset with 1.2% training samples.</figDesc><table><row><cell>Remote Sens. 2020, 12, 582</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>18 of 25</cell></row><row><cell>Class</cell><cell>Color</cell><cell>SVM</cell><cell>CDCNN</cell><cell>SSRN</cell><cell>FDSSC</cell><cell>DBMA</cell><cell>Proposed</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_77"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table><row><cell cols="7">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell>kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_78"><head>Table 10 .</head><label>10</label><figDesc>The categorized results for the BS dataset with 1.2% training samples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_79"><head>Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</head><label></label><figDesc></figDesc><table><row><cell>Remote Sens. 2020, 12, 582</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>18 of 25</cell></row><row><cell></cell><cell>100.0</cell><cell>94.60</cell><cell></cell><cell>94.95</cell><cell>97.41</cell><cell>97.77</cell><cell>95.64</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>100.0</cell><cell>94.60</cell><cell>94.95</cell><cell>97.41</cell><cell>97.77</cell><cell>95.64</cell></row><row><cell>2</cell><cell>97.56</cell><cell>68.64</cell><cell>100.0</cell><cell>98.95</cell><cell>88.89</cell><cell>98.99</cell></row><row><cell>3</cell><cell>86.35</cell><cell>81.11</cell><cell>91.42</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>4</cell><cell>63.51</cell><cell>65.45</cell><cell>97.34</cell><cell>93.03</cell><cell>92.51</cell><cell>91.30</cell></row><row><cell>5</cell><cell>84.33</cell><cell>89.10</cell><cell>92.42</cell><cell>80.74</cell><cell>93.51</cell><cell>95.58</cell></row><row><cell>6</cell><cell>61.27</cell><cell>69.28</cell><cell>66.39</cell><cell>84.93</cell><cell>68.94</cell><cell>82.23</cell></row><row><cell>7</cell><cell>82.09</cell><cell>80.07</cell><cell>100.0</cell><cell>84.62</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>8</cell><cell>63.46</cell><cell>89.36</cell><cell>100.0</cell><cell>93.36</cell><cell>96.10</cell><cell>95.63</cell></row><row><cell>9</cell><cell>63.53</cell><cell>55.53</cell><cell>90.75</cell><cell>88.44</cell><cell>85.15</cell><cell>96.50</cell></row><row><cell>10</cell><cell>65.74</cell><cell>81.69</cell><cell>86.83</cell><cell>99.59</cell><cell>97.60</cell><cell>98.79</cell></row><row><cell>11</cell><cell>93.91</cell><cell>92.48</cell><cell>100.0</cell><cell>99.67</cell><cell>99.66</cell><cell>99.67</cell></row><row><cell>12</cell><cell>90.70</cell><cell>90.91</cell><cell>100.0</cell><cell>100.0</cell><cell>97.79</cell><cell>100.0</cell></row><row><cell>13</cell><cell>73.62</cell><cell>88.59</cell><cell>94.83</cell><cell>81.59</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>14</cell><cell>92.98</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>OA</cell><cell>77.21</cell><cell>80.90</cell><cell>91.89</cell><cell>91.57</cell><cell>93.43</cell><cell>96.24</cell></row><row><cell>AA</cell><cell>79.93</cell><cell>81.92</cell><cell>93.92</cell><cell>93.02</cell><cell>94.14</cell><cell>96.74</cell></row><row><cell>kappa</cell><cell>0.7532</cell><cell>0.7930</cell><cell cols="2">0.9121 0.9086</cell><cell>0.9.89</cell><cell>0.9593</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_80"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table><row><cell cols="7">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell>kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_81"><head>Table 10 .</head><label>10</label><figDesc>The categorized results for the BS dataset with 1.2% training samples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_82"><head>Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</head><label></label><figDesc></figDesc><table><row><cell>Remote Sens. 2020, 12, 582</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>18 of 25</cell></row><row><cell></cell><cell>97.56</cell><cell>68.64</cell><cell></cell><cell>100.0</cell><cell>98.95</cell><cell>88.89</cell><cell>98.99</cell></row><row><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>100.0</cell><cell>94.60</cell><cell>94.95</cell><cell>97.41</cell><cell>97.77</cell><cell>95.64</cell></row><row><cell>2</cell><cell>97.56</cell><cell>68.64</cell><cell>100.0</cell><cell>98.95</cell><cell>88.89</cell><cell>98.99</cell></row><row><cell>3</cell><cell>86.35</cell><cell>81.11</cell><cell>91.42</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>4</cell><cell>63.51</cell><cell>65.45</cell><cell>97.34</cell><cell>93.03</cell><cell>92.51</cell><cell>91.30</cell></row><row><cell>5</cell><cell>84.33</cell><cell>89.10</cell><cell>92.42</cell><cell>80.74</cell><cell>93.51</cell><cell>95.58</cell></row><row><cell>6</cell><cell>61.27</cell><cell>69.28</cell><cell>66.39</cell><cell>84.93</cell><cell>68.94</cell><cell>82.23</cell></row><row><cell>7</cell><cell>82.09</cell><cell>80.07</cell><cell>100.0</cell><cell>84.62</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>8</cell><cell>63.46</cell><cell>89.36</cell><cell>100.0</cell><cell>93.36</cell><cell>96.10</cell><cell>95.63</cell></row><row><cell>9</cell><cell>63.53</cell><cell>55.53</cell><cell>90.75</cell><cell>88.44</cell><cell>85.15</cell><cell>96.50</cell></row><row><cell>10</cell><cell>65.74</cell><cell>81.69</cell><cell>86.83</cell><cell>99.59</cell><cell>97.60</cell><cell>98.79</cell></row><row><cell>11</cell><cell>93.91</cell><cell>92.48</cell><cell>100.0</cell><cell>99.67</cell><cell>99.66</cell><cell>99.67</cell></row><row><cell>12</cell><cell>90.70</cell><cell>90.91</cell><cell>100.0</cell><cell>100.0</cell><cell>97.79</cell><cell>100.0</cell></row><row><cell>13</cell><cell>73.62</cell><cell>88.59</cell><cell>94.83</cell><cell>81.59</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>14</cell><cell>92.98</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>OA</cell><cell>77.21</cell><cell>80.90</cell><cell>91.89</cell><cell>91.57</cell><cell>93.43</cell><cell>96.24</cell></row><row><cell>AA</cell><cell>79.93</cell><cell>81.92</cell><cell>93.92</cell><cell>93.02</cell><cell>94.14</cell><cell>96.74</cell></row><row><cell>kappa</cell><cell>0.7532</cell><cell>0.7930</cell><cell cols="2">0.9121 0.9086</cell><cell>0.9.89</cell><cell>0.9593</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_83"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table><row><cell cols="7">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell>kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_84"><head>Table 10 .</head><label>10</label><figDesc>The categorized results for the BS dataset with 1.2% training samples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_85"><head>Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</head><label></label><figDesc></figDesc><table><row><cell>Remote Sens. 2020, 12, 582</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>18 of 25</cell></row><row><cell></cell><cell>86.35</cell><cell>81.11</cell><cell></cell><cell>91.42</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>100.0</cell><cell>94.60</cell><cell>94.95</cell><cell>97.41</cell><cell>97.77</cell><cell>95.64</cell></row><row><cell>2</cell><cell>97.56</cell><cell>68.64</cell><cell>100.0</cell><cell>98.95</cell><cell>88.89</cell><cell>98.99</cell></row><row><cell>3</cell><cell>86.35</cell><cell>81.11</cell><cell>91.42</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>4</cell><cell>63.51</cell><cell>65.45</cell><cell>97.34</cell><cell>93.03</cell><cell>92.51</cell><cell>91.30</cell></row><row><cell>5</cell><cell>84.33</cell><cell>89.10</cell><cell>92.42</cell><cell>80.74</cell><cell>93.51</cell><cell>95.58</cell></row><row><cell>6</cell><cell>61.27</cell><cell>69.28</cell><cell>66.39</cell><cell>84.93</cell><cell>68.94</cell><cell>82.23</cell></row><row><cell>7</cell><cell>82.09</cell><cell>80.07</cell><cell>100.0</cell><cell>84.62</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>8</cell><cell>63.46</cell><cell>89.36</cell><cell>100.0</cell><cell>93.36</cell><cell>96.10</cell><cell>95.63</cell></row><row><cell>9</cell><cell>63.53</cell><cell>55.53</cell><cell>90.75</cell><cell>88.44</cell><cell>85.15</cell><cell>96.50</cell></row><row><cell>10</cell><cell>65.74</cell><cell>81.69</cell><cell>86.83</cell><cell>99.59</cell><cell>97.60</cell><cell>98.79</cell></row><row><cell>11</cell><cell>93.91</cell><cell>92.48</cell><cell>100.0</cell><cell>99.67</cell><cell>99.66</cell><cell>99.67</cell></row><row><cell>12</cell><cell>90.70</cell><cell>90.91</cell><cell>100.0</cell><cell>100.0</cell><cell>97.79</cell><cell>100.0</cell></row><row><cell>13</cell><cell>73.62</cell><cell>88.59</cell><cell>94.83</cell><cell>81.59</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>14</cell><cell>92.98</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>OA</cell><cell>77.21</cell><cell>80.90</cell><cell>91.89</cell><cell>91.57</cell><cell>93.43</cell><cell>96.24</cell></row><row><cell>AA</cell><cell>79.93</cell><cell>81.92</cell><cell>93.92</cell><cell>93.02</cell><cell>94.14</cell><cell>96.74</cell></row><row><cell>kappa</cell><cell>0.7532</cell><cell>0.7930</cell><cell cols="2">0.9121 0.9086</cell><cell>0.9.89</cell><cell>0.9593</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_86"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table><row><cell cols="7">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell>kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_87"><head>Table 10 .</head><label>10</label><figDesc>The categorized results for the BS dataset with 1.2% training samples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_88"><head>Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</head><label></label><figDesc></figDesc><table><row><cell>Remote Sens. 2020, 12, 582</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>18 of 25</cell></row><row><cell></cell><cell>63.51</cell><cell>65.45</cell><cell></cell><cell>97.34</cell><cell>93.03</cell><cell>92.51</cell><cell>91.30</cell></row><row><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>100.0</cell><cell>94.60</cell><cell>94.95</cell><cell>97.41</cell><cell>97.77</cell><cell>95.64</cell></row><row><cell>2</cell><cell>97.56</cell><cell>68.64</cell><cell>100.0</cell><cell>98.95</cell><cell>88.89</cell><cell>98.99</cell></row><row><cell>3</cell><cell>86.35</cell><cell>81.11</cell><cell>91.42</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>4</cell><cell>63.51</cell><cell>65.45</cell><cell>97.34</cell><cell>93.03</cell><cell>92.51</cell><cell>91.30</cell></row><row><cell>5</cell><cell>84.33</cell><cell>89.10</cell><cell>92.42</cell><cell>80.74</cell><cell>93.51</cell><cell>95.58</cell></row><row><cell>6</cell><cell>61.27</cell><cell>69.28</cell><cell>66.39</cell><cell>84.93</cell><cell>68.94</cell><cell>82.23</cell></row><row><cell>7</cell><cell>82.09</cell><cell>80.07</cell><cell>100.0</cell><cell>84.62</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>8</cell><cell>63.46</cell><cell>89.36</cell><cell>100.0</cell><cell>93.36</cell><cell>96.10</cell><cell>95.63</cell></row><row><cell>9</cell><cell>63.53</cell><cell>55.53</cell><cell>90.75</cell><cell>88.44</cell><cell>85.15</cell><cell>96.50</cell></row><row><cell>10</cell><cell>65.74</cell><cell>81.69</cell><cell>86.83</cell><cell>99.59</cell><cell>97.60</cell><cell>98.79</cell></row><row><cell>11</cell><cell>93.91</cell><cell>92.48</cell><cell>100.0</cell><cell>99.67</cell><cell>99.66</cell><cell>99.67</cell></row><row><cell>12</cell><cell>90.70</cell><cell>90.91</cell><cell>100.0</cell><cell>100.0</cell><cell>97.79</cell><cell>100.0</cell></row><row><cell>13</cell><cell>73.62</cell><cell>88.59</cell><cell>94.83</cell><cell>81.59</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>14</cell><cell>92.98</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>OA</cell><cell>77.21</cell><cell>80.90</cell><cell>91.89</cell><cell>91.57</cell><cell>93.43</cell><cell>96.24</cell></row><row><cell>AA</cell><cell>79.93</cell><cell>81.92</cell><cell>93.92</cell><cell>93.02</cell><cell>94.14</cell><cell>96.74</cell></row><row><cell>kappa</cell><cell>0.7532</cell><cell>0.7930</cell><cell cols="2">0.9121 0.9086</cell><cell>0.9.89</cell><cell>0.9593</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_89"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table><row><cell cols="7">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell>kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_90"><head>Table 10 .</head><label>10</label><figDesc>The categorized results for the BS dataset with 1.2% training samples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_91"><head>Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</head><label></label><figDesc>Remote Sens. 2020,<ref type="bibr" target="#b11">12,</ref> 582    </figDesc><table><row><cell></cell><cell>84.33</cell><cell>89.10</cell><cell></cell><cell>92.42</cell><cell>80.74</cell><cell>93.51</cell><cell>95.58</cell></row><row><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>100.0</cell><cell>94.60</cell><cell>94.95</cell><cell>97.41</cell><cell>97.77</cell><cell>95.64</cell></row><row><cell>2</cell><cell>97.56</cell><cell>68.64</cell><cell>100.0</cell><cell>98.95</cell><cell>88.89</cell><cell>98.99</cell></row><row><cell>3</cell><cell>86.35</cell><cell>81.11</cell><cell>91.42</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>4</cell><cell>63.51</cell><cell>65.45</cell><cell>97.34</cell><cell>93.03</cell><cell>92.51</cell><cell>91.30</cell></row><row><cell>5</cell><cell>84.33</cell><cell>89.10</cell><cell>92.42</cell><cell>80.74</cell><cell>93.51</cell><cell>95.58</cell></row><row><cell>6</cell><cell>61.27</cell><cell>69.28</cell><cell>66.39</cell><cell>84.93</cell><cell>68.94</cell><cell>82.23</cell></row><row><cell>7</cell><cell>82.09</cell><cell>80.07</cell><cell>100.0</cell><cell>84.62</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>8</cell><cell>63.46</cell><cell>89.36</cell><cell>100.0</cell><cell>93.36</cell><cell>96.10</cell><cell>95.63</cell></row><row><cell>9</cell><cell>63.53</cell><cell>55.53</cell><cell>90.75</cell><cell>88.44</cell><cell>85.15</cell><cell>96.50</cell></row><row><cell>10</cell><cell>65.74</cell><cell>81.69</cell><cell>86.83</cell><cell>99.59</cell><cell>97.60</cell><cell>98.79</cell></row><row><cell>11</cell><cell>93.91</cell><cell>92.48</cell><cell>100.0</cell><cell>99.67</cell><cell>99.66</cell><cell>99.67</cell></row><row><cell>12</cell><cell>90.70</cell><cell>90.91</cell><cell>100.0</cell><cell>100.0</cell><cell>97.79</cell><cell>100.0</cell></row><row><cell>13</cell><cell>73.62</cell><cell>88.59</cell><cell>94.83</cell><cell>81.59</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>14</cell><cell>92.98</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>OA</cell><cell>77.21</cell><cell>80.90</cell><cell>91.89</cell><cell>91.57</cell><cell>93.43</cell><cell>96.24</cell></row><row><cell>AA</cell><cell>79.93</cell><cell>81.92</cell><cell>93.92</cell><cell>93.02</cell><cell>94.14</cell><cell>96.74</cell></row><row><cell>kappa</cell><cell>0.7532</cell><cell>0.7930</cell><cell cols="2">0.9121 0.9086</cell><cell>0.9.89</cell><cell>0.9593</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_92"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset</figDesc><table><row><cell cols="2">0.9412 0</cell><cell>0.7547</cell><cell>0.8671</cell><cell>kappa</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>9</cell><cell>96.66</cell><cell>79.86</cell><cell>91.45</cell><cell>AA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>9</cell><cell>94.72</cell><cell>77.79</cell><cell>88.09</cell><cell>OA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>100.0</cell><cell>99.62</cell><cell>99.03</cell><cell>16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>7</cell><cell>89.96</cell><cell cols="2">75.86 Remote Sens. 2020, 12, 582 51.88</cell><cell>15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>18 of 25</cell></row><row><cell>9</cell><cell>97.86</cell><cell>94.92</cell><cell>89.26</cell><cell>14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>100.0</cell><cell>95.71</cell><cell>94.87</cell><cell>13</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>9</cell><cell>99.95</cell><cell>75.49</cell><cell>95.19</cell><cell>12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>9</cell><cell>93.30</cell><cell>72.03</cell><cell>75.92</cell><cell>11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>9</cell><cell>98.81</cell><cell>67.39</cell><cell>90.01</cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>9</cell><cell>98.48</cell><cell>99.26</cell><cell>97.69</cell><cell>9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>9</cell><cell>88.60</cell><cell>88.23</cell><cell>74.73</cell><cell>8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>9</cell><cell>99.75</cell><cell>95.98</cell><cell>91.33</cell><cell>7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>9 9</cell><cell>99.39 99.95</cell><cell>98.24 96.51</cell><cell>93.56 99.89 7</cell><cell>5 6 61.27</cell><cell>69.28</cell><cell>66.39</cell><cell>84.93</cell><cell>68.94</cell><cell>82.23</cell></row><row><cell>9</cell><cell>94.85</cell><cell>82.99</cell><cell>97.30</cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>9</cell><cell>89.72</cell><cell>94.69</cell><cell>89.88</cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>100.0</cell><cell>64.82</cell><cell>98.95</cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>100.0</cell><cell>0.00</cell><cell>99.85</cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Class Color SVM CDCNN SSRN FD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_93"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table><row><cell cols="7">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell>kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_94"><head>Table 10 .</head><label>10</label><figDesc>The categorized results for the BS dataset with 1.2% training samples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_95"><head>Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</head><label></label><figDesc></figDesc><table><row><cell>Remote Sens. 2020, 12, 582</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>18 of 25</cell></row><row><cell></cell><cell>82.09</cell><cell>80.07</cell><cell></cell><cell>100.0</cell><cell>84.62</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>100.0</cell><cell>94.60</cell><cell>94.95</cell><cell>97.41</cell><cell>97.77</cell><cell>95.64</cell></row><row><cell>2</cell><cell>97.56</cell><cell>68.64</cell><cell>100.0</cell><cell>98.95</cell><cell>88.89</cell><cell>98.99</cell></row><row><cell>3</cell><cell>86.35</cell><cell>81.11</cell><cell>91.42</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>4</cell><cell>63.51</cell><cell>65.45</cell><cell>97.34</cell><cell>93.03</cell><cell>92.51</cell><cell>91.30</cell></row><row><cell>5</cell><cell>84.33</cell><cell>89.10</cell><cell>92.42</cell><cell>80.74</cell><cell>93.51</cell><cell>95.58</cell></row><row><cell>6</cell><cell>61.27</cell><cell>69.28</cell><cell>66.39</cell><cell>84.93</cell><cell>68.94</cell><cell>82.23</cell></row><row><cell>7</cell><cell>82.09</cell><cell>80.07</cell><cell>100.0</cell><cell>84.62</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>8</cell><cell>63.46</cell><cell>89.36</cell><cell>100.0</cell><cell>93.36</cell><cell>96.10</cell><cell>95.63</cell></row><row><cell>9</cell><cell>63.53</cell><cell>55.53</cell><cell>90.75</cell><cell>88.44</cell><cell>85.15</cell><cell>96.50</cell></row><row><cell>10</cell><cell>65.74</cell><cell>81.69</cell><cell>86.83</cell><cell>99.59</cell><cell>97.60</cell><cell>98.79</cell></row><row><cell>11</cell><cell>93.91</cell><cell>92.48</cell><cell>100.0</cell><cell>99.67</cell><cell>99.66</cell><cell>99.67</cell></row><row><cell>12</cell><cell>90.70</cell><cell>90.91</cell><cell>100.0</cell><cell>100.0</cell><cell>97.79</cell><cell>100.0</cell></row><row><cell>13</cell><cell>73.62</cell><cell>88.59</cell><cell>94.83</cell><cell>81.59</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>14</cell><cell>92.98</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>OA</cell><cell>77.21</cell><cell>80.90</cell><cell>91.89</cell><cell>91.57</cell><cell>93.43</cell><cell>96.24</cell></row><row><cell>AA</cell><cell>79.93</cell><cell>81.92</cell><cell>93.92</cell><cell>93.02</cell><cell>94.14</cell><cell>96.74</cell></row><row><cell>kappa</cell><cell>0.7532</cell><cell>0.7930</cell><cell cols="2">0.9121 0.9086</cell><cell>0.9.89</cell><cell>0.9593</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_96"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table><row><cell cols="7">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell>kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_97"><head>Table 10 .</head><label>10</label><figDesc>The categorized results for the BS dataset with 1.2% training samples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_98"><head>Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</head><label></label><figDesc></figDesc><table><row><cell>Remote Sens. 2020, 12, 582</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>18 of 25</cell></row><row><cell></cell><cell>63.46</cell><cell>89.36</cell><cell></cell><cell>100.0</cell><cell>93.36</cell><cell>96.10</cell><cell>95.63</cell></row><row><cell>9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>100.0</cell><cell>94.60</cell><cell>94.95</cell><cell>97.41</cell><cell>97.77</cell><cell>95.64</cell></row><row><cell>2</cell><cell>97.56</cell><cell>68.64</cell><cell>100.0</cell><cell>98.95</cell><cell>88.89</cell><cell>98.99</cell></row><row><cell>3</cell><cell>86.35</cell><cell>81.11</cell><cell>91.42</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>4</cell><cell>63.51</cell><cell>65.45</cell><cell>97.34</cell><cell>93.03</cell><cell>92.51</cell><cell>91.30</cell></row><row><cell>5</cell><cell>84.33</cell><cell>89.10</cell><cell>92.42</cell><cell>80.74</cell><cell>93.51</cell><cell>95.58</cell></row><row><cell>6</cell><cell>61.27</cell><cell>69.28</cell><cell>66.39</cell><cell>84.93</cell><cell>68.94</cell><cell>82.23</cell></row><row><cell>7</cell><cell>82.09</cell><cell>80.07</cell><cell>100.0</cell><cell>84.62</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>8</cell><cell>63.46</cell><cell>89.36</cell><cell>100.0</cell><cell>93.36</cell><cell>96.10</cell><cell>95.63</cell></row><row><cell>9</cell><cell>63.53</cell><cell>55.53</cell><cell>90.75</cell><cell>88.44</cell><cell>85.15</cell><cell>96.50</cell></row><row><cell>10</cell><cell>65.74</cell><cell>81.69</cell><cell>86.83</cell><cell>99.59</cell><cell>97.60</cell><cell>98.79</cell></row><row><cell>11</cell><cell>93.91</cell><cell>92.48</cell><cell>100.0</cell><cell>99.67</cell><cell>99.66</cell><cell>99.67</cell></row><row><cell>12</cell><cell>90.70</cell><cell>90.91</cell><cell>100.0</cell><cell>100.0</cell><cell>97.79</cell><cell>100.0</cell></row><row><cell>13</cell><cell>73.62</cell><cell>88.59</cell><cell>94.83</cell><cell>81.59</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>14</cell><cell>92.98</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>OA</cell><cell>77.21</cell><cell>80.90</cell><cell>91.89</cell><cell>91.57</cell><cell>93.43</cell><cell>96.24</cell></row><row><cell>AA</cell><cell>79.93</cell><cell>81.92</cell><cell>93.92</cell><cell>93.02</cell><cell>94.14</cell><cell>96.74</cell></row><row><cell>kappa</cell><cell>0.7532</cell><cell>0.7930</cell><cell cols="2">0.9121 0.9086</cell><cell>0.9.89</cell><cell>0.9593</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_99"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table><row><cell cols="7">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell>kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_100"><head>Table 10 .</head><label>10</label><figDesc>The categorized results for the BS dataset with 1.2% training samples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_101"><head>Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</head><label></label><figDesc></figDesc><table><row><cell>Remote Sens. 2020, 12, 582</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>18 of 25</cell></row><row><cell></cell><cell>63.53</cell><cell>55.53</cell><cell></cell><cell>90.75</cell><cell>88.44</cell><cell>85.15</cell><cell>96.50</cell></row><row><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>100.0</cell><cell>94.60</cell><cell>94.95</cell><cell>97.41</cell><cell>97.77</cell><cell>95.64</cell></row><row><cell>2</cell><cell>97.56</cell><cell>68.64</cell><cell>100.0</cell><cell>98.95</cell><cell>88.89</cell><cell>98.99</cell></row><row><cell>3</cell><cell>86.35</cell><cell>81.11</cell><cell>91.42</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>4</cell><cell>63.51</cell><cell>65.45</cell><cell>97.34</cell><cell>93.03</cell><cell>92.51</cell><cell>91.30</cell></row><row><cell>5</cell><cell>84.33</cell><cell>89.10</cell><cell>92.42</cell><cell>80.74</cell><cell>93.51</cell><cell>95.58</cell></row><row><cell>6</cell><cell>61.27</cell><cell>69.28</cell><cell>66.39</cell><cell>84.93</cell><cell>68.94</cell><cell>82.23</cell></row><row><cell>7</cell><cell>82.09</cell><cell>80.07</cell><cell>100.0</cell><cell>84.62</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>8</cell><cell>63.46</cell><cell>89.36</cell><cell>100.0</cell><cell>93.36</cell><cell>96.10</cell><cell>95.63</cell></row><row><cell>9</cell><cell>63.53</cell><cell>55.53</cell><cell>90.75</cell><cell>88.44</cell><cell>85.15</cell><cell>96.50</cell></row><row><cell>10</cell><cell>65.74</cell><cell>81.69</cell><cell>86.83</cell><cell>99.59</cell><cell>97.60</cell><cell>98.79</cell></row><row><cell>11</cell><cell>93.91</cell><cell>92.48</cell><cell>100.0</cell><cell>99.67</cell><cell>99.66</cell><cell>99.67</cell></row><row><cell>12</cell><cell>90.70</cell><cell>90.91</cell><cell>100.0</cell><cell>100.0</cell><cell>97.79</cell><cell>100.0</cell></row><row><cell>13</cell><cell>73.62</cell><cell>88.59</cell><cell>94.83</cell><cell>81.59</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>14</cell><cell>92.98</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>OA</cell><cell>77.21</cell><cell>80.90</cell><cell>91.89</cell><cell>91.57</cell><cell>93.43</cell><cell>96.24</cell></row><row><cell>AA</cell><cell>79.93</cell><cell>81.92</cell><cell>93.92</cell><cell>93.02</cell><cell>94.14</cell><cell>96.74</cell></row><row><cell>kappa</cell><cell>0.7532</cell><cell>0.7930</cell><cell cols="2">0.9121 0.9086</cell><cell>0.9.89</cell><cell>0.9593</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_102"><head>Table 9 .</head><label>9</label><figDesc>The categorized results for the SV dataset with 0.5% training samples.</figDesc><table><row><cell cols="7">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell>1</cell><cell>99.85</cell><cell>0.00</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>2</cell><cell>98.95</cell><cell>64.82</cell><cell>100.0</cell><cell>100.0</cell><cell>99.51</cell><cell>99.17</cell></row><row><cell>3</cell><cell>89.88</cell><cell>94.69</cell><cell>89.72</cell><cell>99.44</cell><cell>98.92</cell><cell>97.74</cell></row><row><cell>4</cell><cell>97.30</cell><cell>82.99</cell><cell>94.85</cell><cell>98.57</cell><cell>96.39</cell><cell>95.95</cell></row><row><cell>5</cell><cell>93.56</cell><cell>98.24</cell><cell>99.39</cell><cell>99.87</cell><cell>96.39</cell><cell>99.29</cell></row><row><cell>6</cell><cell>99.89</cell><cell>96.51</cell><cell>99.95</cell><cell>99.97</cell><cell>99.17</cell><cell>99.92</cell></row><row><cell>7</cell><cell>91.33</cell><cell>95.98</cell><cell>99.75</cell><cell>99.75</cell><cell>96.80</cell><cell>99.83</cell></row><row><cell>8</cell><cell>74.73</cell><cell>88.23</cell><cell>88.60</cell><cell>99.60</cell><cell>95.60</cell><cell>95.97</cell></row><row><cell>9</cell><cell>97.69</cell><cell>99.26</cell><cell>98.48</cell><cell>99.69</cell><cell>99.22</cell><cell>99.37</cell></row><row><cell>10</cell><cell>90.01</cell><cell>67.39</cell><cell>98.81</cell><cell>99.02</cell><cell>96.20</cell><cell>96.72</cell></row><row><cell>11</cell><cell>75.92</cell><cell>72.03</cell><cell>93.30</cell><cell>92.77</cell><cell>82.29</cell><cell>93.72</cell></row><row><cell>12</cell><cell>95.19</cell><cell>75.49</cell><cell>99.95</cell><cell>99.64</cell><cell>99.17</cell><cell>100.0</cell></row><row><cell>13</cell><cell>94.87</cell><cell>95.71</cell><cell>100.0</cell><cell>100.0</cell><cell>98.91</cell><cell>100.0</cell></row><row><cell>14</cell><cell>89.26</cell><cell>94.92</cell><cell>97.86</cell><cell>98.05</cell><cell>98.22</cell><cell>96.89</cell></row><row><cell>15</cell><cell>75.86</cell><cell>51.88</cell><cell>89.96</cell><cell>74.58</cell><cell>84.71</cell><cell>93.42</cell></row><row><cell>16</cell><cell>99.03</cell><cell>99.62</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>OA</cell><cell>88.09</cell><cell>77.79</cell><cell>94.72</cell><cell>94.99</cell><cell>95.44</cell><cell>97.51</cell></row><row><cell>AA</cell><cell>91.45</cell><cell>79.86</cell><cell>96.66</cell><cell>97.56</cell><cell>96.34</cell><cell>98.00</cell></row><row><cell>kappa</cell><cell>0.8671</cell><cell>0.7547</cell><cell cols="2">0.9412 0.9444</cell><cell>0.9493</cell><cell>0.9723</cell></row></table><note><p>4.3.4. Classification Maps and Categorized Result for the BS Dataset</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_103"><head>Table 10 .</head><label>10</label><figDesc>The categorized results for the BS dataset with 1.2% training samples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_104"><head>Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>65.74</cell><cell>81.69</cell><cell></cell><cell>86.83</cell><cell>99.59</cell><cell>97.60</cell><cell>98.79</cell></row><row><cell>11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>100.0</cell><cell>94.60</cell><cell>94.95</cell><cell>97.41</cell><cell>97.77</cell><cell>95.64</cell></row><row><cell>2</cell><cell>97.56</cell><cell>68.64</cell><cell>100.0</cell><cell>98.95</cell><cell>88.89</cell><cell>98.99</cell></row><row><cell>3</cell><cell>86.35</cell><cell>81.11</cell><cell>91.42</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>4</cell><cell>63.51</cell><cell>65.45</cell><cell>97.34</cell><cell>93.03</cell><cell>92.51</cell><cell>91.30</cell></row><row><cell>5</cell><cell>84.33</cell><cell>89.10</cell><cell>92.42</cell><cell>80.74</cell><cell>93.51</cell><cell>95.58</cell></row><row><cell>6</cell><cell>61.27</cell><cell>69.28</cell><cell>66.39</cell><cell>84.93</cell><cell>68.94</cell><cell>82.23</cell></row><row><cell>7</cell><cell>82.09</cell><cell>80.07</cell><cell>100.0</cell><cell>84.62</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>8</cell><cell>63.46</cell><cell>89.36</cell><cell>100.0</cell><cell>93.36</cell><cell>96.10</cell><cell>95.63</cell></row><row><cell>9</cell><cell>63.53</cell><cell>55.53</cell><cell>90.75</cell><cell>88.44</cell><cell>85.15</cell><cell>96.50</cell></row><row><cell>10</cell><cell>65.74</cell><cell>81.69</cell><cell>86.83</cell><cell>99.59</cell><cell>97.60</cell><cell>98.79</cell></row><row><cell>11</cell><cell>93.91</cell><cell>92.48</cell><cell>100.0</cell><cell>99.67</cell><cell>99.66</cell><cell>99.67</cell></row><row><cell>12</cell><cell>90.70</cell><cell>90.91</cell><cell>100.0</cell><cell>100.0</cell><cell>97.79</cell><cell>100.0</cell></row><row><cell>13</cell><cell>73.62</cell><cell>88.59</cell><cell>94.83</cell><cell>81.59</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>14</cell><cell>92.98</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>OA</cell><cell>77.21</cell><cell>80.90</cell><cell>91.89</cell><cell>91.57</cell><cell>93.43</cell><cell>96.24</cell></row><row><cell>AA</cell><cell>79.93</cell><cell>81.92</cell><cell>93.92</cell><cell>93.02</cell><cell>94.14</cell><cell>96.74</cell></row><row><cell>kappa</cell><cell>0.7532</cell><cell>0.7930</cell><cell cols="2">0.9121 0.9086</cell><cell>0.9.89</cell><cell>0.9593</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_105"><head>Table 7 .</head><label>7</label><figDesc>The categorized results for the IP dataset with 3% training samples.</figDesc><table><row><cell cols="8">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell></cell><cell>1</cell><cell>24.24</cell><cell>0.00</cell><cell>100.0</cell><cell>85.42</cell><cell>93.48</cell><cell>100.0</cell></row><row><cell></cell><cell>2</cell><cell>58.10</cell><cell>62.36</cell><cell>89.14</cell><cell>97.20</cell><cell>91.15</cell><cell>88.49</cell></row><row><cell></cell><cell>3</cell><cell>64.37</cell><cell>57.00</cell><cell>77.49</cell><cell>94.45</cell><cell>99.58</cell><cell>97.12</cell></row><row><cell></cell><cell>4</cell><cell>37.07</cell><cell>37.50</cell><cell>88.95</cell><cell>100.0</cell><cell>98.57</cell><cell>100.0</cell></row><row><cell></cell><cell>5</cell><cell>87.67</cell><cell>88.16</cell><cell>96.48</cell><cell>100.0</cell><cell>97.45</cell><cell>100.0</cell></row><row><cell></cell><cell>6</cell><cell>84.02</cell><cell>79.63</cell><cell>98.15</cell><cell>100.0</cell><cell>95.66</cell><cell>97.18</cell></row><row><cell></cell><cell>7</cell><cell>56.10</cell><cell>0.00</cell><cell>0.00</cell><cell>73.53</cell><cell>40.00</cell><cell>92.59</cell></row><row><cell></cell><cell>8</cell><cell>89.62</cell><cell>84.02</cell><cell>84.54</cell><cell>99.78</cell><cell>100.0</cell><cell>99.78</cell></row><row><cell></cell><cell>9</cell><cell>21.21</cell><cell>0.00</cell><cell>0.00</cell><cell>100.0</cell><cell>38.10</cell><cell>100.0</cell></row><row><cell></cell><cell>10</cell><cell>65.89</cell><cell>37.50</cell><cell>92.07</cell><cell>89.25</cell><cell>85.98</cell><cell>89.87</cell></row><row><cell></cell><cell>11</cell><cell>62.32 93.91</cell><cell>53.25 92.48</cell><cell>90.89</cell><cell>93.97 100.0</cell><cell>94.39 99.67</cell><cell>99.33 99.66</cell><cell>99.67</cell></row><row><cell>12</cell><cell>12</cell><cell>52.40</cell><cell>42.96</cell><cell>84.19</cell><cell>95.41</cell><cell>89.92</cell><cell>98.50</cell></row><row><cell></cell><cell>13</cell><cell>94.30</cell><cell>49.47</cell><cell>98.47</cell><cell>100.0</cell><cell>99.48</cell><cell>96.02</cell></row><row><cell></cell><cell>14</cell><cell>90.15</cell><cell>76.71</cell><cell>94.56</cell><cell>93.14</cell><cell>92.81</cell><cell>93.22</cell></row><row><cell></cell><cell>15</cell><cell>63.96</cell><cell>62.60</cell><cell>84.11</cell><cell>90.61</cell><cell>89.66</cell><cell>96.99</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_106"><head>Table 7 .</head><label>7</label><figDesc>The categorized results for the IP dataset with 3% training samples.</figDesc><table><row><cell cols="8">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell></cell><cell>1</cell><cell>24.24</cell><cell>0.00</cell><cell>100.0</cell><cell>85.42</cell><cell>93.48</cell><cell>100.0</cell></row><row><cell></cell><cell>2</cell><cell>58.10</cell><cell>62.36</cell><cell>89.14</cell><cell>97.20</cell><cell>91.15</cell><cell>88.49</cell></row><row><cell></cell><cell>3</cell><cell>64.37</cell><cell>57.00</cell><cell>77.49</cell><cell>94.45</cell><cell>99.58</cell><cell>97.12</cell></row><row><cell></cell><cell>4</cell><cell>37.07</cell><cell>37.50</cell><cell>88.95</cell><cell>100.0</cell><cell>98.57</cell><cell>100.0</cell></row><row><cell></cell><cell>5</cell><cell>87.67</cell><cell>88.16</cell><cell>96.48</cell><cell>100.0</cell><cell>97.45</cell><cell>100.0</cell></row><row><cell></cell><cell>6</cell><cell>84.02</cell><cell>79.63</cell><cell>98.15</cell><cell>100.0</cell><cell>95.66</cell><cell>97.18</cell></row><row><cell></cell><cell>7</cell><cell>56.10</cell><cell>0.00</cell><cell>0.00</cell><cell>73.53</cell><cell>40.00</cell><cell>92.59</cell></row><row><cell></cell><cell>8</cell><cell>89.62</cell><cell>84.02</cell><cell>84.54</cell><cell>99.78</cell><cell>100.0</cell><cell>99.78</cell></row><row><cell></cell><cell>9</cell><cell>21.21</cell><cell>0.00</cell><cell>0.00</cell><cell>100.0</cell><cell>38.10</cell><cell>100.0</cell></row><row><cell></cell><cell>10</cell><cell>65.89</cell><cell>37.50</cell><cell>92.07</cell><cell>89.25</cell><cell>85.98</cell><cell>89.87</cell></row><row><cell></cell><cell>11</cell><cell>62.32</cell><cell>53.25</cell><cell>90.89</cell><cell>93.97</cell><cell>94.39</cell><cell>99.33</cell></row><row><cell></cell><cell>12</cell><cell>52.40 90.70</cell><cell>42.96 90.91</cell><cell>84.19</cell><cell>95.41 100.0</cell><cell>89.92 100.0</cell><cell>98.50 97.79</cell><cell>100.0</cell></row><row><cell>13</cell><cell>13</cell><cell>94.30</cell><cell>49.47</cell><cell>98.47</cell><cell>100.0</cell><cell>99.48</cell><cell>96.02</cell></row><row><cell></cell><cell>14</cell><cell>90.15</cell><cell>76.71</cell><cell>94.56</cell><cell>93.14</cell><cell>92.81</cell><cell>93.22</cell></row><row><cell></cell><cell>15</cell><cell>63.96</cell><cell>62.60</cell><cell>84.11</cell><cell>90.61</cell><cell>89.66</cell><cell>96.99</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_107"><head>Table 7 .</head><label>7</label><figDesc>The categorized results for the IP dataset with 3% training samples.</figDesc><table><row><cell cols="8">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell></row><row><cell></cell><cell>1</cell><cell>24.24</cell><cell>0.00</cell><cell>100.0</cell><cell>85.42</cell><cell>93.48</cell><cell>100.0</cell></row><row><cell></cell><cell>2</cell><cell>58.10</cell><cell>62.36</cell><cell>89.14</cell><cell>97.20</cell><cell>91.15</cell><cell>88.49</cell></row><row><cell></cell><cell>3</cell><cell>64.37</cell><cell>57.00</cell><cell>77.49</cell><cell>94.45</cell><cell>99.58</cell><cell>97.12</cell></row><row><cell></cell><cell>4</cell><cell>37.07</cell><cell>37.50</cell><cell>88.95</cell><cell>100.0</cell><cell>98.57</cell><cell>100.0</cell></row><row><cell></cell><cell>5</cell><cell>87.67</cell><cell>88.16</cell><cell>96.48</cell><cell>100.0</cell><cell>97.45</cell><cell>100.0</cell></row><row><cell></cell><cell>6</cell><cell>84.02</cell><cell>79.63</cell><cell>98.15</cell><cell>100.0</cell><cell>95.66</cell><cell>97.18</cell></row><row><cell></cell><cell>7</cell><cell>56.10</cell><cell>0.00</cell><cell>0.00</cell><cell>73.53</cell><cell>40.00</cell><cell>92.59</cell></row><row><cell></cell><cell>8</cell><cell>89.62</cell><cell>84.02</cell><cell>84.54</cell><cell>99.78</cell><cell>100.0</cell><cell>99.78</cell></row><row><cell></cell><cell>9</cell><cell>21.21</cell><cell>0.00</cell><cell>0.00</cell><cell>100.0</cell><cell>38.10</cell><cell>100.0</cell></row><row><cell></cell><cell>10</cell><cell>65.89</cell><cell>37.50</cell><cell>92.07</cell><cell>89.25</cell><cell>85.98</cell><cell>89.87</cell></row><row><cell></cell><cell>11</cell><cell>62.32</cell><cell>53.25</cell><cell>90.89</cell><cell>93.97</cell><cell>94.39</cell><cell>99.33</cell></row><row><cell></cell><cell>12</cell><cell>52.40</cell><cell>42.96</cell><cell>84.19</cell><cell>95.41</cell><cell>89.92</cell><cell>98.50</cell></row><row><cell></cell><cell>13</cell><cell>94.30 73.62</cell><cell>49.47 88.59</cell><cell>98.47</cell><cell>100.0 94.83</cell><cell>99.48 81.59</cell><cell>96.02 100.0</cell><cell>100.0</cell></row><row><cell>14</cell><cell>14</cell><cell>90.15</cell><cell>76.71</cell><cell>94.56</cell><cell>93.14</cell><cell>92.81</cell><cell>93.22</cell></row><row><cell></cell><cell>15</cell><cell>63.96</cell><cell>62.60</cell><cell>84.11</cell><cell>90.61</cell><cell>89.66</cell><cell>96.99</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_108"><head>Table 7 .</head><label>7</label><figDesc>The categorized results for the IP dataset with 3% training samples.</figDesc><table><row><cell cols="7">Class Color SVM CDCNN SSRN FDSSC DBMA Proposed</cell><cell></cell></row><row><cell>1</cell><cell>24.24</cell><cell>0.00</cell><cell>100.0</cell><cell>85.42</cell><cell>93.48</cell><cell>100.0</cell><cell></cell></row><row><cell>2</cell><cell>58.10</cell><cell>62.36</cell><cell>89.14</cell><cell>97.20</cell><cell>91.15</cell><cell>88.49</cell><cell></cell></row><row><cell>3</cell><cell>64.37</cell><cell>57.00</cell><cell>77.49</cell><cell>94.45</cell><cell>99.58</cell><cell>97.12</cell><cell></cell></row><row><cell>4</cell><cell>37.07</cell><cell>37.50</cell><cell>88.95</cell><cell>100.0</cell><cell>98.57</cell><cell>100.0</cell><cell></cell></row><row><cell>5</cell><cell>87.67</cell><cell>88.16</cell><cell>96.48</cell><cell>100.0</cell><cell>97.45</cell><cell>100.0</cell><cell></cell></row><row><cell>6</cell><cell>84.02</cell><cell>79.63</cell><cell>98.15</cell><cell>100.0</cell><cell>95.66</cell><cell>97.18</cell><cell></cell></row><row><cell>7</cell><cell>56.10</cell><cell>0.00</cell><cell>0.00</cell><cell>73.53</cell><cell>40.00</cell><cell>92.59</cell><cell></cell></row><row><cell>8</cell><cell>89.62</cell><cell>84.02</cell><cell>84.54</cell><cell>99.78</cell><cell>100.0</cell><cell>99.78</cell><cell></cell></row><row><cell>9</cell><cell>21.21</cell><cell>0.00</cell><cell>0.00</cell><cell>100.0</cell><cell>38.10</cell><cell>100.0</cell><cell></cell></row><row><cell>10</cell><cell>65.89</cell><cell>37.50</cell><cell>92.07</cell><cell>89.25</cell><cell>85.98</cell><cell>89.87</cell><cell></cell></row><row><cell>11</cell><cell>62.32</cell><cell>53.25</cell><cell>90.89</cell><cell>93.97</cell><cell>94.39</cell><cell>99.33</cell><cell></cell></row><row><cell>12</cell><cell>52.40</cell><cell>42.96</cell><cell>84.19</cell><cell>95.41</cell><cell>89.92</cell><cell>98.50</cell><cell></cell></row><row><cell>13</cell><cell>94.30</cell><cell>49.47</cell><cell>98.47</cell><cell>100.0</cell><cell>99.48</cell><cell>96.02</cell><cell></cell></row><row><cell>14</cell><cell>90.15 92.98</cell><cell>76.71 100.0</cell><cell>94.56</cell><cell>93.14 100.0</cell><cell>92.81 100.0</cell><cell>93.22 100.0</cell><cell>100.0</cell></row><row><cell>15 OA</cell><cell>63.96 77.21</cell><cell>62.60 80.90</cell><cell>84.11</cell><cell>90.61 91.89</cell><cell>89.66 91.57</cell><cell>96.99 93.43</cell><cell>96.24</cell></row><row><cell>AA</cell><cell>79.93</cell><cell>81.92</cell><cell></cell><cell>93.92</cell><cell>93.02</cell><cell>94.14</cell><cell>96.74</cell></row><row><cell>kappa</cell><cell>0.7532</cell><cell>0.7930</cell><cell></cell><cell>0.9121</cell><cell>0.9086</cell><cell>0.9.89</cell><cell>0.9593</cell></row></table><note><p>Remote Sens. 2020, 12, 582 19 of 25</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_109"><head>Table 11 .</head><label>11</label><figDesc>Training and testing consumption of support vector machines (SVM), contextual deep convolutional neural networks (CDCNN), spectral-spatial residual network (SSRN), fast dense spectral-spatial convolution (FDSSC), double-branch multi-attention (DBMA), and our method on the IP dataset using 307 training samples (3%) in 16 classes.</figDesc><table><row><cell>Dataset</cell><cell>Algorithm</cell><cell>Training Times (s)</cell><cell>Testing Times (s)</cell></row><row><cell></cell><cell>SVM</cell><cell>20.10</cell><cell>0.66</cell></row><row><cell></cell><cell>CDCNN</cell><cell>11.13</cell><cell>1.54</cell></row><row><cell>Indian Pines</cell><cell>SSRN FDSSC</cell><cell>46.03 105.05</cell><cell>2.71 4.86</cell></row><row><cell></cell><cell>DBMA</cell><cell>94.69</cell><cell>6.35</cell></row><row><cell></cell><cell>Proposed</cell><cell>69.83</cell><cell>5.60</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_110"><head>Table 12 .</head><label>12</label><figDesc>Training and testing consumption of SVM, CDCNN, SSRN, FDSSC, DBMA, and our method on the UP dataset using 210 training samples (0.5%) in nine classes.</figDesc><table><row><cell>Dataset</cell><cell>Algorithms</cell><cell>Training Times (s)</cell><cell>Testing Times (s)</cell></row><row><cell></cell><cell>SVM</cell><cell>3.38</cell><cell>2.29</cell></row><row><cell></cell><cell>CDCNN</cell><cell>10.26</cell><cell>4.92</cell></row><row><cell>Pavia University</cell><cell>SSRN FDSSC</cell><cell>9.93 26.01</cell><cell>6.41 11.56</cell></row><row><cell></cell><cell>DBMA</cell><cell>21.02</cell><cell>11.17</cell></row><row><cell></cell><cell>Proposed</cell><cell>18.46</cell><cell>13.32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_111"><head>Table 13 .</head><label>13</label><figDesc>Training and testing consumption of SVM, CDCNN, SSRN, FDSSC, DBMA, and our method on the SV dataset using 263 training samples (0.5%) in 16 classes.</figDesc><table><row><cell>Dataset</cell><cell>Algorithms</cell><cell>Training Times (s)</cell><cell>Testing Times (s)</cell></row><row><cell></cell><cell>SVM</cell><cell>9.35</cell><cell>3.89</cell></row><row><cell></cell><cell>CDCNN</cell><cell>9.82</cell><cell>6.14</cell></row><row><cell>Salinas</cell><cell>SSRN FDSSC</cell><cell>73.75 99.91</cell><cell>13.99 25.57</cell></row><row><cell></cell><cell>DBMA</cell><cell>105.30</cell><cell>31.82</cell></row><row><cell></cell><cell>Proposed</cell><cell>71.18</cell><cell>23.93</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_112"><head>Table 14 .</head><label>14</label><figDesc>Training and testing consumption of SVM, CDCNN, SSRN, FDSSC, DBMA, and our method on the BS dataset using 40 training samples (1.2%) in 14 classes.</figDesc><table><row><cell>Dataset</cell><cell>Algorithms</cell><cell>Training Times (s)</cell><cell>Testing Times (s)</cell></row><row><cell></cell><cell>SVM</cell><cell>0.93</cell><cell>0.15</cell></row><row><cell></cell><cell>CDCNN</cell><cell>11.10</cell><cell>1.33</cell></row><row><cell>Botswana</cell><cell>SSRN FDSSC</cell><cell>8.87 17.84</cell><cell>1.37 1.45</cell></row><row><cell></cell><cell>DBMA</cell><cell>13.67</cell><cell>2.04</cell></row><row><cell></cell><cell>Proposed</cell><cell>17.19</cell><cell>1.90</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Funding: This research was funded by the National Natural Science Foundations of China (No. 41671452).</p><p>Acknowledgments: I am indebted to my mentor, Shunyi Zheng, who supported my work strongly. I would also like to express my gratitude to Chenxi Duan, my morning sunlight who revised my manuscript earnestly.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Author Contributions: Conceptualization, R.L.; formal analysis, R.L.; funding acquisition, S.Z.; methodology, R.L.; validation, R.L.; writing-original draft, R.L. and C.D.; writing-review and editing, S.Z., C.D., Y.Y., and X.W. All authors have read and agreed to the published version of the manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of Interest:</head><p>The authors declare no conflicts of interest. The founding sponsors had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript; nor in the decision to publish the results.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Computational intelligence in optical remote sensing image processing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.asoc.2017.11.045</idno>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="75" to="93" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Very deep convolutional neural networks for complex land cover mapping using multispectral remote sensing imagery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mahdianpari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rezaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mohammadimanesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs10071119</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="1119">2018. 1119</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Monitoring water surface and level of a reservoir using different remote sensing approaches and comparison with dam displacements evaluated via GNSS</title>
		<author>
			<persName><forename type="first">C</forename><surname>Pipitone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maltese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dardanelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brutto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Loggia</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs10010071</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Global and local real-time anomaly detectors for hyperspectral remote sensing imagery</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs70403966</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3966" to="3985" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Multiscale</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11060695</idno>
	</analytic>
	<monogr>
		<title level="m">Deep Middle-level Feature Fusion Network for Hyperspectral Classification</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improved Capability in Stone Pine Forest Mapping and Management in Lebanon Using Hyperspectral CHRIS-Proba Data Relative to Landsat ETM+</title>
		<author>
			<persName><forename type="first">M</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Jomaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Arab</surname></persName>
		</author>
		<idno type="DOI">10.14358/PERS.80.8.725</idno>
	</analytic>
	<monogr>
		<title level="j">Photogramm. Eng. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="725" to="731" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Atmospheric correction for hyperspectral ocean color retrieval with application to the Hyperspectral Imager for the Coastal Ocean (HICO)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Knobelspiesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Proctor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhai</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.rse.2017.10.041</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Environ</title>
		<imprint>
			<biblScope unit="volume">204</biblScope>
			<biblScope unit="page" from="60" to="75" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A novel change detection method for multitemporal hyperspectral images based on binary hyperspectral change vectors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bovolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="4913" to="4928" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Classification of hyperspectral remote sensing images with support vector machines</title>
		<author>
			<persName><forename type="first">F</forename><surname>Melgani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2004.831865</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1778" to="1790" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semisupervised hyperspectral image segmentation using multinomial logistic regression with active learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2010.2060550</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="4085" to="4098" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spectral-spatial hyperspectral image segmentation using subspace multinomial logistic regression and Markov random fields</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2011.2162649</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="809" to="823" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Random-selection-based anomaly detector for hyperspectral imagery</title>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2010.2081677</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1578" to="1589" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Target detection based on a dynamic subspace</title>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="344" to="358" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generalized composite kernel framework for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Marpu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Benediktsson</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2012.2230268</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="4816" to="4829" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gabor-filtering-based nearest regularized subspace for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTARS.2013.2295313</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1012" to="1022" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Classification of hyperspectral images by exploiting spectral-spatial information of superpixel via multiple kernels</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Benediktsson</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2015.2445767</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="6663" to="6674" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Composite kernels for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Camps-Valls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gomez-Chova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Muñoz-Marí</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vila-Frances</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Calpe-Maravilla</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2005.857031</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="93" to="97" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stereo r-cnn based 3d object detection for autonomous driving</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-20">16-20 June 2019</date>
			<biblScope unit="page" from="7644" to="7652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Bridging the Gap between Training and Inference for Neural Machine Translation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02448</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning a Deep ConvNet for Multi-label Classification with Partial Labels</title>
		<author>
			<persName><forename type="first">T</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mehrasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-20">16-20 June 2019</date>
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning-based classification of hyperspectral data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTARS.2014.2329330</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2094" to="2107" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised spectral-spatial feature learning with stacked sparse autoencoder for hyperspectral imagery classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2438" to="2442" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spectral-spatial classification of hyperspectral image based on deep auto-encoder</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Geng</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTARS.2016.2517204</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="4073" to="4085" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recursive autoencoders-based unsupervised feature learning for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2017.2737823</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1928" to="1932" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spectral-spatial classification of hyperspectral data based on deep belief network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTARS.2015.2388577</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="2381" to="2392" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Spectral-spatial feature extraction for hyperspectral image classification: A dimension reduction and deep learning approach</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2016.2543748</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="4544" to="4554" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Going deeper with contextual CNN for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2017.2725580</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="4843" to="4855" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep feature extraction and classification of hyperspectral images based on convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ghamisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="6232" to="6251" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07-01">26 June-1 July 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spectral-spatial residual network for hyperspectral image classification: A 3-D deep learning framework</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="847" to="858" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A Fast Dense Spectral-Spatial Convolution Network Framework for Hyperspectral Images Classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs10071068</idno>
		<imprint>
			<date type="published" when="1068">2018. 1068</date>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sens</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Hyperspectral Images Classification Based on Dense Convolutional Networks with Spectral-Wise Attention Mechanism</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11020159</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">159</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sens</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Double-Branch Multi-Attention Mechanism Network for Hyperspectral Image Classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11111307</idno>
		<imprint/>
	</monogr>
	<note>Remote Sens. 2019, 11, 1307. [CrossRef</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-16">16 October 2018</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep recurrent neural networks for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ghamisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2016.2636241</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="3639" to="3655" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A novel semi-supervised hyperspectral image classification approach based on spatial neighborhood information and classifier combination</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Du</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2015.03.006</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="19" to="29" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised feature extraction in hyperspectral images based on wasserstein generative adversarial network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2018.2876123</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="2669" to="2688" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Active learning with convolutional neural networks for hyperspectral image classification using a new bayesian approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Haut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paoletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2018.2838665</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="6440" to="6461" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Capsule networks for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Paoletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Haut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fernandez-Beltran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pla</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2018.2871782</idno>
	</analytic>
	<monogr>
		<title level="j">Ieee Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="2145" to="2160" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Self-paced learning-based probability subspace projection for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2018.2841009</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="630" to="635" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Self-taught feature learning for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2017.2651639</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="2693" to="2705" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">SuperBF: Superpixel-Based Bilateral Filtering Algorithm and Its Application in Feature Extraction of Hyperspectral Images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2019.2938397</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="147796" to="147807" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICML 2015 32nd International Conference on Machine Learning</title>
		<meeting>the ICML 2015 32nd International Conference on Machine Learning<address><addrLine>Lile, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07">July 2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The dynamic representation of scenes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rensink</surname></persName>
		</author>
		<idno type="DOI">10.1080/135062800394667</idno>
	</analytic>
	<monogr>
		<title level="j">Vis. Cogn</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="17" to="42" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems<address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12">December 2014</date>
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07">July 2015</date>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Attngan: Fine-grained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="page" from="1316" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><surname>Mish</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08681</idno>
		<title level="m">A Self Regularized Non-Monotonic Neural Activation Function</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems<address><addrLine>Lake Tahoe, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12">December 2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><surname>Sgdr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
