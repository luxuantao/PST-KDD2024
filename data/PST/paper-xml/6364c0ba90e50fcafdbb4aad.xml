<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Berti: an Accurate Local-Delta Data Prefetcher</title>
				<funder ref="#_QEHujP8">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
				<funder ref="#_rVcEe4r #_RjYWKWX">
					<orgName type="full">Government of Aragon</orgName>
				</funder>
				<funder ref="#_8JXKGs4 #_qg4xykk">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Agust?n</forename><surname>Navarro-Torres</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. Inform?tica e Ingenier?a de Sistemas -I3A</orgName>
								<orgName type="institution">Universidad de Zaragoza</orgName>
								<address>
									<settlement>Zaragoza</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Biswabandan</forename><surname>Panda</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Bombay</orgName>
								<address>
									<settlement>Mumbai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jes?s</forename><surname>Alastruey-Bened?</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. Inform?tica e Ingenier?a de Sistemas -I3A</orgName>
								<orgName type="institution">Universidad de Zaragoza</orgName>
								<address>
									<settlement>Zaragoza</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pablo</forename><surname>Ib??ez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. Inform?tica e Ingenier?a de Sistemas -I3A</orgName>
								<orgName type="institution">Universidad de Zaragoza</orgName>
								<address>
									<settlement>Zaragoza</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">V?ctor</forename><surname>Vi?als-Y?fera</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. Inform?tica e Ingenier?a de Sistemas -I3A</orgName>
								<orgName type="institution">Universidad de Zaragoza</orgName>
								<address>
									<settlement>Zaragoza</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Alberto</forename><surname>Ros</surname></persName>
							<email>aros@ditec.um.es</email>
							<affiliation key="aff2">
								<orgName type="department">Computer Engineering Department</orgName>
								<orgName type="institution">University of Murcia</orgName>
								<address>
									<settlement>Murcia</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Berti: an Accurate Local-Delta Data Prefetcher</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data prefetching is a technique that plays a crucial role in modern high-performance processors by hiding long latency memory accesses. Several state-of-the-art hardware prefetchers exploit the concept of deltas, defined as the difference between the cache line addresses of two demand accesses. Existing delta prefetchers, such as best offset prefetching (BOP) and multi-lookahead prefetching (MLOP), train and predict future accesses based on global deltas. We observed that the use of global deltas results in missed opportunities to anticipate memory accesses.</p><p>In this paper, we propose Berti, a first-level data cache prefetcher that selects the best local deltas, i.e., those that consider only demand accesses issued by the same instruction. Thanks to a high-confidence mechanism that precisely detects the timely local deltas with high coverage, Berti generates accurate prefetch requests. Then, it orchestrates the prefetch requests to the memory hierarchy, using the selected deltas.</p><p>Our empirical results using ChampSim and SPEC CPU2017 and GAP workloads show that, with a storage overhead of just 2.55 KB, Berti improves performance by 8.5% compared to a baseline IP-stride and 3.5% compared to IPCP, a state-of-theart prefetcher. Our evaluation also shows that Berti reduces dynamic energy at the memory hierarchy by 33.6% compared to IPCP, thanks to its high prefetch accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Data prefetching techniques play an important role in hiding long-latency memory accesses. Hardware prefetchers learn memory access patterns and fetch data into the cache hierarchy before time so that future memory accesses get cache hits. Data prefetching techniques can be employed either at the private first-level data cache (L1D), second-level cache (L2), or at the shared last-level cache (LLC).</p><p>Most of the recently proposed storage-efficient spatial prefetchers target L2 <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b49">[50]</ref>. Exceptions are the multi-lookahead offset prefetching (MLOP) <ref type="bibr" target="#b47">[48]</ref> and the instruction pointer classifier-based prefetching (IPCP) <ref type="bibr" target="#b39">[40]</ref>, which are L1D prefetchers. It is well known that an L1D prefetcher provides better performance than an L2 prefetcher as the prefetched lines are brought into L1D and not till L2. In addition, an L1D prefetcher sees unfiltered memory access patterns and can predict the future  Figure <ref type="figure">1</ref>. Prefetch accuracy and dynamic energy consumption of the memory hierarchy for state-of-the-art prefetchers (IPCP <ref type="bibr" target="#b39">[40]</ref>, MLOP <ref type="bibr" target="#b47">[48]</ref>, SPP-PPF <ref type="bibr" target="#b16">[17]</ref>, and Bingo <ref type="bibr" target="#b12">[13]</ref>) averaged across single-threaded traces from memory-intensive SPEC CPU2017 <ref type="bibr" target="#b54">[55]</ref> and GAP <ref type="bibr" target="#b13">[14]</ref> workloads.</p><p>accesses better than an L2 or LLC prefetcher. L1D also sees a sequence of virtual addresses as compared to physical addresses at the L2 and LLC, which can facilitate crosspage prefetching <ref type="bibr" target="#b23">[24]</ref>. Also, compared to L1D, additional contextual information is not easy to propagate to L2 and LLC, such as instruction pointer (IP) <ref type="bibr" target="#b35">[36]</ref>, which is usually available at the L1D (e.g., Intel's IP-stride at the L1D <ref type="bibr" target="#b19">[20]</ref>). However, designing a high-performance L1D prefetcher is hindered by (i) storage overhead, (ii) starved L1D bandwidth, (iii) L1D pollution because of inaccurate prefetching, and (iv) narrow scope for aggressive prefetching because of the limited size of the prefetch queue (PQ) and the miss status holding registers (MSHR).</p><p>State-of-the-art data prefetchers push the limit of singlethread performance with average performance boosts of 3% to 5% <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b39">[40]</ref>. However, as shown in Figure <ref type="figure">1</ref>(a), these prefetchers load a large number of useless blocks, ranging from 22.6% to 35.1% for SPEC CPU2017 and from 35.8% to 81.2% for GAP workloads, which results in sub-optimal performance and additional dynamic energy consumption <ref type="bibr" target="#b33">[34]</ref>. <ref type="bibr">Figure 1(b)</ref> shows that state-ofthe-art prefetchers significantly increase the dynamic energy consumption at the memory hierarchy (caches and DRAM) up to 30.1% and 86.9% for SPEC CPU2017 and GAP workloads, respectively.</p><p>Our proposal, Berti, provides an accuracy of almost 90%, which translates into a dynamic energy overhead of only 9.0% and 14.3% for SPEC CPU2017 and GAP, respectively.</p><p>Our approach. We ask the following simple question in designing our approach: "for an L1D access to address X, what is the timely and accurate delta (d) that should be used for prefetching?" The best offset prefetcher (BOP) inspires us to ask this question <ref type="bibr" target="#b37">[38]</ref>. However, our approach is different from BOP and other offset prefetchers <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b47">[48]</ref>. Our key observation is that the best delta for access is dependent on the local contextual information, such as an instruction pointer (IP), and it varies based on the context (e.g., the best delta for IP X is different from IP Y ). We argue that prefetching based on global (context-agnostic) deltas results in missing opportunities <ref type="bibr" target="#b38">[39]</ref>.</p><p>We propose Berti, a cost-effective, per-IP best request time delta L1D prefetcher that makes a strong case for timeliness and accuracy. For each IP, Berti learns the deltas that result in timely prefetch requests, and issue prefetch requests only for the deltas predicted to provide high coverage, which translates to overall high prefetch accuracy. Sited at the L1D and seeing all virtual addresses generated by the processor, Berti orchestrates the prefetch requests to the memory hierarchy. Our Berti prefetcher is inspired by Berti from DPC-3 <ref type="bibr" target="#b45">[46]</ref>.</p><p>Accurate and timely local deltas. We define local delta as the difference in cache line addresses between two demand accesses that are issued by the same IP. The definition of delta differs from the definition of stride, being the later the difference between addresses of consecutive load accesses with the same IP. For example, an IP X that accesses the following cache line addresses: X, X+2, X+4, X+6, sees a sequence of strides as follows: +2, +2, and +2. In this case, the stride is 2. However, access X+6 sees the following deltas: +6, +4, and +2. Figure <ref type="figure" target="#fig_3">2</ref> shows an example differentiating strides, local deltas, and timely local deltas. If the goal of a prefetcher is to cover address 15, then the prefetcher can initiate prefetching with deltas +3, +5, and +8 whenever it sees the demand accesses to addresses 12, 10, and 7, respectively. However, if we consider time to prefetch address 15, then deltas of +3, +5, and +8 will not completely mitigate the L1D miss latency, as they will be late prefetch requests. Instead, if a prefetcher issues a request for address 15 with deltas of +10 or +13 on demand accesses to address 5 or 2, respectively, it can prefetch address 15 well ahead of time.  With Berti, we find the timely local deltas, and compute its respective coverage. We prefetch using deltas that used to show high coverage, which translates to overall high prefetch accuracy as we show in this work. We call these deltas the accurate and timely deltas.</p><p>Contributions. We make the following key contributions:</p><p>? We motivate the need for a local L1D delta prefetcher to achieve a high coverage with accurate and timely prefetch requests (Section II). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RECENT WORKS AND MOTIVATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Recent advances in data prefetching</head><p>Data prefetching plays an important role in designing high performance processors. Recent developments in this field mainly come from the last two data prefetching championships, DPC-2 <ref type="bibr" target="#b1">[2]</ref> and DPC-3 <ref type="bibr" target="#b6">[7]</ref>, co-located with ISCA 2015 and ISCA 2019, respectively.</p><p>Best offset prefetching (BOP). The winner of DPC-2 is a degree-one L2 prefetcher that finds an offset that provides the maximum likelihood of future use at the L2 cache <ref type="bibr" target="#b37">[38]</ref>. An offset of k means that a cache line is k cache lines away from the current demand address. BOP takes timeliness into account while selecting the best offset per application phase. Multi-lookahead offset prefetching (MLOP) <ref type="bibr" target="#b47">[48]</ref> is an extension on BOP that is motivated by Jain's Ph.D. thesis <ref type="bibr" target="#b28">[29]</ref>. MLOP considers multiple lookaheads for each offset and selects the offset and lookahead covering a specific cache miss. Both BOP and MLOP treat the demand addresses in isolation, and for each demand access, trigger prefetch requests based on the prefetch offset <ref type="foot" target="#foot_0">1</ref> . In general, MLOP provides better prefetch coverage than BOP.</p><p>Variable Length Delta Prefetching (VLDP). This spatial data prefetcher uses multiple histories of deltas between successive cache lines observed within an operating system (OS) page to predict the future memory accesses in other OS pages <ref type="bibr" target="#b49">[50]</ref>. One of the key features of VLDP is that it uses multiple prediction tables and makes predictions based on different lengths of history in terms of deltas.</p><p>Signature path prefetching (SPP). This state-of-the-art delta prefetcher predicts irregular strides at the L2 cache <ref type="bibr" target="#b34">[35]</ref>. SPP works by relying on the signatures (hashes of consecutive strides) observed within an OS page to index into a table that predicts future deltas. SPP uses a lookahead mechanism that recursively finds out deltas to prefetch until a delta falls below a confidence. Perceptron prefetch filtering (PPF) is a filter that further improves the effectiveness of SPP by deciding whether to prefetch into L2 or not <ref type="bibr" target="#b16">[17]</ref>. In general, SPP combined with PPF (SPP-PPF) provides better prefetch coverage than VLDP.</p><p>Bingo. This L2 prefetcher makes a case for associating spatial access patterns to both short (such as IP) and long events (such as IP, IP+offset, and memory region) and selecting the best pattern for prefetching <ref type="bibr" target="#b12">[13]</ref>. A key point of Bingo is the use of only one hardware table for both short and long events. This table enables multiple predictions from a single entry, providing better coverage than singleevent prefetching. In general, Bingo outperforms VLDP and SPP-PPF for SPEC CPU2017 traces. However, it requires significantly more storage than VLDP and SPP-PPF.</p><p>Instruction pointer classifier prefetching (IPCP). The winner of DPC-3 is a state-of-the-art L1D data prefetcher that is composite in nature <ref type="bibr" target="#b39">[40]</ref>. It classifies an IP into three classes: constant stride (CS), complex stride (CPLX), and global stream (GS). IPCP uses three lightweight prefetchers that issue prefetch requests according to the IP class. If it fails to classify an IP into one of the three classes, it uses a next-line prefetcher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Motivation: why a new delta prefetcher?</head><p>Why not a global delta prefetcher? We observe that finding the best delta for an entire application results in missing opportunities because the best delta varies based on the program context, e.g., an IP or the OS page. Figure <ref type="figure">3</ref> shows the best deltas selected by BOP (red line) and Berti (gray lines) for different IPs of the mcf-1554B benchmark. We can see that the best delta is different for distinct IPs making a strong case for prefetching local, timely deltas instead of a global best delta (oblivious to per-IP best deltas). We can also see that the global delta (+62) as selected by BOP does not cover all cache accesses, and it is not the best delta. For mcf-1554B, BOP provides coverage of only 2%, whereas Berti, that selects local deltas (per IP), provides better coverage as shown in Figure <ref type="figure">3</ref>. As we will see in Section IV, the use of a global delta may be beneficial in some cases (e.g., in CactuBSSN), but this is not the common case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IP</head><p>Why not existing local L1D prefetchers? A conventional IP-stride prefetcher covers consecutive constant strides and not necessarily timely deltas. For example, IP 0x401cb0 from lbm-2676B generates the following stride sequence: +1, +2, +1, +2, ... +1, +2. For this pattern, an IP-stride prefetcher will provide zero coverage and will not gather enough confidence to prefetch either with stride +1 or +2. IPCP's CPLX prefetcher will be able to detect this pattern. However, IPCP ignores the timeliness of prefetching. In contrast to IP-stride and IPCP's CPLX prefetcher, a more timely, accurate, and flexible approach would be to prefetch with deltas +3 or +6 that provide 100% coverage. Moreover, for the irregular stride sequence: -1, -5, -2, -1, -4, -1 associated to IP 0x402dc7 from mcf-1554B, IPCP's CPLX prefetcher fails to predict a pattern through its lookahead based on confidence. However, a local delta prefetcher with a delta of -1 can provide better coverage.</p><p>Effect of out-of-order loads at the L1D. In an out-oforder processor, memory accesses get reordered due to outof-order scheduling. Hence, the training of a delta prefetcher may be affected by the ordering of memory accesses. Let's consider a loop with a single IP accessing memory addresses 1, 2, 3, 4, 5, 6, and 7 with constant strides of +1, +1, +1, +1, +1, +1. An out-of-order processor can reorder, for example, the accesses to addresses 2 and 3, resulting in the following sequence of addresses: 1, 3, 2, 4, 5, 6, 7 and strides +2, -1, +2, +1, +1, +1 at the L1D. This cannot be covered by an IP-stride or an IPCP's CPLX prefetcher unless a specific mechanism can provide the commit order <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b55">[56]</ref>. However, timely deltas have the important property of seeing past accesses already in order, thus there is no requirement of such in-order commit mechanisms. Indeed, the last three accesses in our example will see the following past deltas: address 5 will see +4, +2, +3, +1, address 6 will see +5, +3, +4, +2, +1, and address 7 will see +6, +4, +5, +3, +2, +1 that is, all possibilities regardless of their order. The prefetcher then can choose the timely deltas that can provide the best coverage from these set of values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. BERTI: A LOCAL-DELTA PREFETCHER</head><p>Berti is a data prefetcher sited at the L1D, where it can see all the requests generated by the processor and orchestrate the prefetch requests to the memory hierarchy. Berti makes a strong case for prefetch accuracy. For each IP, it selects the deltas<ref type="foot" target="#foot_1">2</ref> that are timely and computes their respective local coverage. High accuracy is achieved by only using deltas with high coverage. Additionally, Berti is trained with virtual addresses, which helps in finding larger deltas and facilitates cross-page prefetching. Next, we describe how Berti performs training and prediction. Then, we propose a simple and cost-effective hardware implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training the prefetcher</head><p>The goal of the training mechanism is to estimate the coverage of each seen delta, considering only those deltas that would result in a timely prefetch. The training consists of the following actions: measuring fetch latency, learning timely deltas, and computing the coverage of the deltas.</p><p>Measuring fetch latency. In order to learn the deltas that are timely it is necessary to measure the time required to fetch data to the L1D, i.e., the L1D miss latency. This measurement is performed for any cache line in L1D, both for demand misses and prefetch requests. Computing latency for prefetch requests is fundamental because, in an ideal scenario, there would not be L1D misses but just L1D hits due to timely prefetch requests. In addition, the latency of prefetch requests may be larger than the latency of demand requests due to prefetch queue (PQ) contention or L1D port contention. Fetch latency can be measured by keeping a timestamp for any L1D miss inserted into the MSHR and any prefetch request inserted into the PQ. On an L1D fill, the latency is simply computed by subtracting the stored timestamp from the current one.</p><p>Learning timely and accurate deltas. Once the fetch latency is obtained for each L1D fill, our prefetcher can precisely learn timely deltas, given that the history of accesses and timestamps by the same IP is recorded. By searching in the history of recent accesses and comparing the timestamp of each previous access with the timestamp when a prefetch should be issued to be timely, the accesses that would trigger timely prefetch requests are detected. Deltas are then  computed by subtracting the address of each timely request in the history from the current address. Figure <ref type="figure" target="#fig_5">4</ref> depicts how timely deltas are detected. All addresses represented in the timeline are accessed by the same IP. When address 10 is demanded and its fetch latency computed (Figure <ref type="figure" target="#fig_5">4a</ref>), the history of accesses for that IP is searched, from the point in time, a timely prefetch should have been triggered. In this case, no previous accesses are found. After accessing address 12 and computing its fetch latency (Figure <ref type="figure" target="#fig_5">4b</ref>), a timely delta corresponding to address 2 is found. That is, address 2 should initiate the prefetch request for 12 in order to be timely. The timely delta +10 is therefore learned. Similarly, when computing the latency for access 15 (Figure <ref type="figure" target="#fig_5">4c</ref>), two deltas, +10 and +13, are detected as timely.</p><p>Berti triggers the procedure to learn timely deltas for each miss that would have occurred in the baseline, which translates to two scenarios. First, when a demand miss fills the L1D with the requested data. Second, when a cache line brought into L1D by a prefetch request is demanded (i.e., misses that would have occurred without a prefetcher). Berti does not learn deltas on a cache fill caused by a prefetch request since its demand time is not known. Therefore, it is necessary to keep the latency of prefetch requests until the core demands the cache line.</p><p>Computing the coverage of deltas. On every search in the history, Berti obtains a set of timely deltas. Deltas that frequently appear in the searches would cover a significant fraction of misses, while deltas that rarely appear would result in low coverage. It is easy to compute the coverage by dividing the number of occurrences of a delta by the number of searches in the history. For example, in Figure <ref type="figure" target="#fig_5">4</ref>, after three accesses, the delta +10 has the higher coverage, being in two out of three searches (66.7%). If the same access pattern continues, the delta +10 will reach close to 100% coverage. It is important to note that this local (per IP) coverage translates into accuracy. If a delta covers 100% of cache lines, since each access-delta pair results in only one prefetch request, that delta will bring 100% accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Prediction: issuing prefetch requests</head><p>Once we know the deltas and their associated coverage, we can orchestrate the prefetch requests across the cache hierarchy. Based on both the coverage of each delta and the L1D MSHR occupancy, we decide which deltas to use and till which cache level to prefetch. We use four watermarks to decide where to issue the prefetch requests. If the coverage of a delta is above a high-coverage watermark and the L1D MSHR occupancy is below the occupancy watermark, then prefetch requests using that delta get filled at all the cache levels till L1D. Otherwise, if the coverage is above a medium-coverage watermark, irrespective of the L1D MSHR occupancy, prefetch requests get filled till L2. Finally, if the coverage is above a low-coverage watermark, requests get filled only in the LLC.</p><p>To generate a prefetch request, we add the selected delta to the address of the current access and the resulting address is inserted in the PQ. Requests in the PQ are processed in a first-in-first-out (FIFO) order. Since our prefetcher is trained with virtual addresses, the generated prefetch requests are also in the virtual address space. A prefetch request obtains the physical address from the L2 translation look-ahead buffer (STLB). If the translation misses in the STLB, the prefetch request is dropped. If the translation is obtained, the prefetch request checks if the target block is already present in the cache it wants to fill. In case of a miss, the block is prefetched, and the request is inserted into the MSHR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Hardware implementation</head><p>As outlined in Figure <ref type="figure">5</ref>, Berti can be implemented with a small hardware budget and using simple structures and logic. Next, we describe the structures required to train the prefetcher and decide on the prefetch requests to issue to each cache level.</p><p>Measuring fetch latency. In order to be able to measure the fetch latency, the MSHR is extended with a 16-bit field (represented in Figure <ref type="figure">5</ref> in gray) that stores a timestamp on a demand L1D miss. Similarly, the PQ is also extended with an analogous field that stores the timestamp when a new prefetch request is added. The timestamp can be obtained from the clock of the local processor <ref type="bibr" target="#b46">[47]</ref> or any other metric to approximate time (e.g., number of cache accesses). In our implementation, we use the former. When a prefetch request misses L1D, the timestamp is transferred from the PQ to the newly allocated MSHR entry. On an L1D fill, the latency of the request can be computed with a simple subtraction. The latency is stored using 12 bits. If an overflow is detected when computing the latency, it is set to zero, and therefore not Learning timely deltas. To be able to learn timely deltas, the most recent accesses need to be tracked. The History table (see Figure <ref type="figure">5</ref>) records that information and is organized as an 8-set, 16-way cache with a FIFO replacement policy and indexed and searched with the IP. The format of each entry in the history table is depicted in Figure <ref type="figure">6</ref>. Each entry keeps a tag corresponding to the seven least significant bits of the IP (after removing the bits used for indexing the cache), the 24 least significant bits of the target cache line address, and a 16-bit timestamp. A new entry is inserted in the history table (Write port in Figure <ref type="figure">5</ref>) either on-demand misses (Miss arrow from the L1D in Figure <ref type="figure">5</ref>) or on hits for prefetched cache lines (Hit p in Figure <ref type="figure">5</ref>). The virtual address (VA) and the IP (IP, VA arrow in Figure <ref type="figure">5</ref>) are stored in the new entry along with the current timestamp (not shown in the figure).</p><p>The search for timely deltas (Search port in Figure <ref type="figure">5</ref>) is performed either on a fill due to a demand access (Fill arrow from the MSHR in Figure <ref type="figure">5</ref>) or on a hit due to a prefetched cache line (Hit p in Figure <ref type="figure">5</ref>). In the first case, the search is done using the information from the MSHR (IP, VA, latency arrow in Figure <ref type="figure">5</ref>). In order to enable the search on L1D hits, we keep the latency of the prefetch request (12 bits) along with each entry in the L1D (see Figure <ref type="figure">5</ref> L1D shadow part). Alternatively, an L1D shadow tag could be employed. A latency field set to zero indicates either an overflow when computing the latency or an already demanded cache line. In that case, a search in the history table is not performed. Otherwise, the search is done when the demand hit takes place, using the stored latency (Latency arrow in Figure <ref type="figure">5</ref>), which is reset after the search. On every search, the 16-ways of the history table are looked up for a matching IP tag. A maximum of eight timely deltas, the ones corresponding to the youngest entries that would result in timely prefetch requests, are collected.</p><p>Computing the coverage of deltas. The results of each search in the history table (Timely deltas arrow in Figure <ref type="figure">5</ref>) are accumulated in the Table of deltas, a 16-entry fullyassociative cache with a FIFO replacement policy. The format of each entry in the table of deltas is depicted in Figure <ref type="figure">6</ref>. Each entry consists of a 10-bit tag (based on hash function of the IP), a 4-bit counter, and an array of 16 deltas, each of them containing the delta itself (13 bits), the coverage (4 bits), and the status (2 bits) indicating till which cache level to prefetch. The counter is increased on each search in the history table. For each timely delta found during the search, its coverage counter is increased. When the counter overflows (its value increases to 16), we compute the coverage. Deltas that cross the high-coverage watermark (65% of coverage, i.e., a coverage value higher than 10) set their status to L1D pre f . Deltas in between the high-coverage watermark and the medium-coverage watermark (between 65% and 35%, i.e., a coverage value lower or equal than 10 and higher than 5) set the status to L2 pre f . The maximum number of deltas selected for any of those status is bounded to 12. The remaining deltas' status is set to No pre f (i.e., do not issue prefetch requests for this delta). Once the status is set, the counter and the array of confidences are reset, and a new learning phase begins.</p><p>While warming-up the status fields, prefetch requests are also issued if at least eight deltas have been gathered, increasing the high-coverage watermark to 80%, as with just four deltas the prefetcher needs more confidence. Our empirical study shows that using watermarks higher that 65% leads to high accuracy.</p><p>Although Berti opens the possibility of prefetching to LLC only for low-coverage deltas, our evaluation showed no performance improvements when choosing this option. Hence, we set the low-coverage watermark to 35% (equal to the medium-coverage one), to disable prefetching to LLC only.</p><p>In order to constantly learn new deltas, evictions of deltas may be necessary. On the arrival of a non stored delta, deltas with less than 50% coverage in the previous phase are candidates for evictions in the current phase. To this end, if the coverage when selecting the L2 pre f status is lower than 50%, the status is set to L2 pre f repl. The eviction policy selects the delta with lower coverage whose status is L2 pre f repl or No pre f . In case no such delta exists, the new delta is discarded.  <ref type="figure">5</ref>). Since we use an L1D with two read ports and one write port, the table of deltas requires three search ports. The deltas with status L1D pre f or L2 pre f are added to the current VA to form the prefetch requests that are inserted into the PQ (Pref. requests arrow in Figure <ref type="figure">5</ref>). Those prefetch requests get filled into all cache levels till L1D when the status is L1D pre f and the MSHR occupancy is below 70% (the occupancy watermark). Otherwise, prefetch requests get filled till L2.</p><p>Storage overhead. Berti does not require any complex operation (e.g., multiplications) nor complex logic. Our history table has two read ports and one write port. The latency of this structure is two cycles, based on CACTI-P <ref type="bibr" target="#b36">[37]</ref>. Since prefetching training is out of the critical path of memory accesses, the history table does not affect the cycle time. The storage requirements of Berti, whose breakdown per structure is provided in Table <ref type="table" target="#tab_3">I</ref>, is just 2.55 KB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EVALUATION A. Simulation Methodology</head><p>We use the recently modified version of ChampSim <ref type="bibr" target="#b8">[9]</ref>, a trace-driven simulator used for the 2nd and 3rd Data Prefetching Championships (DPC-2 <ref type="bibr" target="#b1">[2]</ref> and DPC-3 <ref type="bibr" target="#b6">[7]</ref>). Recent prefetching proposals <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b47">[48]</ref> are also coded and evaluated on ChampSim. The recently modified ChampSim extends the one provided with the DPC-3 with a decoupled front-end <ref type="bibr" target="#b44">[45]</ref> and a detailed memory hierarchy support for address translation that further improve the baseline performance. Caches are non-inclusive, although Berti can work similarly with exclusion policies just by bypassing the allocation of memory blocks at the LLC. We faithfully model DRAM, including the queuing delays that contributes to the variable access time because of close vs. open page, page hit vs miss, DRAM bank conflicts, etc. Table II summarizes our system configuration, mimicking an Intel Sunny Cove microarchitecture <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b22">[23]</ref>.</p><p>We evaluate Berti with single-core and multi-core simulations. We warm-up the caches for 50M sim-point in-structions <ref type="bibr" target="#b48">[49]</ref> and collect statistics for the next 200M sim-point instructions. For multi-core simulations, we use heterogeneous mixes of single-threaded traces. For each mix, when a core finishes its 200M instructions, it gets replayed until all the cores finish their respective 200M instructions. For both single-and multi-core, we report performance in terms of IPC improvement (speedup) with respect to an L1D with an IP-stride prefetcher. We use the geometric mean to average the speedups obtained by the different single-thread traces.</p><p>Energy model. We also report the dynamic energy consumption of the memory hierarchy. We obtain the energy consumption of reads and writes to tag and data arrays at each cache level and DRAM with CACTI-P <ref type="bibr" target="#b36">[37]</ref> and Micron DRAM power calculator <ref type="bibr" target="#b2">[3]</ref>. Then, we compute the total energy expenditure by accounting for the number of accesses of each type across the memory hierarchy. We use 22 nm process technology for our energy calculations.</p><p>Workloads. We use traces from SPEC CPU2017 <ref type="bibr" target="#b54">[55]</ref> and single-threaded GAP benchmarks <ref type="bibr" target="#b13">[14]</ref>. We limit our study to memory-intensive traces (MemInt), i.e., those that showed at least one miss per kilo-instruction (MPKI) at the LLC in our modeled baseline system. All GAP traces <ref type="bibr" target="#b19">(20)</ref> and 44 SPEC CPU2017 traces are memory-intensive. SPEC CPU2017 traces were generated with the reference inputs. Both real (Twitter, Web, Road) and synthetic (Kron, Urand) graphs were used as input for the GAP benchmarks.</p><p>We also report performance for the CloudSuite benchmarks <ref type="bibr" target="#b21">[22]</ref>. All traces are publicly available [4], <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>. For multi-core experiments, we simulate 200 random heterogeneous mixes from SPEC CPU2017 and GAP.</p><p>Berti and variable cache fill latency. Modern memory hierarchies can have variable cache fill latency that comes from sources like MSHR contentions at the private and shared caches, read queue (RQ) and write queue (WQ) contentions at various levels of caches and DRAM controller. At the DRAM level, memory access time gets affected because of row buffer conflicts, bank conflicts, etc. Our simulator reflects all this variability. One of the primary reasons we propose Berti is because of the variable response latency. Even non uniform cache access (NUCA) LLCs with multiple banks can cause variable fill latency. For example, suppose in a NUCA LLC with two banks an IP sees local deltas of +1 and +2 that get mapped to bank-1 and bank-0, respectively, and the latency to bank-0 is different from bank-1. Even in this case, Berti is able to learn the best deltas while looking at the history, facilitating timely and accurate prefetching. Note that in our experiments, the fill latency ranges from 22 to 2098 cycles with an average of 278 cycles averaged across SPEC CPU2017, GAP, and CloudSuite benchmarks and multicore mixes.</p><p>Evaluated Prefetching Techniques. We compare the effectiveness of Berti with high performing L1D and L1D+L2 prefetchers. As Berti is an L1D prefetcher, we first compare  <ref type="bibr" target="#b12">[13]</ref> 2 KB region, 64/128/4K-entry FT/AT/PHT MLOP <ref type="bibr" target="#b47">[48]</ref> 128-entry AMT, 500-update, 16-degree IPCP <ref type="bibr" target="#b39">[40]</ref> 128-entry IP its performance with prefetchers designed for L1D (no prefetching at the L2), and then with multi-level prefetching combinations. The L1D prefetchers are i) MLOP <ref type="bibr" target="#b47">[48]</ref> (DPC-3, 3rd place), an extension of the BOP (DPC-2 winner), and ii) IPCP (DPC-3 winner published at ISCA 2020 <ref type="bibr" target="#b39">[40]</ref>). For multi-level prefetching, we evaluate two state-of-the-art L2 prefetchers along with MLOP and Berti at the L1D: SPP-PPF <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b34">[35]</ref> and Bingo <ref type="bibr" target="#b12">[13]</ref>. We also compare with a multi-level IPCP that uses IPCP both at the L1D and L2. The evaluated prefetchers have been briefly described in Section II-A. For all prefetchers, we use a highly tuned implementation as provided by the authors and tune it again for the parameters mentioned in Table <ref type="table" target="#tab_5">II</ref>. Fine tuning was an easy exercise as all the competing prefetchers use ChampSim for their evaluation. Table <ref type="table" target="#tab_5">III</ref> shows the configurations used for all the evaluated prefetchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Speedup vs. storage requirements</head><p>Figure <ref type="figure">7</ref> summarizes the speedup of the evaluated prefetchers with respect to IP-stride for SPEC CPU2017 and GAP, along with their storage requirements. L1D prefetchers are shown with a circle, L2 prefetchers with a square, and multilevel (L1D+L2) prefetchers with a diamond.</p><p>Among the L1D prefetchers, Berti achieves the highest speedup with a size similar to IPCP, the prefetcher with the lowest storage budget. With only 2.55 KB of storage overhead, Berti improves performance by 8.5% over IP-stride 0 5 10 15 20 <ref type="bibr" target="#b24">25</ref>  The Berti+SPP-PPF multi-level prefetcher obtains the highest speedup (10.2%, additional 1.5% on top of Berti at L1D) among all multi-level combinations with 41.8 KB combined storage for L1D and L2 prefetchers. However, the highlight of Figure <ref type="figure">7</ref> is that Berti at L1D without any prefetching at L2 outperforms all the multi-level prefetching combinations that do not include Berti.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance of Berti as an L1D Prefetcher</head><p>Figure <ref type="figure">8</ref> shows the speedup achieved by the L1D prefetchers for SPEC CPU2017 and GAP. Berti is the best prefetcher across both suites. On average, Berti at the L1D improves performance by 11.6% and 1.9% for SPEC CPU2017 and GAP, respectively. All three prefetchers achieve good speedups for SPEC CPU2017, and Berti outperforms IPCP and MLOP by 2.8% and 3.0%, respectively. The speedup differences are more significant with the GAP traces, where Berti is the only L1D prefetcher that improves IP-Stride, by up to 1.9%, while IPCP and MLOP are 2.9% and 7.8% below, respectively. Overall, across SPEC CPU2017 and GAP traces, Berti outperforms IP-stride and IPCP by 8.5% and 3.5%, respectively. This is significant performance improvement on top of the high-performing state-of-the-art IPCP prefetcher.</p><p>Figure <ref type="figure" target="#fig_6">9</ref> shows the individual speedup for the memoryintensive SPEC CPU2017 and GAP traces. For CPU2017, Berti achieves similar or significantly better results than the other prefetchers on all traces except for CactuBSSN. In this benchmark, we observe that the memory access instructions follow stride patterns. However, there are hundreds of these instructions executing interleaved. Therefore, to track the local behavior of instructions in this benchmark, Berti would need very large history and delta tables. In contrast, prefetchers that detect patterns in the global address stream do not have this problem, as is the case with MLOP or the IPCP GS class. Barring the exception of CactuBSSN where global deltas perform better than local deltas, Berti shows that local deltas are prevalent across a large number of benchmarks and it accurately selects them. A key observation is that the state-of-the-art prefetchers do not consistently improve performance over IP-stride across all workloads, but Berti only shows a small degradation of 2.6% with respect to IP-stride for mcf_s-1536.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SPEC17-</head><p>For SPEC CPU2017, Berti achieves its best result for mcf_s-1554 where it provides speedups of 1.89?, 1.65?, and 1.49? with respect to IP-stride, IPCP, and MLOP, respectively. MLOP and IPCP achieve performance at least 1% below IP-stride on five and eight traces out of 44, the worst case being in mcf_s-782 with drops of 16.0% and 21.9%, respectively. In mcf_s-782, only three IPs (0x4049de, 0x4049e5, and 0x4049cc) represent the 75% of all L1D accesses. MLOP uses a global delta to prefetch that is affected by the interleaving of accesses from these three IPs. IPCP uses the CS and CPLX class prefetchers but with an accuracy below 25%.</p><p>As for GAP, Berti is the best prefetcher for all the benchmarks but three (bfs-8, bfs-10 and bfs-14). It consistently achieves similar or better results than IP-stride for all traces, while MLOP and IPCP perform worse than IP-stride in 12 and 16 benchmarks, respectively. In some cases, the MLOP slowdown is very significant, for example, 17.7% in sssp-3. We have also analyzed the behavior of the prefetchers in one of the GAP applications, namely bc-5. All bc-5 IPs show a rather chaotic memory access pattern except for one that is very regular. IP-stride and Berti, by separately tracing the IPs, detect the regular IP pattern and prefetch correctly for it. They do not prefetch for the other IPs. MLOP fails due to the use of a global delta. The accesses issued by IPs with irregular pattern prevent discovering a global delta and therefore the prefetcher issues very few requests, and is not able to prefetch correctly for the regular IP. IPCP detects the delta pattern for the regular IP through its CPLX component, and prefetches correctly for it. However, the GS component generates many useless prefetches that drastically decreases the accuracy of IPCP and results in the loss of performance shown in Figure <ref type="figure" target="#fig_6">9</ref>.</p><p>Accuracy. Figure <ref type="figure" target="#fig_7">10</ref> shows the accuracy of the L1D prefetchers. Berti is a very accurate prefetcher. On average, about 87.2% of its prefetched lines are useful compared to 62.4% for MLOP and 50.6% for IPCP. The effectiveness of IPCP is driven by the performance of several tiny prefetchers: a global stream prefetcher (GS class), a constant stride Geomean-all corresponds to the geometric mean of all the 95 SPEC CPU2017 traces.</p><p>prefetcher (CS class), and a complex stride prefetcher (CPLX class) that work in tandem. For regular access patterns, the CS prefetcher provides high accuracy. However, for complex access patterns, the effectiveness of the CPLX prefetcher is low, with an accuracy of 52.7% and 9.8% for SPEC CPU2017 <ref type="bibr" target="#b54">[55]</ref> and GAP <ref type="bibr" target="#b13">[14]</ref> workloads, respectively. MLOP, like Berti, is based on the detection of the best timely deltas. However, it achieves much lower accuracy. The improvement of Berti over MLOP is mainly due to two factors: i) MLOP uses global deltas for the whole application while Berti detects different deltas for each IP. As we have shown in Section II-B, benchmarks such as mcf generate different delta patterns for each IP. ii) Berti uses a stringent policy to decide which deltas to use for issuing prefetch requests into L1D, as we have described in Sections III-B and III-C, while MLOP generates prefetch requests for the best delta with each lookahead regardless of its confidence.</p><p>Timeliness. The darker part of each bar in Figure <ref type="figure" target="#fig_7">10</ref> represents the prefetch requests whose retrieved data arrive late to L1D. Almost all prefetch requests generated by Berti are timely, while MLOP and IPCP produce a significant number of late requests. IPCP does not use any mechanism to adapt the prefetch requests timing to the miss latency, while MLOP and Berti do. However, Berti achieves better timeliness than MLOP due to specific and timely deltas for each IP. Coverage. Figure <ref type="figure">11</ref> shows demand misses per kilo instructions (MPKI) at the L1D, L2, and LLC with and without L1D prefetchers. Berti and IPCP achieve a similar reduction of misses in L1D (8.7% in GAP, 33.4% in SPEC CPU2017) and slightly higher than MLOP. However, Berti manages to eliminate more misses than IPCP and MLOP at L2 and LLC due to its line preloading policy directed by the L1D prefetcher. The biggest differences are observed in GAP, where Berti reduces LLC demand misses by 17.7% and 12.4% compared to MLOP and IPCP, respectively. Similarly, Berti reduces L2 demand misses by 6.7% and 5.6% compared to MLOP and IPCP, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Multi-level prefetching performance</head><p>Figure <ref type="figure" target="#fig_3">12</ref> shows the speedup achieved with the multilevel prefetching combinations compared to a system with IP-stride. We select the best five multi-level prefetching combinations out of all possible combinations of L1D and L2 prefetchers. Multi-level prefetching combinations do not offer a significant performance boost. The best multi-level prefetching combinations without Berti are MLOP+Bingo for SPEC CPU2017 and MLOP+SPP-PPF for GAP. In both cases, these combinations achieve a similar speedup to Berti alone, with a storage requirement 22 and 18 times higher, respectively. IPCP at L1D and L2 (IPCP+IPCP), with a meager hardware budget as Berti, achieves a significantly lower speedup than Berti alone at the L1D, especially in GAP, with a difference of 4.6%.</p><p>Adding a prefetcher at the L2 cache along with Berti at the L1D achieves a moderate performance gain. The most significant gain is 2.0% and is obtained with the Berti+Bingo configuration in the memory-intensive subset of SPEC CPU2017 traces. Given the high hardware cost of the L2 prefetchers, the configuration with Berti alone at the L1D seems to be a better design in terms of performance and storage trade-off.</p><p>Coverage. Figure <ref type="figure" target="#fig_9">13</ref> shows demand MPKIs at the L1D, L2, and LLC for the multi-level prefetching combinations. We also show MPKIs without prefetching at L2 for ease of analysis. MLOP+Bingo and MLOP+SPP-PPF decrease MPKI relative to MLOP alone in both L2 and LLC consistently across all suites (maximum reduction in MPKI from 13.8 to 11.7 at L2 for SPEC CPU2017). Adding a prefetcher at L2 is less effective for IPCP and Berti. In both cases, MPKIs at L2 and LLC decrease for SPEC CPU2017 but remain the same or even increase slightly when working with the irregular access patterns of GAP. As a result, the MPKIs at the L2 and LLC achieved by Berti at the L1D are always better than those obtained by multi-level prefetchers with no Berti, except for MLOP+Bingo in SPEC CPU2017.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Memory hierarchy traffic and energy</head><p>Figure <ref type="figure" target="#fig_5">14</ref> shows the traffic between the different levels of the memory hierarchy (demand and prefetch requests) for different prefetching combinations normalized to no prefetching. All prefetchers increase traffic as a result of the useless blocks they request. For the systems with prefetcher only at the L1D, we can observe how the traffic increase is inversely proportional to the prefetcher accuracy for SPEC CPU2017 (refer Figure <ref type="figure" target="#fig_7">10</ref>). Consequently, Berti is the prefetcher with the lowest traffic increase at all levels. For GAP, MLOP increases traffic marginally, despite its low accuracy, because it detects few patterns and generates scarce prefetch requests. Berti increases traffic with L2, LLC and DRAM by 1.0%, 9.2% and 13.9% respectively, whereas IPCP increases traffic at these three levels around 90%.</p><p>The L2 prefetchers Bingo and SPP-PPF added to MLOP and Berti on L1 significantly increase traffic with LLC and DRAM, especially at GAP due to the irregular access patterns. MLOP+Bingo induces 69.0% additional off-chip traffic compared to MLOP alone, while Berti+Bingo adds 67.2% additional off-chip traffic compared to Berti alone.</p><p>Energy efficiency. Figure <ref type="figure">15</ref> shows the average dynamic energy consumption in the memory hierarchy (L1D, L2, LLC, and DRAM) normalized to no prefetching. As expected, there is a direct correlation between traffic and energy consumption overheads in the memory hierarchy. If we focus on the stateof-the-art L1D prefetchers, Berti consumes the least extra energy for SPEC CPU2017 (9.0% vs. 29.1 and 30.1% for MLOP and IPCP), despite achieving the highest speedup (see Figure <ref type="figure">8</ref>). As for GAP, the energy overheads of Berti and MLOP are similar (14.3% vs. 14.2%), and significantly lower than for IPCP (86.9%). Berti is the only prefetcher that manages to translate its dynamic energy increase into speedup. The L2 Bingo and SPP-PPF prefetchers on top of MLOP and Berti significantly increase energy consumption, especially in the case of Bingo for GAP, with increases of over 60% with respect to MLOP and Berti alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Effect of constrained DRAM bandwidth</head><p>So far, we have considered the latest DDR5-6400 channel per four cores that provides 6400 million transfers per second (MTPS) with a per-core DRAM bandwidth of approx. 12.8 GBps. This Section evaluates prefetchers with DRAM bandwidth configurations such as DDR4-3200 (MTPS of 3200) and DDR3-1600 (MTPS of 1600) <ref type="bibr" target="#b18">[19]</ref>. <ref type="bibr">Figures 16 and 17</ref> show the effect of DRAM bandwidth on speedup for L1D and multi-level prefetching, respectively. When moving from 6400 to 1600 MTPS, the performance loss is negligible for all the prefetchers with GAP traces and moderate for SPEC CPU2017 traces (maximum reduction of 4.1% with Berti and Berti+SPP-PPF).   Classification is one benchmark where all the prefetchers fail except Berti, thanks to its high prefetch accuracy. Note that for some of the benchmarks like cloud9 and nutch, even an ideal L1D prefetcher (L1D with a hit rate of 100%) fails to provide significant performance, which shows that there is limited scope for data prefetching. The primary reason for this trend is that the L1D MPKI of CloudSuite without prefetch is low: 6.9 on average, with a maximum of 14.5, while the average L1D MPKI of SPEC and GAP is 42.2 and 83.6, respectively. On the other hand, the L1I MPKI of CloudSuite traces are higher than SPEC and GAP traces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. CloudSuite performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Interaction with a temporal prefetcher</head><p>We simulate managed irregular stream buffer (MISB) prefetcher <ref type="bibr" target="#b58">[59]</ref>, a storage efficient version of ISB <ref type="bibr" target="#b29">[30]</ref> at L2 with MLOP, IPCP, and Berti at L1D, as shown in Figure <ref type="figure" target="#fig_14">19</ref>. ISB is an address correlation-based data prefetcher that correlates cache accesses at a new indirection level named structural address space. Berti with MISB improves the effectiveness of multi-level prefetching for CloudSuite traces, in particular for Cassandra and Classification. For SPEC CPU2017 and GAP, MISB performs worse than SPP-PPF with MLOP and Berti at the L1D. Note that the performance improvement with CloudSuite comes with a storage overhead of 98 KB with MISB, out of which 32 KB is used for the metadata cache and 17 KB for the Bloom filter.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Multi-core performance</head><p>Figure <ref type="figure" target="#fig_15">20</ref> shows speedup on a 4-core simulated system averaged across 200 randomly generated heterogeneous mixes based on memory-intensive SPEC CPU2017 and GAP traces. Among the L1D prefetchers, Berti performs the best with a performance improvement of 16.2%, outperforming both MLOP and IPCP on average. There are only nine mixes in which MLOP and/or IPCP gain more than 10% over Berti, and CactuBSSN is part of seven of these mixes. Berti outperforms competing prefetchers for majority of the mixes that do not have CactuBSSN. Overall, Berti performs better because in the case of multicore systems, per core available DRAM bandwidth goes down because of cross-core contention. Thanks to Berti's timely and accurate deltas, it is still able to deliver high coverage even in the presence of shared DRAM bandwidth contention.</p><p>Berti at L1D also outperforms other multi-level prefetching combinations making a strong case for Berti as an L1D-only prefetcher. Note that Berti outperforms MLOP+Bingo, the combination of the second place and first place prefetchers in the 4-core evaluations at the DPC-3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. Sensitivity to design choices</head><p>Effect of L1 and L2 watermarks. Figure <ref type="figure" target="#fig_16">21</ref> shows the effect of L1 and L2 confidence watermarks on overall speedup averaged across single-core SPEC CPU2017 and GAP benchmarks, normalized to the baseline system. Our chosen watermarks, more than 65% for L1 and in between 35% to 65% for L2 provide the sweet-spot in terms of prefetch accuracy and prefetch coverage. Usage of extremely small or extremely large watermarks affect both coverage and accuracy, and negatively affects speedup. Interestingly, a large</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single-level</head><p>Multi-level 1.08 1.10 1.12 1.14    number of watermark configurations provide benefit in terms of speedup. Our chosen high watermarks provide maximum speedup with the maximum prefetch accuracy.</p><p>Effect of the size of Berti tables. Figure <ref type="figure" target="#fig_3">22</ref> shows the effect of the size of the Berti tables (history table, table of deltas, and the number of deltas) on speedup. Decreasing the size of the table of deltas by a quarter degrades performance by 12.1%, whereas decreasing the number of deltas by a quarter reduces performance by 1.2%. Also, doubling/quadrupling the size of the tables provides a marginal performance gain. CactuBSSN is one outlier where increasing the table sizes to 1024 entries with 1024 sets improve performance by 22%.</p><p>Effect of the latency counter. In our evaluations, we use a 12-bit latency counter per line at the L1D. When we increase its size to 32 bits, performance is not improved. However, using a small 4-bit timestamp, we see a performance drop from 1.16 to 1.07, and from 1.02 to 0.98, for SPEC CPU2017 and GAP, respectively.</p><p>Effect of cross-page prefetching. As Berti is an L1D prefetcher and operates on virtual addresses, it does crosspage prefetching as long as prefetch requests get a hit in the STLB. To understand the utility of cross-page prefetching, we evaluate Berti, where we do not issue prefetch requests (but keep training) that cross an OS page. We see an average performance drop from 1.02 to 1.01 and 1.16 to 1.10 for GAP and SPEC CPU2017 traces, respectively. The performance drop shows that most of the deltas selected by Berti are within the OS page boundary of 4 KB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RELATED WORK</head><p>In Section IV we presented a quantitative comparison of Berti with recent hardware prefetching techniques <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b47">[48]</ref>. In this Section we compare other relevant prefetching techniques qualitatively.</p><p>Temporal prefetchers. Temporal prefetchers track the temporal order of cache-line accesses (and not the deltas) <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b56">[57]</ref>. Temporal prefetchers usually demand hundreds of KBs of storage, which demands the storage of prefetch metadata in the off-chip memory. Some of the recent works on temporal prefetching are in the pursuit of improving the storage overhead without affecting the prefetch coverage <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>. Berti, on the other hand, incurs a storage overhead of just 2.55 KB per core.</p><p>Spatial prefetchers. Compared to temporal prefetchers, spatial prefetchers are lightweight in terms of storage overhead and usually learn memory access patterns within a small spatial region of a few KBs. Conventional prefetchers like stride <ref type="bibr" target="#b19">[20]</ref> and stream <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b52">[53]</ref> are already deployed on commercial processors. Timely Stride prefetching improves the timeliness of conventional stride prefetchers <ref type="bibr" target="#b59">[60]</ref>. However, it does not provide better prefetch coverage when compared with state-of-the-art L1D and L2 prefetching techniques. Spatial prefetchers like Spatial Memory Streaming (SMS) <ref type="bibr" target="#b52">[53]</ref> (similar to Bingo) usually learn single repeating deltas or bit patterns within a spatial region, where a set bit denotes a cache line that should be prefetched. All these techniques do not consider prefetch timeliness.</p><p>Kill the program counter (KPC) proposes a holistic cache replacement and prefetching framework <ref type="bibr" target="#b35">[36]</ref>. However, the prefetching technique is similar to SPP, with similar performance improvements as SPP. Multi-level adaptive prefetching based on performance gradient tracking <ref type="bibr" target="#b43">[44]</ref> (3rd place in DPC-1 <ref type="bibr" target="#b0">[1]</ref>) is one of the first proposals that propose a correlation between an IP and delta sequences. DSPatch <ref type="bibr" target="#b15">[16]</ref> tunes a hardware prefetcher based on available DRAM bandwidth and selects memory access patterns based on prefetch accuracy (if the available DRAM bandwidth is low) and prefetch coverage (if the available DRAM bandwidth is high). Overall, SPP-PPF performs marginally better than SPP+DSPatch.</p><p>Machine learning for hardware prefetching. Machine learning (ML) has been used for microarchitecture research, and ML techniques for data prefetching have been proposed in recent years <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b50">[51]</ref>. In ISCA 2021, a prefetching competition with ML techniques shows that non-ML techniques still outperform with limited storage. However, ML techniques have the potential to learn highly complex memory access patterns, and Pythia <ref type="bibr" target="#b14">[15]</ref> shows that with a high performing L2 prefetcher. Berti is an L1D prefetcher in contrast to Pythia, and with Berti at the L1D, we find negligible performance improvement with Pythia (less than 1%).</p><p>Prefetch filters and throttling mechanisms. Similar to PPF <ref type="bibr" target="#b16">[17]</ref> and DSPatch <ref type="bibr" target="#b15">[16]</ref>, there are proposals that control the aggressiveness of prefetchers by controlling its prefetch degree and distance, or decides whether to prefetch into the L2 or to the LLC <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b53">[54]</ref>. These techniques incur additional storage and perform well for conventional prefetchers with low prefetch accuracy. However, with Berti, the accuracy is significantly higher than prior prefetching techniques, and the implicit confidence mechanism acts like a prefetch throttler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>We proposed Berti and made a case for an L1D prefetcher based on local, timely deltas. Berti learns the best delta to prefetch, keeping timeliness (in the form of time to prefetch an address) and prefetch accuracy in mind. We showed that Berti could learn varieties of memory access patterns. We quantified the effectiveness of Berti across SPEC CPU2017 and GAP workloads, and showed high prefetch accuracy and timely prefetching into the cache hierarchy. On average, Berti outperforms state-of-the-art L1D and L2 prefetchers. Berti is equally effective even in the constrained DRAM bandwidth scenarios and also for multi-core mixes. Berti consumes the least dynamic energy at the memory hierarchy among all state-of-the-art prefetchers. In summary, Berti provides high prefetch accuracy, timely prefetching, and good coverage with a limited storage overhead of 2.55 KB per core.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>M L O P ( L 1 D ) I P C P ( L 1 D ) B i n g o ( L 2 ) S P P -P P F ( L 2 ) B e r t i ( L 1 Accuracy of L1 and L2 prefetchers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Dynamic energy consumption normalized to no prefetching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Strides, local deltas, and timely local deltas. The values on the timeline (2, 5, 7 ...) represent the addresses referenced by the same instruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Access address 15: two timely deltas found.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Learning timely deltas.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Speedup with Berti as an L1D prefetcher for (a) 44 SPEC CPU2017 and (b) 20 GAP memory-intensive traces normalized to L1D IP-stride.Geomean-all corresponds to the geometric mean of all the 95 SPEC CPU2017 traces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Prefetch accuracy at the L1D. Percentages of useful requests are broken down into timely (gray) and late (black) prefetch requests.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 .Figure 12 .</head><label>1112</label><figDesc>Figure 11. Prefetch coverage in terms of average L1D, L2, and LLC demand MPKIs for all L1D prefetchers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Prefetch coverage in terms of average L2 and LLC demand MPKIs with multi-level prefetching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 18</head><label>18</label><figDesc>Figure18shows speedup with the CloudSuite benchmarks for L1D and multi-level prefetching combinations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 16 .</head><label>16</label><figDesc>Figure 16.Performance of L1D prefetchers in constrained DRAM bandwidth, in MTPS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 17 .</head><label>17</label><figDesc>Figure 17. Performance of multi-level prefetching in constrained DRAM bandwidth, in MTPS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>CFigure 18 .</head><label>18</label><figDesc>Figure 18. Speedup for CloudSuite.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 19 .</head><label>19</label><figDesc>Figure 19. Speedup with and without MISB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 20 .</head><label>20</label><figDesc>Figure 20. Summary of multi-core speedups relative to a system with L1D IP-stride prefetcher.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 21 .</head><label>21</label><figDesc>Figure 21. Normalized speedup with different L1D and L2 confidence watermarks averaged across memory intensive SPEC CPU2017 and GAP benchmarks. Speedup is rounded to two decimal places (1.085 is rounded to 1.09).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>History table andTable of deltas entry format.</figDesc><table><row><cell cols="3">L1D access</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VA</cell><cell>VA</cell><cell></cell><cell></cell><cell></cell><cell cols="2">IP, VA</cell><cell cols="2">History table</cell><cell></cell></row><row><cell cols="2">L1 dTLB PA Miss Fill STLB PA</cell><cell>way 0 ... Miss</cell><cell cols="2">L1D cache way 1 ... ... MSHR ...</cell><cell cols="2">way 11 ... Fill</cell><cell>Hit p Latency IP, VA, latency</cell><cell cols="3">... Write Search Table of deltas Timely deltas ... Pref. requests</cell></row><row><cell></cell><cell></cell><cell cols="4">Next cache level (L2)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="11">Figure 5. Berti design overview. Hardware extensions are shown in gray.</cell></row><row><cell></cell><cell cols="2">IP tag</cell><cell cols="3">line address</cell><cell cols="2">timestamp</cell><cell></cell><cell></cell></row><row><cell>History table</cell><cell cols="2">7</cell><cell></cell><cell>24</cell><cell></cell><cell></cell><cell>16</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">IP tag</cell><cell cols="2">c o u n t e r delta</cell><cell cols="3">c o v e r a g e s t a t u s</cell><cell>delta</cell><cell cols="2">c o v e r a g e s t a t u s</cell></row><row><cell>Table of deltas</cell><cell cols="2">10</cell><cell>4</cell><cell>13</cell><cell>4</cell><cell>2</cell><cell>...</cell><cell>13</cell><cell>4</cell><cell>2</cell></row><row><cell>Figure 6.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p>PQ</p>... considered for learning timely deltas. Based on our empirical results, on average across GAP and SPEC CPU2017 traces, we see 1.08 overflows per kilo L1D fills.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table I STORAGE</head><label>I</label><figDesc>OVERHEAD OF BERTI.</figDesc><table><row><cell>Structure</cell><cell>Storage</cell></row><row><cell>History</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>table 8 -</head><label>8</label><figDesc>set, 16-way (128-entry) cache, FIFO replacement policy. Each set: 4 bits (replacement policy). On every L1D access, the table of deltas is searched looking with a matching IP (IP, VA arrow pointing to the table of deltas in Figure</figDesc><table><row><cell></cell><cell></cell><cell>0.74 KB</cell></row><row><cell></cell><cell>Each entry: 7-bit IP tag, 24-bit address, 16-bit</cell><cell></cell></row><row><cell></cell><cell>timestamp</cell><cell></cell></row><row><cell>Table of</cell><cell>16-entry, fully-associative, 4-bit FIFO replace-</cell><cell>0.62 KB</cell></row><row><cell>deltas</cell><cell>ment policy. Each entry: 10-bit IP tag, 4-bit</cell><cell></cell></row><row><cell></cell><cell>counter, and an array of 16 deltas (13-bit delta,</cell><cell></cell></row><row><cell></cell><cell>4-bit coverage, 2-bit status)</cell><cell></cell></row><row><cell>PQ +</cell><cell>16+16 entries,</cell><cell>0.06 KB</cell></row><row><cell>MSHR</cell><cell>16-bit timestamp per entry</cell><cell></cell></row><row><cell>L1D</cell><cell>768 cache lines, 12-bit latency per line</cell><cell>1.13 KB</cell></row><row><cell>Total</cell><cell></cell><cell>2.55 KB</cell></row><row><cell cols="2">Issuing prefetch requests.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table II SIMULATION</head><label>II</label><figDesc>PARAMETERS OF THE BASELINE SYSTEM.</figDesc><table><row><cell>Core</cell><cell cols="2">Out-of-order, hashed perceptron branch predictor [32], 4 GHz</cell></row><row><cell></cell><cell cols="2">with 6-issue width, 4-retire width, 352-entry ROB</cell></row><row><cell>TLBs</cell><cell cols="2">L1 iTLB/dTLB: 64 entries, 4-way, 1 cycle</cell></row><row><cell></cell><cell cols="2">STLB: 2048 entries, 16-way, 8 cycles</cell></row><row><cell>MMU</cell><cell cols="2">2-entry PSCL5, 4-entry PSCL4, 8-entry PSCL3, 32-entry</cell></row><row><cell>Caches</cell><cell cols="2">PSCL2, searched in parallel, one cycle</cell></row><row><cell>L1I</cell><cell cols="2">32 KB, 8-way, 4 cycles</cell></row><row><cell>L1D</cell><cell cols="2">48 KB, 12-way, 5 cycles, with a 24-entry, fully associative</cell></row><row><cell></cell><cell cols="2">IP-stride prefetcher [18]</cell></row><row><cell>L2</cell><cell cols="2">512 KB 8-way associative, 10 cycles, SRRIP [31], non-</cell></row><row><cell></cell><cell cols="2">inclusive</cell></row><row><cell>LLC</cell><cell cols="2">2 MB/core, 16-way, 20 cycles, DRRIP [31], non-inclusive</cell></row><row><cell>MSHRs</cell><cell cols="2">8/16/32 at L1I/L1D/L2, 64/core at the LLC</cell></row><row><cell>DRAM</cell><cell cols="2">One channel/4-cores, 6400 MTPS [19], FR-FCFS, 64-entry</cell></row><row><cell>con-</cell><cell cols="2">RQ and WQ, reads prioritized over writes, write watermark:</cell></row><row><cell>troller</cell><cell>7/8th</cell></row><row><cell>DRAM</cell><cell cols="2">4 KB row-buffer per bank, open page, burst length 16, t RP :</cell></row><row><cell>chip</cell><cell cols="2">12.5 ns, t RCD : 12.5 ns, t CAS : 12.5 ns</cell></row><row><cell></cell><cell></cell><cell>Table III</cell></row><row><cell></cell><cell cols="2">CONFIGURATIONS OF EVALUATED PREFETCHERS.</cell></row><row><cell cols="2">SPP-PPF [17]</cell><cell>256-entry ST, 512-entry 4-way PT, 8-entry GHR, Per-</cell></row><row><cell></cell><cell></cell><cell>ceptron weights with the following entries: 4096?4,</cell></row><row><cell></cell><cell></cell><cell>2048?2, 1024?2, and 128?1 entries, 1024-entry</cell></row><row><cell></cell><cell></cell><cell>prefetch table, 1024-entry reject table</cell></row><row><cell>Bingo</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Figure 22. Speedup vs. size of Berti tables and number of deltas. 0.25? to 4? correspond to one-fourth and four times, respectively.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">History Table</cell><cell>Table of deltas</cell><cell></cell><cell>Num. Deltas</cell></row><row><cell></cell><cell>1.10</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Speedup</cell><cell>1.00 1.05</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.95</cell><cell>0.25X</cell><cell>0.50X</cell><cell>1X</cell><cell>2X</cell><cell>4X</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>For BOP and MLOP, we use the term global delta instead of offset for the rest of the paper.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>For the rest of the paper, unless specified we use the terms delta and local delta interchangeably.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>This work was supported by <rs type="grantNumber">MCIN/AEI/10.13039/ 501100011033</rs> and by "<rs type="projectName">ERDF A way of making Europe</rs>" (grants <rs type="grantNumber">PID2019-105660RB-C21</rs>, <rs type="grantNumber">RTI2018-098156-B-C53</rs>), the <rs type="funder">European Research Council (ERC)</rs> under the <rs type="programName">Horizon 2020 research and innovation program</rs> (grant agreement No <rs type="grantNumber">819134</rs>), and by <rs type="funder">Government of Aragon</rs> (<rs type="grantNumber">T5820R</rs> research group).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_8JXKGs4">
					<idno type="grant-number">MCIN/AEI/10.13039/ 501100011033</idno>
					<orgName type="project" subtype="full">ERDF A way of making Europe</orgName>
				</org>
				<org type="funding" xml:id="_qg4xykk">
					<idno type="grant-number">PID2019-105660RB-C21</idno>
				</org>
				<org type="funding" xml:id="_QEHujP8">
					<idno type="grant-number">RTI2018-098156-B-C53</idno>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation program</orgName>
				</org>
				<org type="funding" xml:id="_rVcEe4r">
					<idno type="grant-number">819134</idno>
				</org>
				<org type="funding" xml:id="_RjYWKWX">
					<idno type="grant-number">T5820R</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Abstract</head><p>This artifact contains all the information necessary to reproduce the main experiments in the paper. We describe how the required software and the elements that compose it can be obtained, and how to run the artifact. 2) Software Dependencies: We test the artifact on a system with these features:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Artifact Check-list &amp; Meta-information</head><p>19.1 scipy 1.5.4 However, in our tests, new GNU/Linux systems were able to run the artifact. Only newer GCC compilers may raise errors in execution. In order to ease the process of running the artifact, we provide two options in the main script: (1) The -g flag downloads and builds GCC 7.5.0 from scratch, and (2) -d uses Docker to build the simulator. The Python3 Packages can be installed using pip3.</p><p>3) Data sets: The traces for the full set of experiments were downloaded from different championships. However, for this artifact only the SPEC CPU2017 traces from the 3 rd Data Prefetching Championship (http://hpca23.cse.tamu. edu/champsim-traces/speccpu/) are needed. These traces are automatically downloaded by the artifact. GAP (from ML for Computer Architecture and Systems https://sites.google. com/view/mlarchsys/) and CloudSuite (from the 2 nd Cache Replacement Championship https://crc2.ece.tamu.edu) are also used in the paper, so all results in the paper are easily reproducible by updating our scrips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Installation &amp; Experimental workflow</head><p>The artifact is ready to be built and run automatically by executing the run.sh script. The overall flow is as follows:</p><p>1) Clone the artifact: $ git clone https://github.com/agusnt/Berti-Artifact 2) Enter the cloned repository:</p><p>$ cd Berti-Artifact 3) Run the script:</p><p>$ ./run.sh Optionally, you can speed up the simulations by running them in parallel: $ ./run.sh -p [number of threads] In case of an error while building the artifact or running the simulations, try compiling instead with the Docker flag; it can be used along with the parallelization option (-p): $ ./run.sh -d</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Evaluation and expected results</head><p>The artifact provides speedup and L1D accuracy results for the memory-intensive single-thread SPEC CPU2017 traces at the end of the simulation. The execution of the artifact prints the following information:</p><p>Building </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Experiment customization</head><p>We implement two options to run the artifact in system with newer GCC compilers: <ref type="bibr" target="#b0">(1)</ref> ./run.sh -g downloads and builds GCC 7.5.0 from scratch to build the artifact, and (2) ./run.sh -d builds the artifact with Docker.</p><p>To speedup the artifact we provide a -p [number of threads] flag that allows to run the experiments in parallel.</p><p>The remaining parameters of our script are: -v verbose mode, -h a help menu, -c deletes all temporal files, i.e. traces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Notes</head><p>The L1D accuracy reported by ChampSim is calculated differently from the reported by the artifact. Instead, the L1D accuracy reported by the artifact is calculated as: L1DPre f etchLate + L1DPre f etchTimely L1DPre f etchFill Our L1D accuracy formula represents the unnecessary traffic generated by the prefetcher, i.e. an accuracy of 90% indicates that there is 10% of unnecessary traffic. The nominator includes all successful prefetches (timely or late), prefetches that have not caused unnecessary extra traffic, and the denominator represents the data brought into the cache by the prefetcher.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The 1st data prefetching championship (dpc-1)</title>
		<ptr target="https://jilp.org/dpc/" />
		<imprint>
			<date type="published" when="2009-02">Feb. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The 2nd data prefetching championship (dpc-2)</title>
		<ptr target="https://comparch-conf.gatech.edu/dpc2/" />
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Micron dram power calculator</title>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Cloudsuite traces for champsim</title>
		<ptr target="https://www.dropbox.com/sh/pgmnzfr3hurlutq/AACciuebRwSAOzhJkmj5SEXBa/CRC2trace?dl=0&amp;subfoldernavtracking=1" />
		<imprint>
			<date type="published" when="2017-11">Nov. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">SunnyCove microarhcitecture</title>
		<ptr target="https://en.wikichip.org/wiki/intel/microarchitectures/sunnycove" />
		<imprint>
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">SunnyCove microarhcitecture latency</title>
		<ptr target="https://www.7-cpu.com/cpu/IceLake.html" />
		<imprint>
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The 3rd data prefetching championship (dpc-3)</title>
		<ptr target="https://dpc3.compas.cs.stonybrook.edu/" />
		<imprint>
			<date type="published" when="2019-06">Jun. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">SPEC CPU 2017 traces for champsim</title>
		<ptr target="https://hpca23.cse.tamu.edu/champsim-traces/speccpu/index.html" />
		<imprint>
			<date type="published" when="2019-02">Feb. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">ChampSim simulator</title>
		<ptr target="http://github.com/ChampSim/ChampSim" />
		<imprint>
			<date type="published" when="2020-05">May 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">GAP traces for champsim</title>
		<imprint>
			<date type="published" when="2021-03">Mar. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Abs: A low-cost adaptive controller for prefetching in a banked shared last-level cache</title>
		<author>
			<persName><forename type="first">J</forename><surname>Albericio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ib??ez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vi?als</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Llaber?a</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization (TACO)</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domino temporal data prefetcher</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bakhshalipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sarbazi-Azad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th Int&apos;l Symp. on High-Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2018-02">Feb. 2018</date>
			<biblScope unit="page" from="131" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bingo spatial data prefetcher</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bakhshalipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shakerinava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sarbazi-Azad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th Int&apos;l Symp. on High-Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2019-02">Feb. 2019</date>
			<biblScope unit="page" from="399" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The GAP benchmark suite</title>
		<author>
			<persName><forename type="first">S</forename><surname>Beamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<idno>abs/1508.03619</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015-08">Aug. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pythia: A customizable hardware prefetching framework using online reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kanellopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shahroodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">54th Int&apos;l Symp. on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2021-10">Oct. 2021</date>
			<biblScope unit="page" from="1121" to="1137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dspatch: Dual spatial pattern prefetcher</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">52nd Int&apos;l Symp. on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2019-10">Oct. 2019</date>
			<biblScope unit="page" from="531" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Perceptron-based prefetch filtering</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chacon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Teran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">46th Int&apos;l Symp. on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2019-06">Jun. 2019</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Leaking control flow information via the hardware prefetcher</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<idno>abs/2109.00474</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021-09">Sep. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">DDR standards</title>
		<author>
			<persName><surname>Ddr</surname></persName>
		</author>
		<ptr target="https://en.wikipedia.org/wiki/Doubledatarate" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inside intel core microarchitecture and smart memory access</title>
		<author>
			<persName><forename type="first">J</forename><surname>Doweck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intel whitepaper</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Coordinated control of multiple prefetchers in multi-core systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">42nd Int&apos;l Symp. on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2009-12">Dec. 2009</date>
			<biblScope unit="page" from="316" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Clearing the clouds: A study of emerging scale-out workloads on modern hardware</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">O</forename><surname>Koc ?berber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alisafaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th Int&apos;l Conf. on Architectural Support for Programming Language and Operating Systems (ASPLOS)</title>
		<imprint>
			<date type="published" when="2012-03">Mar. 2012</date>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The microarchitecture of Intel, AMD and VIA CPUs: An optimization guide for assembly programmers and compiler makers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fog</surname></persName>
		</author>
		<ptr target="https://www.agner.org/optimize/microarchitecture.pdf" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Evolution of the samsung exynos cpu microarchitecture</title>
		<author>
			<persName><forename type="first">B</forename><surname>Grayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rupley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Zuraski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Quinnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kitchin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hensley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brekelbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">47th Int&apos;l Symp. on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2020-06">Jun. 2020</date>
			<biblScope unit="page" from="40" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning memory access patterns</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Litz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">35th Int&apos;l Conf. on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018-07">Jul. 2018</date>
			<biblScope unit="page" from="1924" to="1933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Near-side prefetch throttling: Adaptive prefetching for high-performance many-core processors</title>
		<author>
			<persName><forename type="first">W</forename><surname>Heirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Bois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Vandriessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th Int&apos;l Conf. on Parallel Architectures and Compilation Techniques (PACT)</title>
		<imprint>
			<date type="published" when="2018-11">Nov. 2018</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tcp: Tag correlating prefetchers</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th Int&apos;l Symp. on High-Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2003-02">Feb. 2003</date>
			<biblScope unit="page" from="317" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Effective stream-based and execution-based data prefetching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Iacobovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Spracklen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kadambi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th Int&apos;l Conf. on Supercomputing (ICS)</title>
		<imprint>
			<date type="published" when="2004-06">Jun. 2004</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Exploiting long-term behavior for improved memory system performance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
		<respStmt>
			<orgName>The University of Texas at Austin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Linearizing irregular memory accesses for improved correlated prefetching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">46th Int&apos;l Symp. on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2013-12">Dec. 2013</date>
			<biblScope unit="page" from="247" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">High performance cache replacement using re-reference interval prediction (rrip)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C S</forename><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">37th Int&apos;l Symp. on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="60" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dynamic branch prediction with perceptrons</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th Int&apos;l Symp. on High-Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2001-01">Jan. 2001</date>
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Prefetching using markov predictors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th Int&apos;l Symp. on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="1997-06">Jun. 1997</date>
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Instruction criticality based energy-efficient hardware data prefetching</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Kalani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="146" to="149" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Path confidence based lookahead prefetching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L N</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">49th Int&apos;l Symp. on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2016-10">Oct. 2016</date>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Kill the program counter: Reconstructing program behavior in the processor cache hierarchy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Teran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd Int&apos;l Conf. on Architectural Support for Programming Language and Operating Systems (ASPLOS)</title>
		<imprint>
			<date type="published" when="2017-04">Apr. 2017</date>
			<biblScope unit="page" from="737" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cacti-p: Architecture-level modeling for sram-based structures with advanced leakage reduction techniques</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 Int&apos;l Conf. on Computer-Aided Design (ICCAD)</title>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="page" from="694" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Best-offset hardware prefetching</title>
		<author>
			<persName><forename type="first">P</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd Int&apos;l Symp. on High-Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2016-03">Mar. 2016</date>
			<biblScope unit="page" from="469" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An analysis of address and branch patterns with patternfinder</title>
		<author>
			<persName><forename type="first">C</forename><surname>?zt?rk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">B</forename><surname>Karsli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sendag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Symp. on Workload Characterization (IISWC)</title>
		<imprint>
			<date type="published" when="2014-10">Oct. 2014</date>
			<biblScope unit="page" from="232" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bouquet of instruction pointers: Instruction pointer classifier-based spatial hardware prefetching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pakalapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">47th Int&apos;l Symp. on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2020-06">Jun. 2020</date>
			<biblScope unit="page" from="118" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Expert prefetch prediction: An expert predicting the usefulness of hardware prefetchers</title>
		<author>
			<persName><forename type="first">B</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Balachandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="16" />
			<date type="published" when="2016-01">Jan. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">SPAC: A synergistic prefetcher aggressiveness controller for multi-core systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers (TC)</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3740" to="3753" />
			<date type="published" when="2016-12">Dec. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">CAFFEINE: A utilitydriven prefetcher aggressiveness engine for multicores</title>
		<author>
			<persName><forename type="first">B</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Balachandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization (TACO)</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2015-08">Aug. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multilevel adaptive prefetching based on performance gradient tracking</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Briz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Ib??ez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vi?als</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Instruction-Level Parallelism</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2011-01">Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fetch directed instruction prefetching</title>
		<author>
			<persName><forename type="first">G</forename><surname>Reinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd Int&apos;l Symp. on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="1999-12">Dec. 1999</date>
			<biblScope unit="page" from="16" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Berti: A per-page best-request-time delta prefetcher</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 3rd Data Prefetching Championship</title>
		<imprint>
			<date type="published" when="2019-06">Jun. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A cost-effective entangling prefetcher for instructions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jimborean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">47th Int&apos;l Symp. on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2021-06">Jun. 2021</date>
			<biblScope unit="page" from="99" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multi-lookahead offset prefetching</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shakerinava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bakhshalipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sarbazi-Azad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 3rd Data Prefetching Championship</title>
		<imprint>
			<date type="published" when="2019-06">Jun. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Automatically characterizing large scale program behavior</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th Int&apos;l Conf. on Architectural Support for Programming Language and Operating Systems (ASPLOS)</title>
		<imprint>
			<date type="published" when="2002-10">Oct. 2002</date>
			<biblScope unit="page" from="45" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Efficiently prefetching complex address patterns</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shevgoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koladiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">48th Int&apos;l Symp. on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
			<biblScope unit="page" from="141" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A hierarchical neural model of data prefetching</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th Int&apos;l Conf. on Architectural Support for Programming Language and Operating Systems (ASPLOS)</title>
		<imprint>
			<date type="published" when="2021-04">Apr. 2021</date>
			<biblScope unit="page" from="861" to="873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Spatio-temporal memory streaming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">36th Int&apos;l Symp. on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="69" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Spatial memory streaming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd Int&apos;l Symp. on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2006-06">Jun. 2006</date>
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Feedback directed prefetching: Improving the performance and bandwidth-efficiency of hardware prefetchers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Srinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th Int&apos;l Symp. on High-Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2007-02">Feb. 2007</date>
			<biblScope unit="page" from="63" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Standard Performance Evaluation Corporation</title>
		<ptr target="http://www.spec.org/cpu" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note>SPEC CPU2017</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Address re-ordering mechanism for efficient pre-fetch training in an out-of-order processor</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radhakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">U.S. Patent</title>
		<imprint>
			<biblScope unit="volume">9542323</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014-09">Sep. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">RCTP: Region correlated temporal prefetcher</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Varkey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mutyam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">35th Int&apos;l Conf. on Computer Design (ICCD)</title>
		<imprint>
			<date type="published" when="2017-11">Nov. 2017</date>
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Temporal prefetching without the off-chip metadata</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nathella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pusdesris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sunwoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">52nd Int&apos;l Symp. on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2019-10">Oct. 2019</date>
			<biblScope unit="page" from="996" to="1008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Efficient metadata management for irregular data prefetching</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nathella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sunwoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">46th Int&apos;l Symp. on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2019-06">Jun. 2019</date>
			<biblScope unit="page" from="449" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Timing local streams: improving timeliness in data prefetching</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-H</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th Int&apos;l Conf. on Supercomputing (ICS)</title>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
