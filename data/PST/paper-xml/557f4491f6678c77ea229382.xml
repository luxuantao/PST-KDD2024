<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recursive Learning: A New Implication Technique for Efficient Solutions to CAD Problems-Test, Verification, and Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Wolfgang</forename><surname>Kunz</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Dhiraj</forename><forename type="middle">K</forename><surname>Pradhan</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute fur Theoretische Elektrotechnik</orgName>
								<orgName type="institution">University of Hannover</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">the University of Potsdam</orgName>
								<address>
									<postCode>14415</postCode>
									<settlement>Potsdam</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
									<region>MA</region>
									<country>He</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
								<address>
									<postCode>77843</postCode>
									<settlement>College Station</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recursive Learning: A New Implication Technique for Efficient Solutions to CAD Problems-Test, Verification, and Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B2A27696A75F34728D65CBD837C618C3</idno>
					<note type="submission">received September 8, 1992; revised June 9, 1993. This work</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Recursive learning</term>
					<term>unjustified gates</term>
					<term>precise implications</term>
					<term>necessary assignments</term>
					<term>boolean satisfiability</term>
					<term>design verification</term>
					<term>multi-level optimization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Motivated by the problem of test pattern generation in digital circuits, this paper presents a novel technique called recursive learning that is able to perform a logic analysis on digital circuits. By recursively calling certain learning functions, it is possible to extract all logic dependencies between signals in a circuit and to perform precise implications for a given set of value assignments. This is of fundamental importance because it represents a new solution to the Boolean satisfiability problem. Thus, what we present is a new and uniform conceptual framework for a wide range of CAD problems including, but not limited to, test pattern generation, design verification, as well as logic optimization problems. Previous test generators for combinational and sequential circuits use a decision tree to systematically explore the search space when trying to generate a test vector. Recursive learning represents an attractive alternative. Using recursive learning with sufficient depth of recursion during the test generation process guarantees that implications are performed precisely; i.e., all necessary assignments for fault detection are identified at every stage of the algorithm so that no backtracks can occur. Consequently, no decision tree is needed to guarantee the completeness of the test generation algorithm. Recursive learning is not restricted to a particular logic alphabet and can be combined with most test generators for combinational and sequential circuits. Experimental results that demonstrate the efficiency of recursive learning are compared with the conventional branch-and-bound technique for test generation in combinational circuits. In particular, redundancy identification by recursive learning is demonstrated to be much more efficient than by previously reported techniques. In an important recent development, recursive learning has been shown to provide significent progress in design verification problems [22]. Also importantly, recursive learning-based techniques have already been shown to be useful for logic optimization. Specifically, techniques based on recursive learning have already yielded better optimized circuits than the well known MIS-11.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IEEE</head><p>[2] as implicit enumeration of an n-dimensional Boolean search space, where n is the number of primary input signals.</p><p>Traditionally, a decision tree is used to branch and bound through the search space until a test vector has been generated or a fault has been proven redundant. Efficient heuristics have been reported to guide the search [3]-[SI. However, it is the nature of this classical searching technique that it is often very inefficient for hard-to-detect and redundant faults; i.e., in those cases where only few or no solutions exist.</p><p>What we propose here is a fundamentally new searching technique that can handle these pathological cases in test generation much more efficiently than the traditional search. It is important to note that while results presented here are in the context of test generation, they possess a wide range of applications to many important areas in computer-aided circuit design, such as logic verification and optimization <ref type="bibr">[22]</ref>- <ref type="bibr">[25]</ref>. Specifically, for the first time, this new searching technique provides a uniform framework for borh CAD and test problems. The recursive learning implication procedure [ 181 originally developed for test problems has now been shown to provide a powerful tool for improved verification and optimization procedures. It is expected that the real potential of the framework may, indeed, be in the formulation of solutions to CAD problems. For example, using this, it has been shown <ref type="bibr">[22]</ref> that the nonredundant initial MCNC versions were not equivalent to the original ISCAS 85 benchmarks-a surprising result! In addition only a few seconds on a Sparc workstation were sufficient to verify the formidable multiplier c6288 against the nonredundant version. Further application of recursive learning to design verification has recently been reported by the authors <ref type="bibr">[23]</ref>. Also, the potential application of recursive learning to logic optimization and other related problems is currently under investigation by the authors, as well as others <ref type="bibr">[25]</ref>. First results clearly show that recursive learning can be used to design very powerful techniques for multilevel logic optimization <ref type="bibr">[25]</ref>, <ref type="bibr">[26]</ref>. Already, reported results indicate recursive learning-based optimization can yield smaller circuits compared to MIS-I1 <ref type="bibr">[25]</ref>, <ref type="bibr">[26]</ref>.</p><p>In this paper, we first apply our new approach in the context of test generation and then follow up with a discussion of to how test generation techniques can be applied to various synthesis problems. During the process of generating a test vector for a given fault, some assignments of signal values are found to be necessary; i.e., those assignments can be implied in anyway, and therefore they must be satisfied for the given combination of value assignments. Other assignments ~ .</p><p>0278-0070/94$04.00 0 1994 IEEE are optional and their assignment represents a decision in the decision tree. Significant progress has been made, especially in redundancy identification, since techniques have been develcped that are able to identify necessary assignments <ref type="bibr">[5]</ref>, <ref type="bibr">[7]</ref>, <ref type="bibr">[8]</ref>. However, all these techniques are limited in that they fail to produce all necessary assignments. What we propose here is a technique that, for the first time, indeed generates all necessary assignments.</p><p>Knowledge about necessary assignments is crucial for limiting or eliminating altogether the number of backtracks that must be performed. Backtracks occur only after wrong decisions have been made that violate necessary assignments. Hence, it is important to realize that if all necessary (mandatory) assignments are known at every stage of the test generation process, there can be no backtracks at all. Since remaining assignments are only optional, they cannot therefore be violated. All methods presented in the past, such as [5]-[8],</p><p>were not able to identify all necessary assignments, based as they are on polynomial time-complexity algorithms. The problem of identifying all necessary assignments is an NP-complete method that guarantees identifying all necessary assignments must be exponential in time complexity.</p><p>This work develops a new method called recursive learning that can perform a complete search to identify all necessary assignments. It is important to note that previous methods did not provide for this completeness. Searching for all necessary assignments provides a fundamentally new alternative to traditional techniques. For example, traditional test generation is a search for one sufficient solution, whereas recursive learning may be viewed as searching for those conditions that enables purging the nonsolution area from the search space.</p><p>One other significant attribute of recursive learning that has been successfully exploited in verification <ref type="bibr">[22]</ref>- <ref type="bibr">[26]</ref> and the optimization is the ability to provide all indirect implications of a particular value assignment. Such indirect implications form the basis of finding functionally equivalent nodes in verification <ref type="bibr">[22]</ref>- <ref type="bibr">[24]</ref>. In optimization, the recursive learningbased implication can provide a powerful tool for circuit transformations <ref type="bibr">[24]-[26]</ref>.</p><p>Our technique is based on performing learning operations. First introduced in [6], <ref type="bibr">[7]</ref> and further developed in <ref type="bibr">[12]</ref>, learning is defined to mean the temporary injection of logic values at certain signals in the circuit to examine their logical consequences. By applying simple logic rules, certain information about the current situation of value assignments can be learned.</p><p>This work generalizes the concepts of learning in various ways: in previous learning methods [6], [7], learning is restricted to a 3-valued logic alphabet. The method presented here is not restricted to any particular logic alphabet and can be used for any logic value system such as 5-valued logic [ 1 11. 9-valued logic <ref type="bibr">[9]</ref> or <ref type="bibr">16-valued logic [8]</ref>, <ref type="bibr">[lo]</ref>. Secondly, our learning routines can be called recursively and rhus provide for completeness. The maximum recursion depth determines how much is learned about the circuit. The time complexity of our method is exponential in the maximum depth of recursion, rmaX. Memory requirements, however, grow linearly with rmax. As noted before, any method that identifies all necessary assignments must be exponential in time complexity because this problem is NP-complete.</p><p>In broader terms, recursive learning can be understood as a general method to conduct a logic analysis, deriving a maximum amount of information about a given circuit in a minimum amount of time. This paper examines the ability of recursive learning to derive necessary assignments of fundamental importance for many applications throughout the field of computer-aided circuit design <ref type="bibr">[22]-[25]</ref>. The performance of recursive learning is evaluated here by applying it to the problem of test generation. Also, results of recursive-leamingbased techniques in logic verification and optimization are reviewed and summarized in this paper.</p><p>Because most state-of-the-art test generation tools, like [5], [7], <ref type="bibr">[17]</ref>, are further developments of the well-known FAN algorithm [3], we, therefore present and discuss recursive learning with respect to FAN-based test generation. Other approaches to test generation can also make efficient use of this new searching scheme. Other work includes an algebraic method based on Boolean difference <ref type="bibr">[15]</ref> and the use of transitive closure in [ 161 that allows for parallelization of test generation. It should be noted that these methods, unlike our technique, rely on a decision tree when producting a complete search. Thus, they are likely to benefit from the searching scheme presented here. This section is organized into six main sections. Section I1 illustrates recursive learning with an example, Section 111 gives a formal description of recursive learning in finding necessary assignments, Section IV discusses recursive learning for test generation, Section V gives the results for test generation using recursive learning, and Section VI briefly covers different promising applications of recursive learning to verification and synthesis problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11.</head><p>This section introduces the recursive learning concept by first presenting a simplified (preliminary) learning technique, The basic motivation here is to illustrate concepts and show what may or may not lead to complete recognition of all necessary assignments. The formal procedure in Section 111 will be rooted in the observations made in the following example. Table <ref type="table">I</ref> shows a learning procedure for unjustified lines <ref type="bibr">[3]</ref>, which is called recursively.</p><p>Fig. <ref type="figure">1</ref> shows an example to introduce the basic framework of recursive learning referred to as demoxecursivelearningo in Table <ref type="table">I</ref>. For the time being, disregard the pads xe ~ xy, xh, ye, yy, and yh. These will be used later to insert additional circuitry to illustrate other key points. Consider signals i l and j , where i l = 0 and j = 1 are two unjustified lines. Given this, the reader can derive that IC = 1 and i2 = 0 are necessary assignments. This is easy to observe as the nodes are labeled in a special way in Fig. <ref type="figure">1</ref>. Signals that are labeled by the same character (excluding z, y pads), but different indices always assume the same logic value for an arbitrary combination of value assignments at the inputs. Thus, nodes labeled with the same character, except the  We will now explain, step by step, how recursive learning derives the necessary assignments i2 = 0 and k = 1 for the given value assignments 21 = 0 and j = 1 in Fig. <ref type="figure">1</ref>. This is done in Table <ref type="table">11</ref>, which lists the different learning implications that occur when demosecursive-learning is performed for the unjustified line i l = 0.</p><p>The first column represents the situation of value assignments before and after recursive learning has been performed. When learning is performed at the unjustified line i l (column 2), we first assign the controlling value at 91. Assigning the controlling value to a gate with an unjustified line represents a justification, as will be defined more generally in Section 111. With the implications for g1 = 0, we obtain the unjustified lines el and f1 in the first learning level. These signals are treated by recursively calling learning again (column 3 representing level 2). Now, for the unjustified line el = 0, we examine the justifications a1 = 0 and bl = 0. In both these cases, we obtain e2 = 0. Consequently, this value assignment becomes necessary to justify the unjustified line e l .  that fi = 0 is also necessary. Returning to the first learning level, the implications can be completed to yield the necessary assignments k = 1 and i2 = 0, completing the learning.</p><p>Two key points to be noted are: 1) all signal assignments that have been made during learning in each level have to be erased again as soon as learning is finished in the current level of recursion and 2 ) only those values that are learned to be necessary are transferred to the next lower level of recursion. Furthermore, one important aspect of this procedure is that the unjustified lines are considered separately. At each unjustified line, we try the different justifications and then move to the next unjustified line. A natural question arises as to how one takes into account that some necessary assignments result from the presence of several unjustified lines, if the justifications at one unjustified line are not tried in combination with the justifications at the other unjustified We proceed in the same way with unjustified line fi, learning lines? Note that the necessary assignment, k = 1; in the above example, is due to both unjustified line Z1 = 0 and j = 1 and is correctly derived by demo~ecursive-learning().</p><p>Specifically, the interdependence of different unjustified lines is accounted for because forward implications are performed that check the consistency of the justification attempts against all other unjustified lines. However, the completeness of the forward implications is not always guaranteed, and therefore this preliminary version of the recursive learning routine demo_recursive-learning() may fail to identify all necessary assignments. To understand what extension has to be made to identify all necessary assignments, consider Fig. <ref type="figure" target="#fig_3">2</ref>, which provides an example of how forward implications can be incomplete.</p><p>Consider signals z and d in Fig. <ref type="figure" target="#fig_3">2</ref>. No forward implication can be made for signal d after the assignment x = 0 has been made. However, it is easy to see that if z = 0, both assignments d = 0 and d = 1 result in y = 0. Hence, the forward implication x = 0 =+ y = 0 is true.</p><p>In practice, incompleteness of forward implications seems a minor problem. When learning is performed for a particular unjustified line, and when necessary assignments are missed because of incomplete forward implications, then there is often some other unjustified line for which these necessary assignments can be learned.</p><p>This above incompleteness of forward implications can be illustrated using Figs. <ref type="figure">1</ref> and<ref type="figure" target="#fig_3">2</ref>. If we add the circuitry of Fig. <ref type="figure" target="#fig_3">2</ref> between the pads 2 , and :ye in Fig. <ref type="figure">1</ref> such that signal x of Fig. <ref type="figure" target="#fig_3">2</ref> is connected to z, and y is connected to ye, it can be observed that learning for unjustified line i1 will no longer yield the necessary assignments k = 1 and Z2 = 0. However, (as the reader may verify) the necessary assignments IC = 1 and i2 = 0 can still be identified when learning is performed for unjustified line j . Experiments show that this is a frequent phenomenon that can be accounted for, as explained in Section V. Nevertheless, procedure demo_recursive-learningo can miss necessary assignments because of incomplete forward implications. The reader may verify, as an exercise, that learning at line j will also fail to identify IC = 1 and a2 = 0 if we add similar circuitry as in Fig. <ref type="figure" target="#fig_3">2</ref> (remove the invertor and replace NOR by OR) between the pads x g , yg and Z h . Y h . The reason for this incompleteness is that unjustified lines are not the only logic constraints at which learning has to be initialized. To overcome this problem, the concept of unjustified lines will be generalized. Consider the previous example; if recursive learning, as explained, is applied in Fig. <ref type="figure" target="#fig_3">2</ref> to signal d, then the forward implication z = 0 + y = 0 will be the final result. Hence, from this we can deduce that recursive learning should be applied not only to the unjustified lines, but also to certain other signals, to make it complete. This problem is addressed in the next section by defining the concept of unjustified gates, on which recursive learning should be applied to make it identify all the necessary assignments. In the earlier example concerning Fig. <ref type="figure" target="#fig_3">2</ref> , where y = 0, recursive learning was applied on the AND gate whose output signal is d, which is an unjustified gate. The first input for this gate is the output of a NOT gate and the second input is the output of the NAND gate, as shown. There are two justifications possible at this unjustified gate: 1) The output of the NAND and AND both taking a value of 1, and 2 ) The output of the NAND and AND both taking a value 0. By performing direct implications of these justifications, it can be seen that both these justifications lead to y = 0, which yields a necessary assignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">RECURSIVE LEARNING TO DETERMINE ALL NECESSARY ASSIGNMENTS</head><p>In a FAN-type algorithm, necessary assignments are derived in two different ways. The first is based on a structural examination of conditions for fault detection [ 3 ] , <ref type="bibr">[4]</ref>. Secondly, it is the task of an implication procedure to derive necessary assignments that result from previously made signal assignments. The concept of recursive learning allows the design methods to identify all necessary assignments for both cases.</p><p>In Section 3.1, a technique is presented that can make all implications for a given situation of value assignments with absolute precision, time permitting. Section 3.2 introduces a technique to derive all necessary assignments resulting from the requirement to propagate the fault signal to at least one primary output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A . The Precise Implication Procedure</head><p>It was pointed out in the previous section that the concept of justified lines must be generalized to guarantee the completeness of the algorithm. Here, we introduce the more general concept of unjustified gates. Def. 1 uses the common notation of a "specified signal," by which we understand a signal with a fixed value. In the common logic alphabet of</p><formula xml:id="formula_0">[ l l ] , B5 = (0,1, D I D l X), a signal is specified if it has one of the values 'O', 'l', 'D''or 'D'. It is unspecified if it has the value 'X'.</formula><p>Def. I : Given a gate G that has at least one specified input or output signal: Gate G is called unjustified if there are one or several unspecified input or output signals of G for which it is possible to find a combination of value assignments that yields a conflict at G. Otherwise, G is called justified.</p><p>The concept of unjustified gates can be used to give a definition of precise implications and necessary assignments:</p><p>Def. 2: For a given circuit and a given situation of value assignments, let f be an arbitrary but unspecified signal in the circuit and V be some logic value. If all consistent combinations of value assignments for which no unjustified gates are left in the circuit contain the assignment f = V , then the assignment f = V is called necessary for the given situation of value assignments. Implications are called precise or complete when they determine all necessary assignments for a given situation of value assignments.</p><p>To determine necessary assignments, we will consider justifications: The left column of Fig. <ref type="figure">3</ref> shows examples of unjustified and justified gates. The right column depicts the corresponding justifications.</p><formula xml:id="formula_1">Def 3: A set of signal assignments, J = { f l = V I , f 2 = V2,. . . f n = V,},</formula><p>De5 4: Let GC be a set of m justifications J1, 5 2 , . . . , J, for an unjustified gate G. If there is at least one justification</p><formula xml:id="formula_2">Ji E GC, i = 1 , Z . . . m for any possible justification J* of G, such that Ji C J*, then set GC is called complete.</formula><p>For a given unjustified gate, it is straightforward to derive a complete set of justifications. In the worst case, this set consists of all consistent combinations of single assignments representing a justification of the given gate. Often, though, the set can be smaller, as shown in Fig. <ref type="figure" target="#fig_4">4</ref>.</p><p>The following represents a complete set of justifications:</p><formula xml:id="formula_3">C = { J l , J 2 , J 3 , J 4 , 5 5 ) with JI = {a = l}, J2 = { b = l}, J3 = {c = l}, J4 = { d = l}, J5 = { e = 1</formula><p>). Note that for example the justification J* = { a = 1, b = 0} does not have to be in C since all assignments in 51 are contained in J*.</p><p>The concept of justifications for unjustified gates is essential toward understanding how learning is used to derive necessary assignments. Assignments are obviously necessary for the justification of a gate if they have to be made for all possible justifications. By definition, all assignments that have to be made for all justifications that represent a complete set of justification, also have to be made for any order justification at the respective gate. Hence, for a given gate, it is sufficient to consider a complete set of justifications in order to learn assignments that are necessary for all justifications.</p><p>All learning operations rely on a basic implication technique. As in [12], we shall call these implications direct implications:</p><p>De5 5: Direct implications are implications that can be performed by only evaluating the truth table for a given gate with a given combination of value assignments at its input and output signals, and by propagating the signal values according to the connectivity in the circuit. A well-known example of direct implications in combinational circuits are the implications performed in FAN <ref type="bibr">[3]</ref>.</p><p>Notation: r: integer number for the depth of recursion OU = {GI, Gz, G3 ' . .} is the.set of all unjustified gates as they result from the current state of the test generation algorithm.</p><formula xml:id="formula_4">G x J r = {fi = V I , f 2 = V2,. . .} is a set of assignments</formula><p>that represents a justification for some gate G, in a given recursion level T .</p><p>G x c r = { J1, J2, J3, . . .} is a complete set of justifications for a given gate G, in a given recursion level r .</p><p>J x U r = {GI, G2, G3, . . .} is a set of unjustified gates in recursion level T as it results from a given justification J x . r,,,: maximum recursion depth Table <ref type="table">111</ref> depicts procedure make_all_implication(!, which is able to make precise implications for a given set of unjustified gates. Note that the list of unjustified gates being set up in every level of recursion contains all new unjustified gates, but must also include unjustified gates of a previous recursion level if these gates have had an event in the current level of recursion.</p><p>Theorem I : The implication procedure in Table III makes precise implications; i.e., a finite r,,, always exists such that make-allimplication( <ref type="figure">0 ,</ref><ref type="figure">T,</ref><ref type="figure">,</ref><ref type="figure">,</ref><ref type="figure">,</ref><ref type="figure">,</ref><ref type="figure"></ref> ) determines all necessary assignments for a given set of unjustified gates, ' U .</p><p>Preliminary Remarks Making precise implications means identifying all signal values that are uniquely determined due to the unjustified gates contained in the set ' U . Let V be a logic value and let us assume for some signal f that the assignment f = V is necessary for the justification of a gate G, in an arbitrary recursion level r . What this means is that one of the following two cases must be fulfilled for</p><formula xml:id="formula_5">each justification G x Ji' E G 2 C' :</formula><p>Case I : The direct implications for the set of assignments J, at G, yield f = V.</p><p>Case 2: The assignment f = V is necessary for the justification of at least one gate in the set of justified gates In the first case, the necessary assignment is recognized and learned when learning in level r + 1. In the latter, deeper recursion level are entered.</p><p>Complete Induction:</p><formula xml:id="formula_6">1) Take r = rmax-l: Proofi - L JiU'+l</formula><p>The more recursions performed, the more assignments are made; i.e., for all unjustified gates in ' U , the recur-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I I</head><p>sive call of makeallimplications(), will always reach a level mm-, such that Jallmax --/o for all justifications J, E G x c m a x -1 and for all gates G, in Urnax-'. This is the case when the implications have reached the primary inputs or outputs. If there is a necessary assignment f = V in level T = max, we will always recognize it, since U""" = 0 and for any necessary assignment, Case 1 must be fulfilled. This means that all necessary assignments have been learned for all gates in all U""" -' that result from arbitrary JYaX-2 E , which belong to an arbitrary G, E 2 ) Assume that we know all necessary assignments for all unjustified gates in all sets U" for arbitrary Jz"-' E C"-l for arbitrary G, E U"-'.</p><p>3) Then, since it is guaranteed with the above assumption that all uniquely determined values be known that can be implied for all justifications in GzCn-l , the procedure makeall-implication() will correctly identify all necessary assignments for the corresponding gate G, E JUn-l. (These are the signal values common for all justification GxCn-l .) The above is true for any gate G, E JiUn-' , where J, is some justification JZn-' for some gate in U"-2. Hence. all necessary By complete induction we conclude that we learn all necessary assignments for all unjustified gates G x in ' U . Fig. <ref type="figure" target="#fig_6">5</ref> shows some combinational circuitry to illustrate rnake~ll-irnplication().  Fig. <ref type="figure">6</ref> depicts a scheme useful to better understanding the general procedure of recursive learning and the proof of Theorem 1.</p><p>During the test generation process, optional assignments are made. After each optional assignment, the resulting necessary assignments must be determined. This is the task of the implication procedure. Many necessary assignments can be determined by performing direct implications only. Direct implications can handle the special case where there is only one possible justification for an unjustified gate. (Note that this represents another possibility to define "direct" implications.) The left column in Fig. <ref type="figure">6</ref> shows the situation as it occurs after each optional assignment during the test generation process. After performing direct implications, we have obtained a situation of value assignments where only those unjustified gates (dark spots in Fig. <ref type="figure">6</ref>) are left that allow for more than one justification. These are examined by learning. Recursive learning examines the different justifications for each unjustified gate, which results in new situations of value assisgnments in the first learning level. If value assisgnments are valid for all possible justifications of an unjustified gate in level 0, i.e., if they lie in the intersection of the respective sets of value assignments in learning level I (shaded area), then they actually belong to the set of value assignments in level 0. This is indicated schematically in Fig. <ref type="figure">6</ref>. However, the set of value assignments in learning level 1 may be incomplete as well because they also contain unjustified gates and the justifications in level 2 have to be examined. This is continued until the maximum recursion depth T, ,   ,   is reached. This immediately leads to the question: how deep do we have to go in order to perform precise implications? Unfortunately, it is neither possible to predict how many levels of recursion are needed to derive all necessary assignments, nor is it possible to determine if all necessary assignments Schematic illustration of recursive learning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Learnina Level</head><p>have been identified after learning, with a certain recursion depth, has been completed. The choice of T,,,,, is subject to heuristics and depends on the application for which recursive learning is used. For test generation, an algorithm to choose T,,, will be presented in Section 4.1. In general, it can be expected that the maximum depth of recursion to determine all necessary assignments is relatively low. This can be explained as follows: Note, that value assignments in level ,i + 1 are only necessary for level i if they lie within the intersection in level i + 1; to be necessary in level ,i -1 they also have to be in the intersection of level %, and so forth. It is important to realize, however. that we are only interested in the necessary assignments of level 0. It is not very likely that a value necessary in level IO also lies in the corresponding intersections of level 9.8: 7. . . . . 1 and, hence, is not likely to be necessary in level 0. Necessary assignments of level 0 are usually determined by only considering a few levels of recursion. This corresponds to the plausible concept that unknown logic constraints (necessary assignments) must lie in the "logic neighborhood" of the known logic constraints from which they are caused.</p><p>Intuitively, a lot of recursions are only needed if there is a lot of redundant circuitry. Look at the circuits in Figs. I , 2, and 5:</p><p>Necessary assignments are only missed by direct implications because the shown circuits contain sub-optimal circuitry. In the scheme of Fig. <ref type="figure">6</ref>, the intersection of justifications (shown as shaded areas) indicate logic redundancies in the circuit.</p><p>In fact, making precise implications and identifying suboptimal circuitry seem to be closely related. This promises that recursive learning is also useful in logic optimization. Use Fig. <ref type="figure">6</ref> to understand the proof of Theorem 1 : It is important to realize that the process of recursive learning terminates, even if the parameter T,,,,, in make _all-implic,ation(,/.. ~/'rrlas 1, is chosen to be infinite. At some point, the justifications must reach the primary inputs and outputs so that no new unjustified gates requiring further recursions can be caused. In Fig. <ref type="figure">6</ref>, such justifications are represented by circles that do not contain dark spots. If the individual justifications for a considered unjustified gate do not contain unjustified gates, it is impossible (because of Def. 1) that these sets of value assignments produce a conflict with justifications of some other unjustified gates. Since a complete set of justifications is examined and the same argument applies to every unjustified gate in the previous recursion level, it is guaranteed that all necessary assignments for the previous recursion level are identified. This is used in Step 1 of the complete induction for Theorem 1. If all necessary assignments are known in a given recursion level, the intersections of the complete sets of justifications yield all necessary assignments for the previous recursion level and steps 2 and 3 of the complete induction are straightforward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B . Determining All Necessary Assignments for Fault Propagation</head><p>In principle, the problem of test generation is solved with a precise implication technique as given in Section 3.1. Observability constraints can always be expressed in terms of unjustified lines by means of Boolean difference. However, most atpg-algorithms use the concept of a "D-frontier" [ 2 ] . This makes it easier to consider topological properties of the circuit [4]. In this section, we present a technique to identify all necessary assignments that are due to the requirement of propagating the fault signal to at least one primary output. Analogous to the previous section where we injected justifications at unjustified gates in order to perform precise implications, this section shows how recursive learning can derive all conditions for fault propagation by injecting sensitizations [3] at the D-frontier.</p><p>The D-frontier in a recursion level r shall be denoted F' and consists of all signals that have a faulty value and a successor signal that is still upspecified. Fig. <ref type="figure" target="#fig_8">7</ref> shows an example for a D-frontier. If we set up the fault signal D for the stuck-at-0 fault at a line a, we obtain F o = {b. e , g, h}.</p><p>Table V lists procedure faultpropagation_learning(). In   Procedure faultpropagation-learning(), which calls procedure makeall-implications(~, learns all assignments that are necessary to sensitize at least one path from the fault location to an arbitrary output. Note that we are not only considering single path sensitization. Along every path that is sensitized in procedure faultpropagation-learning(), gates becomes unjustified if there is more than one possibility to sensitize them. This is demonstrated in Table <ref type="table" target="#tab_5">VI</ref> for gate G in Fig. <ref type="figure" target="#fig_8">7</ref>.</p><p>Procedure faultpropag~tion-learning() as given in Table <ref type="table" target="#tab_6">V</ref> does not show how to handle XOR-gates. However, the extension to XOR-gates is straightforward. XOR-gates, like XNOR-gates, always allow for more than one way to sensitize them. Therefore, the fault propagation has to stop there and the different possibilities to propagate the fault signal have to be tried after the usual scheme for unjustified gates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>* I v . TEST GENERATION WITH RECURSIVE LEARNING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. An Algorithm to Choose the Maximum Recursion Depth r,,,</head><p>There are many possibilities to design a test generation algorithm with recursive learning. There is unlimited freedom  to make optional assignments. We are not bound to the strict scheme of the decision tree in order to guarantee the completeness of the algorithm; it is possible to "jump around" in the search space. Note that this allows attractive possibilities for new heuristics. To guarantee completeness, we only have to make sure that the maximum recursion depth is eventually incremented. Of course, it is wise to keep the maximum recursion depth T, , , as small as possible so as not to spend much on learning operations. Only if the precision is not sufficient to avoid wrong decisions is it sensible to increment There are many possibilities for choosing T, ,   ,  .   To examine the performance of recursive learning, we combined it with the FAN-algorithm and used the following strategy to generate test vectors: the algorithm proceeds like in FAN and makes optional assignments in the usual way. In the same way as for the decision tree, all optional assignments are stored in a stack. Whenever, a conflict is encountered, we proceed as shown in Fig. <ref type="figure" target="#fig_11">8</ref>. By a conflict, we mean that the previous decisions have either led to an inconsistent situation of value assignments or that there is no more possible propagation path for the fault signal (X-path-check failed). The idea behind the routine in Fig. <ref type="figure" target="#fig_11">8</ref> is that we use learning only to leave the nonsolution areas as quickly as possible. After a conflict has occurred, the previous decision is erased; i.e., the signal at the top of the stack is removed and its value is assigned to 'X'. Now, the resulting situation of value assignments is examined with increased recursion depth. If this leads to a new conflict, another decision has to be erased. If there is no conflict, this can mean two things: either the current precision T,,, is not sufficient to detect that there is still a conflict or we have actually returned into the solution area of the search space. Therefore, it is checked if the opposite of the previous (wrong) assignment is one of the assignments that have been learned to be necessary. This is a good heuristic criterion to determine whether the precision has to be. This criterion also makes sure that we can never enter the same nonsolution area twice, and the algorithm in Fig. <ref type="figure" target="#fig_11">8</ref> guarantees the completeness of test generation and redundancy identification without the use of a decision tree.</p><p>Note that the procedure in Fig. <ref type="figure" target="#fig_11">8</ref> is only one out of many possibilities to integrate recursive learning into existing test generation tools. This algorithm has been chosen because it allows a fair comparison of recursive learning with the decision tree. With the algorithm of Fig. <ref type="figure" target="#fig_11">8</ref>, we initially enter exactly the same nonsolution area of the search space as with the original FAN-algorithm. The comparison is how fast the nonsolution areas are left either by conventional backtracking or by recursive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>~ .</head><p>One disadvantage of the above procedure is that in some cases of redundant faults the algorithm initially traverses very deep into nonsolution areas and recursive learning has to be repeated many time until all optional value assignments (those will be all wrong) are erased step by step. Our current implementation therefore makes use of the following intermediate step (not shown in Fig. <ref type="figure" target="#fig_11">8</ref> for reasons of clarity): when the algorithm of Fig. <ref type="figure" target="#fig_11">8</ref> reaches the point where the maximum depth of recursion has to be incremented, we perform recursive learning with increment recursion depth first only to the situation of values assignments that results if all optional value assignments are removed. If a conflict is encountered. the fault is redundant and we are finished. Otherwise, we proceed, as shown in Fig. <ref type="figure" target="#fig_11">8</ref>, i.e., we perform recursive learning, with the optional value assignments as given on the stack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B . Compatihilitj and Generulit? of the Apptmch</head><p>Many heuristics have been reported in the past to guide decision-making during test generation. Most of these techniques are equally suitable in combination with recursive learning. Just as they reduce the number of backtracks in the decision tree, they similarly reduce the number of rccursions needed when recursive learning is performed. In particular, it seems wise to consider the static learning technique of (61.</p><p>[7] to pre-store indirect implications. Furthermore, a method to identify equivalent search states [ 131 and dominance search states [I41 can also be applied to recursive learning. This is because the formulation of a search state as E-Frontier-[ 131 can also be applied to the different sets of value assignments that occur during recursive learning.</p><p>Note that recursive learning in this paper has been based on the common procedure to perform direct implications. It is also possible to use other implication procedures as the basic "workhorse" of recursive learning. Finally. it is possible to use recursive learning and the decision tree at the same time, to combine the advantage of both searching methods.</p><p>In this work. we have examined the performance of recursive learning only for combinational circuits. However, the approach is also feasible for sequential circuits. Even for hierarchical approaches, recursive learning can be used as an alternative to the decision tree. Although all considerations in this paper have been based on the gate level, it is straightforward to extend the concept of justifications for unjustified gates to high level primitives. A large logic block that is unjustified may have a lot of justifications. However note that we only have to continue to try justifications for a given unjustified gate (or high level primitive), as !ong as there is at least one common value for ull consistent previous justitications. Therefore, many justifications have to be tried only if there are actually common values for many justifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The intuition Behind Recur-sive Leurning</head><p>What is the intuition behind this approach and why is it faster than the traditional searching method? At first glancc, recursive learning may seem similar to the search based on the decision tree as applied in the D-algorithm [ 1 1 I. Note that also the decision tree can be used to derive all necessary assignments: for a given set of unjustified gates the justifications can be tried by making optional value assignments, which are added to a decision tree. We exhaust the decision tree; i.e., we continue even after a sufficient solution has been found and obtain all necessary assignments by checking what assignments are common to all sufficient solutions. Like limiting the depth of recursion for recursive learning, we can limit the number of optional decisions that we put into the decision tree so that we only examine the neighborhood of the given unjustified gates. However, there is a fundamental difference between this approach and recursive learning. As pointed out in Section 11, recursive learning examines the different unjustified gates separately, one after the other, whereas the decision tree (implicitly) enumerates all combinations of justifications at one gate with all combinations of justifications at the other gates. This results in an important difference between the two methods; recursive learning only determines all necessary assignments; in contrast to the decision tree, it neither has to derive all sufficient solutions explicitly nor implicitly to obtain all necessary assignments. Consequently, the behavior of recursive learning is quite different from a decision tree-based search. In recursive learning, signal values are only injected temporarily. Only value assignments that are necessary in the current level of recursion are retained. With the decision tree, however, all assignments that are not proven to be wrong are maintained. If a wrong decision has occurred, it can happen that this decision is "hidden" behind a long sequence of subsequent good decisions, so that the conflict occurs only many steps later. At this point, much enumeration is neccessary with the decision tree until the wrong decision is finally reversed. For the precise implication, however, a lot of searching is needed if necessary assignments are "hidden" by large redundant circuitry. Roughly, it is possible to state that the. computational costs to perform precise implications depend on the size of the redundant circuit structures. The relationship between redundancy in the circuit and the complexity of performing precise implications, at this point, is only understood at an intuitive level and is subject to future research. .</p><formula xml:id="formula_7">Inputs / Circuit A + I 1 -hE Circuit B</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head><p>To examine the performance of recursive learning, we use the FAN-algorithm. For comparison, we use both the original FAN-algorithm with the decision tree as well as a modified version where we replaced the decision tree by recursive learning. No additional techniques were used. Recursive learning is performed, as was shown in Section 4.1.</p><formula xml:id="formula_8">TABLES VI1 EXPERIMENTAL RESULTS FOR R E U L K U A ~T F~IJLTS (SLh SPARC 10/5 I )</formula><p>There are two general aspects of recursive learning in a FAN-based environment that we use for better efficiency: first. as discussed in Section 11, there are very few cases in practical life where it is necessary to perform learning at unjustified gates with an unspecified output signal. Therefore, learning for unjustified gates with an unspecified output signal shall be done with a maximum recursion level of 'r,,,,, = 5 if the current maximum recursion level T,,, is larger than 5. Otherwise, no learning is performed for such gates. '5' was chosen intuitively to suppress the unnecessary recursions so that they contribute little to the total CPU-time but still guarantee the completeness of the algorithm. Second, in a FAN-based algorithm, there are two possibilities how the decision making can fail: inconsistency and X-path check failure. In the first case.</p><p>we initially perform only procedure mukeall_implit~atiotzs( ).</p><p>Only when the maximum recursion level exceeds 3 do we also perform faultpropagation_learnit~~().</p><p>To illustrate the different nature of the two searching schemes, we first compare recursive learning to the traditional search by looking only a redundant faults. In our first experiment, we target only all redundant faults in the ISCASSS <ref type="bibr">[19]</ref> benchmarks and the seven largest ISCAS89 1201 benchmarks. The results are given in Table <ref type="table" target="#tab_7">VII</ref>. The first column lists the circuit that is examined; the second column shows the number of redundant faults of the respective circuit. Only these are targeted by the test generation algorithm. First, we run FAN with a backtrack limit of 1000; i.e., the traditional searching scheme is used and the fault is aborted after 1000 backtracks. The third column shows the number of backtracks for each circuit. The next two columns show the CPU-time in seconds and the number of aborted faults. In the second run, we use recursive learning instead of the decision tree. Columns 6 and 7 give the CPU-time and the number of aborted faults for recursive learning. The next four columns show for how many faults the highest depth of recursion was chosen by the algorithm in Fig. <ref type="figure" target="#fig_11">8</ref>. For example, for circuit c432, one redundancy could be identified without any learning. Three redundancies werc identified in the first learning level.</p><p>Impressively, the results show the superiority of recursive learning for redundancy identification compared to the decision tree. Look, for example, at a circuit c3540: With the decision tree 5 , faults are aborted after performing 1000 backtracks each. There are a total of 5000 backtracks for this circuit. Obviously, for 132 redundancies, there have been no backtracks at all. We observe an "all-or-nothing-effect" typical for redundancy identification with the decision tree. If the implications fail to reveal the conflict, the search space has to be exhausted to prove that no solution exists. This is usually intractable.</p><p>Recursive learning and the search based on the decision tree are in a complementary relationship: the latter is the search for a sufficient solution, its pathological cases being where no solution exists (redundant faults); the former is the search for all necessary conditions. If recursive learning was used to prove that a fault is testable, without constructively generating a test vector, the pathological cases are the cases where no conflict occurs; i.e., we have to exhaust the search space if a solution exists (testable faults).</p><p>Although recursive learning is always used in combination with making optional decisions to generate a test vector such as given in Fig. <ref type="figure" target="#fig_11">8</ref>, it is not wise to use it in cases where many solutions exist that are easy to find. For those cases, it is faster to perform a few backtracks with the decision tree, as we have already shown by the results in <ref type="bibr">[18]</ref>. There are many efficient ways to handle these "easy" faults. In this paper, we choose to perform 20 backtracks with the decision tree. These are split into two groups of ten backtracks, each. For the first ten backtracks, we use the FAN-algorithm with its usual heuristics. When ten backtracks have been performed, the fault is aborted and re-targeted again. The second time we use orthogonal heuristics. This means that we always assign the opposite value at the fanout objectives <ref type="bibr">[3]</ref> of what FAN'S multiple backtrace procedure suggests. This allows us to explore the search space in an orthogonal direction compared to our first attempt. For testable faults, this procedure has been shown to be very effective. As an example, in circuit c6288 when test generation was performed for all faults, 225 faults remained undetected after the first ten backtracks. After the following ten backtracks with orthogonal heuristics, only ten faults were left. Faults that remain undetected after these 20 backtracks are aborted in phase I; they represent the difficult cases for most FAN-based atpg-tools. These pathological faults are the interesting cases when comparing the performance of the two searching techniques.</p><p>Table <ref type="table" target="#tab_8">VI11</ref> shows the results of test generation of the ISCAS85 benchmarks and for the 7 largest ISCAS89 benchmarks. After each generated test vector, fault simulation is used to reduce the fault list (faultdropping). No random vectors are used. The first two columns list the circuits under consideration and the number of faults that have been targeted.</p><p>Columns 3 to 5 show the results of the first phase, in which FAN is performed using its original and orthogonal heuristics with a backtrack limit of ten for each pass. Column 3 gives the number of faults that are identified as redundant, and column 4 lists the CPU-times in seconds. Column 5 gives the figures for the aborted faults. All faults aborted in phase 1 are re-targeted in the second phase, in which we compare the performance of recursive learning to the search based on the decision tree.</p><p>The meaning of columns 6 to 15 is analogous to Table <ref type="table" target="#tab_7">VII</ref>.</p><p>The results in Tables VI11 and IX clearly show the superiority of recursive learning to the traditional searching method in test generation. There are no more aborted faults and the CPU-times for the difficult faults are very short when recursive learning is used. A closer study of the above tables shows that the average CPU-times for each difficult fault is nearly in the same order of magnitude as for the easy faults (with only few exceptions). The results show that recursive learning can replace the "whole bag of tricks" that has to be added to the FAN algorithm if full fault coverage is desired for the ISCAS benchmarks. The implementation of our base algorithm is rather rudimentary, so that a lot of speedup can still be gained by a more sophisticated implementation. Since the focus of this paper is the examination of a new searching method and not presentation of a new tool, no effort has been made to combine recursive learning with a selection of state-of-the-art heuristics as they are used for example in <ref type="bibr">[17]</ref>.</p><p>Recursive learning does not affect the speed and memory requirements of atpg-algorithms as long as it is not invoked; there is no preprocessing or pre-storing of data. This is an important aspect if test generation is performed under limited resources, as pointed out <ref type="bibr">[21]</ref>. If recursive learning is actually invoked, some additional memory is necessary in order to store the current situation of value assignments in each recursion level. The different flags that steer the implication procedure and store the current signal values at each gate have to be handled separately in each level of recursion. As a rough estimate, this results in an overhead of 25 Bytes for each gate in the circuit if we assume that there are 5 flags to steer the implications and a maximum recursion depth of 5. For a circuit with 100,000 gates we obtain an overhead of 2.5 Mbytes, which is usually negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RECURSIVE LEARNING IN LOGIC VERIFICATlON AND OPTIMIZATION</head><p>Recent years have seen much progress in the automation of the design process for large integrated circuits. Tools for automatic synthesis play a crucial role in the progress of the VLSI industry. Still in many case, the designer must resort to manual modifications, or utilization of certain tailormade custom software and algorithms to fulfill the special requirements of a particular design. Because the size of the circuits grows continuously in this phase of the design process, errors are highly likely, the human designer possessing little insight into the functionality of the design-especially after automatic synthesis procedures have been applied in an earlier design phase. Verification techniques have, therefore, become extremely important, with considerable research currently being conducted toward development of efficient verification methods for the various steps in the design process.</p><p>Recursive learning-based techniques have been shown to be very useful <ref type="bibr">[22], [23]</ref> in (formal) logic verification problems for combinational circuits; i.e., the problem of identifying whether two circuits are functionally equivalent. Two com-~ TabIe IX shows the results if all faults are targeted. There is neither a random phase nor fault dropping.</p><p>A logic verification tool, HANNIBAL <ref type="bibr">[22]</ref>, has been developed recently. HANNIBAL proceeds to verify the functional equivalence of two combinational circuits A and B in the following manner. Consider Fig. <ref type="figure" target="#fig_12">9</ref>; for simplicity, assume that two circuits have only one output. By combining the two circuits to form a new circuit with output E that is an EXOR of A and B , the logic verification problem obviously is reduced to solving the Boolean satisfiability problem for the output signal E. The implication procedure described here, in principle, represents a simple method to check the Boolean satisfiability of E : if the precise implications for E = 1 produce a conflict, then it follows that E = 0 and the two circuits are equivalent. If no conflict occurs, the precise implication procedure determines all the value assignments necessary to generate a distinguishing vector. As pointed out, memory requirements grow linearly and CPU-time grows exponentially with the maximum depth of recursion. To complete the verification task in acceptable CPUtime, it becomes essential to keep the recursive depth as small as possible.</p><p>Recursive learning can often make us of "similarities" that exist between the two circuits under consideration. These similarities can, logically, be expressed as (usually indirect) implications between signals of different circuits. Recursive learning is a powerful technique toward identifying precisely these implications, when they exist. Note that these implications immediately indicate functional equivalence of internal nodes (as a special case). In </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE IX</head><p>EXPERIMENTAL. RESULTS FOR TEST GEKERATION WII'HOUT FAULT DROPPING ( S U N SPARC 10/5 1 ) through both circuits using recursive learning to identify and store indirect implications. In phase 2, a test generator based on recursive learning is involved to justify E = 1. For more information, refer to <ref type="bibr">[22]</ref>.</p><p>In <ref type="bibr">[23]</ref>, a method using recursive learning is proposed to make any functional-based verification tool more efficient. Internal equivalencies found by recursive learning greatly aid in limiting the amount of resources used by functional approaches to logic verification. This method also provides a trade-off between structural and functional approaches to logic verification. Using recursive learning, functionally equivalent internal signal are extracted and Ordered Binary Decision Diagrams are formed, treating these internal equivalent signals as the pseudo-inputs. As demonstrated in <ref type="bibr">[23]</ref>, this greatly reduces the OBDD sizes required. In one of the ISCAS benchmark circuits, c3540, this hybrid approach produces better results in terms of CPU times then either of the approaches. In some circuits such as c6288, which are known to consume exponential memory by functional methods, the structural phase of this hybrid method alone proves successful.</p><p>In [26], [24] a novel approach using recursive learning is proposed for multi-level logic optimization. Motivated by the observation in Section 111-A, a general ATPG-based approach to logic optimization is currently developed deriving circuit transformations from implications. In <ref type="bibr">[26]</ref> it is shown that implications can be used to determine for each circuit node those functions in the network with respect to which this node has only one cofactor. Furthermore, recursive learning, if performed for the 5-valued logic alphabet of Roth, can identify permissible functions for minimizing the circuit. In traditional synthesis techniques, an important issue is to perform "good' divisions. Our preliminary research has shown that recursive learning permits to identify good Boolean divisors that justify the effort to attempt a Boolean division.  Some preliminary results of this optimization on combinational benchmark circuits are listed in Table X, which gives a comparison between the number of literals present in the optimized circuit between MIS-I1 and the recursive leamingbased method <ref type="bibr">[26]</ref>. <ref type="bibr">Furthermore,</ref><ref type="bibr">in [25 ]</ref> it was shown that recursive learning can also be used for optimizing sequential circuits. The method of [25] is based on adding and removing connections in the circuit. Recursive learning is used to identify permissible connections that can be added as well as for quick redundancy removal.</p><p>Recursive learning not only allows quick removal of redundancies, but it also gives valuable information on how to transform the circuit. Indirectness of implications closely relates to suboptimality in the circuit. Based on this observation, an approach to logic optimization proposed in [26] can be viewed as a generalization of the earlier proposed approach in (251.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>We have presented a new technique called recursive learning as an alternative search method for test generation in digital circuits and with potential application to other CAD problems. Results clearly show that recursive learning is, by far, superior to the traditional branch-and-bound method based on a decision tree. This is a very promising result, especially if we keep in mind that recursive learning is a general new concept to solve the Boolean satisfiability problem; it therefore has potential for a uniform framework for development of both CAD and test algorithms. Recursive learning can, therefore, be seen under more general aspects. It is a powerful concept to perform a logic analysis on a digital circuit. By recursively calling the presented learning functions, it is possible to exact the entire situation of logic relations between signals in a circuit. This promises the successful application of recursive learning to a wide variety of problems, in addition to the test generation problem discussed in this paper.</p><p>Already, a recursive leaming-based technique has provided fundamental insights in design verification problems [ 2 2 ] , <ref type="bibr">[23]</ref>. Current research also focuses on the application of recursive learning to logic optimization and other related procedures. First results for logic optimization <ref type="bibr">[25]</ref>, <ref type="bibr">[26]</ref> are very promising. Further research of application of recursive learning to various CAD problems, therefore, is warranted.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Log Number 93 13666.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(- 1 Fig. 1 .</head><label>11</label><figDesc>Fig. 1. Circuitry to demonstrate recursive learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Incomplete forward implications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 3. Justification for unjustified gates. E: d-X :=j&gt;+, e-X Fig. 4. Determining a complete set of justifications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>where f l , f z , . . . , fn are unspecified input or output signals of an unjustified gate G is called justification for G if the combination of value assignments in J makes G justified.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Making precise implications for p = 1 assignments are recognized for all unjustified gates in all sets U"-' for arbitrary J r -2 E Cn-2 for arbitrary G, E</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Fig. 6.form the implication p = 1 j p = 1. Note that the learning technique [6], [7], [ 1 I ] cannot perform this implication.Fig.6depicts a scheme useful to better understanding the general procedure of recursive learning and the proof of Theorem 1.During the test generation process, optional assignments are made. After each optional assignment, the resulting necessary assignments must be determined. This is the task of the implication procedure. Many necessary assignments can be determined by performing direct implications only. Direct implications can handle the special case where there is only one possible justification for an unjustified gate. (Note that this represents another possibility to define "direct" implications.) The left column in Fig.6shows the situation as it occurs after each optional assignment during the test generation process. After performing direct implications, we have obtained a situation of value assignments where only those unjustified gates (dark spots in Fig.6) are left that allow for more than one justification. These are examined by learning. Recursive learning examines the different justifications for each unjustified gate, which results in new situations of value assisgnments in the first learning level. If value assisgnments are valid for all possible justifications of an unjustified gate in level 0, i.e., if they lie in the intersection of the respective sets of value assignments in learning level I (shaded area), then they actually belong to the set of value assignments in level 0. This is indicated schematically in Fig.6. However, the set of value assignments in learning level 1 may be incomplete as well because they also contain unjustified gates and the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Necessary assignments for fault propagation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>:= value of successor-signal; successor-signal := successor of successor-signal if (successor-signal is output of inverting gate ) else 1 assign: value of successor-signal := INV(fault-value) assign: value of successor-signal := fault-value make-all-implications(i+l , rmax); set up list of new D-frontier ~r + f ; if ( r&lt; rmax and current sensitization is consistent ) fault-propagation-learning(r+l, rmax); 1 if there is one or several signals f in the circuit, which each assume the same logic value V for all non-conflicting sensitizations, then learn: f=V is uniquely determined in level r, make direct implications for learned values in level r if all sensitizations result in a conflict, then learn: fault propagation in level r impossible (conflict) the necessary assignment n = 0 in Fig. 7 is explained step by step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Algorithm for choosing T ~, , ~, ~.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Logic verification based on indirect implication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>[22], we have presented a prototype verification tool based on these concepts. Verification is conducted in two phases. During the first phase, we pass ~ , IEEE TRANSACTIONS ON COMPUTER-4IDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS, VOL. 13, NO. 9. SEPTEMBER 1994 ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I1 USING demoxecursivelearning()</head><label>I1</label><figDesc></figDesc><table><row><cell>I. learning level</cell></row><row><cell>generally valid</cell></row><row><cell>,ignal values)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I11 PRECISE IMPLICATION PROCEDURE</head><label>I11</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV DEMONSTRATING PROCEDURE MAKE ALL IMPLICATIONSO</head><label>IV</label><figDesc></figDesc><table><row><cell>iitially: FO;</cell><cell></cell></row><row><cell cols="2">flake-all-implications(r, rmax)</cell></row><row><cell>{</cell><cell></cell></row><row><cell cols="2">make all direct implications and set up a list U' of</cell></row><row><cell cols="2">resulting unjustified gates</cell></row><row><cell>if r&lt;r</cell><cell>: learning</cell></row><row><cell>{</cell><cell></cell></row><row><cell cols="2">for each gate G , , x=1,2.., in U': justifications</cell></row><row><cell>(</cell><cell></cell></row><row><cell cols="2">set up list of justifications GXC'</cell></row><row><cell cols="2">for each justification Ji E GxCr:</cell></row><row><cell>{</cell><cell></cell></row><row><cell cols="2">-make the assignments contained in Ji</cell></row><row><cell cols="2">-make-all-implications(r+l, rmax )</cell></row><row><cell>1</cell><cell></cell></row><row><cell cols="2">if there is one or several signals f in the circuit, which</cell></row><row><cell cols="2">assume the same logic value V for all consistent</cell></row><row><cell cols="2">justifications Ji eGxCr then learn: f=V is uniquely</cell></row><row><cell cols="2">determined in level r. make direct implications for all</cell></row><row><cell cols="2">learned values in level r</cell></row><row><cell cols="2">if all justifications are inconsistent, then learn: given</cell></row><row><cell cols="2">situation of value assignments in level r is inconsistent</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table IV lists the single steps to per-</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table VI ,</head><label>VI</label><figDesc>how faultpropagation-learning is used to determine</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V PROCEDURE FAULT PROPAGATION LEARNINGO ault-propagation-learning(r, rmax)</head><label>V</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI DEMONSTRATING FAULT PROPAGATION LEARNINGO</head><label>VI</label><figDesc></figDesc><table><row><cell>learning level</cell><cell>1. learning level</cell></row><row><cell>generally valid</cell><cell>3-frontier sianal b-</cell></row><row><cell>ignal values)</cell><cell>1 sensitization</cell></row><row><cell></cell><cell>successor of b:</cell></row><row><cell>= Ib,e,g,h)</cell><cell></cell></row><row><cell>enter</cell><cell>successor of j.</cell></row><row><cell>learning -&gt;</cell><cell></cell></row><row><cell>n=C</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI11 EXPERIMENTAL RESULTS FOR TEST GENERATION WITH FAULT DROPPING (SUN SPARC 10/5 1)</head><label>VI11</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="3">1. PHASE (eliminate</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">easy faults)</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">FAN with backtrack</cell></row><row><cell></cell><cell></cell><cell cols="2">limit of 10+10</cell><cell>VE LEARNING</cell></row><row><cell cols="3">circuit no. faults red.</cell><cell cols="2">time aborted</cell></row><row><cell></cell><cell>targeted</cell><cell></cell><cell>[SI</cell><cell></cell></row><row><cell>c432</cell><cell>93</cell><cell>I</cell><cell>I</cell><cell>3</cell></row><row><cell>c4YY</cell><cell>122</cell><cell>8</cell><cell>4</cell><cell>0</cell></row><row><cell>c880</cell><cell>95</cell><cell>0</cell><cell>2</cell><cell>0</cell></row><row><cell>cl355</cell><cell>185</cell><cell>8</cell><cell>12</cell><cell>0</cell></row><row><cell>c1908</cell><cell>178</cell><cell>7</cell><cell>11</cell><cell>2</cell></row><row><cell>c2670</cell><cell>343</cell><cell>98</cell><cell>26</cell><cell>19</cell></row><row><cell>c3540</cell><cell>392</cell><cell>127</cell><cell>47</cell><cell>5</cell></row><row><cell>15315</cell><cell>460</cell><cell>59</cell><cell>37</cell><cell>0</cell></row><row><cell>c6288</cell><cell>80</cell><cell>34</cell><cell>15</cell><cell>0</cell></row><row><cell>c7552</cell><cell>533</cell><cell>67</cell><cell>210</cell><cell>64</cell></row><row><cell>s5378</cell><cell>460</cell><cell>7</cell><cell>34</cell><cell>0</cell></row><row><cell>s9234</cell><cell>1230</cell><cell>389</cell><cell>267</cell><cell>56</cell></row><row><cell>~13207</cell><cell>1096</cell><cell>133</cell><cell>309</cell><cell>16</cell></row><row><cell>~15850</cell><cell>1295</cell><cell>374</cell><cell>803</cell><cell>10</cell></row><row><cell>~35932</cell><cell>4194</cell><cell>3128</cell><cell>1308</cell><cell>0</cell></row><row><cell>~38417</cell><cell>4021</cell><cell>153</cell><cell>1108</cell><cell>8</cell></row><row><cell>~38584</cell><cell>3301</cell><cell>1321</cell><cell>890</cell><cell>24</cell></row><row><cell cols="5">binational circuits are functionaily equivalent if they respond</cell></row><row><cell cols="5">to an arbitrary input pattern with the same output pattern.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE X</head><label>X</label><figDesc>COMPARISON OF THE RECURSIVE LEARNINGBASED OETIMIZATION TECHNIQUE WITH MIS</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Recursive</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Learning-based</cell></row><row><cell>Circuit</cell><cell>In i t i a 1</cell><cell>MIS-I1</cell><cell>Method [26]</cell></row><row><cell>cl355</cell><cell>562</cell><cell>550</cell><cell>544</cell></row><row><cell>cl908</cell><cell>769</cell><cell>55 1</cell><cell>517</cell></row><row><cell>c2670</cell><cell>1023</cell><cell>74 1</cell><cell>718</cell></row><row><cell>c3540</cell><cell>1658</cell><cell>1253</cell><cell>1154</cell></row><row><cell>4 3 2</cell><cell>270</cell><cell>196</cell><cell>161</cell></row><row><cell>c499</cell><cell>562</cell><cell>550</cell><cell>544</cell></row><row><cell>c5135</cell><cell>2425</cell><cell>1740</cell><cell>1697</cell></row><row><cell>c6288</cell><cell>3313</cell><cell>3313</cell><cell>3240</cell></row><row><cell>c7552</cell><cell>3087</cell><cell>2157</cell><cell>1855</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>~ b</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors are particularly grateful to Prof. Joachim Mucha, Head of Institut fur Theoretische Elektrotechnik, Universitat Hannover, Germany for his support of this work.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS, VOL. 13, NO. 9, <ref type="bibr">SEPTEMBER 1994</ref> Wolfgang Kunz (S'9O-M'91) was born in <ref type="bibr">Saarbrucken, Germany, on February 7, 1964</ref><ref type="bibr">From 1984</ref><ref type="bibr">to 1989, he</ref>   <ref type="bibr">(Prentice Hall, 1986 and</ref><ref type="bibr">1991)</ref>. Dr. Pradhan is a Fellow of the IEEE and is a recipient of the Humboldt Distinguished Senior Award.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Polynomially complete fault detection problems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ibarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sahni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="242" to="249" />
			<date type="published" when="1975-03">Mar. 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An Implicit Enumeration Algorithm to Generate Tests for Combinational Logic Circuits</title>
		<author>
			<persName><forename type="first">P</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TranJ. Cornput</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="215" to="222" />
			<date type="published" when="1981-03">Mar. 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the Acceleration of Test Generation Algorithms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shimono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th In!. Symp. on Fault Tolerant Computing</title>
		<meeting>13th In!. Symp. on Fault Tolerant Computing</meeting>
		<imprint>
			<date type="published" when="1983">1983</date>
			<biblScope unit="page" from="98" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Topological Search Algorithm for ATPG</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kirkland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th Design Automation Conf</title>
		<meeting>24th Design Automation Conf</meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="502" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">CONTEST: A Fast ATPG Tool of Very Large Combinational Circuits</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">T</forename><surname>Mahlstedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Griining</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ozcan</surname></persName>
		</author>
		<author>
			<persName><surname>Daehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Computer Aided Design</title>
		<meeting>Int. Conf. Computer Aided Design</meeting>
		<imprint>
			<date type="published" when="1990-11">Nov. 1990</date>
			<biblScope unit="page" from="222" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SOCRATES: A highly efficient automatic test pattern generation system</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sarfert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Test Conf</title>
		<meeting>Int. Test Conf</meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="101" to="1026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improved Deterministic Test Pattern Generation with Applications to Redundancy Identification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Auth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Computer-Aided Design</title>
		<imprint>
			<biblScope unit="page" from="81" to="81" />
			<date type="published" when="1989-07">July 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Method to Calculate Necessary Assignments in Algorithmic Test Pattern Generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rajski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Test Conf</title>
		<meeting>Int. Test Conf</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="25" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Nine-Valued Logic Model for Test Generation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Muth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="630" to="636" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Logic System for Fault Test Generation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Akers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1976-06">June 1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Diagnosis of automata failures: A calculus &amp; a method</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM J. Res. DeveZop</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="278" to="291" />
			<date type="published" when="1966-07">July 1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Accelerated Dynamic Learning for Test Generation in Combinational Circuits</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kunz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Computer-Aided Design</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">684694</biblScope>
			<date type="published" when="1993-05">May 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Search State Equivalence for Redundancy Identification and Test Generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Giraldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bushnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Test Conf</title>
		<meeting>Int. Test Conf</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page">184193</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An Efficient Test Generation Algorithm Based on Search State Dominance</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fujino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fujiwara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symp. Faulr-Tolerant Comp</title>
		<meeting>Int. Symp. Faulr-Tolerant Comp</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page">246253</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient Generation of Test Patterns Using Boolean Difference</title>
		<author>
			<persName><forename type="first">T</forename><surname>Larrabee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">inProc. In!. Test Conf</title>
		<imprint>
			<biblScope unit="page" from="795" to="801" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Transitive Closure based Algorithm for Test Generation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Chakradhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th Design Automarion Conf</title>
		<meeting>28th Design Automarion Conf</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="353" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ATPG for Ultra-Large Structured Designs</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Waicukauski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Shupe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Giramma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Matin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Ini. Test Conf</title>
		<meeting>Ini. Test Conf</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recursive Learning: An Attractive Alternative to the Decision Tree for Test Generation in Digital Circuits</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kunz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Pruc. Int. Test Conf</title>
		<imprint>
			<biblScope unit="page">816825</biblScope>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Neutral Netlist of 10 Combinational Benchmark Designs and a Special Translator in Fortran</title>
		<author>
			<persName><forename type="first">F</forename><surname>Brglez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fujiwara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symp. on Circuits and Systems. Special Session on ATPG and Fault Simulation</title>
		<meeting>Int. Symp. on Circuits and Systems. Special Session on ATPG and Fault Simulation</meeting>
		<imprint>
			<date type="published" when="1985-06">June 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Combinational Profiles of Sequential Benchmark Circuits</title>
		<author>
			<persName><forename type="first">F</forename><surname>Brglez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Symp. on Circuits and Systems</title>
		<imprint>
			<date type="published" when="1989-05">May 1989</date>
			<biblScope unit="page" from="1929" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Small Test Generator for Large Designs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kundu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int</title>
		<meeting>Int</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">HANNIBAL: An Efficient Tool for Logic Verification Based on Recursive Learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kunz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conj: Computer-Aided Design</title>
		<meeting>Int. Conj: Computer-Aided Design<address><addrLine>Santa Clara</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-11">Nov. 1993</date>
			<biblScope unit="page" from="538" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Improving OBDD Based Verification using Internal Equivalencies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kunz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pradhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994-01">Jan. 1994</date>
			<pubPlace>College Station, Texas</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science, Texas A&amp;M Univ</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. 94-019</note>
	<note>submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recursive Learning Technique and Applications to CAD</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kunz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pradhan</surname></persName>
		</author>
		<idno>No. 08/26372 1</idno>
	</analytic>
	<monogr>
		<title level="s">US Patent Application</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sequential Logic Optimization by Redundancy Addition and Removal</title>
		<author>
			<persName><forename type="first">L</forename><surname>Entrena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCAD</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="310" to="315" />
			<date type="published" when="1993-11">Nov. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Multilevel Logic Optimization by Implication Analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kunz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Menon</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>ICCAD&apos;94, to be published</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
