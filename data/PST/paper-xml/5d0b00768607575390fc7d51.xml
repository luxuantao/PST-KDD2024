<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-lingual Transfer Learning for Japanese Named Entity Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Johnson</surname></persName>
							<email>ajohnson@coli.uni-saarland.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Saarland University</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Penny</forename><surname>Karanasou</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Judith</forename><surname>Gaspers</surname></persName>
							<email>gaspers@amazon.de</email>
							<affiliation key="aff2">
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
							<email>dietrich.klakow@lsv.uni-saarland.de</email>
							<affiliation key="aff3">
								<orgName type="institution">Spoken Language Systems Saarland University</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-lingual Transfer Learning for Japanese Named Entity Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work explores cross-lingual transfer learning (TL) for named entity recognition, focusing on bootstrapping Japanese from English. A deep neural network model is adopted and the best combination of weights to transfer is extensively investigated. Moreover, a novel approach is presented that overcomes linguistic differences between this language pair by romanizing a portion of the Japanese input. Experiments are conducted on external datasets, as well as internal large-scale realworld ones. Gains with TL are achieved for all evaluated cases. Finally, the influence on TL of the target dataset size and of the target tagset distribution is further investigated.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Due to the growing interest in voice-controlled devices, such as Amazon Alexa-enabled devices or Google Home, porting these devices to new languages quickly and cheaply has become an important goal. One of the main components of such a device is a model for Named Entity Recognition (NER). Typically, NER models are trained on large amounts of annotated training data. However, collecting and annotating the required data to bootstrap a large-scale NER model for an industry application with reasonable performance is time-consuming, costly, and it doesn't scale to a growing number of new languages.</p><p>Aiming to reduce the time and costs needed for bootstrapping an NER model for a new language, we leverage existing resources. In particular, we</p><p>The author Andrew Johnson conducted the work for this paper during an internship at Amazon, Aachen, Germany. explore cross-lingual transfer learning, in which weights from a trained model in the source language are transferred to a model in the target language. Transfer learning (TL) has been shown previously to improve performance for target models <ref type="bibr">(Yang et al., 2017;</ref><ref type="bibr" target="#b12">Lee et al., 2017;</ref><ref type="bibr">Riedl and Padó, 2018)</ref>. However, work related to crosslingual transfer learning for NER has mainly focused on rather similar languages, e.g. transferring from English to German or Spanish. In contrast, we focus on transferring between dissimilar languages, i.e. from English to Japanese.</p><p>We present experimental results on external, i.e. publicly available, corpora, as well as on internally gathered large-scale real-world datasets. First, a deep neural network model is developed for NER, and we extensively explore which combinations of weights are most useful for transferring information from English to Japanese. Furthermore, aiming to overcome the linguistic and orthographic dissimilarity between English and Japanese, we propose to romanize the Japanese input, i.e. convert the Japanese text into the Latin alphabet. This results in a common character embedding space between the two languages, and intuitively should allow for more efficient transfer learning at the character level.</p><p>Gains with TL are achieved on all evaluated target datasets, even large-scale industrial ones. Moreover, the effect of TL on the target dataset size and of the target tagset distribution is investigated. Finally, we show that similar gains are achieved when applying the proposed approach from English to German, indicating the possibility to generalize it both to European and non-European target languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">NER model</head><p>The growth in neural approaches spurred a push towards "NLP from scratch", that is, without engineering task-or language-specific features by hand <ref type="bibr" target="#b4">(Collobert et al., 2011)</ref>. Currently, mainly recurrent and/or convolutional neural networks are applied. In <ref type="bibr" target="#b3">Chiu and Nichols (2015)</ref>, the authors combined a Bi-LSTM to learn long-distance relationships with a CNN to generate character-level representations. A Bi-LSTM-CNN-CRF showed state-of-the-art performance on NER <ref type="bibr" target="#b13">(Ma and Hovy, 2016)</ref>. CNNs have been shown to be less useful for languages like Japanese, in which average NEs are quite short at around two characters on average <ref type="bibr" target="#b14">(Misawa et al., 2017)</ref>. Bi-LSTM-CRF models without any CNN layer have also performed well on NER <ref type="bibr" target="#b6">(Huang et al., 2015;</ref><ref type="bibr" target="#b11">Lample et al., 2016)</ref>. Using this architecture with a novel type of embeddings termed "contextual string embeddings" has recently led to state-of-the-art results <ref type="bibr" target="#b0">(Akbik et al., 2018)</ref>.</p><p>For our baseline NER system we use a Bi-LSTM architecture that takes word and character embeddings as input. The same architecture is used both for the source and the target languages to allow for transfer of weights when the crosslingual TL is applied. This architecture largely resembles the model in <ref type="bibr" target="#b11">Lample et al. (2016)</ref>, except for the final CRF layer. For every token, word and character embeddings are generated. The latter are passed through a character Bi-LSTM, the output of which is concatenated with the word embeddings. This combined representation is then passed into the word Bi-LSTM, followed by a dense layer and a final softmax layer. An example for English is presented in Figure <ref type="figure" target="#fig_0">1</ref>. Note that the character level inputs in this figure are unigrams, but in practice we use bigrams, i.e. "Ye" and "es" for "Yes".</p><p>Although including a CRF as the final layer tends to raise scores overall <ref type="bibr" target="#b16">(Reimers and Gurevych, 2017;</ref><ref type="bibr" target="#b6">Huang et al., 2015)</ref>, others have demonstrated that transferring CRF weights does not contribute to meaningful gain in the context of TL <ref type="bibr" target="#b12">(Lee et al., 2017)</ref>. In this work, a CRF layer is not included in the baseline. In another recent work, monolingual Japanese models have used "character-based models", with labels assigned to each individual character <ref type="bibr" target="#b14">(Misawa et al., 2017)</ref>. We do not employ this approach since our source model in English is not character-based. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Transfer Learning</head><p>Cross-lingual TL is applied to transfer knowledge from the source to the target language. Working with neural network-based models, this is achieved by initializing some layers of the target network using the weights of the source network, which is assumed to be already trained using a (large) available annotated training corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Related work</head><p>One of the first works on cross-lingual TL for NER that did not rely on parallel corpora used a CRF and included hand-crafted features <ref type="bibr">(Zirikly and Hagiwara, 2015)</ref>. Currently, most work on TL is done with neural models. Because neural models often consist of multiple layers, one important design decision is which layers to transfer from source to target. Much related work involves only transferring a single layer or specific combination of layers. In <ref type="bibr" target="#b12">Lee et al. (2017)</ref> the authors present more thorough results combining lower and higher layers, without transferring intermediate layers though. In <ref type="bibr">Yang et al. (2017)</ref> it is suggested to transfer only the character embeddings and the character RNN weights between languages. The reason for this is likely that many languages written in the Latin alphabet have a large charset overlap, but far less vocabulary overlap.</p><p>Another question of interest concerns the pair of languages between which TL can be achieved. Past work has shown transferring to a related language to help more than to an unrelated one for NER, POS tagging, and NMT <ref type="bibr">(Zirikly and Hagiwara, 2015;</ref><ref type="bibr" target="#b9">Kim et al., 2017;</ref><ref type="bibr" target="#b5">Dabre et al., 2017)</ref>. In <ref type="bibr">Yang et al. (2017)</ref> it is mentioned that without additional resources, it is "very difficult for transfer learning between languages with disparate alphabets". This background suggests TL from En-glish to Japanese to be non-trivial.</p><p>Finally, another consideration with TL is the size of the target dataset. For one NER task, TL gains were shown to decrease to nearly zero as the size of the target training data increased to around 50k tokens <ref type="bibr" target="#b12">(Lee et al., 2017)</ref>. Similarly, for domain adaptation, a "phase transition" was observed in the amount of used target data, such that using source data was not effective when the target model was trained on 3.13k or more target instances <ref type="bibr" target="#b2">(Ben-David et al., 2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Specificities of Japanese language</head><p>Transferring between English and Japanese is more challenging and less explored than transferring between languages with the same alphabet. Japanese is written using an unsegmented mixture of two syllabaries as well as thousands of Chinese characters, which encode semantic information.</p><p>A process that we explore in this work to overcome the orthographic dissimilarity is the "romanization" of Japanese text, i.e. the process of transcribing Japanese text into the Latin alphabet. However, when applying romanization we lose the disambiguating effect that characters have. In fact, due to its small phonemic inventory, Japanese contains many homophones. Consider the homophone pairs in Table <ref type="table">1</ref>, actual examples taken from our external Japanese dataset. In their original written forms, there is no ambiguity, as there are different characters representing each meaning. This information, which is crucial here to determining which is the NE, is lost after romanization. Empirical results for sentiment classification have confirmed that romanizing Japanese text hurts performance for a monolingual model <ref type="bibr">(Zhang and LeCun, 2017)</ref>.</p><formula xml:id="formula_0">押収 欧州 加盟 亀井 oushuu oushuu kamei kamei to seize Europe to join Kamei [surname]</formula><p>Table <ref type="table">1</ref>: Japanese Homophones</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Proposed model</head><p>Since we explore transferring weights from a source network, an important design decision is which layers to transfer. Addressing this question, we evaluate different combinations of layers to find the best one for our task. We group our weights together as shown in Figure <ref type="figure" target="#fig_0">1</ref> (grouped layers in boxes): character embeddings and character Bi-LSTM weights form the "character weights", word embeddings and word Bi-LSTM weights form the "word weights", and dense layer weights form the "dense weights". All possible combinations of these three groups are explored. To account for the incomplete overlap when transferring embeddings, we only update the vectors that correspond to char n-grams or words observed in both the source and target training data. This is a limitation that could be overcome if multi-lingual embeddings were used which we leave for future work.</p><p>For transferring to a target language with a different writing system than the source one we propose the Mixed Orthographic Model (MOM). Specifically, the character layer inputs are romanized while the word layer inputs are kept in their original Japanese text. This allows for transfer of character information from a source to a target language with originally different writing systems by creating a common and overlapping character embedding space. At the same time, keeping the original Japanese text in the word level allows us to keep the capacity to disambiguate homophones, which is lost via the romanizing process as explained in the previous section (Section 3.2).</p><p>Here is an example of the MOM for the utterance "play jazz": Raw utterance "ジャズを流して" Word input ["ジャズ", "を", "流して"] Character input ["jazu", "wo", "nagashite"]<ref type="foot" target="#foot_0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental setup</head><p>In this section, the datasets as well as the details of the developed NER model are presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>For our experiments we make use of datasets in three languages. First, an English dataset is used to train the source NER model. Then, a target language dataset, which is smaller in size than the source dataset, is used to build a target NER model. This serves as the target baseline. The weights transferred from the source model are used to initialize this target model, which is then trained with the available target data, resulting in a new target model. As mentioned before, the focus of this paper is TL between dissimilar languages, and thus the main experiments use a Japanese dataset as the target corpus. However, for the sake of comparison, we also conducted some experiments using a German target dataset, thus transferring between more similar languages, i.e. both belonging to the indo-European family, and evaluating the generalization power of the adopted approach.</p><p>We evaluate our approach both on external and internal datasets. External datasets are composed of company data and are mainly used for comparing our monolingual models to the state-of-the-art, while internal datasets are composed of publicly available data and are used to explore potential data reductions in a real-world large-scale industry setting.</p><p>Segmentation and romanization of Japanese text are performed with the open source Japanese text analyzer MeCab<ref type="foot" target="#foot_1">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">External data</head><p>As external English data, we use the English CoNLL 2003 NER dataset <ref type="bibr">(Tjong Kim Sang and De Meulder, 2003)</ref> which contains four named entity (NE) categories.</p><p>We make use of three external datasets for Japanese NER. The first one is the Balanced Corpus of Contemporary Written Japanese (BCCWJ) <ref type="bibr" target="#b8">(Iwakura et al., 2016)</ref>, containing a variety of writing types, such as blogs and magazine articles. In addition, we created a dataset by combining two small Japanese datasets annotated with NEs: i) a small dataset included in the CRF++ tool, and ii) the Kyoto University and NTT Blog Corpus (KNBC) with data from blogs on topics such as tourism, sports, and technology. We are referring to this dataset as "CRF-KNBC".</p><p>Most Japanese NER datasets use IREX tags. Similar to CoNLL 2003, IREX 1999 was a shared task for NER and contains eight tags, three of which are the same as in CoNLL. The remaining tags can be viewed as an expansion of ConLL's fourth category, and hence can be grouped together to have the same tagset as CoNLL. We do this to facilitate TL from English.</p><p>See Table <ref type="table" target="#tab_0">2</ref> for details on the external datasets.  In particular, we shall refer to any dataset containing over one million utterances as "Large", anything with fewer than one million but more than one hundred thousand as "Medium", and anything with fewer than one hundred thousand utterances as "Small". Note the difference in scale from the external data, the largest of which would still be well below the threshold defined here as small. Following this convention, we have the internal datasets presented in Table <ref type="table" target="#tab_2">3</ref>. None of the smaller datasets are subsets of the larger ones; each is an entirely separate dataset. However, each dataset includes the same kind of data and largely shares the same tagset. Another major difference from the external datasets is the size of the tagset. Internal data, including both source and target, use over two hundred distinct tags, which are not evenly distributed. In fact, Figure <ref type="figure" target="#fig_1">2</ref> (in log-log scale) shows a very long tail, with the most frequently observed tags belonging to a very small subset of all possible tags. This characteristic makes the internal data a challenging case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model setup</head><p>For optimizing our NER models we used Adam (Kingma and Ba, 2014) over cross entropy loss. To avoid overfitting, a dropout layer was used before the Word Bi-LSTM. We lowercase all wordlevel input. However, since capitalization is a feature that strongly predisposes a word to be an NE, we did not lowercase the character-level input. No pre-trained word embeddings were used with internal datasets, while external datasets used Polyglot word embeddings <ref type="bibr" target="#b1">(Al-Rfou et al., 2013)</ref>. The word embedding dimensionality was 50, except where Polyglot pre-trained embeddings were used, in which case it was 64. The word LSTM size was set to 300. Character embeddings were 50-dimensional and character bigrams were used. The character LSTM was of size 100 for external datasets and 30 for internal ones. Dropout was set to 0.5. We used the evaluation script from the CoNLL shared task to compute F1 score.</p><p>During the parameter tuning phase, development set performance stabilized after 10 epochs for external models and 20 epochs for internal models. Therefore, we conduct experiments on the test set by training for these respective number of epochs. The scores reported for each model reflect the highest F1 value among all epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Layer combinations for TL</head><p>We first investigate which layer combination yields best results when being transferred. The layer groups defined in Section 3.3 are combined and experiments are conducted on the two external JP datasets as well as on a subset of the JP "Medium" internal one. The results are presented in Table <ref type="table" target="#tab_3">4</ref> as absolute gains against the baseline without TL. Approximate randomization is used for each experiment <ref type="bibr" target="#b15">(Noreen, 1989;</ref><ref type="bibr">Yeh, 2000)</ref>, and all TL gains were found to be significant to p&lt;0.001. The results reported are the average of running experiments five times with different random seeds. In all experiments, the system configuration detailed in Section 4.2 is followed and the The best performing combination (in bold) varies between datasets. However, the "Char+Dense" combination seems to be the most reliable one, providing consistent and significant TL gains over all three evaluated corpora. This combination is different than what was previously reported in litterature <ref type="bibr">(Yang et al., 2017)</ref>, where it was suggested that transferring character level weights suffices. This might be because of the specific nature of our task on transferring weights between languages with dissimilar alphabets. In our case transferring word weights actually performs better than transferring character weights (compare rows "Word" and "Char"). In addition, combining the weights at word or character levels with the next dense layer weights improves further the results (rows "Word+Dense" and "Char+Dense") indicating that this dense layer still captures some language-independent information.</p><p>Due to these results, we use the "Char+Dense" combination in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effect of romanization of Japanese on TL</head><p>The effect of romanization of Japanese is evaluated on one external ("BCCWJ") and a subset of an internal ("Med.-10k") JP dataset. Results are presented in Table <ref type="table" target="#tab_5">5</ref> with and without romanization before and after TL, and consistent gains are shown when MOM is used with TL. In addition, there are significant gains when used without TL in the case of the internal dataset ("Med.-10k"). To the best of our knowledge, this is the first work introducing the MOM and comparing these approaches for Japanese in the context of TL. Since this model gives consistently improved results with TL, all the remaining results on Japanese data will employ this approach.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">TL on external and internal datasets</head><p>Applying the best configuration established previously, i.e. transfer "Char+Dense" layers and use of MOM, the results before and after TL on the full JP datasets are presented in Table <ref type="table" target="#tab_6">6</ref>. Gains with TL are achieved in all evaluated datasets. Moreover, with MOM and TL, we achieve stateof-the-art NER results on BCCWJ, outperforming <ref type="bibr" target="#b7">Ichihara et al. (2015)</ref> (reported F1 score 67.68% vs. ours 69.36%). In addition, important tive gains are achieved by TL in the small external datasets, making our method particularly suited for bootstrapping a new language with very limited available annotated data. Another interesting outcome is that we still see gains in the large internal datasets (i.e. up to 1M training utterances in the internal "Large" set). This will be investigated further in the next section (Section 5.4).</p><p>Results on DE internal datasets are presented for sake of comparison and show the same trends as JP internal datasets, thus revealing the generalization of our approach for cross-lingual TL both to European and non-European target languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Effect of target dataset size on TL</head><p>To further investigate how the size of the target datasets influences the performance of TL, we conducted experiments on different sizes of the internal data. This was done by training on subsets of the original "Large" JP internal training set, with sizes varying from 10k to 1M utterances. Note that the source English training data is still used in full each time. The results are presented in Figure <ref type="figure" target="#fig_2">3</ref>. As expected, larger gains are observed for smaller splits. However, TL still produces statistically significant gains for all split sizes. Note A further analysis of the results on the internal datasets showed that the frequency of a tag class in the target training data correlated the most with TL gain. This is visualized in Figure <ref type="figure" target="#fig_3">4</ref> for a subset of the JP "Small" dataset. An arrow is used for each tag class with the tail of the arrow indicating the F1 score without TL and the point indicating the F1 score of that same class with TL. Thus, classes with gains point upward (blue arrows), while those that performed worse point downwards (red arrows). Classes that showed no change are indicated as circles. These mostly cluster along the bottom as classes that have an F1 score of zero before and after TL. The tags are arranged along the x axis based on their frequency in the target training dataset. generally begin to show gains from TL only after they pass a certain minimum frequency threshold in the target dataset, which appears to be around 100. This may be the reason why we have TL gains even with large target datasets. As infrequent tag classes are observed more and more in larger splits, they begin to cross this threshold and gain from TL. Real-world data generally have long-tailed distributions, thus even very large target datasets are likely to have tag classes with few data which can benefit from TL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>A cross-lingual transfer learning approach for NER was proposed, focusing on dissimilar languages, i.e. English and Japanese. A deep neural network model was adopted and the best layer combination to transfer was extensively investigated. To overcome the orthographic dissimilarity between source and target languages a novel method, the MOM, was proposed that romanizes part of the Japanese input. Gains with TL were consistently achieved on external and large-scale datasets showing that it is possible to transfer knowledge between dissimilar languages, even for large target corpora.</p><p>In the future, the proposed approach could be applied to other dissimilar language pairs, e.g. English and Chinese. Other possible extensions include using multi-lingual embeddings that could complement the currently transferred weights. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: NER model: an English example</figDesc><graphic url="image-1.png" coords="2,304.99,63.80,229.06,139.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Tag distribution-EN internal training data in log-log scale</figDesc><graphic url="image-2.png" coords="5,81.63,62.81,199.00,141.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Applying TL on varying target training size</figDesc><graphic url="image-3.png" coords="6,319.91,62.81,193.00,139.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Gains by tag</figDesc><graphic url="image-4.png" coords="6,307.28,531.46,221.48,159.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Number of utterances per external dataset</figDesc><table><row><cell cols="2">Language Dataset</cell><cell>Train</cell><cell>Test</cell><cell>Dev</cell></row><row><cell>EN</cell><cell cols="4">ConLL 2003 14,987 3,684 3,466</cell></row><row><cell>JP</cell><cell>BCCWJ</cell><cell>3,600</cell><cell>325</cell><cell>324</cell></row><row><cell></cell><cell>CRF-KNBC</cell><cell>2,940</cell><cell>980</cell><cell>979</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Number of utterances per internal dataset</figDesc><table><row><cell>4.1.2 Internal data</cell></row><row><cell>We are mainly interested in exploring TL and the</cell></row><row><cell>resulting potential data reduction in a large-scale</cell></row><row><cell>industry setting with different amounts of target</cell></row><row><cell>data being available, as target data amounts typ-</cell></row><row><cell>ically increase over time due to continuous data</cell></row><row><cell>collection. Internal datasets comprise utterances</cell></row><row><cell>which are representative of user requests to voice-</cell></row><row><cell>controlled devices and are annotated with NEs. To</cell></row><row><cell>explore the benefit of TL during different stages of</cell></row><row><cell>system development, i.e. with availability of dif-</cell></row><row><cell>ferent data sources, we include different datasets</cell></row><row><cell>in our experiments which we distinguish by their</cell></row><row><cell>size.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Absolute gains on JP datasets by transferring different layer combinations MOM (see Section 3.3) is applied.</figDesc><table><row><cell>Layers</cell><cell></cell><cell>Corpora</cell><cell></cell></row><row><cell>Transferred</cell><cell cols="3">BCCWJ CRF-KNBC Med.-10k</cell></row><row><cell>No TL</cell><cell>65.50</cell><cell>50.48</cell><cell>81.64</cell></row><row><cell>Char</cell><cell>+0.34</cell><cell>-2.63</cell><cell>-1.62</cell></row><row><cell>Word</cell><cell>+1.50</cell><cell>+0.86</cell><cell>+1.14</cell></row><row><cell>Dense</cell><cell>-1.54</cell><cell>+2.86</cell><cell>+3.77</cell></row><row><cell>Char+Word</cell><cell>-0.09</cell><cell>+1.33</cell><cell>-0.39</cell></row><row><cell cols="2">Word+Dense +0.63</cell><cell>+4.69</cell><cell>+1.95</cell></row><row><cell>Char+Dense</cell><cell>+3.86</cell><cell>+3.74</cell><cell>+3.72</cell></row><row><cell>Char+Word</cell><cell>+2.35</cell><cell>+3.92</cell><cell>-0.02</cell></row><row><cell>+Dense</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Romanization of Japanese -Effect on TL</figDesc><table><row><cell>Dataset</cell><cell cols="3">No TL With TL Rel. gain</cell></row><row><cell>BCCWJ</cell><cell>65.50</cell><cell>69.36</cell><cell>+5.89</cell></row><row><cell>CRF-KNBC</cell><cell>50.48</cell><cell>54.22</cell><cell>+7.41</cell></row><row><cell>Small</cell><cell>83.15</cell><cell>84.85</cell><cell>+2.04</cell></row><row><cell>Medium</cell><cell>91.64</cell><cell>92.20</cell><cell>+0.61</cell></row><row><cell>Large</cell><cell>91.66</cell><cell>92.21</cell><cell>+0.60</cell></row><row><cell>DE Medium</cell><cell>87.82</cell><cell>88.86</cell><cell>+1.18</cell></row><row><cell>DE Large</cell><cell>89.24</cell><cell>89.63</cell><cell>+0.44</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Results with TL over full JP and DE datasets</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Martin Riedl and Sebastian Padó. 2018. A named entity recognition shootout for german. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), volume 2, pages 120-125. Erik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4, pages 142-147. Association for Computational Linguistics. Zhilin Yang, Ruslan Salakhutdinov, and William W Cohen. 2017. Transfer learning for sequence tagging with hierarchical recurrent networks. arXiv preprint arXiv:1703.06345. Alexander Yeh. 2000. More accurate tests for the statistical significance of result differences. In Proceedings of the 18th conference on Computational linguistics-Volume 2, pages 947-953. Association for Computational Linguistics. Xiang Zhang and Yann LeCun. 2017. Which encoding is the best for text classification in chinese, english, japanese and korean? arXiv preprint arXiv:1708.02657. Ayah Zirikly and Masato Hagiwara. 2015. Crosslingual transfer of named entity recognizers without parallel corpora. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), volume 2, pages 390-396.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Shown prior to conversion into character n-grams</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">http://taku910.github.io/mecab/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contextual string embeddings for sequence labeling</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duncan</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
				<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Polyglot: Distributed word representations for multilingual nlp</title>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
				<meeting>the Seventeenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Named entity recognition with bidirectional lstm-cnns</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><surname>Nichols</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08308</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08">2011. Aug</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An empirical study of language relatedness for transfer learning in neural machine translation</title>
		<author>
			<persName><forename type="first">Raj</forename><surname>Dabre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tetsuji</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Pacific Asia Conference on Language, Information and Computation</title>
				<meeting>the 31st Pacific Asia Conference on Language, Information and Computation</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="282" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Error analysis of named entity recognition in bccwj</title>
		<author>
			<persName><forename type="first">Masaaki</forename><surname>Ichihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanako</forename><surname>Komiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoya</forename><surname>Iwakura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maiko</forename><surname>Yamazaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">2641</biblScope>
		</imprint>
	</monogr>
	<note>Recall</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Constructing a japanese basic named entity corpus of various genres</title>
		<author>
			<persName><forename type="first">Tomoya</forename><surname>Iwakura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanako</forename><surname>Komiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryuichi</forename><surname>Tachibana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Named Entity Workshop</title>
				<meeting>the Sixth Named Entity Workshop</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="41" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cross-lingual transfer learning for pos tagging without cross-lingual resources</title>
		<author>
			<persName><forename type="first">Joo-Kyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Fosler-Lussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2832" to="2838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01360</idno>
		<title level="m">Neural architectures for named entity recognition</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Transfer learning for named-entity recognition with neural networks</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Young Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06273</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01354</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Character-based bidirectional lstm-crf with words and characters for japanese named entity recognition</title>
		<author>
			<persName><forename type="first">Shotaro</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Motoki</forename><surname>Taniguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasuhide</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoko</forename><surname>Ohkuma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Subword and Character Level Models in NLP</title>
				<meeting>the First Workshop on Subword and Character Level Models in NLP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Computer-intensive methods for testing hypotheses</title>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Noreen</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Optimal hyperparameters for deep lstm-networks for sequence labeling tasks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06799</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
