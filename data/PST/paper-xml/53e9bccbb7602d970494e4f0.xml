<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multiple Feature Hashing for Real-time Large Scale Near-duplicate Video Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yiyang@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Jingkuan Song School of ITEE</orgName>
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<postCode>4072</postCode>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
							<email>huang@itee.uq.edu.au</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Heng</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of ITEE</orgName>
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<postCode>4072</postCode>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Shen</surname></persName>
							<email>shenht@itee.uq.edu.au</email>
							<affiliation key="aff2">
								<orgName type="department">School of ITEE</orgName>
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<postCode>4072</postCode>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of ITEE</orgName>
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<postCode>4072</postCode>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">School of Computer and Information</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multiple Feature Hashing for Real-time Large Scale Near-duplicate Video Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9B0D6C0FF44C512EEBEAF15115052738</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Search and Retrieval]: Search Process; H.3.1 [Content Analysis and Indexing ]: Indexing Algorithms</term>
					<term>Measurement</term>
					<term>Experimentation Near-duplicate</term>
					<term>video retrieval</term>
					<term>hashing</term>
					<term>large scale</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Near-duplicate video retrieval (NDVR) has recently attracted lots of research attention due to the exponential growth of online videos. It helps in many areas, such as copyright protection, video tagging, online video usage monitoring, etc. Most of existing approaches use only a single feature to represent a video for NDVR. However, a single feature is often insufficient to characterize the video content. Besides, while the accuracy is the main concern in previous literatures, the scalability of NDVR algorithms for large scale video datasets has been rarely addressed. In this paper, we present a novel approach -Multiple Feature Hashing (MFH) to tackle both the accuracy and the scalability issues of NDVR. MFH preserves the local structure information of each individual feature and also globally consider the local structures for all the features to learn a group of hash functions which map the video keyframes into the Hamming space and generate a series of binary codes to represent the video dataset. We evaluate our approach on a public video dataset and a large scale video dataset consisting of 132,647 videos, which was collected from YouTube by ourselves. The experiment results show that the proposed method outperforms the state-of-the-art techniques in both accuracy and efficiency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>With the rapid development of the Internet techniques, video capturing devices, and video editing softwares, the number of online videos continues to grow at an astonishing speed. Meanwhile, the video-related services, such as video sharing, monitoring, advertising and recommendation, inspire online users' interests and participation to video-related activities, including uploading, downloading, commenting, and searching. A substantial number of videos are uploaded and shared on social websites in each single day. It has been shown that there are a large number of near-duplicate videos (NDVs) on the Web, which are generated in different ways, ranging from simple reformatting, to different acquisitions, transformations, editions, and mixtures of different effects <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b30">30]</ref>. The presence of massive NDVs imposes strong demand for effective near-duplicate video retrieval (NDVR) in many novel applications, such as copyright enforcement, video tagging, online video usage monitoring, video database cleansing, cross-modal divergence detection, video result re-ranking, and so on. To list a few, a typical scenario could be that a Web user wants to get some novel videos but ends up with lots of NDVs in the top-ranked search results returned by the search engine. Another situation could be that a video producer expects to avoid their copyright protected videos being shared on the Internet. Both occasions require NDVR techniques to achieve their goals.</p><p>During recent years, much research effort have been made to NDVR <ref type="bibr" target="#b18">[18,</ref><ref type="bibr">7,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b36">36]</ref>. Most of the existing algorithms usually adopt the following framework for NDVR. First, a video is divided into a sequence of keyframes extracted by time sampling or shotboundary detection algorithms. These keyframes are represented by their visual features, such as color histogram, local points, Local Binary Pattern(LBP), etc. The sequence of the keyframes' features is then regarded as the signature of the video. For the sake of NDVR, the system needs to compare the similarity between the signatures of the query video and each dataset video and return the dataset videos which are most similar to the query example. Generally, there are three typical approaches to measure the similarity between two videos represented by a sequence of keyframes. The first one takes spatial and temporal constraints into consideration by employing sliding window for NDVR <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b29">29]</ref>. The second approach computes pair-wise correlation/distances of each pair of keyframes from the two video clips, where the temporal information is not considered <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b11">11]</ref>. The third kind is to combine all the keyframes into a single compact signature at video level <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b29">29]</ref>. In this way, the similarity between video clips can be directly computed according to the global signature of the videos. The performance of the three approaches is data dependent. Different NDVR performance can be achieved while different video datasets are used.</p><p>Intuitively, the multiple features of video clips, each of which reflects the specific information of video data, are complementary to each other. Simultaneously utilizing the multiple features is helpful for disambiguation. For example, local signature is less robust to the changes in frame rate, video length, captions and global signature is sensitive to changes in contrast, brightness, scale, rotation, camera viewpoint, and so on. It is difficult, if not impossible, to find a single visual feature which is robust to all types of variations. It is therefore particularly important to exploit multiple features for the sake of NDVR, where various variations and distortions could take place between NDVs. For instance, it is easy to figure out that Fig. <ref type="figure" target="#fig_0">1</ref> The scalability of NDVR algorithm is becoming a more and more critical research issue as the number of videos is growing rapidly and most of the video content features are highly complex. How to construct efficient indexing structures to facilitate fast search over large scale video datasets is important for online NDVR, which demands real-time response. Many indexing methods in database literature have been proposed to support multimedia search, such as tree-structures <ref type="bibr" target="#b3">[3]</ref>, dimensionality reduction <ref type="bibr" target="#b28">[28]</ref>, hashing <ref type="bibr" target="#b6">[6]</ref>, and others <ref type="bibr" target="#b5">[5]</ref>. However, these indexing methods mainly aim to improve the efficiency while the accuracy is not the concern. How to better preserve the near-duplicate relationships among videos in the indexing structure needs to be further studied.</p><p>In this work, we propose a new hashing algorithm, namely Multiple Feature Hashing (MFH), for accurate and efficient NDVR based on multiple visual features. To this end, multiple features (i.e., global and local visual features) of videos are analyzed to obtain the hash codes, which are more accurate to represent video content. Here we regard a video as a sequence of keyframes. Inspired by the recent study which shows that it is beneficial for multimedia semantics understanding to exploit the non-linear manifold structure of multimedia data <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b34">34]</ref>, we propose to make use of the manifold information of keyframes when learning the hash codes for keyframes. Different from <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b35">35]</ref> which can only make use of single feature to learn the hash code of keyframes, the individual structural information of each feature type is preserved in MFH proposed in this paper. Therefore MFH is more capable to make best use of the available information contained in different feature types for a more accurate NDVR. To cope with large scale data, MFH simultaneously learns the hash codes of the training data as well as a series of hash functions to infer the hash code of the videos which are outside the training set, making it apparently different from existing related work such as self-taught hashing <ref type="bibr" target="#b35">[35]</ref> and spectral hashing <ref type="bibr" target="#b27">[27]</ref>. NDVs are expected to have the same or similar hash codes. Once the hash codes are obtained, only Hamming distance calculation is performed to compute the similarity between videos. Such an operation is very fast, making it possible to perform NDVR over large scale video datasets in real time. It is worth highlighting the follow aspects of this paper.</p><p>1. We propose a new framework to exploit multiple local and global features of video data for NDVR. While most of the traditional NDVR algorithms use a single local or global feature, we use multiple features to improve the NDVR performance. We show that the combination of multiple local and global features is more capable to characterize the video content and thus yields better performance.</p><p>2. We propose a new hashing algorithm MFH which learns hash codes of the training videos and a group of hash functions to generate hash codes for the videos outside the training set.</p><p>As far as we know, it is the first hashing algorithm on multiple features which is able to preserve the local structural information of each individual feature and also globally consider all the local structures in the optimization.</p><p>3. We have constructed a large scale video dataset consisting of 132,647 videos which have 2,570,554 keyframes. This dataset will be released to public so that other researchers will be able to use it as a test bed. Experiment results validate the proposed algorithm in both efficiency and accuracy.</p><p>The remainder of this paper is organized as follows. In section 2, we briefly discuss the related work. The details of MFH is presented in section 3. Extensive experiment results are given in section 4. Lastly, we draw a conclusion in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Our work is closely related to NDVR, multiple feature fusion, and hashing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">NDVR</head><p>NDVR and its related applications such as video copy and similarity detection have been actively studied in all kinds of real world applications <ref type="bibr" target="#b30">[30]</ref>. During recent years, various approaches using different features and matching algorithms have been proposed for NDVR <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b36">36]</ref>. The existing literatures on NDVR can be roughly divided into two groups, i.e., global feature based approaches and local feature based approaches respectively.</p><p>Many of the existing global feature based NDVR approaches emphasize the rapid identification of NDVs <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b29">29]</ref>. In these works, videos are represented by compact global features. These approaches perform quite well in handling almost identical videos. For example, in <ref type="bibr" target="#b29">[29]</ref>, the authors adopt HSV to represent their keyframes and further generate a video signature by cumulating all the keyframes in the video. This representation achieves fast retrieval speed as well as high accuracy in their dataset. However, a limitation is that global feature based approaches usually become less effective in handling video copies with layers of editing cosmetics <ref type="bibr" target="#b30">[30]</ref>. Meanwhile, global feature based approaches rely heavily on the selected feature types since different feature only performs well in certain specific aspects.</p><p>As an alternative to global feature based approaches, local feature based approaches have also attracted much research attention. Features such as color, texture and shape extracted at the keyframe level are further segmented into multiple region units, and are particularly suitable for retrieving NDVs with complex variations. The well-known local feature based approaches include keypoint based local feature detection approach (e.g., SIFT <ref type="bibr" target="#b14">[14]</ref>) and some others <ref type="bibr" target="#b2">[2]</ref>. However, the local based approaches are computationally expensive and not applicable to large scale NDVR. In <ref type="bibr" target="#b30">[30]</ref>, Wu et al. propose to construct a hierarchy structure to take advantage of both local feature and global feature. They first filter our part of the videos according to color histogram, and then employ pairwise comparison among keyframes by interest points matching. This hierarchy method improves the performance of NDVR. However, pairwise comparison among keyframes by local feature matching is impractical for large scale video datasets due to the heavy computation cost. Besides, using the global feature HSV only to filter out a large portion of videos might be inaccurate, because some NDVs may have quite different HSV feature, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>(a) and Fig. <ref type="figure" target="#fig_0">1(b)</ref>. Some recent proposals (e.g., <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b16">16]</ref>) have also considered designing new features(spatial-temporal feature) or new similarity measuring methods to be adapted to videos characters. Nonetheless, they are single feature methods. Some (e.g., <ref type="bibr" target="#b17">[17]</ref>)has also considered scalability issue. They use keyframes to query within large-scale video archive. However, their main contribution is to investigate different keyframe samplings of the reference video database in order to evaluate the possible trade-off between accuracy and scalability. No new indexing methods are proposed and it is also a single feature method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multiple feature fusion</head><p>Given that multimedia data can be represented by multiple features, it becomes an important research topic to properly combine the evidences derived from different features. The key problem lying here is how to identify the similarity or correlation between two observations represented by multiple features. Late fusion strategies and early fusion strategies are the traditional methods for multiple-source fusion.</p><p>The late fusion algorithms <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25]</ref> first generate separate results from different features, and then combine these results together by different strategies. The methods do not consider the correlation among features. Besides, as indicated in <ref type="bibr" target="#b20">[20]</ref>, late fusion is computationally more expensive for training. The early fusion strategies try to combine multiple features at the input stage <ref type="bibr" target="#b20">[20]</ref>. For example, the algorithms proposed in <ref type="bibr" target="#b5">[5]</ref> project different features into a unified space in which their applications can be performed. However, the individual structural information of each individual feature can not be well preserved and the computation cost is heavy. Some others (e.g., <ref type="bibr" target="#b32">[32]</ref>) utilize tensor to incorporate multiple features, but it focuses on transductive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Hashing</head><p>Hashing is an important approach to achieve fast similarity search. Many hashing methods have been proposed, such as locality sensitive hashing (LSH) <ref type="bibr" target="#b6">[6]</ref>, spectral hashing <ref type="bibr" target="#b27">[27]</ref>, self-taught hashing <ref type="bibr" target="#b35">[35]</ref>, and so on. Hashing methods implement fast nearest neighbor search in sub-linear time by mapping highly similar data points together. LSH <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b10">10]</ref> uses a family of locality sensitive hash functions composed of linear projection over random directions in the feature space. The intuition behind is that for at lease one of the hash functions, nearby data points have high probability of being hashed into the same state. Recent compact binary code approaches <ref type="bibr" target="#b23">[23]</ref> (e.g., spectral hashing, self-taught hashing) are possible to perform real-time search due to the quick similarity computation by using bit XOR operation in the Hamming space. The primary challenge on binary code methods is how to generate the compact binary codes for the data points. A good code requires that similar data in the original space should be mapped into similar binary codes. Spectral hashing utilizes spectral graph partitioning in the learning phase to get the hash codes of training data, which is similar to self-taught hashing. But to calculate the binary codes for a new data point, spectral hashing assumes that the data are uniformly distributed in a hype-rectangle, which is very restrictive, and self-taught hashing has to learn new classification models based on the result of learning phase. None of them takes consideration of multiple features. In this paper, we propose the first Multiple Feature Hashing (MFH) algorithm to learn a group of hash functions which can be utilized to map the keyframes into the Hamming space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MULTIPLE FEATURE HASHING</head><p>The proposed framework based on multiple feature hashing (MFH) for NDVR is shown in Fig. <ref type="figure" target="#fig_2">2</ref>, which comprises two phases. In the first phase which is offline, we use the proposed MFH algorithm to learn a series of s hash functions {h1(•), ..., hs(•)}, each of which generates one bit hash code for a keyframe according to the given multiple features. Each keyframe has s bits. Using the derived hash functions {h1(•), ..., hs(•)}, each keyframe for a dataset video can be represented by the generated s-sized hash codes in linear time. In the second phase which is online, the query video's keyframes are also represented by s-sized hash codes mapped from the s hash functions. NDVR can be efficiently achieved where only efficient XOR operation on the hash codes is performed to compute the similarity between two videos.</p><p>The key research issue is how to train the hash functions which affect both accuracy and efficiency. In this section, we detail the proposed MFH algorithm. It is worthy noting that MFH is a general one which can be potentially applied to other large scale search applications where the data is represented by multiple features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Terms and Notations</head><p>Let v be the number of features 1 . Given an integer g ≤ v, (x g ) t ∈ R dg * 1 denotes the g-th feature of t-th training data, where dg is the dimensionality of the g-th feature. Suppose there are n training keyframes in total. X g = [(x g ) 1 , (x g ) 2 , ..., (x g ) n ] ∈ R dg * n is the feature matrix corresponding to the g-th feature of all the training data. 1 In our system, each keyframe is represented by the local feature LBP and the global feature HSV color histogram. Therefore, v = 2 here. Yet, MFH is able to deal with more feature types when v &gt; 2.  </p><formula xml:id="formula_0">xt = [ x 1 T t , x 2 T t , ..., (x v ) T t ] T ∈ R d×1 ,</formula><formula xml:id="formula_1">       ⋮ l h x h x h x</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The objective function of MFH</head><p>As demonstrated in <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b35">35]</ref>, it is beneficial to exploit the local structure of the training data to infer an accurate and compact representation, especially for multimedia data. While in <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b35">35]</ref>, the local structural information has been utilized to learn the hash codes, there are still two major limitations. First, both the algorithms proposed in <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b35">35]</ref> are two-step training approaches. During the training phase, the hash codes of the training data are first generated according to which the hash functions are then defined. This may incur overfitting when generating the hash codes of training data because only local structure information is considered. Second, the algorithms proposed in <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b35">35]</ref> can not deal with multiple features. Although a straightforward way is to concatenate the multiple features to produce a high dimensional vector, the structural information of each feature type will be lost.</p><p>To exploit the individual structural information of each individual feature, we define v affinity matrices A g ∈ R n×n (1 ≤ g ≤ v), one for each feature as follows:</p><formula xml:id="formula_2">(A g ) pq = 1, if (x g )p ∈ N k ((x g )q) or (x g )q ∈ N k ((x g )p) 0, else<label>(1)</label></formula><p>where N k (•) is the k-nearest-neighbor set and 1 ≤ (p, q) ≤ n.</p><p>To learn the hash codes (y g ) t from the g-th feature, a reasonable criterion is that similar items in the original space should have similar binary codes, which can be formulated as the following minimization problem:</p><formula xml:id="formula_3">min Y g n p,q=1 (A g ) pq (y g ) p -(y g ) q 2 F .<label>(2)</label></formula><p>Given a training keyframe xt, we hope the overall hash codes yt should be consistent with the hash codes y g t | v g=1 derived from each feature. In this way, the local structure information on each single feature is also globally considered and optimized. Therefore, we have the following minimization problem:</p><formula xml:id="formula_4">min Y g ,Y v g=1 n p,q=1 (A g ) pq (y g ) p -(y g ) q 2 F +γ v g=1 n t=1 yt -(y g ) t 2 F (3)</formula><p>where γ is the parameter to balance the two parts. It is easy to see from Eq.3 that the individual manifold structure of each feature type is preserved (the first term) and all the manifold structures are also globally considered in the optimization (the second term).</p><p>Recall that we intend to learn the hash codes of the training keyframes and a series of hash functions {h1(•), ..., hs(•)} in a joint framework. We propose to simultaneously learn the hash codes of the training keyframes and the hash functions by minimizing the empirical error of the hash functions w.r.t. the learnt hash codes Y . The proposed final objective function of MFH is given by: min</p><formula xml:id="formula_5">Y,Y g ,W,b v g=1 n p,q=1 (A g ) pq (y g ) p -(y g ) q 2 F +γ v g=1 n t=1 yt -(y g ) t 2 F +α s l=1 n t=1 h l (xt) -y tl 2 F + βΩ(h l ) s.t. yt ∈ {-1, 1} s , (y g ) t ∈ {-1, 1} s Y Y T = I<label>(4)</label></formula><p>where Ω(h l ) is a regularization function over h l , α and β are the parameters, 1 ∈ R n×1 is a column vector with all ones, and y tl is l-th bit hash code for xt. In Eq.4, yt ∈ {-1, 1} s and (y g ) t ∈ {-1, 1} s enforce the hash codes of yt and (y g ) t to be binary codes. As we will show later on, the constraint Y Y T = I is imposed to avoid the trivial solution. For the simplicity of implementation for large scale datasets, we adopt the linear transformation as the hash function. More specifically, given a keyframe represented by its </p><formula xml:id="formula_6">(A g ) pq (y g ) p -(y g ) q 2 F +γ v g=1 n t=1 yt -(y g ) t 2 F +α X T W + 1b -Y 2 F + β W 2 F s.t. yt ∈ {-1, 1} s , (y g ) t ∈ {-1, 1} s Y Y T = I<label>(6)</label></formula><p>In Eq.6, the term</p><formula xml:id="formula_7">X T W + 1b -Y<label>2</label></formula><p>F learns the hash functions to generate hash codes for the data. Meanwhile, it can be also interpreted as a regularizer in which global information is enclosed. Compared with the hashing algorithms proposed in <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b35">35]</ref> where only local information is considered, it may eliminate the problem of overfitting, making the learning more robust. The objective function shown in Eq.6 is equivalent to the balanced graph partitioning and it is an NP-hard problem. Following <ref type="bibr" target="#b27">[27]</ref>, we relax the constraints yt ∈ {-1, 1} s , y g t ∈ {-1, 1} s to make the problem computationally trackable. The relaxed objective function is given by min</p><formula xml:id="formula_8">Y,Y g ,W,b v g=1 n p,q=1 (A g ) pq (y g ) p -(y g ) q 2 F +γ v g=1 n t=1 yt -(y g ) t 2 F +α X T W + 1b -Y 2 F + β W 2 F s.t.Y Y T = I (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Solution of MFH</head><p>In this subsection, we detail the approach to optimize the objective function of MFH. By setting the derivative of Eq.7 w.r.t. b to zero, we get:</p><formula xml:id="formula_9">1 T X T W + 1b -Y = 0 ⇒ b = 1 n 1 T Y -1 T X T W (8)</formula><p>By setting the derivative of Eq.7 w.r.t. W to zero, we get:</p><formula xml:id="formula_10">X X T W + 1b -Y + βW = 0. (<label>9</label></formula><formula xml:id="formula_11">)</formula><p>Substituting b in Eq.9 by Eq.8, we have</p><formula xml:id="formula_12">XX T W + X1 1 n 1 T Y -1 T X T W -XY + βW = 0 ⇒ W = XLcX T + βI -1</formula><p>XLcY,</p><p>where Lc = I -1 n 11 T is the centering matrix. Note that Lc = (Lc) T = Lc (Lc) T . Replacing W and b by Eq.8 and Eq.10 respectively, we have</p><formula xml:id="formula_14">X T W + 1b -Y = X T W + 1 1 n 1 T Y -1 T X T W -Y = LcX T W -LcY = LcX T XLcX T + βI -1 XLcY -LcY<label>(11)</label></formula><formula xml:id="formula_15">Let M = XLcX T + βI -1 . X T W + 1b -Y 2 F +β W 2</formula><p>F can be rewritten as:</p><formula xml:id="formula_16">X T W + 1b -Y 2 F + β W 2 F = trY T LcX T M XLc -Lc T LcX T M XLc -Lc Y + βtr W T W = trY T LcX T M XLcLcX T M XLc -2LcX T M XLc + Lc Y + βtr Y T LcX T M M XLcY = tr Y T LcX T M M -1 M XLc -2LcX T M XLc + Lc Y = trY T Lc -LcX T M XLc Y = trY T BY</formula><p>where B is defined as:</p><formula xml:id="formula_17">B = Lc -LcX T XLcX T + βI -1 XLc<label>(12)</label></formula><p>Meanwhile, we have n p,q=1</p><formula xml:id="formula_18">(A g ) pq (y g ) p -(y g ) q 2 F = tr (Y g ) T L g Y g<label>(13)</label></formula><p>where L g = N g -A g is the Laplacian matrix of the g-th feature and N g is the diagonal matrix with its diagonal element</p><formula xml:id="formula_19">N g ii = j A g ij . Note that v g=1 n t=1 yt -(y g ) t<label>2</label></formula><p>F can also be written as</p><formula xml:id="formula_20">v g=1 Y -Y g 2 F .</formula><p>Then the objective function shown in Eq.7</p><p>becomes</p><formula xml:id="formula_21">min Y,Y g v g=1 tr (Y g ) T L g Y g + γ Y -Y g 2 F +αtr Y T BY s.t. Y T Y = I. (<label>14</label></formula><formula xml:id="formula_22">)</formula><p>Setting the derivative of Eq.14 w.r.t. Y g to be zero, we have</p><formula xml:id="formula_23">2L g Y g -2γ (Y -Y g ) = 0 ⇒ Y g = γ (L g + γI) -1 Y (15) Let C g = γ (L g + γI) -1 . Note that C g = (C g ) T .</formula><p>Then we arrive at</p><formula xml:id="formula_24">Y g = C g Y (<label>16</label></formula><formula xml:id="formula_25">)</formula><p>Replace Y g with Eq.16, the objective function in Eq.14 becomes:</p><formula xml:id="formula_26">min Y Y T =I tr(Y T DY ). (<label>17</label></formula><formula xml:id="formula_27">)</formula><p>where D is defined as</p><formula xml:id="formula_28">D = v g=1 C g L g C g + γ (I -C g ) 2 + αB = γ v g=1 (I -C g ) + αB<label>(18)</label></formula><p>The optimization problem shown in Eq.17 can be solved by performing the eigen-value decomposition of D. Y can be obtained by the s eigenvectors of D corresponding to the s smallest eigenvalues <ref type="foot" target="#foot_0">2</ref> . In summary, the training processing of the proposed MFH algorithm is listed as follows.</p><p>Algorithm 1 The MFH algorithm. for g = 1 to v do for p = 1 to n do for q = 1 to n do Compute L g p,q according to 13; end for end for end for Compute B according to Eq.( <ref type="formula" target="#formula_17">12</ref>); Compute D according to Eq.( <ref type="formula" target="#formula_28">18</ref>); Compute Y by performing eigenvalue decomposition on D. Output W and b according to Eq.10 and Eq.8.</p><p>Once W and b are obtained, they can be readily used to generate the hash codes of database keyframes as well as query keyframes. For example, given a keyframe xt, we first compute its relaxed hash code yt in continuous domain by yt = W T xt + b. Then, we binarize it by comparing each dimension of yt with its median of all dimensions. If it is greater than the median, we set it as 1.</p><p>Otherwise, we set it as -1. To represent a video clip, Wu et al. proposed to averaging the visual feature of all its keyframes as the representation the whole video clip <ref type="bibr" target="#b29">[29]</ref>. Following Wu's way, we can also generate the hash codes for a video clip by averaging the relaxed codes of all its keyframes and then binarize them. As a result, each video clip is represented by a single s-bit hash code.</p><p>In this work, we use the above video representation to avoid pairwise keyframe comparisons. The number of common bits along all dimensions shared by two videos is approximated as the video similarity.</p><p>For better understanding of MFH, we give an illustration in Fig. <ref type="figure" target="#fig_3">3</ref>. Given a set of videos, to explore the individual manifold structure, we first construct kNN graph on each individual feature space. Then we learn the hash codes of each video under the criterion that similar items in the original space should have similar hash codes by solving the objective function Eq. 14 which preserves the local structure information of each individual feature and also globally considers the local structures among all the features. Meanwhile, a series of hash functions are simultaneously learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>We experimentally evaluate the performance of MFH and compare it with existing state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Video datasets</head><p>We used two datasets in the experiments. CC_WEB_VIDEO Dataset <ref type="bibr" target="#b1">[1]</ref> is provided by CityU of Hong Kong and CMU. It consists of 24 sets of video clips (totally 13,129 video clips) downloaded from video sharing websites including Google Video, Yahoo! Video, and YouTube using specific keywords. For each set of videos, the most popular video is selected as the query video, and all the other videos within the set are manually labeled to get the ground truth.</p><p>Combined Dataset is a larger video dataset created by ourselves by adding CC_WEB_VIDEO to our video dataset downloaded from YouTube. We choose the most popular 400 queries to query the videos from YouTube. Those queries are selected from Google Zeitgeist. Each year, Google examines billions of queries that people around the world have typed into Google search to discover the Zeitgeist saved in Google Zeitgeist Archives. We collect Google Zeitgeist Archives from 2004 to 2009, and choose the most popular 400 queries to search YouTube. The downloaded number of videos for each query is up to 1000. We crawled more than 150K YouTube videos from July 2010 to September 2010. After filtering out the videos who sizes are greater than 10M, the Combined Dataset contains 132,647 videos in total, which is more than twice as big as the one used in <ref type="bibr" target="#b18">[18]</ref>. To our best knowledge, this is the biggest Web video dataset for experimental purpose. We further extract 2,570,554 keyframes from these videos. This dataset will be released to public so that other researchers will be able to use it as a test bed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Visual features</head><p>On the CC_WEB_VIDEO dataset, shot-based sampling method is used to extract key-frames and there are 398,078 keyframes extracted in total. These keyframes are provided in the dataset. We further extract LBP feature on the keyframes as a local feature, which will be used together with the existing global feature HSV provided by dataset.</p><p>On the Combined Dataset, we firstly detect the shots from which the keyframes are extracted. Then we extracted two features for each keyframe, which are HSV and LBP, to be used as a global feature and a local feature respectively. HSV is a global color histogram with 162 dimensions and LBP is a local content feature with 256 dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Algorithms for comparison</head><p>To evaluate NDVR accuracy and efficiency, we present an extensive comparison of the proposed method with a set of existing NDVR methods, which are briefly described as follows.</p><p>Global feature based method: We use the global color histogram method as a baseline method. Each keyframe is represented as a color histogram feature vector. Each video is finally defined as a feature vector of a normalized color histogram over all the keyframes in the video.</p><p>Local feature based method: Wu et al. <ref type="bibr" target="#b29">[29]</ref> proposed a hierarchical method to first use global color histograms to detect nearduplicate videos with high confidence and filter out very dissimilar videos. Then local feature is applied to identify the remaining uncertain videos with high accuracy.</p><p>Spatiotemporal feature based method: Shang et al. <ref type="bibr" target="#b18">[18]</ref> introduced a compact spatiotemporal feature to represent the videos. This feature leverages relative gray-level intensity distribution within a frame and temporal structure of videos along frame sequence. Two feature selection methods, namely CE-based spatiotemporal feature and LBP-based spatiotemporal feature are proposed to generate the compact video signature. A new index structure is also proposed to index the signatures based on inverted file in order to achieve real-time retrieving performance.</p><p>Self-taught hashing: Zhang et al. <ref type="bibr" target="#b35">[35]</ref> designed compact binary codes for a large number of documents so that semantically similar documents are mapped to similar codes. Then the search can be quickly done in the Hamming space. Also, to obtain the codes for previously unseen documents,it trains classifiers via supervised learning based on the binary codes learned from the training data. Here we extend it for video representation.</p><p>Spectral hashing: Weiss et al. <ref type="bibr" target="#b27">[27]</ref> utilizes the results of graph Laplacian eigenvectors to the Laplace-Beltrami eigenfunctions of manifolds to generate compact binary codes of data points by assuming that the data are uniformly distributed in a hype-rectangle.</p><p>We also implemented single feature MFH, which is a special case of our MFH, to prove the significance of using multiple features. For simplicity, we let MFH_hsv and MFH_lbp represent the single feature case of our MFH. Similarly, let ST_lbp and ST_ce represent the two spatiotemporal feature based methods used in <ref type="bibr" target="#b18">[18]</ref>, and GF, LF, STH and SPH represent global feature based method, local feature based method, self-taught hashing and spectral hashing respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">System environment and evaluation metrics</head><p>All of the experiments are implemented on a computer which has Intel(R) Core(TM) i7 2.93GHz 8 processors, 16 GB RAM and 64bit Windows 7 Enterprise N operating system. Following the work in <ref type="bibr" target="#b18">[18]</ref>, we also adopt the metric MAP (Mean Average Precision) to evaluate the performance. Among evaluation measures, MAP has been shown to have especially good discrimination and stability. We further use precision-recall curve to evaluate the performance on both datasets. Time efficiency is calculated on Matlab R2010a for all the comparison methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on CC_WEB_VIDEO</head><p>We first test different methods on the CC_WEB_VIDEO dataset. The same 24 queries as in <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b18">18]</ref> are used. The MAP results and precision-recall curves of different methods can be seen in Table <ref type="table" target="#tab_0">1</ref> and Fig. <ref type="figure" target="#fig_4">4</ref> respectively.</p><p>From Table1, we can see that the proposed MFH achieves best results on MAP, although the margins are minor. Fig. <ref type="figure" target="#fig_4">4</ref> shows that GF performs poorly compared with other four methods which have quite similar performance. This is probably because the dataset  Comparison of precision and recall on CC_WEB_VIDEO.</p><p>is not big enough and thus less noises can be introduced for most methods. The performance of different methods cannot be fully explored. Next, we focus on the large dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on Combined Dataset</head><p>To further test the accuracy and scalability of MFH, we test MFH on the Combined Dataset and compare it with existing methods. More specifically, we use the same 24 queries and groundtruth as in <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b18">18]</ref> to search the whole Combined Dataset. There are several parameters need to be tuned for some of the algorithms. For each parameter we have a set of candidate values, and then we iteratively combine the parameters to form various parameter settings. For each parameter combination we run the same experiment to evaluate its impact on the performance. Finally, the parameter setting which results in the best performance is selected for each method. For the parameters in MFH, SPH and STH, including γ,α,β, we tune them from 10 -6 , 10 -3 , 10 0 , 10 3 , 10 6 and tune the length of the binary code from 200 to 400. On this large Combined Dataset, we cannot compare with local feature based method LF proposed in <ref type="bibr" target="#b29">[29]</ref> because this method utilizes interest points matching to measure the similarity of two keyframes. It is impractical in such a large dataset with 2,570,554 keyframes. We are also unable to extract the spatiotemporal features ST_lbp and ST_ce, because it takes extremely long time for our computer to extract the features from 132,647 videos. As a result, only methods including GF, STH, SPH, MFH_hsv, MFH_lbp, and MFH are compared. The results on MAP, search time are shown in Table <ref type="table" target="#tab_2">2</ref>. To have a clear visualization, averaged precision-recall curves are depicted in Fig. <ref type="figure">5</ref>. We also report the Average Precision (AP) of each query in Fig. <ref type="figure">6</ref>. From Table <ref type="table" target="#tab_2">2</ref> and Fig. <ref type="figure">5</ref>, we have the following observations.</p><p>• MFH achieves the highest MAP and outperforms existing methods by large margins. Especially, MFH can have more than 86% MAP, while SPH has less than 60% MAP.   satisfactory probably because the video dataset is not in accordance with the assumption that all the data points are uniformly distributed in a hyper-rectangle in the high-dimensional space. This also shows that MFH is more capable of preserving local data distribution information.</p><p>• GF performs search using the Euclidean distance and its performance is not good either. Its accuracy is much worse than that of MFH. Furthermore, its search time is about 2-3 times longer than hashing methods.</p><p>• MFH outperforms MFH_lbp and MFH_hsv in general, which demonstrates the effectiveness of combining multiple features in NDVR.</p><p>From Fig. <ref type="figure">6</ref>, we also observe that MFH_lbp is worse than MFH_hsv for the overall MAP. However, for some queries, such as 'Bus uncle' and 'U2 and green day', MFH_lbp achieves much better accuracy than MFH_hsv. This shows that different types of features can character some part of the visual content, and they can only perform well in some certain situations.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Parameter sensitivity study for MFH</head><p>In this subsection we test different parameter settings for our MFH to see the performance variation. There are four parameters in our proposed models: 1) γ, α and β shown in Eqn.7, and 2) s, the length of the binary code. We firstly find the best performance of s by fixing γ, α and β. Then we fix s and one of the parameters in γ, α and β, to report the MAP while the other two parameters are changing. In our paper, we tested s = 200, 220, ..., 400 and γ, α, β = 10 -6 , 10 -3 , 10 0 , 10 3 , 10 6 .</p><p>Among different combinations, MFH gains the best performance when s = 320, γ = 10 -3 , α = 1 and β = 10 -6 . By fixing s = 320, we show the MAP variations on γ, α and β in Fig. <ref type="figure" target="#fig_10">8</ref>. Generally speaking, in order to obtain better performance, β should not be larger than 10 3 , as shown in Fig. <ref type="figure" target="#fig_10">8</ref> By fixing γ = 10 -3 , α = 1 and β = 10 -6 , we show the performance variations on different code lengths in Fig. <ref type="figure" target="#fig_7">7</ref>. When the binary code length is less than 280, the MAP improves dramatically with the increase of binary code length. After that, the MAP keeps steady until reaching peak when s = 320. Then, the MAP fluctuates with slight drop until K = 400. This indicates that the longest binary code length can not guarantee the best MAP. When s &gt; 320, the MAP declines slightly probably because a longer code may contain more noises. An approximately linear increasing trend for query time is illustrated in Fig. <ref type="figure" target="#fig_7">7(b</ref>). This is reasonable since it has a linear relationship with the code length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION AND FUTURE WORK</head><p>In this paper, we have proposed a new multiple feature hashing (MFH) method for large scale NDVR. Compared with the existing algorithms, MFH has two major advantages. First, instead of using single feature, MFH employs a machine learning approach to exploit the local structure of each individual feature and fuse multiple features in a joint framework. Such a method is more stable to well characterize the video content. Second, the MFH method is proposed here to also accelerate the query speed. The extensive experiments verify the superior performance of our method on efficiency and accuracy over existing methods. In the future we intend to incorporate the temporal correlation of the keyframes within the same video to further improve the performance.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Two groups of NDVs. Please refer to color images for better illustration.</figDesc><graphic coords="2,56.27,455.39,106.94,80.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The Proposed Framework for NDVR</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Multi-Feature Hashing</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4:Comparison of precision and recall on CC_WEB_VIDEO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 5 :</head><label>65</label><figDesc>Figure 6: Comparison of AP corresponding to each query on the Combined Dataset. 'All' indicates the MAP of all the 24 queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>The time cost of different code length</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The performance variations of different code lengths</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(a) and Fig.8(b). We observe from Fig.8(a), Fig.8(b) and Fig.8(c) that the MAP is not very sensitive to the parameter γ and α when β is smaller than 10 3 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>γ is fixed as 10 -3 . β is fixed as 10 -6 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The MAP variations of different parameter settings</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Comparison of MAP on CC_WEB_VIDEO</head><label>1</label><figDesc></figDesc><table><row><cell cols="2">Methods</cell><cell>GF</cell><cell>LF</cell><cell cols="3">ST_lbp ST_ce MFH</cell></row><row><cell></cell><cell>MAP</cell><cell cols="2">0.892 0.952</cell><cell>0.953</cell><cell>0.950</cell><cell>0.954</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Precision</cell><cell>0.7 0.6</cell><cell>GF LF</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>ST_lbp</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.5</cell><cell>ST_ce</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>MFH</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0 0.4</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Recall</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 : Comparison of MAP and time on Combined Dataset</head><label>2</label><figDesc></figDesc><table><row><cell>Methods</cell><cell>MAP</cell><cell>Time(s)</cell></row><row><cell>SPH</cell><cell>0.5941</cell><cell>0.4907</cell></row><row><cell>GF</cell><cell>0.6466</cell><cell>1.3917</cell></row><row><cell>STH</cell><cell>0.7536</cell><cell>0.6439</cell></row><row><cell cols="2">MFH_lbp 0.7526</cell><cell>0.6445</cell></row><row><cell cols="2">MFH_hsv 0.8042</cell><cell>0.4508</cell></row><row><cell>MFH</cell><cell>0.8656</cell><cell>0.5533</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>The trivial solution corresponding to the eigenvalue of zero is removed</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGEMENTS</head><p>We are grateful to Dr Wu Xiao for providing us the CC_WEB_VIDEO dataset and to Dr Lifeng Shang for sharing the experiment results of his algorithm <ref type="bibr" target="#b18">[18]</ref> with us. This work was supported by Australia Research Council under research Grant ARC DP1094678 and partially supported by National Science Foundation under Grants No. IIS-0917072 and CNS-0751185. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Near-duplicate web video dataset</title>
		<author>
			<persName><surname>Cc_Web_Video</surname></persName>
		</author>
		<ptr target="http://vireo.cs.cityu.edu.hk/webvideo" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Surf: Speeded up robust features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="404" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Searching in high-dimensional spaces: Index structures for improving the performance of multimedia databases</title>
		<author>
			<persName><forename type="first">C</forename><surname>Böhm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Berchtold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="322" to="373" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast similarity search and clustering of video sequences on the world-wide-web</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zakhor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="524" to="537" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multiple feature fusion for social media applications</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K H</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="435" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Locality-sensitive hashing scheme based on p-stable distributions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Immorlica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Mirrokni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Computational Geometry</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Online near-duplicate video clip detection and retrieval: An accurate and fast system</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1511" to="1514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Practical online near-duplicate subsequence detection for continuous video streams</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="386" to="398" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bounded coordinate system indexing for real-time video clip search</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A posteriori multi-probe locality sensitive hashing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="209" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tiny videos: A large data set for nonparametric video retrieval and frame classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Aarabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="618" to="630" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantic manifold learning for image retrieval</title>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="249" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Correlation-based retrieval for heavily changed near-duplicate videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A performance evaluation of local descriptors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1615" to="1630" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Flexible manifold embedding: a framework for semi-supervised and unsupervised dimension reduction</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Img. Proc</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1921" to="1932" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scaling content-based video copy detection to very large databases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poullot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Crucianu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools Appl</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="279" to="306" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Detecting screen shot images within large-scale video archive</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poullot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3203" to="3207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Real-time large scale near-duplicate web video retrieval</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-P</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Uqlips: a real-time near-duplicate video clip detection system</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1374" to="1377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Early versus late fusion in semantic video analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="399" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Accelerating near-duplicate video matching by combining visual similarity and alignment distortion</title>
		<author>
			<persName><forename type="first">H.-K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="861" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Web image retrieval on imageval: evidences on visualness and textualness concept dependency in fusion model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tollari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIVR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Small codes and large image databases for recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Combining audio content and social context for semantic music discovery</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Turnbull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Barrington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yazdani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="387" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unified video annotation via multigraph learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Techn</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="733" to="746" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Beyond distance measurement: constructing neighborhood similarity for video annotation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="465" to="476" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<title level="m">Spectral hashing. In NIPS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1753" to="1760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient EMD-based similarity search in multimedia databases via flexible dimensionality reduction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wichterich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Assent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kranen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Seidl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="199" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Practical elimination of near-duplicates from web video search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="218" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Real-time near-duplicate elimination for web video search with content and context</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-K</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="196" to="207" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Near-duplicate keyframe retrieval with visual keywords and semantic context</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIVR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="162" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Near-duplicate video matching with transformation recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="549" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ranking with local regression and global alignment for cross media retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="175" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Harmonizing hierarchical manifolds for multimedia document semantics understanding and cross-media retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="437" to="446" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Self-taught hashing for fast similarity search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An efficient near-duplicate video shot detection method using shot-based interest points</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bouguettaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="879" to="891" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
