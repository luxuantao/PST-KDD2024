<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-supervised Adapted HMMs for Unusual Event Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dong</forename><surname>Zhang</surname></persName>
							<email>zhang@idiap.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">IDIAP Research Institute</orgName>
								<address>
									<settlement>Martigny</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Swiss Federal Institute of Technology</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Gatica-Perez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IDIAP Research Institute</orgName>
								<address>
									<settlement>Martigny</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
							<email>bengio@idiap.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">IDIAP Research Institute</orgName>
								<address>
									<settlement>Martigny</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Swiss Federal Institute of Technology</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Iain</forename><surname>Mccowan</surname></persName>
							<email>mccowan@idiap.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">IDIAP Research Institute</orgName>
								<address>
									<settlement>Martigny</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-supervised Adapted HMMs for Unusual Event Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">776B8B213F6BEC0F7450120F4DB9FCED</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of temporal unusual event detection.  Unusual events are characterized by a number of features (rarity, unexpectedness, and relevance) that limit the application of traditional supervised model-based approaches. We propose a semi-supervised adapted Hidden Markov Model (HMM) framework, in which usual event models are first learned from a large amount of (commonly available) training data, while unusual event models are learned by Bayesian adaptation in an unsupervised manner. The proposed framework has an iterative structure, which adapts a new unusual event model at each iteration. We show that such a framework can address problems due to the scarcity of training data and the difficulty in pre-defining unusual events. Experiments on audio, visual, and audiovisual data streams illustrate its effectiveness, compared with both supervised and unsupervised baseline methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In some event detection applications, events of interest occur over a relatively small proportion of the total time: e.g. alarm generation in surveillance systems, and extractive summarization of raw video events. The automatic detection of temporal events that are relevant, but whose occurrence rate is either expected to be very low or cannot be anticipated at all, constitutes a problem which has recently attracted attention in computer vision and multimodal processing under an umbrella of names (abnormal, unusual, or rare events) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b5">6]</ref>. In this paper we employ the term unusual event, which we define as events with the following properties: (1) they seldom occur (rarity); (2) they may not have been thought of in advance (unexpectedness); and (3) they are relevant for a particular task (relevance).</p><p>It is clear from such a definition that unusual event detection entails a number of challenges. The rarity of an unusual event means that collecting sufficient training data for supervised learning will often be infeasible, necessitating methods for learning from small numbers of examples. In addition, more than one type of unusual event may occur in a given data sequence, where the event types can be expected to differ markedly from one another. This implies that training a single model to capture all unusual events will generally be infeasible, further exacerbating the problem of learning from limited data. As well as such modeling problems due to rarity, the unexpectedness of unusual events means that defining a complete event lexicon will not be possible in general, especially considering the genre-and task-dependent nature of event relevance.</p><p>Most existing works on event detection have been designed to work for specific events, with well-defined models and prior expert knowledge, and are therefore ill-posed for handling unusual events. Alternatives to these approaches, addressing some of the issues related to unusual events, have been proposed recently <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b5">6]</ref>. However, the problem remains unsolved.</p><p>In this paper, we propose a framework for unusual event detection. Our approach is motivated by the observation that, while it is unrealistic to obtain a large training data set for unusual events, it is conversely possible to do so for usual events, allowing the creation of a well-estimated model of usual events. In order to overcome the scarcity of training material for unusual events, we propose the use of Bayesian adaptation techniques <ref type="bibr" target="#b13">[14]</ref>, which adapt a usual event model to produce a number of unusual event models in an unsupervised manner. The proposed framework can thus be considered as a semi-supervised learning technique.</p><p>In our framework, a new unusual event model is derived from the usual event model at each step of an iterative process via Bayesian adaptation. Temporal dependencies are modeled using HMMs, which have recently shown good performance for unsupervised learning <ref type="bibr" target="#b0">[1]</ref>. We objectively evaluate our algorithm on a number of audio, visual, and audio-visual data streams, each generated by a sepa-rate source, and containing different events. With relatively simple audio-visual features, and compared to both supervised and unsupervised baseline systems, our framework produces encouraging results.</p><p>The paper is organized as follows. Section 2 describes related work. The proposed framework is introduced in Section 3. In Section 4, we present experimental results and discuss our findings. We conclude the paper in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There is a large amount of work on event detection. Most works have been centered on the detection of predefined events in particular conditions using supervised statistical learning methods, such as HMMs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18]</ref>, and other graphical models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b8">9]</ref>. In particular, some recent work has attempted to recognize highlights in videos, e.g., sports <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18]</ref>. In our view, this concept is related but not identical to unusual event detection. On one hand, typical highlight events in most sports can be well defined from the sports grammar and, although rare, are predictable (e.g., goals in football, home-runs in baseball, etc). On the other hand, truly unusual events (e.g. a blackout in the stadium) could certainly be part of a highlight.</p><p>Fully supervised model-based approaches are appropriate if unusual events are well-defined and enough training samples are available. However, such conditions often do not hold for unusual events, which render fully supervised approaches ineffective and unrealistic. To deal with the problem, an HMM approach was proposed in <ref type="bibr" target="#b5">[6]</ref> to detect unusual events in aerial videos. Without any models for usual activities, and with only one training sample, unusual events models are handcoded using a set of predefined spatial semantic primitives (e.g. "close" or "adjacent"). Although unusual event models can be created with intuitive primitives for simple cases, it is infeasible for complex events, in which primitives are difficult to define.</p><p>As an alternative, unsupervised approaches for unusual event detection have also been proposed <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref>. In a farfield surveillance setting, the use of co-occurrence statistics derived from motion-based features was proposed in <ref type="bibr" target="#b16">[17]</ref> to create a binary-tree representation of common patterns. Unusual events were then detected by measuring aspects of how usual each observation sequence was. The work in <ref type="bibr" target="#b18">[19]</ref> proposed an unsupervised technique to detect unusual human activity events in a surveillance setting, using analysis of co-occurrence between video clips and motion / color features of moving objects, without the need to build models for usual activities.</p><p>Our work attempts to combine the complementary advantages of supervised and unsupervised learning in a probabilistic setting. On one hand, we learn a general usual event model exploiting the common availability of train-</p><formula xml:id="formula_0">Unusual Event 1 1 2 N Usual Event 1 2 N Unusual Event K 1 2 N Figure 1</formula><p>. HMM topology for the proposed framework ing data for such an event type. On the other hand, we use Bayesian adaptation techniques to create models for unusual events in an iterative, data-driven fashion, thus addressing the problem of lack of training samples for unusual events, without relying on pre-defined unusual event sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Iterative Adapted HMM</head><p>In this section, we first introduce our computational framework. We then describe the implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Framework Overview</head><p>As shown in Figures <ref type="figure">1</ref> and<ref type="figure" target="#fig_1">3</ref>, our framework is a hierarchical structure based on an ergodic K-class Hidden Markov Model (HMM) (K is the number of unusual event states plus one usual event state), where each state is a sub-HMM with minimum duration constraint. The central state represents usual events, while the others represent unusual events. All states can reach (or be reached from) other states in one step, and every state can transmit to itself.</p><p>Our method starts by having only one state representing usual events (Figure <ref type="figure" target="#fig_0">2</ref>, step 0). It is normally easy to collect a large number of training samples for usual events, thus obtaining a well-estimated model for usual events. A set of parameters θ * of the usual-event HMM model is learned by maximizing the likelihood of observation sequences {X 1 , X 2 , ..., X M } as follows:</p><formula xml:id="formula_1">θ * = arg max θ M j=1 P (X j |θ).</formula><p>(</p><formula xml:id="formula_2">)<label>1</label></formula><p>The probability density function of each HMM state is assumed to be a Gaussian Mixture Model (GMM). We use the standard Expectation-Maximization (EM) algorithm <ref type="bibr" target="#b4">[5]</ref> to estimate the GMM parameters. The segment with the lowest likelihood given the general model is identified as outlier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Adaptation</head><p>A new unusual event model is adapted from the general usual event model using the detected outlier.</p><p>The usual event model is adapted from the general usual event model using the other segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Viterbi decoding</head><p>Given a new HMM topology (with one more state), the test sequences are decoded using Viterbi algorithm to determine the boundary of events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Outlier detection</head><p>Identify a new outlier, which has the smallest likelihood given the adapted usual event model. 5. Repeat step 2, 3, 4 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stop</head><p>Stop the process after the given number of iterations. likelihood of the data, given the parameters of the GMMs. This is followed by an M-step, where the parameters of the GMMs are re-estimated based on this segmentation. This creates a general usual event model. Given the well-estimated usual event model and an unseen test sequence, we first slice the test sequence into fixed length segments with overlapping. This is done by moving a sliding window. The choice of the sliding window size corresponds to the minimum duration constraint in the HMM framework. Given the usual event model, the likelihood of each segment is then calculated. The segment with the lowest likelihood value is identified as an outlier (Figure <ref type="figure" target="#fig_0">2</ref>, step 1). The outlier is expected to represent one specific unusual event and could be used to train an unusual event model. However, one single outlier is obviously insufficient to give a good estimate of the model parameters for unusual events. In order to overcome the lack of training material, we propose the use of model adaptation techniques, such as Maximum a posteriori (MAP) <ref type="bibr" target="#b13">[14]</ref>, where we adapt the already well-estimated usual event model to a particular unusual event model using the detected outlier, i.e, we start from the usual event model, and move towards an unusual event model in some constrained way (see Section 3.2 for implementation details). The original usual event model is trained using a large number of samples, which generally means that it yields Gaussians with relatively large variances. In order to make the model better suited for test se- quences, the original usual event model is also adapted with the other segments (except for the detected outlier), using the same adaptation technique for the unusual event model (Figure <ref type="figure" target="#fig_0">2</ref>, step 2).</p><p>Given the new unusual and usual event models, both adapted from the general usual event model, the HMM topology is changed with one more state. Hence the current HMM has 2 states, one representing the usual events and one representing the first detected unusual event. The Viterbi algorithm is then used to find the best possible state sequence which could have emitted the observation sequence, according to the maximum likelihood (ML) criterion (Figure <ref type="figure" target="#fig_0">2</ref>, step 3). Transition points, which define new segments, are detected using the current HMM topology and parameters. A new outlier is now identified by sorting the likelihood of all segments given the usual event model (Figure <ref type="figure" target="#fig_0">2</ref>, step 4). The detected outlier provides material for building another unusual event model, which is also adapted from usual event model. At the same time, both the unusual and usual event models are adapted using the detected unusual / usual event samples respectively. The process repeats until we obtain the desired number of unusual events. At each iteration, all usual / unusual event models are adapted from the parent node (see Figure <ref type="figure" target="#fig_1">3</ref>), and a new unusual event model is derived from the usual event model via Bayesian adaptation. The number of iterations thus corresponds to the number of unusual event models, as well as the number of states in the HMM topology.</p><p>As shown in Figure <ref type="figure" target="#fig_1">3</ref>, the proposed framework has a topdown hierarchical structure. Initially, there is only one node in the tree, representing the usual event model. At the first iteration, two new leaf nodes are split from the upper parent node: one representing usual events and the other one representing unusual events. At the second iteration, there are three leaf nodes in the tree: two for unusual events and one for usual events. The tree grows in a top-down fashion until we reach the desired number of iterations. The proposed algorithm is summarized in Figure <ref type="figure" target="#fig_0">2</ref>.</p><p>Compared with previous work on unusual event detection, our framework has a number of advantages. Most existing techniques using supervised learning for event detection require manually labeling of a large number of training samples. As our approach is semi-unsupervised, it does not need explicitly labeled unusual event data, facilitating initial training of the system and hence application to new conditions. Furthermore, we derive both unusual event and usual event models from a general usual event model via adaptation techniques in an online manner, thus allowing for a faster model training. In addition, the minimum duration constraint for temporal events can be easily imposed in the HMM framework by simply changing the number of cascaded states within each class.</p><p>In the next subsection, we give more details on the used adaptation techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MAP Adaptation</head><p>Several adaptation techniques have been proposed for GMM-based HMMs, such as Gaussian clustering, Maximum Likelihood Linear Regression (MLLR) and Maximum a posteriori (MAP) adaptation (also known as Bayesian adaptation) <ref type="bibr" target="#b13">[14]</ref>. These techniques have been widely used in tasks such as speaker and face verification <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b3">4]</ref>. In these cases, a general world model of speakers / faces are trained and then adapted to the particular speaker / face. In our case, we train a general usual event model and then use MAP to adapt both unusual and usual event models.</p><p>According to the MAP principle, we select parameters θ * such that they maximize the posterior probability density, that is:</p><formula xml:id="formula_3">θ * = arg max θ P (θ|X) = arg max θ P (X|θ) • P (θ),<label>(2)</label></formula><p>where P (X|θ) is the data likelihood and P (θ) is the prior distribution. When using MAP adaptation, different parameters can be chosen to be adapted <ref type="bibr" target="#b13">[14]</ref>. In <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b3">4]</ref>, the parameters that are adapted are the Gaussian means, while the mixture weights and standard deviations are kept fixed and equal to their corresponding value in the world model. In our case we adapt all the parameters. The reason to adapt the weights is that we model events (either usual or unusual) with different components in the mixture model. When only one specific event is present, it is expected that the weights of the other components will be adapted to zero (or a relatively small value). We also adapt the variances in order to move from the general model, which may have larger covariance matrix, to a specific model, with smaller variance, focusing on one particular event in the test sequence. Following <ref type="bibr" target="#b13">[14]</ref>, there are two steps in adaptation. First, estimates of the statistics of the training data are computed for each component of the old model. We use</p><formula xml:id="formula_4">{w new i , µ new i , σ new i</formula><p>} to represent the weight, mean and variance for component i in the new model, respectively. These parameters are estimated by ML, using the wellknown equations <ref type="bibr" target="#b1">[2]</ref>,</p><formula xml:id="formula_5">w new i = 1 M M j=1 P (i|x j , θ), (<label>3</label></formula><formula xml:id="formula_6">)</formula><formula xml:id="formula_7">µ new i = M j=1 x j P (i|x j , θ) M j=1 P (i|x j , θ) , (<label>4</label></formula><formula xml:id="formula_8">)</formula><formula xml:id="formula_9">σ new i = M j=1 P (i|x j , θ)(x j -µ new i )(x j -µ new i ) T M j=1 P (i|x j , θ) , (<label>5</label></formula><formula xml:id="formula_10">)</formula><p>where M is the number of data examples.</p><p>In the second step, the parameters of a mixture i are adapted using the following set of update equations <ref type="bibr" target="#b7">[8]</ref>.</p><formula xml:id="formula_11">ŵi = α • w old i + (1 -α) • w new i , (<label>6</label></formula><formula xml:id="formula_12">) μi = α • µ old i + (1 -α) • µ new i , (<label>7</label></formula><formula xml:id="formula_13">) σi = α • (σ old i + (μ i -µ old i )(μ i -µ old i ) T ) +(1 -α) • (σ new i + (μ i -µ new i )(μ i -µ new i ) T ),<label>(8)</label></formula><p>where { ŵi , μi , σi } are weight, mean and variance of the adapted model in component i, {w old i , µ old i , σ old i } are the corresponding parameters in the old component i respectively, and α is a weighting factor to control the balance between old model and new estimates. The smaller the value of α, the more contribution the new data makes to the adapted model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>In this section, we first introduce the performance measures and baseline systems we used to evaluate our results. Then we illustrate the effectiveness of the proposed framework using audio, visual and audio-visual events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Performance Measures</head><p>The problem of unusual event detection is a two-class classification problem (unusual events vs. usual events), with two types of errors: a false alarm (FA), when the method accepts an usual event sample (frame), and a false rejection (FR), when the method rejects an unusual event sample. The performance of the unusual event detection method can be measured in terms of two error rates: the false alarm rate (FAR), and the false rejection rate (FRR), defined as follows: The performance for an ideal event detection algorithm should have low values of both FAR and FRR. We also use the half-total error rate (HTER), which combines FAR and FRR into a single measure: HTER = FAR+FRR 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Systems</head><p>To evaluate the results, we compare the proposed semisupervised framework with the following baseline systems.</p><p>Supervised HMM: Two standard HMM models, one for usual events and one for unusual events, are trained using manually labeled training data according to Equation 1. For testing, the event boundary is obtained by applying Viterbi decoding on the sequences.</p><p>For supervised HMM, we test two cases. In the first case, we train usual and unusual event models using a large (sufficient) number of samples, referred to as supervised-1. In the second case, referred to as supervised-2, around 10% of the unusual event training samples from the first case are used to train the unusual event HMM. The purpose of supervised-2 is to investigate the case where there is only a small number of unusual event training samples.</p><p>Unsupervised HMM: The second baseline system is an agglomerative HMM-based clustering algorithm, recently proposed for speaker clustering <ref type="bibr" target="#b0">[1]</ref>, and that has shown good performance. The unsupervised HMM clustering algorithm starts by over-clustering, i.e. clustering the data into a large number of clusters. Then it searches for the best candidate pair of clusters for merging based on the criterion described in <ref type="bibr" target="#b0">[1]</ref>. The merging process is iterated until there are only two clusters left, one assumed to correspond to usual events, and another one for unusual events. We assume that the cluster with the largest number of samples represents usual events, and the other cluster represents unusual events. This model is referred to as unsupervised.</p><p>For both the proposed approach and the baseline methods, all parameters are selected to minimize half-total error rate (HTER) criterion on a validation data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on Audio Events</head><p>For the first experiment, we used a data set of audio events obtained through a sound search engine <ref type="foot" target="#foot_0">1</ref> . The purpose of this experiment is to have a controlled setup for evaluation of our algorithm. We first selected 60 minutes audio data containing only 'speaking' events. We then manually mixed it with other interesting audio events, namely 'applause', 'cheer', and 'laugh' events. The length of each concatenated segment is random. 'Speaking' is labeled as usual event, while all the other events are considered unusual. The minimum duration for audio events is two seconds. We extracted Mel-Frequency Cepstral Coefficients (MFCCs) features for this task. MFCC are short-term spectral-based features and have been widely used in speech recognition <ref type="bibr" target="#b12">[13]</ref> and audio event classification. We extracted 12 MFCC coefficients from the original audio signal using a sliding window of 40ms at fixed intervals of 20ms. The number of training and testing frames for the different methods is shown in Table <ref type="table" target="#tab_1">1</ref>. Note that there is no need for unusual event training data for our approach. For the unsupervised HMM, there is no need for training data. The percentage of frames for unusual events in the test sequence is around 3%.</p><p>Figure <ref type="figure" target="#fig_4">4</ref>(a) shows the performance of the proposed approach with respect to the number of iterations. We observe that FRR always decreases while FAR continually increases with the increase of the number of iterations. This is because our approach derives a new unusual event modal from the usual event model via Bayesian adaptation at each iteration. With the increase of unusual event models, more unusual events can be detected, while more usual events were falsely accepted as unusual events.</p><p>Figure <ref type="figure" target="#fig_4">4</ref>(b) shows the performance comparison between the proposed approach and baseline systems in terms of HTER. We can see that the supervised HMM with sufficient amount of training data gives the best performance. The proposed approach improves the performance, compared to the supervised-2 and unsupervised baselines. The results show that the benefit of using the proposed approach is not performance improvement when sufficient training data is available, but rather its effectiveness when there are not enough training samples for unusual events. The best result of our approach is obtained at 4 iterations (HTER = 6.65%), slightly worse than supervise-1 (HTER = 5.29%), showing the effectiveness of our approach given that it does not need any unusual event training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on Visual Events</head><p>The visual data we investigate is a 30-minute long poker game video, containing 26 different events and originally manually labeled and used in <ref type="bibr" target="#b18">[19]</ref>. Seven cheating related events, including 'hiding a card', 'exchanging cards', 'passing cards under table', etc., are categorized as unusual  events (see Figure <ref type="figure" target="#fig_6">6</ref>). Other events such as 'playing cards', 'drinking water', and 'scratching', are considered as usual events. The minimum duration for these visual events is 15 frames.</p><p>The number of training and testing frames for different methods is shown in Table <ref type="table" target="#tab_2">2</ref>. While we chose this visual task to show application on an existing data set, we note that the percentage of frames of unusual events in the test sequence is about 17%, which does not correspond very well to the assumption of rarity made by our model. The unusual event testing data for the supervised-1 method is much smaller, compared with other methods. This is because we use a larger number of unusual event frames (1320) for training, and we are left with a small number of unusual event frames (195) for testing. To deal with this problem, we repeat experiments for supervised-1 ten times by randomly splitting total unusual events into two parts: one with 1320 frames for training, and the other one with 195 frames for testing. We report the mean results of the ten runs. Note also that the amount of training data for the unusual model (1320 frames) is smaller than the previous experiments.</p><p>We extract motion and color features from moving blocks of each frame in the video in a similar way as in <ref type="bibr" target="#b18">[19]</ref>. We start with a static background image. We detect the moving objects using background substraction. We then superimpose a 6 × 6 grid on the detected motion mask. We first compute a motion histogram. In each tile of the grid, we calculate the total number of motion pixels, and ). We concatenate the motion histogram and the color histogram into a 108 = 36 + 2 × 36 dimension feature vector. To reduce the feature space dimension and for feature decorrelation, we apply a Principal Component Analysis (PCA) to transform the 108-dimensional features to 36-dimensional features.</p><p>The results are shown in Figure <ref type="figure" target="#fig_5">5</ref>. Overall, this is a more difficult task. We observe the similar trend of FAR and FRR as in audio event detection, with respect to the number of iterations in our approach. The best result of our approach is obtained with 4 iterations, although the values of HTER are relatively stable between 4 iterations and 7 iterations. We come to similar conclusions as for the audio event detection, that is, the supervised approach with sufficient training samples provides the best performance, while the proposed framework is better than the other baseline systems. Note that the supervised approach with small number of training samples performs worse than the unsupervised approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results on Audio-Visual Events</head><p>We also apply our framework to audio-visual unusual event detection using the ICCV'03 recorded presentation videos, publicly available<ref type="foot" target="#foot_1">2</ref> . Each presentation video is about 20 minutes in length with 25 frames per second. We define a set of multimodal unusual events, including 'speaker showing demo, audience applause', 'speaker playing video, audience laugh', and 'speaker interrupted by audience's questions'. Note that since some unusual events in the presentation setting cannot be defined before watching the entire database, the unusual events list we define here should be regarded as a small subset.</p><p>A set of audio-visual features were extracted. For audio features, we use the same features as in section 4.3. For visual features, we extract a motion histogram from each frame of the video, computed in a similar way to section 4.4. Audio and visual features were then concatenated.</p><p>Since the occurrence of unusual events is rare, manually labeling a large amount of samples is impractical, high-    lighting the need for semi-supervised or unsupervised approaches. Due to the lack of sufficient annotated training data for the supervised baselines, we only report results of our approach. Two presentation videos are used for training to build the general usual event model. We then apply our framework to a third meeting for unusual event detection. One of the co-authors labeled the events by hand to obtain a ground truth in the three videos. The results are shown in Figure <ref type="figure" target="#fig_7">7</ref>. We observe that, with the increase of iterations, FRR decreases while FAR increases, which means that more unusual events are detected, but at the cost of falsely accepting more usual events as unusual events. The best result of our approach is obtained when the number of iterations is 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Overall Discussion</head><p>Table <ref type="table" target="#tab_3">3</ref> summarizes overall results of audio, visual and audio-visual unusual event detection. For the proposed approach, the results correspond to the iteration with the minimum HTER. For both audio and visual unusual event detection, we can see that supervised HMM well-trained with sufficient data achieves the best performance while the proposed approach performs better than the other baseline systems.</p><p>As a well-known rule-of-thumb, the number of training samples needed for a well-trained model is directly related with the model complexity (the number of model parameters). The penalty for training with insufficient data is over-fitting, i.e. poor generalization capability. Both our approach and the baseline methods are based on HMMs for usual and unusual events modeling and hence have similar model complexity.</p><p>For the proposed approach, we currently do not determine the optimal number of iterations. As shown in Figures 4, 5 and 7, finding the optimal number of iterations is a trade-off between FAR and FRR. Some applications require more unusual events detected thus need more iterations. Otherwise, we might stop iterations at the early stages if fewer false alarms are expected. Automatic model selection is a difficult problem that we are studying, in particular with the Bayesian Information Criterion (BIC) <ref type="bibr" target="#b15">[16]</ref>. In our approach, there is one additional state in the HMM topology at each iteration, which results in an increase of both the number of model parameters and the likelihood of a test sequence. BIC could be used to handle the trade-off between model complexity and data likelihood.</p><p>We also note that feature selection is a critical issue in unusual event detection, particularly when using a semi-or unsupervised approach. The nature of the events found by the system will necessarily relate to the nature of discrimination provided by the features. In the above experiments, while the audio features seem to allow such discrimination, ongoing research should include investigation of different visual features.</p><p>Finally, regarding the three properties we used to define an unusual event (rarity, unexpectedness, and relevance), our method aims at accounting for the first two (one could argue that unexpectedness is a feature of some rare events). Relevance is a task-dependent property, whose incorporation in our work would require human intervention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we presented a semi-supervised adapted HMM framework for unusual event detection. The proposed framework is well suited for cases in which collecting sufficient unusual event training data is impractical and unusual events cannot be defined in advance. With relatively simple audio-visual features, and compared to both supervised and unsupervised baseline systems, our framework produces encouraging results. In future work, we will investigate the use of some criterion for optimizing the number of iterations, as well as improved feature selection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Iterative adapted HMM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure3. Illustration of the algorithm flow. At each iteration, two leaf nodes, one representing usual events and the other one representing unusual events, are split from the parent usual event node; A leaf node representing an unusual event is also adapted from the parent unusual event node.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Results for audio unusual event detection. The X-axis represents the number of iterations in our approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Results of visual unusual events detection.</figDesc><graphic coords="7,52.01,386.53,232.40,92.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Top: Visual event of 'exchanging cards'; Bottom: Visual event of 'passing cards under table'</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Results of our approach in terms of FAR, FRR and HTER.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Audio events data. Number of frames for various methods (NA: Not Applicable).</figDesc><table><row><cell>method</cell><cell cols="2">train set usual unusual</cell><cell cols="2">test set usual unusual</cell></row><row><cell cols="2">our approach 90000</cell><cell>NA</cell><cell></cell></row><row><cell cols="2">supervised-1 90000 supervised-2 90000</cell><cell>20000 2000</cell><cell>72750</cell><cell>2250</cell></row><row><cell>unsupervised</cell><cell>NA</cell><cell>NA</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Video events data. Number of frames for various methods (NA: Not Applicable).</figDesc><table><row><cell>method</cell><cell cols="4">train set usual unusual usual unusual test set</cell></row><row><cell cols="2">our approach 9000</cell><cell>NA</cell><cell></cell><cell>1515</cell></row><row><cell cols="2">supervised-1 9000 supervised-2 9000</cell><cell>1320 300</cell><cell>7387</cell><cell>195 1215</cell></row><row><cell>unsupervised</cell><cell>NA</cell><cell>NA</cell><cell></cell><cell>1515</cell></row><row><cell cols="5">these features are concatenated to form a 6 × 6 = 36 di-</cell></row><row><cell cols="5">mension feature vector to describe the motion in the cur-</cell></row><row><cell cols="5">rent frame. In a similar way, we can compute the color</cell></row><row><cell cols="5">histogram for the moving objects in chromatic color space (defined by r = R R+G+B , g =</cell></row></table><note><p>G R+G+B</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Overall the best results</figDesc><table><row><cell>Events</cell><cell>Method</cell><cell cols="3">FAR % FRR % HTER %</cell></row><row><cell></cell><cell>our method</cell><cell>2.09</cell><cell>11.2</cell><cell>6.65</cell></row><row><cell>audio</cell><cell>supervised 1 supervised-2</cell><cell>3.97 11.8</cell><cell>6.62 12.6</cell><cell>5.29 12.2</cell></row><row><cell></cell><cell>unsupervised</cell><cell>12.5</cell><cell>24.2</cell><cell>18.3</cell></row><row><cell></cell><cell>our method</cell><cell>42.2</cell><cell>21.4</cell><cell>31.8</cell></row><row><cell>visual</cell><cell>supervised-1 supervised-2</cell><cell>26.8 41.3</cell><cell>29.6 40.2</cell><cell>28.2 40.7</cell></row><row><cell></cell><cell>unsupervised</cell><cell>40.1</cell><cell>35.5</cell><cell>37.8</cell></row><row><cell cols="2">audio-visual our approach</cell><cell>7.20</cell><cell>28.2</cell><cell>17.7</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www.findsounds.com/types.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://www.robots.ox.ac.uk/∼awf/iccv03videos</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Hua Zhong (Carnegie Mellon University), Jianbo Shi and Mirko Visontai (University of Pennsylvania) for providing visual data for experiments. We also thank David Barber (IDIAP Research Institute) for helpful comments.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* This work was supported by the Swiss National Center of Competence in Research on Interactive Multimodal Information Management (IM2), and the EC project Augmented Multi-party Interaction (AMI, pub. AMI-62).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A robust speaker clustering algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ajmera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wooters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Automatic Speech Recognition Understanding Workshop</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A gentle tutorial of the EM algirthm and its application to parameter estimation for gaussian mixture and hidden markov models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<idno>ICSI-TR-97-021</idno>
		<imprint>
			<date type="published" when="1997">1997</date>
			<pubPlace>Berkeley</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Advanced Visual Surveillance using Bayesian Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Buxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Prof. IEEE ICCV</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adapted generative models for face verification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Cardinaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting rare events in video using semantic primitives with HMM</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hoogs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmiederer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICPR</title>
		<meeting>ICPR</meeting>
		<imprint>
			<date type="published" when="2004-08">August 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Highlight detection and classification of baseball game video with hidden markov models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICIP</title>
		<meeting>IEEE ICIP<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-09">Sept. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Maximum a posteriori estimation for multivariate gaussian mixture observation of markov chains</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Gauvain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Speech Audio Processing</title>
		<imprint>
			<date type="published" when="1994-04">April 1994</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="291" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recognition of group activities using a dynamic probabilistic network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV<address><addrLine>Nice</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-10">Oct. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bayesian framework for video surveillance application</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hongeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICPR</title>
		<meeting>ICPR</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Event detection and analysis from video streams</title>
		<author>
			<persName><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hongeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence, archive</title>
		<imprint>
			<date type="published" when="2001-08">August 2001</date>
			<biblScope unit="volume">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Bayesian Computer Vision System for Modeling Human Interactions</title>
		<author>
			<persName><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rosario</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence, archive</title>
		<imprint>
			<date type="published" when="2000-08">August 2000</date>
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Juang</surname></persName>
		</author>
		<title level="m">Fundamentals of Speech Recognition</title>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Speaker verification using adapted gaussian mixture models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Dunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="19" to="41" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatically extracting highlights for tv baseball programs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia</title>
		<meeting>ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2000-10">Oct. 2000</date>
			<biblScope unit="page" from="105" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Estimating the dimension of a model. The Annals of Statistics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schwarz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978">1978</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="461" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning patterns of activity using real-time tracking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Grimson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence, archive</title>
		<imprint>
			<date type="published" when="2000-08">August 2000</date>
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sports highlight detection from keyword sequences using hmm</title>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Chng</surname></persName>
		</author>
		<author>
			<persName><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICME</title>
		<meeting>IEEE ICME<address><addrLine>Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-06">June 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Detecting unusual activity in video</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Visontai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2004-06">June. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
