<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Improved Analysis of Training Over-parameterized Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Difan</forename><surname>Zou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Improved Analysis of Training Over-parameterized Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A recent line of research has shown that gradient-based algorithms with random initialization can converge to the global minima of the training loss for overparameterized (i.e., sufficiently wide) deep neural networks. However, the condition on the width of the neural network to ensure the global convergence is very stringent, which is often a high-degree polynomial in the training sample size n (e.g., O(n 24 )). In this paper, we provide an improved analysis of the global convergence of (stochastic) gradient descent for training deep neural networks, which only requires a milder over-parameterization condition than previous work in terms of the training sample size and other problem-dependent parameters. The main technical contributions of our analysis include (a) a tighter gradient lower bound that leads to a faster convergence of the algorithm, and (b) a sharper characterization of the trajectory length of the algorithm. By specializing our result to two-layer (i.e., one-hidden-layer) neural networks, it also provides a milder over-parameterization condition than the best-known result in prior work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent study <ref type="bibr" target="#b19">[20]</ref> has revealed that deep neural networks trained by gradient-based algorithms can fit training data with random labels and achieve zero training error. Since the loss landscape of training deep neural network is highly nonconvex or even nonsmooth, conventional optimization theory cannot explain why gradient descent (GD) and stochastic gradient descent (SGD) can find the global minimum of the loss function (i.e., achieving zero training error). To better understand the training of neural networks, there is a line of research <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b11">12]</ref> studying two-layer (i.e., one-hidden-layer) neural networks, where it assumes there exists a teacher network (i.e., an underlying ground-truth network) generating the output given the input, and casts neural network learning as weight matrix recovery for the teacher network. However, these studies not only make strong assumptions on the training data (existence of ground-truth network with the same architecture as the learned network), but also need special initialization methods that are very different from the commonly used initialization method <ref type="bibr" target="#b12">[13]</ref> in practice. Li and Liang <ref type="bibr" target="#b14">[15]</ref>, Du et al. <ref type="bibr" target="#b10">[11]</ref> advanced this line of research by proving that under much milder assumptions on the training data, (stochastic) gradient descent can attain a global convergence for training over-parameterized (i.e.,sufficiently wide) two-layer ReLU network with widely used random initialization method <ref type="bibr" target="#b12">[13]</ref>. More recently, Allen-Zhu et al. <ref type="bibr" target="#b1">[2]</ref>, Du et al. <ref type="bibr" target="#b8">[9]</ref>, Zou et al. <ref type="bibr" target="#b23">[24]</ref> generalized the global convergence results from two-layer networks to deep neural networks. However, there is a huge gap between the theory and practice since all these work Li and Liang <ref type="bibr" target="#b14">[15]</ref>, Du et al. <ref type="bibr" target="#b10">[11]</ref>, Allen-Zhu et al. <ref type="bibr" target="#b1">[2]</ref>, Du et al. <ref type="bibr" target="#b8">[9]</ref>, Zou et al. <ref type="bibr" target="#b23">[24]</ref> require unrealistic over-parameterization conditions on the width of neural networks, especially for deep networks. In specific, in order to establish the global convergence for training two-layer ReLU networks, Du et al. <ref type="bibr" target="#b10">[11]</ref> requires the network width, i.e., number of hidden nodes, to be at least ⌦(n 6 / 4 0 ), where n is the training sample size and 0 is the smallest eigenvalue of the so-called Gram matrix defined in Du et al. <ref type="bibr" target="#b10">[11]</ref>, which is essentially the neural tangent kernel <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b6">7]</ref> on the training data. Under the same assumption on the training data, Wu et al. <ref type="bibr" target="#b18">[19]</ref> improved the iteration complexity of GD in Du et al. <ref type="bibr" target="#b10">[11]</ref> from O n 2 log(1/✏)/ 2 0 to O n log(1/✏)/ 0 and Oymak and Soltanolkotabi <ref type="bibr" target="#b16">[17]</ref> improved the over-parameterization condition to ⌦(nkXk 6  2 / 4 0 ), where ✏ is the target error and X 2 R n⇥d is the input data matrix. For deep ReLU networks, the best known result was established in Allen-Zhu et al. <ref type="bibr" target="#b1">[2]</ref>, which requires the network width to be at least e ⌦(kn 24 L 12 8 )<ref type="foot" target="#foot_1">1</ref> to ensure the global convergence of GD and SGD, where L is the number of hidden layers, is the minimum data separation distance and k is the output dimension. This paper continues the line of research, and improves the over-parameterization condition and the global convergence rate of (stochastic) gradient descent for training deep neural networks. In specific, under the same setting as in Allen-Zhu et al. <ref type="bibr" target="#b1">[2]</ref>, we prove faster global convergence rates for both GD and SGD under a significantly milder condition on the neural network width. Furthermore, when specializing our result to two-layer ReLU networks, it also outperforms the best-known result proved in Oymak and Soltanolkotabi <ref type="bibr" target="#b16">[17]</ref>. The improvement in our result is due to the following two innovative proof techniques: (a) a tighter gradient lower bound, which leads to a faster rate of convergence for GD/SGD; and (b) a sharper characterization of the trajectory length for GD/SGD until convergence.</p><p>We highlight our main contributions as follows:</p><p>• We show that, with Gaussian random initialization <ref type="bibr" target="#b12">[13]</ref> on each layer, when the number of hidden nodes per layer is e ⌦ kn 8 L 12 4 , GD can achieve ✏ training loss within e O n 2 L 2 log(1/✏) 1 iterations, where L is the number of hidden layers, is the minimum data separation distance, n is the number of training examples, and k is the output dimension. Compared with the state-of-the-art result <ref type="bibr" target="#b1">[2]</ref>, our over-parameterization condition is milder by a factor of e ⌦(n 16 4 ), and our iteration complexity is better by a factor of e O(n 4 1 ). • We also prove a similar convergence result for SGD. We show that with Gaussian random initialization <ref type="bibr" target="#b12">[13]</ref> on each layer, when the number of hidden nodes per layer is e ⌦ kn For the ease of comparison, we summarize the best-known results <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b16">17]</ref> of training overparameterized neural networks with GD and compare with them in terms of over-parameterization condition and iteration complexity in Table <ref type="table" target="#tab_1">1</ref>. We will show in Section 3 that, under the assumption that all training data points have unit `2 norm, which is the common assumption made in all these work <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b16">17]</ref>, 0 &gt; 0 is equivalent to the fact that all training data are separated by some distance , and we have 0 = O(n 2 ) <ref type="bibr" target="#b16">[17]</ref>. Substituting 0 = ⌦(n 2 ) into Table <ref type="table" target="#tab_1">1</ref>, it is evident that our result outperforms all the other results under the same assumptions.</p><p>Notation For scalars, vectors and matrices, we use lower case, lower case bold face, and upper case bold face letters to denote them respectively. For a positive integer, we denote by [k] the set {1, . . . , k}.</p><p>For a vector x = (x 1 , . . . , x d ) &gt; and a positive integer p, we denote by kxk p = P d i=1 |x i | p 1/p the `p norm of x. In addition, we denote by kxk 1 = max i=1,...,d |x i | the `1 norm of x, and kxk 0 = |{x i : x i 6 = 0, i = 1, . . . , d}| the `0 norm of x. For a matrix A 2 R m⇥n , we denote by kAk F the Frobenius norm of A, kAk 2 the spectral norm (maximum singular value), min (A) the smallest singular value, kAk 0 the number of nonzero entries, and kAk 2,1 the maximum `2 norm over all row vectors, i.e., kAk 2,1 = max i=1,...,m kA i⇤ k 2 . For a collection of matrices </p><formula xml:id="formula_0">W = {W 1 , . . . , W L }, we denote kWk F = q P L l=1 kW l k 2 F , kWk 2 = max l2[L] kW l k 2 and</formula><formula xml:id="formula_1">⌘ O ⇣ n log(1/✏) 0 ⌘ no yes Oymak and Soltanolkotabi [17] ⌦ ⇣ nkXk 6 2 4 0 ⌘ O ⇣ kXk 2 2 log(1/✏) 0 ⌘ no yes Du et al. [9] ⌦ ⇣ 2 O(L) •n 4 4 min (K (L) ) ⌘ O ⇣ 2 O(L) •n 2 log(1/✏) 2 min (K (L) ) ⌘ yes no Allen-Zhu et al. [2] e ⌦ ⇣ kn 24 L 12 8 ⌘ O ⇣ n 6 L 2 log(1/✏) 2 ⌘ yes yes This paper e ⌦ ⇣ kn 8 L 12 4 ⌘ O ⇣ n 2 L 2 log(1/✏) ⌘ yes yes kWk 2,1 = max l2[L] kW l k 2,1 . Given two collections of matrices f W = { f W 1 , . . . , f W L } and c W = { c W 1 , . . . , c W L }, we define their inner product as h f W, c Wi = P L l=1 h f W l , c W l i. For two sequences {a n } and {b n }, we use a n = O(b n ) to denote that a n  C 1 b n for some absolute constant C 1 &gt; 0, and use a n = ⌦(b n ) to denote that a n C 2 b n for some absolute constant C 2 &gt; 0.</formula><p>In addition, we use e O(•) and e ⌦(•) to hide logarithmic factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem setup and algorithms</head><p>In this section, we introduce the problem setup and the training algorithms.</p><p>Following Allen-Zhu et al. <ref type="bibr" target="#b1">[2]</ref>, we consider the training of an L-hidden layer fully connected neural network, which takes x 2 R d as input, and outputs y 2 R k . In specific, the neural network is a vector-valued function f W : R d ! R k , which is defined as</p><formula xml:id="formula_2">f W (x) = V (W L (W L 1 • • • (W 1 x) • • • )),</formula><p>where W 1 2 R m⇥d , W 2 , . . . , W L 2 R m⇥m denote the weight matrices for the hidden layers, and V 2 R k⇥m denotes the weight matrix in the output layer, (x) = max{0, x} is the entry-wise ReLU activation function. In addition, we denote by 0 (x) = 1(x) the derivative of ReLU activation function and w l,j the weight vector of the j-th node in the l-th layer.</p><p>Given a training set {(x i , y i )} i=1,...,n where x i 2 R d and y i 2 R k , the empirical loss function for training the neural network is defined as</p><formula xml:id="formula_3">L(W) := 1 n n X i=1 `(b y i , y i ),<label>(2.1)</label></formula><p>where `(•, •) is the loss function, and b</p><formula xml:id="formula_4">y i = f W (x i ).</formula><p>In this paper, for the ease of exposition, we follow Allen-Zhu et al. <ref type="bibr" target="#b1">[2]</ref>, Du et al. <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b8">9]</ref>, Oymak and Soltanolkotabi <ref type="bibr" target="#b16">[17]</ref> and consider square loss as follows</p><formula xml:id="formula_5">`(b y i , y i ) = 1 2 ky i b y i k 2 2 ,</formula><p>where b</p><formula xml:id="formula_6">y i = f W (x i ) 2 R k denotes the output of the neural network given input x i .</formula><p>It is worth noting that our result can be easily extended to other loss functions such as cross entropy loss <ref type="bibr" target="#b23">[24]</ref> as well.</p><p>We will study both gradient descent and stochastic gradient descent as training algorithms, which are displayed in Algorithm 1. For gradient descent, we update the weight matrix W (t) l using full partial gradient r W l L(W (t) ). For stochastic gradient descent, we update the weight matrix W (t) is independently generated from N (0, 2/mI), each row of V is independently generated from N (0, I/k) Gradient Descent 3: for t = 0, . . . , T do 4:</p><formula xml:id="formula_7">l using stochastic partial gradient 1/B P s2B (t) r W l ` f W (t) (x s ),</formula><formula xml:id="formula_8">W (t+1) l = W (t) l ⌘r W l L(W (t) ) for all l 2 [L] 5: end for 6: output: {W (T ) l } l2[L]</formula><p>Stochastic Gradient Descent 7: for t = 0, . . . , T do 8:</p><p>Uniformly sample a minibatch of training data</p><formula xml:id="formula_9">B (t) 2 [n] 9: W (t+1) l = W (t) l ⌘ B P s2B (t) r W l ` f W (t) (x s ), y s for all l 2 [L] 10: end for 11: output: {W (T ) l } l2[L]</formula><p>way as Allen-Zhu et al. <ref type="bibr" target="#b1">[2]</ref>, which is essentially the initialization method <ref type="bibr" target="#b12">[13]</ref> widely used in practice. In the remaining of this paper, we denote by</p><formula xml:id="formula_10">rL(W (t) ) = {r W l L(W (t) )} l2[L] and r` f W (t) (x i ), y i = {r W l ` f W (t) (x i ), y i } l2[L]</formula><p>the collections of all partial gradients of L(W (t) ) and ` f W (t) (x i ), y i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Main theory</head><p>In this section, we present our main theoretical results. We make the following assumptions on the training data. Assumption 3.1. For any x i , it holds that kx i k 2 = 1 and (x i ) d = µ, where µ is an positive constant.</p><p>The same assumption has been made in all previous work along this line <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b16">17]</ref>. Note that requiring the norm of all training examples to be 1 is not essential, and this assumption can be relaxed to be kx i k 2 is lower and upper bounded by some constants. Assumption 3.2. For any two different training data points x i and x j , there exists a positive constant &gt; 0 such that kx i x j k 2 .</p><p>This assumption has also been made in Allen-Zhu et al. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref>, which is essential to guarantee zero training error for deep neural networks. It is a quite mild assumption for the regression problem as studied in this paper. Note that Du et al. <ref type="bibr" target="#b8">[9]</ref> made a different assumption on training data, which requires the Gram matrix K (L) (See their paper for details) defined on the L-hidden-layer networks is positive definite. However, their assumption is not easy to verify for neural networks with more than two layers.</p><p>Based on Assumptions 3.1 and 3.2, we are able to establish the global convergence rates of GD and SGD for training deep ReLU networks. We start with the result of GD for L-hidden-layer networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training L-hidden-layer ReLU networks with GD</head><p>The global convergence of GD for training deep neural networks is stated in the following theorem.  <ref type="table" target="#tab_1">1</ref>, the over-parameterization condition and iteration complexity in Du et al. <ref type="bibr" target="#b8">[9]</ref> have an exponential dependency on L, which is much worse than the polynomial dependency on L as in Allen-Zhu et al. <ref type="bibr" target="#b1">[2]</ref> and our result.</p><p>We now specialize our results in Theorem 3.3 to two-layer networks by removing the dependency on the number of hidden layers, i.e., L. We state this result in the following corollary. For training two-layer ReLU networks, Du et al. <ref type="bibr" target="#b10">[11]</ref> made a different assumption on the training data to establish the global convergence of GD. Specifically, Du et al. <ref type="bibr" target="#b10">[11]</ref> defined a Gram matrix, which is also known as neural tangent kernel <ref type="bibr" target="#b13">[14]</ref>, based on the training data {x i } i=1,...,n and assumed that the smallest eigenvalue of such Gram matrix is strictly positive. In fact, for two-layer neural networks, their assumption is equivalent to Assumption 3.2, as shown in the following proposition. Proposition 3.6. Under Assumption 3.1, define the Gram matrix H 2 R n⇥n as follows</p><formula xml:id="formula_11">H ij = E w⇠N (0,I) [x &gt; i x j 0 (w &gt; x i ) 0 (w &gt; x j )],</formula><p>then the assumption 0 = min (H) &gt; 0 is equivalent to Assumption 3.2. In addition, there exists a sufficiently small constant C such that 0 C n 2 .</p><p>Remark 3.7. According to Proposition 3.6, we can make a direct comparison between our convergence results for two-layer ReLU networks in Corollary 3.5 with those in Du et al. <ref type="bibr" target="#b10">[11]</ref>, Oymak and Soltanolkotabi <ref type="bibr" target="#b16">[17]</ref>. In specific, as shown in Table <ref type="table" target="#tab_1">1</ref>, the iteration complexity and overparameterization condition proved in Du et al. <ref type="bibr" target="#b10">[11]</ref> can be translated to O(n 6 log(1/✏)/ 2 ) and ⌦(n 14 / 4 ) respectively under Assumption 3.2. Oymak and Soltanolkotabi <ref type="bibr" target="#b16">[17]</ref> improved the result in Du et al. <ref type="bibr" target="#b10">[11]</ref> and the improved iteration complexity and over-parameterization condition can be translated to O n 2 kXk 2 2 log(1/✏)/ 2 and ⌦ n 9 kXk 6 2 / 4 respectively, where X = [x 1 , . . . , x n ] &gt; 2 R d⇥n is the input data matrix. Our iteration complexity for two-layer ReLU networks is better than that in Oymak and Soltanolkotabi <ref type="bibr" target="#b16">[17]</ref> by a factor of O(kXk 2  2 ) 3 , and the over-parameterization condition is also strictly milder than the that in Oymak and Soltanolkotabi <ref type="bibr" target="#b16">[17]</ref> by a factor of O(nkXk 6  2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Extension to training L-hidden-layer ReLU networks with SGD</head><p>Then we extend the convergence results of GD to SGD in the following theorem. </p><formula xml:id="formula_12">T = O n 5 L 2 log(m) log 2 (1/✏)/(B 2 )</formula><p>iterations. 2 It is worth noting that kXk 2 2 = O(1) if d . n, kXk 2 2 = O(n/d) if X is randomly generated, and kXk 2 2 = O(n) in the worst case. 3 Here we set k = 1 in order to match the problem setting in Du et al. <ref type="bibr" target="#b10">[11]</ref>, Oymak and Soltanolkotabi <ref type="bibr" target="#b16">[17]</ref>.</p><p>Remark 3.9. We first compare our result with the state-of-the-art proved in Allen-Zhu et al. <ref type="bibr" target="#b1">[2]</ref>, where they showed that SGD can find a point with ✏-training loss within e O n 7</p><formula xml:id="formula_13">L 2 log(1/✏)/(B 2 ) iterations if m = e ⌦ n 24 L 12</formula><p>Bk/ 8 . In stark contrast, our result on the over-parameterization condition is strictly better than it by a factor of e ⌦(n 7 B 5 ), and our result on the iteration complexity is also faster by a factor of O(n 2 ).</p><p>Moreover, we also characterize the convergence rate and over-parameterization condition of SGD for training two-layer networks. Unlike the gradient descent, which has the same convergence rate and over-parameterization condition for training both deep and two-layer networks in terms of training data size n, we find that the over-parameterization condition of SGD can be further improved for training two-layer neural networks. We state this improved result in the following theorem.</p><p>Theorem 3.10. Under the same assumptions made in Theorem 3.8. For two-layer ReLU networks, if set the number of hidden nodes and step size as</p><formula xml:id="formula_14">m = ⌦ k 5/2 n 11 log 3 (m)/( 5 B) , ⌘ = O kB /(n 3 m log(m)) ,</formula><p>then with probability at least 1 O(n 1 ), stochastic gradient descent is able to achieve ✏ training loss within T = O n 5 log(m) log(1/✏)/(B 2 ) iterations.</p><p>Remark 3.11. From Theorem 3.8, we can also obtain the convergence results of SGD for two-layer ReLU networks by choosing L = 1. However, the resulting over-parameterization condition is m = ⌦ kn 17 log 3 (m)B 4 8 , which is much worse than that in Theorem 3.10. This is because for two-layer networks, the training loss enjoys nicer local properties around the initialization, which can be leveraged to improve the convergence of SGD. Due to space limit, we defer more details to Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proof sketch of the main theory</head><p>In this section, we provide the proof sketch for Theorems 3.3, and highlight our technical contributions and innovative proof techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview of the technical contributions</head><p>The improvements in our result are mainly attributed to the following two aspects: (1) a tighter gradient lower bound leading to faster convergence; and (2) a sharper characterization of the trajectory length of the algorithm.</p><p>We first define the following perturbation region based on the initialization,</p><formula xml:id="formula_15">B(W (0) , ⌧) = {W : kW l W<label>(0)</label></formula><p>l k 2  ⌧ for all l 2 [L]}, where ⌧ &gt; 0 is the preset perturbation radius for each weight matrix W l .</p><p>Tighter gradient lower bound. By the definition of rL(W), we have krL(W)k 2</p><formula xml:id="formula_16">F = P L l=1 kr W l L(W)k 2 F kr W L L(W)k 2 F .</formula><p>Therefore, we can focus on the partial gradient of L(W) with respect to the weight matrix at the last hidden layer. Note that we further have</p><formula xml:id="formula_17">kr W L L(W)k 2 F = P m j=1 kr w L,j L(W)k 2 2</formula><p>, where <ref type="figure">and</ref> x L 1,i denotes the output of the (L 1)-th hidden layer with input x i . In order to prove the gradient lower bound, for each x L 1,i , we introduce a region namely "gradient region", denoted by W j , which is almost orthogonal to x L 1,i . Then we prove two major properties of these n regions {W 1 , . . . , W n }: (1) W i \ W j = ; if i 6 = j, and (2) if w L,j 2 W i for any i, with probability at least 1/2, kr w L,j L(W)k 2 is sufficiently large. We visualize these "gradient regions" in Figure <ref type="figure">1</ref>(a). Since {w L,j } j2[m] are randomly generated at the initialization, in order to get a larger bound of kr W L L(W)k 2 F , we hope the size of these "gradient regions" to be as large as possible. We take the union of the "gradient regions" for all training data, i.e., [ n i=1 W i , which is shown in Figure <ref type="figure">1</ref>(a). As a comparison, Allen-Zhu et al. <ref type="bibr" target="#b1">[2]</ref>, Zou et al. <ref type="bibr" target="#b23">[24]</ref> only leveraged the "gradient region" for one training data point to establish the gradient lower bound, which is shown in Figure <ref type="figure">1(b)</ref>. Roughly speaking, the size of "gradient regions" utilized in our proof is n times larger than those used in Allen-Zhu et al.</p><formula xml:id="formula_18">r w L,j L(W) = 1 n n X i=1 hf W (x i ) y i , v j i 0 hw L,j , x L 1,i i x L 1,i ,</formula><p>[2], Zou et al. <ref type="bibr" target="#b23">[24]</ref>, which consequently leads to an O(n) improvement on the gradient lower bound. The improved gradient lower bound is formally stated in the following lemma.</p><formula xml:id="formula_19">Lemma 4.1 (Gradient lower bound). Let ⌧ = O 3/2 n 3 L 6 log 3/2 (m) , then for all W 2 B(W (0)</formula><p>, ⌧), with probability at least</p><formula xml:id="formula_20">1 exp O(m /(dn))), it holds that krL(W)k 2 F O m L(W)/(kn 2 ) .</formula><p>Sharper characterization of the trajectory length. The improved analysis of the trajectory length is motivated by the following observation: at the t-th iteration, the decrease of the training loss after one-step gradient descent is proportional to the gradient norm, i.e., L(W (t) ) L(W (t+1) ) / krL(W (t) )k 2 F . In addition, the gradient norm krL(W (t) )k F determines the trajectory length in the t-th iteration. Putting them together, we can obtain</p><formula xml:id="formula_21">kW (t+1) l W (t) l k 2 = ⌘kr W l L(W (t) )k 2  p Ckn 2 /(m ) • ⇣ q L(W (t) ) q L(W (t+1) ) ⌘ , (4.1)</formula><p>where C is an absolute constant. (4.1) enables the use of telescope sum, which yields kW</p><formula xml:id="formula_22">(t) l W (0) l k 2  p</formula><p>Ckn 2 L(W (0) )/m . In stark contrast, Allen-Zhu et al. <ref type="bibr" target="#b1">[2]</ref> bounds the trajectory length as</p><formula xml:id="formula_23">kW (t+1) l W (t) l k 2 = ⌘kr W l L(W (t) )k 2  ⌘ q C 0 mL(W (t) )/k,</formula><p>and further prove that kW</p><formula xml:id="formula_24">(t) l W (0) l k 2  p C 0 kn 6 L 2 (W (0)</formula><p>)/(m 2 ) by taking summation over t, where C 0 is an absolute constant. Our sharp characterization of the trajectory length is formally summarized in the following lemma.  Our proof road map can be organized in three steps: (i) prove that the training loss enjoys good curvature properties within the perturbation region B(W (0) , ⌧); (ii) show that gradient descent is able to converge to global minima based on such good curvature properties; and (iii) ensure all iterates stay inside the perturbation region until convergence.</p><p>Step (i) Training loss properties. We first show some key properties of the training loss within B(W (0)  , ⌧), which are essential to establish the convergence guarantees of gradient descent. Moreover, the training loss L(W) is nonsmooth due to the non-differentiable ReLU activation function. Generally speaking, smoothness is essential to achieve linear rate of convergence for gradient-based algorithms. Fortunately, Allen-Zhu et al. <ref type="bibr" target="#b1">[2]</ref> showed that the training loss satisfies locally semi-smoothness property, which is summarized in the following lemma.</p><p>Lemma 4.4 (Semi-smoothness <ref type="bibr" target="#b1">[2]</ref>). Let</p><formula xml:id="formula_25">⌧ 2 ⇥ ⌦ k 3/2 /(m 3/2 L 3/2 log 3/2 (m)) , O 1/(L 4.5 log 3/2 (m))</formula><p>⇤ .</p><p>Then for any two collections c</p><formula xml:id="formula_26">W = { c W l } l2[L] and f W = { f W l } l2[L] satisfying c W, f W 2 B(W (0)</formula><p>, ⌧), with probability at least 1 exp( ⌦( m⌧ 3/2 L)), there exist two constants C 0 and C 00 such that This lemma suggests that if the perturbation region is small, i.e., ⌧ ⌧ 1, the non-smooth term (third term on the R.H.S. of (4.2)) is small and dominated by the gradient term (the second term on the the R.H.S. of (4.2)). Therefore, the training loss behaves like a smooth function in the perturbation region and the linear rate of convergence can be proved.</p><formula xml:id="formula_27">L( f W)  L( c W) + hrL( c W), f W c Wi + C 0 q L( c W) • ⌧ 1/3 L 2 p m log(m) p k • k f W c Wk 2 + C 00 L 2 m k k f W c Wk 2 2 . (<label>4</label></formula><p>Step (ii) Convergence rate of GD. Now we are going to establish the convergence rate for gradient descent under the assumption that all iterates stay inside the region B(W (0) , ⌧), where ⌧ will be specified later. </p><formula xml:id="formula_28">L(W (t) )  ✓ 1 O ✓ m ⌘ kn 2 ◆◆ t L(W (0) ).</formula><p>Lemma 4.5 suggests that gradient descent is able to decrease the training loss to zero at a linear rate.</p><p>Step (iii) Verifying all iterates of GD stay inside the perturbation region. Then we are going to ensure that all iterates of GD are staying inside the required region B(W (0) , ⌧). Note that we have proved the distance kW</p><formula xml:id="formula_29">(t) l W (0) l k 2 in Lemma 4.2.</formula><p>Therefore, it suffices to verify that such distance is smaller than the preset value ⌧ . Thus, we can complete the proof of Theorem 3.3 by verifying the conditions based on our choice of m. Note that we have set the required number of m in (3.1), plugging (3.1) into the result of Lemma 4.2, we have with probability at least 1 O(n 1 ), the following holds for all t  T and l 2</p><formula xml:id="formula_30">[L] kW (t) l W (0) l k 2  O 3/2 n 3 L 6 log 3/2 (m) ,</formula><p>which is exactly in the same order of ⌧ in Lemma 4.5. Therefore, our choice of m guarantees that all iterates are inside the required perturbation region. In addition, by Lemma 4.5, in order to achieve ✏ accuracy, we require</p><formula xml:id="formula_31">T ⌘ = O kn 2 log 1/✏ m 1 1 . (4.3)</formula><p>Then substituting our choice of step size ⌘ = O k/(L 2 m) into (4.3) and applying Lemma 4.3, we can get the desired result for T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Optimizing both top and hidden layers</head><p>Here we would like to briefly discuss the extension to the case where the top layer is also optimized. The proof sketch is as follows: similar to our current proof, we can also define a small perturbation region around the initialization, but the new definition involves a constraint on the top layer weights. Specifically, such new perturbation region can be defined as follows, B(W (0)  , ⌧) = {W : kW l W (0) l k 2  ⌧ for all l 2 [L], kV V (0) k 2  ⌧ 0 }. Then, it can be proved that the neural network also enjoys good properties inside such region. Similar to the proof in this paper, based on these good properties, we can prove that until convergence the neural network weights, including the top layer weights, would not escape from such region. Note that optimizing more parameter can lead to larger gradient, thus we can prove a larger gradient lower bound during the training process which can potential speed up the convergence of optimization algorithm (e.g., GD, SGD).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and future work</head><p>In this paper, we studied the global convergence of (stochastic) gradient descent for training overparameterized ReLU networks, and improved the state-of-the-art results. Our proof technique can be also applied to prove similar results for other loss functions such as cross-entropy loss and other neural network architectures such as convolutional neural networks (CNN) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref> and ResNet <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21]</ref>. One important future work is to investigate whether the over-parameterization condition and the convergence rate can be further improved. It is promising that if we can further improve the characterization of "gradient region", as it may provide a tighter gradient lower bound and consequently sharpen the over-parameterization condition. Another interesting future direction is to explore the use of our proof technique to improve the generalization analysis of overparameterized neural networks trained by gradient-based algorithms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b3">4]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Corollary 3 . 5 .</head><label>35</label><figDesc>Under the same assumptions made in Theorem 3.3. For training two-layer ReLU networks, if set the number of hidden nodes m = ⌦ kn 8 log 3 (m)/ 4 and step size ⌘ = O(k/m), then with probability at least 1 O(n 1 ), GD is able to find a point that achieves ✏-training loss within T = O n 2 log(1/✏)/ iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Theorem 3 . 8 .</head><label>38</label><figDesc>Under Assumptions 3.1 and 3.2, and suppose the number of hidden nodes per layer satisfies m = ⌦ kn 17 L 12 log 3 (m)/(B 4 8 ) . (3.2) Then if set the step size as ⌘ = O kB /(n 3 m log(m)) , with probability at least 1 O(n 1 ), SGD is able to achieve ✏ expected training loss within</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 Figure 1 :</head><label>11</label><figDesc>Figure 1: (a): "gradient region" for all training data (b): "gradient region" for one training example.</figDesc><graphic url="image-2.png" coords="7,307.25,80.66,158.39,117.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Lemma 4 . 2 ./2 n 3 L 6 kn 2</head><label>42362</label><figDesc>Assuming all iterates are staying inside the regionB(W (0) , ⌧) with ⌧ = O 3log 3/2 (m) , if set the step size ⌘ = O k/(L 2m) , with probability least 1 O(n 1 ), the following holds for all t 0 and l 2 [L], log(n)/(m ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4. 2</head><label>2</label><figDesc>Proof of Theorem 3.3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Lemma 4 . 3 .</head><label>43</label><figDesc>If m O(L log(nL)), with probability at least 1 O(n 1 ) it holds that L(W (0) )  e O(1). Lemma 4.3 suggests that the training loss L(W) at the initial point does not depend on the number of hidden nodes per layer, i.e., m.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>. 2 )</head><label>2</label><figDesc>Lemma 4.4 is a rescaled version of Theorem 4 in Allen-Zhu et al. [2], since the training loss L(W) in (2.1) is divided by the training sample size n, as opposed to the training loss in Allen-Zhu et al. [2].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Lemma 4 . 5 .O 3 /2 n 3 L 6</head><label>45336</label><figDesc>Assume all iterates stay inside the region B(W (0) , ⌧), where ⌧ = log 3/2 (m) . Then under Assumptions 3.1 and 3.2, if set the step size ⌘ = O k/(L 2 m) , with probability least 1 exp O(m⌧ 3/2 L) , it holds that</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Over-parameterization conditions and iteration complexities of GD for training overparamterized neural networks. K (L) is the Gram matrix for L-hidden-layer neural network<ref type="bibr" target="#b8">[9]</ref>. Note that the dimension of the output is k = 1 in Du et al.<ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b8">9]</ref>, Wu et al.<ref type="bibr" target="#b18">[19]</ref>, Oymak and Soltanolkotabi<ref type="bibr" target="#b16">[17]</ref>.</figDesc><table><row><cell>Du et al. [11] Wu et al. [19]</cell><cell>Over-para. condition Iteration complexity Deep? ReLU? ⌦ ⇣ n 6 4 0 ⌘ O ⇣ ⌘ n 2 log(1/✏) 2 0 no yes ⌦ ⇣ n 6 4 0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Algorithm 1 (Stochastic) Gradient descent with Gaussian random initialization 1: input: Training data {x i , y i } i2[n] , step size ⌘, total number of iterations T , minibatch size B.</figDesc><table><row><cell>(0)</cell></row><row><cell>l</cell></row></table><note>y s , where B (t) with |B (t) | = B denotes the minibatch of training examples at the t-th iteration. Both algorithms are initialized in the same 2: initialization: For all l 2 [L], each row of weight matrix W</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Remark 3.4. The state-of-the-art results for training deep ReLU network are provided by Allen-Zhu et al.<ref type="bibr" target="#b1">[2]</ref>, where the authors showed that GD can achieve ✏-training loss within O n 6 L 2 log(1/✏)/ 2 iterations if the neural network width satisfies m = e ⌦ kn 24 L 12 / 8 . As a clear comparison, our result on the iteration complexity is better than theirs by a factor of O(n 4 / ), and our overparameterization condition is milder than theirs by a factor of e ⌦(n 16 / 4 ). Du et al.<ref type="bibr" target="#b8">[9]</ref> also proved the global convergence of GD for training deep neural network with smooth activation functions. As shown in Table</figDesc><table><row><cell cols="4">Theorem 3.3. Under Assumptions 3.1 and 3.2, and suppose the number of hidden nodes per layer</cell></row><row><cell>satisfies</cell><cell></cell><cell></cell><cell></cell></row><row><cell>m = ⌦ kn 8</cell><cell>L 12 log 3 (m)/ 4</cell><cell>.</cell><cell>(3.1)</cell></row></table><note>Then if set the step size ⌘ = O k/(L 2 m) , with probability at least 1 O(n 1 ), gradient descent is able to find a point that achieves ✏ training loss withinT = O n 2 L 2 log(1/✏)/ iterations.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="33" xml:id="foot_0">33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">Here e ⌦(•) hides constants and the logarithmic dependencies on problem dependent parameters except ✏.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank the anonymous reviewers and area chair for their helpful comments. This research was sponsored in part by the National Science Foundation CAREER Award IIS-1906169, BIGDATA IIS-1855099, and Salesforce Deep Learning Research Award. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning and generalization in overparameterized neural networks, going beyond two layers</title>
		<author>
			<persName><forename type="first">Allen-Zhu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1811.04918</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A convergence theory for deep learning via over-parameterization</title>
		<author>
			<persName><forename type="first">Allen-Zhu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03962</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">On the convergence rate of training recurrent neural networks</title>
		<author>
			<persName><forename type="first">Allen-Zhu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12065</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08584</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Globally optimal gradient descent for a convnet with gaussian inputs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brutzkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A generalization theory of gradient descent for learning overparameterized deep relu networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01384</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A note on lazy training in supervised differentiable programming</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chizat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07956</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">On the power of over-parametrization in neural networks with quadratic activation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01206</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03804</idno>
		<title level="m">Gradient descent finds global minima of deep neural networks</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">When is a convolutional filter easy to learn?</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06129</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02054</idno>
		<title level="m">Gradient descent provably optimizes over-parameterized neural networks</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning one-hidden-layer neural networks under general input distributions</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makkuva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Viswanath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural tangent kernel: Convergence and generalization in neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hongler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning overparameterized neural networks via stochastic gradient descent on structured data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01204</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Convergence analysis of two-layer neural networks with ReLU activation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09886</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Towards moderate overparameterization: global convergence guarantees for training shallow neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Oymak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04674</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">An analytical formula of population gradient for two-layered ReLU network and its applications in convergence and critical point analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00560</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ward</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07111</idno>
		<title level="m">Global convergence of adaptive gradient methods for an over-parameterized neural network</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Training over-parameterized deep resnet is almost as easy as training a two-layer network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.07120</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning one-hidden-layer ReLU networks via gradient descent</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07808</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03175</idno>
		<title level="m">Recovery guarantees for one-hidden-layer neural networks</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08888</idno>
		<title level="m">Stochastic gradient descent optimizes over-parameterized deep relu networks</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
