<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pretraining with Artificial Language: Studying Transferable Knowledge in Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ryokan</forename><surname>Ri</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<addrLine>7-3-1 Hongo, Bunkyo-ku</addrLine>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
							<email>tsuruoka@logos.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<addrLine>7-3-1 Hongo, Bunkyo-ku</addrLine>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pretraining with Artificial Language: Studying Transferable Knowledge in Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We investigate what kind of structural knowledge learned in neural network encoders is transferable to processing natural language. We design artificial languages with structural properties that mimic natural language, pretrain encoders on the data, and see how much performance the encoder exhibits on downstream tasks in natural language. Our experimental results show that pretraining with an artificial language with a nesting dependency structure provides some knowledge transferable to natural language. A follow-up probing analysis indicates that its success in the transfer is related to the amount of encoded contextual information and what is transferred is the knowledge of position-aware context dependence of language. Our results provide insights into how neural network encoders process human languages and the source of cross-lingual transferability of recent multilingual language models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pretrained language models <ref type="bibr" target="#b10">(Devlin et al., 2019;</ref><ref type="bibr" target="#b44">Yang et al., 2019;</ref><ref type="bibr" target="#b33">Raffel et al., 2020)</ref> have demonstrated strong empirical performance not only within a language but also across languages. Language models pretrained with a mix of monolingual corpora, such as multilingual BERT, exhibit a decent zero-shot cross-lingual transfer capability, i.e., a model fine-tuned in a single source language (L1) can solve the task in another language (L2) <ref type="bibr" target="#b7">(Conneau et al., 2020a;</ref><ref type="bibr">Xue et al., 2021)</ref>. Surprisingly, the transfer happens without lexical overlaps between L1 and L2 <ref type="bibr" target="#b16">(Karthikeyan K and Roth, 2020;</ref><ref type="bibr" target="#b9">Conneau et al., 2020b)</ref> or even without joint pretraining <ref type="bibr" target="#b1">(Artetxe et al., 2020)</ref>: an encoder only pretrained on L1 can be transferred to L2 without any parameter updates. These results suggest that, whether the encoder is trained on single or multiple languages, it learns some transferable knowledge about language.</p><p>Figure <ref type="figure">1</ref>: Transfer from artificial language to natural language. The artificial language encodes some structural properties (e.g., token distributions, dependency structures) and we study how the learning of such properties can be transferred to natural language.</p><p>However, the characteristics of such transferable knowledge are still underexplored. Recent studies with the probing methodology <ref type="bibr" target="#b15">(Hupkes and Zuidema, 2018;</ref><ref type="bibr" target="#b8">Conneau et al., 2018)</ref> have revealed that multilingual BERT captures languageindependent linguistic structures such as universal dependency relations <ref type="bibr" target="#b3">(Chi et al., 2020)</ref> and subjecthood <ref type="bibr" target="#b29">(Papadimitriou et al., 2021)</ref>, but it remains unknown whether learning such linguistic properties actually contributes to the performance, and whether there exists more abstract knowledge transferred across languages.</p><p>In this study, we try to shed light on these questions with the framework of the Test for Inductive Bias via Language Model Transfer <ref type="bibr" target="#b30">(Papadimitriou and Jurafsky, 2020)</ref>, focusing on designing artificial languages with natural-language-like structural properties (Figure <ref type="figure">1</ref>). We pretrain encoders with artificial languages and transfer the encoders to natural language tasks with their parameters frozen. This enables us to see how learning the specific structural properties of the artificial language affects the downstream performance.</p><p>Specifically, we explore whether it is beneficial for the encoder to know the following two characteristics of natural language: word distributions and latent dependency structures. We design artificial languages that represent such characteristics and perform an extensive study with different encoder architectures (LSTM and Transformer) pretraining objectives (causal and masked language modelings).</p><p>The contribution is summarized as follows:</p><p>• We first start by complementing the study in <ref type="bibr" target="#b30">Papadimitriou and Jurafsky (2020)</ref>. We train LSTM and Transformer encoders with the sentence-level causal language modeling task and evaluate the encoders in English. We show that an artificial language that models simple statistical dependency within a sentence provides decent transferable knowledge on natural language modeling. Furthermore, we find that the inductive bias of a nesting head-to-tail dependency structure is more useful than a flat one.</p><p>• We then proceed to investigate transfer learning in masked language modeling <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>, one of the current dominant pretraining paradigms. We evaluate pretrained Transformer encoders with dependency parsing and confirm that the nesting dependency structure is important to learn the structure of natural language.</p><p>• We hypothesize that the transfer performance of pretrained encoders is related to the way the encoder preserves the input contextual information in the output vectors. We perform a probing experiment and find that the artificial language with the nesting dependency structure trains encoders to encode the information on adjacent tokens into the output vector of each token. We conclude this paper with the hypothesis that a part of transferable knowledge in language models could be explained by the knowledge of position-aware context dependence of language.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Transferable Structural Knowledge in Pretrained Encoders</head><p>Multilingual language models trained with masked language modeling objective <ref type="bibr" target="#b10">(Devlin et al., 2019;</ref><ref type="bibr" target="#b11">Doddapaneni et al., 2021)</ref> have demonstrated a surprisingly strong cross-lingual transfer capability <ref type="bibr" target="#b22">(Liu et al., 2020)</ref>, given the model is only trained with a mix of monolingual corpora. This leads to several studies investigating the source of the cross-lingual capability of multilingual models. An early common hypothesis was that the models take advantage of a common word-piece vocabulary across languages <ref type="bibr" target="#b42">(Wu and Dredze, 2019;</ref><ref type="bibr" target="#b31">Pires et al., 2019)</ref>, which provides cross-lingual alignment signals to learn useful multilingual representations. However, this hypothesis has been questioned by recent studies <ref type="bibr" target="#b16">(Karthikeyan K and Roth, 2020;</ref><ref type="bibr" target="#b9">Conneau et al., 2020b)</ref> which show that shared word-pieces only play a minor role in the performance. These studies suggest that the model can exploit abstract structures of languages to learn shared multilingual representations.</p><p>Another line of research suggests that the learning of transferable knowledge happens even in monolingual pretraining. <ref type="bibr" target="#b1">Artetxe et al. (2020)</ref> showed that a Transformer encoder pretrained only on L1 exhibits strong cross-lingual transfer performance simply by aligning the L2 embeddings to the encoder. <ref type="bibr" target="#b30">Papadimitriou and Jurafsky (2020)</ref> pretrained LSTM encoders with natural languages and non-linguistic data (e.g., code, music, and artificial data) to demonstrate that the encoders achieve reasonable performance in Spanish language modeling. These studies provide additional evidence for the existence of transferable linguistic knowledge learned in the model.</p><p>Then what is such knowledge? Probing studies <ref type="bibr" target="#b15">(Hupkes and Zuidema, 2018;</ref><ref type="bibr" target="#b8">Conneau et al., 2018)</ref> have revealed that the model captures languageindependent structures such as universal dependency relations <ref type="bibr" target="#b3">(Chi et al., 2020)</ref> and subjecthood <ref type="bibr" target="#b29">(Papadimitriou et al., 2021)</ref>. However, the probing methodology does not answer whether such linguistic knowledge contributes to the performance in cross-lingual transfer.</p><p>In this study, we shed light on this question by studying transfer learning from artificial language with the Test for Inductive Bias via Language Model Transfer (TILT) <ref type="bibr" target="#b30">(Papadimitriou and Jurafsky, 2020)</ref>. This framework enables us to assess if abstract features generalizable to L2 (natural language) are encoded in L1. Here we explicitly design artificial languages with some structural properties as L1 to investigate their transferability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Studying Language Models with Artificial Language</head><p>To study the behavior of language models, several studies have employed a specific type of artificial language: artificial variants of natural languages. A typical experimental framework is as follows:</p><p>(1) create an artificial language that differs from a natural language in one linguistic property, such as word orders <ref type="bibr" target="#b37">(Sinha et al., 2021b;</ref><ref type="bibr" target="#b13">Dufter and Schütze, 2020;</ref><ref type="bibr" target="#b36">Sinha et al., 2021a)</ref>, scripts <ref type="bibr" target="#b16">(Karthikeyan K and Roth, 2020;</ref><ref type="bibr" target="#b13">Dufter and Schütze, 2020;</ref><ref type="bibr" target="#b9">Conneau et al., 2020b)</ref>, or morphology <ref type="bibr" target="#b34">(Ravfogel et al., 2019)</ref>; (2) train or evaluate the natural/artificial language models and compare the performance to analyze the model's sensitivity to the linguistic property. However, this methodology is limited to studying linguistic properties that are easily editable to create artificial variants and also offers limited control over the experiments. To overcome this problem, <ref type="bibr" target="#b41">White and Cotterell (2021)</ref> created artificial languages by defining their own probabilistic context-free grammars (PCFG). As the concurrent work, Chiang and yi <ref type="bibr" target="#b4">Lee (2022)</ref> trained Transformer encoders on artificial data with token dependencies in the sequences and showed that they perform reasonably well on the GLUE benchmark <ref type="bibr" target="#b40">(Wang et al., 2019)</ref>. In this research, we design artificial languages with certain structural properties from scratch to study knowledge transferable to natural language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Framework</head><p>We first describe the experimental framework used throughout this paper, the Test for Inductive Bias via Language Model Transfer (TILT) introduced by <ref type="bibr" target="#b30">Papadimitriou and Jurafsky (2020)</ref>. TILT consists of pretraining and transfer steps:</p><p>1. Pretrain an encoder with a pretraining task in the source language (L1). We explore pretraining with causal language modeling in §4 and masked language modeling in §5.</p><p>2. Transfer the encoder to the target language (L2) in a downstream task. As we are inter-ested in structural prior knowledge learned in the encoder, we discard the learned L1 word embeddings and initialize the embedding layer with the L2 vocabulary. We then train the model with the encoder parameters frozen and evaluate the task performance.</p><p>TILT reveals how transferrable the computation induced to solve the L1 pretraining task is to processing L2. In this study, we are interested in the transferability of certain types of structures to natural language, and thus we primarily use handdesigned artificial languages with the structural properties as L1 and natural language as L2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Designing Artificial Languages</head><p>Artificial languages are designed to mimic a certain property of natural language. After providing a formal definition of artificial language, we introduce several languages used in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Formulation of Artificial Language</head><p>A artificial language refers to a set of a vocabulary and algorithms to generate sequential data for pretraining. Each language has a sentence-length distribution p len (l), token vocabulary {w|w ∈ V}, and sentence-sampling function f (l) : l → V l . The training data is generated sentence by sentence as follows: we first sample a sentence length (l ∼ p len (l)) and then sample a sequence of tokens of that length ([w 1 , ..., w l ] ∼ f (l)).</p><p>In this study, the token vocabulary V simply consists of integers (or integers with a special symbol) and is not intended to correspond to a vocabulary of any natural language. Also the sentence-length distribution p len (l) is fitted with a baseline dataset in each experiment. The focus is how to design the sentence-sampling function f (l). This determines what kind of characteristics we want to encode in the artificial dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Modeling Word Distribution</head><p>Words in natural language are distributed in nontrivial fashions. We will study whether prior knowledge of token distribution facilitates learning from natural language. We first present the simplest artificial language that serves as a baseline. Uniform language samples each token in a sentence independently and uniformly. Specifically, the probability of a token w being sampled is</p><formula xml:id="formula_0">p(w) = 1 |V| .<label>(1)</label></formula><p>However, this deviates from the token distribution of natural language. Natural language is empirically known to follow the Zipf's law <ref type="bibr" target="#b46">(Zipf, 1949)</ref>, i.e., the relation between the frequency of a word and its rank is given by frequency(w) ∝ rank(w) −α . The coefficient α is typically around 1, although the coefficient shows some variation according to the corpus domain (Zanette and <ref type="bibr" target="#b45">Montemurro, 2005)</ref>. Zipf language captures this property and samples each token w from the following probability distribution assuming α = 1:</p><formula xml:id="formula_1">p(w) ∝ 1 rank(w)</formula><p>.</p><p>(2)</p><p>The two languages introduced so far generate tokens in a sentence independently. However, words within a sentence of natural language are known to have statistical dependencies, i.e., specific cooccurrence patterns <ref type="bibr" target="#b6">(Church and Hanks, 1989)</ref>. Consider the sentence "The cat and dog are fighting over food." The words the and cat would cooccur much more often than by chance because cat (noun) is dependent on the (determinant); so would dog and cat because they are topically related. The words in a sentence are usually coherent according to some syntactic and semantic dependencies. Log-linear language is designed to capture this property. Inspired by the log-linear model in <ref type="bibr" target="#b0">Arora et al. (2016)</ref>, tokens in a sentence s are drawn from the following probability distribution:</p><formula xml:id="formula_2">p(w|s) ∝ exp(⃗ c s • ⃗ v w ),<label>(3)</label></formula><p>where ⃗ c s is the discourse vector of the sentence and ⃗ v w is the word vector of the token w. Intuitively, we can imagine that the discourse vector represents the topic of the sentence and determines the unigram distribution over the vocabulary <ref type="bibr" target="#b2">(Blei et al., 2003)</ref>. Sampling tokens this way, non-trivial cooccurrence patterns within sentences emerge in the language. We speculate that pretraining with the Log-linear language will endow the model with an inductive bias to aggregate the context in a sentence to predict the identity or property of tokens, which is likely to benefit natural language processing.</p><p>In the experiments, the word vectors ⃗ v w are initialized with the normal distribution, and the discourse vector ⃗ c s is also drawn from the normal distribution each time we generate a sentence. We set the dimension of the word and discourse vector to 10 as we empirically find that this makes the entire token distribution close to the Zipfian distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Modeling Latent Dependency Structure</head><p>Sentences in natural language are known to have latent structures, which are often described in the form of trees <ref type="bibr" target="#b5">(Chomsky, 1957)</ref> or dependency graphs <ref type="bibr" target="#b26">(Mel'čuk, 1988)</ref>. Now we consider how to endow the sampled tokens with such structures.</p><p>In this study, we adopt a dependency-based latent structure. Words in sentences of natural language often have dependency relations and the existence of a certain word can be predictive of another word (e.g., the verb am always cooccurs with I). We hypothesize that, pretrained on such data, language models may acquire inductive bias towards finding relations between tokens in the input, which is presumably important in processing natural language.</p><p>Inspired by Papadimitriou and Jurafsky (2020), we design algorithms that generate structured sentences given a set of tokens sampled with any of the strategies described in §3.2.2. The general idea is that half of the tokens (heads) in the vocabulary are all paired with another half of tokens (tails). A pair of head and tail can be represented in right and left brackets with the same integer (e.g., "&lt;123", "123&gt;"). The pairs always appear together in a sentence and express simple dependency relations. After determining the sentence length l ∼ f (l), we first sample l 2 (rounded to an integer) pairs of head and tail and then arrange them with one of the following structures. Flat Dependency structure simply arranges the tokens randomly while keeping the right order of the brackets (e.g., ["&lt;5", "&lt;84", "5&gt;", "&lt;123", "123&gt;", "84&gt;"]). The dependency arcs are allowed to be crossed and thus often result in a nonprojective dependency structure. Nesting Dependency language, by contrast, does not allow any dependency arcs to be crossed, and the brackets are nested hierarchically (e.g., ["&lt;5", "&lt;84", "84&gt;", "5&gt;", "&lt;123", "123&gt;"]). The sentences are generated from the stack-based algorithm described in Appendix A.</p><p>These structures are similar to the Parenthesis languages used to study the inductive bias of language models in Papadimitriou and Jurafsky (2020). However, our Dependency languages differ from them in how to represent the head and tail tokens. In the Parenthesis language, the head and tail are represented with the same token (e.g., ["5", "84", "84", "5", "123", "123"]), which we argue deviates from the dependency structure in natural language, because in natural language, dependency relations usually hold between different words (e.g., I and am). We will show that this difference is in fact crucial and draw a different conclusion from <ref type="bibr" target="#b30">Papadimitriou and Jurafsky (2020)</ref> on the importance of the nested structure ( §4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Causal Language Model Pretraining with Artificial Language</head><p>In this section, we complement the study of Papadimitriou and Jurafsky (2020). While they studied the inductive bias learned in LSTM encoders with some artificial languages, here we provide additional studies with the newly introduced Loglinear and Dependency artificial languages, and the Transformer encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setups</head><p>Task. We study sentence-level causal (left-to-right) language modeling (CLM), where the model needs to predict the next word given the previous context in the sentence. Note that, Papadimitriou and Jurafsky (2020) experiment with language modeling across sentences, but we adopt sentence-level modeling because we would like to focus on the learning of sentence structures here. As we will see in §4.2, we observe the same tendency in regard to the effect of artificial pretraining where we share the setups. The task performance is measured by the average perplexity scores for each token.</p><p>Model. We study two encoder architectures: LSTM <ref type="bibr" target="#b14">(Hochreiter and Schmidhuber, 1997)</ref> and Transformer <ref type="bibr" target="#b39">(Vaswani et al., 2017)</ref>. These architectures are known to exhibit different abilities in capturing the underlying hierarchical structure of sequential data <ref type="bibr" target="#b38">(Tran et al., 2018)</ref>. The size of word embeddings is set to 300. For both LSTM and Transformer encoders, the number of layers is set to 3, and the number of parameters is configured to be the same (6.9M parameters) to enable a fair comparison between architectures (for further details, see Appendix B). Pretraining Data. We generate artificial corpora with three unstructured languages, which randomly arrange the tokens sampled from Uniform, Zipf, and Log-linear languages, and four structured languages which combine the Zipf sampling strategy with the structures of Flat Parenthesis, Nesting Parenthesis, Flat Dependency, and Nesting Dependency.</p><p>We also experiment with natural language corpora. We create training corpora from Wikipedia dumps of English, Japanese, and Spanish. The sentences are tokenized with the Moses tokenizer<ref type="foot" target="#foot_0">1</ref> for English and Spanish and MeCab<ref type="foot" target="#foot_1">2</ref> for Japanese.</p><p>The sentence lengths of artificial data were sampled from the empirical distribution of the English Wikipedia corpus. The size of the vocabulary |V | is set to 32,000 for both artificial and natural corpora, and out-of-vocabulary words in natural language are replaced with the OOV token. For each corpus, we sample 12.8 M sentences and train the model with one iteration over the corpus. Evaluation Data. We evaluate the pretrained encoders on the Penn Treebank (PTB) corpus <ref type="bibr" target="#b25">(Marcus et al., 1993)</ref> with preprocessing from <ref type="bibr" target="#b27">Mikolov et al. (2010)</ref>. Note that, when we train language models with the pretrained encoders, the parameters of the encoder are not updated and only the English word embeddings are learned from scratch (optimization details in Appendix B.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>We provide two baseline models trained on the L2 training corpus from scratch and trained with frozen random weights in the encoder to compare with pretrained encoders. For each configuration, we pretrain three encoders with different random seeds, and for each encoder fine-tuned three models, which results in nine models in total. We summarize the average scores and standard deviations in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>The Transformer encoder is more flexible than LSTM. We start by discussing overall trends. We observe that the Transformer encoders give lower perplexity scores compared to LSTM regardless of pretraining language. This tendency is in line with the observations on the surprisingly good transferability or pretrained Transformer encoders to other languages <ref type="bibr" target="#b7">(Conneau et al., 2020a)</ref>, or even other modalities <ref type="bibr" target="#b24">(Lu et al., 2021;</ref><ref type="bibr">Reid et al., 2022)</ref>. We think that this is because Transformer encoders are better at aggregating and preserving the context information at each time step, as we will see in §6, presumably because the Transformer architecture has self-attention and residual connections.  Natural languages are better than the artificial languages. As expected, pretraining with natural languages (English, Spanish and Japanese) provides better encoders for language modeling than the artificial languages both with LSTM and Transformer. However, the performance differences between natural languages seem to be negligible, indicating that there is not much difference in the way the encoders process these different languages, conforming with the observation of cross-lingual transferability of pretrained encoders <ref type="bibr" target="#b1">(Artetxe et al., 2020)</ref>.</p><p>The Uniform and Zipf languages degrade the encoders. Looking at the difference among unstructured languages (Figure <ref type="figure" target="#fig_1">2a</ref>), Uniform and Zipf languages give higher perplexities than the Random weights baseline particularly with LSTM. In hindsight, it is natural that encoders would be degraded even from random weights when trained with sequences where tokens are drawn independently from each other because the encoders are not incentivized to use contextual information and will even learn to discard the input information. We will demonstrate this with a follow-up probing experiment in §6.</p><p>The Log-linear language provides a useful inductive bias to language modeling. On the contrary, the Log-linear language gives reasonably lower perplexities compared to Random weights (Figure <ref type="figure" target="#fig_1">2a</ref>). This indicates that knowing the existence of statistical dependency within a sentence, or learning to predict tokens from the cooccurrence information, is a useful inductive bias even though the cooccurrence statistics is not necessarily in line with L2.</p><p>We do not observe the importance of the nested structure in the Parenthesis languages. <ref type="bibr" target="#b30">Papadimitriou and Jurafsky (2020)</ref> showed that LSTM encoders trained on the Flat Parenthesis and Nesting Parenthesis structures do not provide a significant difference in perplexity, and concluded that simple non-hierarchical head-dependent-type relations are important in LSTM language processing. A similar observation can be made in Figure <ref type="figure" target="#fig_1">2b</ref>: although the Nesting Parenthesis exhibits the lower average score, there is no significant difference between Flat Parenthesis and Nesting Parenthesis (232.9 ± 30.0 vs. 203.8 ± 7.7, p &gt; 0.01 in Welch's t-test) with the unstable results of Flat Parenthesis. Also, the trend of the average scores is reversed in Transformer: the Nesting Parenthesis exhibits the higher average score (212.4 ± 8.8) than Flat Parenthesis (191.9 ± 11.8), which makes it difficult to draw a consistent conclusion from here.</p><p>However, the Dependency languages suggest that the nested structure is actually important in language modeling. While the Parenthesis language represents dependency relations with two identical tokens (e.g., "4543" and "4543"), our Dependency language represents relations with two different tokens (e.g., "&lt;4543" and "4543&gt;"). We expect that expressing dependency relations with two different tokens is closer to natural language and thus provides more viable insights into natural language. When we compare the scores of the Dependency languages, Nesting Dependency provides the lower and more stable perplexity than Flat Dependency with LSTM (175.7 ± 4.3 vs. 187.2±10.7) and the significantly lower score with Transformer (160.6±1.6 vs. 175.7±4.3, p &gt; 0.01 in Welch's t-test). Overall, Nesting Dependency performs best among other artificial languages, indicating our Dependency language is closer to natural language and the nested structure is useful for language modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Masked Language Model Pretraining with Artificial Language</head><p>We proceed to investigate transfer learning from artificial languages in one of the most successful pretraining paradigms, masked language modeling (MLM) <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> to see if we can observe similar trends to what we see in the CLM experiment ( §4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setups</head><p>Pretraining. To allow for fast experimentation, we train small Transformer encoders. The size of word embeddings is set to 300 and the encoders have three layers (further details in Appendix C). The pretraining datasets are the same as in §4.1. Downstream Task. We evaluate the pretrained encoders with dependency parsing to see if the structural knowledge learned with artificial language is beneficial to predict the structure of natural language. We use the English EWT dataset from Universal Dependencies (UD) v2.8 <ref type="bibr" target="#b28">(Nivre et al., 2020)</ref> 3 . Model. We adopt the biaffine graph-based parser <ref type="bibr" target="#b12">(Dozat and Manning, 2017)</ref> with the Transformer encoder. The input word representations are the concatenation of word embeddings and character features computed by a character-level bidirectional LSTM encoder <ref type="bibr" target="#b20">(Ling et al., 2015)</ref>. For 3 https://universaldependencies.org/ the details on fine-tuning these models, please refer to Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>We provide two baseline models trained from scratch and trained with random encoder weights.</p><p>For each pretraining language, we again train three encoders and fine-tune three models for each, and take the mean and standard deviation of the nine models. Figure <ref type="figure" target="#fig_2">3</ref> shows the results.</p><p>The unstructured languages do not provide useful transferable knowledge for dependency parsing. The Uniform, Zipf, and Log-linear encoders perform comparably to or worse than the Random weights baseline. This is in contrast with the causal language modeling task, where the Loglinear language at least outperforms the Random weights baseline ( §4.2).</p><p>On the other hand, learning from structured languages seems to be important in dependency parsing. The Dependency encoders outperform the Random weights baseline, and also we can observe that learning from the nesting structure is more effective than the flat structure, and Dependency languages outperform Parenthesis languages, as observed in the CLM in §4.</p><p>6 How much contextual information do the pretrained encoders capture?</p><p>In the previous sections, we have seen that the encoders pretrained with different artificial languages exhibit various degrees of transferability to natural language. In this section, we try to explain why pretraining with some artificial languages is better or worse for the transfer to natural language from the perspective of the amount of contextual information in the encoder outputs. The intuition is, for example, if a pretrained encoder has learned to discard the input information, we cannot expect the encoder to perform well when transferred to any tasks. Also, existing studies show that neural language models assign more importance to local context when they make predictions <ref type="bibr" target="#b17">(Khandelwal et al., 2018;</ref><ref type="bibr" target="#b18">Lai et al., 2020)</ref>. Can we observe that encoders pretrained with artificial languages exhibit similar patterns to natural languages regarding how they encode the contextual information?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setups</head><p>We investigate how much contextual information can be extracted from the outputs of the pretrained encoders by setting up a simple probing task. In this task, the encoder is asked to recover the identity of the contextual words given the contextualized vector of a target word.</p><p>Specifically, we first randomly generate 100K sequences of integers with the length of 15 ∼ 25 (close to most frequent sequence lengths in the pretrained corpus) with the vocabulary size 100 and split them into training (90K sequences), validation (5K) and test (5K) sets.</p><p>Then we simultaneously train several linear classifiers, each of which predicts the ID of the context word at a fixed relative position to the target word in the sequence, on top of a frozen pretrained encoder. For the encoders pretrained with CLM in §4, the target word is the last word in sequences and the classifiers predict the words at the positions of [-9, -4, -3, -2, -1, 0]; for the encoders pretrained with MLM in §5, the target word is the middle word and the classifiers predict the words at [-6, -3, -2, -1, 0, 1, 2, 3, 6].</p><p>After training, we measure the accuracy of predicting the words at each position on the test set and interpret this as how much information on each contextual word the encoder preserves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head><p>Figure <ref type="figure" target="#fig_4">4</ref> summarizes the results of the encoders trained in §4 and §5.</p><p>The amount of the encoded contextual information can explain the transfer performance in some obvious cases. In the experiment of CLM (Figure <ref type="figure" target="#fig_1">2a</ref>), we observed that the Uniform and Zipf encoders tend to perform worse even than Random weights. Figure <ref type="figure" target="#fig_4">4a</ref> and 4d demonstrate that their poor performance is because the encoders are trained to discard the input information. The Uniform and Zipf encoders tend to preserve less contextual information even than Random weights because capturing the contextual information does not lead to solving the pretraining task in these languages.</p><p>On the other hand, if words are predictable from the context, encoders are encouraged to learn to preserve the contextual information. The Loglinear encoders trained with CLM encode a decent amount of the contextual information (Figure <ref type="figure" target="#fig_4">4a and 4d</ref>) and also performed best among the unstructured artificial languages in CLM (Figure <ref type="figure" target="#fig_1">2a</ref>). Moreover, encoders trained with natural languages (Figure <ref type="figure" target="#fig_4">4c</ref>, 4f and 4i) capture not only the local context well (at distance 0 ∼ 2) but also a modest amount of the farther context (at distance 3 ∼), which is consistent with the existing observation that LSTM encoders trained with natural language are better at memorizing the inputs than ones trained with randomly sampled data <ref type="bibr" target="#b21">(Liu et al., 2018)</ref>. In these cases, the downstream performance and the amount of the encoded contextual information seem to be correlated.</p><p>However, this trend is not as clear when comparing the structured artificial languages. For example, the Nesting Dependency encoders perform the best for the downstream tasks among the structured artificial languages but do not necessarily in the probing task (Figure <ref type="figure" target="#fig_4">4b and 4e</ref>).</p><p>The nesting structure seems to facilitate encoders to remember the local context with MLM. The difference between the Nesting and Flat languages is striking in Figure <ref type="figure" target="#fig_4">4f</ref>. The Nesting encoders are consistently better at capturing the local contextual information (at positions −2 ∼ 2) than their flat counterparts, which may explain the better performance of the Nesting encoders in dependency parsing (Figure <ref type="figure" target="#fig_2">3</ref>), given that the local contextual information is particularly important to predict the syntactic characteristics of words <ref type="bibr" target="#b19">(Levy and Goldberg, 2014;</ref><ref type="bibr" target="#b35">Ri and Tsuruoka, 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion and Future Work</head><p>In this paper, we studied what kind of structural properties in pretraining data is useful to train encoders for natural language tasks. We have found  that to achieve decent results, L1 needs at least statistical dependency in a sentence ( §4), and having the head-to-tail dependency with the nesting structure is further beneficial ( §4 and §5). The probing experiment in §6 suggests that the encoders trained with languages with the above characteristics are good at capturing the positions and identities of the context words.</p><p>From these observations, we suggest a tentative answer to the initial research question: what knowledge in pretrained encoders are transferred across different languages? That is position-aware context dependence of language, in other words, "tokens in a sequence can be characterized by its neighbor tokens at specific positions".</p><p>We think that it can explain the success of transferring the encoder across languages to some extent. To solve natural language tasks, it is often useful to characterize words in a sentence by the words around them. For example, to understand the semantics of a sentence, it would be useful to look for the subject by looking for a noun that precedes the word is; to parse a sentence, a word can be identified as a noun because it follows the article the. If the encoder computes the output representation of a word in a sentence by aggregating the information from its surrounding words, that should be a useful inductive bias to solve most NLP tasks in any language. Also, it is easy to imagine that the knowledge of position-aware context dependence gives a reasonable prior for solving sequence modeling problems in other domains, which may explain the success of cross-modality transfer of language models <ref type="bibr" target="#b24">(Lu et al., 2021;</ref><ref type="bibr">Reid et al., 2022)</ref>.</p><p>Of course, we do not expect that the knowledge of position-aware context dependence explains every aspect of the success of cross-lingual transfer. As future work, we need further investigation for a more fine-grained view of the transferred knowledge. Important questions include how much the model size affects the transferability of the encoder or if there is any difference in the knowledge transferred among different downstream tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Comparison of token distributions. (b) Comparison of dependency structures. (c) Comparison of natural languages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The perplexity scores (the lower the better) on the sentence-level causal language modeling task with the English Penn Treebank dataset. The two baselines (From scratch and Random weights) are not pretrained, and the others are the results of pretrained encoders.</figDesc><graphic url="image-3.png" coords="6,77.11,210.83,246.04,119.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The downstream performance on two syntactic tasks with the English EWT dataset. The two baselines (From scratch and Random weights) are not pretrained, and the others are the results of encoders pretrained with masked language modeling.</figDesc><graphic url="image-5.png" coords="7,316.07,70.85,198.43,168.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The accuracy of the task of recovering the contextual words from the encoder output of target words.</figDesc><graphic url="image-13.png" coords="9,223.94,299.16,147.41,110.91" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/moses-smt/ mosesdecoder</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">http://taku910.github.io/mecab/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank the anonymous reviewers for their insightful comments and constructive suggestions to improve the paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Appendix for "Pretraining with Artificial Language: Studying Transferable Knowledge in Language Models" A Generating the Nesting Structure</p><p>In the Nesting languages introduced in §3.2.3, tokens are ordered in a way that any dependency arcs in a sequence are not crossed. This is realized by the stack-based algorithm in Algorithm 1. We set the probability of closing a dependency pair to 0.4 following <ref type="bibr" target="#b30">Papadimitriou and Jurafsky (2020)</ref>. For the experiment with causal language modeling ( §4), we set the number of layers of the LSTM and Transformer encoders to 3 and configure them so that they have the same number of parameters (2.1 M parameters without the embedding and output projection layers). The details of configuration are shown in Table <ref type="table">1</ref> and Table <ref type="table">2</ref>. The weights of the output projection layer are tied with the word embedding layer <ref type="bibr">(Press and Wolf, 2017)</ref>. Note that, to enable this, the LSTM encoder has an additional linear layer to project the hidden vector (294 dim) to the input size (300 dim), which the Transformer encoder does not have.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># of layers</head><p>3 input size 300 hidden size 294 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Optimization</head><p>We optimize the pretrained models for 10k steps with 12.8 M sentences and the batch size of 128 using AdamW <ref type="bibr" target="#b23">(Loshchilov and Hutter, 2019)</ref>. We use the the Noam Learning rate scheduler described in <ref type="bibr" target="#b39">Vaswani et al. (2017)</ref> with the warmup steps of 4000, and the other hyper-parameter details are shown in Table <ref type="table">3</ref>. We use the same hyperparameters for fine-tuning with the L2 language. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Details of Masked Language Modeling Task C.1 Model configuration</head><p>For the experiment with masked language modeling ( §5), we set the number of layers of the Transformer encoders to 3. The details of configuration are shown in Table <ref type="table">4</ref> (2.1 M parameters without the embedding and output projection layers). The hyper-parameters for the masked language modeling task is shown in Mask probability for words 15% Random-word probability for words 10% Unmasked probability for words 10% </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Computing Infrastructure</head><p>We ran the experiments on a server with a Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz CPU and 10 NVIDIA TITAN Xp GPUs. Each pretraining and finetuning were run with a single GPU.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Latent Variable Model Approach to PMI-based Word Embeddings</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Risteski</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00106</idno>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="385" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the Cross-lingual Transferability of Monolingual Representations</title>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.421</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Finding Universal Grammatical Relations in Multilingual BERT</title>
		<author>
			<persName><forename type="first">Ethan</forename><forename type="middle">A</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.493</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the</title>
				<meeting>the 58th Annual Meeting of the</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the Transferability of Pre-trained Language Models: A Study from Artificial Datasets</title>
		<author>
			<persName><forename type="first">Cheng-</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Sixth AAAI Conference on Artificial Intelligence</title>
				<meeting>the Thirty-Sixth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Syntactic Structures</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Chomsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957">1957</date>
			<publisher>De Gruyter Mouton</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Word Association Norms, Mutual Information, and Lexicography</title>
		<author>
			<persName><forename type="first">Ward</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName><surname>Hanks</surname></persName>
		</author>
		<idno type="DOI">10.3115/981623.981633</idno>
	</analytic>
	<monogr>
		<title level="m">27th Annual Meeting of the Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised Cross-lingual Representation Learning at Scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">What you can cram into a single $&amp;!#* vector: Probing sentence embeddings for linguistic properties</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">German</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1198</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Emerging Cross-lingual Structure in Pretrained Language Models</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.536</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
				<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A Primer on Pretrained Multilingual Language Models</title>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Doddapaneni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gowtham</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Kunchukuttan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratyush</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitesh</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
		</author>
		<idno>ArXiv, abs/2107.00676</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep Biaffine Attention for Neural Dependency Parsing</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Identifying Elements Essential for BERT&apos;s Multilinguality</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Dufter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.358</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visualisation and &apos;Diagnostic Classifiers&apos; Reveal how Recurrent and Recursive Neural Networks Process Hierarchical Structure (Extended Abstract)</title>
		<author>
			<persName><forename type="first">Dieuwke</forename><surname>Hupkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2018/796</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence</title>
				<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cross-Lingual Ability of Multilingual BERT: An Empirical Study</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Mayhew Karthikeyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context</title>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1027</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Context Analysis for Pre-trained Masked Language Models</title>
		<author>
			<persName><forename type="first">Yi-An</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garima</forename><surname>Lalwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.338</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dependency-Based Word Embeddings</title>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-2050</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation</title>
		<author>
			<persName><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramón</forename><surname>Fermandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luís</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Luís</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1176</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">LSTMs Exploit Linguistic Attributes of Data</title>
		<author>
			<persName><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-3024</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Third Workshop on Representation Learning for NLP</title>
				<meeting>The Third Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multilingual Denoising Pre-training for Neural Machine Translation</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00343</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="726" to="742" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Pretrained Transformers as Universal Computation Engines</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<idno>ArXiv, abs/2103.05247</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Building a Large Annotated Corpus of English: The Penn Treebank</title>
		<author>
			<persName><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Dependency Syntax: Theory and Practice</title>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">'</forename><surname>Čuk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>State University Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukás</forename><surname>Burget</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2010-343</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Conference of the International Speech Communication Association</title>
				<meeting>Annual Conference of the International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2010-01">Jan Honza Cernocký, and Sanjeev Khudanpur. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Universal Dependencies v2: An Evergrowing Multilingual Treebank Collection</title>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Tyers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference</title>
				<meeting>the 12th Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2020-01">Jan Hajič,. 2020</date>
		</imprint>
	</monogr>
	<note>Filip Ginter</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep Subjecthood: Higher-Order Grammatical Features in Multilingual BERT</title>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><forename type="middle">A</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Futrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mahowald</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.215</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter</title>
				<meeting>the 16th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Main Volume</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning Music Helps You Read: Using Transfer to Study Linguistic Structure in Language Models</title>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.554</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">How Multilingual is Multilingual BERT?</title>
		<author>
			<persName><forename type="first">Telmo</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><surname>Schlinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1493</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Using the output embedding to improve language models</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
				<meeting>the 15th Conference of the European Chapter<address><addrLine>Short Papers</addrLine></address></meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Studying the Inductive Biases of RNNs with Synthetic Variations of Natural Languages</title>
		<author>
			<persName><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1356</idno>
		<idno>ArXiv, abs/2201.12122</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<editor>
			<persName><forename type="first">Machel</forename><surname>Reid</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yutaro</forename><surname>Yamada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shixiang</forename><forename type="middle">Shane</forename><surname>Gu</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Can Wikipedia Help Offline Reinforcement Learning</publisher>
			<date type="published" when="2019">2019. 2022</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Revisiting the Context Window for Cross-lingual Word Embeddings</title>
		<author>
			<persName><forename type="first">Ryokan</forename><surname>Ri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.94</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Masked language modeling and the distributional hypothesis: Order word matters pre-training for little</title>
		<author>
			<persName><forename type="first">Koustuv</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dieuwke</forename><surname>Hupkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.230</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">UnNatural Language Inference</title>
		<author>
			<persName><forename type="first">Koustuv</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasanna</forename><surname>Parthasarathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.569</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
				<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The Importance of Being Recurrent for Modeling Hierarchical Structure</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1503</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Examining the Inductive Bias of Neural Language Models with Artificial Languages</title>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">C</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.38</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
				<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Becas: The Surprising Cross-Lingual Effectiveness of BERT</title>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1077</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Beto, Bentz</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Aditya Barua, and Colin Raffel. 2021. mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer</title>
		<author>
			<persName><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.41</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics</title>
				<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">XLNet: Generalized Autoregressive Pretraining for Language Understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dynamics of Text Generation with Realistic Zipf&apos;s Distribution</title>
		<author>
			<persName><forename type="first">H</forename><surname>Damián</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcelo</forename><forename type="middle">A</forename><surname>Zanette</surname></persName>
		</author>
		<author>
			<persName><surname>Montemurro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Quantitative Linguistics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="29" to="40" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Human behavior and the principle of least effort</title>
		<author>
			<persName><forename type="first">George</forename><surname>Kingsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zipf</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1949">1949</date>
			<publisher>Addison-Wesley Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
