<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EigenGait: Motion-based Recognition of People using Image Self-Similarity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chiraz</forename><surname>Benabdelkaderý</surname></persName>
							<email>chiraz@umiacs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Ý University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ross</forename><surname>Cutlerþ</surname></persName>
							<email>rcutler@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Ý University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Harsh</forename><surname>Nandaý</surname></persName>
							<email>nanda@umiacs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Ý University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Larry</forename><surname>Davisý</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ý University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EigenGait: Motion-based Recognition of People using Image Self-Similarity</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8390104BE4C438D0281F7546C706219C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel technique for motion-based recognition of individual gaits in monocular sequences. Recent work has suggested that the image self-similarity plot of a moving person/object is a projection of its planar dynamics. Hence we expect that these plots encode much information about gait motion patterns, and that they can serve as good discriminants between gaits of different people. We propose a method for gait recognition that uses similarity plots the same way that face images are used in eigenface-based face recognition techniques. Specifically, we first apply Principal Component Analysis (PCA) to a set of training similarity plots, mapping them to a lower dimensional space that contains less unwanted variation and offers better separability of the data. Recognition of a new gait is then done via standard pattern classification of its corresponding similarity plot within this simpler space. We use the k-nearest neighbor rule and the Euclidian distance. We test this method on a data set of 40 sequences of six different walking subjects, at 30 FPS each. We use the leaveone-out cross-validation technique to obtain an unbiased estimate of the recognition rate of 93%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human gait has long been an active subject of study in biomechanics, kinesiology, psychophysics, and physical medicine <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28]</ref>. The reasons and applications for this interest include: detection of gait pathologies, rehabilitation of an injured person, improving athletic performance, and designing ergonomic-based athletic and office equipment. These gait studies typically analyze 3D temporal trajectories of marked points on the body in terms of their frequency and phase. The relationships among the component patterns of the gait, such as phase differences between the trajectories, are also a valuable source of information.</p><p>In machine vision, gait recognition has received growing interest due to its emergent importance as a biometric <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b3">4]</ref>. The term gait recognition is used to signify recognizing individuals by the way they walk in image sequences. Gait detection is the recognition of different types of human locomotion, such as running, limping, hopping, etc. Because human ambulation (gait) is one form of human movement, gait recognition is closely related to vision methods for detection, tracking and recognition of human movement in general (such as actions and gestures).</p><p>Gait recognition research has largely been motivated by Johansson's experiments <ref type="bibr" target="#b20">[21]</ref> and the ability of humans for motion perception from Moving Light Displays (MLDs). In these experiments, human subjects were able to recognize the type of movement of a person solely from observing the 2D motion pattern generated by light bulbs attached to the person. Similar experiments later showed some evidence that the identity of a familiar person ('a friend') <ref type="bibr" target="#b2">[3]</ref>, as well as the gender of the person <ref type="bibr" target="#b10">[11]</ref> might be recognizable from MLDs, though in the latter case a recognition rate of 60% is hardly significantly better than chance (50%).</p><p>Despite the agreement that humans can perceive motion from MLDs, there is still no consensus on how humans interpret this MLD-type stimuli (i.e. how it is they use it to achieve motion recognition). Two main theories exist: The first maintains that people use motion information in the MLDs to recover the 3D structure of the moving object (person), and subsequently use the structure for recognition; and the second theory states that motion information is directly used to recognize a motion, without structure recovery. In machine vision, methods that subscribe to the former theory are known as structure from motion (SFM) <ref type="bibr" target="#b16">[17]</ref>, and those that favor the latter are known as motion-based recognition <ref type="bibr" target="#b7">[8]</ref>.</p><p>Consequently, there exist two main approaches for gait recognition each of which favors one of the two above theories. In SFMbased methods, a set of body points are tracked (as a result of body structure recovery), and their motion trajectories are used to characterize, and thereby recognize the motion or action performed by the body. Note that this approach emulates MLD-based motion perception in humans, since the body part trajectories are in fact identical to MLD-type stimuli. Furthermore, this approach is supported by biomedical gait research <ref type="bibr" target="#b27">[28]</ref> which found that the dynamics of a certain number of body parts/points totally characterize gait. However, because tracking body parts in 3D over a long period of time remains a challenge in vision, the effectiveness of SFM-based methods remains limited.</p><p>Motion-based recognition methods, on the other hand, characterize the motion pattern of the body, without regard to its underlying structure. Two main approaches exist; one which represents human movement as a sequence (i.e. discrete number) of poses/configurations; and another which characterizes the spatiotemporal distribution generated by the motion in its continuum.</p><p>The method we describe in this paper takes a motion-based recognition approach. Our method makes the following assumptions:</p><p>¯People walk with constant speed and direction for about 3-4 seconds.</p><p>¯People walk approximately parallel to the image plane. ¯The camera is sufficiently fast to capture dynamics of motion (we use 30Hz).</p><p>We review vision methods used in detection, tracking and recognition of human movement in general, as they are closely related to gait recognition ( <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b13">14]</ref> are good surveys on this topic). These methods can be divided into two main categories: methods that recover high-level structure of the body and use this structure for motion recognition, and those that directly model how the person moves. We shall describe the latter in more detail as it is more relevant to the gait recognition approach proposed in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Structural Methods</head><p>A 2D or 3D structural model of the human body is assumed, and body pose is recovered by extracting image features and mapping them to the structural components of the model (i.e. body labelling). Hence a human is detected in the image if there exists a labelling that fits the model well enough (based on some measure of goodness of fit) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b32">33]</ref>. Once a person has been detected and tracked in several images, motion recognition is done based on the temporal trajectories of the body parts, typically by mapping them to some low-dimensional feature vector and then applying standard pattern classification techniques <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Structure-free Methods</head><p>To recognize a moving object (or person), these methods characterize its motion pattern, without regard to its underlying structure. They can be further divided into two main classes. The first class of methods consider the human action or gait to be comprised of a sequence of poses of the moving person, and recognize it by recognizing a sequence of static configurations of the body in each pose <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b15">16]</ref>. The second class of methods characterizes the spatiotemporal distribution generated by the motion in its continuum, and hence analyze the spatial and temporal dimensions simultaneously <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b9">10]</ref>. Our method is closely related to the work of <ref type="bibr" target="#b9">[10]</ref>, in that both use similarity plots to characterize human motion, though we use them for gait recognition, and Cutler and Davis use them mainly for human detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>State-space Methods</head><p>These methods represent human movement as a sequence of static configurations. Each configuration is recognized by learning the appearance of the body (as a function of its color/texture, shape or motion flow) in the corresponding pose.</p><p>Murase and Sakai <ref type="bibr" target="#b26">[27]</ref> describe a template matching method which uses the parametric eigenspace representation as applied in face recognition <ref type="bibr" target="#b34">[35]</ref>. Specifically, they use PCA (Principal Component Analysis) to compute a 16-dimensional manifold for all the possible grey-scale images of a walking person. An input sequence of images (after normalization) is hence mapped to a trajectory in this 16-dimensional feature space, and gait recognition is achieved by computing the distance between the trajectories of the input image sequence and a reference sequence.</p><p>Huang et al. <ref type="bibr" target="#b19">[20]</ref> use a similar technique, as they apply PCA to map the binary silhouette of the moving figure to a low dimensional feature space. The gait of an individual person is represented as a cluster (of silhouettes) in this space, and gait recognition is done by determining if all the input silhouettes belong to this cluster.</p><p>He and Debrunner <ref type="bibr" target="#b15">[16]</ref> recognize individual gaits via an HMM that uses the quantized vector of Hu moments of a moving person's silhouette as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatiotemporal Methods</head><p>Here, the action or motion is characterized via the entire 3D spatiotemporal (XYT) data volume spanned by the moving person in the image. It could for example consist of the sequence of grey-scale images, optical flow images, or binary silhouettes of the person. This volume is hence treated as a 'large' vector, and motion recognition is typically done by mapping this vector to a low-dimensional feature vector, and applying standard pattern classification technique in this space. The following methods describe different ways of doing this.</p><p>Of particular interest is the recent work by Cutler and Davis <ref type="bibr" target="#b9">[10]</ref>, in which they show that human motion is characterized by specific periodic patterns in the similarity plot (a 2D matrix of all pairwise image matching correlations), and describe a method for human detection by recognizing such patterns. They also use similarity plots to estimate the stride of a walking and running person, assuming a calibrated camera. Here, a person is tracked over a ground plane, and their distance travelled, , is estimated. The number of steps AE is also automatically estimated using periodic motion, which can be a non-integer number. The stride is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AE,</head><p>which could be used as a biometric, though they have not conducted any study showing how useful it is as a biometric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Motivation</head><p>In this section, we give the motivation for the methodology used in this paper. One method of motion-based recognition is to first explicitly extract the dynamics of points on a moving object.</p><p>Consider a point È ´Øµ Ǘ´Øµ Ý ´Øµ Þ ´Øµµ on a moving object as a function of time Ø (see Figure <ref type="figure" target="#fig_0">1</ref>). The dynamics of the point can be represented by the phase plot ´È ´Øµ È Ø´Øµ µ. Since we wish to recognize different types of motions (viz. gaits), it is important to know what can be determined from the projection Ì of È ´Øµ onto an image plane, ´Ù Úµ Ì ´È µ. Under orthographic projection, and if È ´Øµ is constrained to planar motion, the object dynamics are completely preserved up to a scalar factor. That is, the phase space for the point constructed from ´Ù Úµ is identical (up to a scalar factor) to the phase space constructed from È ´Øµ. However, if the motion is not constrained to a plane, then the dynamics are not preserved. Under perspective projection, the dynamics of planar and arbitrary motion are in general not preserved.</p><p>Fortunately, planar motion is an important class of motion, and includes "biological motion" <ref type="bibr" target="#b16">[17]</ref>. In addition, if the object is sufficiently far from the camera, the camera projection becomes approximately orthographic (with scaling). In this case, and assuming we can accurately track a point È ´Øµ in the image plane, then we can completely reconstruct the phase space of the dynamic system (up to a scalar factor). The phase space can then be used directly to classify the object motion (e.g., <ref type="bibr" target="#b6">[7]</ref>). In general, point correspondence is not always possible in realistic image sequences (without the use of special markers), due to occlusion boundaries, lighting changes, insufficient texture, image noise, etc. However, for classifying motions, we do not necessarily have to extract the complete dynamics of the system; qualitative measures may suffice to distinguish a class of motions from each other. In this paper, we use correspondence-free, qualitative measures for motion-based gait recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Self-similarity Plots</head><p>Computation from an Image Sequence Given a sequence of grey-scale images obtained from a static camera, we detect and track the moving person, extract an image template corresponding to the person's motion blob in each frame, then compute the image self-similarity plot from the obtained sequence of templates. For this, we use the method described in <ref type="bibr" target="#b9">[10]</ref>, except that we use background modeling and subtraction <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19]</ref> for foreground detection, since the camera is assumed to be stationary.</p><p>Moving objects are tracked in each frame based on spatial and temporal image coherence. An image template at time Ø, denoted by ÇØ, is extracted for each tracked object, consisting of the image region enclosed within the bounding box of its motion blob in the current frame. Deciding whether a moving object corresponds to a walking person is currently done based on simple shape (such as aspect ratio of the bounding box and blob size) and periodicity cues.</p><p>Once a person has been tracked for AE consecutive frames, its AE image templates are scaled to the same dimensions ÀxÏ , as their sizes may vary due to change in camera viewpoint and segmentation errors. The image self-similarity, Ë, of the person is then computed as follows:</p><formula xml:id="formula_0">Ë´Ø½ Ø ¾µ ´Ü Ýµ¾ Ø ½ ÇØ ½ ´Ü Ýµ ÇØ ¾ ´Ü Ýµ</formula><p>where ½ Ø½ Ø ¾ AE, Ø ½ is the bounding box of the person in frame Ø½, and ÇØ ½ Ç Ø ¾ ÇØ AE are the scaled image templates of the person. In order to account for tracking errors, we compute the minimal Ë ¼ by translating over a small search radius Ö: Properties The similarity plot, Ë ¼ , of a walking person has the following properties:</p><formula xml:id="formula_1">Ë ¼ ´Ø½ Ø ¾µ Ñ Ò Ü Ý Ö ´Ü Ýµ¾ Ø ½ ÇØ ½ ´Ü • Ü Ý • Ýµ ÇØ ¾ ´Ü Ýµ</formula><p>1. Ë ¼ ´Ø Øµ ¼ , i.e. it has a dark main diagonal.</p><p>2. Ë ¼ ´Ø½ Ø ¾µ Ë´Ø¾ Ø ½ µ, i.e. it is symmetric along the main diagonal.</p><p>3. Ë ¼ ´Ø½ Ô ¾ • Ø½µ ³ ¼, i.e. it has dark lines parallel to the main diagonal.</p><p>4. Ë ¼ ´Ø½ Ô ¾ Ø½µ ³ ¼, i.e. it has dark lines perpendicular to the main diagonal.</p><p>where Ø½ Ø ¾ ¾ ½ AE , Ô is the period of walking, and is an integer. The first two properties are generally true for any similarity function (though if substantial image scaling is required, the second property may not hold). The latter two, however, are a direct consequence of the periodicity and the bilateral symmetry, respectively, of the human gait. That Ë ¼ encodes the frequency and phase of gait can be explained by the fact that the similarity plot of a walking person is (approximately) a projection of the planar dynamics of the walking person when viewed sufficiently far from the camera, as previously suggested in <ref type="bibr" target="#b9">[10]</ref>. Intuitively, this is because Ë ¼ is obtained via a sequence of transformations (image projection and template matching) applied to the set of 3D points constituting the person's body. It can be shown that these transformations approximately preserve the dynamics of these points (and hence the dynamics of the gait) under certain assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Gait Classifier</head><p>As mentioned in the previous section, the similarity plot is a projection of the dynamics of the walking person that preserves the frequency and phase of the gait. The question then arises as to whether this projection preserves more detailed (higherdimensional) aspects of gait dynamics, that capture the unique way a person walks. In other words, does a similarity plot contain sufficient information to distinguish (not necessarily uniquely) the walking gaits of different people?</p><p>To evaluate the usefulness of the self-similarity plot in characterizing and recognizing individual gaits, we propose to build a gait pattern classifier that takes an SP (self-similarity plot) as the input feature vector. For this, we take an 'eigenface' approach <ref type="bibr" target="#b34">[35]</ref>, in which we treat a similarity plot the same way that a face image is used in a face recognizer. The gist of this approach is that it extracts 'relevant information' from input feature vectors (face images or SPs) by finding the principal components of the distribution of the feature space, then applies standard pattern classification of new feature vectors in the lower-dimensional space spanned by the principal components. We use a simple non-parametric pattern classification technique for recognition. In the following, we explain the details of the proposed gait classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normalizing the Input</head><p>In order to account for different walking paces and starting poses, we need to normalize the selfsimilarity plots so that they are phase-aligned, have the same frequency, and contain the same number of cycles. To this end, we compute the frequency and phase of each similarity plot using the method in <ref type="bibr" target="#b9">[10]</ref>. We choose the pose corresponding to when the legs are maximally apart, i.e. poses A or C in Figure <ref type="figure" target="#fig_1">2</ref>(a), for phase alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training the Classifier</head><formula xml:id="formula_2">Let Ë ¼ ½ Ë ¼ ½ Ë ¼</formula><p>Å be a given training set of Å labelled (i.e. corresponding to a known person) normalized similarity plots, of size AE Ü AE each, and let × ¼ be the vector of length AE ¾ corresponding to the th similarity plot Ë ¼ (obtained by concatenating all its rows). We compute the principal components <ref type="bibr" target="#b21">[22]</ref> of the space spanned by × ¼ ½ × ¼ Å by computing the eigenvalue decomposition (also called Karhunen-Loeve expansion) of their covariance matrix:</p><formula xml:id="formula_3">× ½ Å Å ½ ´×¼ × ¼ µ´× ¼ × ¼ µ Ì</formula><p>where × ¼ is the simple mean of all training vectors × ¼ ½ × ¼ Å . This can be efficiently computed in Ç´Åµ time (instead of the brute force Ç´AE ¾ µ) <ref type="bibr" target="#b34">[35]</ref>.</p><p>We then consider the space spanned by the Ò most significant eigenvectors, Ù½ Ù Ò , that account for ¼± of the variation in the training SPs 1 . We denote this space the Eigengait. Hence each training vector × ¼ can be sufficiently approximated by a Òdimensional vector Û obtained by projecting it onto the Eigengait, i.e. Û È Ò ½ Ù Ì × ¼ . Furthermore, assuming that the training vectors are representative of the variation in the entire feature space, then any new feature vector can be similarly approximated by a point in Eigengait space.</p><p>Classification Gait recognition now reduces to a standard pattern classification in a Ò-dimensional Eigengait space. The advantage of doing pattern classification in this space is not only that Ò is typically much smaller than AE ¾ and Å, but also that it contains less unwanted variation (i.e. random noise) 2 and hence provides better separability of the feature vectors, or SPs.</p><p>Given a new SP (corresponding to an unknown person), the procedure for recognizing it is to first convert it to a AE ¾ -vector, map it to a point in Eigengait, find the closest training points to it, then decide its class (or label) via the k-nearest neighbor rule <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b30">31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>To evaluate our method, we build the gait classifier described above using k-nearest neighbor classification and the gait data set of Little and Boyd <ref type="bibr" target="#b22">[23]</ref>. We use the leave-one-out cross-validation to obtain a statistically accurate estimate of the recognition rate <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>The data set consists of 40 image sequences and six different subjects (7 sequences per person except for the 5th person). Figure <ref type="figure" target="#fig_4">3</ref> shows all six subjects overlaid at once on the background image.</p><p>Since the camera is static we used median filtering to recover the background image. Templates of the moving person were extracted from each image by computing the difference of the image and the background and subsequently applying a threshold as well as morphological operations to clean up noise. This simple method for person detection and tracking was sufficient because the background is static and each sequence only contains one moving person. Absolute correlation was used to compute the self-similarity plot for each of the 40 template sequences. For temporal normalization (since the image sequences were of varying lengths and the 1 According to the theory of PCA, if ½ Ò are the Ò largest eigenvalues, then the space spanned by their corresponding eigenvectors account for È Ò ½ ØÖ ´ ×µ of the total vari- ation in the original feature vectors. 2 Assuming data variation is much larger than noise variation. persons had different gait cycle lengths), the similarity plots were cropped and scaled so that they all contained 4 gait cycles starting on the same phase, and are of size 64x64. Figure <ref type="figure" target="#fig_5">4</ref> shows examples of these normalized similarity plots, where each column of three plots corresponds to one person.</p><p>Since our data set is relatively small, we use the leave-one-out cross-validation method. The leave-one-out error rate estimator is known to be an (almost) unbiased estimator of the true error rate of the classifier. Hence, out of the 40 similarity plots, we build (or train) our classifier on all but one of the samples, test the classifier on the sample missed (or left out), and record the classification result. This is repeated 40 times, leaving out each of the 40 samples in turn. The recognition rate is then obtained as the ratio of the number of correctly classified test samples out of the total 40.</p><p>The classifier is built simply by storing the training vectors as points in Eigengait space, and the test sample is classified by determining its -nearest neighbor with , using the Euclidian distance as a distance metric and simple majority as a decision rule. The recognition rate thus obtained is 0.93 (37 out of 40).</p><p>To visualize how well separated the gaits of the 6 people are, we applied PCA to all 40 SPs. Figure <ref type="figure" target="#fig_6">5</ref> shows all 40 SPs projected onto the 3 most significant eigenvectors thus obtained. Each closed contour encloses the samples (points) corresponding to one person.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and future work</head><p>In this paper, we have used a correspondence-free motionbased method to recognize the gaits of a small population (6) of people. While the results are promising, more evaluation of the method needs to be done. Future studies include larger test populations (20-100 people) and images taken from multiple view points (not just parallel to the image plane) <ref type="foot" target="#foot_0">3</ref> . In addition, image sequences of the same individual need to be acquired in different lighting conditions, and with various types of clothing.</p><p>Finally, we are working to combine the results of this correspondence-free gait recognition method with more featureoriented methods, such as stride and height estimation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Point È ´Øµ on an object moving in Ê ¿ , imaged onto a 2D plane.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 (</head><label>2</label><figDesc>Figure 2(b) shows a plot of Ë ¼ for all combinations of Ø½ and Ø¾, for the two walking sequences (80 frames each) shown in Figure 2(a) (note the similarity values have been linearly scaled for visualization to the grayscale intensity range [0,255], where dark regions show more similarity).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 (</head><label>2</label><figDesc>a) shows the 5 key poses over one walking cycle for two different persons; poses and correspond to when the person is in maximum swing (i.e. the two legs are furthest apart), and Pose corresponds to when the two legs are together. One can easily see that the intersections of the dark lines in Ë ¼ (i.e. the diagonals and cross diagonals) correspond to pose combinations AA, BB and CC, AC, and CA. Thus these intersections, which are the local minima of Ë ¼ , can be used to determine the frequency and phase of walking<ref type="bibr" target="#b9">[10]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) A few frames of the key poses in a walking person sequence, and the corresponding (b) Image self-similarity plots.</figDesc><graphic coords="4,52.92,73.56,161.50,144.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The six people contained in the test sequences, overlaid on the background image.</figDesc><graphic coords="5,69.00,345.22,198.40,99.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Normalized self-similarity plots (columns correspond to a single person).</figDesc><graphic coords="5,69.00,534.11,198.40,96.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Similarity plots projected onto the space spanned by the three most dominant eigenvectors.</figDesc><graphic coords="5,327.72,58.12,198.51,150.50" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>The Keck Laboratory<ref type="bibr" target="#b5">[6]</ref> will be utilized to acquire multiview (up to 64) images sequences of a person walking.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We would like to thank Dr. Jeffrey E. Boyd of the Department of Computer Science at the University of Calgary, Canada, for providing the gait data we used in testing. The support of DARPA (Human ID project, grant No. 5-28944) is also gratefully acknowledged.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human motion analysis: a review</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Computer Society Workshop on Motion of Non-Rigid and Articulated Objects</title>
		<meeting>of IEEE Computer Society Workshop on Motion of Non-Rigid and Articulated Objects</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image Sequence Analysis of Real World Human Motion</title>
		<author>
			<persName><forename type="first">K</forename><surname>Akita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="78" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Temporal and Spatial Factors in Gait Perception that Influence Gender Recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Barclay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cutting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kozlowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception and Psychophysics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="145" to="152" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Audio-and Videobased Biometric Person Authentication</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bigun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Borgefors</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Neural Networks for Pattern Recognition</title>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Clarendon Press</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multiperspective Analysis of Human Actions</title>
		<author>
			<persName><forename type="first">E</forename><surname>Borovikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Horprasert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third International Workshop on Cooperative Distributed Vision</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recognition of Human Body Motion Using Phase Space Constraints</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A survey of motion analysis from moving light displays</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cedras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision and Pattern Recognition</title>
		<meeting>the Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="214" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using Gait as a Biometric, via Phase-Weighted Magnitude Spectra</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cunado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1st Int. Conf. on Audio-and Video-Based Biometric Person Authentication</title>
		<meeting>1st Int. Conf. on Audio-and Video-Based Biometric Person Authentication</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="95" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust Real-time Periodic Motion Detection, Analysis and Applications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="155" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recognizing Friends by Their Walk: Gait Perception Without Familiarity Cues</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cutting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kozlowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin Psychonomic Soc</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="353" to="356" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The representation and recognition of action using temporal templates</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision and Pattern Recognition</title>
		<meeting>the Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="928" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Non-parametric model for background subtraction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The visual analysis of human movement: a survey</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="82" to="98" />
			<date type="published" when="1999-01">January 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards 3-D Model-based Tracking and Recognition of Human Movement: a Multi-View Approach</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Individual Recognition from Periodic Activity Using Hidden Markov Models</title>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Debrunner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Human Motion</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The interpretation of biological motion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Flinchbaugh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Model-based vision: a program to see a walking person</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Robust Background Subtraction and Shadow Detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Horprasert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Comparing Different Template Features for Recognizing People by their Gait</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Nixon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Visual Motion Perception</title>
		<author>
			<persName><forename type="first">G</forename><surname>Johansson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975-06">June 1975</date>
			<biblScope unit="page" from="75" to="88" />
		</imprint>
	</monogr>
	<note>Scientific American</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Joliffe</surname></persName>
		</author>
		<title level="m">Principal Component Analysis</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recognizing people by their gait: the shape of motion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Videre</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Finding periodicity in space and time</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="1998-01">January 1998</date>
			<biblScope unit="page" from="376" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Scientific Basis of Human Motion</title>
		<author>
			<persName><forename type="first">K</forename><surname>Luttgens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kinesiology</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<publisher>Saunders College Publishing</publisher>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note>th ed.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gait Classification with HMMs for Trajectories of Body Parts Extracted by Mixture Densities</title>
		<author>
			<persName><forename type="first">D</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Psl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Niemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Moving object recognition in eigenspace representation: gait analysis and lip reading</title>
		<author>
			<persName><forename type="first">H</forename><surname>Murase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="155" to="162" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gait as a total pattern of movement</title>
		<author>
			<persName><forename type="first">M</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Physical Medicine</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="290" to="332" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Analyzing and recognizing walking figures in XYT</title>
		<author>
			<persName><forename type="first">S</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision and Pattern Recognition</title>
		<meeting>the Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="469" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Detection and Recognition of Periodic, Non-rigid Motion</title>
		<author>
			<persName><forename type="first">R</forename><surname>Polana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="261" to="282" />
			<date type="published" when="1997-07">June/July 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Ripley</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Neural Networks</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards model-based recognition of human movements in image sequences</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rohr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVGIP</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards Detection of Human Motion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision and Pattern Recognition</title>
		<meeting>the Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cyclic Motion Detection for Motion-based Recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kasparis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1591" to="1603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Face Recognition using Eigenfaces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision and Pattern Recognition</title>
		<meeting>the Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kulikowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Systems that Learn</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Morgan Kaufman</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
