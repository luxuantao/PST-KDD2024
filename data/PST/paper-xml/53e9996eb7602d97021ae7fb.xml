<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compression of facial images using the K-SVD algorithm q</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2008-03-16">16 March 2008</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ori</forename><surname>Bryt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Electrical Engineering Department</orgName>
								<orgName type="institution">The Technion-Israel Institute of Technology</orgName>
								<address>
									<postCode>32000</postCode>
									<settlement>Technion City, Haifa</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Michael</forename><surname>Elad</surname></persName>
							<email>elad@cs.technion.ac.il</email>
							<affiliation key="aff1">
								<orgName type="department">The Computer Science Department</orgName>
								<orgName type="institution">The Technion-Israel Institute of Technology</orgName>
								<address>
									<addrLine>Taub Building, Office 516</addrLine>
									<postCode>32000</postCode>
									<settlement>Technion City, Haifa</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Compression of facial images using the K-SVD algorithm q</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2008-03-16">16 March 2008</date>
						</imprint>
					</monogr>
					<idno type="MD5">8902886DCA4E241E8A4941E5607CB70F</idno>
					<idno type="DOI">10.1016/j.jvcir.2008.03.001</idno>
					<note type="submission">Received 3 November 2007 Accepted 7 March 2008</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Image compression Sparse representations Redundancy K-SVD OMP Facial images PCA JPEG JPEG2000 VQ</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The use of sparse representations in signal and image processing is gradually increasing in the past several years. Obtaining an overcomplete dictionary from a set of signals allows us to represent them as a sparse linear combination of dictionary atoms. Pursuit algorithms are then used for signal decomposition. A recent work introduced the K-SVD algorithm, which is a novel method for training overcomplete dictionaries that lead to sparse signal representation. In this work we propose a new method for compressing facial images, based on the K-SVD algorithm. We train K-SVD dictionaries for predefined image patches, and compress each new image according to these dictionaries. The encoding is based on sparse coding of each image patch using the relevant trained dictionary, and the decoding is a simple reconstruction of the patches by linear combination of atoms. An essential pre-process stage for this method is an image alignment procedure, where several facial features are detected and geometrically warped into a canonical spatial location. We present this new method, analyze its results and compare it to several competing compression techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Compression of still images is a very active and matured field of research, vivid in both research and engineering communities. Compression of images is possible because of their vast spatial redundancy and the ability to absorb moderate errors in the reconstructed image. This field of work offers many contributions, some of which became standard algorithms that are wide-spread and popular. Among the many methods for image compression, one of the best is the JPEG2000 standard-a general purpose wavelet based image compression algorithm with very good compression performance <ref type="bibr" target="#b0">[1]</ref>.</p><p>When considering the compression of a specific and narrow family of images, the above-mentioned redundancy increases, thus enabling a better compression performance. Such is the case with compression of facial images. Indeed, this expected gain has been observed and exploited in several recent publications that offer tailored compression algorithms for facial images <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>. Among those contributions, the more recent ones show performance surpassing those of the JPEG2000 <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>Compression of facial images is an appealing problem, both because of the research challenges it provides, and the important applications it serves. From a research perspective, one should primarily wonder how to exploit the additional redundancy that such a focused family of images exhibits, and how to surpass general purpose compression algorithms this way. This is not an easy challenge due to the vast efforts put to the general purpose algorithms, and especially their entropy coding parts.</p><p>Application-wise, facial images are perhaps the most popular images, held in large databases by various organizations, such as police and law-enforcement, schools and universities, states, and in databases of employees in large companies. Efficient storage of such images is of value, and especially so when considering photo-ID in an electronic ID cards. In such applications, very-low bit-rate compression is to be considered, where general purpose algorithms fail utterly.</p><p>Motivated by the above, In this work we address compression of facial images as well, very much in line with the above activity. In order to focus the discussion, we target photo-ID images of prespecified and fixed size 358 Â 441 grayscale images with 8 bits per pixel. 1 The goal of our work is very-low bit-rates (compression ratios of 200 and beyond), where most algorithms are unable to show recognizable faces. We use a database containing around 6000 such facial images, some of which are used for training and tuning the algorithm, and the others for testing it, similar to the approach taken in <ref type="bibr" target="#b16">[17]</ref>.</p><p>In our work we propose a novel compression algorithm, related to the one presented in <ref type="bibr" target="#b16">[17]</ref>, improving over it.</p><p>Our algorithm relies strongly on recent advancements made in using sparse and redundant representation of signals <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref>, and learning their sparsifying dictionaries <ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref>. We use the K-SVD algorithm for learning the dictionaries for representing small image patches in a locally adaptive way, and use these to sparsecode the patches' content. This is a relatively simple and straight-forward algorithm with hardly any entropy coding stage. Yet, it is shown to be superior to several competing algorithms: (i) the JPEG2000, (ii) the VQ-based algorithm presented in <ref type="bibr" target="#b16">[17]</ref>, and (iii) A Principal Component Analysis (PCA) approach. <ref type="foot" target="#foot_0">2</ref>In the next section we provide some background material for this work: we start by presenting the details of the compression algorithm developed in <ref type="bibr" target="#b16">[17]</ref>, as their scheme is the one we embark from in the development of ours. We also describe the topic of sparse and redundant representations and the K-SVD, that are the foundations for our algorithm. In Section 3 we turn to present the proposed algorithm in details, showing its various steps, and discussing its computational/memory complexities. Section 4 presents results of our method, demonstrating the claimed superiority. We conclude in Section 5 with a list of future activities that can further improve over the proposed scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">VQ-based image compression</head><p>Among the thousands of papers that study still image compression algorithms, there are relatively few that consider the treatment of facial images <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>. Among those, the most recent and the best performing algorithm is the one reported in <ref type="bibr" target="#b16">[17]</ref>. That paper also provides a thorough literature survey that compares the various methods and discusses similarities and differences between them. Therefore, rather than repeating such a survey here, we refer the interested reader to <ref type="bibr" target="#b16">[17]</ref>. In this sub-section we concentrate on the description of the algorithm in <ref type="bibr" target="#b16">[17]</ref> as our method resembles it to some extent. This algorithm, like some others before it, starts with a geometrical alignment of the input image, so that the main features (ears, nose, mouth, hair-line, etc.) are aligned with those of a database of pre-aligned facial images. Such alignment increases further the redundancy in the handled image, due to its high cross similarity to the database. The warping in <ref type="bibr" target="#b16">[17]</ref> is done by an automatic detection of 13 feature points on the face, and moving them to pre-determined canonical locations. These points define a slicing of the input image into disjoint and covering set of triangles, each exhibiting an affine warp, being a function of the motion of its three vertices. Side information on these 13 feature locations enables a reverse warp of the reconstructed image in the decoder. Fig. <ref type="figure" target="#fig_0">1</ref> (left side) shows the features and the induced triangles. After the warping, the image is sliced into square and non-overlapping patches (of size 8 Â 8 pixels), each of which is coded separately. Such possible slicing (for illustration purpose we show this slicing with larger patches) is shown in Fig. <ref type="figure" target="#fig_0">1</ref> (right side).</p><p>Coding of the image patches in <ref type="bibr" target="#b16">[17]</ref> is done using vector quantization (VQ) <ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>. The VQ dictionaries are trained (using tree-K-Means) per each patch separately, using patches taken from the same location from 5000 training images. This way, each VQ is adapted to the expected local content, and thus the high performance presented by this algorithm. The number of code-words in the VQ is a function of the bit-allocation for the patches. As we argue in the next section, VQ coding is limited by the available number of examples and the desired rate, forcing relatively small patch sizes. This, in turn, leads to a loss of some redundancy between adjacent patches, and thus loss of potential compression.</p><p>Another ingredient in this algorithm that partly compensates for the above-described shortcoming is a multi-scale coding scheme. The image is scaled down and VQ-coded using patches of size 8 Â 8. Then it is interpolated back to the original resolution, and the residual is coded using VQ on 8 Â 8 pixel patches once again. This method can be applied on a Laplacian pyramid of the original (warped) image with several scales <ref type="bibr" target="#b32">[33]</ref>.</p><p>As already mentioned above, the results shown in <ref type="bibr" target="#b16">[17]</ref> surpass those obtained by JPEG2000, both visually and in Peak-Signal-to-Noise Ratio (PSNR) quantitative comparisons. In our work we propose to replace the coding stage from VQ to sparse and redundant representations-this leads us to the next subsection, were we describe the principles behind this coding strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Sparse and redundant representations</head><p>We now turn to describe a model for signals known as Sparseland <ref type="bibr" target="#b28">[29]</ref>. This model suggests a parametric description of signal sources in a way that adapts to their true nature. This model will be harnessed in this work to provide the coding mechanism for the image patches. We consider a family of image patches of size N Â N pixels, ordered lexicographically as column vectors x 2 R n (with n ¼ N 2 ). Assume that we are given a matrix D 2 R nÂk (with possibly k &gt; n). We refer hereafter to this matrix as the dictionary. The Sparseland model suggests that every such image patch, x, could be represented sparsely using this dictionary, i.e., the solution of â ¼ arg min a kak 0 subject to kDa À xk 2 2 6 e 2 ; ð1Þ is expected to be very sparse, kâk 0 ( n. The notation kak 0 counts the non-zero entries in a. Thus, every signal instance from the family we consider is assumed to be represented as a linear combination of few columns (referred to hereafter as atoms) from the redundant dictionary D.</p><p>The requirement kDa À xk 2 6 e suggests that the approximation of x using Da need not be exact, and could absorb a moderate error e. This suggests an approximation that trades-off accuracy of representation with its simplicity, very much like the rate-distortion curve in compression. Indeed, compression of x can be achieved by transmission of the vector â, by specifying the indices of its nonzero elements and their magnitudes. Quantization on the non-zero entries in â leads to higher approximation error but with the blessed fact that transmission of â now requires a finite and small number of bits. This is exactly the strategy we are about to deploy. Similar approach in a different context has been practiced in <ref type="bibr" target="#b33">[34]</ref> for coding of general images.</p><p>In order to use the Sparseland model for compression of image patches, we need an effective way to solve the problem posed in Eq. <ref type="bibr" target="#b0">(1)</ref>. While this problem is in general very hard to solve, the matching and the basis pursuit algorithms can be used quite effectively <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> to get an approximated solution. Recent work established that those approximation techniques can be quite accurate if the solution is sparse enough to begin with <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref>. In this work we make use of the Orthogonal Matching Pursuit (OMP) because of its simplicity and efficiency. We refer hereafter to the solution of ðP 0 Þ as sparse coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Training A dictionary</head><p>Given a set of image patches of size N Â N to be coded, X ¼ fx j g M j¼1 , the assumption that they emerge from the Sparseland model helps us in devising the new coding strategy. However, we must have the dictionary D in order to use this coding scheme. This can be achieved by training D-minimizing the following energy functional with respect to both D and fa j g M j¼1 ,</p><formula xml:id="formula_0">e D; fa j g M j¼1 ¼ X M j¼1 l j ka j k 0 þ Da j À x j 2 2 h i :<label>ð2Þ</label></formula><p>This expression seeks to get a sparse representation per each of the examples in X, and obtain a small representation error. The choice for l j dictates how those two forces should be weighted, so as to make one of them a clear constraint. For example, constraining 8jka j k 0 ¼ L implies specific values for l j , while requiring 8jkDa j À z j k 2 6 e leads to others. The K-SVD algorithm proposes an iterative algorithm designed to handle the above task effectively <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. Adopting a block-coordinate descent idea, the computations of D and fa j g M j¼1 are separated. Assuming that D is known, the penalty posed in Eq. ( <ref type="formula" target="#formula_0">2</ref>) reduces to a set of M sparse coding operations, very much like in Eq. ( <ref type="formula">1</ref>). Thus, OMP can be used to obtain the near-optimal solutions. Similarly, assuming that these representation vectors are fixed, the K-SVD algorithm proposes an update of the dictionary one column at a time. As it turns out, this update can be done optimally, leading to the need to perform a singular value decomposition (SVD) operation on residual data matrices, computed only on the examples that use this atom. This way, the value of eðD; fa j g M j¼1 Þ is guaranteed to drop per an update of each dictionary atom, and along with this update, the representation coefficients change as well (see <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> for more details).</p><p>Fig. <ref type="figure" target="#fig_1">2</ref> shows a typical representation error reduction graph in a K-SVD dictionary training process performing a gradual increment of the number of atoms used for the representation of the examples. This error is shown as Root Mean Squared Error (RMSE), being the squared-root of the mean of the errors kDa j À x j k 2 2 , i.e., setting 8jka j k 0 ¼ L, the algorithm starts with L ¼ 1 and after obtaining convergence, L is incremented by 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The proposed method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The general scheme</head><p>We now turn to describe our compression scheme. A block diagram of the encoder and decoder are given in Fig. <ref type="figure" target="#fig_2">3</ref>. Our algorithm consists of two main processes: An offline K-SVD training process and an online image compression/decompression processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">K-SVD training</head><p>The K-SVD training process is an off-line procedure, preceding any image compression. The training produces a set of K-SVD dictionaries that are then considered fixed for the image compression stage. A single dictionary is trained for each 15 Â 15 patch over a set of examples that will referred to as the learning set. The training follows the description from the previous section, with parameters detailed in Section 4. Prior to the training process for each patch, the mean patch image of the examples in the learning set is calculated and subtracted from all the examples in this set.</p><p>The image compression process uses the following steps in the encoder, followed by a mirror application of them at the decoder:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Pre-processing</head><p>We use the same pre-process stage of geometrical warping as in <ref type="bibr" target="#b16">[17]</ref>. This leads to better conditioning of the input image, increasing its redundancy versus the image database. The parameters of the warp are sent as side information for the inverse warp at the decoder. Those parameters are coded using 20 bytes. The pre-processing stage also includes a simple scale-down by a factor of 2:1 of the input image, so that later processing is applied on a smaller image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Slicing to patches</head><p>Our approach also works on fixed size square patches, coding each patch separately and adaptively as in <ref type="bibr" target="#b16">[17]</ref>. However, our coding strategy is different, being based on sparse and redundant representations. The change in methods leads to the ability to handle larger patches, and thus get more efficient coding. In our simulations we have used N ¼ 15 pixels, <ref type="foot" target="#foot_1">3</ref> although larger patch sizes are also possible. The patches must also fit the dictionary in content, so the same mean patch images that were calculated for the learning set before the training process is subtracted out of the relevant patches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4.">Sparse coding</head><p>Every patch location has a pre-trained dictionary of code-words (atoms) D ij . The size we use for k is 512-a discussion on this choice appears in Section 4. These dictionaries are generated using the K-SVD algorithm on a set of 4415 training examples. The coding itself is done by assigning a linear combination of few atoms to describe the patch content (sparse coding). Thus, information about the patch content includes both the linear weights and the involved indices. Note that the number of atoms vary from one patch to another, based on its average complexity, and this varied number is known at the decoder.</p><p>Both encoder and decoder are storing the dictionaries, just as in <ref type="bibr" target="#b16">[17]</ref>. The sparse-coding stage for encoding of the patches is done using the OMP algorithm. The decoder is simply aggregating the proper atoms with the proper weights to reconstruct the patch, building the output one patch at a time independent of the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5.">Entropy coding and quantization</head><p>We use a straight-forward Huffman table applied only to the indices. The representations' weights are quantized with a 7-bit uniform quantization, with boundaries chosen based on the training information for each patch separately.</p><p>Since the above compression scheme strongly relies on a successful detection of the features to warp by, a natural question is whether errors in this stage are destructive to the overall compression algorithm. When the detection of the features fails utterly, the compression necessarily leads to very low performance. Since we are performing a compression on an incoming image, we have access to both the resulting PSNR, and also the average PSNR we expect to get for this bits allocation. Thus, if the compression PSNR is below some threshold, we obviously know that the detection failed (or possibly the input image is not a face image). In such a case, and in a completely automatic fashion, we can employ the JPEG2000, which is indeed weaker, but still reasonable. A second option we have in such a system is to prompt the user to detect the features manually for this image. Thus, even rare situations where the features are not found do not lead to a breakdown in a system that employs the proposed algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">VQ versus sparse representations</head><p>As mentioned in Section 2, the destination compression ratio and the number of training examples pose a hard limit on the allowed patch size, when using VQ. This is because as the patch size grows while keeping the rate (bits per pixel) fixed, the dictionary is required to grow exponentially, reaching sizes far beyond the number of examples to train on. More specifically, using patch size of N Â N pixels, with k codewords in the VQ dictionary, considering an image with P pixels, and a target rate of B bits for the entire image, all these ingredients are related to each other by</p><formula xml:id="formula_1">B ¼ P N 2 Á log 2 ðkÞ ) k ¼ 2 N 2 B P :<label>ð3Þ</label></formula><p>For example, coding an image of size 180 Â 220 (P = 39,600) pixels <ref type="foot" target="#foot_2">4</ref>with a target rate of B ¼ 500 bytes, a patch of size N ¼ 8 leads to k ¼ 88, which is reasonable. However, increasing the patch size to N ¼ 12 already leads to k % 24; 000, which cannot be trained from 6000 examples. Thus, the VQ approach is limited to relatively small patch sizes (8 Â 8 used in <ref type="bibr" target="#b16">[17]</ref>), implying that much of the spatial redundancy between adjacent patches is overlooked. When turning to sparse and redundant dictionaries, the picture changes dramatically. We consider a patch size of N Â N pixels, with k atoms in the dictionary, L of which are used on average 5 in the representation (assuming 7-bit quantization of the weights). Handling of an image with P pixels, and a target rate of B bits for the entire image, all these ingredients lead to the relation</p><formula xml:id="formula_2">B ¼ P N 2 Á L Á log 2 ðkÞ þ 7 ð Þ :<label>ð4Þ</label></formula><p>This means that per patch, instead of using log 2 k bits when working with VQ, we need Lðlog 2 k þ 7Þ bits on average. Thus, if the required amount of bits per patch is too high (as indeed happens when N grows), it can be absorbed by varying L. For example, for an image with P = 39,600 pixels, using k ¼ 512 atoms, a target rate of B ¼ 500-1000 bytes, and a patch sizes in the range N ¼ 8 $ 30, all these lead to a required average number of atoms L as shown in Fig. <ref type="figure">4</ref>. As can be seen, the values are reasonable and so we have the flexibility to control the rate by changing L in the range 1-12.</p><p>Aside from the flexibility that the Sparseland model provides, this model also proposes a flexible description of the patches' content, in a way that leads to high coding performance. The fact that the dictionaries are trained from relevant examples leads to very effective and tight model. Indeed, this model has been deployed in recent years to a variety of applications in signal and image processing, leading typically to state-of-the-art results <ref type="bibr" target="#b28">[29]</ref>. More work is required to explain the origin of this model's strength, though.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Run-time and memory requirements</head><p>In assessing complexities, we should separate the discussion between the training and the compression processes. The training process is composed of iterating on two main stages-sparse coding, which is done using the OMP algorithm, and dictionary update. As mentioned in <ref type="bibr" target="#b28">[29]</ref>, both stages can be done efficiently in OðJnkL Tr S L Þ per patch, where J is the number of iterations, n is the size of the patch, k is the number of atoms in the dictionary, L Tr is the maximal number of non-zero elements in each coefficient vector in each patch during the training, and S L is the number of examples in the learning set.</p><p>Since the training process is done to the entire learning set and to each patch separately, The complexity of training all the patches is Oð P n JnkL Tr S L Þ ¼ OðPJkL Tr S L Þ, where P is the number of pixels in the image and P n is therefore approximately (due to border issues) the number of disjoint square patches.</p><p>The compression process is also composed of two stages-the encoding and the decoding stages. The encoding is done using the OMP algorithm, which has a complexity of OðnkL Av Þ per image, where L Av is the average number of non-zero elements in the coefficient vector in all the patches. The encoding stage is done to each patch separately, and therefore the complexity of the entire encoding process is O</p><formula xml:id="formula_3">P n nkL Av À Á ¼ O PkL Av ð Þ.</formula><p>The decoding stage is simply calculating the linear combination result, which can be done in OðnL Average Þ per patch. The overall complexity of this stage is therefore</p><formula xml:id="formula_4">O P n nL Av À Á ¼ O PL Av ð Þ.</formula><p>In assessing the required memory for the proposed compression method we need to take into account the dictionaries that were produced for each patch, the previously mentioned mean patch images, the Huffman tables, the coefficient usage tables and the quantization levels for each patch. Calculating in bytes, the required memory for the above mentioned data is given roughly by OðPkÞ bytes. In our tests with P = 39,600 and k ¼ 512 this leads to less than 20 Mbytes.</p><p>Practically, the training and testing processes has been all implemented using non-optimized Matlab code on a regular PC (Pentium 2, 2 GHz, 1 GByte RAM). Training of all the required dictionaries requires 100 h to complete (and thus has been implemented on several computers in parallel, each handling a different group of patches). The compression of a single image requires 5 s, and its decompression takes less than 1 s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">More details and results</head><p>We conducted several experiments in order to evaluate the performance of our compression method and the quality of its resulting images. In this section we show some statistical results as well as reconstructed images from our compression method and a comparison between our results to several other competitive compression techniques.</p><p>We used 4415 face images as our learning set and a different 100 images as our test set. All the images in both sets are of a fixed size of 358 Â 441 pixels prior to the pre-processing, and of size 179 Â 221 after a simple scale-down. The slicing to patches is uniform over the image with the previously mentioned size, and exceptions at the borders. Examples of such pre-processed training/testing images can be seen in Fig. <ref type="figure">5</ref>. 5 If L varies from one patch to another, side information is necessary in order to instruct the decoder how many atoms to use in each patch. However, in our scheme, while L indeed varies spatially, it remains fixed for all images and thus the decoder knows the atom allocated per patch with no additional side information. Before turning to preset the results we should add the following: while all the results shown here refer to the specific database we operate on, the overall scheme proposed is general and should apply to other face images databases just as well. Naturally, some changes in the parameters might be necessary, and among those, the patch size is the most important to consider. We also note that as one shifts from one source of images to another, the relative size of the background in the photos may vary, and this necessarily leads to changes in performance. More specifically, when the background regions are larger (e.g., the images we use here have relatively small such regions), the compression performance is expected to improve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">K-SVD dictionaries</head><p>The primary stopping condition for the training process was set to be a limitation on the maximal number of K-SVD iterations (being 100). A secondary stopping condition was a limitation on the minimal representation error. In the image compression stage we added a limitation on the maximal number of atoms per patch. These conditions were used to allow us to better control the rates of the resulting images and the overall simulation time.</p><p>Every obtained dictionary contains 512 patches of size 15 Â 15 pixels as atoms. In Fig. <ref type="figure">6</ref> we can see the dictionary that was trained for patch number 80 (The left eye) for L ¼ 4 sparse coding atoms, and similarly, in Fig. <ref type="figure">7</ref> we can see the dictionary that was trained for patch number 87 (The right nostril) also for L ¼ 4 sparse coding atoms. It can be seen that both dictionaries contain images similar in nature to the image patch for which they were trained for. A similar behavior was observed in other dictionaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Reconstructed images</head><p>Our coding strategy allows us to learn which parts of the image are more difficult than others to code. This is done by assigning the same representation error threshold to all of the patches, and observing how many atoms are required for the representation of each patch on average. Clearly, patches with a small number of allocated atoms are simpler to represent than others. We would expect that the representation of smooth areas of the image such as the background, parts of the face and maybe parts of the clothes will be simpler than the representation of areas containing high frequency elements such as the hair or the eyes. Fig. <ref type="figure" target="#fig_5">8</ref> shows maps of atom allocation per patch and representation error (RMSE-squared-root of the mean squared error) per patch for the images in the test set in two different bit-rates. It can be seen that more atoms were allocated to patches containing the facial details (hair, mouth, eyes, and O. Bryt, M. Elad / J. Vis. Commun. Image R. <ref type="bibr">19 (2008)</ref> 270-282 facial borders), and that the representation error is higher in these patches. It can also be seen that the overall number of allocated atoms increases and the representation error decreases with the image bit-rate.</p><p>As in other compression methods, our reconstructed images suffer from several kinds of artifacts. These artifacts are caused by the slicing to disjoint patches, coding them independently. Contrary to methods such as JPEG and JPEG2000, the reconstructed images in our method do not have a strong smearing artifact all over the image, but only local small areas in which the image is smeared. Other artifacts include blockiness effect due to the slicing to patches, inaccurate representation of high frequency elements in the image, inability to represent complicated textures (mainly in the clothes) and inconsistency in the spatial location of several facial elements comparing to the original image. These artifacts are caused by the nature of our compression method, which has a limited ability to build the reconstructed image out of the given dictionaries. In Fig. <ref type="figure" target="#fig_6">9</ref> we can see two original images from the learning set and their reconstructed images in a bit-rate of 630 bytes. The mentioned artifacts can be seen in these images especially in the areas of the chin, the neck, the mouth, the clothes and the outline of the hair. Although there are areas in the images in which there is a smearing effect, the majority of the image is clear and sharp, and certainly in a high visual quality. Fig. <ref type="figure" target="#fig_7">10</ref> shows three original images from the test set and their reconstructed images in a bit-rate of 630 bytes. As in the previous images, the mentioned artifacts can be seen in these images as well, in the same image areas as before. As expected, the quality of the images from the test set is not as high as the quality of images from the learning set, but these images too are clear and sharp, and in a high visual quality.    Much like other compression methods, the quality of the reconstructed images in our method improves as the bit-rate increases. However, the contribution gained from such a rate increment is not divided equally over the image. Additional bits are allocated to patches with higher representation error, and those are improved first. This property is directly caused by the nature of the compression process, which is RMSE oriented and not bit-rate oriented. The compression process sets a single RMSE threshold for all the patches, forcing each of them to reach it without fixing the number of allocated atoms per patch. Patches with simple (smooth) content are most likely to have a representation error far below the threshold even using zero or one atom, whereas patches with more complex content are expected to give a representation error very close to the threshold. Such problematic patches will be forced to improve their representation error by increasing the number of atoms they use as the RMSE threshold is decreased, while patches with a representation error below the threshold will not be forced to change at all. Fig. <ref type="figure" target="#fig_8">11</ref> illustrates the gradual improvement in the image quality as the bit-rate increases. As can be seen, not all the patches improve as the bit-rate increases but only some of them, such as several patches in the clothes area, in the ears and in the outline of the hair. These patches were more difficult to represent than others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparing to other techniques</head><p>An important part in assessing the performance of our compression method is its comparison to known and competitive compression techniques. As mentioned before, we compare our results in this work with JPEG, JPEG2000, The VQ-Based compression method described in <ref type="bibr" target="#b16">[17]</ref>, and a PCA-Based compression method that was built especially for this work as a competitive benchmark. We therefore start with a brief description of the PCA technique.</p><p>The PCA-Based compression method is very similar to the scheme described in this work, simply replacing the K-SVD dictionaries with a Principal Component Analysis (PCA) ones. These dictionaries are square matrices storing the eigenvectors of the autocorrelation matrices of the training examples in each patch, sorted by a decreasing order of their corresponding eigenvalues. Another induced difference is the replacement of the sparse coding representation with a simple linear approximation that leans on the first several eigenvectors, till the representation error decrease below the specified threshold. Comparison to the PCA-Based compression method is important in this work, because whereas our technique could be interpreted as an adaptation of the JPEG2000 to the image family by training, PCA could be seen as a similar step emerging from JPEG.</p><p>Figs. 12-14 show a visual comparison of our results with the JPEG, JPEG2000, and the PCA for three different bit-rates. Note that Fig. <ref type="figure" target="#fig_9">12</ref> does not contain JPEG examples because the rate is below the minimum possible one in this case. These figures clearly show the visual and quantitative advantage of our method over all the alternatives. We can especially notice the strong smearing effect in the JPEG and JPEG2000 images, whereas our images are clear and sharp.</p><p>Fig. <ref type="figure" target="#fig_13">15</ref> shows a Rate-Distortion curves comparison of the compression methods mentioned before, averaged on the 100 test images. The PSNR shown here corresponds to the aligned grayscale images for all methods, in order to evaluate the representation error from the sparse coding and reconstruction stages alone, without the error that result from the geometrical warp process. Adding the error induced by the geometrical warp implies a decrease of 0.2-0.4 dB for the VQ, PCA, and K-SVD methods.</p><p>In addition to these Rate-Distortion curves we added a curve for a ''Clean" JPEG2000 method, which is simply a horizontally shifted version of the JPEG2000 curve taking into account the header embedded in the JPEG2000 standard. This header was found to be of size 220 bytes.</p><p>It can be seen from this comparative graph that the K-SVD compression method has the best performance and especially so in the very low bit-rates of 300-1000 bytes, which is our area of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Dictionary redundancy</head><p>Dictionary redundancy, or overcompleteness, is defined as k=n, being 1 for complete (non-redundant) representations, and above this in our experiments. A natural question is whether such redundancy is truly necessary for getting the compression results shown. Fixing the patch size, n, what would be the effect of changing the parameter k? Fig. <ref type="figure" target="#fig_12">16</ref> shows four curves of the averaged PSNR on the test set images with varying We can see in Fig. <ref type="figure" target="#fig_12">16</ref> that the quality of the images is improving (PSNR-wise) with the dictionary redundancy, but this increase is subsiding. Intuitively, we would expect a change in the orientation of the curves at some point, partly due to overfitting of the dictionaries (due to the limited number of examples), and partly because too high redundancy is expected to cause deterioration in performance. Such change in tendency has not been observed because of limitations in the training data size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper we present a facial image compression method, based on sparse and redundant representations and the K-SVD dictionary learning algorithm. The proposed compression method is tested in various bit-rates and options, and compared to several known compression techniques with great success.</p><p>Results on the importance of redundancy in the deployed dictionaries are presented. The contribution of this work has several facets: first, while sparse and redundant representations and learned dictionaries have shown to be effective in various image processing problems, their role in compression has been less explored, and this work provides the first evidence to its success in this arena as well. Second, the proposed scheme is very practical, and could be the foundation for systems that use large databases of face images. Third, among the various ways to imitate the VQ and yet be practical, the proposed method stands as an interesting option that should be further explored.</p><p>As for future work, we are currently exploring several extensions of this activity, such as reducing or eliminating the much troubling blockiness effects due to the slicing to patches, generalization to compression of color images, and adopting the ideas in this work for compression of finger-prints images. The horizon and the ultimate goal, in this respect, is a successful harnessing of the presented methodology for general images, in a way that surpasses the JPEG2000 performance-we believe that this is achievable.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (Left) Piece-wise affine warping of the image by triangulation. (Right) A uniform slicing to disjoint square patches for coding purposes.</figDesc><graphic coords="2,315.04,67.92,244.27,148.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. A typical representation error (RMSE) as a function of the iterations of the K-SVD algorithm. This graph corresponds to a gradual increment of atoms in the representation ðLÞ from 1 to 7.</figDesc><graphic coords="3,305.35,67.92,244.07,189.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Detailed block diagram of the proposed encoding/decoding method.</figDesc><graphic coords="4,62.36,67.92,482.05,356.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. The Required average number of atoms L as a function of the patch size N, with the influence of the overall allocated number of bits B.</figDesc><graphic coords="5,305.35,67.92,244.07,191.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. The Dictionary obtained by K-SVD for Patch No. 80 (the left eye) using the OMP method with L ¼ 4.</figDesc><graphic coords="6,130.39,340.84,340.33,170.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. (a) The atom allocation map for 400 bytes, (b) the representation error map for 400 bytes, (c) the atom allocation map for 820 bytes, and (d) the representation error map for 820 bytes.</figDesc><graphic coords="7,117.75,437.61,351.74,292.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Fig.9. Examples of original (top) and reconstructed (bottom) images from the learning set, with a ''good" 4.04 (on the left) and a ''bad" 5.9 (on the right) representation RMSE, both using 630 bytes.</figDesc><graphic coords="7,305.23,67.92,244.19,299.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Fig.10. Examples of original and reconstructed images from the test set, with a ''good" 5.3 (left), an ''average" 6.84 (middle) and a ''bad" 11.2 (right) representation RMSE, all three using 630 bytes.</figDesc><graphic coords="8,116.22,67.92,368.80,299.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Comparing the visual quality of a reconstructed image from the test set in several bit-rates. From top left,clockwise: 285 bytes (RMSE 10.6), 459 bytes (RMSE 8.27), 630 bytes (RMSE 7.61), 820 bytes (RMSE 7.11), 1021 bytes (RMSE 6.8), original image.</figDesc><graphic coords="8,116.22,430.70,368.80,299.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 12 .</head><label>12</label><figDesc>Fig.12. Facial images compression with a bit-rate of 400 bytes. Comparing results of JPEG2000, the PCA results, and our K-SVD method. The values in the brackets are the representation RMSE.</figDesc><graphic coords="9,92.24,325.98,397.00,404.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Facial images compression with a bit-rate of 550 bytes. Comparing results of JPEG, JPEG2000, the PCA results, and our K-SVD method. The values in the brackets show the representation RMSE.</figDesc><graphic coords="10,53.86,67.92,496.06,404.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Facial images compression with a bit-rate of 820 bytes. Comparing results of JPEG, JPEG2000, the PCA results, and our K-SVD method. The values in the brackets show the representation RMSE.</figDesc><graphic coords="11,44.05,67.92,496.07,401.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. The effect of redundancy in the trained dictionaries on the quality of the test set images in PSNR for a fixed bit-rate (four different fixed values).</figDesc><graphic coords="12,138.90,352.63,324.39,249.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 15 .</head><label>15</label><figDesc>Fig.15. Rate-Distortion curves for the JPEG, JPEG2000, ''Clean" JPEG2000 (i.e., after removal of the header bits), PCA, VQ and the K-SVD methods.</figDesc><graphic coords="12,141.73,67.92,319.69,249.39" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>The PCA algorithm is developed in this work as a competitive benchmark, and while it is generally performing very well, it is inferior to the main algorithm presented in this work.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Due to the inconsistency between the image and patch sizes, some of the patches at the right and bottom boundaries are of different sizes than the rest.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>This is close to the actual size we use in our method, scaling down the original image as mentioned earlier by 2:1 in each axis as part of the pre-process stage. The actual size is 179 Â 221.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>O. Bryt, M. Elad / J. Vis. Commun. Image R. 19 (2008) 270-282</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors are grateful to Dr. Michal Aharon for her support and help throughout this work. We also thank Dr. Roman Goldenberg for sharing his work with us for comparison purposes.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>q This research was partly supported by the Israel Science Foundation Grant No.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Image Compression Fundamentals, Standards and Practice</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Taubman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Marcellin</surname></persName>
		</author>
		<author>
			<persName><surname>Jpeg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000. 2001</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Norwell, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An automatic system for model-based coding of faces</title>
		<author>
			<persName><forename type="first">B</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of DCC &apos;95 Data Compression Conference</title>
		<meeting>DCC &apos;95 Data Compression Conference<address><addrLine>Snowbird, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-03">March 1995</date>
			<biblScope unit="page" from="28" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Compression of personal identification pictures using vector quantization with facial feature correction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical-Engineering</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="198" to="203" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic interpretation and coding of face images using flexible models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lanitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="743" to="756" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Model-based multi-stage compression of human face images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sakalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kondo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 14th International Conference on Pattern Recognition (ICPR)</title>
		<meeting>14th International Conference on Pattern Recognition (ICPR)<address><addrLine>Brisbane, Qld., Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-08">August 1998</date>
			<biblScope unit="page" from="16" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Feature-based compression of human face images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sakalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical-Engineering</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1520" to="1529" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Compression of color facial images using feature correction two-stage vector quantization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="102" to="109" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A region-based scheme using RKLT and predictive classified vector quantization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sakalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="269" to="280" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Linear image coding for regression and classification using the tensor-rank principle</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Kauai, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-12">December 2001</date>
			<biblScope unit="page" from="8" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Class-adapted image compression using independent component analysis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A T</forename><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Image Processing (ICIP)</title>
		<meeting>the International Conference on Image Processing (ICIP)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-09">September 2003</date>
			<biblScope unit="page" from="14" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image compression using orthogonalized independent components bases</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A T</forename><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE XIII Workshop on Neural Networks for Signal Processing</title>
		<meeting>the IEEE XIII Workshop on Neural Networks for Signal Processing<address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-09">September 2003</date>
			<biblScope unit="page" from="17" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Segmentation based coding of human face images for retrieval</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">N</forename><surname>Gerek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hatice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal-Processing</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="1041" to="1047" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DSVD: a tensor-based image compression and recognition method</title>
		<author>
			<persName><forename type="first">K</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Urahama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<meeting>the IEEE International Symposium on Circuits and Systems (ISCAS)<address><addrLine>Kobe, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">Mat 2005</date>
			<biblScope unit="page" from="23" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Color personal ID photo compression based on object segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiuyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Suozhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Pacific Rim Conference on Communications, Computers and Signal Processing (PACRIM)</title>
		<meeting>the Pacific Rim Conference on Communications, Computers and Signal Processing (PACRIM)<address><addrLine>Victoria, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-08">August 2005</date>
			<biblScope unit="page" from="24" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sparse image coding using a 3D non-negative tensor factorization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Polak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the 10th IEEE International Conference on Computer Vision (ICCV)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-10">October 2005</date>
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the use of independent component analysis for image compression</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A T</forename><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="378" to="389" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Low bit-rate compression of facial images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goldenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="2379" to="2383" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Matching pursuits with time-frequency dictionaries</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="3397" to="3415" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Pati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rezaiifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Krishnaprasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Rec. 27th Asilomar Conf. Signals, Syst. Comput</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Greed is good: algorithmic results for sparse approximation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="2231" to="2242" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Just relax: convex programming methods for subset selection and sparse approximation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1030" to="1051" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Atomic decomposition by basis pursuit</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Saunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="129" to="159" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sparse signal reconstruction from limited data using FOCUSS: a re-weighted norm minimization algorithm</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">F</forename><surname>Gorodnitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="600" to="616" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Uncertainty principles and ideal atomic decomposition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="2845" to="2862" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Optimally sparse representation in general (nonorthogonal) dictionaries via &apos; 1 minimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences of the United States of America</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="2197" to="2202" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stable recovery of sparse overcomplete representations in the presence of noise</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Temlyakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="page" from="185" to="195" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the uniqueness of overcomplete dictionaries, and a practical way to retrieve them</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Linear Algebra and Applications</title>
		<imprint>
			<biblScope unit="volume">416</biblScope>
			<biblScope unit="page" from="48" to="67" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The K-SVD: an algorithm for designing of overcomplete dictionaries for sparse representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="4311" to="4322" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image denoising via sparse and redundant representations over learned dictionaries</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="3736" to="3745" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An algorithm for vector quantiser design</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Linde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="84" to="95" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Vector Quantization And Signal Compression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Dordrecht, Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Vector quantization of images subbands: a survey</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cosman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vetterli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="202" to="225" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The Laplacian pyramid as a compact image code</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="532" to="540" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image compression using an edge adapted redundant dictionary and wavelets</title>
		<author>
			<persName><forename type="first">L</forename><surname>Peotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Granai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal-Processing</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="444" to="456" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
