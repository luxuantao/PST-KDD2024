<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ClusterGNN: Cluster-based Coarse-to-Fine Graph Neural Network for Efficient Feature Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-04-25">25 Apr 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yan</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun-Xiong</forename><surname>Cai</surname></persName>
							<email>caijunxiong000@163.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yoli</forename><surname>Shavit</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Engineering</orgName>
								<orgName type="institution">Bar Ilan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tai-Jiang</forename><surname>Mu</surname></persName>
							<email>taijiang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wensen</forename><surname>Feng</surname></persName>
							<email>fengwensen@huawei.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
							<email>zhangkai@sz.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ClusterGNN: Cluster-based Coarse-to-Fine Graph Neural Network for Efficient Feature Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-25">25 Apr 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2204.11700v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) with attention have been successfully applied for learning visual feature matching. However, current methods learn with complete graphs, resulting in a quadratic complexity in the number of features. Motivated by a prior observation that self-and cross-attention matrices converge to a sparse representation, we propose ClusterGNN, an attentional GNN architecture which operates on clusters for learning the feature matching task. Using a progressive clustering module we adaptively divide keypoints into different subgraphs to reduce redundant connectivity, and employ a coarse-tofine paradigm for mitigating miss-classification within images. Our approach yields a 59.7% reduction in runtime and 58.4% reduction in memory consumption for dense detection, compared to current state-of-the-art GNN-based matching, while achieving a competitive performance on various computer vision tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Finding correspondences between images is an essential task for many computer vision applications such as Simultaneous Localization and Mapping (SLAM) <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b20">20]</ref>, Structure-from-Motion (SfM) <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b44">43]</ref> and camera pose estimation <ref type="bibr" target="#b49">[48]</ref>. Given a pair of images, correspondences can be established through point-to-point feature matching. Classical pipelines typically obtain correspondences with a nearest neighbor (NN) search of feature descriptors and reject outliers based on their match score or using a mutual NN check. Such methods focus only on the local similarity between feature descriptors while ignoring geometric information and the global receptive field.</p><p>Recent works <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b36">35]</ref> have proposed to learn the task of feature matching using graph neural networks (GNNs) Figure <ref type="figure">1</ref>. Sparse attention in cluster-based feature matching. Each keypoint interacts only with points within its cluster (proposed method) instead of interacting with all keypoints as in current GNN-based feature matching <ref type="bibr" target="#b30">[30]</ref>. and attention. In SuperGlue <ref type="bibr" target="#b30">[30]</ref>, Transformer <ref type="bibr" target="#b41">[40]</ref> based GNNs are applied on the complete graph of keypoints within (intra graph) and between (inter graph) images. Each node is represented with an encoded keypoint descriptor and updated using self-and cross-multi-head attention, while alternating between the intra-and inter-complete graphs, respectively. Learning complete graphs with attention suffers from a computational and memory complexity which is quadratic in N, where N is the number of keypoints. However, keypoints typically show a strong correlation with just a small number of points (sparse adjacency matrix). Furthermore, in the context of feature matching, a large portion of keypoints are non-repeatable and irrelevant for matching. A complete graph representation is thus redundant and results in wasteful attention-based message passing.</p><p>Efforts to reduce the quadratic complexity of attention mainly focused on self-attention. For example, reducing the attention dimension by splitting the input sequence into local windows <ref type="bibr" target="#b23">[23]</ref> or by approximating attention with kernels <ref type="bibr" target="#b8">[8]</ref>. However, these works are less suitable for fea-ture matching, where we are required to perform self-and cross-attention on features within and between images, respectively. Inspired by the Routing Transformer <ref type="bibr" target="#b27">[27]</ref>, we propose a coarse-to-fine cluster-based GNN to learn the feature matching task with a lower redundancy and computational complexity. We extract query and key features of keypoints across images to classify the points with strong correlation into the same cluster and establish local graphs using points from the same category. Each point interacts only with points in the same local graph resulting with a sparser attention computation (Fig. <ref type="figure">1</ref>) . Since clustering keypoints across images may lead to an erroneous grouping within images, we take a coarse-to-fine approach and first divide the points into a small number of major clusters which are gradually divided into multiple smaller clusters. We evaluate our method on multiple tasks, namely: relative pose estimation, homography estimation and visual localization. Our model achieves state-of-the-art accuracy across tasks, with a significant improvement in efficiency for dense detection (59.7% and 58.4% reduction in runtime and memory, respectively).</p><p>In summary, our contributions are as follows:</p><p>1. We present a learnable, coarse-to-fine clustering method to establish local graphs for feature matching, which reduces the spread of redundant information and makes message passing more effective.</p><p>2. We introduce the ClusterGNN architecture, an attentional GNN for learning the feature matching task using gradually forming clusters, approximating attention on complete graphs.</p><p>3. The proposed ClusterGNN method achieves state-of the-art results on various tasks, with a significant reduction in runtime and memory consumption on dense detection (59.7% and 58.4%, respectively) compared to the current leading feature matching method (Su-perGlue).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Visual Feature matching. Matching features between a pair of images is used in different computer vision applications, such as visual localization and relative pose estimation. In classical pipelines, keypoints are first detected and described using hand-crafted methods such as SIFT <ref type="bibr" target="#b18">[18]</ref> and ORB <ref type="bibr" target="#b28">[28]</ref>. The nearest neighbor of each descriptor is obtained based on Euclidean distances, followed by filtering of incorrect matches using mutual consistency check, Lowe's ratio test <ref type="bibr" target="#b18">[18]</ref>, matching scores and neighborhood consensus.</p><p>In recent years, deep learning methods have been employed for improving different aspects of the feature match-ing pipeline. Learning-based feature detectors and descriptors such as SuperPoint <ref type="bibr" target="#b10">[10]</ref>, ASLFeat <ref type="bibr" target="#b19">[19]</ref> and R2D2 <ref type="bibr" target="#b25">[25]</ref>, were proposed for learning more robust, discriminative and repeatable representations of keypoints. Other methods focused instead on the matching and filtering tasks <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b45">44]</ref>. A significant improvement in matching accuracy was recently achieved by SuperGlue <ref type="bibr" target="#b30">[30]</ref>, an attentional GNN architecture, which establishes complete graphs over keypoints within and between images and update their representations with an attention-based message passing. Albeit, since the runtime and memory complexity of attention is quadratic in the number of nodes, operating on the complete graph does not scale well. This in turns limits usability of SuperGlue, especially when matching a large number of points. In this work, we build on the success of Super-Glue and propose a sparser alternative to reduce the propagation of redundant messages, achieving a significant decrease in runtime and memory while preserving matching performance.</p><p>Efficient Attention. The attention mechanism was popularized through the Transformer architecture <ref type="bibr" target="#b41">[40]</ref>, achieving state-of-the-art results across different natural language processing and computer vision tasks <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b12">12]</ref>. In the context of attentional GNNs, Transformers can be viewed a graph-like model operating on the complete graph of tokens, which are updated using self-and cross-attention <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b42">41]</ref>. Since Transformers (and attentional GNNs operating on complete graphs) are hampered by the quadratic complexity of attention in the sequence length (number of nodes), different methods were recently proposed to sparsify connections or linearize the attention complexity.</p><p>In <ref type="bibr" target="#b5">[5]</ref>, a small set of reliable nodes are established as seeds to reduce the cost of attention. In <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23]</ref>, data independent or fixed sparsity patterns were proposed to bound temporal dependencies, such as local or strided attention. Drawing inspiration from CNN networks, those works suggested to apply attention within a fixed local neighborhood, which bounds the complexity, but limits the ability to establish long range interactions. In the context of feature matching, such methods may degrade performance since the distribution of keypoints is not regular, and the two matched images may have large scale and viewing angle differences. Other approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b43">42]</ref> proposed to approximate attention by either lowering the sequence dimension through pooling or lowering the attention matrix dimension using low-rank methods. Such approximations, however, include assumptions which are less appropriate for feature matching.</p><p>Inspired by content-based sparse attention <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b27">27]</ref>, which use data-driven methods to cluster tokens and operate within clusters, we propose a learned clustering module, which utilizes k-means to construct sub-graphs in a coarseto-fine module, and passes messages within each local clus- ter graph for saving memory and computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Definition</head><p>Given an image pair (I a , I b ) and extracted keypoints (K a , K b ) with corresponding confidence scores (C a , C b ) and descriptors (D a , D b ), the feature matching problem is defined as pairing keypoints which match in real-world coordinates:</p><formula xml:id="formula_0">M a,b = {(i, j)| T a (K (i) a ) -T b (K (j) b ) ? }<label>(1)</label></formula><p>where T x represent the function which transfer pixel coordinates to world coordinates. In practice, T x is usually obtained by homography matrix estimation based on predicted matches. When faced with large differences between camera views and long-term environmental changes, traditional methods which rely on NN search often produce wrong matches. Recent GNN-based methods apply attention in an iterative manner to remove matching-unrelated environmental noise and learn global geometric distribution information in order to achieve optimal matching and increase matching robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Motivation</head><p>Existing works <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b36">35]</ref> which learn the feature matching task with attention-based GNNs, use densely connected graphs. However, as shown in Fig. <ref type="figure" target="#fig_1">3a</ref>, a large portion of keypoints are non-repeatable and irrelevant for feature matching. In addition, the respective self-and cross-attention matrices tend to converge to sparse matrices (visualized in Fig. <ref type="figure" target="#fig_1">3b</ref>), where most of the attention values are distributed around zero (Fig. <ref type="figure" target="#fig_1">3c</ref>). Therefore, it is important to design a sparse structure for efficient feature matching.</p><p>In order to learn over sparse graphs and reduce redundancy, we extract query and key features of keypoints to classify strongly correlated points into the same cluster (Fig. <ref type="figure" target="#fig_1">3d</ref>). The points in each cluster are used to build a small sub-graph. Using attention, we can then pass information and update keypoints representation within each local subgraph. Due to different descriptor statistics between images, direct classification may cause points from the same image to be wrongly classified into the same cluster. In order to address this problem, we propose a cluster-based coarseto-fine paradigm and apply attentional GNN layers withing clusters for learning feature matching between two sets of feature points and their associated descriptors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Architecture</head><p>Similarly to <ref type="bibr" target="#b30">[30]</ref>, our method first applies a Complete Graph Initialization Module (Section 3.3.1), which constructs complete graphs over encoded keypoints and descriptors within and between images (intra-and intergraphs respectively), and updates them using attention. Instead of learning multiple attentional GNN layers over theses complete graphs, we design a ClusterGNN module (Section 3.3.2), which learns to hierarchically partition the complete graph into smaller sub-graphs, and then applies the attention update mechanism within these graphs. Fi-nally, the matching probability between keypoints is computed with a Matching Module (Section 3.3.3), based on the dot product of updated feature representations and the Dual-Softmax operator <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b36">35]</ref>. An overview of our proposed method is shown in Fig. <ref type="figure" target="#fig_0">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Complete Graph Initialization Module</head><p>Given input keypoints K, confidence scores C and local descriptors D, we generate a joint representation as in <ref type="bibr" target="#b30">[30]</ref> by adding descriptors and encoded keypoints and confidence scores. We then construct complete intra (within images) and inter (between images) graphs over the keypoints in (I a , I b ) and apply multi-head attention as in <ref type="bibr" target="#b41">[40]</ref> and <ref type="bibr" target="#b30">[30]</ref> for updating node representation. This construction and initialization process can be expressed as follows:</p><formula xml:id="formula_1">D 0 a , D 0 b = CA(SA(KE(I a )), SA(KE(I b ))), D i a , D i b = CA(SA(D i-1 a ), SA(D i-1 b )), KE(I x ) = D x + M LP (K x ? C x ),<label>(2)</label></formula><p>where SA/CA stands for self/cross attention (introduced below), KE is the keypoint encoding layer, and ? denotes the concatenation operator. For two input feature sets (F a , F b ), we implement attention-based GNNs to pass and aggregate messages between nodes, as follows:</p><formula xml:id="formula_2">CA(F a , F b ) = F a,b + M LP (F a,b ? Att(F a,b , F b,a )), Att(F a , F b ) = sof tM ax( Q a K T b ? dim )V b , (Q x , K x , V x ) = Linear (Q,K,V ) (F x ), x ? {a, b},<label>(3)</label></formula><formula xml:id="formula_3">SA(F x ) = CA(F x , F x ), x ? {a, b}.<label>(4)</label></formula><p>In practice, we use multi-head attention <ref type="bibr" target="#b41">[40]</ref> to improve the expressivity. In addition, all MLP operators are followed with batch normalization and ReLU before the last layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Cluster GNN Module</head><p>Cluster-based Sparse Attention The computational bottleneck of the attentional GNNs (Equation <ref type="formula" target="#formula_2">3</ref>) is due to the matrix multiplication operation between the query and the key matrices. However, since keypoints are likely to be correlated to only a small number of points (Fig. <ref type="figure" target="#fig_1">3b</ref>), it is desirable to operate over local sub-graphs (where the corresponding matrices are much smaller). The Cluster-GNN module implements a cluster-based sparse attention to approximate self-and cross-attention over complete graphs, as follows:</p><formula xml:id="formula_4">Att( F) = M ? sof tM ax( Q K T ? dim ) V , F = F a ? F b , Q = Q a ? Q b , K = K a ? K b , V = V a ? V b<label>(5)</label></formula><p>where M = {m ij } is the cluster matrix. m ij = 1 when query i (the ith query vector of F) and key j (the jth key vector of F) belong to the same cluster. Note that with this formulation, we no longer distinguish between features from different images (like the self/cross attention in Section 3.3.1), and directly operate on their union. While other works also apply the K-NN or top-k operators to determine the index matrix M , they do not apply well to cross attention in matching tasks. Learnable Hierarchical Clustering. The key to our clustering method is the definition of cluster centers and the discrepancy function. We note that the attention map itself provides a means for measuring similarity, where the attention weights between two features are determined according to their query and key vectors, as follows:</p><formula xml:id="formula_5">? i,j = exp(query i ? key j ) k exp(query i ? key k )<label>(6)</label></formula><p>Since the query and key vectors are different linear projections derived from the input features (Eq. 3), ? i,j = ? j,i is satisfied in most cases. Thus, instead of directly clustering the feature vectors, we propose to cluster the union space of the query and key vectors in order to get a better sparse approximation. Given k known cluster vectors {c i } and features {f i |f i ? Q ? K}, the discrepancy function is defined as follows:</p><formula xml:id="formula_6">dis(c i , f j ) = 1 - c i ? f j c i f j ,<label>(7)</label></formula><p>Using this function, we assign each feature with a cluster center index cid based on its closest cluster and construct the cluster Matrix M in Equation <ref type="formula" target="#formula_4">5</ref>. Note that, in practice, M does not need to be calculated explicitly.</p><p>During the training process, we initialize clusters centers with the k-means algorithm, and update them as follows:</p><formula xml:id="formula_7">c i = ?c i + (1 -?)( cidj =i f j ) (<label>8</label></formula><formula xml:id="formula_8">)</formula><p>where ? is a weight to balance the old and new value of c, which is set to 0.99. During test time, the trained cluster centers are directly used to cluster the input features, thus avoiding the iterative process used in traditional clustering algorithms. By using hierarchical clustering, we reduce the theoretical complexity of attention from O(n 2 ) to O(n 2 k -2 ), where n is the number of features and k is the number of clusters. Since k controls the tradeoff between the quality of approximation (better for small values of k) and the amount of calculations (smaller for large values of k), its value should be set with consideration. In theory, the best balance between clustering semantics and computational complexity can be reached when we set cluster number k = ? n <ref type="bibr" target="#b27">[27]</ref>. However, the input features processed in <ref type="bibr" target="#b27">[27]</ref> come from the same image and share similar statistics. For the image matching problem, the input features are keypoint descriptors extracted from two images, which may present significant style differences. Directly using the fixed ? n as the number of clusters can thus lead to over-segmentation, so that each cluster only contains features from the same image. In order to mitigate this behavior we apply clusterGNN with an increasing number of clusters, realizing a coarse-tofine semantic clustering (Fig. <ref type="figure" target="#fig_2">4</ref>). We further carry an ablation study to guide the choice of k and evaluate its effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Matching Module</head><p>We establish a matching confidence matrix C, by applying the dot product operator between the features computed with the ClusterGNN module, {F o a (i), i = 1, 2, ..., n} and {F o b (i), i = 1, 2, ..., m}:</p><formula xml:id="formula_9">C = {C i,j = F o a (i) ? F o b (j)}<label>(9)</label></formula><p>We further add an extra learnable dustbin channel on both row and column of the confidence matrix C ( C) in order to detect keypoints that are not matched. We adopt the Dual-softmax operator <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b36">35]</ref> for computing the matching probability matrix P, by applying the log-softmax operator on both the row and column dimensions of C, as follows:</p><formula xml:id="formula_10">P i,j = logSof tM ax( C i,? ) j + logSof tM ax( C ?,j ) i . (10)</formula><p>At test time, we predict the matches using the argmax operator and a mutual check mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss</head><p>The ground truth matches set M and non-matching keypoints set (M a , M b ) are supervised with the cross projection error (less than 3 pixels for matches and more than 5 pixels for non-matches). The matching loss L m is defined as:</p><formula xml:id="formula_11">L m = -|M| (i,j)?M P ij - M a i?Ma P i,m+1 -M b j?M b P n+1,j<label>(11)</label></formula><p>In order to achieve a better clustering effect and enable semi-supervision, we further introduce a clustering loss:</p><formula xml:id="formula_12">L c : L t c = i,j c t i -f t j .<label>(12)</label></formula><p>Our total loss consists of the matching loss L m and the clustering losses L t c computed across different clustering stages:</p><formula xml:id="formula_13">L = L m + ? t?{1,2,..,L} L t c ,<label>(13)</label></formula><p>where ? is set to 0.1 for balancing the two losses, t indicates the clustering stage and L is the total number of stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementation Details</head><p>We train our model on the MegaDepth dataset <ref type="bibr" target="#b17">[17]</ref>, a large outdoor dataset, including 1M internet images from 196 different locations and sparse 3D models computed with COLMAP <ref type="bibr" target="#b32">[32]</ref>. It also uses multi-view stereo to generate depth maps. We select training image pairs as in <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b21">21]</ref>, based on the overlap rate from the SfM co-visibility and resize images so that the larger edge is of size 1600.</p><p>Our model is optimized using Adam with an initial learning rate of 1 ? 10 -4 . We implement attention with a four heads multi-head attention throughout. Our Graph Initialization module consists of three stacked self/cross attention GNN layers. In order to reduce the GPU memory cost, we further chunk query vectors into four parts for this module. Our Cluster GNN module involves four stages with a different number of clusters, set as {16, 32, 64, 128}, respectively, where each stage consists of two cluster-based GNN attention layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Feature matching is a challenging task due to different factors such as occlusions, illumination and weather changes. We evaluate ClusterGNN on three different tasks, which heavily rely of feature matching, namely: pose estimation, homography estimation and visual localization.</p><p>We compare our method with NN-search, SGMNet <ref type="bibr" target="#b5">[5]</ref> and SuperGlue <ref type="bibr" target="#b30">[30]</ref>, using both hand crafted features (SIFT <ref type="bibr" target="#b18">[18]</ref>) and learning-based features (ASLFeat <ref type="bibr" target="#b19">[19]</ref> and Su-perPoint <ref type="bibr" target="#b10">[10]</ref>). For SGMNet, we retrain it ASLFeat using the official training code. For SuperGlue, since the official training code is not available and since its public model (denoted as Superglue * ) was trained on MegaDepth <ref type="bibr" target="#b17">[17]</ref>, Oxford and Paris datasets <ref type="bibr" target="#b24">[24]</ref>), we retrain it with different features following the training method described in the Su-perGlue paper. We report both our own implementation and the official results reported in the SuperGlue paper. We also provide analysis of computation and memory efficiency. All the reported experiments were run on a Tesla P-100 GPU with 16GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Pose Estimation</head><p>We use the YFCC100M <ref type="bibr" target="#b39">[38]</ref> dataset to evaluate the performance of ClusterGNN on the pose estimation task. This dataset provides a sparse reconstruction from SfM <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b33">33]</ref> and ground truth poses. Following <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b45">44,</ref><ref type="bibr" target="#b46">45]</ref>, we report the AUC of pose errors at different thresholds (5 ? , 10 ? , 20 ? ), where the pose errors are computed by the maximum angular differences between estimated results and ground truth in rotation and translation. For pose estimation, we use RANSAC as a post-processing tool to compute the essential matrix from predicted matches.</p><p>As shown in Table <ref type="table">1</ref>, ClusterGNN outperforms other methods with ASLFeat. When using SuperPoint, our method outperforms our re-implemented SuperGlue and SGMNet, with a slight degradation compared to the official version. When using SIFT, our method outperforms Su-perGlue, with a slight degradation compared to SGMNet. These results demonstrate the approximation and denoising abilities of ClusterGNN. Qualitative inspection (Fig. <ref type="figure">5</ref>), further shows that ClusterGNN detects a larger number of true matches compared to other methods, resulting in an improved pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Homography Estimation</head><p>We evaluate our method on the homography estimation task using the HPathces dataset <ref type="bibr" target="#b0">[1]</ref>. HPatches consist of 52 sequences that exhibit large illumination changes and 56 sequences under significant viewpoint changes. We follow the setup proposed in Patch2Pix <ref type="bibr" target="#b48">[47]</ref>, and report the percentage of correctly estimated homographies whose average corner error distance is below 1/3/5 pixels. For all compared methods, we apply the OpenCV RANSAC toolbox to estimate homography matrix. The local keypoints of SuperPoint <ref type="bibr" target="#b10">[10]</ref> and ASLFeat <ref type="bibr" target="#b19">[19]</ref> are both tested in detail. To make a fair comparison, we choose 4k keypoints for all methods and use the same hyper-parameters.</p><p>As shown in Table <ref type="table">2</ref>, ClusterGNN achieves competitive performance with a slight improvement compared to Super-Glue and SGMNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Visual localization</head><p>Visual localization is one of the most important applications of feature matching and its performance heavily relies on the matching quality. Given a query image, visual time with Sinkhorn iterations and dual softmax. It should be noted that memory consumption is consistent in both Sinhorn and dual softmax. As shown in Fig. <ref type="figure" target="#fig_3">6a</ref>, for 10k keypoints(dense detection), ClusterGNN reduces the runtime by 59.7%, compared to Superglue when using dual softmax. Although SGMNet is more time efficient, Clus-terGNN can achieve better memory efficiency and performances at the same time. As shown in Fig. <ref type="figure" target="#fig_3">6b</ref>,for dense detection, our method the proposed reduces 58.4% of the memory required by Superglue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>We conduct ablation studies on the YFCC100M dataset using ASLFeat <ref type="bibr" target="#b19">[19]</ref> for detecting keypoints and the same Table <ref type="table">6</ref>. The effect of different operators for computing matching probability. We report pose accuracy, processing time and GPU memory for the YFCC100M dataset using 2K keypoints.</p><p>experimental setting as in Section 4.1. Our ablation focuses on the effect of fixing or varying the number of clusters and on the operator used for generating the matching probability matrix.</p><p>Fixed vs. Varying Number of Clusters. Our Clus-terGNN strategy gradually decreases the size of clusters while increasing their number due to the tradeoff between over segmentation and efficiency. In this experiment we evaluate the effect of fixing the number of clusters versus our choice to gradually increase it. Table <ref type="table" target="#tab_0">5</ref> shows the results of fixing the number of cluster to {16, 32, 64, 128} versus the results of our varying strategy. ClusterGNN with a fixed number of cluster suffers from significant drop in performance.</p><p>Sinkhorn vs Dual Softmax. Superglue <ref type="bibr" target="#b30">[30]</ref> uses a differential iterative optimal transport,namely Sinkhorn <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b35">34]</ref>, to solve the matching confidence. In our work, we adopt non-iterative dual-softmax <ref type="bibr" target="#b26">[26]</ref> operator for efficiency. As shown in Table <ref type="table">6</ref>, dual-softmax achieves competitive performance compared with Sinkhorn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we have addressed the quadratic complexity of GNN methods for feature matching. The proposed method, named ClusterGNN, leverages on the inherent sparsity of self-and cross-attention between keypoints in the complete graph and dynamically constructs local subgraphs through a learned coarse-to-fine clustering. Extensive evaluation on several computer vision tasks demonstrates the effectiveness of our approach, achieving a competitive performance while reducing runtime and memory by 59.7% and 58.4%, respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The ClusterGNN architecture. Our model is composed on three components. A Complete Graph Initialization Module (Section 3.3.1) first constructs and updates complete inter-and intra-graphs with attentional GNN layers. In order to leverage on the inherent sparsity of keypoint attention maps, a ClusterGNN Module (Section 3.3.2), learns to hierarchically partition the joint complete graph into smaller sub-graphs before applying attention-base updates. Finally, a Matching Module (Section 3.3.3), establishes the matching probability matrix using dot product and the Dual Softmax operator. A learnable dustbin is further appended to account for non-matching keypoints.</figDesc><graphic url="image-2.png" coords="3,50.11,72.00,495.00,188.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. (a) Visualization of feature matching between two images. Keypoints and matches are represented by white points and colored lines, respectively where color varies between green to red according to the matching confidence (higher in red). (b) Cross attention matrix of 50 sampled keypoints from two images. (c) The distribution of attention values in an example row in the attention matrix. (d) The cluster attention matrix. The attention matrix within each cluster is framed with a red box.</figDesc><graphic url="image-5.png" coords="4,130.44,255.96,75.60,56.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Visualization of hierarchical clustering. We show two clusters (colored in red and blue, respectively) at four different stages of ClusterGNN. The hierarchical clustering enables coarseto-fine grouping</figDesc><graphic url="image-7.png" coords="5,308.86,72.00,236.26,420.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Efficiency Comparison. We report the time and memory consumption with an increasing number of input keypoints.</figDesc><graphic url="image-9.png" coords="8,51.23,391.38,115.76,86.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-8.png" coords="7,50.11,72.00,495.01,162.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 5 .</head><label>5</label><figDesc>The effect of fixing or varying the number of clusters). We report pose accuracy under different thresholds based on YFCC100M using 2K keypoints.</figDesc><table><row><cell>Methods</cell><cell></cell><cell></cell><cell cols="3">Pose estimation</cell></row><row><cell></cell><cell></cell><cell></cell><cell>5 ?</cell><cell>10 ?</cell><cell>20 ?</cell></row><row><cell>Fixed Cluster:16</cell><cell></cell><cell cols="2">31.26</cell><cell>48.55</cell><cell>64.50</cell></row><row><cell>Fixed Cluster:32</cell><cell></cell><cell cols="2">37.05</cell><cell>56.54</cell><cell>72.90</cell></row><row><cell>Fixed Cluster:64</cell><cell></cell><cell cols="2">37.44</cell><cell>56.58</cell><cell>72.68</cell></row><row><cell cols="2">Fixed Cluster:128</cell><cell cols="2">36.77</cell><cell>55.90</cell><cell>72.08</cell></row><row><cell>ClusterGNN</cell><cell></cell><cell cols="2">42.62</cell><cell>61.22</cell><cell>76.75</cell></row><row><cell>Methods</cell><cell></cell><cell cols="3">Pose estimation</cell><cell>Time</cell><cell>Memory</cell></row><row><cell></cell><cell>5 ?</cell><cell></cell><cell>10 ?</cell><cell>20 ?</cell><cell>(ms)</cell><cell>(MB)</cell></row><row><cell>ClusterGNN w sinkhorn</cell><cell cols="2">38.83</cell><cell>58.87</cell><cell>75.11</cell><cell>68</cell><cell>94</cell></row><row><cell>ClusterGNN w dual softmax</cell><cell cols="2">42.62</cell><cell>61.22</cell><cell>76.75</cell><cell>61</cell><cell>94</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>(a) The comparison of time (b) The comparison of memory</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>localization aims to estimate its 6-DOF position, based on a 3D reconstructed model. We integrate our method into the official HLoc <ref type="bibr" target="#b29">[29]</ref> pipeline for visual localization and evaluate it on the Long-Term Visual Localization Benchmark <ref type="bibr" target="#b40">[39]</ref>. This benchmark assesses performance under different conditions, such as texture-less scenes of indoor environments and day-and-night changes, thus requiring highly robust matching. Specifically, we use the Aachen Day-Night dataset <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b47">46]</ref> for evaluating outdoor localization and the InLoc dataset <ref type="bibr" target="#b38">[37]</ref> for evaluating indoor localization. of the Aachen city and 922 query images including 824 daytime images and 98 nighttime images taken by mobile phone cameras. The InLoc dataset <ref type="bibr" target="#b38">[37]</ref> offers 9972 reference images and 329 query images which contain significant occlusions and variation in viewpoint and illumination. We use the official HLoc <ref type="bibr" target="#b29">[29]</ref> pipeline for visual localization tasks. Consistent with the official benchmark, we report the pose estimation accuracy under different thresholds. Tables <ref type="table">3</ref> and<ref type="table">4</ref> report the results for indoor and outdoor localization, respectively. ClusterGNN achieves competitive performance compared to Superglue on both indoor and outdoor localization tasks, across different features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Efficiency</head><p>Improving the processing efficiency and GPU memory requirements, while maintaining or improving matching performance is the main motivation for developing Clus-terGNN. In this section, we compare the runtime and memory of our method with SGMNet and SuperGlue.</p><p>Fig. <ref type="figure">6a</ref> and<ref type="figure">6b</ref>, report the time and memory consumption for different numbers of detected keypoints on a Telsa P-100 GPU with 16GB memory. Specifically, we test both run </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hpatches: A benchmark and evaluation of handcrafted and learned local descriptors</title>
		<author>
			<persName><forename type="first">Vassileios</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5173" to="5182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>Corr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2004.05150, 2020. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural-guided ransac: Learning where to sample model hypotheses</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4322" to="4331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to match features with seeded graph matching network</title>
		<author>
			<persName><forename type="first">Hongkai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuyang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiew-Lan</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2021. 1, 2, 6</date>
			<biblScope unit="page" from="6301" to="6310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904">1904.10509, 2019. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Masked language modeling for proteins via linearly scalable long-context transformers</title>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Colwell</surname></persName>
		</author>
		<idno>CoRR, abs/2006.03555</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14794</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2292" to="2300" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Superpoint: Self-supervised interest point detection and description</title>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Daniel Detone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2008">2018. 2, 6, 7, 8</date>
			<biblScope unit="page" from="224" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno>ICLR, 2021. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">D2-net: A trainable cnn for joint detection and description of local features</title>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Dusmanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8092" to="8101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Direct sparse odometry</title>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="611" to="625" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep graph matching consensus</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Megadepth: Learning singleview depth prediction from internet photos</title>
		<author>
			<persName><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2041" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName><surname>David G Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Aslfeat: Learning local features of accurate shape and localization</title>
		<author>
			<persName><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuyang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2020. 2, 6, 7, 8</date>
			<biblScope unit="page" from="6589" to="6598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Orbslam: A versatile and accurate monocular slam system</title>
		<author>
			<persName><forename type="first">Ra?l</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">D</forename><surname>Tard?s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1147" to="1163" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lf-net: Learning local features from images</title>
		<author>
			<persName><forename type="first">Yuki</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwang</forename><surname>Moo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6237" to="6247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4055" to="4064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Blockwise self-attention for long document understanding</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Revisiting oxford and paris: Large-scale image retrieval benchmarking</title>
		<author>
			<persName><forename type="first">Filip</forename><surname>Radenovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmet</forename><surname>Iscen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5706" to="5715" />
		</imprint>
	</monogr>
	<note>Giorgos Tolias, Yannis Avrithis, and Ond?ej Chum</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">repeatable and reliable detector and descriptor</title>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C?sar</forename><forename type="middle">De</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noe</forename><surname>Pion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yohann</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">R</forename><surname>Humenberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="12405" to="12415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neighbourhood consensus networks</title>
		<author>
			<persName><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1658" to="1669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Orb: An efficient alternative to sift or surf</title>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2564" to="2571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">From coarse to fine: Robust hierarchical localization at large scale</title>
		<author>
			<persName><forename type="first">Paul-Edouard</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cesar</forename><surname>Cadena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Dymczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="12716" to="12725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Superglue: Learning feature matching with graph neural networks</title>
		<author>
			<persName><forename type="first">Paul-Edouard</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2020. 1, 2, 3, 4, 6, 8</date>
			<biblScope unit="page" from="4938" to="4947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image retrieval for image-based localization revisited</title>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leif</forename><surname>Kobbelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4" to="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Structurefrom-motion revisited</title>
		<author>
			<persName><forename type="first">L</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Michael</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4104" to="4113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pixelwise view selection for unstructured multi-view stereo</title>
		<author>
			<persName><forename type="first">Enliang</forename><surname>Johannes L Sch?nberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Michael</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="page" from="501" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Concerning nonnegative matrices and doubly stochastic matrices</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Sinkhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Knopp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pacific Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="343" to="348" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Detector-free local feature matching with transformers</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Loftr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005">2021. 1, 3, 4, 5</date>
			<biblScope unit="page" from="8922" to="8931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Acne: Attentive context normalization for robust permutation-equivariant learning</title>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwang</forename><surname>Moo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11286" to="11295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Inloc: Indoor visual localization with dense matching and view synthesis</title>
		<author>
			<persName><forename type="first">Hajime</forename><surname>Taira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masatoshi</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7199" to="7209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<author>
			<persName><forename type="first">Bart</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerald</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damian</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Yfcc100m: The new data in multimedia research</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="64" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Long-term visual localization revisited</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Toft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Hammarstrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Stenborg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Safari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masatoshi</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2004">2017. 1, 2, 4</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning combinatorial embedding networks for deep graph matching</title>
		<author>
			<persName><forename type="first">Runzhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3056" to="3065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno>CoRR, abs/2006.04768</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Towards linear-time incremental structure from motion</title>
		<author>
			<persName><forename type="first">Changchang</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="127" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning to find good correspondences</title>
		<author>
			<persName><forename type="first">Kwang</forename><surname>Moo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuki</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning two-view correspondences and geometry using order-aware network</title>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongen</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5845" to="5854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Reference pose generation for visual localization via learned features and view synthesis</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="821" to="844" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Patch2pix: Epipolar-guided pixel-level correspondences</title>
		<author>
			<persName><forename type="first">Qunjie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Leal-Taixe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4669" to="4678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">To learn or not to learn: Visual localization from essential matrices</title>
		<author>
			<persName><forename type="first">Qunjie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Leal-Taixe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3319" to="3326" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
