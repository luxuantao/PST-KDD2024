<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-03-11">11 Mar 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Manli</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guangzhen</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haoyu</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yizhao</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guoxing</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingyuan</forename><surname>Wen</surname></persName>
							<email>jrwen@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Heng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Baogui</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weihao</forename><surname>Zheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zongzheng</forename><surname>Xi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yueqian</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anwen</forename><surname>Hu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinming</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruichen</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yida</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Liang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuqing</forename><surname>Song</surname></persName>
							<email>rsong@ruc.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Hong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wanqing</forename><surname>Cui</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Danyang</forename><surname>Hou</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yingyan</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junyi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peiyu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zheng</forename><surname>Gong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chuhao</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuchong</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shizhe</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
							<email>luzhiwu@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qin</forename><surname>Jin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruihua</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-03-11">11 Mar 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2103.06561v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-modal pre-training models have been intensively explored to bridge vision and language in recent years. However, most of them explicitly model the cross-modal interaction between image-text pairs, by assuming that there exists strong semantic correlation between the text and image modalities. Since this strong assumption is often invalid in real-world scenarios, we choose to implicitly model the cross-modal correlation for large-scale multi-modal pretraining, which is the focus of the Chinese project 'Wen-Lan' led by our team. Specifically, with the weak correlation assumption over image-text pairs, we propose a two-tower pre-training model within the cross-modal contrastive learning (CMCL) framework. Unlike OpenAI CLIP that adopts a simple contrastive learning method, we devise a more advanced algorithm by adapting the latest method MoCo into the cross-modal scenario. By building a large queue-based dictionary, our CMCL can incorporate more negative samples in limited GPU resources. We further construct a large Chinese multi-source imagetext dataset called RUC-CAS-WenLan for pre-training our CMCL model. Extensive experiments demonstrate that the pre-trained CMCL model outperforms both UNITER and OpenAI CLIP on various downstream tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, pre-training models have become topical in natural language processing (NLP). A number of pretraining language models such as BERT <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b18">19</ref>] and * Co-corresponding authors.</p><p>"There are several burning candles on a fruit cake."</p><p>"Happy Birthday! Make a wish." GPT <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b2">3]</ref> have achieved significant improvements on various downstream NLP tasks. With the release of GPT-3 <ref type="bibr" target="#b2">[3]</ref> (i.e., the latest large-scale language model of OpenAI), pre-training language models <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b14">15]</ref> have now drawn the most attention of the NLP community.</p><note type="other">Strong Correlation Weak Correlation</note><p>Compared with text understanding in the single-modal scenario, understanding multiple modalities is more attractive and has a broader rang of application scenarios. In fact, with the success of pre-training models in NLP <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29]</ref>, they have recently been extended to understand the text and the image simultaneously, that is, multi-modal pretraining models have been intensively explored to bridge vision and language in the last two years. Particularly, in January 2021, OpenAI released a multi-modal version of GPT-3 <ref type="bibr" target="#b2">[3]</ref> called DALL•E <ref type="bibr" target="#b0">[1]</ref>, demonstrating its excellent textto-image generation capability. This clearly declares the power of multi-modal pre-training, and also encourages researchers to explore the potential of large-scale multi-modal pre-training in the vision+language area.</p><p>Along this line of research, our team started a Chinese project called 'WenLan' on large-scale multi-modal pretraining since September 2020, and released the first version to demonstrate its understanding ability on the Chinese multi-modal data. At this moment, our released model presents the strong image-text retrieval ability.</p><p>As we have mentioned, with the considerable progress made by pre-training models, multi-modal pre-training has started to attract significant attention from machine learning, computer vision, and natural language processing in recent years, i.e., it has now been a hot interdisciplinary research topic. Note that multi-modal pre-training models typically adopt two network architectures: single-tower architecture (e.g., UNITER <ref type="bibr" target="#b5">[6]</ref>) and two-tower architecture (e.g., OpenAI CLIP <ref type="bibr" target="#b25">[26]</ref>). In this project, similar to OpenAI CLIP, 'WenLan' adopts a two-tower architecture within the cross-modal contrastive learning (CMCL) framework. Our motivation for model design is detailed below.</p><p>Most existing multi-modal pre-training models, especially those with the single-tower architecture <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14]</ref>, take an assumption that there exists strong semantic correlation between the input imagetext pair. With this strong assumption, the interaction between image-text pairs can thus be modeled with crossmodal transformers. However, in real-world application scenarios, the strong correlation assumption is often invalid. For example, there often exists only weak correlation between image-text pairs, as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. Moreover, we also conduct extensive experiments and find that the performance of the two-tower models is significantly better than that of the simple-tower models on the noisy imagetext data (e.g., crawled from the Web). In this project, we thus choose the two-tower architecture to devise our largescale multi-modal pre-training model.</p><p>Specifically, given the web-crawled image-text data for pre-training, we need to design a multi-modal pre-training model based on the two-tower architecture. However, such network architecture is too simple (without fine-grained cross-modal interaction like UNITER) and its representation ability must be enforced for multi-modal pre-training. Thanks to the recent progress of self-supervised learning <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b1">2]</ref>, contrastive learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b4">5]</ref> has been found to significantly improve the representation ability of deep neural networks. Following this idea, we introduce comparative learning into our two-tower architecture. However, unlike OpenAI CLIP <ref type="bibr" target="#b25">[26]</ref> that adopts a simple contrastive learning method, we propose a more advanced cross-modal contrastive learning algorithm. As illustrated in Figure <ref type="figure" target="#fig_2">2</ref>, given a specific image-text pair, the image modality or the text modality can be used to construct absent samples of the image-text pair, and the number of negative samples is expanded based on the latest MoCo <ref type="bibr" target="#b15">[16]</ref> framework to improve the representation ability of the neu-  ral network. By building a large queue-based dictionary, our model can incorporate more negative samples in limited GPU resources, leading to even better results in image-text retrieval. Since the true cross-modal contrastive learning is included, our proposed model is called as CMCL.</p><p>Due to the usage of the two-tower architecture as well as the contrastive-learning based pre-training strategy, our proposed CMCL model has a high flexibility and can be readily deployed in real-world application scenarios. It mainly has three advantages: (i) With a two-tower architecture, the text encoder and the image encoder can be easily replaced with the latest larger single-modal pre-training models, further enforcing the representation ability of our CMCL model. (ii) Once our CMCL model is pre-trained, it can provide cloud-accessible APIs of the image and text feature embeddings as well as the matching score of an image-text pair, which are very convenient to be deployed in various downstream tasks. Particularly, when a vector engine is used to speed up the inference stage, the efficiency of image-text retrieval can be significantly improved. (iii) It is convenient to add other pre-training tasks (e.g., image-to-text generation) into our CMCL model. Note that our image-to-text generation (i.e., image captioning) model achieves the new state-of-the-art on the AIC-ICC <ref type="bibr" target="#b33">[34]</ref>  ment, culture, and other topics. In the near future, this pre-training dataset will be enlarged to 5 billion imagetext pairs. ( <ref type="formula" target="#formula_1">2</ref>) We have proposed the first large-scale Chinese multi-modal pre-training model called CMCL. The first version of our CMCL model pre-trained on RUC-CAS-WenLan has 1 billion parameters. Importantly, our CMCL model outperforms both UNITER <ref type="bibr" target="#b5">[6]</ref> and OpenAI CLIP <ref type="bibr" target="#b25">[26]</ref> on the RUC-CAS-WenLan test set and AIC-ICC <ref type="bibr" target="#b33">[34]</ref> test set. In the near future, our CMCL model will contain 10 billion parameters, which will be pre-trained with 400 million image-text pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methodology</head><p>Our cross-modal pre-training model is defined based on the image-text retrieval task. Our main goal is thus to learn two encoders that can embed image and text samples into the same space for effective image-text retrieval. To enforce such cross-modal embedding learning, we introduce contrastive learning with the InfoNCE loss <ref type="bibr" target="#b22">[23]</ref> into our pre-training model, and thus call it as cross-modal contrastive learning (CMCL) (see Figure <ref type="figure" target="#fig_3">3</ref>). Specifically, for a given text embedding, our learning objective is to find the best image embedding from a batch of image embeddings. Similarly, for a given image embedding, our learning objective is to find the best text embedding from a batch of text embeddings. In one word, our pre-training model learns a cross-modal embedding space by jointly training the image and text encoders to maximize the cosine similarity of the image and text embeddings of the true pair for each sample in the batch while minimizing the cosine similarity of the embeddings of the other incorrect pairs. This results in an InfoNCE loss over each batch of image-text pairs for pre-training our CMCL model. Note that our model can in-corporate more negative samples in limited GPU resources comparing to OPENAI CLIP, leading to even better results in image-text retrieval (see Section 3.3).</p><p>Formally, for the image-text retrieval task, we denote the training set as</p><formula xml:id="formula_0">D = (x I i , x T i )|i = 1, • • • , N , where (x I i , x T i</formula><p>) is a matched image-text pair from the RUC-CAS-WenLan dataset, and N is the size of D. Our image-text retrieval model leverages contrastive learning and expands the latest MoCo <ref type="bibr" target="#b15">[16]</ref> as the pre-training framework, as illustrated in Figure <ref type="figure" target="#fig_3">3</ref>. Each image x I i (or each text x T i ) is encoded by the image encoder f I (or text encoder f T ) to obtain its 1-D embedding z I i (or z T i ). The image encoder (see Figure <ref type="figure" target="#fig_3">3(b)</ref>) contains a CNN backbone and a successive self-attention block. A sequence of object embeddings is obtained using a object detector to downsample the feature map from CNN and then encoded by the self-attention block. The text encoder is stacked by several self-attention blocks such as RoBERTa <ref type="bibr" target="#b20">[21]</ref>. A two-layer Muti-Layer Perception (MLP) block with a RELU activation function is used for mapping each encoder's representation to the joint cross-modal embedding space. The parameters of f I and f T are denoted as θ I and θ T , respectively.</p><p>Note that MoCo provides a mechanism of building dynamic dictionaries for contrastive learning, which can be used with various pretext tasks. In this work, we adopt a simple instance discrimination task: a query of an image matches a key of an augmented text if the image corresponds to the text, and vice versa. Further, the introduction of a queue decouples the dictionary size from the minibatch size. As a result, the dictionary size can be much larger than a typical mini-batch size, and we can set it as a hyper-parameter. Given the momentum parameter m, two momentum-updated encoders f I m (with the parameters θ I m ) and f T m (with the parameters θ T m ) are kept for the image and text modalities, respectively. Their update rule is given by:</p><formula xml:id="formula_1">θ I m = m • θ I m + (1 − m) • θ I (1) θ T m = m • θ T m + (1 − m) • θ T<label>(2)</label></formula><p>Let the image set</p><formula xml:id="formula_2">D I = {x I i |(x I i , x T i ) ∈ D, i = 1, • • • , N } and the text set D T = {x T i |(x I i , x T i ) ∈ D, i = 1, • • • , N }. An image augmentation function t(•) is ap- plied to D I . For each image x I i ∈ D I and each text x T i ∈ D T (i = 1, 2, • • • , N</formula><p>), we denote the encoded image feature vector of x I i as z I i = f I (t(x I i )), and the encoded text feature vector of x T i as</p><formula xml:id="formula_3">z T i = f T (x T i ). Two queues of negative keys Q I = {q I j |j = 1, • • • , K} and Q T = {q T j |j = 1, • • • , K}<label>are</label></formula><p>maintained, which would be updated during pre-training. The image negative set is</p><formula xml:id="formula_4">N I i = {f T (x T j )|j ∈ Q T },</formula><p>and the text negative set is</p><formula xml:id="formula_5">N T i = {f I (t(x I j ))|j ∈ Q I }.</formula><p>For query image I i , we compute the contrastive loss between the image query z I i and all the text keys. Since the text keys contain one positive key z T i and all the negative keys in N T i , the loss function is formed as an InfoNCE loss:</p><formula xml:id="formula_6">L I2T =− i log exp(z I i • z T i /τ ) exp(z I i • z T i /τ ) + q T j ∈N T i exp(z I i • q T j /τ )<label>(3)</label></formula><p>where the similarity is measured by dot product and the hyper-parameter τ is the temperature. Similarly, for query text, the InfoNCE loss is formulated as:</p><formula xml:id="formula_7">L T 2I =− i log exp(z T i • z I i /τ ) exp(z T i • z I i /τ ) + q I j ∈N I i exp(z T i • q I j /τ )<label>(4)</label></formula><p>The total loss function used for CMCL is defined as:</p><formula xml:id="formula_8">L cmcl = L I2T + L T 2I<label>(5)</label></formula><p>In the test/evaluation stage, the query image (or text) is also retrieved simply by the dot product defined over the output (i.e., embeddings) of the pre-trained encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset and Settings</head><p>Pre-Training Dataset Our CMCL model is pre-trained on a web-crawled multi-source image-text dataset. This dataset is part of the WenLan project, called as RUC-CAS-WenLan for short. RUC-CAS-WenLan collects image-text pairs from multi information sources on the Web, including news, encyclopedia (i.e., Baidu Baike) and Weibo. Images from these data sources are selected to form image-text pairs together with their corresponding text descriptions.</p><p>Since the obtained image-text pairs are crawled from the Web, there exist much noise in the original data. Thus, we then perform an elaborate cleaning process (e.g., stopword removal, topic extraction, length limit, and sensitive information detection) to filter out sensitive or low-quality pairs. To create the text vocabulary, we also employ topic models as a preprocessing approach to identifying more meaningful words by topics. Such an approach is also useful to balance the distribution of content in different information sources. For top-ranked topic words, we further enrich the related image-text pairs by issuing them as queries to the search engines. Finally, our dataset has contained 30 million image-text pairs covering a variety of topics or content categories, including news, art, education, sports, entertainment, games, and culture. Out of them, 11,000 pairs are randomly selected to form the test set.</p><p>Text Encoder As mentioned in Section 2, a text encoder consists of a textual backbone, a self-attention block, and a two-layer MLP. We choose the encoder of Chinese RoBERTa-Large<ref type="foot" target="#foot_0">1</ref> as our textual backbone. Note that RoBERTa-Large includes a total of 24 transformer layers with 1,024 hidden units and 16 heads. We only adopt the first 12 transformer encoder layers as our textual backbone. The self-attention block consists of 4 transformer layers, designed for capturing the relationships across the textual tokens. The two-layer MLP is used to project the textual embedding to the cross-modal embedding space. Image Encoder Following UNITER <ref type="bibr" target="#b5">[6]</ref>, we first employ pre-trained Faster-RCNN <ref type="bibr" target="#b29">[30]</ref> to detect object boundingboxes from each image. We further utilize EfficientNet L2 <ref type="bibr" target="#b31">[32]</ref> to extract image features for computation efficiency. By applying RoI pooling <ref type="bibr" target="#b11">[12]</ref> on the output of EfficientNet L2, we obtain the features of multiple objects and then combine them with a self-attention block (of 4 transformer layers). The fused object features are fed into a two-layer MLP and projected to the cross-modal embedding space. Implementation Details We utilize the momentumupdated history queue as in MoCo <ref type="bibr" target="#b15">[16]</ref> for contrastive learning. We adopt clip-wise random crops, horizontal flips, Gaussian blur, graying, and color jittering for data augmentation over input images. A non-linear projection head is attached to the text/image encoder to obtain feature vectors in the same size 2,560. Our CMCL model is trained with 100 epochs. We select hyper-parameters heuristically due to computational constraint: the learnable temperature parameter τ = 0.07, momentum m = 0.999, and the queue size is 16,384. We adopt the Adam optimizer with decoupled weight decay regularization over all weights that are not gains or biases, and decay the learning rate using a cosine schedule. We use a mini-batch size of 128 for each of the 16 machines (each machine has 8 V100 GPUs), resulting in a total batch size of 2,048. The mixed-precision Table <ref type="table">1</ref>. Evaluation results for the text-image retrieval downstream task on the AIC-ICC test set.</p><p>Tasks Image-to-Text Retrieval Text-to-Image Retrieval Metrics R@1 R@5 R@10 R@1 R@5 R@10 CLIP <ref type="bibr" target="#b25">[26]</ref> 13 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Results on AIC-ICC</head><p>We select the AIC-ICC caption competition <ref type="bibr" target="#b33">[34]</ref> to evaluate our pre-trained CMCL model because it is the only publicly-available Chinese multi-modal dataset. This Chinese caption dataset (called as AIC-ICC) includes about 300,000 images, with 5 candidate Chinese caption texts per image. The test split (with 30,000 images) of this dataset is used for performance evaluation on two downstream tasks (i.e., image-text retrieval and image captioning). To make a comparison with OpenAI CLIP on this dataset, we have to translate the Chinese captions in the test split into the English ones (with Google Translation). It is noticeable that we can only obtain the inference code<ref type="foot" target="#foot_1">2</ref> (but not the training code) of CLIP from OpenAI, and thus are unable to pretrain CLIP on our own RUC-CAS-WenLan dataset.</p><p>Table <ref type="table">1</ref> presents the image-text retrieval results. We directly leverage the extracted features for nearest-neighbour (NN) retrieval without fine-tuning. We can observe that our CMCL significantly outperforms CLIP and UNITER on both the text-to-image and image-to-text retrieval subtasks, showing the effectiveness of the proposed CMCL in multi-modal pre-training. Note that our CMCL runs about 20 times faster than UNITER (but as fast as CLIP).</p><p>Table <ref type="table" target="#tab_0">2</ref> presents the image captioning results. Finetuning is conducted on the training split of AIC-ICC. We adopt four widely-used evaluation metrics: BLEU, ME-TEOR, ROUGE-L, and CIDEr. It can be clearly seen that our CMCL performs better than the competitors in terms of three of the four metrics, i.e., our CMCL achieves the best overall performance on the AIC-ICC dataset. This means that our CMCL model also has a good generalization ability in the image captioning downstream task. Table <ref type="table">3</ref>. Evaluation results for the text-image retrieval downstream task on the RUC-CAS-WenLan test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tasks</head><p>Image-to-Text Retrieval Text-to-Image Retrieval Metrics R@1 R@5 R@10 R@1 R@5 R@10 CLIP <ref type="bibr" target="#b25">[26]</ref> 7 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Results on RUC-CAS-WenLan</head><p>We further make performance evaluation on the textimage retrieval task on the test split of RUC-CAS-WenLan, which includes 11,000 image-text pairs. Table <ref type="table">3</ref> presents the text-image retrieval results on the RUC-CAS-WenLan test set. It is noticeable that our CMCL achieves significant improvements over UNITER and OpenAI CLIP <ref type="foot" target="#foot_2">3</ref> . Particularly, our CMCL leads to more than 45% performance gaps in terms of R@10 on both retrieval subtasks. This demonstrate the largest advantage of our CMCL in multi-modal pre-training. Furthermore, our CMCL is pre-trained by using 128 GPUs for about 10 days, comparing to OpenAI CLIP using 256 GPUs for 12 days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">User Study Results</head><p>The user study is carried out over the text-image retrieval results obtained by the pre-training models (e.g., CMLC and CLIP <ref type="bibr" target="#b25">[26]</ref>). We select a group of image and text queries for testing. For each text (or image) query, we retrieve the first 30 results with the tested model from the specified candidate set, and manually score each of the 30 results by 3 ratings (i.e., 0, 1, and 2). Note that the higher the score is, the stronger the correlation between the image and text is. Since five human annotators are involved independently, the final score for each of the 30 results is obtained with 7 ratings (0-6). The scores of each text (or image) query are thus formed into a 30-length score sequence.</p><p>The NDCG and MAP metrics are used to the human retrieval quality. Note that these metrics are widely used for evaluating retrieval quality. Particularly, during computing MAP, the text-image pair is considered to be rele-  vant if the corresponding score is higher than 2. The obtained comparative results are presented in Table <ref type="table" target="#tab_1">4</ref>. As expected, the user study does validate that our CMCL outperforms OpenAI CLIP <ref type="bibr" target="#b25">[26]</ref>. When the candidate set (per query) of UNITER is obtained using our CMCL, UNITER is shown to lead to further improvements over our CMCL (see CMCL+UNITER vs. CMCL).</p><formula xml:id="formula_9">Prediction: 一 个 面 带 微 笑 的 女 人 在 硕 果 累 累 的 果 园 里 采 摘 水 果 (A</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Visual Results</head><p>Figure <ref type="figure" target="#fig_5">4</ref> provides the visualization examples obtained by our image captioning model. Note that three data sources (caption, web, and anime) are used in the three rows (from top to bottom), respectively. We can observe that the generated captions by our model are fluent, vivid, and accurate to express the semantic meanings of the input pictures. Moreover, our model can also predict accurate tags for each anime image. These observations provide evidence that multi-model pre-training indeed brings benefits to the image captioning downstream task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Downstream Applications</head><p>Although 'WenLan' can be applied to a variety of crossmodal downstream tasks, we have only developed two web applications, MatchSoul and MatchSoul-Music, at this moment. Our main goal is to directly demonstrate the power of multi-modal pre-training in real-world scenarios. We will develop more applications in the near future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">MatchSoul</head><p>MatchSoul is developed based on our pre-trained CMCL model. Note that we directly deploy our pre-trained model without any fine-tuning. This application is devised as follows: given a picture uploaded by user, it returns an 'golden' sentence that is the most relevant to this picture.</p><p>Unlike the general image generation, this application does not generate a descriptive sentence for the input picture. In contrast, it chooses to match the picture with the 'golden' sentence (from a candidate set of 300,000 'golden' sentences) according to the characteristics of the picture, as illustrated in Figure <ref type="figure" target="#fig_6">5</ref>(a). The chosen 'golden' sentences are humor, literary, and philosophical thinking. We look forward to giving users a sense of surprise and playing the finishing touch to the picture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">MatchSoul-Music</head><p>Similar to MatchSoul, MatchSoul-Music is also developed based on our pre-trained CMCL model. Specifically, given a picture uploaded by user, MatchSoul-music returns a song lyric that well fits the artistic conception of this picture. As illustrated in Figure <ref type="figure" target="#fig_6">5</ref>(b), MatchSoul-Music matches the input picture with the most relevant song lyric, and even accurately localizes a part of the song lyric which best matches the characteristics of this picture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>This paper presents the first large-scale Chinese multimodal pre-training model called CMCL. The first version of our CMCL model has 1 billion parameters, which is pre-trained on the RUC-CAS-WenLan dataset with 30 million image-text pairs. As a part of this project, RUC-CAS-WenLan is a large Chinese multi-source image-text dataset constructed by ourselves for multi-modal pre-training. It is noticeable that our CMCL model significantly outperforms both UNITER and OpenAI CLIP on the RUC-CAS-WenLan test set and AIC-ICC test set. With the pre-trained CMCL model, we have also developed two web applications called MatchSoul and MatchSoul-Music. In the near future, our CMCL model will be enlarged to 10 billion parameters, which will be pre-trained with 5 billion imagetext pairs. Moreover, we will also exploit the text-to-image generation pretext task for multi-modal pre-training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Example of strong correlation versus weak correlation between image-text pairs. Note that the strong correlation assumption widely used in many multi-model pre-training models is often invalid in real-world scenarios.</figDesc><graphic url="image-1.png" coords="1,311.06,315.88,99.60,66.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. A schematic illustration of the proposed two-tower pretraining model within the CMCL framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. (a) A schematic illustration of the proposed CMCL model for large-scale multi-model pre-training. (b) The architecture of the image encoder f I used for CMCL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>woman with a slight smile is picking fruits in the fruitful orchard)Prediction: 一 架 飞 机 在 晴 朗 的 天 气 里 飞 过 天 空 (Anaeroplane flies across the sky in a sunny day) Prediction: 一个穿着戏服的男人和一个 穿着戏服的女孩在一起 (A man in a costume is staying with a girl in a costume) Prediction: 一条城市街道，有一个红绿灯， 上面有一个标志 (On a city street, there is a traffic light with a sign on it) Prediction: '双挥舞', ' 黑色连衣 裙', '鼹鼠', ' 大腿', '白发 ', '剑', '武器 ', '连衣裙' Prediction: '大衣', '全身 ', '站立', '编 织物', '长袖', '棕色的眼睛 ', '金发', '短 发', '长发' ('double waving', 'black dress', 'mole', 'thigh', 'white hair', 'sword', 'weapon', 'dress') ('Coat', 'full body', 'standing', 'braid', 'long sleeve', 'brown eyes', 'blonde', 'short hair', 'long hair' )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Visualization examples obtained by our image captioning model. Note that three data sources (caption, web, and anime) are used in the three rows (from top to bottom), respectively.</figDesc><graphic url="image-11.png" coords="6,55.61,321.29,52.83,73.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Demonstration of our downstream application. (a) MatchSoul: matching pictures with 'golden' sentences. (b) MatchSoul-Music: matching pictures with 'golden' lyrics.</figDesc><graphic url="image-13.png" coords="6,312.31,73.06,97.13,171.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Evaluation results for the image captioning downstream task on the AIC-ICC test set.</figDesc><table><row><cell></cell><cell>.4</cell><cell>27.3</cell><cell>35.1</cell><cell>7.8</cell><cell>18.5</cell><cell>25.0</cell></row><row><cell>UNITER [6]</cell><cell>14.8</cell><cell>29.8</cell><cell>37.9</cell><cell>9.8</cell><cell>23.3</cell><cell>31.4</cell></row><row><cell>CMCL (ours)</cell><cell>20.3</cell><cell>37.0</cell><cell>45.6</cell><cell>14.4</cell><cell>30.4</cell><cell>39.1</cell></row><row><cell>Metrics</cell><cell cols="2">BLEU</cell><cell>METEOR</cell><cell cols="2">ROUGE-L</cell><cell>CIDEr</cell></row><row><cell>CHAMPION'17</cell><cell></cell><cell>62.8</cell><cell>43.0</cell><cell>-</cell><cell></cell><cell>210.4</cell></row><row><cell>UNITER [6]</cell><cell></cell><cell>62.8</cell><cell>38.7</cell><cell>69.2</cell><cell></cell><cell>199.7</cell></row><row><cell>CMCL (ours)</cell><cell></cell><cell>66.1</cell><cell>41.1</cell><cell>71.9</cell><cell></cell><cell>220.7</cell></row><row><cell cols="7">and half-precision Adam statistics are used to accelerate the</cell></row><row><cell cols="7">pre-training process and save the memory. It takes 10 days</cell></row><row><cell cols="6">to pre-train our CMCL model on 128 V100 GPUs.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 .</head><label>4</label><figDesc>User study results for the text-image retrieval downstream task. Three human annotators are involved in such user study.</figDesc><table><row><cell></cell><cell>.3</cell><cell>15.0</cell><cell>19.0</cell><cell>7.8</cell><cell>15.9</cell><cell>19.9</cell></row><row><cell>UNITER [6]</cell><cell>5.3</cell><cell>16.9</cell><cell>24.6</cell><cell>5.7</cell><cell>16.7</cell><cell>24.3</cell></row><row><cell>CMCL (ours)</cell><cell>36.1</cell><cell>55.5</cell><cell>62.2</cell><cell>36.0</cell><cell>55.4</cell><cell>62.1</cell></row><row><cell>Tasks</cell><cell></cell><cell></cell><cell cols="3">Image-to-Text Retrieval</cell><cell></cell></row><row><cell>Metrics</cell><cell cols="6">NDCG@5 NDCG@10 NDCG@20 MAP</cell></row><row><cell>CLIP [26]</cell><cell></cell><cell>32.9</cell><cell>38.8</cell><cell></cell><cell>53.0</cell><cell>30.3</cell></row><row><cell>CMCL</cell><cell></cell><cell>37.5</cell><cell>42.8</cell><cell></cell><cell>55.5</cell><cell>38.3</cell></row><row><cell cols="2">CMCL+UNITER</cell><cell>37.0</cell><cell>43.5</cell><cell></cell><cell>56.3</cell><cell>37.6</cell></row><row><cell>Tasks</cell><cell></cell><cell></cell><cell cols="3">Text-to-Image Retrieval</cell><cell></cell></row><row><cell>Metrics</cell><cell cols="6">NDCG@5 NDCG@10 NDCG@20 MAP</cell></row><row><cell>CLIP [26]</cell><cell></cell><cell>28.0</cell><cell>32.3</cell><cell></cell><cell>43.7</cell><cell>16.7</cell></row><row><cell>CMCL</cell><cell></cell><cell>46.9</cell><cell>51.5</cell><cell></cell><cell>61.6</cell><cell>47.2</cell></row><row><cell cols="2">CMCL+UNITER</cell><cell>49.9</cell><cell>55.0</cell><cell></cell><cell>65.1</cell><cell>52.5</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/brightmart/roberta zh</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://github.com/openai/CLIP</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">The inference code of OpenAI CLIP is directly implemented on the translated test split with Google Translation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by National Natural Science Foundation of China (61976220 and 61832017), Beijing Outstanding Young Scientist Program (BJJWZYJH012019100020098), Large-Scale Pre-Training Program of Beijing Academy of Artificial Intelligence (BAAI), and Alibaba Innovative Research (AIR) Program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Aditya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavlov</forename><surname>Mikhail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goh</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gray</forename><surname>Scott</surname></persName>
		</author>
		<title level="m">Creating images from text. Ope-nAI Blog</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15535" to="15545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10566</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">UNITER: Universal image-text representation learning</title>
		<author>
			<persName><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2005">2020. 2, 3, 4, 5</date>
			<biblScope unit="page" from="104" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">X-LXMERT: Paint, caption and answer questions with multi-modal transformers</title>
		<author>
			<persName><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.11278</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.01432</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Karan</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><surname>Virtex</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06666</idno>
		<title level="m">Learning visual representations from textual annotations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Large-scale adversarial training for vision-and-language representation learning</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06195</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent -a new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="21271" to="21284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Selfsupervised relationship probing</title>
		<author>
			<persName><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Handong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08909</idno>
		<title level="m">REALM: Retrievalaugmented language model pre-training</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06146</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">AL-BERT: A lite BERT for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">RoBERTa: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fine-grained visual textual alignment for cross-modal retrieval using transformer encoders</title>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Messina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Amato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Falchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Gennaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stéphane</forename><surname>Marchand-Maillet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05231</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">ImageBERT: Cross-modal pretraining with large-scale weak-supervised image-text data</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taroon</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Sacheti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07966</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<date type="published" when="2006">2021. 1, 2, 3, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">How much knowledge can you pack into the parameters of a language model</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08910</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Emerging trends of multimodal research in vision and language</title>
		<author>
			<persName><forename type="first">Shagun</forename><surname>Uppal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarthak</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navonil</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09522</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">AI challenger: A largescale dataset for going deeper in image understanding</title>
		<author>
			<persName><forename type="first">Jiahong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shipei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guosen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06475</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">XGPT: Cross-modal generative pre-training for image captioning</title>
		<author>
			<persName><forename type="first">Qiaolin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taroon</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.01473</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Fei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiji</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weichong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Ernie-Vil</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16934</idno>
		<title level="m">Knowledge enhanced vision-language representations through scene graph</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName><forename type="first">Chengxu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">Lin</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6002" to="6012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
