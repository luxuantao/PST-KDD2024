<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Constrained Multi-view Video Face Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-07-29">July 29, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
							<email>caoxiaochun@iie.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Changqing</forename><surname>Zhang</surname></persName>
							<email>zhangchangqing@tju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Chengju</forename><surname>Zhou</surname></persName>
							<email>zhoucj@tju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
							<email>huazhufu@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Hassan</forename><surname>Foroosh</surname></persName>
							<email>foroosh@cs.ucf.edu</email>
						</author>
						<author>
							<persName><forename type="first">)</forename><forename type="middle">X</forename><surname>Cao</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Institute of Information Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory of Information Security</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<postCode>300072</postCode>
									<settlement>Tianjin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100093</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<postCode>300072</postCode>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<addrLine>Nanyang Avenue</addrLine>
									<postCode>639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Central Florida (UCF)</orgName>
								<address>
									<settlement>Orlando</settlement>
									<region>Florida</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<address>
									<addrLine>34 2 2 2 2 2 2 2 2 2 2 2 2 2 2 40 2 2 2 2 2 2 2 2 2 2 2 33 01 2 2 2 2 2 2 2 2 2 2 2 2 2 2 03 2 2 2 3 2 2 41 2 2 2 2 2 2 3 2 5 2 33 2 2 6 2 5 3 3 2 3 2 0 32 2 6 3 7 2 2 2 2 2 82 2 2 2 2 2 2 34 2 2 2 2 2 2 2 2 2 2 2 2 2 2 40 2 2 2 2 2 2 2 2 2 2 2 33 01 2 2 2 2 2 2 2 2 2 2 2 2 2 2 03 2 2 2 3 2 2 41 2 2 2 2 2 2 3 2 5 2 33 2 2 6 2 5 3</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Constrained Multi-view Video Face Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-07-29">July 29, 2015</date>
						</imprint>
					</monogr>
					<idno type="MD5">0713187DC733ECA50F8CE788EAEF6F59</idno>
					<idno type="DOI">10.1109/TIP.2015.2463223</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2015.2463223, IEEE Transactions on Image Processing IEEE TRANSACTIONS ON IMAGE PROCESSING 1 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2015.2463223, IEEE Transactions on Image Processing IEEE TRANSACTIONS ON IMAGE PROCESSING 2 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2015.2463223, IEEE Transactions on Image Processing IEEE TRANSACTIONS ON IMAGE PROCESSING 3 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2015.2463223, IEEE Transactions on Image Processing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2015.2463223, IEEE Transactions on Image Processing This article has been accepted for publication in a issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2015.2463223, IEEE Transactions on Image Processing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we focus on face clustering in videos. To promote the performance of video clustering by multiple intrinsic cues, i.e., pairwise constraints and multiple views, we propose a Constrained Multiview Video Face Clustering (CMVFC) method under a unified graph-based model. First, unlike most existing video face clustering methods which only employ these constraints in the clustering step, we strengthen the pairwise constraints through the whole video face clustering framework, both in sparse subspace representation and spectral clustering. In the constrained sparse subspace representation, the sparse representation is forced to explore unknown relationships. In the constrained spectral clustering, the constraints are used to guide for learning more reasonable new representations. Second, our method takes into account both the video face pairwise constraints as well as the multi-view consistence simultaneously. Specifically, the graph regularization enforces the pairwise constraints to be respected and the coregularization penalizes the disagreement among different graphs of multiple views. Experiments on three real-world video benchmark datasets demonstrate the significant improvements of our method over the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. <ref type="figure" target="#fig_1">1</ref>. The framework of our method. From the input video (a), we extract the frames and detect the facial images (b). Then, we build up the must-link matrix based on the face tracks and cannot-link matrix based on the frames. Meanwhile, we extract the multiple features for the detected facial images (c). Based on these constraints and features, we perform our CMVFC algorithm in two steps, constrained sparse representation and constrained spectral clustering (d) to get the clustering result (e). <ref type="bibr" target="#b12">[13]</ref>. However, most multi-view methods do not consider the prior constraints in clustering, which are usually critical in many applications. In this paper, we introduce pairwise constraints into the multi-view clustering to effectively exploit the complementary information in different views.</p><p>In this paper, we provide a Constrained Multi-view Video Face Clustering (CMVFC) method, which combines the pairwise constraints in both sparse subspace representation and spectral clustering procedures. Moreover, we also introduce the multi-view clustering fashion to exploit multiple features. We effectively exploit the pairwise constraints and the multi-view consistence as regularization simultaneously.</p><p>The experiments show that the proposed method outperforms the state-of-the-art methods on three realworld benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>We give a brief introduction about some face clustering techniques and some general clustering methods based on sparse subspace representation or multiple views, which are highly related to our work.</p><p>Video face clustering: Most existing video face clustering methods focus on obtaining a good representation for the structure of inter-personal dissimilarities. For example, Fitzgibbon et al. <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref> proposed an affine invariant distance metric which is robust to a desired group of transformations for video face clustering. Huang et al. <ref type="bibr" target="#b13">[14]</ref> proposed to cluster faces with multi-views in a video sequence. They clustered is presented in <ref type="bibr" target="#b5">[6]</ref>, which incorporates pairwise constraints within a Hidden Markov Random Fields.</p><p>However, this work only utilizes the pairwise constraints in clustering procedure with single feature. In contrast, we use pairwise constraints in both sparse subspace representation and spectral clustering to fully explore the constraints. Moreover, we cluster the facial images under a multi-view framework to exploit the complementary information.</p><p>Constrained clustering: Clustering with pairwise constraints has been attracting more and more attentions in the machine learning and data mining communities. Generally, there are two categories of methods using pairwise constraints in clustering. The first category introduces the metric learning fashion which aims to learn a Mahalanobis distance that minimizes the distance between must-link samples and maximizes the distance between cannot-link samples. However, the metric learning step and clustering step are often isolated in these methods, and thus the performance cannot be guaranteed <ref type="bibr" target="#b5">[6]</ref>. The second category adopts the traditional centroid-based clustering methods, such as K-means <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> or Gaussian mixtures <ref type="bibr" target="#b18">[19]</ref> to meet the pairwise constraints. There exist few works to blend the pairwise constraints in a natural way. The work in <ref type="bibr" target="#b19">[20]</ref> simply uses the Gaussian kernel as the affinity but replaces entries for must-link pairs with 1 and cannot-link pairs with 0. The work in <ref type="bibr" target="#b20">[21]</ref> combines must-link and cannotlink affinity by propagating the pairwise constraints over the original affinity matrix. Instead of directly modifying the affinity matrix <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, we utilize the pairwise constraints as regularization into spectral clustering which directly aims to obtain a more reasonable representation.</p><p>Multi-view clustering: Multi-view clustering is of great importance since an abundance of complementary perspectives and multi-view representations of data are often available. The method in <ref type="bibr" target="#b11">[12]</ref> develops multi-view spectral clustering via generalizing the normalized cut from a single view to multiple views.</p><p>The authors gave a random walk based formulation for the problem. The clustering algorithm in <ref type="bibr" target="#b21">[22]</ref> creates a bipartite graph and is based on the minimizing-disagreement. However, it concentrates on the data with only two views. The method in <ref type="bibr" target="#b10">[11]</ref> uses Linked Matrix Factorization to fuse the information from multiple graph sources. The authors in <ref type="bibr" target="#b12">[13]</ref> proposed a spectral clustering framework which co-regularizes the clustering hypotheses, and propose the co-regularization scheme to penalize the disagreement across different views. The method in <ref type="bibr" target="#b22">[23]</ref> employs Hilbert Schmidt Independence Criterion (HSIC) to enhance the complementarity across different views. Our method introduces the pairwise constraints into multi-view spectral clustering in an elegant manner, which effectively exploits the pairwise constraints and the multi-view consistence simultaneously.</p><p>Sparse subspace representation: There has been a great interest in sparse representation during the last decade. Wright et al. <ref type="bibr" target="#b23">[24]</ref> use 1 -norm minimization to deal with missing or corrupted data in face recognition. Most of the sparse representation literature assumes that the data lie in a single linear subspace. Furthermore, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> propose to use the sparse representation of vectors lying on a union of subspaces to cluster the data into separated subspaces. However, these methods do not consider prior constraints in representation. In this paper, we introduce pairwise constraints into sparse subspace representation, aiming to better explore the face relationships for clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image set based face recognition:</head><p>There are some face recognition methods focusing on learning over facial image sets <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, in which each test and training example is a set of images of an individual's face. These methods usually try to design or learn different similarity metrics for matching image sets (e.g., canonical angles between two subspaces <ref type="bibr" target="#b28">[29]</ref>). A video face track can be regarded as a facial image set. Therefore, the set models (e.g., modeling each image set as a manifold <ref type="bibr" target="#b29">[30]</ref>) in these methods can be employed to represent face tracks. Consequently, the corresponding similarity metrics for image set can be utilized to cluster these face tracks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. FRAMEWORK OF OUR APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Preprocessing</head><p>For face detection, one of the most important method is proposed by Viola and Jones <ref type="bibr" target="#b35">[36]</ref>, which builds a successful face detector running in real time. Any or more advanced similar works can be easily integrated into our approach. We employ the face tracking method in <ref type="bibr" target="#b36">[37]</ref>, which consists of two metrics: histogram intersection and frame overlap. For face alignment, the work in <ref type="bibr" target="#b37">[38]</ref> which jointly aligns complex images in a unsupervised manner is employed in our framework. It has shown high quality results on the faces in the Wild dataset <ref type="bibr" target="#b38">[39]</ref>, which is also under large variation of head poses, lighting conditions, backgrounds as in the real-world videos. To concentrate on face clustering approach, in our work, we assume that a set of face windows are well extracted. The preprocessing is similar to other video face clustering methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b39">[40]</ref>. First, most false positives of face detections can be easily eliminated by selecting the tracks with a sufficiently large number of faces. Second, we manually select the tracks corresponding to main characters to eliminate the wrong detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pairwise constraints</head><p>Given a set of facial images F = {f 1 , f 2 , ..., f n }, where n is the number of the total faces, we extract a d-dimensional feature vector x i ∈ R d for each image f i , forming the corresponding feature matrix</p><formula xml:id="formula_0">X = [x 1 , x 2 , ..., x n ].</formula><p>Two matrices are built up to describe the pairwise constraints of the faces, i.e., the must-link matrix M ∈ R n×n and the cannot-link matrix C ∈ R n×n . The matrix M represents the must-link constraints, where the elements corresponding to the face pairs in the same track are set to 1 while others are set to 0. The matrix C represents the cannot-link constraints, the elements of which corresponding to the face pairs belonging to the overlapped tracks are set to -1 while others are set to 0.</p><p>For convenience, we also define M = {(x i , x j ) ∈ M|m ij = 1} and C = {(x i , x j ) ∈ C|c ij = -1} as the sets of the must-link and cannot-link constraints, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Constrained sparse subspace representation</head><p>Ideally, the face x i can be sparsely represented by a small subset of facial images from the same person in the dataset <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. The relationship can be written as</p><formula xml:id="formula_1">x i = Xa i s. t. a ii = 0,<label>(1)</label></formula><p>where a i = [a 1i , a 2i , ..., a ni ] T , and the constraint a ii = 0 eliminates the trivial solution of representing a facial image with itself. The coefficient vector a i should have nonzero entries for a few facial images from the same person and zeros from the rest. In other words, the matrix X is a self-expressive dictionary in which each facial image can be represented by a linear combination of the others.</p><p>Some relationships among faces have been known from the must-link and the cannot-link constraints.</p><p>Therefore, we pay attention to exploring the unknown relationships by utilizing the prior constraints</p><formula xml:id="formula_2">x i = Xa i s. t. a ji = 0, ∀(x j , x i ) ∈ M ∪ C.<label>(2)</label></formula><p>The reason to eliminate the a ji for (x j , x i ) ∈ M is to avoid the representation using faces in the same track. Consequently, the sparse representation is forced to relate the faces with unknown relationships.</p><p>The reason to eliminate the a ji for (x j , x i ) ∈ C is to avoid the representation between faces in the same video frame. On the other hand, the known must-link and cannot-link constraints are later re-exploited in spectral clustering (Eq. ( <ref type="formula" target="#formula_18">13</ref>)).</p><p>One limitation of Eq. ( <ref type="formula" target="#formula_2">2</ref>) is that the representation of x i in the dictionary X is not unique in general.</p><p>Since we are interested in efficiently finding a nontrivial sparse representation of x i in the data set X, we use the tightest convex relaxation of the 0 -norm, i.e.,</p><formula xml:id="formula_3">min a i 1 s. t. x i = Xa i and a ji = 0, ∀(x j , x i ) ∈ M ∪ C.<label>(3)</label></formula><p>Moreover, considering clustering of data points that are contaminated with sparse outlying entries and noise <ref type="bibr" target="#b24">[25]</ref>, the constrained sparse representation is obtained by the following equation</p><formula xml:id="formula_4">min a i 1 + λ e ||e i || 1 + λ z ||z i || 2 s. t. x i = Xa i + e i + z i and a ji = 0, ∀(x j , x i ) ∈ M ∪ C,<label>(4)</label></formula><p>where e i ∈ R d and z i ∈ R d are the error and noise, respectively. The two parameters λ e and λ z balance the three terms in Eq. ( <ref type="formula" target="#formula_4">4</ref>). Without loss of generality, we can rewrite the sparse optimization problem <ref type="bibr" target="#b3">(4)</ref> for all faces in the following matrix form</p><formula xml:id="formula_5">min A 1 + λ e ||E|| 1 + λ z ||Z|| 2 F s. t. X = XA + E + Z and a ji = 0, ∀(x j , x i ) ∈ M ∪ C,<label>(5)</label></formula><p>where A = [a 1 , a 2 , ..., a n ] ∈ R n×n is the coefficient matrix, the i th column of which corresponds to the sparse representation of x i . More specifically, each column of A corresponds to a new representation of a facial image, whose nonzero elements ideally correspond to faces from the same person. Since the optimization problem in Eq. ( <ref type="formula" target="#formula_5">5</ref>) is convex with respect to the variables A, E and Z, it can be solved efficiently using convex programming tools <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Constrained spectral clustering</head><p>Before introducing the multi-view clustering, we first depict our face clustering method using the single feature in this subsection. With Eq. ( <ref type="formula" target="#formula_5">5</ref>), we obtain a sparse coefficient matrix A for the whole facial image set. Afterwards, we build a weighted graph G = (V, E, W), where V denotes the set of n nodes in graph G corresponding to the set of n faces, and E denotes the edges between nodes. W ∈ R n×n is a symmetric nonnegative similarity matrix representing the weights of the edges. Typically, an ideal similarity graph G should have connections corresponding to the same person and have no connections corresponding to different persons.</p><p>In the sparse representation solution A from subsection III-C, nonzero elements can be regarded as a measurement of the relationships between faces. This provides a choice of constructing the similarity matrix <ref type="bibr" target="#b24">[25]</ref>,</p><formula xml:id="formula_6">W = |A| + |A| T . (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>We normalize A as a i ← a i / a i ∞ to make sure the weights in similarity graph are of the same scale.</p><p>A straightforward combination way <ref type="bibr" target="#b42">[43]</ref> is incorporating the must-link and cannot-link constraints into the similarity matrix directly. It can be written as</p><formula xml:id="formula_8">W pc = W + ζM + ηC,<label>(7)</label></formula><p>where the trade-off factors ζ and η encode the belief degrees for the must-link and cannot-link constraints, respectively. However, instead of directly combining the two pairwise constraint matrices into the similarity matrix, we regularize the pairwise constraints in spectral clustering which directly aims to obtain a more reasonable embedding representation.</p><p>For the k-way spectral clustering with a single view, we aim to obtain a new embedding representation U of the original data X by optimizing the following objective function <ref type="bibr" target="#b43">[44]</ref> argmax</p><formula xml:id="formula_9">U∈R n×k T r(U T LU) s. t. U T U = I,<label>(8)</label></formula><p>where L = D -1/2 WD -1/2 is the normalized graph Laplacian matrix, and D is a diagonal matrix with</p><formula xml:id="formula_10">element d ii = n j=1 w ij . For convenience, we denote d i = d ii .</formula><p>W is the similarity matrix, which is often constructed by the original feature X. T r(•) denotes the trace of a matrix. Note that, different from the work in <ref type="bibr" target="#b12">[13]</ref>, we use the constrained sparse subspace representation in Eq. ( <ref type="formula" target="#formula_6">6</ref> Considering the pairwise constraints, a regularized term is designed to ensure that the representation of must-link pairs are close and the representation of cannot-link pairs are far away from each other.</p><p>We denote the distance of two points u i and u j according to the two constraint matrices, M and C, as follow</p><formula xml:id="formula_11">d ml (u i , u j ) = || u i d ml i - u j d ml j || 2 , (<label>9</label></formula><formula xml:id="formula_12">)</formula><p>and</p><formula xml:id="formula_13">d cl (u i , u j ) = || u i dcl i - u j dcl j || 2 , (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>where the distance is normalized by d ml i ( dcl i ) and d ml j ( dcl j ) in order to reduce the impact of popularity of nodes as in traditional graph-based learning <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, and the effectiveness of the normalized technique is well proved. Accordingly, the pairwise constraints as regularization is defined as:</p><formula xml:id="formula_15">R(U; M, C) = 1 2 n i,j=1 || u i d ml i - u j d ml j || 2 m ij + 1 2 n i,j=1 || u i dcl i - u j dcl j || 2 c ij = T r(U T (I -L ml )U) + T r(U T (I -Lcl )U),<label>(11)</label></formula><p>where L ml is the normalized graph Laplacian matrix corresponding to the must-link constraint matrix M.</p><p>Note that we denote Lcl = Dcl -1/2 C Dcl -1/2 as the new graph Laplacian corresponding to the cannot-link</p><formula xml:id="formula_16">matrix C with dcl i = dcl ii = n j=1 |c ij |.</formula><p>The absolute operator is to handle a graph with negatively weighted edges, which is proved by <ref type="bibr" target="#b46">[47]</ref>. By ignoring the constant additive term, Eq. ( <ref type="formula" target="#formula_15">11</ref>) can be rewritten as:</p><formula xml:id="formula_17">R(U; M, C) = -T r(U T L ml U) -T r(U T Lcl U).<label>(12)</label></formula><p>Intuitively, minimizing the term in Eq. ( <ref type="formula" target="#formula_17">12</ref>) will enforce the new representation U to simultaneously meets the graphs corresponding to the must-link matrix M and the cannot-link matrix C.</p><p>For our constrained clustering method, we combine the pairwise constraint regularization in Eq. <ref type="bibr" target="#b11">(12)</ref> into Eq. ( <ref type="formula" target="#formula_9">8</ref>) as a new objective function</p><formula xml:id="formula_18">argmax U∈R n×k T r(U T LU) + λ ml T r(U T L ml U) + λ cl T r(U T Lcl U) s. t. U T U = I,<label>(13)</label></formula><p>where λ ml and λ cl encode the different belief degrees for the must-link and cannot-link constraints, respectively. The Eq. ( <ref type="formula" target="#formula_18">13</ref>) can be reformulated as a standard spectral clustering objective function with a new combined graph Laplacian argmax</p><formula xml:id="formula_19">U∈R n×k T r(U T L cst U) s. t. U T U = I<label>(14)</label></formula><p>where L cst = L + λ ml L ml + λ cl Lcl is the combined Laplacian. Thus, both the must-links and cannotlinks are incorporated into the standard spectral clustering framework, which can be efficiently solved by eigen-decomposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Constrained multi-view spectral clustering</head><p>The constrained spectral clustering achieves state-of-the-art performance by exploiting the pairwise constraints both in sparse subspace representation and spectral clustering steps. We further improve the approach into the multi-view framework, named Constrained Multi-view Video Face Clustering (CMVFC). To distinguish our method from the method <ref type="bibr" target="#b13">[14]</ref> which defines view in a geometry point of view, we define the multi-view face clustering of our interest as follow:</p><p>Multi-view face clustering. For each of the n facial images detected from the input video, we extract their V types of features. The task of multi-view face clustering is to cluster these facial images by simultaneously utilizing the matrices X (1) , X (2) , ..., X (V ) , with</p><formula xml:id="formula_20">X (v) = [x (v) 1 , ..., x<label>(v)</label></formula><p>n ] T corresponding to the v th type of feature matrix.</p><p>Considering the multi-view setting and inspired by the method <ref type="bibr" target="#b12">[13]</ref>, we co-regularize the disagreement between different views, and extend it to our constrained multi-view spectral clustering. We define the eigenvector matrix U (v) as the new data representation derived from the v th original feature. Encouraging the pairwise similarity to be similar across the V views will enforce the clustering results to be the same across all the features. For any two similarity matrices corresponding to U (v) and U (w) , the measure of disagreement between them is defined as</p><formula xml:id="formula_21">D(U (v) , U (w) ) = W U (v) ||W U (v) || 2 F - W U (w) ||W U (w) || 2 F 2 F , (<label>15</label></formula><formula xml:id="formula_22">)</formula><p>where W U (v) is the similarity matrix for U (v) . || • || F denotes the Frobenius norm of a matrix. The similarity matrices are normalized by their Frobenius norms, which makes them to be comparable across different similarity matrices. With the linear kernel k(u i , u j ) = u T i u j as the similarity measure in Eq. ( <ref type="formula" target="#formula_21">15</ref>), we have</p><formula xml:id="formula_23">W U (v) = U (v) U (v) T and ||W U (v) || 2 F = k,</formula><p>where k is the number of clusters. The Eq. ( <ref type="formula" target="#formula_21">15</ref>) can be rewritten as follow by ignoring the constant additive and scaling terms</p><formula xml:id="formula_24">D(U (v) , U (w) ) = -T r(U (v) U (v) T U (w) U (w) T ).<label>(16)</label></formula><p>The term should be minimized to ensure the clustering consistence across all the different views. For our constrained multi-view spectral clustering method, we combine the disagreement penalty term D(•)</p><p>in Eq. ( <ref type="formula" target="#formula_24">16</ref>) and the constraint regularization term R(•) in Eq. ( <ref type="formula" target="#formula_17">12</ref>) into Eq. ( <ref type="formula" target="#formula_9">8</ref>), then the new objective function, i.e., constrained multi-view spectral clustering is obtained as argmax</p><formula xml:id="formula_25">U (1) ,...,U (V ) ∈R n×k 1≤v≤V T r(U (v) T L (v) U (v) ) + α 1≤v≤V T r(U (v) T L ml U (v) ) + β 1≤v≤V T r(U (v) T Lcl U (v) ) + γ 1≤v,w≤V ;v =w T r(U (v) U (v) T U (w) U (w) T )<label>(17)</label></formula><formula xml:id="formula_26">s. t. U (v) T U (v) = I, ∀ 1 ≤ v ≤ V,</formula><p>where α, β and γ are trade-off factors for the must-link constraints, cannot-link constraints and clustering agreement across different features, respectively. The objective function in Eq. ( <ref type="formula" target="#formula_25">17</ref>) tries to balance a trade off between the individual spectral clustering objectives, the agreement of each pair of view-specific new representations U (v) 's, as well as the pairwise constraints.</p><p>We optimize it by alternating maximization cycling over the views. Specifically, with all but one U (v)   fixed, we have the following optimization problem argmax</p><formula xml:id="formula_27">U (v) ∈R n×k T r{U (v) T L new U (v) }</formula><p>with</p><formula xml:id="formula_28">L new = L (v) + αL ml + β Lcl + γ w = v U (w) U (w) T . (<label>18</label></formula><formula xml:id="formula_29">)</formula><p>While by denoting L new as the new graph Laplacian, it is a standard spectral clustering objective on view v.</p><p>We initialize all U (v) , 2 ≤ v ≤ V by solving the spectral clustering problem for each single view.</p><p>Thus, the objective of Eq. ( <ref type="formula" target="#formula_25">17</ref>) for the first view U (1) can be solved given all other U (v) . The optimization is then cycled over all views while keeping the previously obtained U (•) s fixed. Since the objective is nondecreasing with each iteration, the convergence is guaranteed. In practice, we monitor the convergence is reached within less than 5 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Computational complexity</head><p>The major computation of CMVFC is composed of three parts, i.e., sparse subspace representation, iteration of updating each view-specific eigenvectors and the final K-means clustering. For simplicity, we suppose the dimensionality of each view is M . The computation complexity of sparse subspace</p><formula xml:id="formula_30">representation is O(M N 2 + N 3 ) [42]</formula><p>for one view, where N is the number of samples. The computation complexity of eigenvalue decomposition is O(N 3 ), hence the complexity of all views' eigenvectors is</p><formula xml:id="formula_31">O(T 1 V N 3 )</formula><p>, where V is the number of views and T 1 is the number of iterations. The final step of spectral clustering is using K-means, and the computational complexity of K-means is O(T 2 KN ), where T 2 and K are number of iterations and number of clusters, respectively. Finally, the complexity of the</p><formula xml:id="formula_32">proposed method is O(V M N 2 + V N 3 + T 1 V N 3 + T 2 KN ).</formula><p>In practice, the main computation complexity</p><formula xml:id="formula_33">(O(V M N 2 + V N 3 )</formula><p>) is decided by sparse subspace representation step since T 1 , T 2 , V and K are often much smaller than M and N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we present experimental results and compare our approach with several state-of-the-art face clustering methods on three datasets. Four main evaluation metrics are used for comparison. After giving experimental settings in subsection IV-A, we first analyze the key components of our method in subsection IV-B. Then, both the qualitative and quantitative results on the three benchmark datasets are given in subsection IV-C. We also validate the robustness of our method by varying sampling numbers per track and considering the detection error in subsection IV-D and subsection IV-E, respectively. Finally, we test the parameter tuning of our method in subsection IV-F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental settings</head><p>Datasets: We conduct our experiments on three datasets. The dataset Notting-Hill <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>,</p><p>[50] is derived from the movie "Notting-Hill". Faces of 5 main casts are used, including 4660 faces in 76 tracks. The original dataset consists of the facial images of the size of 120×150. To reduce the computational cost and the memory requirements, we downsample each facial image to 40×50 and get the 2000-dimensional vector as the intensity feature. We build up the dataset TBBTS06E12 from the Season 6 Episodes 12 of TV series "The Big Bang Theory". The detected faces of 9 main casts are used, which is a part of YouTube Face Dataset <ref type="bibr" target="#b50">[51]</ref>. The faces are from different videos and thus it is more challenging than the others. Note that only face tracks are provided but no frame indices for the faces in this dataset. So there are no cannot-link constraints. We select the individuals with the number of face tracks being larger than 5. Finally, we get the facial images corresponding to 8 people, each of whom has 6 face tracks. We also downsample the facial images to 50×50 and use the 2500-dimensional vector as intensity feature.</p><p>Features: All compared methods use the intensity feature except the CMVFC. For our multi-view method, three types of features are employed in our experiments: intensity, LBP <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref> and Gabor <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>. The standard LBP features are extracted from 72×80 loosely cropped images with a histogram size of 59 over 9×10 pixel patches. Gabor wavelets are extracted with one scale λ = 4 at four orientations θ = {0 o , 45 o , 90 o , 135 o } with a loose face crop at a resolution of 25×30 pixels. A null Gabor filter includes the raw pixel image in the descriptor. All descriptors except the intensity are scaled to unit norm, and the dimensionality of each descriptor is reduced with PCA to 1536 dimensions, and zero-meaned. In HMRF-com, we follow the same setting as <ref type="bibr" target="#b5">[6]</ref>. PCA is used to project the original scale feature space to a lower dimensional space which is equal to the number of clusters.</p><p>Comparisons: We compare our algorithm, CMVFC, to several baselines and state-of-the-art methods.</p><p>Moreover, we test these algorithms in four cases: with no-links, with only cannot-links, with only mustlinks and with all-links, respectively. All the comparisons are devised for incorporating with constraints except SSC <ref type="bibr" target="#b24">[25]</ref>. Such a setting provides a clear view of effects of different constraints. The experiments are repeated 10 times, and the mean value and standard deviation are reported. Specifically, the comparisons include the following approaches:</p><p>• SSC <ref type="bibr" target="#b24">[25]</ref>: The sparse subspace clustering method, which is a special case of our CS-VFC (without  using any constraints). Thus, we do not show the result explicitly.</p><p>• CSC <ref type="bibr" target="#b55">[56]</ref>: The constrained spectral clustering algorithm, which can be interpreted as finding the normalized min-cut of a labeled graph.</p><p>• CSC-AP <ref type="bibr" target="#b20">[21]</ref>: The constrained spectral clustering algorithm through affinity propagation, which propagates the pairwise constraints information over the original affinity matrix.</p><p>• MI-VFC <ref type="bibr" target="#b15">[16]</ref>: The method uses a novel formulation of the mutual information as a facial image similarity criterion.</p><p>• ULDML <ref type="bibr" target="#b39">[40]</ref>: The method learns a Mahalanobis metric through the logistic regression, in which positive pairs are generated based on the must-link constraints, while negative pairs based on cannot-link constraints. Then the K-means is employed based on the new metric.</p><p>• HMRF-com <ref type="bibr" target="#b5">[6]</ref>: The latest algorithm focusing on video face clustering, which incorporates the pairwise constraints into a generative clustering model based on Hidden Markov Random Fields (HMRF-com).</p><p>• FeatConcate: The method which concatenates all the three types of features and then clustering with the proposed single-view clustering method.</p><p>• CS-VFC: The constrained single-view clustering method proposed in this paper.</p><p>• CMVFC: Our constrained multi-view clustering method.</p><p>We use the authors' codes of methods CSC, CSC-AP, HMRF-com and ULMDL. For MI-VFC, we have implemented the code by ourselves.</p><p>Evaluation metrics: Following the convention of the clustering, we set the number of clusters to be the ground-truth number of classes for all the compared methods. The clustering quality is evaluated by 2 standard measurements, i.e., Normalized Mutual Information (NMI) <ref type="bibr" target="#b56">[57]</ref> and Accuracy. The 2 metrics are employed to assess different aspects of a given clustering result. For each of the metrics, the higher it is, the better the performance is. The accuracy is calculated based on confusion matrix, which is derived from the match between the predicted labels of all faces and the ground-truth labels. The NMI as the clustering quality evaluation measure, gives the mutual dependence of the predicted clustering and the ground-truth partions from the information-theoretic perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation of key components</head><p>Impact of pairwise constraints: First, we evaluate the effect of the constrained sparse representation and constrained spectral clustering for both the CS-VFC and CMVFC as shown in Tables <ref type="table" target="#tab_0">I</ref> and<ref type="table" target="#tab_1">II</ref>, respectively. We compare four cases: without using pairwise constraints (NoPC), pairwise constraints used in sparse representation (PCInSR), pairwise constraints used in spectral clustering (PCInSC) and used in both steps (PCInBoth). These results clearly show that fully utilizing the constraints in the twostep manner significantly outperforms the others. The performance of PCInSR is majorly better than that of NoPC, which shows the effectiveness of our constrained sparse subspace representation. On average, the accuracies of PCInSR are higher than those of NoPC about 10% and 7% on the three datasets for CS-VFC and CMVFC, respectively. The contribution of constraints only in spectral clustering is slightly larger than that of PCInSR, which further validates the advantage by introducing the pairwise constraints as regularization into spectral clustering.    Impact of multi-view consistence: Then, to evaluate our constrained multi-view clustering algorithm qualitatively, we visualize the similarity matrices based on new representations of faces which are obtained according to each feature by selecting the top k max eigenvectors using Eq. ( <ref type="formula" target="#formula_18">13</ref>). For multi-view clustering, we obtain the new representation using Eq. ( <ref type="formula" target="#formula_25">17</ref>) considering the multi-view consistence. Since these new representations all act as the input in K-means, and the similarity matrix usually strongly affects the clustering result. By looking into these similarity matrices, we can evaluate the quality of the new representations individually. Specifically, we use the linear kernel k(u i , u j ) = u T i u j as the similarity measure for constructing these similarity matrices. Fig. <ref type="figure" target="#fig_3">2</ref> shows the similarity matrices derived from three different types of features, where we plot the edges according to the intended clusters. From the plot, we can see that the clustering on the three datasets becomes more challenging from top to bottom, especially for the YOUTUBE-6. For the Notting-Hill dataset, the similarity matrix corresponding to the multi-view method reveals the underlying clustering structure more clearly than that of each single type of features as shown in the bottom right part of the multi-view similarity matrix. Generally, this can lead a better performance for the subsequent clustering.</p><p>For the TBBTS06E12 dataset, both the top left part and bottom right part of the multi-view similarity matrix are more clear than those of each single feature. For the YOUTUBE-6 dataset, the central part of the figure corresponding to multi-view reveals the underlying structure of clusters better than that of each single view, and the spearman rank coefficient is obviously larger. Please note that, the intensity feature is obviously better than the other two types of features for the Notting-Hill and TBBTS06E12. But the Gabor gives better Spearman rank correlation coefficient for the YOUTUBE-6 dataset. Even so, with the help of the less powerful features, our multi-view clustering algorithm outperforms the best single-view case, especially on the most challenging YOUTUBE-6 dataset, our Spearman rank correlation coefficient is about 0.33 while the second performer is about 0.29. Generally, different features may work well on different datasets for single-view algorithms, and it is usually difficult to choose feature adaptively.</p><p>However, the method CMVFC relieves the limitation because it makes use of the different features simultaneously.  for constraints to build up the neighborhood system. Our method outperforms the latest best method, HMRF-com, in all the three datasets. Table <ref type="table" target="#tab_1">III</ref> shows the clustering result on Notting-Hill. On the accuracy measure, both CS-VFC and CMVFC outperform all other methods in all cases. Compared to the second performer, except CS-VFC, CMVFC has at least 26%, 20%, 25% and 23% increase in the four cases:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Qualitative &amp; quantitative results</head><p>clustering without using constraints, with using cannot-link, with must-link and with all-link constraints, respectively. The results of CS-VFC/CMVFC in all-link case are much higher than those without links, about 6% and 10% higher than the no-link case, respectively. This demonstrates the effectiveness of our methods in exploiting the pairwise constraints. Similar performance is observed on both the TBBTS06E12 and YOUTUBE-6 datasets. In terms of accuracy, CMVFC outperforms the second performer 25% and 22%, respectively. On the other hand, the method CMVFC outperforms CS-VFC significantly in all cases, which verifies the benefit of considering multi-view consistence. With the increase of the pairwise constraints, the advantage of CMVFC to CS-VFC is reduced. That is mainly because of the natural diminishing returns property for the multi-view consistence.    To further investigate the benefit of the proposed method from the joint consideration of the pairwise constrains and from the using of multi-view features for clustering, we conduct all the single-view methods using every type of features and show the best performance in Fig. <ref type="figure">4</ref>. Our single-view method (CS-VFC) outperforms the other comparisons in terms of NMI, which indicates the improvement from the pairwise constraints. In detail, the improvements over the best compared method are about 12.6%, 9.7% ,7.2%</p><p>for Notting-Hill, TBBTS06E12 and YOUTUBE-6, respectively. In the other point of view, these three features have various representation power for face clustering. Overall, LBP is a promising feature in our experiments. Moreover, the proposed multi-view method (CMVFC) further outperforms the proposed single-view method (CS-VFC) using the best feature, which indicates the benefit from using multi-view features for clustering. On average, the improvement is about 4% on these three datasets in terms of NMI.</p><p>of 0.1. The general picture is that both the pairwise constraints and multi-view consistence clearly play important roles. The performance is relatively robust for α since a relatively large value is sufficient. This demonstrates that although the must-link constraints have been incorporated in the representation step, we further improve the performance in clustering step by exploiting these priors. Similarly, an increasing performance is achieved after introducing the cannot-link constraints in the clustering step. Our method gives a relatively good performance when 0.2 ≤ γ ≤ 1. This implies that it is not always reasonable to enforce the consistence across multiple views too much.</p><p>Convergence rate: Fig. <ref type="figure" target="#fig_9">8</ref>(a) gives the clear instruction for setting the iteration number in Eq. ( <ref type="formula" target="#formula_25">17</ref>).</p><p>For each iteration, the new representation U (v) corresponding to each descriptor matrix X (v) is updated.</p><p>According to each new representation, the value of the objective function is calculated. It is observed that the value of our objective function is nearly maximized and stable when the iteration number is larger than 3. Thus, in our experiments, the iteration number is set to 5 to ensure the stable solutions. We plot the similarity matrices corresponding to U (v) at each iteration. The similarity matrix is stable after the second iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This paper has shown how to utilize the inherent benefits of a video to help face clustering. Together with multi-view features, we have proposed a novel algorithm, Constrained Multi-View Video Face Clustering (CMVFC), in which the inherent benefits are used as must-link and cannot-link constraints. We fully take advantage of must-links and cannot-links in two steps, including constrained sparse subspace representation and constrained spectral clustering. The constrained sparse subspace representation enforces our representation to focus on exploring unknown relationships. In the constrained spectral clustering step, we further exploit these constraints. Moreover, we extend our method to the multi-view framework to exploit multiple types of features and pairwise constrains simultaneously. Experiments have demonstrated the significant improvement of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1</head><label>1</label><figDesc>Fig.1shows the framework of our method. Given the input video, we extract the faces for each frame, as shown in Fig.1 (b). In our work, we employ face detector to get an initial face set. The face tracking technique is employed to link the detected face. After detecting the faces, we align them and extract the features of each facial image. The constrained matrix as shown in Fig.1(c) is built up based on the must-link and cannot-link constraints. Next, the sparse representation with these constraints is provided to obtain the sparse coefficient matrix corresponding to each feature as shown in Fig.1(d). Finally, based on these constrained sparse representations, we apply multi-view spectral clustering with pairwise constraints on the similarity matrix to get the final clustering result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>) to construct the similarity matrix W instead of the original feature based on Euclidean distance or kernels. Therefore, the pairwise constraints are incorporated into the similarity matrices to boost the clustering performance. With each row of U = [u 1 , u 2 , ..., u n ] T acting as a new representation of an original data point, we cluster them into k clusters with K-means algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visualization of similarity matrices on Notting-Hill (top row), TBBTS06E12 (middle row) and YOUTUBE-6 (bottom row) corresponding to each single feature and our multi-view method, respectively. The value in the top right corner of each figure indicates the Spearman rank correlation coefficient.</figDesc><graphic coords="14,170.38,390.71,86.67,82.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>6 Fig. 3 .</head><label>63</label><figDesc>Fig. 3. The clustering results of HMRF-com and CMVFC. The false clustering faces are highlighted by the red rectangles and the incorrect rate in each row is approximately equal to its proportion in the clusters.</figDesc><graphic coords="16,309.83,217.88,209.62,73.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 . 3 (</head><label>43</label><figDesc>Fig. 4. The performance of each method using the best features. The labels INT, GAB are short for INTENSITY and GABOR features, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>6 Fig. 5 .Fig. 6 .</head><label>656</label><figDesc>Fig. 5. The performance in terms of NMI with respect to different sampling numbers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The performance of our method with respect to errors in face detection and tracking. Error rate of tracks with respect to the threshold T of track length (a), and the confusion matrices on the data with (b) and without (c) detection errors. The elements below the red line in (b) indicate the detection errors.</figDesc><graphic coords="21,270.83,279.72,76.96,74.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Iteration number tuning of objective function on Notting-Hill. (a) The intermediate result of objective function in Eq. (17). (b)-(e) The similarity matrices corresponding to U(v) in Eq. (17) in different iterations.</figDesc><graphic coords="21,178.92,279.72,76.96,74.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF CONSTRAINTS IN DIFFERENT STEPS ON NMI AND ACCURACY (%) FOR CS-VFC.</figDesc><table><row><cell>Datasets</cell><cell>Metrics</cell><cell>NoPC</cell><cell cols="3">PCInSR PCInSC PCInBoth</cell></row><row><cell>Notting-Hill</cell><cell cols="2">NMI Accuracy 81.79 68.92</cell><cell>74.30 92.10</cell><cell>75.71 84.21</cell><cell>88.90 92.11</cell></row><row><cell>TBBTS06E12</cell><cell cols="2">NMI Accuracy 52.99 51.56</cell><cell>61.32 64.16</cell><cell>55.96 53.25</cell><cell>76.86 76.26</cell></row><row><cell>YOUTUBE-6</cell><cell cols="2">NMI Accuracy 31.63 26.32</cell><cell>29.26 40.52</cell><cell>31.36 33.33</cell><cell>52.40 51.10</cell></row><row><cell cols="6">including 17168 faces in 385 tracks. Similar to the dataset Notting-Hill, we downsample facial images</cell></row><row><cell cols="6">to 50×50 and use the 2500-dimensional vector as intensity feature. The third dataset is YOUTUBE-6,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARISON</head><label>II</label><figDesc>OF CONSTRAINTS IN DIFFERENT STEPS ON NMI AND ACCURACY (%) FOR CMVFC.</figDesc><table><row><cell>Datasets</cell><cell>Metrics</cell><cell>NoPC</cell><cell cols="3">PCInSR PCInSC PCInBoth</cell></row><row><cell>Notting-Hill</cell><cell cols="2">NMI Accuracy 86.84 77.19</cell><cell>82.89 90.78</cell><cell>86.17 93.42</cell><cell>92.07 93.42</cell></row><row><cell>TBBTS06E12</cell><cell cols="2">NMI Accuracy 54.03 63.72</cell><cell>70.85 69.10</cell><cell>74.26 68.57</cell><cell>82.88 81.74</cell></row><row><cell>YOUTUBE-6</cell><cell cols="2">NMI Accuracy 41.67 37.91</cell><cell>28.51 43.75</cell><cell>45.42 45.83</cell><cell>60.70 62.74</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>TABLE IV RESULTS (MEAN ± STANDARD DEVIATION) OF COMPARISONS ON NMI AND ACCURACY (%) ON TBBTS06E12 WITH SAMPLING NUMBER 5.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>TBBTS06E12</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Metrics</cell><cell>No-link</cell><cell>Cannot-link</cell><cell>Must-link</cell><cell>All-link</cell></row><row><cell>CSC</cell><cell>NMI Accuracy</cell><cell>56.45 ± 1.49 56.42 ± 3.72</cell><cell>43.28 ± 2.08 42.91 ± 2.96</cell><cell>55.30 ± 1.21 50.43 ± 4.99</cell><cell>46.77 ± 1.91 48.94 ± 2.90</cell></row><row><cell>CSC-AP  *</cell><cell>NMI Accuracy</cell><cell>55.21 ± 2.18 55.14 ± 2.90</cell><cell>49.83 ± 1.21 67.47 ± 0.28</cell><cell>55.39 ± 2.09 68.21 ± 0.96</cell><cell>59.39 ± 1.00 70.21 ± 0.15</cell></row><row><cell>MI-VFC</cell><cell>NMI Accuracy</cell><cell>49.51 ± 0.89 52.47 ± 1.14</cell><cell>35.07 ± 1.12 44.94 ± 1.86</cell><cell>53.70 ± 0.67 51.69 ± 1.13</cell><cell>30.79 ± 0.71 44.68 ± 0.43</cell></row><row><cell>ULDML</cell><cell>NMI Accuracy</cell><cell>57.01 ± 2.72 50.88 ± 5.67</cell><cell>55.76 ± 3.24 47.53 ± 5.21</cell><cell>57.92 ± 1.28 35.98 ± 3.00</cell><cell>59.29 ± 0.47 56.73 ± 5.93</cell></row><row><cell>HMRF-com  *</cell><cell>NMI Accuracy</cell><cell>--</cell><cell>56.43 ± 0.91 54.16 ± 3.09</cell><cell>55.51 ± 0.95 53.87 ± 1.50</cell><cell>58.38 ± 1.20 55.32 ± 0.96</cell></row><row><cell>FeatConcate</cell><cell>NMI Accuracy</cell><cell>40.31 ± 1.24 44.67 ± 1.89</cell><cell>42.61 ± 1.24 44.41 ± 0.25</cell><cell>49.42 ± 1.54 49.35 ± 1.05</cell><cell>50.75 ± 2.46 57.40 ± 4.67</cell></row><row><cell>CS-VFC</cell><cell>NMI Accuracy</cell><cell>51.56 ± 0.57 52.99 ± 2.41</cell><cell>51.17 ± 1.12 53.97 ± 2.43</cell><cell>74.49 ± 1.61 67.17 ± 3.67</cell><cell>76.86 ± 0.99 76.26 ± 2.53</cell></row><row><cell>CMVFC  *</cell><cell>NMI Accuracy</cell><cell>63.72 ± 0.31 54.03 ± 0.81</cell><cell>74.47 ± 0.96 65.06 ± 1.50</cell><cell>81.96 ± 1.45 76.49 ± 4.41</cell><cell>82.88 ± 0.71 81.74 ± 1.69</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>July 29, 2015 DRAFT</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1057-7149 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2015.2463223, IEEE Transactions on Image Processing</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by National Natural Science Foundation of China (No. 61332012 and No. 61100121), National Basic Research Program of China (2013CB329305), National High-tech R&amp;D Program of China (2014BAK11B03), and 100 Talents Programme of The Chinese Academy of Sciences. Hassan Foroosh was supported in part by the National Science Foundation under grant IIS-1212948.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr">2 33</ref> <p>(b)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Robustness with different sampling numbers</head><p>The face sampling number from tracks often affects both the clustering accuracy and the computational cost. We conduct experiments on the three datasets with all-link constraints, and test the influence of different sampling numbers. For both the Notting-Hill and TBBTS06E12 dataset, the sampling numbers range from 3 to 10. For the YOUTUBE-6 dataset, a slightly larger sampling number is taken for a better measurement of influence of sampling number, since the number of face tracks in YOUTUBE-6 dataset corresponding to each individual is much less than those of the other two datasets. As shown in Fig. <ref type="figure">5</ref>, CS-VFC mostly outperforms the previous work significantly with all different sampling numbers. Note that, although the CS-VFC achieves the promising performance, CMVFC consistently improves it under each sampling number. The general picture is that the method CMVFC clearly outperforms the other methods under different sampling number from face tracks, which implies the robustness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Robustness with detection error</head><p>Generally, it is challenging to accurately cluster video faces for a totally automatic end-to-end system.</p><p>We conduct experiments on TBBTS06E12 to evaluate the proposed method under detection error. As shown in Fig. <ref type="figure">7</ref>(a), the detection error rate of tracks degrades significantly while the face number threshold T increases from 10 to 40. This validates the reasonability of setting the threshold of track length as stated in subsection III-A. Note that, the larger threshold means the less available tracks (e.g., there are less than 19 tracks when T &gt; 138, though the error rate is 0, as indicated by the red dash line in Fig. <ref type="figure">7(a)</ref>). Therefore, we choose an appropriate value for T to well tradeoff the number of available tracks and the error rate. Specifically, we set T = 30 and obtain 267 tracks, 21 out of which have detection errors. We sample 5 faces in each face track and conduct CMVFC on this noisy data. Fig. <ref type="figure">7</ref>(b) and Fig. <ref type="figure">7</ref>(c) are confusion matrices for CMVFC on the data with/without detection errors, respectively. We adopt the same metric as in <ref type="bibr" target="#b14">[15]</ref> to evaluate the clustering result on noisy data. On the 246 clean tracks with correct face detections, our method achieves 74.39% in terms of accuracy. It is observed that our method still achieves a promising clustering result, 64.04% when 7.86% inaccurate face tracks are involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Parameter tuning</head><p>Trade-off factors: In our experiments, there are mainly three parameters, α, β and γ in Eq. ( <ref type="formula">17</ref>), which correspond to the must-link pairwise constraint, the cannot-link constraint regularization terms and the multi-view consistence, respectively. We tune one parameter by fixing the others. For all the three datasets, the default values for α and γ are 1, and 0 for β. The parameters are tuned from 0 to 1.5 with an interval </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scene-based movie summarization via role-community networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1927" to="1940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Character-based movie summarization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="855" to="858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On affine invariant clustering and automatic cast listing in movies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="304" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic cast listing in feature-length films with anisotropic manifold space</title>
		<author>
			<persName><forename type="first">O</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1513" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint manifold distance: A new approach to appearance based clustering</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="26" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Constrained clustering and its application to face clustering in videos</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3507" to="3514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Talking pictures: Temporal grouping and dialog-supervised person recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nagle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1014" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A sparsity-enforcing method for learning face features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Destrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">De</forename><surname>Mol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Odone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="188" to="201" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Local gabor binary pattern histogram sequence (lgbphs): A novel non-statistical model for face representation and recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="786" to="791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Normalization of face illumination based on large-and small-scale features</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1807" to="1821" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Clustering with multiple graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1016" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spectral clustering and transductive learning with multiple views</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1159" to="1166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Co-regularized multi-view spectral clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1413" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A new method for multi-view face clustering in video sequence</title>
		<author>
			<persName><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM Workshop</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="869" to="873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A mutual information based face clustering algorithm for movies</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vretos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Solachidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1013" to="1016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A mutual information based face clustering algorithm for movie content analysis</title>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="693" to="705" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A probabilistic framework for semi-supervised clustering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Constrained k-means clustering with background knowledge</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wagstaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schrödl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="577" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Penalized probabilistic clustering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Leen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1528" to="1567" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spectral learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kamvar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sepandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Christopher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="561" to="566" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Constrained spectral clustering through affinity propagation</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">D</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Carreira-Perpinán</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spectral clustering with two views</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>De Sa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML workshop on learning with multiple views</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Diversity-induced multi-view subspace clustering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="586" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sparse subspace clustering: Algorithm, theory, and applications</title>
		<author>
			<persName><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2765" to="2781" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust recovery of subspace structures by low-rank representation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="171" to="184" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sparse approximated nearest points for image set classification</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Manifold-manifold distance and its application to face recognition with image sets</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4466" to="4479" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Discriminative learning and recognition of image set classes using canonical correlations</title>
		<author>
			<persName><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1005" to="1018" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Manifold discriminant analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="429" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Face recognition based on image sets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cevikalp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2567" to="2573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Covariance discriminative learning: A natural and efficient approach to image set classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2496" to="2503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Image set classification using holistic multiple order statistics features and localized multi-kernel metric learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="329" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Simultaneous feature and dictionary learning for image set based face recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Prototype based feature learning for face image set classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FG</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Face recognition in movie trailers via mean sequence sparse representation-based classification</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3531" to="3538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised joint alignment of complex images</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Who is in the picture</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="264" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised metric learning for face identification in tv video</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1559" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex optimization</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An interior-point method for large-scale l1-regularized logistic regression</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Byod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gorinevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1519" to="1555" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Video face clustering via constrained sparse representation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICME</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On spectral clustering analysis and an algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Graph regularized transductive classification on heterogeneous information networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Danilevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>ECMLPKDD</publisher>
			<biblScope unit="page" from="570" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Spectral analysis of signed graphs for clustering, prediction and visualization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kunegis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lommatzsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W D</forename><surname>Luca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Albayrak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="559" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Character identification in feature-length films using global face-name matching</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1276" to="1288" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Weighted block-sparse low rank representation for face clustering in videos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="123" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Falrr: A fast low rank representation solver</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="4612" to="4620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Face recognition in unconstrained videos with matched background similarity</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="529" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A comparative study of texture measures with classification based on feature distributions</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="775" to="779" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invarianat texture classification with local binary patterns</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maenpaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Distortion invariant object recognition in the dynamic link architecture</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Vorbruggen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Der Malsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Wurtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Konen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="300" to="311" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Face recognition by elastic bunch graph matching</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K L</forename><surname>Wiskott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Fellous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Der Malsburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="775" to="779" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Flexible constrained spectral clustering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="563" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Entropy and correlation: Some comments</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">O</forename><surname>Kvalseth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="517" to="519" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">D. degree in computer science from the University of Central Florida, USA, with his dissertation nominated for the university level Outstanding Dissertation Award. After graduation, he spent about three years at ObjectVideo Inc. as a Research Scientist</title>
	</analytic>
	<monogr>
		<title level="m">He received the B.E. and M.E. degrees both in computer science from Beihang University (BUAA), China, and the Ph</title>
		<imprint>
			<date type="published" when="2004">2008 to 2012. 2004</date>
		</imprint>
		<respStmt>
			<orgName>Xiaochun Cao is a Professor of the Institute of Information Engineering, Chinese Academy of Sciences</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">He is a fellow of IET and a Senior Member of IEEE</title>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing. Changqing Zhang is currently working toward the Ph.D. degree in the School of Computer Science and Technology, Tianjin University. He received his B.S. and M.E. degrees both in the College of Computer Science, Sichuan University in 2005 and 2008, respectively. His current research interests include machine learning, data mining and computer vision</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>International Conference on Pattern Recognition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
