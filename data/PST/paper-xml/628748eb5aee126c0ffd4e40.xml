<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DEEPSTRUCT: Pretraining of Language Models for Structure Prediction</title>
				<funder ref="#_grWxcFF #_C6W9aGU">
					<orgName type="full">NSFC</orgName>
				</funder>
				<funder>
					<orgName type="full">Berkeley DeepDrive and Berkeley Artificial Intelligence Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-05-21">21 May 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
							<email>chenguangwang@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
							<email>liuxiao21@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zui</forename><surname>Chen</surname></persName>
							<email>chenzui19@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>jietang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
							<email>dawnsong@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">U</forename><forename type="middle">C</forename><surname>Berkeley</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DEEPSTRUCT: Pretraining of Language Models for Structure Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-05-21">21 May 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2205.10475v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a method for improving the structural understanding abilities of language models. Unlike previous approaches that finetune the models with task-specific augmentation, we pretrain language models on a collection of task-agnostic corpora to generate structures from text. Our structure pretraining enables zero-shot transfer of the learned knowledge that models have about the structure tasks. We study the performance of this approach on 28 datasets, spanning 10 structure prediction tasks including open information extraction, joint entity and relation extraction, named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, factual probe, intent detection, and dialogue state tracking. We further enhance the pretraining with the task-specific training sets. We show that a 10B parameter language model transfers non-trivially to most tasks and obtains state-of-the-art performance on 21 of 28 datasets that we evaluate. 1  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pretrained language models (LMs) have revolutionized NLP over the last few years <ref type="bibr" target="#b46">(Peters et al., 2018;</ref><ref type="bibr" target="#b14">Devlin et al., 2019;</ref><ref type="bibr">Radford et al., 2019b)</ref>, increasingly adept in performing the flexible and task-agnostic downstream transfer. Their transfer performance is less studied in structure prediction tasks, however. Well-studied tasks mainly focus on understanding one particular aspect of the text, such as predicting the next word that comes after as in language modeling. Unlike those downstream tasks, structure prediction requires the structural understanding of the text for further integrating multiple relevant aspects into a structure. For instance, a typical structure prediction task, called open information extraction, seeks the entire structural in- ? Equal contribution. 1 The code and datasets are available at https:// github.com/cgraywang/deepstruct.  formation in a sentence (Figure <ref type="figure" target="#fig_3">2</ref>). Different from traditional NLP tasks, structure prediction takes one step further and serves as a natural testbed for the structural understanding competence of LMs.</p><p>It is non-trivial to transfer LMs to downstream structure prediction tasks. While the structure prediction requires structural understanding, the LMs are pretrained to understand an independent aspect. For example, <ref type="bibr">GPT-3 (Brown et al., 2020)</ref> is trained to predict the next word, and BERT <ref type="bibr" target="#b14">(Devlin et al., 2019)</ref> is trained to recover the masked tokens. Recent work has made efforts in bridging the gap in transferring pretrained models to structure prediction tasks with a focus on two directions. As shown in Figure <ref type="figure" target="#fig_4">3</ref>, first, task-specific architectures are proposed to model the structures for different structure prediction tasks <ref type="bibr" target="#b68">(Stanovsky et al., 2018;</ref><ref type="bibr" target="#b64">Soares et al., 2019)</ref>. Second, task-specific data augmentation <ref type="bibr">(Paolini et al., 2021;</ref><ref type="bibr" target="#b71">Wang et al., 2021;</ref><ref type="bibr">Wei et al., 2021)</ref> is introduced, aiming to enrich text format with structure information. These approaches involve custom-designed task augmentations, impeding their usability in general structure prediction tasks.  In this paper, we improve the structural understanding capabilities of LMs. In contrast to previous approaches relying on task augmentations, we introduce structure pretraining, which systematically teaches LMs to better understand structures of text beyond independent aspects in a pretraining phase (Figure <ref type="figure" target="#fig_1">1</ref>). This enables the zero-shot transfer of knowledge that LMs learned about structures during our pretraining to downstream structure prediction tasks. For example, our zero-shot 10B parameter LM significantly outperforms the zero-shot GPT-3 (175B) on a structure prediction benchmark dataset (Figure <ref type="figure" target="#fig_1">1</ref>). We accomplish this by reformulating structure prediction as a series of unit taskstriple prediction tasks. We then train LMs on a collection of task-agnostic structural corpora to generate triples from text. The design of triple representation is important: it unifies a wide set of standard structure prediction tasks into the same task format. We apply our pretrained model DEEPSTRUCT to 28 datasets spanning 10 structure prediction tasks, including open information extraction, joint entity and relation extraction, named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, factual probe, intent detection, and dialogue state tracking. We further enhance the pretraining with multiple downstream structure prediction training sets and obtain state-of-the-art performance on 21 of 28 datasets. Our contributions are as follows:</p><p>? We improve structural understanding abilities of pretrained LMs. Compared to traditional NLP tasks that only consider the understanding of an independent aspect of the text, structural understanding takes a step further that requires the ability to integrate multiple relevant aspects into a structure. We argue that it is important for LMs to go beyond traditional understanding toward structural understanding, as it requires a higher level of intelligent competence and is more challenging. It can also benefit a wide spectrum of NLP tasks that require structure-level understanding capability.</p><p>? We propose structure pretraining, which pretrains the LMs to understand structures in the text. The basic intuition is that the standard pretraining helps LMs to understand individual aspects of the information in the text, our method learns to integrate those individual aspects into structures.</p><p>Compared to existing approaches, this method enables the zero-shot transfer of LMs to structure prediction tasks. For instance, our 10B LM produces superior zero-shot performance compared to 175B GPT-3 on a representative structure prediction task.</p><p>? We further equip our pretraining with multi-task learning and apply our method to 28 structure prediction datasets across 10 tasks. We achieve state-of-the-art performance on 21 of 28 datasets that we evaluate. We hope this can help facilitate the structural understanding research in the NLP community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Structure Pretraining</head><p>Pretrained LM</p><p>(1) Task augmented pretrain-finetune </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-task</head><p>Inference on multiple tasks The goal of our method is to improve the structural understanding capabilities of language models (LMs), i.e., understanding the structures of text. As shown in Figure <ref type="figure" target="#fig_4">3</ref>, instead of using the standard pretrain-finetune paradigm for each task, we introduce structure pretraining that aims to teach LMs to correspond to structures in a wide spectrum of tasks at the same time. We evaluate the structural understanding ability on multiple structure prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Generative Pretraining</head><p>While the LM is pretrained to understand a single aspect of the text, structural understanding aims to recover the entire structure of the text (Figure <ref type="figure" target="#fig_3">2</ref>). Structure pretraining is designed to bridge the gap via guiding LMs to produce structures from the text. It is ideal to generate arbitrary structures as needed. However, this is infeasible due to the highly complex nature of such structures.</p><p>As an alternative, we reformulate the structure prediction as a combination of triple generation tasks. We refer to a triple as (head entity; relation; tail entity) describing relations between entities. We design three pretraining tasks with a focus on predicting the entities, relations, and triples respectively. As shown in Figure <ref type="figure" target="#fig_1">1</ref>, (i) entity prediction aims to output triples regarding the entities and their types in an input sentence. We implement this via prepending "entity:" as a prefix in the input. (ii) Relation prediction aims to recover the relations and corresponding types in the input as a triple. Similarly, we add "relation:" including a task separator ":" to each input. (iii) Triple prediction outputs the entire triple structure from the input. We attach "triple:" to indicate this task. These pretraining tasks are task-agnostic to downstream tasks, enabling the zero-shot downstream transfer (Sec. 2.3).</p><p>Although the triple formulation is straightforward, we find that it is very flexible and able to model all structure prediction tasks we consider. A structure prediction task can be generally decomposed into generating the entities, relations, or triples. For example, named entity recognition predicts the entities and their types. It can be naturally represented as an entity prediction problem.  <ref type="bibr" target="#b0">(Agarwal et al., 2021)</ref> 7.9M 16M entity, relation KELM <ref type="bibr" target="#b0">(Agarwal et al., 2021)</ref> 18M 45M entity, relation WebNLG <ref type="bibr" target="#b22">(Gardent et al., 2017)</ref> 85K 261K relation ConceptNet <ref type="bibr" target="#b65">(Speer and Havasi, 2012</ref>) 610K 610K relation OPIEC <ref type="bibr" target="#b23">(Gashteovski et al., 2019)</ref> 26.8M 104M triple spectively. A summary of all downstream tasks is described in Sec. 2.2. We frame the pretraining as a conditional generation task where the input corresponds to text x, and the output y is a sequence of triples. Our pretraining can be expressed as estimating a conditional distribution p(y|x) in a probabilistic framework. We use an autoregressive LM to model p(y|x).</p><p>Pretraining Data We train the model on a collection of task-agnostic corpora including prebuilt large-scale alignments between text and triples. In particular, we use T-REx <ref type="bibr" target="#b18">(ElSahar et al., 2018)</ref>, TEKGEN and KELM <ref type="bibr" target="#b0">(Agarwal et al., 2021)</ref>, WebNLG <ref type="bibr" target="#b22">(Gardent et al., 2017)</ref>, Concept-Net <ref type="bibr" target="#b65">(Speer and Havasi, 2012)</ref>. These corpora align text to triples consisting of high-quality entities and relations in knowledge graphs (e.g., Wikidata), which are used for entity and relation prediction tasks. In addition, for triple prediction tasks, we use OPIEC <ref type="bibr" target="#b23">(Gashteovski et al., 2019</ref>) that provides open schema triples. The pretraining data statistics and the corresponding pretraining tasks are shown in Table <ref type="table" target="#tab_2">1</ref>. Appendix A.1 shows additional details of our pretraining data. Figure <ref type="figure">4</ref> shows an example of the training procedure for the entity prediction task based on the input and output sample below.  person) of the output triple then serve as the predictions (i.e., entity mention and entity type) of named entity recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tasks</head><p>It is resource-intensive to create large-scale structural understanding datasets from scratch. Therefore, we collect existing datasets in the field of structure prediction for evaluation. We consider 28 datasets spanning 10 structure prediction tasks as shown in Figure <ref type="figure" target="#fig_5">5</ref>. Detailed descriptions and sizes of datasets are shown in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Zero-Shot</head><p>The zero-shot DEEPSTRUCT refers to the setting where the pretrained model is used without any task-specific training at inference time. This differs from prior fully supervised methods. This setting is challenging as it might be difficult for humans to understand the tasks without prior examples. For example, if we are asked about "semantic role labeling" that aims to recover the predicate-argument structure, it is hard to understand what this really means. Nevertheless, the existing zero-shot setting resonates with human behaviors. For example, for named entity recognition, a human can understand and follow the instruction.</p><p>We enable the zero-shot transfer to structure prediction tasks via converting the downstream tasks to one or a combination of the pretraining tasks. As shown in Figure <ref type="figure" target="#fig_5">5</ref>, at inference time, seven structure prediction tasks are formulated as entity prediction with the prefix "entity:" attached to the input example (in blue), while one task is cast as relation prediction with the prefix "relation:" (in red). In addition, open information extraction is a triple prediction task with the prefix "triple:" (in yellow). Joint entity and relation extraction (JER) is a combination of entity and relation prediction (in purple). For the entity and relation prediction, we use the prefix "entity:" and "relation:" respectively. Besides, for each dataset, we build a schema alignment between the pretraining dataset and downstream dataset (details are described in Sec. 5). The output triples are then decoded as corresponding structure predictions based on the pre-built schema alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Multi-Task</head><p>However, the distribution of the pretraining data is not perfectly aligned with the distribution of downstream datasets. This results in a shift in the output distribution of the pretrained model. The zero-shot setting cannot perform at its best on several out-ofdistribution tasks including dialogue state tracking. The reason is that its desired output is a dialogue state, which is lacking in our task-agnostic pretraining corpora. To mitigate this, we integrate multiple structure prediction datasets into the pretraining corpora, and train our method on the mixture of the datasets. We list an example input and output format for each task in Figure <ref type="figure" target="#fig_5">5</ref>. For all datasets of a particular task, we adopt the same input and output format. We also add task name and dataset name followed by the separator ":" as a prefix to each input example. For example, we add "jer ade:" to indicate one of the JER datasets, ADE. More examples of each task and dataset are shown in Table <ref type="table" target="#tab_16">16</ref>. In contrast to fully pretrain-finetuned models that store a copy of parameters for each task, this setting is a lightweight alternative and produces a single model for all tasks, improving parameter sharing.</p><p>After multi-task training, we further finetune our method on the task-specific dataset for each task. The intuition is that finetuning is the de facto way to leverage pretrained LMs to perform downstream tasks. We aim to test an upper bound of the transfer performance of our structure pretraining via the additional finetuning phase. For both multi-task settings, we use the same data format with the training at test time. Basically, we add the task name and dataset name followed by the separator to the input example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we show that DEEPSTRUCT successfully transfers to the structure prediction tasks considered and obtain state-of-the-art results on 21 of 28 datasets we evaluate. All results are obtained via structure pretraining a pretrained 10B parameter LM, GLM <ref type="bibr" target="#b15">(Du et al., 2021)</ref>. The details of the experimental setup, datasets, and comparison methods are described in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Main Results</head><p>We have two settings as described in Sec. 2: zeroshot and multi-task. We also finetune the multi-task version on each downstream dataset. In total, we have three versions of DEEPSTRUCT. For comparison: we report the performance of TANL <ref type="bibr">(Paolini et al., 2021)</ref> when available. We also show the best performance among the task-specific models that are described in Appendix A. Table <ref type="table" target="#tab_5">2</ref> reports the results.</p><p>With the zero-shot setting, a single model is used to perform on multiple tasks without the need of any task-specific training. This is in contrast to previous approaches that rely on task-specific models and datasets for each task. In Table <ref type="table" target="#tab_6">3</ref>, we also report the zero-shot performance of <ref type="bibr">GPT-3 175B (Brown et al., 2020)</ref> on CoNLL04 and ADE (JER) via formulating the task as a question answering problem through prompting (details of the formulation are described in Sec. 5). JER requires the model to extract a set of entities and a set of relations between pairs of entities from the input text. Each predicted entity or relation has to be also assigned to an entity or a relation type. Zero-shot DEEPSTRUCT 10B outperforms zero-shot GPT-3 175B by a large margin without any prompt engineering. As shown in Table <ref type="table" target="#tab_5">2</ref>, overall, DEEPSTRUCT's zero-shot performance is still far from that of task-specific supervised models on most tasks. The only exception is that the zero-shot setting obtains the new state-of-the-art performance on the factual probe with averaging 20% P@1 improvement. This is because the taskspecific method is also zero-shot. Note that we have removed the overlapped sentences with the T-REx test sets (factual probe) from our pretraining data. The result indicates that the structure pretraining is able to adapt the LM knowledge to the tasks by making LM aware of symbolic knowledge in the pretraining corpora. Besides, the zeroshot approach generalizes well to all task-agnostic pretraining tasks including entity prediction (e.g., named entity recognition), relation prediction (e.g., relation classification), and triple prediction (e.g., open information extraction).</p><p>Similar to the zero-shot setup, we only train a single model to conduct all the downstream tasks under the multi-task setting. This is different from the supervised models that use task-specific models and parameters. We achieve state-of-the-art performance on three datasets. For the other datasets, we obtain a competitive performance within a few points of the best-compared methods. Notably, most structure prediction tasks show a large gain from zero-shot to multi-task. This suggests that most tasks are out-of-distribution of our structure pretrained model. Nevertheless, our method appears to be able to adapt to the downstream distributions, presenting a fair and strong performance with multi-task learning. Another explanation is that multi-task examples help the model better understand the downstream tasks, such as the output format of each task. We also observe strong multi-task performance on FewRel, which is a low-resource structure prediction benchmark. This suggests that the multi-task setting is beneficial in low-resource regimes via transferring knowledge from similar tasks. For our multi-task training, we leave out the ACE2005 named entity recognition dataset due to the overlap between train and test splits for different tasks. After finetuning, we obtain state-of-theart performance on 21 datasets. For instance, we obtain a +8.0 absolute improvement and a +2.9 absolute improvement on CoNLL05 Brown (semantic role labeling) and TACRED (relation classification) compared to the state-of-the-art methods.</p><p>All above results are based on a pretrained 10B parameter LM, GLM. GLM is an autoregressive LM. In addition, the context x is encoded by a bidirectional encoder. In principle, generative LMs, such as T5 <ref type="bibr" target="#b52">(Raffel et al., 2019)</ref>, BART <ref type="bibr" target="#b34">(Lewis et al., 2020)</ref> and <ref type="bibr">GPT-3 (Brown et al., 2020)</ref>, can also be used with the proposed structure pretraining for the structure prediction tasks as well. We leave this as one of the future investigations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation Studies</head><p>Pretraining Strategies As the key question of our work is to investigate how structure pretraining improves the structural understanding ability of LMs, we examine how different pretraining strategies impact the downstream performance. We evaluate the below settings on the CoNLL04 (JER). The first two settings examine the relative importance of the pretraining data: (i) With example-proportional mixing: We follow <ref type="bibr" target="#b52">(Raffel et al., 2019)</ref>  Table <ref type="table">4</ref> shows the results. First, the distribution of pretraining data does not significantly shift from that of most tasks. This limits the impact of the balanced strategy (i). The data augmentation (ii) does not bring additional benefits to the downstream performance. This confirms that the key to the success of structure prediction is our formulation that narrows down a complex structure to a set of triple prediction tasks. This allows the pretraining to capture the entities and relations that are important for tasks. Second, removing the structure pretraining (iii) provides the most direct ablation of how much structure pretraining helps. Structure pretraining significantly improves the LM in structure prediction. This is due to the gap between LM pretraining and downstream structural understanding. For example, the distribution of structure prediction datasets is different from or is considered as out-of-distribution for the pretraining data. Structure pretraining improves the adaptation to those datasets. Next, similar to the findings in Table <ref type="table" target="#tab_5">2</ref>, we find that both task-agnostic training sets (iv) and multi-tasks datasets (v) contribute to the strength of structure pretraining. In particular, finetuning is still very important to improve the downstream performance <ref type="bibr" target="#b30">(IV et al., 2021)</ref>. However, it produces a task-specific model for each dataset instead of a unified model for all tasks as in our zero-shot or multitask setup. Compared to only finetuning the model on a downstream dataset (vi), the multi-task setting obtains sizable improvements. This is because if the downstream dataset sizes are small such as of CoNLL04, multi-task learning can be extremely helpful in the low-resource regimes <ref type="bibr">(Paolini et al., 2021)</ref>. We conduct the above ablation studies using a base version of DEEPSTRUCT with 220M parameters.</p><p>Scaling Laws As it is often the case that larger models substantially improve the transferring capabilities of LMs <ref type="bibr" target="#b4">(Brown et al., 2020;</ref><ref type="bibr">Wei et al., 2021)</ref>, we explore how model scaling benefits the structure pretraining. We evaluate the effect on models with 110M, 220M, 2B, 10B parameters on JER datasets with multi-task and multi-task finetuned DEEPSTRUCT (Figure <ref type="figure" target="#fig_7">6</ref>). As expected, average performance across the datasets improves as models grow larger. We find that when the models reach the order of 10B parameters, structure pretraining obtains the best performance. The 10B parameter model significantly improves the results compared to the 110M parameter model. One reason is that for small-scale models, learning across 28 structure prediction datasets during the structure pretraining may exceed the model capacity. For larger models, structure pretraining fully utilizes the model capacity and also teaches the models to generate triples according to the downstream tasks, allowing them to generalize well to most tasks with the rest capacity. It is also interesting that the performance does not seem to significantly saturate, indicating that the performance may further improve with larger-scale models. Under both setups, we observe similar trends. We also see that the model size matters more to the multi-task setting than to the finetuned version, suggesting finetuning is able to help specifically adapt to a task given a limited model size. The main pitfall is its generalization to more tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Pretrained LMs <ref type="bibr" target="#b14">(Devlin et al., 2019;</ref><ref type="bibr">Radford et al., 2019b;</ref><ref type="bibr" target="#b79">Yang et al., 2019)</ref> are the key ingredients in contemporary NLP. Sequence-to-sequence (seq2seq) LMs target conditional generation, such as T5 <ref type="bibr" target="#b52">(Raffel et al., 2019)</ref>, BART <ref type="bibr" target="#b34">(Lewis et al., 2020)</ref> and GLM <ref type="bibr" target="#b15">(Du et al., 2021)</ref>. These models have benefited a wide range of nature language generation tasks such as summarization <ref type="bibr" target="#b85">(Zhang et al., 2020</ref>) and text infilling <ref type="bibr" target="#b88">(Zhu et al., 2019;</ref><ref type="bibr" target="#b62">Shen et al., 2020)</ref>. Recent attempts of generative prediction <ref type="bibr">(Paolini et al., 2021;</ref><ref type="bibr" target="#b61">Schick and Sch?tze, 2021;</ref><ref type="bibr" target="#b33">Lester et al., 2021)</ref> have found that seq2seq models are able to provide a unified solution for modeling a wide set of NLP tasks. While existing approaches focus on text-to-text generation, DEEP-STRUCT aims to perform text-to-triple generation.</p><p>Multi-task learning <ref type="bibr" target="#b7">(Caruana, 1997)</ref> aims to train a model for multiple tasks simultaneously. In deep learning, it is usually categorized into hard weight sharing and soft weight constraints <ref type="bibr" target="#b56">(Ruder, 2017)</ref>. In the context of NLP, weight sharing has been adopted in <ref type="bibr" target="#b10">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b80">Yang et al., 2016;</ref><ref type="bibr">Liu et al., 2020)</ref>. Since the emergence of large pretrained LMs <ref type="bibr">(Radford et al., 2019a;</ref><ref type="bibr" target="#b14">Devlin et al., 2019;</ref><ref type="bibr" target="#b79">Yang et al., 2019)</ref>, multi-task training has been shown effective to enhance LMs' transferability to downstream tasks <ref type="bibr" target="#b52">(Raffel et al., 2019)</ref>. Recent studies <ref type="bibr">(Wei et al., 2021</ref>) also show that pretrained models finetuned with abundant downstream tasks can conduct effective zero-shot learning. The main difference is that DEEPSTRUCT trains across multiple structure prediction datasets in structure pretraining with task-agnostic corpora, where we cast all datasets into triple formats.</p><p>Structure prediction is a long-standing challenge that relates to many NLP applications such as open information extraction <ref type="bibr" target="#b23">(Gashteovski et al., 2019)</ref>, named entity recognition <ref type="bibr" target="#b59">(Sang and Meulder, 2003;</ref><ref type="bibr" target="#b74">Weischedel et al., 2013)</ref>, and relation classification <ref type="bibr" target="#b86">(Zhang et al., 2017;</ref><ref type="bibr" target="#b26">Han et al., 2018;</ref><ref type="bibr" target="#b21">Gao et al., 2019)</ref>. To handle different structure prediction problems, prior work presents a variety of task-specific models in the form of sequence tagging <ref type="bibr" target="#b68">(Stanovsky et al., 2018;</ref><ref type="bibr" target="#b37">Li et al., 2019)</ref>, machine reading comprehension <ref type="bibr" target="#b87">(Zhao et al., 2020</ref>) and text classification <ref type="bibr" target="#b64">(Soares et al., 2019)</ref>, which hinders the knowledge transfer across different tasks. TANL <ref type="bibr">(Paolini et al., 2021)</ref> proposes a translation-based approach to unify different structure prediction tasks with task-specific data augmentation. By contrast, our DEEPSTRUCT unifies more structure prediction tasks via a single model and a uniform data format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Related Models Recent studies have provided unified solutions for structural prediction tasks. We focus on the comparison between our DEEP-STRUCT to the state-of-the-art TANL <ref type="bibr">(Paolini et al., 2021)</ref> and DeepEx <ref type="bibr" target="#b71">(Wang et al., 2021)</ref>. TANL <ref type="bibr">(Paolini et al., 2021)</ref> proposes task-specific data augmentation (i.e., augmented natural language) that annotates task information and predictions in the input and output respectively for each structure prediction task. The main difference is that DEEPSTRUCT decomposes the structure prediction tasks into a collection of triple generation tasks. The triple format serves as the unified representation for all considered structure prediction tasks without the need of introducing new data augmentation as in TANL. While TANL mainly works in the multi-task setting, we additionally enable the zero-shot transfer via the task-agnostic structure pretraining. DeepEx <ref type="bibr" target="#b71">(Wang et al., 2021)</ref> explores the attention matrices of pretrained LMs via beam search to generate triples for information extraction tasks. Following the search, DeepEx introduces an extra ranking stage to improve the quality of the triples. Differently, DEEPSTRUCT aims to generate the triples for a wide set structure prediction tasks in an end-to-end fashion thanks to the proposed structure pretraining.</p><p>Besides, both TANL and DeepEx explore relatively small-scale pretrained LMs. Instead, DEEP-STRUCT scales up to 10 billion parameters. Figure <ref type="figure" target="#fig_7">6</ref> shows that the performance improvements follow the scaling law <ref type="bibr" target="#b52">(Raffel et al., 2019;</ref><ref type="bibr" target="#b33">Lester et al., 2021;</ref><ref type="bibr">Wei et al., 2021;</ref><ref type="bibr" target="#b60">Sanh et al., 2021;</ref><ref type="bibr">Liu</ref>  Zero-Shot Setup For our zero-shot setup, we follow the zero-shot usage in recent pretrained LM studies <ref type="bibr" target="#b4">(Brown et al., 2020;</ref><ref type="bibr">Wei et al., 2021;</ref><ref type="bibr" target="#b60">Sanh et al., 2021)</ref>. It refers to the setting where a pretrained model is used to directly perform downstream tasks without including downstream training sets in its own pretraining data. For DEEP-STRUCT, our pretraining data is task-agnostic. For each task, we build an offline alignment between the schema of the pretraining data and the task dataset based on co-occurrence information in the pretraining data and downstream data <ref type="bibr" target="#b1">(Angeli et al., 2015)</ref>. We then manually curate the alignment. The resulting schema alignment is part of our release 1 . At test time, we convert each task to one or a combination of the pretraining tasks based on Figure <ref type="figure" target="#fig_5">5</ref>: entity, relation, or triple prediction. After producing the triples, we use the pre-built schema alignment to obtain the task predictions.</p><p>For GPT-3 zero-shot setting, we follow the prompting method introduced by GPT-3 <ref type="bibr" target="#b4">(Brown et al., 2020)</ref>. In more details, we aim to test the upper bound performance of GPT-3 for structure prediction, in particular the JER task. Therefore, instead of using standard prompts in the form of question answering, we design the prompts for "trueor-false" questions based on the ground truth. In this case, GPT-3 only answers with "yes" or "no" to produce a task prediction (Figure <ref type="figure" target="#fig_8">7</ref>), which is apparently an easier task compared to generating the structures from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We improve structural understanding capabilities of language models. We evaluate it on a wide set of structure prediction tasks including 10 tasks and 28 datasets, and successfully transfer pretrained language models to them through the proposed structure pretraining, which teaches language models to output triples from the text. We enable both zero-shot and multi-task transfer learning. DEEP-STRUCT obtains state-of-the-art results on 21 of 28 datasets. The result shows that pretrained language models can handle higher-level understanding (e.g., structural understanding), which may benefit more NLP tasks. We hope it will foster future research along the language structural understanding direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Ethical Considerations</head><p>We hereby acknowledge that all of the co-authors of this work are aware of the provided ACM Code of Ethics and honor the code of conduct. This work is mainly about the pretraining and multitask learning of LMs for structural prediction. The followings give the aspects of both our ethical considerations and our potential impacts to the community. This work uses LMs, for which the risks and potential harms are discussed in <ref type="bibr" target="#b4">(Brown et al., 2020)</ref>. There are potential undesirable biases that existed in task-agnostic data (e.g., from Wikipedia) and multi-task downstream datasets (mostly created from news articles). We do not anticipate the production of harmful outputs, especially towards vulnerable populations, after using our model or training NLP models on our datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Environmental Considerations</head><p>We adopt the pretrained LMs from the <ref type="bibr" target="#b15">(Du et al., 2021)</ref>, whose energy cost and carbon footprint during pretraining were 80.6 MWh and 4.6 tCO2e, respectively. Additionally, the structure pretraining takes less than 5% gradient-steps of the number of pretraining steps of LMs, and thus the estimated auxiliary cost for energy is comparatively smaller. In addition, training and tuning pretrained LMs on a wide range of tasks and datasets consume a plenitude of energy and increase emissions of carbon dioxide. To alleviate the problem, in this work we make efforts to study the multi-task training, which only involves training on a combination of all datasets at once. Our results (e.g., Figure <ref type="figure" target="#fig_7">6</ref>) show that, despite the gap between multi-task and multi-task finetune on smaller models, the performance gap becomes minor when the model size scales up to 10 billion parameters. This indicates that we can reduce energy consumption when training large pretrained models via employing multitask training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Task Training</head><p>We conduct the multi-task training on 8 NVIDIA DGX-A100 machines using an Adam optimizer with a 5e-6 learning rate and 0.1 weight decay. We train the model with batch size 4 per GPU for 6 epochs and use the checkpoint with the best performance on the corresponding validation set for each dataset.</p><p>Inference During the inference, length penalty and minimum target length are the most important hyperparameters. Length penalty is a float between 0 and 1 to control the GLM's generation length. The larger the length penalty is, the longer the generation length is. In general, for entity prediction tasks (e.g., NER, SRL, event extraction), a larger length penalty is used. For entity and relation prediction or triple prediction tasks (e.g., JER and OIE), a smaller one is used. For other tasks that require a specific number of output triples (e.g., relation classification, intent detection, factual probe), we trim the generation results according to the requirements of different tasks. We show details of the task-specific hyperparameters in Appendix A.2 to Appendix A.11.</p><p>Pretraining Data We apply the task-agnostic pretraining data presented in Sec. 2.1 during structure pretraining. A small portion of T-REx <ref type="bibr">(El-Sahar et al., 2018)</ref> is used in the factual probe task <ref type="bibr" target="#b47">(Petroni et al., 2020)</ref>. To avoid the test leakage, we (i) sample a small portion of the T-REx as our pretraining data, and (ii) remove samples that appeared in the T-REx dataset of the factual probe task from the pretraining data. We integrate WebNLG 2.1 and 3.0 into a WebNLG dataset. For OPIEC, We use its OPIEC-clean version. Similar to T-REx, we also sample a portion of the OPIEC for our pretraining due to its large size.</p><p>The following sections introduce the dataset formats, comparison methods, and training details for all 10 structure prediction tasks. We show additional input and output examples on all datasets in Table <ref type="table" target="#tab_16">16</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Open Information Extraction</head><p>For OIE, given a sentence, we are asked to extract triples consisting of arguments and predicates. An example of the input and output format is as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input and Output Format</head><p>Input Born in 1951 in Tbilisi, Iago is a Georgian artist.   <ref type="bibr">et al., 2013)</ref>. The statistics of all datasets are shown in Table <ref type="table" target="#tab_9">5</ref>.</p><p>Comparison Methods We compare our method DEEPSTRUCT to the following OIE systems presented in <ref type="bibr" target="#b68">(Stanovsky et al., 2018)</ref>: (i) ClausIE (Corro and Gemulla, 2013): which leverages linguistic and grammatical knowledge to split clauses in a sentence and identify their roles, (ii) OpenIE 4 3 : which integrates SRLIE <ref type="bibr" target="#b9">(Christensen et al., 2011)</ref> and Relnoun <ref type="bibr" target="#b44">(Pal et al., 2016)</ref> systems, (iii) PropS <ref type="bibr">(Stanovsky et al., 2016)</ref>: which focuses on prepositional phrases structure in sentences for OIE, (iv) RnnOIE <ref type="bibr" target="#b68">(Stanovsky et al., 2018)</ref>: a supervised recurrent neural network based approach that views the OIE task as a sequence tagging problem. We additionally compare to MAMA using BERT LARGE from <ref type="bibr" target="#b72">(Wang et al., 2020)</ref>, which proposes to leverage knowledge stored in attention matrices for OIE.</p><p>Training Details We train our model on the OIE2016 training set for 5 epochs during multi-task finetuning. The per GPU batch size is 4. During inference, for OIE2016, we choose a length penalty of 0.8. WEB, NYT, and PENN only contain the test sets. For these datasets, we use a length penalty of 0.5 and trim the outputs to only contain the first triple. We adopt the preprocessed OIE datasets provided by <ref type="bibr" target="#b68">Stanovsky et al. (2018)</ref>.</p><p>Additional Results A detailed comparison between DEEPSTRUCT and compared approaches is shown in Table <ref type="table" target="#tab_16">6</ref>. On OIE2016, NYT, and PENN datasets, DEEPSTRUCT presents significant improvements compared to the OIE systems. While on WEB, PropS <ref type="bibr">(Stanovsky et al., 2016)</ref> outperforms our method. The reason is that the arguments of WEB are very short and concise (e.g., "( google ; assimilates ; youtube )"), which aligns better with the phrase extraction paradigm of PropS. We also observe that finetuning hurts the performance on WEB and PENN. This is because we are only able to finetune DEEPSTRUCT on the OIE2016 training set and this can lead to overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Relation Classification</head><p>Given head and tail entities in the target sentence, we seek to identify the relation between them. An example is as below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input and Output Format</head><p>Input The 1976 Thomas Cup was the tenth edition of Thomas Cup, the world championship of men's international team badminton (its female counterpart is the Uber Cup). The relationship between Uber Cup and badminton is Output (Uber Cup; sport; badminton)</p><p>where "Uber Cup" and "badminton" are the corresponding head and tail entities, and "sport" is a relation from a predefined category. In addition, we augment the input sentence with the task-specific suffix "The relationship between [head entity] and [tail entity] is" following <ref type="bibr">(Paolini et al., 2021)</ref> as shown in the above example.</p><p>Datasets We evaluate on FewRel <ref type="bibr" target="#b26">(Han et al., 2018)</ref> and TACRED <ref type="bibr" target="#b86">(Zhang et al., 2017)</ref>.</p><p>? FewRel is a few-shot N-way K-shot relation classification dataset for meta learning. For all 100 relations, train (64 relations), validation (16 relations), and test set (20 relations) are constructed accordingly. We report the results on the dev set.</p><p>? TACRED is a large-scale benchmark including over 100K samples and 41 relation types.</p><p>We select the checkpoint on the dev set, and report results on the test set.</p><p>We show the dataset statistics in Table <ref type="table" target="#tab_9">5</ref>. We use F1 to evaluate the results. We parse every relation type and the corresponding head and tail entities from every original sample and formulate it as the input and output format as shown in the above example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison Methods</head><p>The compared models are as follows: (i) BERT-PAIR <ref type="bibr" target="#b21">(Gao et al., 2019)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Results</head><p>We show the results in Table <ref type="table" target="#tab_17">7</ref>. DEEPSTRUCT outperforms all supervised methods on both TACRED and FewRel. We find that our task-agnostic pretraining can significantly help improve relation classification. This is vital to few-shot settings (FewRel), where DEEPSTRUCT can achieve almost perfect F1 scores. We also notice that multi-task DEEPSTRUCT outperforms all compared approaches except for the 5-5 FewRel setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Factual Probe</head><p>Given an input sentence, and a gold head entity and relation, the task is to predict the missing tail entity in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input and Output Format</head><p>Input Daniel Bowen, born in 1970, is a Melbourne resident best known as the author of the blog, Diary of an Average Australian.</p><p>Output <ref type="bibr">(Daniel Bowen;</ref><ref type="bibr">date_of_birth;</ref><ref type="bibr">1970)</ref> where "(Daniel Bowen; date of birth; " is provided in the output, and the model is asked to generate "1970)".</p><p>Datasets We use the Google-RE dataset consisting of 3 relations ("place of birth", "place of death", and "date of birth"), and T-REx with 41 relations from the LAMA benchmark <ref type="bibr" target="#b48">(Petroni et al., 2019)</ref>. The task is evaluated using mean precision at one (P@1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison Methods</head><p>We compare to the following approaches: (i) LAMA <ref type="bibr" target="#b48">(Petroni et al., 2019)</ref> uses only the head and relation to form the query without using the oracle context, and (ii) LAMA-Oracle <ref type="bibr" target="#b47">(Petroni et al., 2020</ref>) takes (at most) five gold sentences as additional context in the query.</p><p>Both methods are based on BERT LARGE and the query is constructed based on natural language templates. For example, the Wikidata relation "place_of_birth" is aligned with a template "was born in".</p><p>Training Details As the factual probe task is usually performed without training sets, we only report DEEPSTRUCT's results in the zero-shot and multitask setting (without finetuning). We follow the task format of LAMA-Oracle <ref type="bibr" target="#b47">(Petroni et al., 2020)</ref>, which appends the query to five oracle context sentences. We use the relation labels (e.g., "place of birth") rather than the templates (e.g., "was born in") as they align better with our task-agnostic pretraining. Note that we have removed the T-REx data in the LAMA benchmark from the pretraining data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Results</head><p>Table <ref type="table" target="#tab_18">8</ref> shows the results. DEEPSTRUCT significantly outperforms compared approaches, which is attributed to the larger model size and knowledge-intensive task-agnostic pretraining. Multi-task setting actually hurts the performance due to the difference between the relation schema of downstream datasets and task-agnostic pretraining datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Joint Entity and Relation Extraction</head><p>The goal of the task is to extract entities and relations (with their type information) from a given sentence. We formulate the task as two unit tasks: the first task is entity prediction to generate the entities, while the second task is relation prediction to generate the relations. Our task formalization is different compared to traditional JER, where our two unit tasks are independent. An example is as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input and Output Format</head><p>Input Blackstone already holds a 50 percent stake in the two parks that make up Universal Orlando. Entity Output (Blackstone; instance of; organization) (parks; instance of; organization) (Universal Orlando; instance of; organization) Relation Output (Blackstone; employer; parks)</p><p>where the entity output contains entity predictions and relation output contains relation predictions.</p><p>The entity mentions are detected following the procedure below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Details</head><p>The conventional entity prediction evaluation is based on extractive span matching. To ensure a fair comparison in situations where there are multiple entities with the same surface, we adopt the following strategy: we match the spans of the generated entities from left to right in the original sentence when they are first mentioned.</p><p>If there are duplicated entities, they are matched sequentially. For example, the first generated one matches the first mention span, while the second one matches the second mention span, etc. This strategy applies to all tasks that involve entity mention detection such as named entity recognition.</p><p>Datasets We experiment on the following datasets:</p><p>? The CoNLL04 <ref type="bibr" target="#b55">(Roth and Yih, 2004)</ref> dataset: CoNLL04 consists of four types of entities ("location", "organization", "person", "other") and five types of relations ("work for", "kill", "organization based in", "live in", "located in") on sentences taken from WSJ, AP, etc, containing 922 samples for training, 231 samples for validating, and 288 samples for testing. We use the same split as in <ref type="bibr" target="#b24">(Gupta et al., 2016)</ref>. We use the same type names of entities and relations with TANL <ref type="bibr">(Paolini et al., 2021)</ref>.</p><p>? The ADE <ref type="bibr" target="#b25">(Gurulingappa et al., 2012)</ref> dataset: ADE contains annotated documents for drugrelated adverse effects over medical case reports corpus. It consists of two entity types ("Adverse-Effect" and "Drug") and one relation type ("(Has-)Adverse-Effect"). We use the same type names as in <ref type="bibr">(Paolini et al., 2021)</ref>.</p><p>? The NYT <ref type="bibr" target="#b54">(Riedel et al., 2010)</ref> dataset: NYT is a distantly-supervised joint entity and relation extraction dataset based on New York Times corpus. The dataset consists of three entity types ("PER", "ORG", and "LOC") and 24 Freebase relations. We use a preprocessed version of this dataset from <ref type="bibr">(Yu et al., 2020a)</ref> and use the same type names with TANL <ref type="bibr">(Paolini et al., 2021)</ref>.</p><p>? The ACE2005 (Walker and Consortium, 2005) dataset: ACE2005 is based on the ACE 2005 Multilingual Training Corpus. We use a preprocessed version of this dataset in <ref type="bibr" target="#b40">(Luan et al., 2019)</ref>. We make use of seven entity types and six relation types with the same type names as in TANL.</p><p>The dataset statistics are shown in Table <ref type="table" target="#tab_9">5</ref>.</p><p>Comparison Methods We compare our method DEEPSTRUCT on the four datasets to the following JER methods: Training Details We train our model on JER training sets during multi-task finetuning for (i) 10 epochs on CoNLL04, (ii) 10 epochs on ADE, (iii) 3 epochs on NYT, and (iv) 10 epochs on ACE2005.</p><p>We employ less number of epochs on NYT, as its size is much larger compared to other datasets. We find that the relation prediction task and the entity prediction task need different length penalties. Therefore, we split the training sets corresponding to the two tasks. We choose a length penalty of 0.8 for entity prediction and 0.3 for relation prediction during inference. We use the same evaluation scripts as in <ref type="bibr">(Paolini et al., 2021)</ref>. As multi-task training and finetuning for 10 folds on ADE is too expensive for DEEPSTRUCT 10B, only the first split of ADE is included in the multi-task training and finetuning. In the ablation study (Sec. 3.2), we also present a model variant with entity and relation augmentation. For this setting, we augment the output with entity boundary information. For example, for the example in Figure <ref type="figure" target="#fig_1">1</ref>, "([Iago]; instance of; person) ([Iago]; city_of_birth; [Tbilisi])" are the augmented outputs.</p><p>Additional Results Table <ref type="table" target="#tab_19">9</ref> presents the results. DEEPSTRUCT outperforms or is competitive compared to state-of-the-art supervised approaches.</p><p>We also find the multi-task DEEPSTRUCT performs competitively with previous task-specific approaches on both ADE and NYT. This indicates that multi-task trained models are cost-effective alternatives to per-task finetuned models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Named Entity Recognition</head><p>Compared to joint entity and relation extraction, named entity recognition only focuses on predicting the entities and their corresponding types in the target sentence. We show an example below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input and Output Format</head><p>Input What we need to do is to make sure that state boards, number one, have adequate funding.</p><p>Output (we; instance of; human) (state; instance of; geographical entity) (state boards; instance of; organization)</p><p>where the head entities of these triples are the entity mentions in the given sentence, and tail entities are from a predefined list of entity types.</p><p>Datasets We experiment on the following datasets:</p><p>? The CoNLL03 (Sang and Meulder, 2003) dataset: CoNLL03 (English) data was taken from the Reuters Corpus, containing 14,041 training samples, 3,250 validating samples and 3,453 testing samples. It consists four entity types ("LOC", "ORG", "PER", and "MISC"). We use the preprocessed version of this dataset from <ref type="bibr">(Li et al., 2020a)</ref>.</p><p>? The OntoNotes (Pradhan et al., 2013) dataset: OntoNotes contains 59,924 training samples, 8,528 validating samples, and 8,262 testing samples. It consists 18 different entity types (e.g., "ORG", "PER"). We use the preprocessing scripts provided by <ref type="bibr" target="#b40">(Luan et al., 2019)</ref>.</p><p>? The GENIA <ref type="bibr" target="#b43">(Ohta et al., 2002)</ref> dataset: GE-NIA consists of compiled and annotated biomedical literature, which contains 14,824 training samples, 1,855 validating samples, and 1,854 testing samples. It consists five entity types ("DNA", "RNA", "cell_line", "cell_type", and "protein"). We use a preprocessed version of this dataset <ref type="bibr">(Li et al., 2020a)</ref>.</p><p>? The ACE2005 (Walker and Consortium, 2005) dataset: ACE2005 contains 7,299 training samples, 971 validating samples, and 1,060 testing samples. Note that it is also processed based on the ACE2005 corpus but with different data splits compared to that of the ACE2005 JER dataset. It includes seven entity types. We use the preprocessed version of this dataset in <ref type="bibr">(Li et al., 2020a)</ref>, and exclude this dataset in the DEEPSTRUCT's multi-task setting due to its overlap with the ACE2005 JER dataset.</p><p>Comparison Methods We compare our method DEEPSTRUCT on the four datasets to the following NER methods: (i) BERT-MRC <ref type="bibr">(Li et al., 2020a)</ref>: this method formulates NER as a machine reading comprehension problem, (ii) BERT-MRC+DSC <ref type="bibr">(Li et al., 2020b)</ref>: this model is a dice-loss enhanced version of BERT-MRC, (iii): Cloze-CNN <ref type="bibr" target="#b3">(Baevski et al., 2019)</ref>: the model leverages cloze-style pretraining on convolutional neural networks for natural languages, (iv) GSL <ref type="bibr" target="#b2">(Athiwaratkun et al., 2020)</ref>: the method uses augmented language for intent detection, slot filling, and named entity recognition, (v) BiaffineLSTM <ref type="bibr">(Yu et al., 2020b)</ref>: the model transforms NER into dependency parsing using biaffine LSTMs, (vi) TANL <ref type="bibr">(Paolini et al., 2021)</ref>:</p><p>it presents a sequence-to-sequence extraction approach using augmented natural languages.</p><p>Training Details We train our model on NER training sets for 15 epochs on every dataset during multi-task finetuning with early stopping. The per GPU batch size is 4. We choose a length penalty of 0.8 during inference. Since some datasets may contain null predictions, we set the minimum target length to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Results</head><p>Table <ref type="table" target="#tab_20">10</ref> shows the results. DEEPSTRUCT achieves comparable performance to task-specific supervised approaches, except for OntoNotes. We suppose that OntoNotes contains a relatively large number of entity types, making it more challenging for models to use labels for considering their semantic meanings. On GENIA and ACE2005, DEEPSTRUCT outperforms state-ofthe-art task-specific methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Semantic Role Labeling</head><p>In semantic role labeling, we seek to identify the corresponding arguments in the form of spans (or the semantic roles) given a certain predicate. Consider an example as follow.</p><p>Input and Output Format The predicate is marked in the input. The model then yields arguments according to the predicate in the output with their corresponding argument types from a predefined set.</p><p>Input Scotty [ accepted ] the decision with indifference and did not enter the arguments. Output (Scotty; instance of; subject) (decision; instance of; object)</p><p>where "[ accepted ]" is the given predicate, and arguments such as "Scotty", "the decision" and their corresponding types are generated in the form of triples.</p><p>Datasets We experiment on the following datasets: CoNLL05 WSJ, CoNLL05 Brown <ref type="bibr" target="#b6">(Carreras and M?rquez, 2005)</ref> and CoNLL12 <ref type="bibr">(Pradhan et al., 2013)</ref>. Table <ref type="table" target="#tab_9">5</ref> shows the dataset statistics.</p><p>? The CoNLL05 WSJ and CoNLL05 Brown datasets: CoNLL05 WSJ and CoNLL05 Brown datasets share the same train and validation splits. They have different test sets. For CoNLL05 WSJ and CoNLL05 Brown, the corresponding test datasets are taken from the WSJ and Brown corpus respectively. The datasets consist of seven different types including "V" (verb), "A0" (subject), "A1" (object), "A2", "A3", "AM-MOD", and "AM-NEG". We use the same type names as in <ref type="bibr">(Paolini et al., 2021)</ref>.</p><p>? The CoNLL12 dataset: CoNLL12 dataset is built upon OntoNotes dataset including 39 argument types. We leverage the same type names as in <ref type="bibr">(Paolini et al., 2021)</ref>.</p><p>Comparison Methods We compare our method DEEPSTRUCT on the datasets to the following SRL models: (i) Dep and Span <ref type="bibr" target="#b37">(Li et al., 2019)</ref>: this model formulates semantic role labeling as an end-to-end dependency parsing task, (ii) BERT SRL <ref type="bibr" target="#b63">(Shi and Lin, 2019)</ref>: it is a sequence-tagging version of BERT, (iii) TANL <ref type="bibr">(Paolini et al., 2021)</ref>: this is a sequence-to-sequence extraction model using augmented natural languages.</p><p>Training Details During multi-task finetuning, for CoNLL05, the model is trained on CoNLL05 WSJ's training set, and evaluated on both CoNLL05 WSJ and CoNLL05 Brown test sets. We train DEEPSTRUCT on CoNLL05 WSJ and CoNLL12 for 5 epochs respectively. The per GPU batch size equals 4. The length penalty is 0.8.</p><p>Evaluation Details Sentences with multiple target predicates are duplicated during data preprocessing. So, each sentence is only related to one target predicate that is marked by "[]". We adopt the same evaluation scripts as in <ref type="bibr">(Paolini et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Results</head><p>Table <ref type="table" target="#tab_21">11</ref> shows the results. Both multi-task and multi-task finetuned DEEP-STRUCT outperform task-specific models by a large margin. An important reason is that Prop-Bank <ref type="bibr" target="#b32">(Kingsbury and Palmer, 2003)</ref> is included in the multi-task training. The knowledge of Prop-Bank transfers well to other SRL datasets. We find that the performance gain is significant since the large-scale model has the capacity to capture the PropBank knowledge. We also observe a minor performance drop from multi-task to multi-task finetuned DEEPSTRUCT on CoNLL05 WSJ and CoNLL12 datasets. This might be attributed to overfitting. Besides, the issue can be relieved if a better hyperparameter combination is used in the multi-task finetuning setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 Event Extraction</head><p>This task contains two sequential subtasks: (i) event triggers identification and classification: this subtask first identifies the trigger words in target sentences that refer to certain types of events, and (ii) trigger arguments identification and classification: this subtask then extracts arguments from the target sentences that can be mapped to certain roles in the event from (i).</p><p>Input where "summit" is an extracted trigger and "meet" is its corresponding trigger event. Then, based on the "summit" event, we can further extract the role of "place" in this event as "Saint Petersburg".</p><p>Datasets We experiment on the ACE2005 dataset. For detailed dataset statistics, please refer to Table <ref type="table" target="#tab_9">5</ref>.</p><p>? The ACE2005 dataset (Walker and Consortium, 2005): ACE2005 contains 33 types of event triggers, and each of them corresponds to a set of argument roles. We follow the preprocessing in TANL <ref type="bibr">(Paolini et al., 2021)</ref> and use the same evaluation scripts.</p><p>Comparison Methods We compare our method DEEPSTRUCT on the dataset to the following methods: (i) J3EE (Nguyen and Nguyen, 2019): This method presents a joint model based on a recurrent neural network to first extract mention spans for triggers and arguments and then perform pairwise classification, (ii) DyGIE++ <ref type="bibr" target="#b69">(Wadden et al., 2019)</ref>: the method leverages BERT for sequence tagging to identify mention spans and then classify each mention with triggers in pair for argument roles, (iii) TANL <ref type="bibr">(Paolini et al., 2021)</ref>: this is a sequenceto-sequence extraction approach using augmented natural languages.</p><p>Training Details We train our model on ACE2005 event trigger and argument training sets for 20 epochs during multi-task finetuning. The per GPU batch size is 4. During inference, we choose a length penalty of 0.8. The argument prediction task requires triggers as input to make predictions. An example is shown in Table <ref type="table" target="#tab_16">16</ref>. For the argument prediction task, we first generate all trigger predictions using our 10B model. If there is more than one trigger in a sentence, we will duplicate the sentence to make sure that every sample corresponds to a single trigger. The ACE2005 dataset is processed similarly to the named entity recognition task.</p><p>Additional Results Table <ref type="table" target="#tab_22">12</ref> presents the results. DEEPSTRUCT is competitive with the state-of-theart task-specific supervised models on trigger identification and classification, as well as the argument identification task. In the meantime, DEEPSTRUCT outperforms the comparison methods on the argument classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.9 Coreference Resolution</head><p>The coreference resolution aims to identify and cluster mentions in a document that refers to the same entity. An example is as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input and Output Format</head><p>Input And deterrents don't work well when an enemy values your death more than his life. Output (an enemy; refer to; his)</p><p>where "an enemy" appears as the target entity and "his" is the mention it refers to. "refer to" is provided as part of the output triple.  <ref type="bibr">, 2016)</ref>, (ii) B 3 : a singlemention based metric which computes the macro F1 of all entity mentions, and (iii) CEAF ?4 : a similarity metric based on the assumption that the coreference map should be one-to-one. Due to the limited maximum sequence length of language models, the dataset is chunked with a fixed size of 512 during data preprocessing. Following TANL <ref type="bibr">(Paolini et al., 2021)</ref>, only intra-chunk coreferences are preserved. We also use the same evaluation scripts with <ref type="bibr">(Paolini et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Results</head><p>Table <ref type="table" target="#tab_23">13</ref> shows the results. DEEPSTRUCT presents better results compared to TANL and classic task-specific supervised approaches. However, DEEPSTRUCT fails when compared with the state-of-the-art coreference method. The main reason is that this task requires taskspecific model architectures. In the meantime, we argue that it is promising to employ a unified framework for multiple structure prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.10 Dialogue State Tracking</head><p>We are presented with a dialogue between a user and an agent to identify what information is known given a list of slots by the end of each round of the conversation. An example is as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input and Output Format</head><p>Input in which by the end of this conversation, we know that the user wants to get to Pizza Hut Fen Ditton from Saint Johns College, leaving at 17:15, while the taxi's arrival time is unknown. The slots "taxi arrive by", "taxi departure", "taxi destination", and "taxi leave at" are provided for the output.</p><p>Datasets We use the MultiWOZ 2.1 <ref type="bibr" target="#b5">(Budzianowski et al., 2018;</ref><ref type="bibr">Ramadan et al., 2018;</ref><ref type="bibr" target="#b19">Eric et al., 2020;</ref><ref type="bibr" target="#b84">Zang et al., 2020)</ref>, which is a daily dialogue dataset for task-oriented conversations. We follow the preprocessing in <ref type="bibr" target="#b75">(Wu et al., 2019)</ref>. Following TANL <ref type="bibr">(Paolini et al., 2021)</ref> Training Details We finetune for 20 epochs. The maximum sequence length is 512, and the per GPU batch size is 4. Given a domain and all possible slots, DEEPSTRUCT generates triples regarding the slots: if the information is not yet provided, the tail should be "not given". We use the same type names with <ref type="bibr">(Paolini et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Results</head><p>Table <ref type="table" target="#tab_24">14</ref> shows the results. While DEEPSTRUCT does not outperform the stateof-the-art SimpleTOD, it is still competitive compared to the task-specific supervised models. This demonstrates the effectiveness of DEEPSTRUCT in dealing with different structure prediction tasks under the same architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.11 Intent Detection</head><p>Intent detection identifies the user's intent in the conversation with the agent based on a predefined list of slots. It resonates with the classical sentence classification task. Below is an example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input and Output Format</head><p>Input Show flight and prices from Kansas City to Chicago next Wednesday arriving in Chicago by 7 pm. Output (intent; is; flight and airfare) where our prediction is the "flight and airfare". The head entity "intent" and predicate "is" are given for all outputs.</p><p>Datasets We use two datasets, the ATIS dataset <ref type="bibr" target="#b28">(Hemphill et al., 1990)</ref>, which contains flight and airline-related conversation and queries, and the SNIPS dataset <ref type="bibr" target="#b12">(Coucke et al., 2018)</ref>, which consists of daily queries from the interaction between the users and dialogue agents. The dataset statistics are shown in Table <ref type="table" target="#tab_9">5</ref>.</p><p>Comparison Methods We compare our method to SF-ID <ref type="bibr">(E et al., 2019)</ref> and TANL <ref type="bibr">(Paolini et al., 2021)</ref> in this task.</p><p>Training Details We formulate the label of every sample as "(intent; is; [label])". We parse every intent from every original sample and formulate it into the input and output format as shown above.</p><p>We finetune for 20 epochs. The maximum sequence length is 512, and the per GPU batch size is 4. We report F1 for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Results</head><p>Table <ref type="table" target="#tab_25">15</ref> shows the results.</p><p>DEEPSTRUCT is comparable to task-specific supervised approaches on both ATIS and SNIPS datasets.</p><p>For TANL, we produce the results using the released code<ref type="foot" target="#foot_2">4</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Error Analysis</head><p>We analyze the errors of DEEPSTRUCT 10B multitask in recall on the CoNLL04 (JER) dataset. We specifically investigate the errors in the relation outputs. Table <ref type="table" target="#tab_17">17</ref> shows the error cases. We find that most errors are caused by minor differences between ground truth entities and predicted entities from entity outputs. For example, the predicated entity has almost the same span as the ground truth entity (e.g., "U.S." and "the U.S."). Besides, we observe some false-positive errors that are due to the noise in the datasets. In such cases, our predictions are reasonable while they are missing due to the incompleteness of human annotations.  <ref type="bibr">(Stanovsky et al., 2016)</ref> 55.6 58.9 37.2 39.1 RnnOIE <ref type="bibr" target="#b68">(Stanovsky et al., 2018)</ref> 67.0 58.1 28.3 34.5 MAMA <ref type="bibr" target="#b72">(Wang et al., 2020)</ref> 36  <ref type="bibr" target="#b8">(Chen et al., 2020)</ref> 71.5 ----BERT-PAIR <ref type="bibr" target="#b21">(Gao et al., 2019)</ref> 85.7 89.5 76.8 81.8 NLI-DeBERTa <ref type="bibr">(Sainz et al., 2021)</ref> 73.9 TANL <ref type="bibr">(Paolini et al., 2021)</ref> 71.9 93.6?5.4 97.6?3.2 82.2?5.1 89.8?3.6 TANL (multitask) <ref type="bibr">(Paolini et al., 2021)</ref> 69  <ref type="bibr" target="#b40">(Luan et al., 2019)</ref> 88.4 63.2 MRC4ERE <ref type="bibr" target="#b87">(Zhao et al., 2020)</ref> 88.9 71.9 85.5 62.1 RSAN <ref type="bibr" target="#b83">(Yuan et al., 2020)</ref> 84.6 TANL <ref type="bibr">(Paolini et al., 2021)</ref> 89.4 71.4 90.2 80.6 94.9 90.8 88.9 63.7 TANL (multitask) <ref type="bibr">(Paolini et al., 2021)</ref> 90 BERT-MRC <ref type="bibr">(Li et al., 2020a)</ref> 93.0 91.1 -86.9 BERT-MRC+DSC <ref type="bibr">(Li et al., 2020b)</ref> 93.3 92.1 Cloze-CNN <ref type="bibr" target="#b3">(Baevski et al., 2019)</ref> 93.5 GSL <ref type="bibr" target="#b2">(Athiwaratkun et al., 2020)</ref> 90.7 90.2 BiaffineLSTM <ref type="bibr">(Yu et al., 2020b)</ref> 93.5 91.3 80.5 85.4 TANL <ref type="bibr">(Paolini et al., 2021)</ref> 91.7 89.8 76.4 84.9 TANL (multitask) <ref type="bibr">(Paolini et al., 2021)</ref> 91  <ref type="bibr" target="#b63">(Shi and Lin, 2019)</ref> 88.8 82.0 86.5 TANL <ref type="bibr">(Paolini et al., 2021)</ref> 89.3 82.0 87.7 TANL (multitask) <ref type="bibr">(Paolini et al., 2021)</ref> 89  <ref type="bibr" target="#b69">(Wadden et al., 2019)</ref> 69.7 55.4 52.5 TANL <ref type="bibr">(Paolini et al., 2021)</ref> 72.9 68.4 50.1 47.6 TANL (multitask) <ref type="bibr">(Paolini et al., 2021)</ref> 71  <ref type="bibr" target="#b31">(Joshi et al., 2019)</ref> 81.4 71.7 68.8 73.9 CorefQA+SpanBERT <ref type="bibr" target="#b76">(Wu et al., 2020)</ref> 86.3 77.6 75.8 79.9 TANL <ref type="bibr">(Paolini et al., 2021)</ref> 81.0 69.0 68.4 72.8 TANL (multitask) <ref type="bibr">(Paolini et al., 2021)</ref> 78 MultiWOZ 2.1 TRADE <ref type="bibr" target="#b75">(Wu et al., 2019</ref><ref type="bibr">) 45.6 SimpleTOD (Hosseini-Asl et al., 2020)</ref> 55.7 TANL <ref type="bibr">(Paolini et al., 2021)</ref> 50.5 TANL (multitask) <ref type="bibr">(Paolini et al., 2021)</ref> 51.4</p><p>DEEPSTRUCT multi-task 53.5 w/ finetune 54.2 </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Open information extraction (Iago; Born in; 1951) Joint entity and relation extraction (Iago; instance of; person) (Iago; city_of_birth; Tbilisi) Relation classification (Iago; city_of_birth; Tbilisi) Multi-task corpora (optional) input: jer conll04: An art exhibit is at the Haka Theatre (Haka Theatre; instance of; theatre) output: ner genia: Japan began the defence of their Asian Cup title input: output: Named entity recognition (Iago; instance of; person) (Asian Cup; instance of; race) (Japan; instance of; location) (art exhibit; located in; Haka Theatre) (Iago; is a; Georgian artist) Born in 1951 in Tbilisi, Iago is a Georgian artist DeepStruct input: triple: The couple have a daughter output: (couple; have; a daughter) : The book Fly is in English output: (Fly; language; English) output: (He; instance of; human) (FIFA; instance of; club) ......</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Summary of our approach and results. Upper: an overview of DEEPSTRUCT and the proposed structure pretraining. Lower: performance of our 10B DEEPSTRUCT zero-shot and multi-task, compared with 175B GPT-3 zero-shot.</figDesc><graphic url="image-1.png" coords="1,306.43,390.18,217.69,102.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>in Tbilisi, Iago is a Georgian Predicts an independent aspect (e.g., word(s) and label(s)) Predicts a structure that integrates multiple relevant aspects</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison between structural understanding and traditional understanding of text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparing structure pretraining with standard pretrain-finetune paradigm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Summary of tasks and datasets. Blue: entity prediction task; Red: relation prediction task; Purple: entity and relation prediction task; Yellow: triple prediction task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>with a mixing rate maximum of 10K to balance the different sizes of datasets. All other components are kept the same with DEEPSTRUCT multi-task with finetuning. (ii) With entity and relation augmentation: We add special tokens "[]" to indicate the positions of the entities and relations in a sentence. Additional details are shown in Appendix A.5. (iii) No pretrain, finetune: We remove structure pretraining, and only finetune the LM on CoNLL04. (iv) Zero-shot: We only use the task-agnostic datasets and exclude the multi-task datasets in the pretraining. (v) Multi-task: We use the multi-task model without finetuning. (iv) and (v) are the same with the zero-shot and multi-task settings in Sec. 2. (vi) Finetune: The multiple downstream datasets are excluded in the structure pretraining, but the model is finetuned on CoNLL04.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Model scaling results on joint entity and relation extraction (JER) datasets. Left: entity F1; Right: relation F1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: An example of GPT-3 zero-shot setting. To predict entities, we convert the gold entity triple (Iago; instance of; person) to an entity based true-or-false question. Similarly, to predict relations, the gold relation triple (Iago; lives in; Tbilisi) is turned into a relation based true-or-false question. The task predictions are correct if the answers are "yes".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(i) SpERT (Eberts and Ulges, 2020): The BERT-based model first conducts named entity recognition formulated as sequence tagging, and performs relation classification between recognized entities; (ii) DyGIE (Luan et al., 2019): The general information extraction framework organizes dynamic spans into graphs; (iii) MRC4ERE (Zhao et al., 2020): The model formulates the joint entity and relation extraction task as machine reading comprehension; (iv) RSAN (Yuan et al., 2020): The work presents a relation-specific attention network to jointly extract entities and relations; (v) TANL (Paolini et al., 2021): It is a sequence-tosequence extraction model using augmented natural languages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Besides, traditional structure prediction tasks focusing on relations (e.g., relation classification) or triples (e.g., open information extraction) can be formulated as relation or triple prediction tasks re-</figDesc><table><row><cell>Dataset</cell><cell>#Sent.</cell><cell>#Rel. (#Tri.)</cell><cell>Task</cell></row><row><cell>T-REx (ElSahar et al., 2018)</cell><cell cols="3">6.2M 11.1M entity, relation</cell></row><row><cell>TEKGEN</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Pretraining dataset statistics and corresponding pretraining tasks. #Sent. and #Rel. denote the number of sentences and relations respectively.</figDesc><table><row><cell>entity:</cell><cell>Iago</cell><cell>is</cell><cell cols="2">born in 1951 (Iago; instance</cell><cell>of;</cell><cell>person)</cell><cell>&lt;e&gt;</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Deepstruct</cell><cell></cell><cell></cell></row><row><cell>&lt;s&gt;</cell><cell>entity:</cell><cell cols="2">Iago is</cell><cell cols="3">born in 1951 (Iago; instance of;</cell><cell>person)</cell></row><row><cell></cell><cell cols="6">Figure 4: Summary of training procedure.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>is a; Georgian artist) Relation classification (2 datasets)</head><label></label><figDesc>Input entity: Iago is born in 1951 Output (Iago; instance of; person)where the input text and output triple are aligned, and the alignment is provided by our pretraining data. Tokens are predicted autoregressively starting with &lt;s&gt; token and ending with &lt;e&gt; token. The head entity (i.e., Iago) and the tail entity (i.e.,</figDesc><table><row><cell cols="2">Event extraction</cell><cell cols="2">Semantic role labeling</cell><cell></cell><cell></cell><cell>Open information extraction</cell></row><row><cell></cell><cell>(4 datasets)</cell><cell></cell><cell>(3 datasets)</cell><cell></cell><cell></cell><cell>(4 datasets)</cell></row><row><cell cols="2">ACE2005 Trigger Id</cell><cell>CoNLL05 WSJ</cell><cell>CoNLL05 Brown CoNLL12</cell><cell>TACRED</cell><cell>FewRel 1.0</cell><cell>OIE2016 WEB NYT PENN</cell></row><row><cell></cell><cell></cell><cell></cell><cell>input</cell><cell cols="2">input</cell><cell>input</cell></row><row><cell cols="2">input ee ace2005: Barack Obama won the 44th President of the United States output</cell><cell cols="2">srl conll05: Scotty [accepted] the decision with indifference and did not enter the arguments output (decision; instance of; object)</cell><cell cols="2">rc tacred: Alice is Bob's mother. The relationship between Alice and Bob is output</cell><cell>oie oie2016: Born in 1951 in Tbilisi, Iago is a Georgian artist (Iago; Born in; 1951)</cell></row><row><cell cols="2">(Obama; instance of;</cell><cell cols="2">(Scotty; instance of; subject)</cell><cell cols="2">(Alice; mother; Bob)</cell></row><row><cell cols="2">Factual probe</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">(2 datasets)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Google-RE</cell><cell>T-REx</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>input</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">fp t-rex: Daniel, born in</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">1970, is an Astralian author</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>output</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">(Daniel; date_of_birth; 1970)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p>output</p>(Iago;</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>president) Joint entity and relation extraction (4 datasets) CoNLL04 ADE NYT ACE2005 Coreference resolution (1 datasets) CoNLL12 Dialogue state tracking (1 datasets)</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>ACE2005 Argument Id</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ACE2005 Trigger CI</cell><cell>ACE2005 Argument CI</cell><cell></cell><cell></cell><cell>input</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">cr conll12: Deterrents</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">doesn't work terribly well</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">when an enemy values your</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">death more than his life</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>output</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(an enemy; refer to; his)</cell></row><row><cell></cell><cell></cell><cell cols="3">Named entity recognition</cell><cell>Intent detection</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">(4 datasets)</cell><cell>(2 datasets)</cell></row><row><cell></cell><cell>MultiWOZ 2.1</cell><cell>OntoNotes</cell><cell>CoNLL03</cell><cell>GENIA ACE2005</cell><cell>ATIS SNIPS</cell></row><row><cell></cell><cell>input dst multiwoz: [User]: I am looking for a place to to stay that is cheap in a type of hotel. [Agent]: Okay output ([User]; hotel price range; cheap) ([User]; hotel type; hotel)</cell><cell cols="3">input ner genia: Japan began the defence of their Asian Cup title output (Japan; instance of; location) (Asian Cup; instance of; race)</cell><cell>input id atis: Play the song little robin redbreast output (intent; is; play music)</cell><cell>jer ade: The Davao Medical Center is a regional government hospital input output (hospital; part of; government) (Davao Medical Center; instance of; hospital)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DEEPSTRUCT</cell></row><row><cell>Task</cell><cell>Dataset</cell><cell></cell><cell></cell><cell cols="3">Metric Task-specific model</cell><cell>TANL</cell><cell>zero-shot</cell><cell>multi-task</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>w/ finetune</cell></row><row><cell>Open information extraction</cell><cell>OIE2016 WEB NYT PENN</cell><cell></cell><cell></cell><cell>F1</cell><cell cols="2">67.0 (Stanovsky et al., 2018) 58.9 (Stanovsky et al., 2016) 38.3 (Saha and Mausam, 2018) 42.6 (OpenIE4 3 )</cell><cell>----</cell><cell>28.1 43.8 28.9 51.0</cell><cell>71.2 50.8 43.6 54.5</cell><cell>71.3 49.1 45.0 45.1</cell></row><row><cell></cell><cell>TACRED</cell><cell></cell><cell></cell><cell></cell><cell cols="2">73.9 (Sainz et al., 2021)</cell><cell>71.9</cell><cell>36.1</cell><cell>74.9</cell><cell>76.8</cell></row><row><cell>Relation classification</cell><cell>FewRel 1.0</cell><cell cols="2">5-way 1-shot 5-way 5-shot 10-way 1-shot</cell><cell>F1</cell><cell cols="2">90.1 (Soares et al., 2019) 89.5 (Gao et al., 2019) 83.4 (Soares et al., 2019)</cell><cell cols="3">93.6?5.4 72.4?6.9 93.6?6.0 98.4?2.8 97.6?3.2 70.8?8.0 96.4?4.2 100.0?0.0 82.2?5.1 67.6?4.5 92.2?6.4 97.8?2.0</cell></row><row><cell></cell><cell></cell><cell cols="2">10-way 5-shot</cell><cell></cell><cell cols="2">81.8 (Gao et al., 2019)</cell><cell cols="3">89.8?3.6 66.4?6.3 94.6?3.6 99.8?0.6</cell></row><row><cell></cell><cell>CoNLL04</cell><cell></cell><cell></cell><cell></cell><cell cols="2">88.9 (Zhao et al., 2020) 71.9</cell><cell>90.3 71.4</cell><cell>48.3 25.8</cell><cell>87.4 69.6</cell><cell>90.7 78.3</cell></row><row><cell>Joint entity and relation extraction</cell><cell>ADE NYT</cell><cell></cell><cell></cell><cell>F1 ( Ent. Rel. )</cell><cell cols="2">89.3 (Eberts and Ulges, 2020) 78.8 -(Yuan et al., 2020) 84.6</cell><cell>91.2 83.8 94.9 90.8</cell><cell>60.7 10.6 60.5 28.6</cell><cell>90.2 83.7 95.4 93.9</cell><cell>91.1 83.8 95.9 93.3</cell></row><row><cell></cell><cell>ACE2005</cell><cell></cell><cell></cell><cell></cell><cell cols="2">88.4 (Luan et al., 2019) 63.2</cell><cell>88.9 63.7</cell><cell>31.8 5.3</cell><cell>87.8 54.0</cell><cell>90.0 66.8</cell></row><row><cell></cell><cell></cell><cell>Trigger Id</cell><cell></cell><cell></cell><cell cols="2">72.5 (Nguyen and Nguyen, 2019)</cell><cell>72.9</cell><cell>-</cell><cell>71.7</cell><cell>73.5</cell></row><row><cell>Event extraction</cell><cell>ACE2005</cell><cell cols="2">Trigger Cl Argument Id</cell><cell>F1</cell><cell cols="2">69.8 (Nguyen and Nguyen, 2019) 59.9 (Nguyen and Nguyen, 2019)</cell><cell>68.5 50.1</cell><cell>--</cell><cell>67.9 54.9</cell><cell>69.8 59.4</cell></row><row><cell></cell><cell></cell><cell cols="2">Argument Cl</cell><cell></cell><cell cols="2">52.5 (Wadden et al., 2019)</cell><cell>48.5</cell><cell>-</cell><cell>52.7</cell><cell>56.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MUC 86.3</cell><cell></cell><cell>81.0</cell><cell>-</cell><cell>63.9</cell><cell>74.9</cell></row><row><cell>Coreference resolution</cell><cell>CoNLL12</cell><cell></cell><cell cols="3">B 3 CEAF?4 75.8 77.6</cell><cell>(Wu et al., 2020)</cell><cell>69.0 68.4</cell><cell>--</cell><cell>57.7 60.2</cell><cell>71.3 73.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Ave. F1 79.9</cell><cell></cell><cell>72.8</cell><cell>-</cell><cell>60.6</cell><cell>73.1</cell></row><row><cell>Intent detection</cell><cell>ATIS SNIPS</cell><cell></cell><cell></cell><cell>F1</cell><cell cols="2">97.8 (E et al., 2019) 97.4</cell><cell>97.6 98.7</cell><cell>--</cell><cell>66.6 78.4</cell><cell>97.8 97.3</cell></row><row><cell>Semantic</cell><cell>CoNLL05 WSJ</cell><cell></cell><cell></cell><cell></cell><cell>88.8</cell><cell></cell><cell>89.3</cell><cell>-</cell><cell>95.6</cell><cell>95.2</cell></row><row><cell>role labeling</cell><cell>CoNLL05 Brown CoNLL12</cell><cell></cell><cell></cell><cell>F1</cell><cell>82.0 86.5</cell><cell>(Shi and Lin, 2019)</cell><cell>84.1 87.7</cell><cell>--</cell><cell>92.0 97.6</cell><cell>92.1 96.0</cell></row><row><cell>Named entity recognition</cell><cell>CoNLL03 OntoNotes GENIA ACE2005</cell><cell></cell><cell></cell><cell>F1</cell><cell cols="2">93.5 (Yu et al., 2020b) 90.4 (Yan et al., 2021) 80.5 (Yu et al., 2020b) 86.9 (Li et al., 2020a)</cell><cell>91.7 89.9 76.4 84.9</cell><cell>44.4 2.5 47.2 28.1</cell><cell>93.1 87.6 80.2 -</cell><cell>93.0 87.8 80.8 86.9</cell></row><row><cell>Dialogue state tracking</cell><cell>MultiWOZ 2.1</cell><cell></cell><cell></cell><cell>Joint Acc.</cell><cell cols="2">55.7 (Hosseini-Asl et al., 2020)</cell><cell>51.4</cell><cell>-</cell><cell>53.5</cell><cell>54.2</cell></row><row><cell>Factual probe</cell><cell>Google-RE T-REx</cell><cell></cell><cell></cell><cell>P@1</cell><cell cols="2">78.0 (Petroni et al., 2020) 62.6</cell><cell>--</cell><cell>97.9 85.0</cell><cell>90.3 71.0</cell><cell>--</cell></row><row><cell>Model</cell><cell></cell><cell cols="2">CoNLL04</cell><cell cols="2">ADE</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Ent.</cell><cell>Rel.</cell><cell>Ent.</cell><cell cols="2">Rel.</cell><cell></cell><cell></cell></row><row><cell cols="2">GPT-3 175B zero-shot</cell><cell cols="2">34.7 18.1</cell><cell>5.8</cell><cell cols="2">1.3</cell><cell></cell><cell></cell></row><row><cell></cell><cell>zero-shot</cell><cell cols="5">48.3 25.8 60.7 10.6</cell><cell></cell><cell></cell></row><row><cell cols="2">DEEPSTRUCT multi-task</cell><cell cols="5">87.4 69.6 90.2 83.7</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="6">w/ finetune 90.7 78.3 91.1 83.8</cell><cell></cell><cell></cell></row></table><note><p><p><p>Results on all tasks. All evaluation scores are higher the better. TANL is introduced in</p>(Paolini et al.,  2021)</p>. The bold denotes the best, and the underline indicates the second best.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Compare DEEPSTRUCT to GPT-3 (Brown et al., 2020) 175B zero-shot on CoNLL04 and ADE datasets (joint entity and relation extraction). Ent. and Rel. denote entity F1 and relation F1 respectively.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Statistics of downstream datasets.</figDesc><table><row><cell>Datasets We evaluate the performance of the</cell></row><row><cell>compared approaches on OIE benchmark datasets</cell></row><row><cell>including OIE2016 (Stanovsky and Dagan, 2016),</cell></row><row><cell>a dataset converted from QA-SRL (He et al.,</cell></row><row><cell>2015) based on Newswire and Wikipedia; three</cell></row><row><cell>datasets transformed from news corpus, in-</cell></row></table><note><p><p><p><p><p>Output (Iago; Born in; 1951) (Iago; is a; Georgian artist) where we extract arguments (e.g., Iago) and their predicates (e.g., Born in) in the form of triples as outputs from the input sentence. cluding NYT (de S?</p><ref type="bibr" target="#b13">Mesquita et al., 2013)</ref></p>, WEB (de S?</p><ref type="bibr" target="#b13">Mesquita et al., 2013)</ref></p>, PENN (Xu</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 6 :</head><label>6</label><figDesc>Results on open information extraction.</figDesc><table><row><cell></cell><cell></cell><cell>.6</cell><cell></cell><cell>54.3</cell><cell>32.9</cell><cell>33.0</cell></row><row><cell></cell><cell>zero-shot</cell><cell>28.1</cell><cell></cell><cell>43.8</cell><cell>28.9</cell><cell>51.0</cell></row><row><cell>DEEPSTRUCT</cell><cell>multi-task</cell><cell>71.2</cell><cell></cell><cell>50.8</cell><cell>43.6</cell><cell>54.5</cell></row><row><cell></cell><cell>w/ finetune</cell><cell>71.3</cell><cell></cell><cell>49.1</cell><cell>45.0</cell><cell>45.1</cell></row><row><cell></cell><cell></cell><cell>TACRED</cell><cell>5-1</cell><cell>5-5</cell><cell>FewRel 1.0 10-1</cell><cell>10-5</cell></row><row><cell cols="2">BERTEM (Soares et al., 2019)</cell><cell>70.1</cell><cell>88.9</cell><cell>-</cell><cell>82.8</cell><cell>-</cell></row><row><cell cols="2">BERTEM+MTB (Soares et al., 2019)</cell><cell>71.5</cell><cell>90.1</cell><cell>-</cell><cell>83.4</cell><cell>-</cell></row><row><cell>DG-SpanBERT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 7 :</head><label>7</label><figDesc>Results on relation classification.</figDesc><table><row><cell></cell><cell></cell><cell>.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>zero-shot</cell><cell>36.1</cell><cell>72.4?6.9</cell><cell>70.8?8.0</cell><cell>67.6?4.5</cell><cell>66.4?6.3</cell></row><row><cell>DEEPSTRUCT</cell><cell>multi-task</cell><cell>74.9</cell><cell>93.6?6.0</cell><cell>96.4?4.2</cell><cell>92.2?6.4</cell><cell>94.6?3.6</cell></row><row><cell></cell><cell>w/ finetune</cell><cell>76.8</cell><cell>98.4?2.8</cell><cell>100?0.0</cell><cell>97.8?2.0</cell><cell>99.8?0.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Google-RE</cell><cell></cell><cell>T-Rex</cell></row><row><cell cols="3">LAMA (Petroni et al., 2019)</cell><cell></cell><cell>10.5</cell><cell></cell><cell>32.3</cell></row><row><cell cols="3">LAMA-Oracle (Petroni et al., 2020)</cell><cell></cell><cell>74.3</cell><cell></cell><cell>66.0</cell></row><row><cell cols="2">DEEPSTRUCT</cell><cell>zero-shot multi-task</cell><cell></cell><cell>97.9 90.3</cell><cell></cell><cell>85.0 71.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 8 :</head><label>8</label><figDesc>Results on factual probe.</figDesc><table><row><cell></cell><cell cols="2">CoNLL04</cell><cell cols="2">ADE</cell><cell></cell><cell>NYT</cell><cell cols="2">ACE2005</cell></row><row><cell></cell><cell>Ent</cell><cell>Rel</cell><cell>Ent</cell><cell>Rel</cell><cell>Ent</cell><cell>Rel</cell><cell>Ent</cell><cell>Rel</cell></row><row><cell>SpERT (Eberts and Ulges, 2020)</cell><cell>88.9</cell><cell>71.5</cell><cell>89.3</cell><cell>78.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DyGIE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 9 :</head><label>9</label><figDesc>Results on joint entity and relation extraction.</figDesc><table><row><cell></cell><cell></cell><cell>.3</cell><cell>70.0</cell><cell>91.2</cell><cell>83.8</cell><cell>94.7</cell><cell>90.7</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>zero-shot</cell><cell>48.3</cell><cell>25.8</cell><cell>60.7</cell><cell>10.6</cell><cell>60.5</cell><cell>28.6</cell><cell>31.8</cell><cell>5.3</cell></row><row><cell>DEEPSTRUCT</cell><cell>multi-task</cell><cell>87.4</cell><cell>69.6</cell><cell>90.2</cell><cell>83.7</cell><cell>95.4</cell><cell>93.9</cell><cell>87.8</cell><cell>54.0</cell></row><row><cell></cell><cell>w/ finetune</cell><cell>90.7</cell><cell>78.3</cell><cell>91.1</cell><cell>83.8</cell><cell>95.9</cell><cell>93.3</cell><cell>90.0</cell><cell>66.8</cell></row><row><cell></cell><cell></cell><cell cols="2">CoNLL03</cell><cell cols="2">OntoNotes</cell><cell></cell><cell>GENIA</cell><cell cols="2">ACE2005</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 10 :</head><label>10</label><figDesc>Results on named entity recognition.</figDesc><table><row><cell></cell><cell></cell><cell>.7</cell><cell>89.4</cell><cell>76.4</cell><cell>-</cell></row><row><cell></cell><cell>zero-shot</cell><cell>44.4</cell><cell>42.5</cell><cell>47.2</cell><cell>28.1</cell></row><row><cell>DEEPSTRUCT</cell><cell>multi-task</cell><cell>93.1</cell><cell>87.6</cell><cell>80.2</cell><cell>-</cell></row><row><cell></cell><cell>w/ finetune</cell><cell>93.0</cell><cell>87.8</cell><cell>80.8</cell><cell>86.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 11 :</head><label>11</label><figDesc>Results on semantic role labeling.</figDesc><table><row><cell></cell><cell></cell><cell>.1</cell><cell></cell><cell>84.1</cell><cell>87.7</cell></row><row><cell>DEEPSTRUCT</cell><cell>multi-task w/ finetune</cell><cell>95.6 95.2</cell><cell></cell><cell>92.0 92.1</cell><cell>97.6 96.0</cell></row><row><cell></cell><cell></cell><cell>Trigger Id</cell><cell>Trigger Cl</cell><cell>Argument Id</cell><cell>Argument Cl</cell></row><row><cell cols="2">J3EE (Nguyen and Nguyen, 2019)</cell><cell>72.5</cell><cell>69.8</cell><cell>59.9</cell><cell>52.1</cell></row><row><cell>DyGIE++</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 12 :</head><label>12</label><figDesc>Results on event extraction (ACE2005).</figDesc><table><row><cell></cell><cell></cell><cell>.8</cell><cell>68.5</cell><cell>48.5</cell><cell>48.5</cell></row><row><cell>DEEPSTRUCT</cell><cell>multi-task w/ finetune</cell><cell>71.7 73.5</cell><cell>67.9 69.8</cell><cell>54.9 59.4</cell><cell>52.7 56.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CoNLL12</cell><cell></cell></row><row><cell></cell><cell></cell><cell>MUC</cell><cell>B 3</cell><cell>CEAF ?4</cell><cell>Avg. F1</cell></row><row><cell cols="2">Higher-order c2f-coref (Lee et al., 2018)</cell><cell>80.4</cell><cell>70.8</cell><cell>67.6</cell><cell>73</cell></row><row><cell>BERT+c2f-coref</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 13 :</head><label>13</label><figDesc>Results on coreference resolution.</figDesc><table><row><cell></cell><cell></cell><cell>.7</cell><cell>65.7</cell><cell>63.8</cell><cell>69.4</cell></row><row><cell>DEEPSTRUCT</cell><cell>multi-task w/ finetune</cell><cell>63.9 74.9</cell><cell>57.7 71.3</cell><cell>60.2 73.1</cell><cell>60.6 73.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 14 :</head><label>14</label><figDesc>Results on dialogue state tracking.</figDesc><table><row><cell>ATIS</cell><cell>SNIPS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 15 :</head><label>15</label><figDesc>Results on intent detection.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://github.com/THUDM/GLM</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>https://github.com/dair-iitd/OpenIE-standalone</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://github.com/amazon-research/tanl</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>We would like to thank the anonymous reviewers for their suggestions and comments. This material is in part based upon work supported by <rs type="funder">Berkeley DeepDrive and Berkeley Artificial Intelligence Research</rs>. <rs type="person">Xiao Liu</rs>, <rs type="person">Zui Chen</rs>, <rs type="person">Haoyun Hong</rs>, and <rs type="person">Jie Tang</rs> are supported by the <rs type="funder">NSFC</rs> for <rs type="grantName">Distinguished Young Scholar</rs> (<rs type="grantNumber">61825602</rs>) and <rs type="funder">NSFC</rs> (<rs type="grantNumber">61836013</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_grWxcFF">
					<idno type="grant-number">61825602</idno>
					<orgName type="grant-name">Distinguished Young Scholar</orgName>
				</org>
				<org type="funding" xml:id="_C6W9aGU">
					<idno type="grant-number">61836013</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experimental Setup</head><p>A.1 Implementation Details Model Architecture We leverage the Generalized Language Model (GLM) <ref type="bibr" target="#b15">(Du et al., 2021)</ref> as our base language model pretrained on autoregressive blank infilling objectives. GLM follows an adaptive encoder-decoder architecture. It improves the pretrain-finetune consistency via clozestyle finetuning. GLM adopts the Byte Pair Encoding <ref type="bibr">(Radford et al., 2019b)</ref>, covering 50,257 tokens. In this work, we leverage the models in four different scales: 110M, 220M, 2B, and 10B 2 . The 110M model is pretrained over English Wikipedia and BookCorpus, and the others are pretrained over the Pile corpora <ref type="bibr" target="#b20">(Gao et al., 2021)</ref>. The Pile corpora are regarded as the similar corpora for training GPT-3. GLM outperforms T5 on text summarization, which shares a similar nature with structure prediction tasks. Compared to GPT-3, GLM is a bidirectional model and is able to perform autoregressive generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structure Pretraining Procedure</head><p>Task-Agnostic Pretraining We conduct the pretraining on 8 NVIDIA DGX-A100 machines using an Adam optimizer with a 5e-6 learning rate and 0.1 weight decay. We train the model with batch size 4 per GPU for 3 epochs and use the checkpoint of the last iteration.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training</title>
		<author>
			<persName><forename type="first">Oshin</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heming</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3554" to="3565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Leveraging linguistic structure for open domain information extraction</title>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><forename type="middle">Jose</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Johnson</forename><surname>Premkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="344" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Augmented natural language for generative sequence labeling</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C?cero</forename><surname>Nogueira Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Krone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="375" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cloze-driven pretraining of self-attention networks</title>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5359" to="5368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mc-Candlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multiwoz -A large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling</title>
		<author>
			<persName><forename type="first">Pawel</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo-Hsiang</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I?igo</forename><surname>Casanueva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milica</forename><surname>Osman Ramadan</surname></persName>
		</author>
		<author>
			<persName><surname>Gasic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5016" to="5026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2005 shared task: Semantic role labeling</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llu?s</forename><surname>M?rquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="152" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Efficient long-distance relation extraction with dg-spanbert</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Hoehndorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An analysis of open information extraction based on semantic role labeling</title>
		<author>
			<persName><forename type="first">Janara</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth international conference on Knowledge capture</title>
		<meeting>the sixth international conference on Knowledge capture</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: deep neural networks with multitask learning</title>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Clausie: clause-based open information extraction</title>
		<author>
			<persName><forename type="first">Luciano</forename><surname>Del</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corro</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="355" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Snips voice platform: an embedded spoken language understanding system for privateby-design voice interfaces</title>
		<author>
			<persName><forename type="first">Alice</forename><surname>Coucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alaa</forename><surname>Saade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Th?odore</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Caulier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cl?ment</forename><surname>Doumouro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Gisselbrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Caltagirone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ma?l</forename><surname>Primet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Dureau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Effectiveness and efficiency of open relation extraction</title>
		<author>
			<persName><forename type="first">Filipe</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S?</forename><surname>Mesquita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Schmidek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denilson</forename><surname>Barbosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-ACL</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="447" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">All NLP tasks are generation tasks: A general pretraining framework</title>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A novel bi-directional interrelated model for joint intent detection and slot filling</title>
		<author>
			<persName><forename type="first">E</forename><surname>Haihong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiqing</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongfu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meina</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5467" to="5471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Span-based joint entity and relation extraction with transformer pre-training</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Eberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Ulges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI-PAIS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2006" to="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">T-rex: A large scale alignment of natural language with knowledge base triples</title>
		<author>
			<persName><forename type="first">Hady</forename><surname>Elsahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavlos</forename><surname>Vougiouklis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arslen</forename><surname>Remaci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><forename type="middle">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fr?d?rique</forename><surname>Laforest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Simperl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC-ELRA</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multiwoz 2.1: A consolidated multidomain dialogue dataset with state corrections and state tracking baselines</title>
		<author>
			<persName><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shachi</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanchit</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adarsh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anuj</forename><forename type="middle">Kumar</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilek</forename><surname>Hakkani-T?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="422" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The pile: An 800gb dataset of diverse text for language modeling</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Presser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fewrel 2.0: Towards more challenging few-shot relation classification</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6249" to="6254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The webnlg challenge: Generating text from RDF data</title>
		<author>
			<persName><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INLG</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="124" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">OPIEC: an open information extraction corpus</title>
		<author>
			<persName><forename type="first">Kiril</forename><surname>Gashteovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Wanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Hertling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Broscheit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Table filling multi-task recurrent neural network for joint entity and relation extraction</title>
		<author>
			<persName><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Andrassy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COL-ING</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2537" to="2547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Development of a benchmark corpus to support the automatic extraction of drugrelated adverse effects from medical case reports</title>
		<author>
			<persName><forename type="first">Harsha</forename><surname>Gurulingappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdul</forename><surname>Mateen Rajput</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angus</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juliane</forename><surname>Fluck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Hofmann-Apitius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Toldo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomed. Informatics</title>
		<imprint>
			<biblScope unit="page" from="885" to="892" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fewrel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4803" to="4809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Question-answer driven semantic role labeling: Using natural language to annotate natural language</title>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="643" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The ATIS spoken language systems pilot corpus</title>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">T</forename><surname>Hemphill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">J</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">R</forename><surname>Doddington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech and Natural Language: Proceedings of a Workshop</title>
		<meeting><address><addrLine>Hidden Valley, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990-06-24">1990. June 24-27, 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A simple language model for task-oriented dialogue</title>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Hosseini-Asl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Semih</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Cutting down on prompts and parameters: Simple few-shot learning with language models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivana</forename><surname>Balazevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">BERT for coreference resolution: Baselines and analysis</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5802" to="5807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Higher-order coreference resolution with coarse-tofine inference</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Treebanks and lexical Theories</title>
		<meeting>Treebanks and lexical Theories</meeting>
		<imprint>
			<date type="published" when="2003">2003. 2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="687" to="692" />
		</imprint>
	</monogr>
	<note>NAACL-HLT</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3045" to="3059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">BART: denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">Marjan Ghazvininejad,. 2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">2020a. A unified MRC framework for named entity recognition</title>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<biblScope unit="page" from="5849" to="5859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dice loss for dataimbalanced NLP tasks</title>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="465" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dependency or span, end-to-end uniform semantic role labeling</title>
		<author>
			<persName><forename type="first">Zuchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shexia</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6730" to="6737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixuan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hoifung Poon, Guihong Cao, and Jianfeng Gao. 2020. The microsoft toolkit of multitask deep neural networks for natural language understanding</title>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueyun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Awa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A general framework for information extraction using dynamic span graphs</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3036" to="3046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Which coreference evaluation metric do you trust? a proposal for a link-based entity aware metric</title>
		<author>
			<persName><forename type="first">Sadat</forename><surname>Nafise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Moosavi</surname></persName>
		</author>
		<author>
			<persName><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">One for all: Neural joint modeling of entities and events</title>
		<author>
			<persName><forename type="first">Minh</forename><surname>Trung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thien</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Huu Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6851" to="6858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The genia corpus: An annotated research abstract corpus in molecular biology domain</title>
		<author>
			<persName><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuka</forename><surname>Tateisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin-Dong</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="82" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Demonyms and compound relational nouns in nominal open ie</title>
		<author>
			<persName><forename type="first">Harinder</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Automated Knowledge Base Construction</title>
		<meeting>the 5th Workshop on Automated Knowledge Base Construction</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="35" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">C?cero Nogueira dos Santos, Bing Xiang, and Stefano Soatto. 2021. Structured prediction as translation between augmented natural languages</title>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Paolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Krone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishita</forename><surname>Anubhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">How context affects language models&apos; factual predictions</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><surname>Riedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2019</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Towards robust linguistic analysis using ontonotes</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tou</forename><surname>Hwee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Bj?rkelund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>OpenAI blog</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Large-scale multi-domain belief tracking with knowledge sharing</title>
		<author>
			<persName><forename type="first">Pawel</forename><surname>Osman Ramadan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milica</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName><surname>Gasic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="432" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML-PKDD</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A linear programming formulation for global inference in natural language tasks</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">An overview of multi-task learning in deep neural networks</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Open information extraction from conjunctive sentences</title>
		<author>
			<persName><forename type="first">Swarnadeep</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mausam</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COL-ING</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2288" to="2299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Ander Barrena, and Eneko Agirre. 2021. Label verbalization and entailment for effective zero and fewshot relation extraction</title>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Sainz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oier</forename><surname>Lopez De Lacalle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="1199" to="1212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fien</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meulder</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Multitask prompted training enables zero-shot task generalization</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saiful</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urmish</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanya</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taewoon</forename><surname>Szczechla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunjan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nihal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debajyoti</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><surname>Tian-Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Manica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng Xin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshit</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Bawden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<editor>Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault F?vry, Jason Alan Fries, Ryan Teehan</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>CoRR</publisher>
			<pubPlace>Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">It&apos;s not just size that matters: Small language models are also few-shot learners</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2339" to="2352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Blank language models</title>
		<author>
			<persName><forename type="first">Tianxiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Quach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5186" to="5198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Simple BERT models for relation extraction and semantic role labeling</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Matching the blanks: Distributional similarity for relation learning</title>
		<author>
			<persName><forename type="first">Baldini</forename><surname>Livio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><surname>Kwiatkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2895" to="2905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Representing general relational knowledge in conceptnet 5</title>
		<author>
			<persName><forename type="first">Robyn</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3679" to="3686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Creating a large benchmark for open information extraction</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2300" to="2305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Getting more out of syntax with props</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Ficler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Supervised open information extraction</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="885" to="895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Entity, relation, and event extraction with contextualized span representations</title>
		<author>
			<persName><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulme</forename><surname>Wennberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5783" to="5788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Walker and Linguistic Data Consortium</title>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACE 2005 Multilingual Training Corpus. Linguistic Data Consortium</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Zero-shot information extraction as a unified text-to-triple translation</title>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyun</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1225" to="1238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Language models are open knowledge graphs</title>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><surname>Du</surname></persName>
		</author>
		<editor>M. Dai, and Quoc V. Le. 2021</editor>
		<imprint/>
	</monogr>
	<note>Finetuned language models are zero-shot learners</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Franchini</surname></persName>
		</author>
		<title level="m">Ontonotes release 5.0 ldc2013t19. Linguistic Data Consortium, Philadelphia</title>
		<imprint>
			<publisher>PA</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Transferable multi-domain state generator for task-oriented dialogue systems</title>
		<author>
			<persName><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Hosseini-Asl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="808" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Corefqa: Coreference resolution as querybased span prediction</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arianna</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6953" to="6963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Open information extraction with tree kernels</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mi-Young</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randy</forename><surname>Goebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denilson</forename><surname>Barbosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="868" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">A unified generative framework for various NER subtasks</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL/IJCNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5808" to="5822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Multi-task cross-lingual sequence tagging from scratch</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Joint extraction of entities and relations based on a novel decomposition strategy</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobo</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI-PAIS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2282" to="2289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Named entity recognition as dependency parsing</title>
		<author>
			<persName><forename type="first">Juntao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6470" to="6476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">A relation-specific attention network for joint entity and relation extraction</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiannan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeliang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4054" to="4060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Multiwoz 2.2 : A dialogue dataset with additional annotation corrections and state tracking baselines</title>
		<author>
			<persName><forename type="first">Xiaoxue</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivas</forename><surname>Sunkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jindong</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">PEGASUS: pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11328" to="11339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Positionaware attention and supervised data improve slot filling</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Asking effective and diverse questions: A machine reading comprehension based framework for joint entity-relation extraction</title>
		<author>
			<persName><forename type="first">Tianyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunbo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3948" to="3954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Wanrong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Text infilling</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
