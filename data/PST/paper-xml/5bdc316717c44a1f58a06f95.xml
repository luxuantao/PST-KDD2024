<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Autoencoder-like Nonnegative Matrix Factorization for Community Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fanghua</forename><surname>Ye</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Chuan</forename><surname>Chen</surname></persName>
							<email>chenchuan@mail.sysu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Zibin</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">National Engineering Research Center of Digital Life</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Autoencoder-like Nonnegative Matrix Factorization for Community Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C1A02C64753907ABAC1357FCFEB5418C</idno>
					<idno type="DOI">10.1145/3269206.3271697</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep nonnegative matrix factorization</term>
					<term>Community detection</term>
					<term>Graph clustering</term>
					<term>Deep learning</term>
					<term>Network analytics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Community structure is ubiquitous in real-world complex networks. The task of community detection over these networks is of paramount importance in a variety of applications. Recently, nonnegative matrix factorization (NMF) has been widely adopted for community detection due to its great interpretability and its natural fitness for capturing the community membership of nodes. However, the existing NMF-based community detection approaches are shallow methods. They learn the community assignment by mapping the original network to the community membership space directly. Considering the complicated and diversified topology structures of real-world networks, it is highly possible that the mapping between the original network and the community membership space contains rather complex hierarchical information, which cannot be interpreted by classic shallow NMF-based approaches. Inspired by the unique feature representation learning capability of deep autoencoder, we propose a novel model, named Deep Autoencoder-like NMF (DANMF), for community detection. Similar to deep autoencoder, DANMF consists of an encoder component and a decoder component. This architecture empowers DANMF to learn the hierarchical mappings between the original network and the final community assignment with implicit low-to-high level hidden attributes of the original network learnt in the intermediate layers. Thus, DANMF should be better suited to the community detection task. Extensive experiments on benchmark datasets demonstrate that DANMF can achieve better performance than the state-of-the-art NMF-based community detection approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Many real-world complex interaction systems in nature and society can be characterized by complex networks, such as social networks, collaboration networks, citation networks, biological neural networks, and protein interaction networks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b50">51]</ref>. These networks often consist of functional units, which manifest in the form of communities, i.e., groups of nodes with dense internal connections and sparse external connections <ref type="bibr" target="#b10">[11]</ref>. It is well understood that analyzing the underlying community structure is of significant importance to reveal the organizational patterns and structural functions of network systems. Besides, community detection has boosted diversified practical applications, such as advertising, viral marketing, friend recommendation, and infectious disease control <ref type="bibr" target="#b8">[9]</ref>, to name but a few.</p><p>Over the past two decades, a great deal of effort has been devoted to analyzing the community structure of networks. Thus a plethora of community detection approaches have been proposed and successfully applied to specific networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b43">44]</ref>. Traditional community detection approaches seek to find the optimal community structure via optimizing certain criteria, e.g., modularity <ref type="bibr" target="#b23">[24]</ref>, normalized cut <ref type="bibr" target="#b30">[31]</ref>, permanence <ref type="bibr" target="#b6">[7]</ref>, and conductance <ref type="bibr" target="#b17">[18]</ref>. These approaches usually assign each node to only one community, which contradicts the fact that a node can naturally participate in multiple communities. For example, a person can join in several discussion groups in an online forum, a researcher may be active in several areas. In recent years, nonnegative matrix factorization (NMF) has been broadly adopted for community detection <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref>, mostly because of the better interpretability derived from the nonnegative constraints and its natural fitness for disjoint and overlapping community detection. NMF-based community detection approaches approximately factorize the adjacency matrix A of a given network into two nonnegative factor matrices U and V, i.e., A ≈ UV (U ≥ 0, V ≥ 0). As such, each column of the factor matrix V can be interpreted as the propensity of a node belonging to different communities (i.e., the community membership), and the factor matrix U can be treated as the mapping between the original network and the community membership space. Some NMF-variants have also been utilized to deal with the community detection task, including bayesian NMF <ref type="bibr" target="#b27">[28]</ref>, nonnegative matrix tri-factorization <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b48">49]</ref>, and binary NMF <ref type="bibr" target="#b49">[50]</ref>.</p><p>Although various NMF-based approaches have been developed for community detection and promising performance has been delivered on some specific applications, it is still a big challenge to identify the intrinsic community structure of networks <ref type="bibr" target="#b9">[10]</ref>. In addition, the existing NMF-based community detection approaches are all shallow methods. As aforementioned, there is only one layer mapping between the original network and the community membership space. Considering the complicated and diversified organizational patterns of real-world networks, it is highly possible that the mapping between the original network and the community membership space contains rather complex hierarchical and structural information with implicit lower-level hidden attributes, which cannot be interpreted by classic shallow NMF-based community detection approaches. Intuitively, similar nodes are more likely to be contained in the same community. In this regard, the classic shallow NMF-based methods actually learn the community-level similarity between nodes directly. Recently, deep autoencoder has been widely applied in unsupervised learning problems due to its unique feature representation learning capability <ref type="bibr" target="#b13">[14]</ref>. Besides, deep autoencoder is an excellent scheme to narrow the gap between the lower-level abstraction and the higher-level abstraction of the original data <ref type="bibr" target="#b1">[2]</ref>. Inspired by deep autoencoder, we can argue that by further factoring the mapping U, in a way that each factor adds an extra layer of abstraction of the similarity between nodes from lower level to higher level, we can then obtain a better community-level similarity between nodes (i.e., a more accurate community membership matrix V), as demonstrated in Figure <ref type="figure" target="#fig_0">1</ref>. For example, we can learn the similarity between nodes from the first-order proximity <ref type="bibr" target="#b33">[34]</ref>, to the degree assortativity <ref type="bibr" target="#b7">[8]</ref>, the structural identity <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30]</ref>, and finally the community-level similarity.</p><p>Based on the discussions above, in this paper, we propose a novel model, named Deep Autoencoder-like NMF (DANMF), to deal with the community detection task. Instead of merely applying the concept of NMF to a multi-layer structure as shown in Figure <ref type="figure" target="#fig_0">1</ref> this way, DANMF inherits the representation learning capability of deep autoencoder <ref type="bibr" target="#b1">[2]</ref>, while it improves the model's interpretability due to the nonnegative constraints, and it is suited for both disjoint community detection and overlapping community detection. Besides, DANMF incorporates a graph regularizer to respect the intrinsic geometric structure of node pairs. The overall framework of DANMF is illustrated in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>Our main contributions can be summarized as follows:</p><p>• We propose a deep autoencoder-like NMF model, namely DANMF, to deal with the community detection task.</p><p>To the best of our knowledge, it is the first approach to introduce deep NMF for community detection. • We develop an efficient learning algorithm to optimize the proposed DANMF model, inspired by recent advances in deep learning <ref type="bibr" target="#b13">[14]</ref>. • We conduct extensive experiments to evaluate the effectiveness and efficiency of DANMF. The results demonstrate that DANMF is superior over the state-of-the-art shallow NMF-based community detection methods. The rest of this paper is organized as follows. Section 2 provides an overview of the related work. Section 3 describes the proposed DANMF model in detail. The learning algorithm is presented in Section 4. Then, we report the experimental results in Section 5. Finally, we conclude this paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we briefly review the related work regarding community detection and deep matrix factorization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Community Detection</head><p>Real-world complex networks often exhibit distinct characteristics, one of them is the presence of densely connected subnetworks, also referred to as communities. The task of community detection is to find the community structure of a given network. The problem has been a very popular research Session 9B: IR Applications CIKM <ref type="bibr">'18, October 22-26, 2018</ref>, Torino, Italy topic in recent years and lots of effort has been devoted to developing delicate community detection methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b41">42]</ref>. However, there is no consensus on the formalization of the community detection task and a variety of criteria are proposed to characterize the property of a community, such as modularity <ref type="bibr" target="#b23">[24]</ref>, normalized cut <ref type="bibr" target="#b30">[31]</ref>, permanence <ref type="bibr" target="#b6">[7]</ref>, and conductance <ref type="bibr" target="#b17">[18]</ref>. For a detailed review of such criteria, please refer to <ref type="bibr" target="#b5">[6]</ref>. Among these criteria, modularity has received the most extensive attention, which requires that the number of edges within a community should be significantly larger than the expected number of edges when all edges are randomly distributed. The typical modularity-based methods include greedy algorithm <ref type="bibr" target="#b22">[23]</ref>, Louvain <ref type="bibr" target="#b2">[3]</ref>, and spectral optimization <ref type="bibr" target="#b23">[24]</ref>. However, most of these methods aim to find disjoint communities, which contradicts the fact that a node can naturally participate in multiple communities.</p><p>As another research topic, NMF has emerged as an imperative tool for clustering analysis due to its powerful interpretability. The key of NMF is to reconstruct the original data from low-dimensional representations. With the nonnegative constraints, NMF naturally fits into disjoint community detection and overlapping community detection. As a result, numerous NMF-based community detection approaches have been proposed <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50]</ref>. For example, Psorakis et al. <ref type="bibr" target="#b27">[28]</ref> utilize a bayesian generative model to extract communities, which puts a half-normal prior over each community and then maximizes the log-likelihood of generating the original network. Zhang et al. <ref type="bibr" target="#b46">[47]</ref> propose a preference-based NMF model to incorporate the implicit link preference information into overlapping community detection based on a basic assumption that a node prefers to build links with nodes inside its community than those outside. Yang and Leskovec <ref type="bibr" target="#b42">[43]</ref> develop a scalable NMF-based model, which can be applied to detect densely overlapping, hierarchically nested as well as non-overlapping communities in massive networks. Recently, several network embedding techniques have also been employed to detect communities <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40]</ref>, which are able to learn higher-order similarity between nodes. These methods have been proven to be closely related to NMF or standard matrix factorization as well <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Matrix Factorization</head><p>It is common that complex data objects consist of hierarchical attributes, each of which represents a different level of abstract understanding of the objects. This phenomenon has motivated the rapid development of deep learning, which is a powerful technique to do representation learning <ref type="bibr" target="#b1">[2]</ref>. As the success of deep learning, there have been some explorations on deep matrix factorization <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45]</ref>. The general idea of them is to stack one-layer matrix factorization into multiple layers, in the hope that hierarchical mappings can be obtained. In <ref type="bibr" target="#b36">[37]</ref>, a multi-layer semi-NMF model with a complete deep architecture is proposed to automatically learn a hierarchy of attributes to facilitate clustering tasks. In <ref type="bibr" target="#b31">[32]</ref>, Song et al. propose a structure of multi-layer NMF for classification tasks, where non-smooth NMF is adopted to solve typical NMF in each layer. A sparse deep NMF model is then proposed and successfully applied to explore the sparse structure of data objects by using the Nesterov's accelerated gradient descent algorithm <ref type="bibr" target="#b12">[13]</ref>. More recently, Yu et al. <ref type="bibr" target="#b44">[45]</ref> propose a deep non-smooth NMF architecture to learn partbased and hierarchical attributes simultaneously. However, all these models only consist of a decoder component.</p><p>Our proposed deep autoencoder-like NMF model DANMF integrates the encoder component and the decoder component. Thus, DANMF is fundamentally different from the existing deep matrix factorization models. What's more, DANMF is able to better inherit the representation learning capability of deep autoencoder. It is worth mentioning that some deep learning approaches like GraphEncoder <ref type="bibr" target="#b34">[35]</ref> have already been employed for community detection. However, these approaches do not lend themselves to overlapping community detection, and there are usually a lot of parameters to be tuned. A nonnegative symmetric encoder-decoder approach <ref type="bibr" target="#b32">[33]</ref> has also been developed for community detection, but it is a shallow model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DEEP AUTOENCODER-LIKE NMF FOR COMMUNITY DETECTION</head><p>In this section, we describe our proposed deep autoencoderlike NMF mode (i.e., DANMF) for community detection. We start by introducing the notations and some preliminaries. Then, we present the details of DANMF. The architecture of DANMF is shown in Figure <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notations and Preliminaries</head><p>Throughout this paper, we denote matrices by bold uppercase letters. For a given matrix X, its (𝑖, 𝑗)-th entry is denoted by [X]𝑖𝑗. The trace and Frobenius norm of X are denoted by 𝑡𝑟(X) and ‖X‖𝐹 , respectively. The zero matrix is denoted by 0, and the identity matrix is denoted by I.</p><p>Let 𝒢 = (𝒱, ℰ) be a given network with 𝑛 = |𝒱| nodes and 𝑚 = |ℰ| edges, where 𝒱 and ℰ denote the node set and the edge set respectively. Typically, network 𝒢 is described by an adjacency matrix A, whose each entry [A]𝑖𝑗 characterizes the relationship between nodes 𝑖 and 𝑗. For unweighted networks, we have [A]𝑖𝑗 = 1 if there is an edge between nodes 𝑖 and 𝑗, and [A]𝑖𝑗 = 0 otherwise. If network 𝒢 is weighted, then A is real-valued. When A violates the nonnegative constraints, we can normalize each entry of A to the range of [0, 1].</p><p>Assume that network 𝒢 consists of 𝑘 communities. Let 𝒞 denote the set of communities, i.e., 𝒞 = {𝐶𝑖|𝐶𝑖 ̸ = ∅, 𝐶𝑖 ̸ = 𝐶𝑗, 1 ≤ 𝑖, 𝑗 ≤ 𝑘}, where 𝐶𝑖 represents the 𝑖-th community. For disjoint community detection, it is required that 𝐶𝑖 ∩ 𝐶𝑗 = ∅ if 𝑖 ̸ = 𝑗. For overlapping community detection, this constraint is neglected. Suppose that we have two nonnegative matrices U ∈ R 𝑛×𝑘 nodes 𝑖 and 𝑗 is the result of their mutual participation in the same communities <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28]</ref>. Obviously, [ Â]𝑖𝑗 should be as closely consistent as possible with [A]𝑖𝑗, which results in the following objective function:</p><formula xml:id="formula_0">min U,V ‖A -UV‖ 2 𝐹 , s.t. U ≥ 0, V ≥ 0.<label>(1)</label></formula><p>Based on the learnt V, we can extract the community membership of nodes. For disjoint community detection, each node is assigned to the community where it gets the largest belonging propensity. For overlapping community detection, we need to set a threshold in order to determine whether a node belongs to a community or not. Such a threshold can be obtained by taking the same strategy as suggested in <ref type="bibr" target="#b46">[47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deep NMF</head><p>As shown in Eq. ( <ref type="formula" target="#formula_0">1</ref>), NMF learns a one-layer mapping U and a community-level similarity between nodes (i.e., the community membership matrix V) directly. However, realworld networks often consist of complicated and diversified organizational patterns. Therefore, it is highly possible that the mapping between the original network and the community membership space contains rather complex hierarchical and structural information with implicit lower-level hidden attributes. It is well known that deep learning is able to narrow the gap between the lower-level abstraction and the higher-level abstraction of the original data <ref type="bibr" target="#b1">[2]</ref>. In this sense, we propose to further factorize the mapping U, in the hope that each factor adds an extra layer of abstraction of the similarity between nodes from low level to high level. Specifically, the adjacency matrix A is factorized into 𝑝 + 1 nonnegative factor matrices, as follows:</p><formula xml:id="formula_1">A ≈ U1U2 • • • U𝑝V𝑝,<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">V𝑝 ∈ R 𝑘×𝑛 + , U𝑖 ∈ R 𝑟 𝑖-1 ×𝑟 𝑖 +</formula><p>(1 ≤ 𝑖 ≤ 𝑝), and we set</p><formula xml:id="formula_3">𝑛 = 𝑟0 ≥ 𝑟1 ≥ • • • ≥ 𝑟𝑝-1 ≥ 𝑟𝑝 = 𝑘.</formula><p>The formulation in Eq. ( <ref type="formula" target="#formula_1">2</ref>) allows for a hierarchy of 𝑝 layers of abstract understanding of the original network, which can be given by the following factorizations:</p><formula xml:id="formula_4">V𝑝-1 ≈ U𝑝V𝑝, . . . V2 ≈ U3 • • • U𝑝V𝑝, V1 ≈ U2 • • • U𝑝V𝑝. (3)</formula><p>We retain the nonnegative constraints on V𝑖 (1 ≤ 𝑖 &lt; 𝑝) as well. By doing so, each layer of abstraction V𝑖 captures the similarity between nodes at different levels of granularity, ranging from the first-order proximity, to the structural identity, and finally the community-level similarity. This deep structure will lead to more accurate community detection results, i.e., a better V𝑝. In order to learn the factor matrices, we derive the following objective function:</p><formula xml:id="formula_5">min U 𝑖 ,V𝑝 ℒD = ‖A -U1U2 • • • U𝑝V𝑝‖ 2 𝐹 , s.t. V𝑝 ≥ 0, U𝑖 ≥ 0, ∀𝑖 = 1, 2, • • • , 𝑝.<label>(4)</label></formula><p>After optimizing Eq. ( <ref type="formula" target="#formula_5">4</ref>), we can obtain the hidden attributes V𝑖 (𝑖 &lt; 𝑝) by solving ‖A -U1U2 • • • U𝑖V𝑖‖ 2 𝐹 , similar to <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Deep Autoencoder-like NMF</head><p>As can be seen, both Eq. ( <ref type="formula" target="#formula_0">1</ref>) and Eq. ( <ref type="formula" target="#formula_5">4</ref>) are based on reconstructing the original network, which corresponds to the decoder component of an autoencoder. To better inherit the representation learning capability of autoencoders, it is essential to incorporate the encoder component into the NMFbased community detection models, resulting in autoencoderlike NMF models. The rationality of an autoencoder-like NMF model is quite straightforward. For an ideal community membership matrix V, on the one hand, it should be able to reconstruct the original network via the mapping U with smaller reconstruction error, and on the other hand, it should be obtained by directly projecting the original network A into the community membership space with the aid of the mapping U, i.e., V = U 𝑇 A. By integrating the encoder component and the decoder component into a unified loss function, the two components are capable of guiding each other during the learning process, and thus we tend to obtain the ideal community membership of nodes. To achieve this goal in the deep model, we derive the following objective function for the encoder component: min</p><formula xml:id="formula_6">U 𝑖 ,V𝑝 ℒE = ‖V𝑝 -U 𝑇 𝑝 • • • U 𝑇 2 U 𝑇 1 A‖ 2 𝐹 , s.t. V𝑝 ≥ 0, U𝑖 ≥ 0, ∀𝑖 = 1, 2, • • • , 𝑝.<label>(5)</label></formula><p>By combining Eq. ( <ref type="formula" target="#formula_5">4</ref>) and Eq. ( <ref type="formula" target="#formula_6">5</ref>), the unified objective function of our deep autoencoder-like NMF model (i.e., DAN-MF) is then given as follows: min</p><formula xml:id="formula_7">U 𝑖 ,V𝑝 ℒ = ℒD + ℒE + 𝜆ℒreg = ‖A -U1U2 • • • U𝑝V𝑝‖ 2 𝐹 + ‖V𝑝 -U 𝑇 𝑝 • • • U 𝑇 2 U 𝑇 1 A‖ 2 𝐹 + 𝜆𝑡𝑟(V𝑝LV 𝑇 𝑝 ), s.t. V𝑝 ≥ 0, U𝑖 ≥ 0, ∀𝑖 = 1, 2, • • • , 𝑝.<label>(6)</label></formula><p>Session 9B: IR Applications CIKM'18, October 22-26, 2018, Torino, Italy</p><p>In Eq. ( <ref type="formula" target="#formula_7">6</ref>), a graph regularizer ℒreg = 𝑡𝑟(V𝑝LV 𝑇 𝑝 ) is further introduced to respect the intrinsic geometric structure of node pairs. 𝜆 denotes the regularization parameter, and L represents the graph Laplacian matrix. There are many ways to define L <ref type="bibr" target="#b25">[26]</ref>. In this paper, we focus on undirected networks, and we set L = D -A (D is a diagonal matrix whose elements are row sums of A), based on a basic assumption that linked nodes are more likely to be contained in the same communities <ref type="bibr" target="#b47">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OPTIMIZATION</head><p>To expedite the approximation of the factor matrices in the proposed model, we pre-train each of the layers to have an initial approximation of the factor matrices U𝑖 and V𝑖. This pre-training process can greatly reduce the training time of our model. The effectiveness of pre-training has been proven before on deep autoencoder networks <ref type="bibr" target="#b14">[15]</ref>. To perform the pretraining, we first decompose the adjacency matrix A ≈ U1V1 by minimizing ‖A -</p><formula xml:id="formula_8">U1V1‖ 2 𝐹 + ‖V1 -U 𝑇 1 A‖ 2 𝐹 , where U1 ∈ R 𝑛×𝑟 1 + and V1 ∈ R 𝑟 1 ×𝑛</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>+</head><p>. Then, we decompose the matrix</p><formula xml:id="formula_9">V1 as V1 ≈ U2V2 by minimizing ‖V1 -U2V2‖ 2 𝐹 + ‖V2 - U 𝑇 2 V1‖ 2 𝐹 , where U2 ∈ R 𝑟 1 ×𝑟 2 + and V2 ∈ R 𝑟 2 ×𝑛</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>+</head><p>. Continue to do so until all of the layers have been pre-trained. Afterwards, each layer is fine-tuned by alternating minimization of the proposed objective function in Eq. ( <ref type="formula" target="#formula_7">6</ref>). In the following, we present the updating rules. By fixing all the variables except for U𝑖, the objective function in Eq. ( <ref type="formula" target="#formula_7">6</ref>) is reduced to: min</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Updating Rules</head><formula xml:id="formula_10">U 𝑖 ℒ(U𝑖) = ‖A -Ψ𝑖-1U𝑖Φ𝑖+1V𝑝‖ 2 𝐹 + ‖V𝑝 -Φ 𝑇 𝑖+1 U 𝑇 𝑖 Ψ 𝑇 𝑖-1 A‖ 2 𝐹 , s.t. U𝑖 ≥ 0,<label>(7)</label></formula><p>where</p><formula xml:id="formula_11">Ψ𝑖-1 = U1U2 • • • U𝑖-1 and Φ𝑖+1 = U𝑖+1 • • • U𝑝-1U𝑝.</formula><p>When 𝑖 = 1, we set Ψ0 = I. Similarly, when 𝑖 = 𝑝, we set Φ𝑝+1 = I.</p><p>To solve Eq. ( <ref type="formula" target="#formula_10">7</ref>), we introduce a Lagrangian multiplier matrix Θ𝑖 to enforce the nonnegative constraints on U𝑖, resulting in the following equivalent objective function:</p><formula xml:id="formula_12">min U 𝑖 ,Θ 𝑖 ℒ(U𝑖, Θ𝑖) = ‖A -Ψ𝑖-1U𝑖Φ𝑖+1V𝑝‖ 2 𝐹 + ‖V𝑝 -Φ 𝑇 𝑖+1 U 𝑇 𝑖 Ψ 𝑇 𝑖-1 A‖ 2 𝐹 -𝑡𝑟(Θ𝑖U 𝑇 𝑖 ),<label>(8)</label></formula><p>which can be further rewritten as follows:</p><p>min</p><formula xml:id="formula_13">U 𝑖 ,Θ 𝑖 ℒ(U𝑖, Θ𝑖) = 𝑡𝑟(A 𝑇 A + V 𝑇 𝑝 V𝑝 -4A 𝑇 Ψ𝑖-1U𝑖Φ𝑖+1V𝑝 + V 𝑇 𝑝 Φ 𝑇 𝑖+1 U 𝑇 𝑖 Ψ 𝑇 𝑖-1 Ψ𝑖-1U𝑖Φ𝑖+1V𝑝 + A 𝑇 Ψ𝑖-1U𝑖Φ𝑖+1Φ 𝑇 𝑖+1 U 𝑇 𝑖 Ψ 𝑇 𝑖-1 A -Θ𝑖U 𝑇 𝑖 ).<label>(9)</label></formula><p>By setting the partial derivative of ℒ(U𝑖, Θ𝑖) with respect to U𝑖 to 0, we have:</p><formula xml:id="formula_14">Θ𝑖 = -4Ψ 𝑇 𝑖-1 AV 𝑇 𝑝 Φ 𝑇 𝑖+1 + 2Π𝑖,<label>(10)</label></formula><p>where</p><formula xml:id="formula_15">Π𝑖 = Ψ 𝑇 𝑖-1 Ψ𝑖-1U𝑖Φ𝑖+1V𝑝V 𝑇 𝑝 Φ 𝑇 𝑖+1 + Ψ 𝑇 𝑖-1 AA 𝑇 Ψ𝑖-1U𝑖Φ𝑖+1Φ 𝑇 𝑖+1 .<label>(11)</label></formula><p>From the complementary slackness condition of the Karush-Kuhn-Tucker (KKT) conditions <ref type="bibr" target="#b3">[4]</ref>, we obtain:</p><formula xml:id="formula_16">Θ𝑖 ⊙ U𝑖 = (-4Ψ 𝑇 𝑖-1 AV 𝑇 𝑝 Φ 𝑇 𝑖+1 + 2Π𝑖) ⊙ U𝑖 = 0,<label>(12</label></formula><p>) where ⊙ denotes the element-wise product. Equation ( <ref type="formula" target="#formula_16">12</ref>) is the fixed point equation that the solution must satisfy at convergence. By solving this equation, we derive the following updating rule for U𝑖:</p><formula xml:id="formula_17">U𝑖 ← U𝑖 ⊙ 2Ψ 𝑇 𝑖-1 AV 𝑇 𝑝 Φ 𝑇 𝑖+1 Π𝑖 .<label>(13)</label></formula><p>4.1.2 Updating rule for the community membership matrix V𝑝. By fixing all the variables except for V𝑝, the objective function in Eq. ( <ref type="formula" target="#formula_7">6</ref>) is reduced to:</p><formula xml:id="formula_18">min V𝑝 ℒ(V𝑝) = ‖A -Ψ𝑝V𝑝‖ 2 𝐹 + ‖V𝑝 -Ψ 𝑇 𝑝 A‖ 2 𝐹 + 𝜆𝑡𝑟(V𝑝LV 𝑇 𝑝 ), s.t. V𝑝 ≥ 0. (<label>14</label></formula><formula xml:id="formula_19">)</formula><p>Following similar derivation process of the updating rule for U𝑖, the updating rule for V𝑝 is formulated as follows:</p><formula xml:id="formula_20">V𝑝 ← V𝑝 ⊙ 2Ψ 𝑇 𝑝 A + 𝜆V𝑝A Ψ 𝑇 𝑝 Ψ𝑝V𝑝 + V𝑝 + 𝜆V𝑝D .<label>(15)</label></formula><p>4.1.3 Updating rule for the feature matrix V𝑖 (1 ≤ 𝑖 &lt; 𝑝). The updating of V𝑖 is optional, since it does not affect the value of the objective function in Eq. ( <ref type="formula" target="#formula_7">6</ref>). However, we would like to extract the hidden attributes in each intermediate layer.</p><p>To optimize V𝑖, we in fact seek to optimize the following objective function:</p><formula xml:id="formula_21">min V 𝑖 ℒ(V𝑖) = ‖A -Ψ𝑖V𝑖‖ 2 𝐹 + ‖V𝑖 -Ψ 𝑇 𝑖 A‖ 2 𝐹 , s.t. V𝑖 ≥ 0.<label>(16)</label></formula><p>Similar to V𝑝, V𝑖 can be updated by</p><formula xml:id="formula_22">V𝑖 ← V𝑖 ⊙ 2Ψ 𝑇 𝑖 A Ψ 𝑇 𝑖 Ψ𝑖V𝑖 + V𝑖 . (<label>17</label></formula><formula xml:id="formula_23">)</formula><p>Until now, we have all the updating rules done. The overall optimization process of DANMF is outlined in Algorithm 1, where the "ShallowNMF" procedure performs the pretraining as described earlier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Convergence Analysis</head><p>The convergence of the updating rules is guaranteed by the following two theorems. </p><formula xml:id="formula_24">U𝑖 = U𝑖 ⊙ 2Ψ 𝑇 𝑖-1 AV 𝑇 𝑝 Φ 𝑇 𝑖+1 Π𝑖 ,<label>(18)</label></formula><p>Session 9B: IR Applications CIKM <ref type="bibr">'</ref> Update U𝑖 according to Eq. ( <ref type="formula" target="#formula_17">13</ref>);</p><p>12:</p><p>Ψ𝑖 ← Ψ𝑖-1U𝑖;</p><p>13:</p><p>Update V𝑖 according to Eq. ( <ref type="formula" target="#formula_22">17</ref>) (𝑖 &lt; 𝑝, optional) or according to Eq. ( <ref type="formula" target="#formula_20">15</ref>) (𝑖 = 𝑝);</p><p>14:</p><p>end for 15: end while</p><formula xml:id="formula_25">16: return U𝑖, V𝑖, ∀𝑖 = 1, 2, • • • , 𝑝 ; which is equivalent to (-4Ψ 𝑇 𝑖-1 AV 𝑇 𝑝 Φ 𝑇 𝑖+1 + 2Π𝑖) ⊙ U𝑖 = 0.<label>(19)</label></formula><p>Clearly, Eq. ( <ref type="formula" target="#formula_25">19</ref>) is identical to Eq. ( <ref type="formula" target="#formula_16">12</ref>). In the same way, the correctness of the updating rule in Eq. ( <ref type="formula" target="#formula_20">15</ref>) for V𝑝 can be proved.</p><p>Theorem 4.2. The objective function ℒ in Eq. ( <ref type="formula" target="#formula_7">6</ref>) is nonincreasing under the updating rules in Eq. (13) and Eq. (15).</p><p>The theorem above can be proved by leveraging an auxiliary function, following a similar process as described in <ref type="bibr" target="#b16">[17]</ref>. To save space, we omit the proof here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Time Complexity</head><p>Algorithm 1 is composed of two stages, i.e., the pre-training stage and the fine-tuning stage. The computational complexity for the pre-training stage is of order 𝒪(𝑝𝑡𝑝(𝑛 2 𝑟 + 𝑛𝑟 2 )), where 𝑝 is the number of layers, 𝑡𝑝 is the number of iterations to achieve convergence, and 𝑟 is the maximal layer size out of all layers. The computational complexity for the fine-tuning stage is of order 𝒪(𝑝𝑡 𝑓 (𝑛 2 𝑟 + 𝑛𝑟 2 + 𝑟 3 )), where 𝑡 𝑓 is the number of iterations in the fine-tuning process. In general, 𝑟 &lt; 𝑛, thus the complexity is 𝒪(𝑝𝑡 𝑓 (𝑛 2 𝑟 + 𝑛𝑟 2 )). To sum up, the overall time complexity is 𝒪(𝑝(𝑡𝑝 + 𝑡 𝑓 )(𝑛 2 𝑟 + 𝑛𝑟 2 )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussion</head><p>Our DANMF model is closely related to orthogonal NMF (ONMF) <ref type="bibr" target="#b26">[27]</ref> and projective NMF (PNMF) <ref type="bibr" target="#b45">[46]</ref>. As DANMF aims to optimize the encoder component and the decoder component simultaneously, we have V𝑝 ≈ Ψ 𝑇 𝑝 A and A ≈ Ψ𝑝V𝑝. Then, we have V𝑝 ≈ Ψ 𝑇 𝑝 Ψ𝑝V𝑝, which requires that Ψ 𝑇 𝑝 Ψ𝑝 ≈ I. In this sense, DANMF is related to ONMF. On the other hand, we have A ≈ Ψ𝑝Ψ 𝑇 𝑝 A, which leads to the PNMF model. However, both ONMF and PNMF are shallow models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>Now we move forward to evaluate the performance of the proposed DANMF model for disjoint community detection and overlapping community detection. All experiments are conducted on a server with two 2.4GHz Intel Xeon CPUs and 128GB main memory running Ubuntu 14.04.5 (64-bit).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baseline Methods</head><p>Our basic hypothesis in this paper is that DANMF is able to learn a better community-level similarity between nodes (i.e., a more accurate community membership matrix) than shallow NMF-based community detection approaches, with the aid of a hierarchy of hidden attributes extracted in the intermediate layers of the autoencoder-like deep structure. To verify this hypothesis, we choose seven representative shallow NMF-based methods as baselines. We also compare DANMF with three state-of-the-art network embedding methods based on the considerations that these methods can learn higherorder similarity between nodes and that they are also closely related to matrix factorization <ref type="bibr" target="#b28">[29]</ref>.</p><p>The NMF-based shallow models include:</p><p>• NMF: NMF is the fundamental component of the proposed DANMF model. It has been adopted for community detection in <ref type="bibr" target="#b20">[21]</ref>. • ONMF: ONMF is a variant of NMF by enforcing orthogonal constraints on the mapping matrix U, i.e., U 𝑇 U = I <ref type="bibr" target="#b26">[27]</ref>. • PNMF: PNMF directly projects the original network to a subspace by minimizing ‖A -UU 𝑇 A‖ 2 𝐹 <ref type="bibr" target="#b45">[46]</ref>. • BNMF: BNMF is a bayesian NMF model. It has been adopted for community detection in <ref type="bibr" target="#b27">[28]</ref>. • BigClam: BigClam is a cluster affiliation model, which relaxes the graph fitting problem into a continuous optimization problem <ref type="bibr" target="#b42">[43]</ref>. • HNMF: HNMF is a probabilistic approach. It models the homogeneous relationships between edges and communities for community detection <ref type="bibr" target="#b47">[48]</ref>. • NSED: NSED is a nonnegative symmetric encoderdecoder approach proposed for community detection.</p><p>Though it takes into account the encoder component, it extracts the community membership from the mapping matrix U rather than the feature matrix V <ref type="bibr" target="#b32">[33]</ref>. The network embedding methods include:</p><p>• LINE: LINE preserves the first-order and second-order proximities between nodes for learning low-dimensional representations of nodes <ref type="bibr" target="#b33">[34]</ref>. • Node2Vec: Node2Vec aims to learn higher-order similarity between nodes via truncated random walks <ref type="bibr" target="#b11">[12]</ref>. The in-out hyperparameter is fixed at 2 to better capture the community structure of networks.</p><p>Session 9B: IR Applications CIKM'18, October 22-26, 2018, Torino, Italy • MNMF: MNMF is a modularized NMF model, which incorporates the community structure into network embedding <ref type="bibr" target="#b39">[40]</ref>. For the network embedding methods, we set the size of latent representations to be 64, and then apply the standard 𝑘means algorithm to identify communities. We also implement a pruned version of DANMF, named DNMF, which ignores the encoder component. For a fair comparison, we run each algorithm 20 times and the average results are reported. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Disjoint Community Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Community detection results.</head><p>To measure the community detection results, we employ three evaluation metrics including Adjusted Rand Index (ARI), Normalized Mutual Information (NMI), and Accuracy (ACC). For these metrics, larger value indicates better performance. A detailed description of them can be found in the survey paper <ref type="bibr" target="#b5">[6]</ref>. The regularization parameter of DANMF is tuned in the range of {10 -3 , 10 -2 , 10 -1 , 10 0 , 10 1 }. And the layer size configuration of DANMF is provided in Table <ref type="table" target="#tab_1">1</ref>. We implement DANMF with three hidden layers. Although we have experimented with more hidden layers, the performance promotion is not significant while much more time is taken to train the model. 1 See https://snap.stanford.edu/ and https://linqs.soe.ucsc.edu. Since we assume that DANMF is able to better inherit the learning capability of deep autoencoder, our first experiment is to evaluate whether it achieves lower coding and reconstruction error. The coding error corresponding to the encoder component is calculated as 1  𝑛 ‖V -U 𝑇 A‖𝐹 , where 𝑛 denotes the number of nodes. The reconstruction error corresponding to the decoder component is calculated as 1  𝑛 ‖A -UV‖𝐹 . Note that in PNMF, we set V = U 𝑇 A directly. In DNMF and DANMF, we have V = V𝑝 and U = U1U2 • • • U𝑝. The average results of 20 runs on Wiki is reported in Table <ref type="table" target="#tab_2">2</ref>. We only report the results of the methods that involve the mapping U explicitly. The results show that DANMF achieves much lower coding error than NMF and DNMF, which verifies the necessity of the encoder component. Although DANMF is much harder to train due to the multiple factor matrices, it achieves comparable reconstruction error with NMF, which demonstrates the effectiveness of our optimization algorithm. Although the coding error of PNMF is 0, its reconstruction error is extremely large.</p><p>Next we introduce the community detection results. Tabels 3-5 show the comparison in ARI, NMI, and ACC respectively. The best results are presented in blue color. As can be seen, our DANMF model outperforms all the baselines across different evaluation metrics except for ARI on Wiki. For example, on the largest network Pubmed, DANMF achieves One may note that the network embedding methods do not show satisfactory performance, even though they seek to preserve higher-order similarity between nodes. The reason for LINE and Node2Vec is that they are primarily focused on modeling the microscopic structure instead of the mesoscopic community structure of networks.</p><p>The reason for MNMF may be that it adopts modularity to reveal the community structure. However, modularity may suffer from the resolution limit problem <ref type="bibr" target="#b5">[6]</ref>.</p><p>Although DANMF has a deep structure, it can be trained efficiently. The runtime of DANMF on all the benchmark networks is depicted in Figure <ref type="figure" target="#fig_6">3</ref>. Note that Figure <ref type="figure" target="#fig_6">3</ref> is plotted in log scale. It is observed that DANMF is quite efficient on small networks. On the largest network Pubmed, DANMF can also finish its training process in about 4000 seconds.      the pre-training stage, each layer in fact performs a shallow NMF model, whose convergence has already been analyzed <ref type="bibr" target="#b32">[33]</ref>. Thus we focus on the fine-tuning stage and analyze its convergence rate, which denotes the change rate of the objective function value. To test the convergence speed, we fix the regularization parameter at 1. The results on Cora and Citeseer are shown in Figure <ref type="figure" target="#fig_9">4</ref>. Similar results can be observed on other networks. From Figure <ref type="figure" target="#fig_9">4</ref>, we can see that DANMF can achieve fast convergence within about 10 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Parameter sensitivity.</head><p>In DANMF, the parameter 𝜆 is used to adjust the contribution of the graph regularizer. It is tuned in the range of {10 -3 , 10 -2 , 10 -1 , 10 0 , 10 1 }. The effect of 𝜆 on Cora and Citeseer is shown in Figure <ref type="figure" target="#fig_10">5</ref>. To some extent, DANMF is robust to the parameter 𝜆. On both Cora and Citeseer, DANMF tends to obtain the best performance when 𝜆 = 1. The results indicate that although the performance of DANMF is stable with respect to 𝜆, a proper 𝜆 can make DANMF be more robust.</p><p>There is no doubt that the performance of DANMF will be affected by the layer size configuration of each layer, an in-depth exploration of which is left as our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5">Visualization.</head><p>As DANMF is expected to be able to learn the similarity between nodes from different levels of granularity, we feed the learnt hidden attributes (i.e., the feature matrix V𝑖 and the community membership matrix V𝑝) into the standard t-SNE tool <ref type="bibr" target="#b19">[20]</ref> to visualize them. For comparison, we also visualize the original network. The result on Cora is shown in Figure <ref type="figure">6</ref>, where nodes belonging to the same community share the same color. It is observed that the original network represented by the adjacency matrix does not embody clear community structure. While the hidden attributes learnt in the intermediate layers of DANMF capture the similarity between nodes more accurately. Besides, nodes belonging to the same community gather more and more closer to each other as the layers go deeper.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Overlapping Community Detection</head><p>5.3.1 Datasets. Since few networks with ground-truth overlapping communities are publically available, we employ the well-known LFR toolkit <ref type="bibr" target="#b15">[16]</ref> to generate synthetic networks with overlapping community structure. The parameters of the LFR benchmarks are set as follows. The number of nodes is 5000, the average degree is 20, and the maximum degree is 50. The community size ranges from 100 to 250. The exponents of the power-law distributions of node degree and community size are kept at 2 and 1, respectively. The number of communities that an overlapping node belongs to is fixed at 2. The mixing parameter 𝜇 (each node shares a fraction 𝜇 of its edges with nodes in other communities) is set to either 0.1 or 0.3, and the fraction of overlapping nodes varies from 0.1 to 0.6 with an increment of 0.1. Thus, there are 12 synthetic networks in total. For each network, the layer size configuration of DANMF is set to 5000-512-128-𝑘, where 𝑘 denotes the number of ground-truth communities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Community detection results</head><p>. Following <ref type="bibr" target="#b15">[16]</ref>, we use the Overlapping NMI (ONMI) metric to evaluate the overlapping community detection results. The detailed description of ONMI can also be found in the survey paper <ref type="bibr" target="#b5">[6]</ref>. The results with respect to 𝜇 = 0.1 and 𝜇 = 0.3 are shown in Figure <ref type="figure">7</ref> and Figure <ref type="figure" target="#fig_13">8</ref>, respectively. Since the network embedding methods are not suitable for overlapping community detection, their results are neglected. As shown in Figure <ref type="figure">7</ref> and Figure <ref type="figure" target="#fig_13">8</ref>, DANMF outperforms the other methods on all the LFR benchmark networks. When the mixing parameter 𝜇 = 0.1, the performance of DANMF is relatively stable with the change of the fraction of overlapping nodes. For example, even on the network with 60% overlapping nodes, the ONMI of DANMF reaches 0.9. While the performance of all the methods on the LFR benchmark networks with 𝜇 = 0.3 drops off precipitously as the fraction of overlapping nodes increases. This is because when 𝜇 = 0.3, the community structure becomes less significant, which makes the community detection task more difficult and more challenging. However, DANMF still shows superior performance over the other methods consistently. The results demonstrate that DANMF is capable of detecting overlapping communities with better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we have introduced a novel deep autoencoderlike model DANMF to combat the problem of community detection. Different from traditional NMF-based community detection methods, DANMF integrates the encoder component and the decoder component into a unified loss function.</p><p>Both components are with deep structures. This architecture empowers DANMF to better inherit the learning capability of deep autoencoder. Although DANMF is much harder to train due to the multiple factor matrices, the proposed optimization algorithm can solve it efficiently. We have also conducted extensive experiments for both disjoint community detection and overlapping community detection. The results demonstrate the superiority of DANMF over shallow NMF-based methods. For future work, we plan to use other cost functions to quantify the quality of the approximation, e.g., the Kullback-Leibler divergence.</p><p>and the Pearl River S&amp;T Nova Program of Guangzhou (201710010046). Chuan Chen is the corresponding author.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) The architecture of NMF. (b) The architecture of deep NMF. Deep NMF learns a hierarchy of hidden attributes that aid in uncovering the final community membership of nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>+Figure 2 :</head><label>2</label><figDesc>Figure 2: The architecture of DANMF. For illustration purpose, the depth is fixed at 2. The encoder component (the left part) transforms the network into the community membership space. The decoder component (the right part) reconstructs the network from the community membership space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4. 1 . 1</head><label>11</label><figDesc>Updating rule for the mapping matrix U𝑖 (1 ≤ 𝑖 ≤ 𝑝).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Theorem 4 . 1 .</head><label>41</label><figDesc>The limited solutions of the updating rules in Eq. (13) and Eq. (15) satisfy the KKT optimality condition.Proof. At convergence, we have U (∞) 𝑖 = U (𝑡+1) 𝑖 = U (𝑡) 𝑖 = U𝑖, where 𝑡 denotes the 𝑡-th iteration. That is,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>5. 2 . 1</head><label>21</label><figDesc>Datasets. We adopt five real-world networks1 for disjoint community detection. • Email: A communication network involving 1005 researchers from 42 departments and 25571 relationships. • Wiki: A document network consisting of 2405 web pages from 19 categories and 17981 edges. • Cora: A citation network with 2708 nodes and 5429 edges. Each node is classified into one of 7 classes. • Citeseer: A citation network with 3312 nodes and 4732 edges. Each node is classified into one of 6 classes. • Pubmed: A citation network with 19717 nodes and 44338 edges. Each node is divided into one of 3 classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Time overheads of DANMF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>5. 2 . 3</head><label>23</label><figDesc>Convergence analysis. The updating rules of our optimization algorithm are essentially iterative. Different from the exact runtime, here we further investigate how fast these rules can converge. Recall that our optimization algorithm consists of the pre-training stage and the fine-tuning stage. In</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Convergence rate analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The effect of parameter 𝜆.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Session 9B: IR Applications CIKM'18, October 22-26, 2018, Torino, Italy(a) Cora -before training (b) Cora -layer 1 (c) Cora -layer 2 (d) Cora -layer 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: 2D visualization of the representations learnt in different layers of DANMF on Cora.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: ONMI on LFR benchmarks with 𝜇 = 0.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>18, October 22-26, 2018, Torino, Italy</figDesc><table><row><cell cols="3">Algorithm 1 Optimization algorithm of DANMF</cell></row><row><cell cols="3">Input: The adjacency matrix of network 𝒢, A;</cell></row><row><cell></cell><cell cols="2">The layer size of each layer, 𝑟𝑖;</cell></row><row><cell></cell><cell cols="2">The regularization parameter, 𝜆;</cell></row><row><cell cols="3">Output: The mapping matrix U𝑖 (1 ≤ 𝑖 ≤ 𝑝), the feature</cell></row><row><cell></cell><cell cols="2">matrix V𝑖 (1 ≤ 𝑖 &lt; 𝑝), and the community membership</cell></row><row><cell></cell><cell>matrix V𝑝;</cell><cell></cell></row><row><cell cols="3">1: ◁ Pre-training process:</cell></row><row><cell cols="3">2: U1, V1 ← ShallowNMF(A, 𝑟1);</cell></row><row><cell cols="3">3: for 𝑖 = 2 to 𝑝 do</cell></row><row><cell>4:</cell><cell cols="2">U𝑖, V𝑖 ← ShallowNMF(V𝑖-1, 𝑟𝑖);</cell></row><row><cell cols="2">5: end for</cell><cell></cell></row><row><cell cols="3">6: ◁ Fine-tuning process:</cell></row><row><cell cols="3">7: while not converged do</cell></row><row><cell>8:</cell><cell cols="2">for 𝑖 = 1 to 𝑝 do</cell></row><row><cell>9: 10:</cell><cell>Ψ𝑖-1 ← Φ𝑖+1 ←</cell><cell>∏︀ 𝑖-1 𝜏 =1 U𝜏 (Ψ0 ← I); ∏︀ 𝑝 𝜏 =𝑖+1 U𝜏 (Φ𝑝+1 ← I);</cell></row><row><cell>11:</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Layers configuration of DANMF</figDesc><table><row><cell>Dataset</cell><cell>𝑛</cell><cell>Layers Configuration</cell></row><row><cell>Email</cell><cell>1005</cell><cell>1005-256-128-42</cell></row><row><cell>Wiki</cell><cell>2405</cell><cell>2405-256-128-19</cell></row><row><cell>Cora</cell><cell>2708</cell><cell>2708-256-64-7</cell></row><row><cell>Citeseer</cell><cell>3312</cell><cell>3312-256-64-6</cell></row><row><cell>Pubmed</cell><cell>19717</cell><cell>19717-512-64-3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Error comparison on Wiki</figDesc><table><row><cell>Method</cell><cell cols="3">Encoder Decoder Encoder+Decoder</cell></row><row><cell>NMF</cell><cell>0.2326</cell><cell>0.0543</cell><cell>0.2869</cell></row><row><cell>ONMF</cell><cell>0.0043</cell><cell>0.0547</cell><cell>0.0590</cell></row><row><cell>PNMF</cell><cell>0.0000</cell><cell>445.58</cell><cell>445.58</cell></row><row><cell>NSED</cell><cell>0.0025</cell><cell>0.0547</cell><cell>0.0572</cell></row><row><cell>DNMF</cell><cell>0.2131</cell><cell>0.0546</cell><cell>0.2677</cell></row><row><cell>DANMF</cell><cell>0.0020</cell><cell>0.0541</cell><cell>0.0561</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance evaluation based on ARI</figDesc><table><row><cell>Method</cell><cell>Email</cell><cell>Wiki</cell><cell>Cora</cell><cell>Citeseer Pubmed</cell></row><row><cell>NMF</cell><cell cols="4">0.4989 0.1195 0.2145 0.0590 0.0978</cell></row><row><cell>ONMF</cell><cell cols="4">0.4832 0.1233 0.1964 0.0825 0.1589</cell></row><row><cell>PNMF</cell><cell cols="4">0.4641 0.1151 0.1863 0.0801 0.0967</cell></row><row><cell>BNMF</cell><cell cols="4">0.3545 0.1705 0.1812 0.0838 0.0872</cell></row><row><cell>BigClam</cell><cell cols="4">0.2478 0.0217 0.0306 0.0283 0.0258</cell></row><row><cell>HNMF</cell><cell cols="4">0.2079 0.1448 0.1113 0.0262 0.0360</cell></row><row><cell>NSED</cell><cell cols="4">0.5215 0.1253 0.1782 0.0866 0.1258</cell></row><row><cell>LINE</cell><cell cols="4">0.3325 0.1344 0.1271 0.0278 0.1017</cell></row><row><cell cols="5">Node2Vec 0.4195 0.1621 0.1063 0.0182 0.0170</cell></row><row><cell>MNMF</cell><cell cols="4">0.0041 0.0016 0.0002 0.0007 0.0001</cell></row><row><cell>DNMF</cell><cell cols="4">0.5256 0.1341 0.2452 0.0990 0.1185</cell></row><row><cell>DANMF</cell><cell cols="4">0.5521 0.1628 0.3194 0.1343 0.2563</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance evaluation based on NMI</figDesc><table><row><cell>Method</cell><cell>Email</cell><cell>Wiki</cell><cell>Cora</cell><cell>Citeseer Pubmed</cell></row><row><cell>NMF</cell><cell cols="4">0.6751 0.2673 0.2851 0.1319 0.1606</cell></row><row><cell>ONMF</cell><cell cols="4">0.6734 0.2607 0.2416 0.1423 0.1582</cell></row><row><cell>PNMF</cell><cell cols="4">0.6770 0.2684 0.2893 0.1355 0.1511</cell></row><row><cell>BNMF</cell><cell cols="4">0.5960 0.2903 0.2521 0.0835 0.0714</cell></row><row><cell>BigClam</cell><cell cols="4">0.5796 0.2722 0.1864 0.0735 0.0291</cell></row><row><cell>HNMF</cell><cell cols="4">0.5146 0.2959 0.1425 0.0312 0.0311</cell></row><row><cell>NSED</cell><cell cols="4">0.6845 0.2659 0.2928 0.1492 0.1729</cell></row><row><cell>LINE</cell><cell cols="4">0.6393 0.2772 0.2376 0.0573 0.1357</cell></row><row><cell cols="5">Node2Vec 0.6784 0.3331 0.1978 0.0486 0.0635</cell></row><row><cell>MNMF</cell><cell cols="4">0.2138 0.0274 0.0035 0.0031 0.0002</cell></row><row><cell>DNMF</cell><cell cols="4">0.6850 0.2798 0.3572 0.1582 0.1709</cell></row><row><cell>DANMF</cell><cell cols="4">0.6943 0.3406 0.4114 0.1831 0.2221</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Performance evaluation based on ACC</figDesc><table><row><cell>Method</cell><cell>Email</cell><cell>Wiki</cell><cell>Cora</cell><cell>Citeseer Pubmed</cell></row><row><cell>NMF</cell><cell cols="4">0.5851 0.3027 0.4103 0.3074 0.5133</cell></row><row><cell>ONMF</cell><cell cols="4">0.5761 0.3069 0.3811 0.3330 0.5575</cell></row><row><cell>PNMF</cell><cell cols="4">0.5791 0.3052 0.4029 0.3451 0.5073</cell></row><row><cell>BNMF</cell><cell cols="4">0.4299 0.3751 0.4191 0.3324 0.5110</cell></row><row><cell>BigClam</cell><cell cols="4">0.4768 0.2545 0.3781 0.3046 0.3978</cell></row><row><cell>HNMF</cell><cell cols="4">0.3463 0.3518 0.3903 0.2569 0.4128</cell></row><row><cell>NSED</cell><cell cols="4">0.6179 0.2981 0.4234 0.3448 0.5201</cell></row><row><cell>LINE</cell><cell cols="4">0.4657 0.3289 0.4044 0.3019 0.4990</cell></row><row><cell cols="5">Node2Vec 0.5244 0.3568 0.3674 0.2521 0.4067</cell></row><row><cell>MNMF</cell><cell cols="4">0.1075 0.0886 0.1647 0.1890 0.3397</cell></row><row><cell>DNMF</cell><cell cols="4">0.6199 0.3543 0.4849 0.3635 0.5389</cell></row><row><cell>DANMF</cell><cell cols="4">0.6358 0.4112 0.5499 0.4242 0.6393</cell></row><row><cell cols="5">a relative performance promotion of 9.74%, 4.92% and 8.18%</cell></row><row><cell cols="5">with respect to ARI, NMI and ACC respectively. It is noted</cell></row><row><cell cols="5">that DNMF also outperforms NMF consistently, which shows</cell></row><row><cell cols="5">that with the deep structure, we are indeed able to learn a</cell></row><row><cell cols="5">hierarchy of abstract understanding of the original networks</cell></row><row><cell cols="5">that can aid in uncovering the community membership of n-</cell></row><row><cell cols="5">odes. The superiority of DANMF over DNMF further verifies</cell></row><row><cell cols="5">that by integrating the encoder component and the decoder</cell></row><row><cell cols="5">component, DANMF is able to better inherit the learning ca-</cell></row><row><cell cols="3">pability of deep autoencoder.</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by the National Key Research and Development Plan (2018YFB1003800), the National Natural Science Foundation of China (11801595), the Guangdong Province Universities and Colleges Pearl River Scholar Funded Scheme 2016, the Program for Guangdong Introducing Innovative and Entrepreneurial Teams (2016ZT06D211)</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modularity and community detection in bipartite networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Barber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page">66102</biblScope>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast unfolding of communities in large networks</title>
		<author>
			<persName><forename type="first">Jean-Loup</forename><surname>Vincent D Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renaud</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Lambiotte</surname></persName>
		</author>
		<author>
			<persName><surname>Lefebvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Stat. Mech</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">10008</biblScope>
			<date type="published" when="2008">2008. 2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lieven</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex optimization</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning community embedding with community detection and node embedding on graphs</title>
		<author>
			<persName><forename type="first">Sandro</forename><surname>Cavallari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyun</forename><surname>Vincent W Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="377" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Metrics for community analysis: A survey</title>
		<author>
			<persName><forename type="first">Tanmoy</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayushi</forename><surname>Dalmia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niloy</forename><surname>Ganguly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing curveys (csur)</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page">54</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the permanence of vertices in network communities</title>
		<author>
			<persName><forename type="first">Tanmoy</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriram</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niloy</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjukta</forename><surname>Bhowmick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1396" to="1405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On community detection in real-world networks and the importance of degree assortativity</title>
		<author>
			<persName><forename type="first">Marek</forename><surname>Ciglan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Laclavík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kjetil</forename><surname>Nørvåg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1007" to="1015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Local search of communities in large graphs</title>
		<author>
			<persName><forename type="first">Wanyun</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghua</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD. ACM</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="991" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Community detection in graphs</title>
		<author>
			<persName><forename type="first">Santo</forename><surname>Fortunato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics reports</title>
		<imprint>
			<biblScope unit="volume">486</biblScope>
			<biblScope unit="page" from="75" to="174" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Community structure in social and biological networks</title>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Girvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">Ej</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PNAS</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="7821" to="7826" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Zhenxing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shihua</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09316</idno>
		<title level="m">Sparse deep nonnegative matrix factorization</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee-Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detecting the overlapping and hierarchical community structure in complex networks</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Lancichinetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santo</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">János</forename><surname>Kertész</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Journal of Physics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">33015</biblScope>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Algorithms for nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung</forename><surname>Sebastian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="556" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Empirical comparison of algorithms for network community detection</title>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="631" to="640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Enhancing the network embedding quality with structural similarity</title>
		<author>
			<persName><forename type="first">Tianshu</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="147" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008. 2008</date>
			<pubPlace>Nov</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Structural and functional discovery in dynamic networks with non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Mankad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Michailidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page">42812</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The structure and function of complex networks</title>
		<author>
			<persName><surname>Mark Ej Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="167" to="256" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast algorithm for detecting community structure in networks</title>
		<author>
			<persName><surname>Mark Ej Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">66133</biblScope>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Modularity and community structure in networks</title>
		<author>
			<persName><surname>Mark Ej Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PNAS</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="8577" to="8582" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Overlapping communities in dynamic networks: Their detection and mobile applications</title>
		<author>
			<persName><forename type="first">Thang</forename><forename type="middle">N</forename><surname>Nam P Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sindhura</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">My</forename><forename type="middle">T</forename><surname>Tokala</surname></persName>
		</author>
		<author>
			<persName><surname>Thai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MobiCom</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="85" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nonnegative matrix tri-factorization with graph regularization for community detection in social networks</title>
		<author>
			<persName><forename type="first">Yulong</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nilanjan</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katia</forename><surname>Sycara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2083" to="2089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Two algorithms for orthogonal nonnegative matrix factorization with application to clustering</title>
		<author>
			<persName><forename type="first">Filippo</forename><surname>Pompili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Gillis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P-A</forename><surname>Absil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Glineur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page" from="15" to="25" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Overlapping community detection using bayesian nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Psorakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Ebden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Sheldon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">66114</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Network embedding as matrix factorization: Unifying DeepWalk, LINE, PTE, and node2vec</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="459" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">struc2vec: Learning node representations from structural identity</title>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">Hp</forename><surname>Leonardo Fr Ribeiro</surname></persName>
		</author>
		<author>
			<persName><surname>Saverese</surname></persName>
		</author>
		<author>
			<persName><surname>Daniel R Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hierarchical feature extraction by multi-layer non-negative matrix factorization network for classification task</title>
		<author>
			<persName><forename type="first">Hyun Ah</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo-Kyeong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thanh</forename><surname>Luong Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soo-Young</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="page" from="63" to="74" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A non-negative symmetric encoder-decoder approach for community detection</title>
		<author>
			<persName><forename type="first">Bing-Jie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="597" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning deep representations for graph clustering</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Fei Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1293" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Community detection in networks with positive and negative links</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeroen</forename><surname>Traag</surname></persName>
		</author>
		<author>
			<persName><surname>Bruggeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page">36115</biblScope>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A deep semi-nmf model for learning hidden representations</title>
		<author>
			<persName><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bjoern</forename><surname>Schuller</surname></persName>
		</author>
		<idno>ICML. 1692-1700</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A correlation clustering framework for community detection</title>
		<author>
			<persName><forename type="first">Nate</forename><surname>Veldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">F</forename><surname>Gleich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Wirth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW. ACM</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="439" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Community discovery using nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghuo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DMKD</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="493" to="521" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Community preserving network embedding</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="203" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Nonnegative matrix factorization with mixed hypergraph regularization for community detection</title>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuheng</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">435</biblScope>
			<biblScope unit="page" from="263" to="281" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Overlapping community detection in networks: The state-of-theart and comparative study</title>
		<author>
			<persName><forename type="first">Jierui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boleslaw K</forename><surname>Szymanski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM CSUR</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page">43</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Overlapping community detection at scale: A nonnegative matrix factorization approach</title>
		<author>
			<persName><forename type="first">Jaewon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM. ACM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="587" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Community detection in networks with node attributes</title>
		<author>
			<persName><forename type="first">Jaewon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1151" to="1156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning the hierarchical parts of objects by deep non-smooth nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">Jinshi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoxu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrzej</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengli</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07226</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Projective nonnegative matrix factorization for image compression and feature extraction</title>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erkki</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scandinavian Conference on Image Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="333" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Incorporating implicit link preference into overlapping community detection</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><surname>Michael R Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="396" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Modeling the homophily effect between links and communities for overlapping community detection</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><surname>Michael R Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3938" to="3944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Overlapping community detection via bounded nonnegative matrix tri-factorization</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="606" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Overlapping community detection in complex networks using symmetric binary matrix factorization</title>
		<author>
			<persName><surname>Zhong-Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Yeol</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page">62803</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Finding weighted k-truss communities in large networks</title>
		<author>
			<persName><forename type="first">Zibin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanghua</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong-Hua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohui</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tan</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">417</biblScope>
			<biblScope unit="page" from="344" to="360" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
