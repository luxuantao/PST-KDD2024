<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ElegantRL-Podracer: Scalable and Elastic Library for Cloud-Native Deep Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-04-04">4 Apr 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiao-Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zechu</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhuoran</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiahao</forename><surname>Zheng</surname></persName>
							<email>jh.zheng@siat.ac.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Inst. of Advanced Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhaoran</forename><surname>Wang</surname></persName>
							<email>zhaoranwang@gmail.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Northwestern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anwar</forename><surname>Walid</surname></persName>
							<email>anwar.i.walid@gmail.com</email>
							<affiliation key="aff4">
								<orgName type="department">Amazon &amp;</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Guo</surname></persName>
							<email>guojian@idea.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
							<email>jordan@cs.berkeley.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ElegantRL-Podracer: Scalable and Elastic Library for Cloud-Native Deep Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-04">4 Apr 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2112.05923v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep reinforcement learning (DRL) has revolutionized learning and actuation in applications such as game playing and robotic control. The cost of data collection, i.e., generating transitions from agent-environment interactions, remains a major challenge for wider DRL adoption in complex real-world problems. Following a cloud-native paradigm to train DRL agents on a GPU cloud platform is a promising solution. In this paper, we present a scalable and elastic library ElegantRL-podracer for cloud-native deep reinforcement learning, which efficiently supports millions of GPU cores to carry out massively parallel training at multiple levels. At a high-level, ElegantRL-podracer employs a tournament-based ensemble scheme to orchestrate the training process on hundreds or even thousands of GPUs, scheduling the interactions between a leaderboard and a training pool with hundreds of pods. At a low-level, each pod simulates agent-environment interactions in parallel by fully utilizing nearly 7, 000 GPU CUDA cores in a single GPU. Our ElegantRLpodracer library features high scalability, elasticity and accessibility by following the development principles of containerization, microservices and MLOps. Using an NVIDIA DGX SuperPOD cloud, we conduct extensive experiments on various tasks in locomotion and stock trading and show that ElegantRL-podracer substantially outperforms RLlib. Our codes are available on GitHub Liu et al. [2021]. * Equal contribution. ? A. Walid finished this project at Bell labs, before joining Amazon. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep reinforcement learning (DRL), which balances the exploration (of uncharted territory) and exploitation (of current information), has revolutionized learning and actuation in applications such as game playing <ref type="bibr" target="#b24">Silver et al. [2017]</ref> and robotic control <ref type="bibr" target="#b30">Zhang and Mo [2021]</ref>. DRL employs a trial-and-error manner to generate transitions from agent-environment interactions, along with the learning procedure. However, the cost of data collection remains a major challenge for wider DRL adoption in real-world problems with complex and dynamic environments. Therefore, a compelling solution is massively parallel training on hundreds or even thousands of GPUs, say millions of GPU cores.</p><p>Existing DRL frameworks are not satisfactory with respect to scalability and accessibility. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, OpenAI Baselines <ref type="bibr" target="#b5">Dhariwal et al. [2017]</ref>, Stable Baselines 3 <ref type="bibr" target="#b20">Raffin et al. [2019]</ref> and OpenAI Spinning Up OpenAI <ref type="bibr">[2018]</ref> utilize a single GPU, while RLlib <ref type="bibr" target="#b12">Liang et al. [2018]</ref> and rlpyt <ref type="bibr" target="#b25">Stooke and Abbeel [2019]</ref> can support multiple GPUs. However, there is no existing DRL framework for a cloud with hundreds or even thousands of GPUs. We aim to fully utilize two core techonlogies: 1). GPUDirect technology system reference architecture <ref type="bibr">[2020]</ref> that provides a path for data to bypass CPUs and travel on "open highways" offered by GPUs, storage, and networking devices; and 2). massively parallel simulations using thousands of GPU cores on a single GPU. On the other hand, the above frameworks, except OpenAI Spinning Up serving an educational purpose, involve a steep learning curve or a lack of customization flexibility, which results in low accessibility.</p><p>Scaling out the training process of DRL agents to hundreds or even thousands of GPUs is challenging for researchers and practitioners. The cloud-native paradigm aims to scalably and elastically utilize the cloud computing resources. Therefore, we believe it is practically promising to schedule the training of DRL agents by following the cloud-native paradigm, such as employing standardized software stack, e.g., Kubernetes (K8s) <ref type="bibr" target="#b1">Bernstein [2014]</ref>, and adopting core technologies including containers, microservices, continuous integration (CI) and continuous delivery (CD) Balalaie et al. <ref type="bibr">[2016]</ref>, <ref type="bibr" target="#b7">Gannon et al. [2017]</ref>.</p><p>In this paper, we present a scalable and elastic library ElegantRL-podracer for cloud-native deep reinforcement learning, which efficiently utilizes millions of GPU cores to carry out massively parallel training at multiple levels. At a high-level, ElegantRL-podracer employs a tournamentbased ensemble scheme to orchestrate the training process on hundreds or even thousands of GPUs, scheduling the interactions between a leaderboard and a training pool with hundreds of pods. At a low-level, each pod simulates agent-environment interactions in parallel by fully utilizing over 7, 000 GPU cores in a single GPU. Our ElegantRL-podracer library features high scalability, elasticity and accessibility by following the development principles of containerization, microservices and MLOps.</p><p>Our main contributions are summarized as follows:</p><p>? We present a scalable and elastic open-source library for cloud-native deep reinforcement learning, ElegantRL-podracer, that can utilize millions of GPU cores to train effective DRL agents for complex real-world problems.</p><p>? To accelerate data collection for efficient exploration, we propose a tournament-based ensemble training scheme and employ massive parallel simulations.</p><p>? ElegantRL-podracer follows a cloud-native paradigm by realizing the development principles of containerization, microservices and MLOps (e.g., continuous integration and continuous delivery), and achieves high accessibility.</p><p>? Using an NVIDIA DGX SuperPOD cloud system reference architecture [2020], we conduct extensive experiments on various tasks in locomotion and stock trading and show that ElegantRLpodracer substantially outperforms RLlib <ref type="bibr" target="#b12">Liang et al. [2018]</ref>.</p><p>The remainder of this paper is organized as follows. Section 2 describes related works. Section 3 presents our design principles. Section 4 describes the ElegantRL-podracer library. In Section 5, we present experimental results. We conclude this paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>We review open-source DRL frameworks/libraries and environment simulation packages. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Design Principles and Overview</head><p>We aim to develop a user-friendly open-source library that fully exploits cloud resources to train DRL agents. The library emphasizes the following design principles:</p><p>? Scaling-out: scalability and elasticity.</p><p>? Efficiency: low communication overhead, massively parallel simulations and robustness of agents.</p><p>? Accessibility: lightweightness and customization.</p><p>For algorithm design, ElegantRL-podracer employs a tournament-based ensemble training scheme to balance exploration and exploitation. In contrast to Evolutionary Strategies (ES) <ref type="bibr" target="#b21">Salimans et al. [2017]</ref> where a population of agents evolve over generations, our tournament-based ensemble training scheme updates agents asynchronously in parallel, which decouples population evolution and singleagent learning. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, the key of the tournament-based ensemble training scheme is the interaction between a leaderboard and a training pool. The training pool contains hundreds of agents (pods) that 1) are trained in an asynchronous manner, and 2) can be initialized with different DRL algorithms for an ensemble purpose. The leaderboard records the agents with high performance and continually updates as more agents (pods) are trained.</p><p>As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, the tournament-based ensemble training scheme proceeds as follows:</p><p>1. An orchestrator instantiates a new agent (pod) and put it into a training pool. 2. A generator initializes an agent (pod) with networks and optimizers selected from a leaderboard. The generator is a class of subordinate functions associated with the leaderboard, which has different variations to support different evolution strategies. 3. An updater determines whether and where to insert an agent into the leaderboard according to its performance, after a pod has been trained for a certain number of steps or certain amount of time.</p><p>For system design, ElegantRL-podracer follows the cloud-native paradigm. ElegantRL-podracer achieves containerization by implementing the tournament-based ensemble training scheme as the synergy of microservices. Such a paradigm allows a lightweight usage via simple APIs and a high degree of customization through the flexible cooperation of microservices.</p><p>At a high-level, ElegantRL-podracer has the following capabilities to embody our design principles:</p><p>? 4 ElegantRL-Podracer: Scalable and Elastic Cloud-native Library</p><p>In this section, we propose a scalable and elastic cloud-native library, called ElegantRL-podracer.</p><p>We first describe its key components and then present its features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ensemble Training Using Microservices</head><p>As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, the ensemble training scheme exploits the synergy of the following microservices: orchestrator, leaderboard (including updater and generator), and agents (pods) in the training pool, where each microservice maps to a container.</p><p>Orchestrator: An orchestrator monitors the available computing resources and determines the number of pods in the training pool. When K8s signals that the workload is light, the orchestrator generates a set of new pods and insert them into the training pool. When a training objective (i.e., target rewards) is achieved, the orchestrator will terminate the training process. Leaderboard: A leaderboard records a set of candidate agents with high performance, say cumulative reward. An updater updates the candidate agents to the leaderboard, while a generator instantiates a new pod by referring to candidate agents. The leaderboard may also track other information, such as the covariance matrix, mean, variance, etc, which helps the generator to adaptively allocate computing resources to highly potential candidate agents.</p><p>? Updater: An updater receives a trained agent (pod) and may insert its training files (including actor network, critic network, optimizer parameters, replay buffer (for off-policy algorithms)) into the leaderboard if it has high performance. ? Generator: A generator generates training files for a newly created pod. The generator may perform a mutation of the candidate agents to increase the diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Agent Learning Using Microservices</head><p>ElegantRL-podracer maps the training process of an agent to a pod, which is the smallest deployable unit in K8s <ref type="bibr" target="#b1">Bernstein [2014]</ref>. As shown in Fig. <ref type="figure" target="#fig_3">3</ref>, ElegantRL-podracer separates an agent learning into three microservices: worker, learner and evaluator.</p><p>Worker: A worker generates batches of transitions from interactions between the actor and the batched environment. A batched environment consists of multiple independent sub-environments. Each actor collects transitions from the sub-environments of the batched environment in parallel to accelerate the data collection process.</p><p>Learner: A learner fetches a batch of transitions from the replay buffer to train the neural networks. ElegantRL-podracer trains multiple learners in parallel and fuses the networks by aggregating network parameters. In this way, ElegantRL-podracer experiments much less communication overhead than distributed SGD in RLlib <ref type="bibr" target="#b12">Liang et al. [2018]</ref>.</p><p>Evaluator: An evaluator continuously evaluates a pod and records its performance and corresponding networks during the training process. Commonly used performance metrics are the mean and variance of the episodic reward. Note that the evaluator effectively mitigates the performance loss caused by either overfitting or early stopping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Features of ElegantRL-podracer</head><p>ElegantRL-podracer achieves several features that facilitate the implementation of a lightweight and powerful training process on a GPU cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scalable parallelism:</head><p>The multi-level parallelism of ElegantRL-podracer leads to high scalability.</p><p>? Agent parallelism: The agents in the training pool are parallel, thus can easily scale out to a large number. The asynchronous training of parallel agents can also reduce the frequency of agent-to-agent communication. ? Learner parallelism: An agent employs multiple learners to train the neural networks in parallel, and then fuse the networks parameters to obtain a result agent, instead of using distributed SGD.</p><p>Such a model fusion through network parameters involves a much lower frequency communication as the fusion process only happens at the end of an epoch.</p><p>? Worker parallelism: An agent utilizes multiple rollout workers to sample transitions in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Elastic resource allocation:</head><p>The elasticity is critical for cloud-level applications as it helps users adapt to the changes in cloud resources and prevent over-provisioning and under-provisioning of resources <ref type="bibr" target="#b17">Mell and Grance [2011]</ref>, <ref type="bibr" target="#b9">Herbst et al. [2013]</ref>. ElegantRL-podracer can elastically allocate the number of agents (pods) by employing an orchestrator to monitor the available computing resources and the current training status.</p><p>Cloud-oriented optimizations: ElegantRL-podracer co-locates microservices on GPUs to accelerate the parallel computation on both data collection and model training. For the data transfer and storage, ElegantRL-podracer represents data as tensors to speedup the communication and allocates the shared replay buffer on the contiguous memory of GPUs to increase the addressing speed. Continuous integration/delivery (CI/CD): ElegantRL-podracer enables a robust CI/CD for users to explore new ideas by modifying existing microservices or build new microservices. The microservices are loosely coupled, such that the change of one microservice will not break existing ones. Also modularity allows for more comprehensive search over the experiment space, e.g., instead of designing one experiment at a time, we could theoretically be able to test c 1 ? c 2 ? ? ? c n experiments in an automated fashion, where n is the number of components for a DRL algorithm and c i is the number of optional microservices for component i, i = 1, ..., n:</p><p>? Environment variation: ElegantRL-podracer supports any environment written in gym-style and provides a class PreprocessVecEnv to convert a normal environment to a batch mode.  <ref type="bibr">et al. [2017]</ref>. New algorithms may be used as long as they adhere to the agent interface.</p><p>? Evolution variation: ElegantRL-podracer allows users to customize the evaluator, updater and generator in the leaderboard to decide how to evaluate, where to update, and what to generate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Performance Evaluation</head><p>In the section, we describe the cloud platform in our experiments and the performance of ElegantRLpodracer on various tasks from robotic control to stock trading task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Platform: GPU Cloud</head><p>All experiments were executed using NVIDIA DGX-2 servers <ref type="bibr" target="#b4">Choquette et al. [2021]</ref> in a DGX SuperPOD cloud system reference architecture [2020], a cloud-native infrastructure. Each DGX-2 server contains 8 A100 GPUs and 320 GB GPU memory in total, and also has 128 CPU cores of Dual AMD Rome 7742 running at 2.25GHz. Among the 8 A100 GPUs in one DGX-2 server, any two A100 GPUs are connected with each other through 12 NVLinks, providing 600 Gbps of full-duplex bandwidth <ref type="bibr" target="#b4">Choquette et al. [2021]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Robotic Control Tasks</head><p>Ant and humanoid are two canonical robotic control tasks that simulate an ant and a humanoid, respectively, where each task has both MuJoCo <ref type="bibr" target="#b27">Todorov et al. [2012]</ref> version and Isaac Gym <ref type="bibr" target="#b16">Makoviychuk et al. [2021]</ref> version, as shown in Fig. <ref type="figure" target="#fig_6">4</ref>. The ant task is a simple environment to simulate due to its stability in the initial state, while the humanoid task is often used as a testbed for  <ref type="bibr">Gym Makoviychuk et al. [2021]</ref> are slightly different, the objective of both is to have the agent move forward as fast as possible. The state space, action space and reward function are given in Table <ref type="table" target="#tab_2">1</ref>.</p><p>We select the same tasks from the two platforms in order to show that 1) ElegantRL-podracer can support different simulator platforms, and 2) the potential of massively parallel simulations  Performance metrics: We employ two different metrics to evaluate the agent's performance:</p><p>? Episodic reward vs. training time (wall-clock time): we measure the episodic reward at different training time, which can be affected by the convergence speed, communication overhead, scheduling efficiency, etc. ? Episodic reward vs. training step: from the same testings, we also measure the episodic reward at different training steps. This result can be used to investigate the massive parallel simulation capability of GPUs, and also check the algorithm's performance.</p><p>During the training process, we evaluate the agent 10 times to obtain 10 episodic rewards and report the average episodic reward and standard deviation.</p><p>For the four tasks in Fig. <ref type="figure" target="#fig_6">4</ref>, we terminate the training processes at 8, 000s (? 2.2 hours), 32, 000s (? 8.9 hours), 25, 000s (? 6.9 hours) and 9, 000s (? 2.5 hours), respectively. As shown in Fig. <ref type="figure">5</ref>, we can see that given the same training time, ElegantRL-podracer achieves substantially higher episodic rewards than RLlib. For Isaac Gym in particular, the corresponding episodic rewards have been nearly doubled. In the task ant (Isaac Gym), RLlib needs approximately 7.0 hours to achieve a reward 9, 000, while Elegant-podracer only needs approximately 1.4 hours to get the same reward, which is 5? faster.</p><p>We run 2.0 ? 10 7 steps, 2.5 ? 10 7 steps, 4 ? 10 8 steps and 8 ? 10 8 steps, respectively. Take a closer look at Fig. <ref type="figure">6</ref>, we can see that ElegantRL-podracer achieves higher episodic rewards than RLlib in all four tasks. A possible reason is the tournament-based ensemble training scheme guide a population of agents update toward a direction with higher rewards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Stock Trading Task</head><p>Finance is a promising and challenging real-world application of DRL algorithms. We apply ElegantRL-podracer to a stock trading task as an example <ref type="bibr" target="#b14">Liu et al. [2020]</ref>, <ref type="bibr" target="#b15">Li et al. [2021]</ref> to show its potential in quantitative finance.</p><p>Stock trading task: we aim to train a DRL agent that decides where to trade, at what price and what quantity in a stock market, thus the objective of the problem is to maximize the expected return and minimize the risk. We model the stock trading task as a Markov Decision Process (MDP) as in FinRL <ref type="bibr" target="#b14">Liu et al. [2020]</ref>, <ref type="bibr" target="#b15">Li et al. [2021]</ref>, where the state space, action space and reward function are given in Table <ref type="table" target="#tab_2">1</ref>.</p><p>Data pre-processing: We select the NASDAQ-100 constituent stocks as our stock pool and use the minute-level dataset for our experiment. Performance metrics: We evaluate trading performance and training performance, respectively. Five common metrics are used to quantify the trading performance:</p><p>? Cumulative return: subtracting the initial value from the final portfolio value, then dividing by the initial value.</p><p>? Annual return and volatility: geometric average return in a yearly sense, and the deviation.</p><p>? Maximum drawdown: the maximum observed loss from a historical peak to a trough of a portfolio, before a new peak is achieved. It is an indicator of downside risk over a time period.</p><p>? Sharpe ratio: the average return earned in excess of the risk-free rate per unit of volatility.</p><p>? Calmar ratio: the fund's average compounded annual rate of return versus its maximum drawdown.</p><p>For the training performance, we use the metric, episodic reward vs training time (wall-clock time), in Section 5.2. We record the required training time for reaching a specific cumulative return. Compared methods: We compare ElegantRL-podracer with RLlib <ref type="bibr" target="#b12">Liang et al. [2018]</ref> with the same setup in Section 5.2. Invesco QQQ ETF is the benchmark to represent the market performance. There are in total 80 A100 GPUs assigned to our usage.</p><p>From Fig. <ref type="figure">7</ref>, all DRL agents can achieve a better performance than the market benchmark with respect to the cumulative return, demonstrating the algorithm's effectiveness. According to Table <ref type="table" target="#tab_3">2</ref>, we observe that ElegantRL-podracer has cumulative return 104.743%, annual return 103.591%, and Sharpe ratio 2.20, which outperforms RLlib substantially. However, ElegantRL-podracer is not as stable as RLlib during the backtesting period: it achieves an annual volatility 35.357%, max. drawdown -17.187%, and Calmar ratio 6.02. There are two possible reasons to account for such an instability: 1) the reward design in the stock trading environment is mainly related to the cumulative return, thus leading the agent to take less care of the risk; 2) ElegantRL-podracer holds a large amount of funds around 2021-03, as shown in Fig. <ref type="figure">7</ref>, which naturally leads to a larger slip.</p><p>We compare the training performance on a varying number of GPUs, i.e., 8, 16, 32, and 80. We measure the required training time to obtain two cumulative returns 1.7 and 1.8, respectively. In Fig. <ref type="figure">7</ref>, both ElegantRL-podracer and RLlib <ref type="bibr" target="#b12">Liang et al. [2018]</ref> requires less training time to achieve the same cumulative return as the number of GPUs increases, which directly demonstrates the advantage of cloud computing resources on the DRL training. For ElegantRL-podracer with 80 GPUs, it requires (1900s, 2200s) to reach cumulative returns of 1.7 and 1.8. ElegantRL-podracer with 32 and 16 GPUs need (2400s, 2800s) and (3400s, 4000s) to achieve the same cumulative returns. It demonstrates the high scalability of ElegantRL-podracer and the effectiveness of our cloud-oriented optimizations. For the experiments using RLlib <ref type="bibr" target="#b12">Liang et al. [2018]</ref>, increasing the number of GPUs does not lead to much speed-up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion and Conclusion</head><p>In this paper, we have introduced ElegantRL-podracer, a scalable and elastic library for cloud-native deep reinforcement learning. To efficiently utilize millions of GPU cores for DRL training, we first propose a tournament-based ensemble training scheme to orchestrate the training process on hundreds of GPUs, and then enable massively parallel simulation on thousands of GPU cores in a single GPU. Moreover, we follow the cloud-native paradigm to schedule the training of DRL agents by adhering to containerization, microservices, and MLOps. Thus, ElegantRL-podracer realizes the design principles in the respect of scaling-out, efficiency, and accessibility.</p><p>By presenting ElegantRL-podracer to the DRL community, we hope that ElegantRL-podracer can help address the data collection bottleneck using the manycore GPU architecture and apply DRL algorithms to complex real-world problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A comparison of different frameworks/libraries.</figDesc><graphic url="image-1.png" coords="2,124.76,72.00,360.01,186.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: ElegantRL-podracer employs a tournament-based ensemble training, where a leaderboard is updated by a training pool of pods.</figDesc><graphic url="image-2.png" coords="4,108.00,72.01,395.99,176.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Asynchronously distributed training is made possible through a training pool. ElegantRLpodracer can scale out to hundreds or even thousands of computing nodes and elastically adjust the number of agents according to the available computing resources. ? Tournament-based ensemble training is made possible through a leaderboard. Tournamentbased training scheme decouples the agent learning and population evolution to achieve low communication overhead between pods. Ensembling many DRL algorithms increases efficiency by exploiting agent robustness and diversity. ? Cloud-nativity is made possible with the containerization, microservices, and MLOps adherence. MLOps achieves continuous training/integration/delivery (CT/CI/CD) by exploiting the Kurbernetes (K8s) Bernstein [2014] software for automated cloud orchestration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An agent (pod) is split into three types of microservices: worker, learner and evaluator.</figDesc><graphic url="image-3.png" coords="5,108.00,72.00,396.01,138.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Continuous training (CT) pipeline: Continuous training, which is a part of the MLOps practice, seeks to automatically and continuously retrain the model to adapt to changes that might occur in the data. ElegantRL-podracer performs the CT of a DRL agent by automating a lightweight DRL training pipeline, which is composed of microservices. Users can conduct different experiments and hyper-parameter search by modifying workers, learners and other microservices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>in the DRL training by comparing the CPU-based MuJoCo Todorov et al. [2012] with the GPU-based Isaac Gym Makoviychuk et al. [2021].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Snapshots of robotic control environments. From left to right, the ant and humanoid tasks from MuJoCo Todorov et al. [2012], and the ant and humanoid tasks from Isaac Gym Makoviychuk et al. [2021]. Compared methods: On one DGX-2 server, we compare ElegantRL-podracer with RLlib Liang et al. [2018], since both support multiple GPUs. ElegantRL-podracer used PPO Schulman et al. [2017] from ElegantRL Liu et al. [2021], while in RLlib Liang et al. [2018] we used the Decentralized Distributed Proximal Policy Optimization (DD-PPO)<ref type="bibr" target="#b29">Wijmans et al. [2020]</ref> algorithm that scales well to multiple GPUs. For fair comparison, we keep all adjustable parameters and computing resources the same, such as the depth and width of neural networks, total training steps/time, number of workers, and GPU and CPU resources. Specifically, we use a batch size of 1024, learning rate of 0.001, and a replay buffer size of 4096 across tasks.</figDesc><graphic url="image-6.png" coords="7,311.50,330.91,86.40,86.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Episodic reward vs. training time (wall-clock time) for the four tasks in Fig. 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>For the data preparation, we download the raw data from the Compustat database through the Wharton Research Data Services (WRDS) Service [2015]. Next, we process it to an open-high-low-close-volume (OHLCV) format and extract technical indicators. Finally, we follow a training-backtesting pipeline and split the dataset into two sets: the data from 01/01/2016 to 05/25/2020 for training, and the data from 05/26/2020 to 05/26/2021 for backtesting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Many open-source DRL frameworks/libraries have been developed in recent years with varied capabilities. OpenAI Spinning Up OpenAI[2018]  and Google Dopamine Castro et al.[2018]  are research frameworks for the fast prototyping of DRL algorithms. They both implement numerous DRL algorithms with simple and pedagogical codes. Stable Baseline3<ref type="bibr" target="#b20">Raffin et al. [2019]</ref> is a stable and efficient DRL library, introducing the parallelism of sampling through vectorized environment.</figDesc><table><row><cell>2.1 Deep Reinforcement Learning Framework/Libraries</cell></row><row><cell>RLlib Liang et al. [2018] is a generic DRL library that derives its strength from Ray communication</cell></row><row><cell>protocols that enable scalable, distributed training. Podracer Hessel et al. [2021] from DeepMind</cell></row><row><cell>is closely related to our ElegantRL-podracer, which also focuses on the efficient usage of large</cell></row><row><cell>computing resources for training DRL agents. However, it is designed for Google's tensor processing</cell></row><row><cell>units (TPUs) that are inaccessible to many researchers and practitioners.</cell></row><row><cell>2.2 Simulation Packages</cell></row><row><cell>Environment simulation is a critical component of DRL training, and lots of platforms that provide</cell></row><row><cell>various task simulations are emerging to close the simulation-to-reality gap. OpenAI Gym Brockman</cell></row><row><cell>et al. [2016] is a fundamental simulation toolkit for DRL research, which defines a standard interface</cell></row><row><cell>for follow-up works. It includes a collection of benchmark problems, e.g., classic control, Atari</cell></row><row><cell>games, and 2D and 3D robots. MuJoCo Todorov et al. [2012] and Isaac Gym Makoviychuk et al.</cell></row><row><cell>[2021] are two powerful platforms for robotic simulations. MuJoCo Todorov et al. [2012] is a</cell></row><row><cell>popular physics simulator that efficiently simulates joint contact models. The recently released</cell></row><row><cell>Isaac Gym Makoviychuk et al. [2021] is a high-performance simulation environment for physics.</cell></row><row><cell>It enables thousands of environments running in parallel on a single GPU. FinRL Liu et al. [2020],</cell></row><row><cell>Li et al. [2021] is a new finance-related DRL platform, which simulates various markets as training</cell></row><row><cell>environments that are built on historical data and live trading APIs.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The state space, action space and reward function of ant, humanoid and stock trading tasks. locomotion learning. Even though the implementations of MuJoCo<ref type="bibr" target="#b27">Todorov et al. [2012]</ref> and Isaac</figDesc><table><row><cell>Tasks</cell><cell>State space S</cell><cell>Action space A</cell><cell>Reward r(s, a, s )</cell></row><row><cell>Ant Todorov et al.</cell><cell>Body height and rotation</cell><cell>8 controllable joints</cell><cell>Alive bonus</cell></row><row><cell>[2012],</cell><cell>Velocity and angular velocity</cell><cell></cell><cell>Running speed</cell></row><row><cell>Makoviychuk et al.</cell><cell>Joint angles</cell><cell></cell><cell>Standing and Heading</cell></row><row><cell>[2021]</cell><cell>Forces, etc.</cell><cell></cell><cell>Contact forces</cell></row><row><cell>Humanoid</cell><cell>Body height and rotation</cell><cell>17 joints for MuJoCo</cell><cell>Alive bonus</cell></row><row><cell>Todorov et al. [2012], Makoviychuk et al. [2021]</cell><cell cols="3">Velocity and angular velocity 21 joints for Isaac gym Running speed Joint angles Standing and Heading Forces, etc. Contact forces</cell></row><row><cell>Stock trading Liu et al. [2020]</cell><cell>Balance, Shares Close prices Technical indicators</cell><cell>Buy Sell Hold</cell><cell>Change of account value</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Stock trading performance on NASDAQ-100 constituent stocks with minute-level data.We reserve a time period not used for training but only testing to evaluate generalization performance for the stock trading problem. Since the agent cannot access the testing dataset during the training, we store the model snapshots at different training times, say every 100 seconds. Later, we use each snapshot model to perform inference on the testing dataset to obtain the cumulative return.</figDesc><table><row><cell>Cumulative Return</cell><cell>1.2 1.4 1.6 1.8 2.0</cell><cell>ElegantRL-Podracer RLlib QQQ</cell><cell></cell><cell>Training Time (1000s)</cell><cell>2 4 6 8 10</cell><cell>ElegantRL-Podracer-1.7 RLlib-1.7 ElegantRL-Podracer-1.8 RLlib-1.8</cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">2020-07 2020-09 2020-11 2021-01 2021-03 2021-05</cell><cell>8</cell><cell>16</cell><cell># GPUs</cell><cell>32</cell><cell>80</cell></row><row><cell cols="7">Figure 7: Left: cumulative return on minute-level NASDAQ-100 constituents stocks (initial capital</cell></row><row><cell cols="7">$1, 000, 000, transaction cost 0.2%). Right: training time (wall-clock time) for reaching cumulative</cell></row><row><cell cols="7">rewards 1.7 and 1.8, using the model snapshots of ElegantRL-podracer and RLlib Liang et al. [2018].</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Cumul.</cell><cell>Annual</cell><cell>Annual</cell><cell>Max.</cell><cell>Sharpe</cell><cell>Calmar</cell></row><row><cell></cell><cell></cell><cell></cell><cell>return</cell><cell>return</cell><cell>volatility</cell><cell>drawdown</cell><cell>ratio</cell><cell>ratio</cell></row><row><cell cols="3">ElegantRL-podracer</cell><cell cols="4">104.743% 103.591% 35.357% -17.187%</cell><cell>2.20</cell><cell>6.02</cell></row><row><cell cols="3">RLlib Liang et al. [2018]</cell><cell>86.274%</cell><cell>85.364%</cell><cell cols="2">34.319% -13.689%</cell><cell>1.98</cell><cell>6.24</cell></row><row><cell cols="3">Invesco QQQ ETF</cell><cell>46.586%</cell><cell>46.146%</cell><cell cols="2">23.39% -12.749%</cell><cell>1.75</cell><cell>3.62</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This research used computational resources of the GPU cloud platform system reference architecture [2020] provided by the <rs type="institution">IDEA Research institute</rs>.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Microservices architecture enables devops: Migration to a cloud-native architecture</title>
		<author>
			<persName><forename type="first">A</forename><surname>Armin Balalaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pooyan</forename><surname>Heydarnoori</surname></persName>
		</author>
		<author>
			<persName><surname>Jamshidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Software</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="42" to="52" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Containers and cloud: From lxc to docker to kubernetes</title>
		<author>
			<persName><forename type="first">David</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Cloud Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="81" to="84" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01540</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">OpenAI Gym. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Samuel Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhodeep</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carles</forename><surname>Gelada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.06110</idno>
		<title level="m">Dopamine: A research framework for deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">NVIDIA A100 tensor core GPU: Performance and innovation</title>
		<author>
			<persName><forename type="first">Jack</forename><surname>Choquette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wishwesh</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Giroux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Stam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronny</forename><surname>Krashinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="29" to="35" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Szymon</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Zhokhov</surname></persName>
		</author>
		<ptr target="https://github.com/openai/baselines" />
	</analytic>
	<monogr>
		<title level="j">OpenAI baselines</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Addressing function approximation error in actorcritic methods</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herke</forename><surname>Hoof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Meger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1587" to="1596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Cloud-native applications</title>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Gannon</surname></persName>
			<affiliation>
				<orgName type="collaboration">IEEE Cloud Comput</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barga</surname></persName>
			<affiliation>
				<orgName type="collaboration">IEEE Cloud Comput</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Sundaresan</surname></persName>
			<affiliation>
				<orgName type="collaboration">IEEE Cloud Comput</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="16" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor</title>
		<author>
			<persName><forename type="first">Tuomas</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurick</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Elasticity in cloud computing: What it is, and what it is not</title>
		<author>
			<persName><forename type="first">N</forename><surname>Herbst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Kounev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><forename type="middle">H</forename><surname>Reussner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICAC</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Kroiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iurii</forename><surname>Kemaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Keck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hado</forename><surname>Van Hasselt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06272</idno>
		<title level="m">Podracer architectures for scalable reinforcement learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">FinRL-Podracer: High performance and scalable deep reinforcement learning for quantitative finance</title>
		<author>
			<persName><forename type="first">Zechu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anwar</forename><surname>Walid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on AI in Finance (ICAIF)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">RLlib: Abstractions for distributed reinforcement learning</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3053" to="3062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Continuous control with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Timothy P Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">FinRL: A deep reinforcement learning library for automated stock trading in quantitative finance</title>
		<author>
			<persName><forename type="first">Xiao-Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runjia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liuqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><forename type="middle">Dan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deep Reinforcement Learning Workshop</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Xiao-Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zechu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="https://github.com/AI4Finance-Foundation/ElegantRL" />
		<title level="m">ElegantRL: A lightweight and stable deep reinforcement learning library</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Makoviychuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Wawrzyniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunrong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kier</forename><surname>Storey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Macklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Hoeller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Allshire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Handa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10470</idno>
		<title level="m">Isaac Gym: High performance GPU-based physics simulation for robot learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The nist definition of cloud computing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Grance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CSRC</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno>ArXiv, abs/1312.5602</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">OpenAI spinning up</title>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target="https://spinningup.openai.com" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Antonin</forename><surname>Raffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashley</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Ernestus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Gleave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anssi</forename><surname>Kanervisto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Dormann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Stable baselines3. GitHub repository</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Szymon</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03864</idno>
		<title level="m">Evolution strategies as a scalable alternative to reinforcement learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Proximal policy optimization algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno>ArXiv, abs/1707.06347</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Standard &amp; Poor&apos;s compustat</title>
	</analytic>
	<monogr>
		<title level="m">Data retrieved from Wharton Research Data Service</title>
		<imprint>
			<publisher>Wharton Research Data Service</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mastering the game of Go without human knowledge</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Bolton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">550</biblScope>
			<biblScope unit="issue">7676</biblScope>
			<biblScope unit="page" from="354" to="359" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Adam</forename><surname>Stooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01500</idno>
		<title level="m">rlpyt: A research code base for deep reinforcement learning in pytorch</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">NVIDIA DGX SuperPOD: Scalable infrastructure for AI leadership</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
		<respStmt>
			<orgName>NVIDIA Corporation</orgName>
		</respStmt>
	</monogr>
	<note>NVIDIA DGX A100 system reference architecture</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">MuJoCo: A physics engine for model-based control</title>
		<author>
			<persName><forename type="first">Emanuel</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="5026" to="5033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with double Qlearning</title>
		<author>
			<persName><forename type="first">Hado</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">DD-PPO: Learning near-perfect pointgoal navigators from 2.5 billion frames</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irfan</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reinforcement learning for robot research: A comprehensive review and open issues</title>
		<author>
			<persName><forename type="first">Tengteng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Mo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Advanced Robotic Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">17298814211007305</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
