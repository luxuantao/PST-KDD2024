<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Finding Motifs in Knowledge Graphs using Compression</title>
				<funder ref="#_AzByvEg">
					<orgName type="full">Amsterdam Academic Alliance Data Science</orgName>
					<orgName type="abbreviated">DS</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-04-19">April 19, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
							<email>vu@peterbloem.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">Vrije Universiteit Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Finding Motifs in Knowledge Graphs using Compression</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-04-19">April 19, 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2104.08163v1[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a method to find network motifs in knowledge graphs. Network motifs are useful patterns or meaningful subunits of the graph that recur frequently. We extend the common definition of a network motif to coincide with a basic graph pattern. We introduce an approach, inspired by recent work for simple graphs, to induce these from a given knowledge graph, and show that the motifs found reflect the basic structure of the graph. Specifically, we show that in random graphs, no motifs are found, and that when we insert a motif artificially, it can be detected. Finally, we show the results of motif induction on three real-world knowledge graphs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge graphs are an extremely versatile and flexible data model. They allow knowledge to be encoded without a predefined format and they are extremely robust in the face of missing data. This versatility comes at a price. For a given knowledge graph, it can be difficult to see the forest for the trees: how is the graph structured at the lowest level? What kind of things can I ask of what types of entities? What are small, recurring patterns that might represent a novel insight into the data? Answering these questions could benefit problem domains like graph simplification, graph navigation and schema induction.</p><p>In the domain of unlabeled simple graphs, network motifs <ref type="bibr" target="#b13">[9]</ref> were introduced as a tool to provide insight into local graph structure. Network motifs are small subgraphs whose frequency in the graph is unexpected with respect to a null model.</p><p>Unfortunately, estimating this probability usually requires repeating the subgraph count on many samples from the null model. To avoid this costly operation, <ref type="bibr" target="#b7">[3]</ref> introduces an alternative method, using compression as a heuristic for motif relevance: the better a motif compresses the data, the more likely it is to be meaningful.</p><p>In this paper, we extend this compression-based motif analysis to knowledge graphs. For the purposes of this research we define knowledge graphs as labeled, directed multigraphs. Nodes are uniquely labeled with entity identifier, and links are nonuniquely labeled with relations. We extend the definition of a motif to that of a basic graph pattern: a small graph labeled with both variables and explicit entitites and relations. A pattern matches if the variables can be replaced with specific values from the graph so that the pattern becomes a subgraph as a result.</p><p>The intuition behind our method is that we can use graph patterns to compress the graph: we store the pattern, its instances, and the remainder of the graph. The better this representation compresses the graph, the more relevant the pattern. Figure <ref type="figure" target="#fig_0">1</ref> illustrates the principle. In Section 1.1, we justify this intuition more formally.</p><p>We perform several experiments to show that our method returns meaningful subgraphs. First we test the intuition that a random graph should contain no motifs. We also show that when we artificially insert motifs into a random graph, we can then detect these as motifs. Finally, we show the results of motif analy- We consider only the integer indices of the nodes and relations. Labels are included only for readability. b) A motif that occurs frequently. c) A compressed representation; we remove all edges that are part of an occurrence of the motif and store separately which nodes match the motif. Together with a the motif, this allows us to reconstruct the data. sis on three real-world knowledge graphs, compared to the baseline of selecting the most frequent graph patterns.</p><p>All code and datasets used in this paper are available online. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Network motifs for unlabeled simple graphs were introduced in <ref type="bibr" target="#b13">[9]</ref>. A more comprehensive overview of the related literature can be found in <ref type="bibr" target="#b7">[3]</ref>. In <ref type="bibr" target="#b7">[3]</ref>, the principle of Minimum Description Length (MDL) was first connected to motif analysis. However, the idea had earlier been exploited for detecting meaningful subgraphs in the SUBDUE algorithm <ref type="bibr" target="#b8">[4]</ref>.</p><p>A few other methods have been proposed for inducing the structure of a given knowledge graph in terms of subgraphs. In <ref type="bibr" target="#b17">[13]</ref>, the authors use the principle of characteristic sets to characterize a knowledge graph in terms of the star patterns it contains. In <ref type="bibr" target="#b16">[12]</ref>, they show that the majority of the LOD cloud can be efficiently described using such principles, showing the highly tabular structure of many knowledge graphs. In <ref type="bibr" target="#b21">[17]</ref>, association rule mining is used to induce basic patterns in the graph.</p><p>To the best of our knowledge, ours is the first method presented that can potentially induce any basic graph pattern.</p><p>1 https://github.com/MaestroGraph/motive-rdf</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Preliminaries</head><p>Minimum Description Length Our method is based on the MDL principle: we should favour models that compress the data. We will show briefly how this intuition can be made mathematically precise. For more details, we refer the reader to <ref type="bibr" target="#b10">[6]</ref> for MDL in general, and to <ref type="bibr" target="#b6">[2]</ref>, for a more extensive discussion these principles in the domain of graph analysis.</p><p>Let B be the set of all finite-length binary strings. We use |b| to represent the length of b ? B. Let log(x) = log 2 (x). A code for a set of objects X is an injective function f : X ? B. All codes in this paper are prefix-free: no code word is the prefix of another. We will denote a codelength function with the letter L, ie. L(x) = |f(x)|. We commonly compute L(x) directly, without first computing f(x).</p><p>There is a strong relation between codes and probability distributions: for each probability distribution p on X, there exists a prefix-free code L such that for all x ? X:log p(x) L(x) &lt;log p(x) + 1. Inversely, for every prefix-free code L for X, there exists a probability distribution p such that for all x ? X: p(x) = 2 -L(x) . For proofs, see [6, Section 3. Relevance testing We will use the MDL principle to perform a hypothesis test. Assume we have some data x ? B and a null hypothesis stating that it was sampled from distribution p null (with corresponding code L null ). A simple but crucial result, known as the no-hypercompression inequality <ref type="bibr">[6, p103]</ref> tells us that the probability of sampling any data x from p null that can be described in less than L null (x) -k or more bits, using any code is less than 2 -k . Thus, we can reject the hypothesis that the data was sampled from p null by designing an alternative code L alt which compresses the data better than L null by, say, 10 bits (L null (x) -L alt (x) 10) and rejecting the null hypothesis with confidence 2 -10 . For a longer, more intuitive explanation of this principle in pattern induction, we refer the reader to <ref type="bibr" target="#b6">[2]</ref>.</p><p>Note that when we use this procedure to find motifs, we are not providing statistical evidence for the hypothesis that the motif is "correct" [3, Section 6.1]. We are simply using the principle of hypothesis testing as a heuristic for pattern mining. The only assertion we are proving (in a statistical sense) is that the data did not come from the null model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Common codes</head><p>In the construction of our graph codes, we require some simpler codes as building blocks. First, when we store any positive integer n, we do so with the code corresponding to the distribution p N (n) = 1/(n(n + 1)), and denote it L N (n). For nonnegative numbers we add 1 to the argument. For the full range of integers (L Z ), we add an extra bit for the sign, and then use the first code for negative integers and the second for positive ones.</p><p>We will often need to encode sequences of integers as well. These will be highly skewed, with only a subset of integers occurring frequently, and others occurring infrequently or not at all. As noted in <ref type="bibr" target="#b19">[15]</ref> a code based on the Pitman-Yor model <ref type="bibr" target="#b18">[14]</ref> is very effective in such situations. Let S = S 1 , ..., S n be a sequence of integers of length n. We first store the set of its members m(S) (the vocabulary) in the order in which they occur: we store n and the first member using L N and then store each subsequent member by encoding the distance to the previous member using L Z . Having encoded the members of S we can store the sequence itself using the Pitman-Yor model as follows.</p><p>Let f(A, B) be the frequency of symbol A in se-quence B. We then store the complete sequence using the code corresponding to the following distribution:</p><formula xml:id="formula_0">p(S) = i?[1,k] p(S i | S 1:i-1 ) with p(A | B) = ?-d|m(B)| |m(B)|+? if f(A, B) = 0 f(A,B)-d |m(B)|+?</formula><p>otherwise See <ref type="bibr" target="#b19">[15]</ref> for a more intuitive explanation. In all experiments we use ? = 0.5, d = 0.1. We will refer to the total resulting codelength as L PY (S).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>We will first give a precise definition of a knowledge graph as used in this paper. We will then describe the null model which is used both as a point of comparison in our hypothesis test, and within the motif code to compress the remainder of the graph. Next, we describe how to compress a graph using a given motif, and a set of instances. Finally, we will describe how to search for likely motifs using simulated annealing.</p><p>We analyse the structure of knowledge graphs only, ignoring any meaning in relation to other graphs, encoded in the content of names or literals, or from ontology languages. Specifically, we model a knowledge graph as a multigraph with nodes and edges labeled with integers that map to entities and relations. This mapping is stored, but only the integer-labeled graph is modelled. <ref type="foot" target="#foot_0">2</ref>A knowledge graph G, is a tuple This definition is compatible with RDF data. We interpret literals as nodes, considered the same node if they are expressed by the same string.</p><formula xml:id="formula_1">G = (v G , r G , E G ). v G ? N is the number of nodes in the graph, and r G ? N is the number of relations. We define the nodeset of G as V G = {0, . . . , v G -1} and the relation-set as R G = {0, . . . , r G }. The triple- set E G ? V G ? R ? V G determines</formula><p>A pattern M for graph G is a tuple</p><formula xml:id="formula_2">(V M , R M , G, E M ).</formula><p>Let v M and r M indicate the number of variable nodes and variable links in M respectively, then</p><formula xml:id="formula_3">V M ? {-v M , . . . , v G -1} and R M ? {-(r M + v M ), . . . , -v M , 0, . . . , r G -1}, with E M ? V M ? R M ? V M representing</formula><p>the edges as before. That is; nodes in a pattern can be labeled either with nonnegative integers referring to G's nodes or with negative integers representing a variable node, and similar for relations. The negative integers are always contiguous within a single pattern, with the highest representing the node labels and the lowest representing the edge labels An instance for pattern M in graph G is a pair of sequences of integers: I = (I n , I r ). I n is a sequence of distinct integers of length v M . I r is a sequence of non-distinct integers of length r M . For each edge (s, p, o) ? E M with any or all of s, p and o negative, there is a corresponding link in E G with a negative s replaced by I n -s , a negative o replaced by I n -o , and a negative p replaced by I r -p-v M . Put simply: for a pattern to match, variable edges marked with the same negative integer, must map to the same relation in order for the pattern to match, but variable links labeled with different negative integers may map to the same relation. Variable nodes are always labeled distinctly and may never map to the same node in G. An instance describes a subgraph of G that matches the pattern M. Each edge in the motif may only match one edge in the graph. In other words, the occurrence of the motif in the graph must have as many edges as the motif itself. <ref type="foot" target="#foot_1">3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Null model</head><p>For a proper hypothesis test, we must compare the compression achieved by our motif code to the compression under a general model for knowledge graphs: a null model.</p><p>The most common null model in classical motif analysis is the degree-sequence model (also known as the configuration model <ref type="bibr" target="#b15">[11]</ref>): a uniform distribution over all graphs with a particular degree sequence. We extend this to knowledge graphs by also including the degree of each relation: that is, degree of a relation is the frequency with which it occurs in the tripleset. Let a degree sequence D of length n be a triple of three integer sequences: (D in , D rel , D out ). If D is the degree sequence of a graph, then node i has D in i incoming links, D out i outgoing links and for each relation r, there are D rel r triples. Let G D be the set of all graphs with degree sequence D. Then the degree-sequence model can be expressed simply as</p><formula xml:id="formula_4">p DS (G) = 1 |G D |</formula><p>for any G that satisfies D and p(G) = 0 otherwise. Unfortunately, there is no efficient way to compute |G D | and even approximations tend to be costly for large graphs. Following the approach in <ref type="bibr" target="#b7">[3]</ref>, we define a fast approximation to the configuration model, which works well in practice for motif detection.</p><p>We can describe a knowledge graph by three length-m integer sequences: S, P, O, such that {(S j , P j , O j )} j is the graph's tripleset. If the graph satisfies degree sequence D, then we know that S should contain node j D out j times, P should contain relation r D rel r times and O should contain node j D in j times. Let S D be the set of all such triples of integer sequences satisfying D. We have</p><formula xml:id="formula_5">|S D | = m D out 1 , . . . , D out n m D rel 1 , . . . , D rel |R G | m D in 1 , . . . , D in n .</formula><p>While every member of S D represents a valid graph satisfying D, many graphs are represented multiple times. Firstly, many elements of S D contain the same link multiple times. We call the set without these elements S D ? S D . Secondly the links of the graph are listed in arbitrary order; if we apply the same permutation to all three lists S, P and O, we get a new representation of the same graph. Since we know that any element in S D contains only unique triples, we know that each graph is present exactly m! times. This gives us</p><formula xml:id="formula_6">|G D | = |S D | 1 m! |S D | 1 m! .</formula><p>We can thus use</p><formula xml:id="formula_7">p EL D (G) = m! |S D | p DS (G).</formula><p>Filling in the definition of the multinomial coefficient, and rewriting, we get a codelength of:</p><formula xml:id="formula_8">-log p EL D (G) = 2 log(m!) - i log(D in i !) - i log(D rel i !) - i log(D out i !)</formula><p>as an approximation for the DS model. We call this the edgelist (EL) model. It gives a probability that always lower-bounds the configuration model, since it affords some probability mass to graphs that cannot exist. Experiments in the classical motif setting have shown that the EL model is an acceptable proxy for the DS model <ref type="bibr" target="#b7">[3]</ref>, especially considering the extra scalability it affords.</p><p>Encoding D In order to encode a graph with L EL D , we must first encode D. <ref type="foot" target="#foot_2">4</ref> For each of the three sequences D in D we use the following model:</p><formula xml:id="formula_9">p(D ) = i q N (D i ) L(D ) = - i log q N (D i )</formula><p>where 1 N is any distribution on the natural numbers. This is an optimal encoding for D assuming that its members are independently drawn from q N . When we use p EL as the null model, we use the data distribution for q N to ensure that we have a lower bound to the optimal code-lenngth (in essence, we cheat in favor of the null model,giving it a slightly lower than optimal codelength). When we use p EL as part of the motif code, we must use a fair encoding, so we use the Pitman-Yor code to store each sequence in D.</p><p>In the design of our method, we will constantly aim to find a trade-off between completeness and efficiency that allows the method to scale to very large graphs. Specifically, when we economize, we will only do so in a way that makes the hypothesis test more conservative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Motif code</head><p>Having defined our representation of a knowledge graph, and a general null model for compressing such knowledge graphs, we can now define how we use a given pattern (together with its instances) to compress a dataset.</p><p>We will assume that a target pattern M is given for the data G and that we have a set of instances I of M in G. Moreover, we require that all instances in I are mutually disjoint: no two subgraphs defined by a member of I may share an edge, but nodes may be shared. Given this information, we will define a motif code that will help us determine whether or not M is a likely motif for G. In section Section 2.3, we detail a method to search for pairs (M, I) to pass to the motif code.</p><p>As described above, we can perform our relevance test with any compression method which exploits the pattern M and its instances I to store the graph efficiently. The better our method, the more motifs we will find. Note that there is no need for our code to be optimal in any sense. We know that we will not find all motifs that exist, and we will not use them optimally to represent the graph, but the test is still valid. This also means that we are free to trade off compression performance against efficiency of computation.</p><p>We store the graph by encoding various aspects, one after the other. The information in all of these together is sufficient to reconstruct the graph. Note that everything is stored using prefix-free codes, so that we can simply concatenate the codewords we get for each aspect, to get a codeword for the whole graph.</p><p>We also assume that we are given a code L base for generic knowledge graphs (in practice, this will be the null model, although the motif code is valid for any base code).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1</head><p>The motif code L motif (G; M, I, L base ). Note that the nodes and relations of the graph are integers.</p><formula xml:id="formula_10">function codelength(G; M, I, L base ): a graph G, a pattern M instances I of M in G, a code L base . b dim ? L N (v G ) + L N (r G ) + L N (|E G |)</formula><p>-Turn the pattern into a normal knowledge graph E M ? the edges of M with positive integer labels</p><formula xml:id="formula_11">M ? (v M , r M , E M ) S M ? the labels of M in canonical order b pattern ? L base (M ) + L PY (S M ) -Store the template graph E G ? E G -? I?I triples(I) b template ? L base ((v G , r G , E G )) b instances ? -log p M (I) + D?D I L PY (D) return b dim + b pattern + b template + b instances</formula><p>We store, in order:</p><p>the graph dimensions We first store v G , r G and |E G | using the generic code L N (?).</p><p>the pattern We store the structure of the pattern using the base code, and its labels as a sequence using the Pitman-Yor code.</p><p>the template This is the graph, minus all links occurring in instances of M. Let E G be E G minus any link occurring in any member of I. We then store</p><formula xml:id="formula_12">(v G , r G , E G ) using L base (?).</formula><p>the instances To store the instances, we view the connections between the nodes made by motifs as a hypergraph, and we extend the EL code to store it. The details are given below.</p><p>The precise computation of the codelength is given in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoding motif instances</head><p>To encode a list of instances I of a given pattern M, we generalize the idea of the edgelist model described above.</p><p>To generalize this notion to arbitrary patterns, to be defined for a given template graph, we define the degree constraint D I of a list of instances for a given pattern as follows: for each variable node i in the pattern, the degree constraint provides an integer sequence D i of length v G , indicating how often each node in the completed knowledge graph takes that position in the pattern. Similarly, for each variable edge j in the pattern, the degree constraint provides an integer sequence C j of length r G indicating for each relation how often it takes that position in the pattern.</p><p>We store these sequences in the same manner as the degree sequence of the template graph, using the Pitman-Yor code for each.</p><p>Given this information, all we need to do is describe which of the possible sequences of matches for this pattern satisfying the given degree constraint we are encoding. As with the configuration model, the ideal is a uniform code over all possible configurations, for which we will define an approximation. Given w variable nodes in a pattern, and l variable edges, we can define such a collection of instances using w+l integer sequences: N 1 , . . . , N n , L 1 , . . . , L l , with the t-th instance defined by the integer tuple (N 1 t , . . . , N n t , L 1 t , . . . , L l t ). If this set of sequences satisfies the degree constraint, we know that node q must occur D i q times in sequence N i , and similarly for the variable links. Let S I be the set of all such integer sequences satisfying the constraint. We follow the same logic as for the EL model. Let k be the number of matches of the pattern. We have:</p><formula xml:id="formula_13">|S I | = k D 1 1 , . . . , D 1 v ? . . . ? k D w 1 , . . . , D w v ? k C 1 1 , . . . , C 1 r ? . . . ? k C l 1 , . . . , C l r</formula><p>As before, this set is larger than the set we are interested in. First, each set of pattern matches is contained multiple times (once for each permutation) and second, not all elements are valid pattern matches (in some, a single triple may be represented by multiple instances). Let S I be the subset representing only valid matches, and let G I be the set of valid instances with permutations removed. As before, we have</p><formula xml:id="formula_14">|G I | = |S I | 1 k! |S I | 1 k! .</formula><p>Which gives us the following distribution</p><formula xml:id="formula_15">p M (G) = k! |S D | &lt; 1 G D ,</formula><p>withlog p M (I) as a code to store the instances. Rewriting as before, gives us a codelength of</p><formula xml:id="formula_16">-log p M (G) = (w + l -1) log(k!) - j?[1,w],i log(D j i !) - j?[1,l],i log(C j i !)</formula><p>Note that if we store a graph with the pattern ?n1 ?rel ?n2 we obtain an empty template graph, and this code reduces to the EL code, achieving the same codelength as the edgelist model, up to a small constant amount for storing the pattern.</p><p>For a given graph and pattern, we can simply find the complete list of instances using a graph pattern search. Since we require a slightly different semantics than standard graph pattern matchers, we adapt the DualIso algorithm <ref type="bibr" target="#b20">[16]</ref> for knowledge graph matching. Before computing the motif code, we prune the list of instances provided by this search iterating over the instances and removing any instance that produces a triple also produced by an earlier instance. To guard against rare patterns that produce long-running searches we terminate all searches after 5 seconds, returning only those matches that were found within the time limit.</p><p>We express the strength of a motif by its log-factor:</p><formula xml:id="formula_17">L null (G) -L motif (G; M, I, L base ) .</formula><p>If this value is positive, the motif code compresses the graph better than the null model. If the log-factor is greater than 10 bits, it corresponds to a rejection of the null model at p &lt; 0.001.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Motif search</head><p>Ultimately, we want to find any patterns that have a high log-factor for a given graph G. Since we can readily compute the log-factor for any given pattern, any black-box optimization algorithm can be used to search the space of all possible motifs. For the sake of simplicity, we will use basic simulated annealing: We start with a given pattern, and iterate by modifying the pattern in one of seven ways, chosen randomly. At each iteration, we search for instances of the pattern (limiting the time per search to 5 seconds) and compute the log-factor. If the log factor is better, we move to the new pattern, if it is worse, we move to the new pattern with probability 0.5.</p><p>The starting pattern is always a single random triple from the graph, with its relation made a variable. We define seven possible transition from one pattern to another: Extend Choose an instance of the pattern and an adjacent triple not part of the instance. Add the triple to the pattern. Make a node a variable Choose a random constant node, and turn it in to a variable node. Make an edge a variable Choose a random constant edge label, and turn it in to a variable (always introducing a new variable). Make a variable node constant Choose a random variable node and turn it into a constant. Take the value from a random instance. Make a variable edge constant Choose a random variable edge and turn it into a constant. Take the value from a random instance. Remove an edge Remove a random edge from the pattern, ensuring that it stays connected. Couple Take two distinct edge variables, which for at least one instance hold the same value and turn them into a single variable.</p><p>All transitions are equally likely. If the transition cannot be made (for instance, there are no constant nodes to make variable) or if the resultant pattern is in some way invalid, we sample a new transition.</p><p>Once a new pattern has been sampled, we compare its codelength under the motif model to that of the previous sample. If the codelength is lower, we continue with the new pattern. If the codelength is longer, we continue with the new sample with probability ? or return to the previous pattern otherwise. We use ? = 0.5 in all experiments.</p><p>We store all encountered patterns and their scores. In order to exploit all available processor cores, we run several searches in parallel. We take the top 1000 patterns from each and sort them by motif codelength. Variables are re-ordered to a canonical ordering using the Nauty algorithm <ref type="bibr" target="#b12">[8]</ref>, so that isomorphic patterns are not tested twice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Random graphs To validate the method, we first test it on random graphs. The aim is to test two requirements of a succesful pattern mining method:</p><p>? In a fully random graph, there should be no motifs, and we do not expect a motif code to outperform the null model.</p><p>? If we insert a small number of instances of a particular pattern into the graph manually, these should be recognized as motifs.</p><p>We sample a directed graph with a given number of nodes n and edges m, with no self-connections and multiple edges (that is, we sample from from the G(n, m) Erd?s-Renyi model). We then label the nodes uniformly at random with one of the relations in 0, . . . , r. To make the dimensions realistic we base We then take one randomly chosen pattern, and insert k = 75 instances of the pattern into the graph. We run a search for 100 000 iterations. And collect the 10 motifs with the best log-factor. We then sample two other graphs by the same method: one with k = 0 and one with k = 150. We also test each of the 10 motifs found on these two graphs.</p><p>The results are shown in Figure <ref type="figure" target="#fig_4">2</ref>. For k = 0, as expected, we find no patterns with positive compression. We also ran a full search on this graph to verify that no motifs can be found unless they are explicitly added to the graph. As in <ref type="bibr" target="#b7">[3]</ref>, we find that the inserted motif is recovered, even at a low frequency, but many other subgraphs, that share structure properties with the inserted pattern are also marked as motifs. We can recognize the inserted motif as the one, with the highest log-factor, but we see that many of these "partial motifs" will be included in the resulting list of patterns with a positive log factor.</p><p>This experiment only tests a single pattern. To see the effect of multiple random patterns, we repeat the experiment many times, sampling both the pattern and the random graph.</p><p>To sample the pattern we first sample a random number of nodes n from U <ref type="bibr" target="#b7">(3,</ref><ref type="bibr" target="#b10">6)</ref>, the uniform distribution over the integer range (3, 6) (including both end points). We then sample a random number of links m from U(n, n 2 -n), and sample a random directed graph from G(n, m). We make U(0, n) nodes and U(0, m) links into variables, choosing constants for the rest uniformly from the data. If the pattern is disconnected, we reject and sample again.</p><p>We sample a random graph as in the previous experiments, using the dimensions from the three real world datasets used later. We then add k instances of the motif to the graph and compute the log-factor of the sampled pattern (we do not use simulated annealing here).</p><p>We let k range from 0 to 200, and repeat the experiment 25 times for each k, sampling a new graph and pattern each time. The results are shown in Figure <ref type="figure" target="#fig_5">3</ref>. We observe first that under this ad-hoc sampling regime, we produce some patterns that create only very few instances in the graph, after overlapping instances are pruned. Since it is no surprise that these don't allow significant compression, we plot these as small points so that they don't obscure the other points.</p><p>We see that most of the other instances-those that generate enough non-overlapping instances-result in high positive log factors, allowing them to be retrieved as motifs.</p><p>Real data Finally, we will test our method on real data, to confirm that the motifs found coincide with our intuition. We test three datasets: The Semantic Web dogfood dataset <ref type="bibr" target="#b14">[10]</ref> (n = 7611, m = 242256, r = 170) describing researchers and publications in the Semantic Web domain, the AIFB dataset <ref type="bibr" target="#b5">[1]</ref> (n = 8285, m = 29226, r = 47) describing the structure of the AIFB institute, and the Mutag RDF dataset <ref type="foot" target="#foot_3">5</ref> (n = 23644, m = 74567, r = 24), describing a set of carcinogenic and non-carcinogenic molecules both in structure and properties.</p><p>For all datasets, we run 32 parallel searches, with Table <ref type="table">1</ref>: Results of the experiment on real-world data. For each experiment we also report the number of motifs found with a positive log-factor.</p><p>3125 iterations per search. Table <ref type="table">1</ref> reports the top 5 motifs by log-factor, and the top 3 motifs by frequency. We provide the top 100 motifs under both criteria online. 6  The method provides many positives. To see that these are not just random noise, consider those patterns that have high frequency, but a negative logfactor. For instance, the most frequent pattern in the AIFB data describes two entities having the same "year" property. Clearly, such a pattern can be matched often, and in many different ways, but it does not provide a satisfying explanation of the the structure of the graph.</p><p>Much of what the motif code picks up on is redundancy in the original data. For instance, in the AIFB data both the swrs:publication relation and its inverse swrs:author are always included. Extracting these into a motif is simple way of achieving compression. In fact, the AIFB data contains so many of these relation pairs that the two-node loop with variable labels is the highest scoring motif. In the Dogfood data, we see similar patterns emerge.</p><p>Table <ref type="table">2</ref> shows some interesting motifs from the top 100 for each dataset. We see, for instance that the assertions that something is true or false are both motifs. While these are single triples with only one variable, they occur often enough, that encoding them separately provides a positive compression. The example from the AIFB date shows a typical "star" pattern likely to emerge from relational data: a single entity, surrounded by a set of attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>We have presented a new method for mining graph patterns from knowledge graphs. To our knowledge, this is the first method presented that can potentially find arbitrary basic graph patterns to describe the innate structure of a knowledge graph.</p><p>Limitations and future work Currently, the greatest limitation of this method is scalability. We note that this limitation only exists when motifs need to 6   <ref type="table">2</ref>: Selected motifs. The frequency is the number of matches found in the set time limit. The last column indicates the dataset (Dogfood, MUTAG and AIFB, respectively). be found. To test whether a given pattern is a motif, the most expensive step required is simply to find instances of the pattern in the graph (as many as is feasible). However, the search space of all patterns is large and complex, making a search for motifs an expensive task.</p><p>In <ref type="bibr" target="#b7">[3]</ref>, the original method on which this method is based was shown to scale to graphs with billions of links. However, this scalability does not translate directly to knowledge graphs: the random walk sampling used there, to generate likely motifs fails in the face of common knowledge graph topologies with many very strong hubs. In such cases, the subgraphs that have a positive log factor are so unlikely to be sampled, that none are ever put to the test.</p><p>For now, we have resorted to black box optimization for search. If a faster search algorithm can be designed specifically for this code, the problem of scaling may be overcome. One option is to replace the random walk used in <ref type="bibr" target="#b7">[3]</ref> by a biased random walk more suited to the topology of knowledge graphs Our method currently produces a large number of motifs. We can show that worthwhile motifs are included, and that it performs better than a frequency baseline, but it still takes some manual effort to sort through the suggestions to find the kind of motifs that fit a particular use case. This is not surprising; it is the nature of knowledge graphs that many different and overlapping substructures can be seen as natural or meaningful. One promising avenue to reduce this manual effort is to search for a set of motifs which together compress well, each motif claiming a certain part of the knowledge graph to represent.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: An example of the principle behind our motif code. a) A basic knowledge graph. We consider only the integer indices of the nodes and relations. Labels are included only for readability. b) A motif that occurs frequently. c) A compressed representation; we remove all edges that are part of an occurrence of the motif and store separately which nodes match the motif. Together with a the motif, this allows us to reconstruct the data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>the edges of the graph and their labels: each triple (s, p, o) ? E G encodes the subject node s, the object node o and the predicate or relation p of an edge in the graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The result of the random graph experiment. We sort the motifs by their score in the k = 75 experiment and plot their frequency and log-factor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The result of the repeated random graph experiment. Color and size show the number of matches of the pattern after pruning. Plot titles show the graph dimensions before adding instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>https://github.com/MaestroGraph/motive-rdf</figDesc><table><row><cell>log-factor</cell><cell>freq.</cell><cell></cell><cell></cell></row><row><cell cols="2">220360.2 12138</cell><cell>?n1 foaf:maker ?n2. ?n2 foaf:made ?n1.</cell><cell>D</cell></row><row><cell>3157.0</cell><cell>1011</cell><cell>?n1 ?p2 "false".</cell><cell>M</cell></row><row><cell>3150.2</cell><cell>985</cell><cell>?n1 ?p2 "true".</cell><cell>M</cell></row><row><cell></cell><cell></cell><cell>?n1 rdf:type ?n2.</cell><cell></cell></row><row><cell>12871.8</cell><cell>8308</cell><cell>?n1 swrs:year ?n3.</cell><cell></cell></row><row><cell></cell><cell></cell><cell>?n4 swrs:publication ?n1.</cell><cell></cell></row></table><note><p>A Table</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>For practitioners this restriction is not noticeable, as the indices can simply be mapped back to the original strings when the found motifs are presented.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>In this aspect our definition differs from the SPARQL Basic Graph Pattern. Patterns for which this distinction is relevant are rare, and patterns returned by our method are still compatible with SPARQL.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>Or, equivalently, to make p EL a complete distribution on all graphs, we must provide it with a prior on D.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>Originally distributed as an example dataset with the DL-Learner framework<ref type="bibr" target="#b11">[7]</ref>. log-factor frequency Dogfood , top 5 by log-factor (&gt; 100 positive) 361495.0</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements This publication was supported by the <rs type="funder">Amsterdam Academic Alliance Data Science (AAA-DS)</rs> <rs type="grantName">Program Award</rs> to the <rs type="institution">UvA and VU Universities</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_AzByvEg">
					<orgName type="grant-name">Program Award</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m">2 foaf:made ?n1. 244579.5 7674 ?n1 dc</title>
		<imprint/>
	</monogr>
	<note>n1 swrs:author ?n2. 220360.2 12138 ?n1 foaf:maker ?n2. ?n2 foaf:made ?n1. 189627.3 9888 ?n1 foaf:member ?n2. ?n2 swrs:affiliation ?n1. 187972.9 10475 ?n1 dc:creator ?n2. ?n2 foaf:made ?n1</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Dogfood</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>top 3 by frequency -3076.2 134853 ?n1 rdf: 1 ?n2. ?n1 rdf: 2 ?n4. ?n1 rdf: 3 ?n3. -3435.0 116074 ?n1 swc:heldBy ?n3. ?n1 swc:heldBy ?n2. -2379.9 110461 ?n1 rdf:type owl:Thing. ?n2 rdf:type owl:Thing</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>Aifb</surname></persName>
		</author>
		<idno>79234.0 7549</idno>
		<imprint/>
	</monogr>
	<note>4 4154 ?n1 swrs:publication ?n2. ?n2 ?p3 ?n1. 57641.1 3965 ?n1 swrs:publication ?n2. ?n2 swrs:author ?n1. 57603.1 3965 ?n1 swrs:author ?n2. ?n2 ?p3 ?n1. 33168.0 7930 ?n1 swrs:publication ?n2. ?n2 rdf:type ?n3. ?n2 swrs:author ?n1. AIFB, top 3 by frequency -908.2 181246 ?n1 swrs:year ?n3. ?n2 swrs:year ?n3. -1524.3 173059 ?n1 swrs:publication ?n3. ?n1 swrs:publication ?n2. -1667.9 103434 ?n1 swrs:member ?n2. ?n3 ?p5 ?n1. ?n4 swrs:author ?n2</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mutag</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>top 5 by log-factor (87 positive) 178304.4 18634 ?n1 mtg: hasAtom ?n3. ?n1 mtg: hasBond ?n2. ?n2 mtg: inBond ?n3. 97237.8 9189 ?n1 mtg: hasAtom ?n2. ?n2 mtg: charge ?n3. 93819.3 8924 ?n2 rdf:type ?n3. ?n2 mtg: charge ?n1. 90447.5 18634 ?n1 mtg: hasBond ?n2. ?n2 mtg: inBond ?n4. ?n2 mtg: inBond ?n3. 79027.5 8924 ?n1 mtg: hasAtom ?n2. ?n2 rdf:type ?n3</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mutag</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>top 3 by frequency -2040.6 68514 ?n1 rdfs:subClassOf ?n2. ?n3 rdf:type owl:Class. ?n4 rdf:type owl:Class. ?n4 rdfs:subClassOf ?n2. -2077.8 60832 ?n1 ?p5 owl:Class. ?n3 rdfs:subClassOf ?n2. ?n4 ?p5 owl:Class. ?n4 rdfs:subClassOf ?n2. -1532.6 32009 ?n1 mtg: cytogen sce &quot;true&quot;. ?n1 mtg: salmonella ?n3. ?n2 mtg: amesTestPositive ?n3</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Kernel methods for mining instance data in ontologies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bloehdorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sure</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="58" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>De Rooij</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.13163</idno>
		<title level="m">A tutorial on mdl hypothesis testing for graph analysis</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large-scale network motif analysis using compression</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>De Rooij</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10618-020-00691-y</idno>
		<ptr target="https://doi.org/10.1007/s10618-020-00691-y" />
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Discov</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1421" to="1453" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Substructure discovery using minimum description length and background knowledge</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Holder</surname></persName>
		</author>
		<idno>cs.AI/9402102</idno>
		<ptr target="http://arxiv.org/abs/cs.AI/9402102" />
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">Elements of information theory</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The minimum description length principle</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gr?nwald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dl-learner: learning concepts in description logics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="2639" to="2642" />
			<date type="published" when="2009-11">Nov. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Practical graph isomorphism</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Mckay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<pubPlace>US</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Vanderbilt University Tennessee</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Network motifs: simple building blocks of complex networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen-Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Itzkovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kashtan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chklovskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">298</biblScope>
			<biblScope unit="issue">5594</biblScope>
			<biblScope unit="page" from="824" to="827" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Recipes for semantic web dog food-the eswc and iswc metadata projects</title>
		<author>
			<persName><forename type="first">K</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Heath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Handschuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Domingue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="802" to="815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Networks: an introduction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Newman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploiting emergent schemas to make rdf systems more efficient</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Boncz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="463" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deriving an emergent relational schema from rdf data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Passing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Erling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Boncz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="864" to="874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The two-parameter poisson-dirichlet distribution derived from a stable subordinator</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Probability</title>
		<imprint>
			<biblScope unit="page" from="855" to="900" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Are names meaningful? quantifying social meaning on the semantic web</title>
		<author>
			<persName><forename type="first">S</forename><surname>De Rooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Beek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Van Harmelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schlobach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dualiso: An algorithm for subgraph pattern matching on very large labeled graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saltz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ramaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<editor>
			<persName><forename type="first">Big</forename><surname>Data</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="498" to="505" />
			<date type="published" when="2014">2014. 2014</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Statistical schema induction</title>
		<author>
			<persName><forename type="first">J</forename><surname>V?lker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extended Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="124" to="138" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
