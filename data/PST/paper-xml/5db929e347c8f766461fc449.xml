<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Better Exploration with Optimistic Actor-Critic</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kamil</forename><surname>Ciosek</surname></persName>
							<email>kamil.ciosek@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Quan</forename><surname>Vuong</surname></persName>
							<email>qvuong@ucsd.edu</email>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Loftin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
							<email>katja.hofmann@microsoft.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of California San Diego</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<country>Cambridge</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="laboratory">See Appendix C for details. 33rd Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2019)</postCode>
									<settlement>Vancouver</settlement>
									<region>NeurIPS</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Better Exploration with Optimistic Actor-Critic</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">916E4FDDA410C1757DA02BF8702FFFCE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Actor-critic methods, a type of model-free Reinforcement Learning, have been successfully applied to challenging tasks in continuous control, often achieving state-of-the art performance. However, wide-scale adoption of these methods in real-world domains is made difficult by their poor sample efficiency. We address this problem both theoretically and empirically. On the theoretical side, we identify two phenomena preventing efficient exploration in existing state-of-the-art algorithms such as Soft Actor Critic. First, combining a greedy actor update with a pessimistic estimate of the critic leads to the avoidance of actions that the agent does not know about, a phenomenon we call pessimistic underexploration. Second, current algorithms are directionally uninformed, sampling actions with equal probability in opposite directions from the current mean. This is wasteful, since we typically need actions taken along certain directions much more than others. To address both of these phenomena, we introduce a new algorithm, Optimistic Actor Critic, which approximates a lower and upper confidence bound on the state-action value function. This allows us to apply the principle of optimism in the face of uncertainty to perform directed exploration using the upper bound while still using the lower bound to avoid overestimation. We evaluate OAC in several challenging continuous control tasks, achieving state-of the art sample efficiency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A major obstacle that impedes a wider adoption of actor-critic methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b43">44]</ref> for control tasks is their poor sample efficiency. In practice, despite impressive recent advances <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b16">17]</ref>, millions of environment interactions are needed to obtain a reasonably performant policy for control problems with moderate complexity. In systems where obtaining samples is expensive, this often makes the deployment of these algorithms prohibitively costly.</p><p>This paper aims at mitigating this problem by more efficient exploration . We begin by examining the exploration behavior of SAC <ref type="bibr" target="#b23">[24]</ref> and TD3 <ref type="bibr" target="#b16">[17]</ref>, two recent model-free algorithms with stateof-the-art sample efficiency and make two insights. First, in order to avoid overestimation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr">46]</ref>, SAC and TD3 use a critic that computes an approximate lower confidence bound 2 . The actor then adjusts the exploration policy to maximize this lower bound. This improves the stability of the updates and allows the use of larger learning rates. However, using the lower bound can also seriously inhibit exploration if it is far from the true Q-function. If the lower bound has a spurious maximum, the covariance of the policy will decrease, causing pessimistic underexploration, i.e. discouraging the algorithm from sampling actions that would lead to an improvement to the flawed estimate of the critic. Moreover, Gaussian policies are directionally uninformed, sampling actions with equal probability in any two opposing directions from the mean. This is wasteful since some regions in the action space close to the current policy are likely to have already been explored by past policies and do not require more samples.</p><p>We formulate Optimistic Actor-Critic (OAC), an algorithm which explores more efficiently by applying the principle of optimism in the face of uncertainty <ref type="bibr" target="#b8">[9]</ref>. OAC uses an off-policy exploration strategy that is adjusted to maximize an upper confidence bound to the critic, obtained from an epistemic uncertainty estimate on the Q-function computed with the bootstrap <ref type="bibr" target="#b34">[35]</ref>. OAC avoids pessimistic underexploration because it uses an upper bound to determine exploration covariance. Because the exploration policy is not constrained to have the same mean as the target policy, OAC is directionally informed, reducing the waste arising from sampling parts of action space that have already been explored by past policies.</p><p>Off-policy Reinforcement Leaning is known to be prone to instability when combined with function approximation, a phenomenon known as the deadly triad <ref type="bibr" target="#b42">[43,</ref><ref type="bibr">47]</ref>. OAC achieves stability by enforcing a KL constraint between the exploration policy and the target policy. Moreover, similarly to SAC and TD3, OAC mitigates overestimation by updating its target policy using a lower confidence bound of the critic <ref type="bibr" target="#b25">[26,</ref><ref type="bibr">46]</ref>. Empirically, we evaluate Optimistic Actor Critic in several challenging continuous control tasks and achieve state-of-the-art sample efficiency on the Humanoid benchmark. We perform ablations and isolate the effect of bootstrapped uncertainty estimates on performance. Moreover, we perform hyperparameter ablations and demonstrate that OAC is stable in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Reinforcement learning (RL) aims to learn optimal behavior policies for an agent acting in an environment with a scalar reward signal. Formally, we consider a Markov decision process <ref type="bibr" target="#b38">[39]</ref>, defined as a tuple (S, A, R, p, p 0 , γ). An agent observes an environmental state s ∈ S = R n ; takes a sequence of actions a 1 , a 2 , ..., where a t ∈ A ⊆ R d ; transitions to the next state s ′ ∼ p(•|s, a) under the state transition distribution p(s ′ |s, a); and receives a scalar reward r ∈ R. The agent's initial state s 0 is distributed as s 0 ∼ p 0 (•).</p><p>A policy π can be used to generate actions a ∼ π(•|s). Using the policy to sequentially generate actions allows us to obtain a trajectory through the environment τ = (s 0 , a 0 , r 0 , s 1 , a 1 , r 1 , ...). For any given policy, we define the action-value function as Q π (s, a) = E τ :s0=s,a0=a [ t γ t r t ], where γ ∈ [0, 1) is a discount factor. We assume that Q π (s, a) is differentiable with respect to the action. The objective of Reinforcement Learning is to find a deployment policy π eval which maximizes the total return J = E τ :s0∼p0 [ t γ t r t ]. In order to provide regularization and aid exploration, most actor-critic algorithms <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31]</ref> do not adjust π eval directly. Instead, they use a target policy π T , trained to have high entropy in addition to maximizing the expected return J. <ref type="foot" target="#foot_0">3</ref> The deployment policy π eval is typically deterministic and set to the mean of the stochastic target policy π T .</p><p>Actor-critic methods <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b6">7]</ref> seek a locally optimal target policy π T by maintaining a critic, learned using a value-based method, and an actor, adjusted using a policy gradient update. The critic is learned with a variant of SARSA <ref type="bibr">[48,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b40">41]</ref>. In order to limit overestimation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr">46]</ref>, modern actor-critic methods learn an approximate lower confidence bound on the Q-function <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b16">17]</ref>, obtained by using two networks Q1 LB and Q2 LB , which have identical structure, but are initialized with different weights. In order to avoid cumbersome terminology, we refer to QLB simply as a lower bound in the remainder of the paper. Another set of target networks <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b30">31]</ref>    Meanwhile, the actor adjusts the policy parameter vector θ of the policy π T in order to maximize J by following its gradient. The gradient can be written in several forms <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. Recent actor-critic methods use a reparametrised policy gradient <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. We denote a random variable sampled from a standard multivariate Gaussian as ε ∼ N (0, I) and denote the standard normal density as φ(ε). The re-parametrisation function f is defined such that the probability density of the random variable f θ (s, ε) is the same as the density of π T (a|s), where ε ∼ N (0, I). The gradient of the return can then be written as:</p><formula xml:id="formula_0">∇ θ J = s ρ(s) ε ∇ θ QLB (s, f θ (s, ε))φ(ε)dεds<label>(3)</label></formula><p>where ρ(s) ∞ t=0 γ t p(s t = s|s 0 ) is the discounted-ergodic occupancy measure. In order to provide regularization and encourage exploration, it is common to use a gradient ∇ θ J α that adds an additional entropy term ∇ θ H(π(•, s)).</p><formula xml:id="formula_1">∇ θ J α QLB = s ρ(s) ε ∇ θ QLB (s, f θ (s, ε))φ(ε)dε + α ε -∇ θ log f θ (s, ε)φ(ε)dε ∇ θ H(π(•,s)) ds<label>(4)</label></formula><p>During training, ( <ref type="formula" target="#formula_1">4</ref>) is approximated with samples by replacing integration over ε with Monte-Carlo estimates and integration over the state space with a sum along the trajectory.</p><formula xml:id="formula_2">∇ θ J α QLB ≈ ∇ θ Ĵα QLB = N t=0 γ t ∇ θ QLB (s t , f θ (s, ε t )) + α -∇ θ log f θ (s t , ε t ).<label>(5)</label></formula><p>In the standard set-up, actions used in (1) and ( <ref type="formula" target="#formula_2">5</ref>) are generated using π T . In the table-lookup case, the update can be reliably applied off-policy, using an action generated with a separate exploration policy π E . In the function approximation setting, this leads to updates that can be biased because of the changes to ρ(s). In this work, we address these issues by imposing a KL constraint between the exploration policy and the target policy. We give a more detailed account of addressing the associated stability issues in section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Existing Exploration Strategy is Inefficient</head><p>As mentioned earlier, modern actor-critic methods such as SAC <ref type="bibr" target="#b23">[24]</ref> and TD3 <ref type="bibr" target="#b16">[17]</ref> explore in an inefficient way. We now give more details about the phenomena that lead to this inefficiency.</p><p>Pessimistic underexploration. In order to improve sample efficiency by preventing the catastrophic overestimation of the critic <ref type="bibr" target="#b25">[26,</ref><ref type="bibr">46]</ref>, SAC and TD3 <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24]</ref> use a lower bound approximation to the critic, similar to (1). However, relying on this lower bound for exploration is inefficient. By greedily maximizing the lower bound, the policy becomes very concentrated near a maximum. When the critic is inaccurate and the maximum is spurious, this can be very harmful. This is illustrated in Figure <ref type="figure" target="#fig_1">1a</ref>. At first, the agent explores with a broad policy, denoted π past . Since QLB increases to the left, the policy gradually moves in that direction, becoming π current . Because QLB (shown in red)</p><p>has a maximum at the mean µ of π current , the policy π current has a small standard deviation. This is suboptimal since we need to sample actions far away from the mean to find out that the true critic Q π does not have a maximum at µ. We include evidence that this problem actually happens in MuJoCo Ant in Appendix F.</p><p>The phenomenon of underexploration is specific to the lower as opposed to an upper bound. An upper bound which is too large in certain areas of the action space encourages the agent to explore them and correct the critic, akin to optimistic initialization in the tabular setting <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>. We include more intuition about the difference between the upper and lower bound in Appendix I. Due to overestimation, we cannot address pessimistic underexploration by simply using the upper bound in the actor <ref type="bibr" target="#b16">[17]</ref>. Instead, recent algorithms have used an entropy term (4) in the actor update. While this helps exploration somewhat by preventing the covariance from collapsing to zero, it does not address the core issue that we need to explore more around a spurious maximum. We propose a more effective solution in section 4.</p><p>Directional uninformedness. Actor-critic algorithms that use Gaussian policies, like SAC <ref type="bibr" target="#b24">[25]</ref> and TD3 <ref type="bibr" target="#b16">[17]</ref>, sample actions in opposite directions from the mean with equal probability. However, in a policy gradient algorithm, the current policy will have been obtained by incremental updates, which means that it won't be very different from recent past policies. Therefore, exploration in both directions is wasteful, since the parts of the action space where past policies had high density are likely to have already been explored. This phenomenon is shown in Figure <ref type="figure" target="#fig_1">1b</ref>. Since the policy π current is Gaussian and symmetric around the mean, it is equally likely to sample actions to the left and to the right. However, while sampling to the left would be useful for learning an improved critic, sampling to the right is wasteful, since the critic estimate in that part of the action space is already good enough. In section 4, we address this issue by using an exploration policy shifted relative to the target policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Better Exploration with Optimism</head><p>Optimistic Actor Critic (OAC) is based on the principle of optimism in the face of uncertainty <ref type="bibr" target="#b49">[50]</ref>.</p><p>Inspired by recent theoretical results about efficient exploration in model-free RL <ref type="bibr" target="#b27">[28]</ref>, OAC obtains an exploration policy π E which locally maximizes an approximate upper confidence bound of Q π each time the agent enters a new state. The policy π E is separate from the target policy π T learned using <ref type="bibr" target="#b4">(5)</ref> and is used only to sample actions in the environment. Formally, the exploration policy π E = N (µ E , Σ E ), is defined as</p><formula xml:id="formula_3">µ e , Σ E = arg max µ,Σ: KL(N (µ,Σ),N (µ T ,Σ T ))≤δ E a∼N (µ,Σ) QUB (s, a) .<label>(6)</label></formula><p>Below, we derive the OAC algorithm formally. We begin by obtaining the upper bound QUB (s, a) (section 4.1). We then motivate the optimization problem <ref type="bibr" target="#b5">(6)</ref>, in particular the use of the KL constraint (section 4.2). Finally, in section 4.3, we describe the OAC algorithm and outline how it mitigates pessimistic underexploration and directional uninformedness while still maintaining the stability of learning. In Section 4.4, we compare OAC to related work. In Appendix B, we derive an alternative variant of OAC that works with deterministic policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Obtaining an Upper Bound</head><p>The approximate upper confidence bound QUB used by OAC is derived in three stages. First, we obtain an epistemic uncertainty estimate σ Q about the true state-action value function Q. We then use it to define an upper bound QUB . Finally, we introduce its linear approximation QUB , which allows us to obtain a tractable algorithm.</p><p>Epistemic uncertainty For computational efficiency, we use a Gaussian distribution to model epistemic uncertainty. We fit mean and standard deviation based on bootstraps <ref type="bibr" target="#b15">[16]</ref> of the critic. The mean belief is defined as</p><formula xml:id="formula_4">µ Q (s, a) = 1 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q1</head><p>LB (s, a) + Q2 LB (s, a) , while the standard deviation is</p><formula xml:id="formula_5">σ Q (s, a) = i∈{1,2}<label>1</label></formula><formula xml:id="formula_6">2 Qi LB (s, a) -µ Q (s, a) 2 = 1 2 Q1 LB (s, a) -Q2 LB (s, a) .<label>(7)</label></formula><p>Here, the second equality is derived in appendix C. The bootstraps are obtained using <ref type="bibr" target="#b0">(1)</ref>. Since existing algorithms <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b16">17]</ref> already maintain two bootstraps, we can obtain µ Q and σ Q at negligible computational cost. Despite the fact that (1) uses the same target value for both bootstraps, we demonstrate in Section 5 that using a two-network bootstrap leads to a large performance improvement in practice. Moreover, OAC can be easily extended to to use more expensive and better uncertainty estimates if required.</p><p>Upper bound. Using the uncertainty estimate <ref type="bibr" target="#b23">(24)</ref>, we define the upper bound as QUB (s, a) = µ Q (s, a) + β UB σ Q (s, a). We use the parameter β UB ∈ R + to fix the level of optimism. In order to obtain a tractable algorithm, we approximate QUB with a linear function QUB .</p><p>QUB (s, a) = a ⊤ ∇ a QUB (s, a)</p><formula xml:id="formula_7">a=µ T + const<label>(8)</label></formula><p>By Taylor's theorem, QUB (s, a) is the best possible linear fit to QUB (s, a) in a sufficiently small region near the current policy mean µ T for any fixed state s [10, <ref type="bibr">Theorem 3.22]</ref>. Since the gradient ∇ a QUB (s, a)</p><p>a=µ T is computationally similar to the lower-bound gradients in <ref type="bibr" target="#b4">(5)</ref>, our upper bound estimate can be easily obtained in practice without additional tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Optimistic Exploration</head><p>Our exploration policy π E , introduced in ( <ref type="formula" target="#formula_3">6</ref>), trades off between two criteria: the maximization of an upper bound QUB (s, a), defined in <ref type="bibr" target="#b7">(8)</ref>, which increases our chances of executing informative actions, according to the principle of optimism in the face of uncertainty <ref type="bibr" target="#b8">[9]</ref>, and constraining the maximum KL divergence between the exploration policy and the target policy π T , which ensures the stability of updates. The KL constraint in ( <ref type="formula" target="#formula_3">6</ref>) is crucial for two reasons. First, it guarantees that the exploration policy π E is not very different from the target policy π T . This allows us to preserve the stability of optimization and makes it less likely that we take catastrophically bad actions, ending the episode and preventing further learning. Second, it makes sure that the exploration policy remains within the action range where the approximate upper bound QUB is accurate. We chose the KL divergence over other similarity measures for probability distributions since it leads to tractable updates.</p><p>Thanks to the linear form on QUB and because both π E and π T are Gaussian, the maximization of ( <ref type="formula" target="#formula_3">6</ref>) can be solved in closed form. We state the solution below. Proposition 1. The exploration policy resulting from (6) has the form π E = N (µ E , Σ E ), where</p><formula xml:id="formula_8">µ E = µ T + √ 2δ [∇a QUB(s,a)] a=µ T Σ Σ T [∇a QUB(s,a)] a=µ T and Σ E = Σ T .<label>(9)</label></formula><p>We stress that the covariance of the exploration policy is the same as the target policy. The proof is deferred to Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The Optimistic Actor-Critic Algorithm</head><p>Optimistic Actor Critic (see Algorithm 1) samples actions using the exploration policy (9) in line 4 and stores it in a memory buffer. The term ∇ a QUB (s, a)</p><p>a=µ T in ( <ref type="formula" target="#formula_8">9</ref>) is computed at minimal cost <ref type="foot" target="#foot_1">4</ref> using automatic differentiation, analogous to the critic derivative in the actor update (4). OAC then uses its memory buffer to train the critic (line 10) and the actor (line 12). We also introduced a modification of the lower bound used in the actor, using OAC avoids the pitfalls of greedy exploration Figure <ref type="figure">2</ref> illustrates OAC's exploration policy π E visually. Since the policy π E is far from the spurious maximum of QLB (red line in figure <ref type="figure">2</ref>), executing actions sampled from π E leads to a quick correction to the critic estimate. This way, OAC avoids pessimistic underexploration. Since π E is not symmetric with respect to the mean of π T (dashed line), OAC also avoids directional uninformedness.</p><formula xml:id="formula_9">Q′ LB = µ Q (s, a) + β LB σ Q (s,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Optimistic Actor-Critic (OAC).</head><p>Require: w 1 , w 2 , θ ⊲ Initial parameters w 1 , w 2 of the critic and θ of the target policy π T . 1: w1 ← w 1 , w2 ← w 2 , D ← ∅ ⊲ Initialize target network weights and replay pool 2: for each iteration do 3:</p><p>for each environment step do 4:</p><formula xml:id="formula_10">a t ∼ π E (a t |s t )</formula><p>⊲ Sample action from exploration policy as in ( <ref type="formula" target="#formula_8">9</ref>). </p><formula xml:id="formula_11">w1 ← τ w 1 + (1 -τ ) w1 , w2 ← τ w 2 + (1 -τ ) w2 ⊲ Update target networks 14:</formula><p>end for 15: end for Output:</p><formula xml:id="formula_12">w 1 , w 2 , θ ⊲ Optimized parameters a Q π , π<label>(a) QLB (s, a) Q π (s, a) QUB (s, a)</label></formula><p>π T π E samples needed less samples needed more Figure <ref type="figure">2:</ref> The OAC exploration policy π E avoids pessimistic underexploration by sampling far from the spurious maximum of the lower bound QLB . Since π E is not symmetric wrt. the mean of the target policy (dashed line), it also addresses directional uninformedness.</p><p>Stability While off-policy deep Reinforcement Learning is difficult to stabilize in general <ref type="bibr" target="#b42">[43,</ref><ref type="bibr">47]</ref>, OAC is remarkably stable. Due to the KL constraint in equation ( <ref type="formula" target="#formula_3">6</ref>), the exploration policy π E remains close to the target policy π T . In fact, despite using a separate exploration policy, OAC isn't very different in this respect from SAC <ref type="bibr" target="#b23">[24]</ref> or TD3 <ref type="bibr" target="#b16">[17]</ref>, which explore with a stochastic policy but use a deterministic policy for evaluation. In Section 5, we demonstrate empirically that OAC and SAC are equally stable in practice. Moreover, similarly to other recent state-of-the-art actor-critic algorithms <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25]</ref>, we use target networks <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b30">31]</ref> to stabilize learning. We provide the details in Appendix D.</p><p>Overestimation vs Optimism While OAC is an optimistic algorithm, it does not exhibit catastrophic overestimation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr">46]</ref>. OAC uses the optimistic estimate (8) for exploration only. The policy π E is computed from scratch (line 4 in Algorithm 1) every time the algorithm takes an action and is used only for exploration. The critic and actor updates (1) and ( <ref type="formula" target="#formula_2">5</ref>) are still performed with a lower bound. This means that there is no way the upper bound can influence the critic except indirectly through the distribution of state-action pairs in the memory buffer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Related work</head><p>OAC is distinct from other methods that maintain uncertainty estimates over the state-action value function. Actor-Expert <ref type="bibr" target="#b31">[32]</ref> uses a point estimate of Q ⋆ , unlike OAC, which uses a bootstrap approximating Q π . Bayesian actor-critic methods <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref> model the probability distribution over Q π , but unlike OAC, do not use it for exploration. Approaches combining DQN with bootstrap <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b35">36]</ref> and the uncertainty Bellman equation <ref type="bibr" target="#b33">[34]</ref> are designed for discrete actions. Model-based reinforcement learning methods thet involve uncertainty <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b11">12]</ref> are very computationally expensive due to the need of learning a distribution over environment models. OAC may seem superficially similar to natural actor critic <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref> due to the KL constraint in <ref type="bibr" target="#b5">(6)</ref>. In fact, it is very different. While natural actor critic uses KL to enforce the similarity between infinitesimally small updates to the target policy, OAC constrains the exploration policy to be within a non-trivial distance of the target policy. Other approaches that define the exploration policy as a solution to a KL-constrained optimization problem include MOTO <ref type="bibr" target="#b1">[2]</ref>, MORE <ref type="bibr" target="#b3">[4]</ref> and Maximum a Posteriori Policy optimization <ref type="bibr" target="#b2">[3]</ref>. These methods differ from OAC in that they do not use epistemic uncertainty estimates and explore by enforcing entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Our experiments have three main goals. First, to test whether Optimistic Actor Critic has performance competitive to state-of-the art algorithms. Second, to assess whether optimistic exploration based on the bootstrapped uncertainty estimate <ref type="bibr" target="#b23">(24)</ref>, is sufficient to produce a performance improvement. Third, to assess whether optimistic exploration adversely affects the stability of the learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MuJoCo Continuous Control</head><p>We test OAC on the MuJoCo <ref type="bibr" target="#b44">[45]</ref> continuous control benchmarks. We compare OAC to SAC <ref type="bibr" target="#b24">[25]</ref> and TD3 <ref type="bibr" target="#b16">[17]</ref>, two recent model-free RL methods that achieve state-of-the art performance. For completeness, we also include a comparison to a tuned version of DDPG <ref type="bibr" target="#b30">[31]</ref>, an established algorithm that does not maintain multiple bootstraps of the critic network. OAC uses 3 hyper-parameters related to exploration. The parameters β UB and β LB control the amount of uncertainty used to compute the upper and lower bound respectively. The parameter δ controls the maximal allowed divergence between the exploration policy and the target policy. We provide the values of all hyper-parameters and details of the hyper-parameter tuning in Appendix D. Results in Figure <ref type="figure" target="#fig_3">3</ref> show that using optimism improves the overall performance of actor-critic methods. On Ant, OAC improves the performance somewhat. On Hopper, OAC achieves state-of the art final performance. On Walker, we achieve the same performance as SAC while the high variance of results on HalfCheetah makes it difficult to draw conclusions on which algorithm performs better. <ref type="foot" target="#foot_2">5</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>State-of-the art result on Humanoid</head><p>The upper-right plot of Figure <ref type="figure" target="#fig_3">3</ref> shows that the vanilla version of OAC outperforms SAC the on the Humanoid task. To test the statistical significance of our result, we re-ran both SAC and OAC in a setting where 4 training steps per iteration are used. By exploiting the memory buffer more fully, the 4-step versions show the benefit of improved exploration   <ref type="figure" target="#fig_3">3</ref>. At the end of training, the 90% confidence interval <ref type="foot" target="#foot_3">6</ref> for the performance of OAC was 5033 ± 147 while the performance of SAC was 4586 ± 117. We stress that we did not tune hyper-parameters on the Humanoid environment. Overall, the fact that we are able to improve on Soft-Actor-Critic, which is currently the most sample-efficient model-free RL algorithm for continuous tasks shows that optimism can be leveraged to benefit sample efficiency. We provide an explicit plot of sample-efficiency in Appendix J.</p><p>Usefulness of the Bootstrapped Uncertainty Estimate OAC uses an epistemic uncertainty estimate obtained using two bootstraps of the critic network. To investigate its benefit, we compare the performance of OAC to a modified version of the algorithm, which adjusts the exploration policy to maximize the approximate lower bound, replacing QUB with QLB in equation <ref type="bibr" target="#b8">(9)</ref>. While the modified algorithm does not use the uncertainty estimate, it still uses a shifted exploration policy, preferring actions that achieve higher state-action values. The results is shown in Figure <ref type="figure" target="#fig_4">4</ref> (we include more plots in Figure <ref type="figure">8</ref> in the Appendix). Using the bootstrapped uncertainty estimate improves performance on the most challenging Humanoid domain, while producing either a slight improvement or a no change in performance on others domains. Since the upper bound is computationally very cheap to obtain, we conclude that it is worthwhile to use it.</p><p>Sensitivity to the KL constraint OAC relies on the hyperparameter δ, which controls the maximum allowed KL divergence between the exploration policy and the target policy. In Figure <ref type="figure" target="#fig_5">5</ref>, we evaluate how the term √ 2δ used in the the exploration policy (9) affects average performance of OAC trained for 1 million environment steps on the Ant-v2 domain. The results demonstrate that there is a broad range of settings for the hyperparameter δ, which leads to good performance.</p><p>Learning is Stable in Practice Since OAC explores with a shifted policy, it might at first be expected of having poorer learning stability relative to algorithms that use the target policy for exploration. While we have already shown above that the performance difference between OAC and SAC is statistically significant and not due to increased variance across runs, we now investigate stability further. In Figure <ref type="figure" target="#fig_5">5</ref> we compare individual learning runs across both algorithms. We conclude that OAC and SAC are similarly stable, avoiding the problems associated with stabilising deep off-policy RL <ref type="bibr" target="#b42">[43,</ref><ref type="bibr">47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We present Optimistic Actor Critic (OAC), a model-free deep reinforcement learning algorithm which explores by maximizing an approximate confidence bound on the state-action value function. By addressing the inefficiencies of pessimistic underexploration and directional uninformedness, we are able to achieve state-of-the art sample efficiency in continuous control tasks. Our results suggest that the principle of optimism in the face of uncertainty can be used to improve the sample efficiency of policy gradient algorithms in a way which carries almost no additional computational overhead.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Exploration inefficiencies in actor-critic methods. The state s is fixed. The graph shows Q π , which is unknown to the algorithm, its known lower bound QLB (in red) and two policies π current and π past at different time-steps of the algorithm (in blue).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>a), allowing us to use more conservative policy updates. The critic (1) is recovered by setting β LB = -1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: OAC versus SAC, TD3, DDPG on 5 Mujoco environments. The horizontal axis indicates number of environment steps. The vertical axis indicates the total undiscounted return. The shaded areas denote one standard deviation.</figDesc><graphic coords="7,121.16,159.50,122.40,86.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Impact of the bootstrapped uncertainty estimate on the performance of OAC.</figDesc><graphic coords="8,183.60,72.00,122.40,86.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Left figure: individual runs of OAC vs SAC. Right figure: sensitivity to the KL constraint δ. Error bars indicate 90% confidence interval.</figDesc><graphic coords="8,309.74,189.99,134.64,97.49" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>Policy improvement results can still be obtained with the entropy term present, in a certain idealized setting<ref type="bibr" target="#b23">[24]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>In practice, the per-iteration wall clock time it takes to run OAC is the same as SAC.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>Because of this high variance, we measured a lower mean performance of SAC in Figure3than previously reported. We provide details in Appendix E.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>Due to computational constraints, we used a slightly different target update rate for OAC. We describe the details in Appendix D.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">Gordon</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">A</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernanda</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
		<idno>CoRR, abs/1603.04467</idno>
		<ptr target="http://arxiv.org/abs/1603.04467" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Model-Based Relative Entropy Stochastic Search</title>
		<author>
			<persName><forename type="first">Abbas</forename><surname>Abdolmaleki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rudolf</forename><surname>Lioutikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">R</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuno</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><forename type="middle">Pualo</forename><surname>Reis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3537" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Maximum a Posteriori Policy Optimisation</title>
		<author>
			<persName><forename type="first">Abbas</forename><surname>Abdolmaleki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30">2018. April 30 -May 3, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Model-Free Trajectory Optimization for Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Riad</forename><surname>Akrour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hany</forename><surname>Abdulsamad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abbas</forename><surname>Abdolmaleki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning, ICML 2016</title>
		<editor>
			<persName><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting>the 33nd International Conference on Machine Learning, ICML 2016<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">June 19-24, 2016. 2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2961" to="2970" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural Gradient Works Efficiently in Learning</title>
		<author>
			<persName><forename type="first">Shun-Ichi</forename><surname>Amari</surname></persName>
		</author>
		<idno type="DOI">10.1162/089976698300017746</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="276" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Direct gradient-based reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCAS.2000.856049</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Circuits and Systems, ISCAS 2000, Emerging Technologies for the 21st Century</title>
		<meeting><address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000-05">May 2000. 2000</date>
			<biblScope unit="page" from="271" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Infinite-Horizon Policy-Gradient Estimation</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<idno type="DOI">10.1613/jair.806</idno>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="319" to="350" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Experiments with Infinite-Horizon, Policy-Gradient Estimation</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lex</forename><surname>Weaver</surname></persName>
		</author>
		<idno type="DOI">10.1613/jair.807</idno>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="351" to="381" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">R-max-a general polynomial time algorithm for near-optimal reinforcement learning</title>
		<author>
			<persName><forename type="first">Ronen</forename><forename type="middle">I</forename><surname>Brafman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moshe</forename><surname>Tennenholtz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="213" to="231" />
			<date type="published" when="2002-10">Oct. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Advanced Calculus: A Geometric View</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">J</forename><surname>Callahan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010-09">September 2010</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Ucb exploration via q-ensembles</title>
		<author>
			<persName><forename type="first">Szymon</forename><surname>Richard Y Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01502</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning in a handful of trials using probabilistic dynamics models</title>
		<author>
			<persName><forename type="first">Kurtland</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Calandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7725-deep-reinforcement-learning-in-a-handful-of-trials-using-probabilistic-dynamics-models" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-08">2018. 2018, 3-8 December 2018. 2018</date>
			<biblScope unit="page" from="4759" to="4770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Expected Policy Gradients</title>
		<author>
			<persName><forename type="first">Kamil</forename><surname>Ciosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th Innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<editor>
			<persName><forename type="first">Sheila</forename><forename type="middle">A</forename><surname>Mcilraith</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th Innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018">February 2-7, 2018. 2018</date>
			<biblScope unit="page" from="2868" to="2875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Expected Policy Gradients for Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Kamil</forename><surname>Ciosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
		</author>
		<idno>CoRR, abs/1801.03326</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning and policy search in stochastic dynamical systems with bayesian neural networks</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Depeweg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Miguel Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Udluft</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07127</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An Introduction to the Bootstrap</title>
		<author>
			<persName><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<idno type="DOI">10.1137/1036171</idno>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="677" to="678" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Addressing Function Approximation Error in Actor-Critic Methods</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herke</forename><surname>Hoof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Meger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1582" to="1591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving pilco with bayesian neural network dynamics models</title>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><forename type="middle">Edward</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data-Efficient Machine Learning workshop, ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bayesian actor-critic algorithms</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaakov</forename><surname>Engel</surname></persName>
		</author>
		<idno type="DOI">10.1145/1273496.1273534</idno>
		<ptr target="https://doi.org/10.1145/1273496.1273534" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the Twenty-Fourth International Conference (ICML 2007)</title>
		<meeting><address><addrLine>Corvallis, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">June 20-24, 2007. 2007</date>
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bayesian reinforcement learning: A survey</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviv</forename><surname>Tamar</surname></persName>
		</author>
		<idno type="DOI">10.1561/2200000049</idno>
		<ptr target="https://doi.org/10.1561/2200000049" />
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="359" to="483" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bayesian policy gradient and actor-critic algorithms</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaakov</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v17/10-245.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="66" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Samy</forename><surname>Ulrike Von Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hanna</forename><forename type="middle">M</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rob</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-09">2017, 4-9 December 2017. 2017</date>
			<biblScope unit="page" from="3849" to="3858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic</title>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</title>
		<author>
			<persName><forename type="first">Tuomas</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurick</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1856" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Soft Actor-Critic Algorithms and Applications</title>
		<author>
			<persName><forename type="first">Tuomas</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurick</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Hartikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sehoon</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikash</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno>CoRR, abs/1812.05905</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Double Q-learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Hado</surname></persName>
		</author>
		<author>
			<persName><surname>Hasselt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2613" to="2621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning Continuous Control Policies by Stochastic Value Gradients</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Daniel</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07">2015. December 7-12, 2015. 2015</date>
			<biblScope unit="page" from="2944" to="2952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Is Q-Learning Provably Efficient?</title>
		<author>
			<persName><forename type="first">Chi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Nicolò</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-08">2018. 2018, 3-8 December 2018. 2018</date>
			<biblScope unit="page" from="4868" to="4878" />
		</imprint>
	</monogr>
	<note>Samy Bengio</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Natural Policy Gradient</title>
		<author>
			<persName><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 14 [Neural Information Processing Systems: Natural and Synthetic</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Thomas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Suzanna</forename><surname>Dietterich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zoubin</forename><surname>Becker</surname></persName>
		</editor>
		<editor>
			<persName><surname>Ghahramani</surname></persName>
		</editor>
		<meeting><address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001-12-03">2001. December 3-8, 2001. 2001</date>
			<biblScope unit="page" from="1531" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">May 7-9, 2015. 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Continuous control with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02">2016. May 2-4, 2016. 2016</date>
		</imprint>
	</monogr>
	<note>Yoshua Bengio and Yann LeCun</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Actor-expert: A framework for using action-value methods in continuous action spaces</title>
		<author>
			<persName><forename type="first">Sungsu</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajin</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangchen</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>White</surname></persName>
		</author>
		<idno>CoRR, abs/1810.09103</idno>
		<ptr target="http://arxiv.org/abs/1810.09103" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stig</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14236</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The uncertainty bellman equation and exploration</title>
		<author>
			<persName><forename type="first">O'</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Donoghue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><surname>Mnih</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/o-donoghue18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018</title>
		<meeting>the 35th International Conference on Machine Learning, ICML 2018<address><addrLine>Stockholmsmässan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">July 10-15, 2018. 2018</date>
			<biblScope unit="page" from="3836" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep Exploration via Bootstrapped DQN</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Daniel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Masashi</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Isabelle</forename><surname>Ulrike Von Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05">2016. December 5-10, 2016. 2016</date>
			<biblScope unit="page" from="4026" to="4034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Randomized prior functions for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albin</forename><surname>Cassirer</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/8080-randomized-prior-functions-for-deep-reinforcement-learning" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-08">2018. 2018, 3-8 December 2018. 2018</date>
			<biblScope unit="page" from="8626" to="8638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Policy Gradient Methods for Robotics</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Schaal</surname></persName>
		</author>
		<idno type="DOI">10.1109/IROS.2006.282564</idno>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2006</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">October 9-15, 2006. 2006</date>
			<biblScope unit="page" from="2219" to="2225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Natural Actor-Critic</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Schaal</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2007.11.026</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">7-9</biblScope>
			<biblScope unit="page" from="1180" to="1190" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">L</forename><surname>Puterman</surname></persName>
		</author>
		<title level="m">Markov Decision Processes: Discrete Stochastic Dynamic Programming</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deterministic Policy Gradient Algorithms</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning, ICML 2014</title>
		<meeting>the 31th International Conference on Machine Learning, ICML 2014<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">June 2014. 2014</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="387" to="395" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mozer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Hasselmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 8, NIPS</title>
		<meeting><address><addrLine>Denver, CO, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1995">November 27-30, 1995. 1995</date>
			<biblScope unit="page" from="1038" to="1044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding</title>
		<author>
			<persName><surname>Richard S Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Mozer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Hasselmo</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1038" to="1044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Policy Gradient Methods for Reinforcement Learning with Function Approximation</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Satinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yishay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Sara</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Todd</forename><forename type="middle">K</forename><surname>Leen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</editor>
		<meeting><address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1999-12-04">November 29 -December 4, 1999. 1999</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1057" to="1063" />
		</imprint>
	</monogr>
	<note>NIPS Conference</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">MuJoCo: A physics engine for model-based control</title>
		<author>
			<persName><forename type="first">Emanuel</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="5026" to="5033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep Reinforcement Learning with Double Q-Learning</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><forename type="middle">P</forename><surname>Wellman</surname></persName>
		</editor>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016">February 12-17, 2016. 2016</date>
			<biblScope unit="page" from="2094" to="2100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Deep Reinforcement Learning and the Deadly Triad</title>
		<author>
			<persName><forename type="first">Yotam</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Sonnerat</surname></persName>
		</author>
		<author>
			<persName><surname>Modayil</surname></persName>
		</author>
		<idno>CoRR, abs/1812.02648</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A theoretical and empirical analysis of Expected Sarsa</title>
		<author>
			<persName><surname>Van Seijen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimon</forename><surname>Hado Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><forename type="middle">A</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName><surname>Wiering</surname></persName>
		</author>
		<idno type="DOI">10.1109/ADPRL.2009.4927542</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning</title>
		<meeting><address><addrLine>Nashville, TN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009-03-31">2009. March 31 -April 1, 2009. 2009</date>
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF00992696</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy</title>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<pubPlace>Pittsburgh, PA, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
