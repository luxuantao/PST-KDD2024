<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DropMessage: Unifying Random Dropping for Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Taoran</forename><surname>Fang</surname></persName>
							<email>fangtr@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiqing</forename><surname>Xiao</surname></persName>
							<email>zhiqing.xiao@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chunping</forename><surname>Wang</surname></persName>
							<email>wangchunping02@xinye.com</email>
							<affiliation key="aff1">
								<orgName type="department">FinVolution Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiarong</forename><surname>Xu</surname></persName>
							<email>jiarongxu@fudan.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuan</forename><surname>Yang</surname></persName>
							<email>yangya@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DropMessage: Unifying Random Dropping for Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) are powerful tools for graph representation learning. Despite their rapid development, GNNs also faces some challenges, such as over-fitting, over-smoothing, and non-robustness. Previous works indicate that these problems can be alleviated by random dropping methods, which integrate noises into models by randomly masking parts of the input. However, some open-ended problems of random dropping on GNNs remain to solve. First, it is challenging to find a universal method that are suitable for all cases considering the divergence of different datasets and models. Second, random noises introduced to GNNs cause the incomplete coverage of parameters and unstable training process. In this paper, we propose a novel random dropping method called DropMessage, which performs dropping operations directly on the message matrix and can be applied to any message-passing GNNs. Furthermore, we elaborate the superiority of DropMessage: it stabilizes the training process by reducing sample variance; it keeps information diversity from the perspective of information theory, which makes it a theoretical upper bound of other methods. Also, we unify existing random dropping methods into our framework and analyze their effects on GNNs. To evaluate our proposed method, we conduct experiments that aims for multiple tasks on five public datasets and two industrial datasets with various backbone models. The experimental results show that DropMessage has both advantages of effectiveness and generalization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graphs, which exist ubiquitously in the real world, are used to present complex relationships among various objects in numerous domains such as social media (social networks), finance (trading networks), and biology (biological networks). As powerful tools for representation learning on graphs, graph neural networks (GNNs) have attracted considerable attention recently <ref type="bibr" target="#b6">Bruna et al. [2013]</ref>; <ref type="bibr" target="#b12">Defferrard et al. [2016]</ref>; <ref type="bibr" target="#b13">Ding et al. [2018]</ref>; <ref type="bibr" target="#b26">Kipf and Welling [2017]</ref>; <ref type="bibr" target="#b29">Li et al. [2015]</ref>; <ref type="bibr" target="#b53">Velickovic et al. [2018]</ref>. In particular, GNNs adopt a message-passing schema <ref type="bibr" target="#b16">Gilmer et al. [2017]</ref>, in which each node aggregates information from its neighbors in each convolutional layer, and have been widely applied in various downstream tasks such as node classification <ref type="bibr" target="#b26">Kipf and Welling [2017]</ref>, link prediction Kipf and Welling <ref type="bibr">[2016]</ref>, vertex clustering <ref type="bibr" target="#b45">Ramaswamy et al. [2005]</ref>, and recommendation systems <ref type="bibr" target="#b64">Ying et al. [2018]</ref>.</p><p>Yet, despite their rapid development, training GNNs on large-scale graphs is facing several serious issues such as over-fitting, over-smoothing, and non-robustness. Indeed, compared to other data forms, gathering labels for graph data is both expensive and inherently biased, which causes the limitation to the generalization ability of GNNs that brought by overfitting. Besides, representations of different nodes in a GNN tend to become indistinguishable as a result of aggregating information from neighbors recursively. This phenomenon of over-smoothing prevents GNNs from effectively modeling the higher-order dependencies from multihop neighbors <ref type="bibr" target="#b68">Chen et al. [2020]</ref>; <ref type="bibr">Li et al. [2018b]</ref>; <ref type="bibr" target="#b41">Oono and Suzuki [2020]</ref>; <ref type="bibr" target="#b59">Xu et al. [2018]</ref>; <ref type="bibr" target="#b66">Zhao and Akoglu [2020]</ref>. Recursively aggregating schema leads GNNs to be vulnerable to the quality of input graphs <ref type="bibr" target="#b71">Zhu et al. [2019]</ref>; <ref type="bibr" target="#b72">Z?gner et al. [2018]</ref>. In other words, noisy graphs or adversarial attacks can easily influence a GNN's performance.</p><p>The aforementioned problems can be helped by random dropping methods <ref type="bibr" target="#b14">Feng et al. [2020]</ref>; <ref type="bibr" target="#b19">Hinton et al. [2012]</ref>; <ref type="bibr" target="#b48">Rong et al. [2019]</ref>, which integrate noises into models by randomly masking parts of the input. These methods are applied to focus on randomly dropping or sampling existing information, and can also be considered as a data augmentation technique. Benefiting from the advantages of being unbiased, adaptive, and free of parameters, random dropping methods have contributed a lot to improve the performance of most GNNs.</p><p>However, some open questions related to random dropping methods on GNNs still exist. First, a general and critical issue of existing random dropping methods is that random noises introduced to GNNs make parameters difficult to converge and the training process unstable. Especially, difficulty is aggravated when dealing with graph data which contains arXiv: <ref type="bibr">2204.10037v1 [cs.</ref>LG] 21 Apr 2022  We present the source node (red circle) to deliver messages and its 1-hop neighbors (gray circles). Solid red grids represent reserved messages, while dotted white grids represent the masked ones. Solid lines represent the reserved edges, and dotted lines represent the masked ones.</p><p>both node features and topology information. Moreover, it is challenging to find an optimal dropping method suitable to all graphs and models, because different graphs and models are equipped with their own properties and the model performance can be influenced greatly by employing various dropping strategies. Furthermore, the answer of how to choose a proper dropping rate when applying these methods is still unclear, while no theoretical guarantee has been provided until now to explain why random dropping methods are capable to improve a GNN's performance.</p><p>In this paper, we propose a novel random dropping method called DropMessage, which can be applied to all messagepassing GNNs. As Figure <ref type="figure" target="#fig_1">1</ref> suggests, existing random dropping methods perform dropping on either the node feature matrix <ref type="bibr" target="#b14">Feng et al. [2020]</ref>; <ref type="bibr" target="#b19">Hinton et al. [2012]</ref> or the adjacency matrix <ref type="bibr" target="#b14">Feng et al. [2020]</ref>, while our DropMessage performs dropping operations on the message matrix, which allows the same node to propagate different messages to its different neighbors. Besides, we unify existing dropping methods to our framework and demonstrate theoretically that conducting random dropping methods on GNNs is equivalent to introducing additional regularization terms to their loss functions, which makes the models more robust. Furthermore, we also elaborate the superiority of our DropMessage whose sample variance is much smaller and training process is more stable. From the perspective of information theory, DropMessage keeps the property of information diversity, and is theoretically regarded as an upper bound of other random dropping methods. To sum up, the contributions of this paper are as follows:</p><p>? We propose a novel random dropping method, called DropMessage, for all message-passing GNNs. Most of existing random dropping methods can be unified into our framework via performing masking in accordance with the certain rule on the message matrix. Then it is possible to regard these methods as one special form of DropMessage.</p><p>? We give theoretical analysis on random dropping methods and provide an upper bound of the dropping rate for DropMessage.</p><p>? We conduct sufficient experiments on seven datasets, containing two industrial datasets, in different downstream tasks. Experimental results show that DropMessage consistently performs well compared to different random dropping methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>To sort out the key logic of our work, we first review some related work about random dropping methods with a particular focus on GNNs.</p><p>In general, random dropping can be regarded as a form of feature-noising schema that alleviates over-fitting by artificially corrupting the training data. As a representative work, Dropout is first introduced by Hinton et al. <ref type="bibr" target="#b19">Hinton et al. [2012]</ref> and has been proved to be effective in many scenarios Abu- <ref type="bibr" target="#b0">Mostafa [1990]</ref>; <ref type="bibr" target="#b7">Burges and Sch?lkopf [1996]</ref>; <ref type="bibr" target="#b35">Maaten et al. [2013]</ref>; <ref type="bibr">Rifai et al. [2011a]</ref>; <ref type="bibr">Simard et al. [1998]</ref>. <ref type="bibr">Besides, Bishop Bishop [1995]</ref> demonstrates the equivalence of corrupted features and L 2 -type regularization. <ref type="bibr" target="#b54">Wager et al. Wager et al. [2013]</ref> show that the dropout regularizer is first-order equivalent to an L 2 regularizer that being applied after scaling the features by an estimate of the inverse diagonal Fisher information matrix.</p><p>With the rapid development of GNNs, random dropping has also been generalized to the graph field, thus leading to three most common methods: Dropout, DropEdge Rong et al.</p><p>[2019] and DropNode <ref type="bibr" target="#b14">Feng et al. [2020]</ref>. Dropout performs random dropping operation on the node feature matrix, while DropEdge and DropNode, as the name implies, respectively act on the adjacency matrix (edges) and nodes. These random dropping methods can also be regarded as special forms of data augmentation, with the advantage of not requiring parameter estimation and easy to apply. All the methods mentioned above can be used to alleviate over-fitting and oversmoothing on GNNs. However, they can achieve effective performance only on some specific datasets and GNNs. The question of how to find an optimal dropping method that suitable for most cases still remains to be explored. Moreover, there is no theoretical explanation about the effectiveness of random dropping methods on GNNs, which adds some ambiguity to the function of these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Notations and Preliminaries</head><p>Notations. Let G = (V, E) represent the graph, where V = {v 1 , . . . , v n } denotes the set of n nodes, and E ? V ? V is the set of edges between nodes. The node features can be denoted as a matrix X = {x 1 , . . . , x n } ? R n?c , where x i is the feature vector of the node v i , and c is the dimensionality of node features. The edges describe the relations between nodes and can be represented as an adjacent matrix A = {a 1 , . . . , a n } ? R n?n , where a i denotes the i-th row of the adjacency matrix, and A (i, j) denotes the relation between nodes v i and v j . Also, the node degrees are given by d = {d 1 , . . . , d n }, where d i computes the sum of edge weights connected to node v i . Meanwhile, the degree of the whole graph is calculated by d(G) = n i d i . When we apply message-passing GNNs on G, the message matrix can be represented as M = {m 1 , . . . , m k } ? R k?c , where m i is a message propagated between nodes, and k is the total number of messages propagated on the graph. Message-passing GNNs. Most of the existing GNN models adopt the message-passing framework, where each node sends messages to its neighbors and simultaneously receives messages from its neighbors. In the process of the propagation, node representations are updated based on node feature information and messages from neighbors, which can be formulated as</p><formula xml:id="formula_0">h (l+1) i = ? (l) (h (l) i , AGG j?N (i) (? (l) (h (l) i , h (l) j , e j,i ))) (1)</formula><p>where h (l) i denotes the hidden representation of node v i in the l-th layer, and N (i) is a set of nodes adjacent to node v i ; e j,i represents the edge from node j to node i; ? (l) and ? (l) are differentiable functions; and AGG represents the aggregation operation. From the perspective of the whole graph, messagepassing GNNs can also be regarded as conducting propagation on the message matrix M which can be presented as follows:</p><formula xml:id="formula_1">M (l) = K (l) H (l)<label>(2)</label></formula><p>where K (l) ? R k?n comprises one-hot row vectors and each row vector indicates the source node of the message; H (l) is the hidden representations of the l-th layer and H (0) = X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our Approach</head><p>In this section, we introduce our proposed DropMessage, which can be applied to all message-passing GNNs. We first describe the details of our approach, and further prove that the most common existing random dropping methods, i.e., Dropout, DropEdge and DropNode, can be unified into our framework. Based on that, we give a theoretical explanation of the effectiveness of these methods. After that, we theoretically analyze the superiority of DropMessage in terms of stabilizing the training process and keeping information diversity. Finally, we derive a theoretical upper bound to guide the selection of dropping rate ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DropMessage</head><p>Algorithm description.</p><p>Different from existing random dropping methods, DropMessage performs directly on the message matrix M instead of the feature matrix or the adjacency matrix. More specifically, DropMessage conducts dropping on the message matrix with the dropping rate ?, which means that ?|M| elements of the message matrix will be masked in expectation. Formally, this operation can be </p><formula xml:id="formula_2">Xi,j = Xi,j DropEdge Ai,j = Ai,j DropNode Xi = Xi DropMessage Mi,j = Mi,j s.t. ? Bernoulli(1 -?)</formula><p>regarded as a sampling process. For each element M i,j in the message matrix, we generate an independent mask i,j to determine whether it will be preserved or not, according to a Bernoulli distribution i,j ? Bernoulli(1 -?). Then, we obtain the perturbed message matrix M by multiplying each element with its mask. Finally, we scale M with the factor of 1 1-? to guarantee that the perturbed message matrix is equal to the original message matrix in expectation. Thus, the whole process can be expressed as M i,j = 1 1-? i,j M i,j , where i,j ? Bernoulli(1 -?). The applied GNN model then propagates information via the perturbed message matrix M instead of the original message matrix. It should be moted that DropMessage only affects on the training process. Unifying random dropping methods. As we have mentioned above, DropMessage differs from existing methods by directly performing on messages instead of graphs. However, in intuition, the dropping of features, edges, nodes or messages will all eventually act on the message matrix. It inspires us to explore the theoretical connection between different dropping methods. As a start, we demonstrate that Dropout, DropEdge, DropNode, and DropMessage can all be formulated as Bernoulli sampling processes in Table <ref type="table" target="#tab_0">1</ref>. More importantly, we find that existing random dropping methods are actually special cases of DropMessage, and thus can be expressed in a uniform framework.</p><p>Lemma 1. Dropout, DropEdge, DropNode, and DropMessage perform random masking on the message matrices in accordance with certain rules.</p><p>We provide the equivalent operation on the message matrix of each method below. Dropout. Dropping the elements X drop = {Xi,j| i,j = 0} in the feature matrix X is equivalent to masking elements M drop = {Mi,j|source(Mi,j) ? X drop } in the message matrix M, where source(M i,j ) indicates which element in the feature matrix that M i,j corresponds to. DropEdge. Dropping the elements E drop = {Ei,j|Ai,j = 1 and i,j = 0} in the adjacency matrix A is equivalent to masking elements M drop = {Mi|edge(Mi) ? E drop } in the message matrix M, where edge(M i ) indicates which edge that M i corresponds to. DropNode. Dropping the elements V drop = {Xi| i = 0} in the feature matrix X is equivalent to masking elements M drop = {Mi|node(Mi) ? V drop } in the message matrix M, where node(M i ) indicates which row in the feature matrix that M i corresponds to.</p><p>DropMessage. This method directly performs random masking on the message matrix M.</p><p>According to above descriptions, we find DropMessage conduct finest-grained masking on the message matrix, which makes it the most flexible dropping method, and other methods can be regarded as a special form of DropMessage. Theoretical explanation of effectiveness. Previous studies have explored and explained why random dropping works in the filed of computer vision <ref type="bibr" target="#b54">Wager et al. [2013]</ref>; <ref type="bibr" target="#b55">Wan et al. [2013]</ref>. However, to the best of our knowledge, the effectiveness of random dropping on GNNs has not been studied yet. To fill this gap, based on the unified framework of existing methods, we next provide a theoretical analysis. Theorem 1. Unbiased random dropping on GNNs methods introduce an additional regularization term into the objective functions, which makes the models more robust.</p><p>Proof. For analytical simplicity, we assume that the downstream task is a binary classification and we apply a single layer <ref type="bibr">GCN Kipf and Welling [2017]</ref> as the backbone model, which can be formulated as H = BMW, where M denotes the message matrix, W denotes the transformation matrix, B ? R n?k indicates which messages should be aggregated by each node and B is its normalized form. Also, we adopt sigmoid as non-linear function and present the result as Z = sigmoid(H). When we use cross-entropy as loss function, the objective function can be expressed as follows:</p><formula xml:id="formula_3">LCE = j,y j =1 log(1 + e -h j ) + k,y k =0 log(1 + e h k )<label>(3)</label></formula><p>When performing random dropping on graphs, we use the perturbed message matrix M instead of the original message matrix M. Thus, the objective function in expectation can be expressed as follows:</p><formula xml:id="formula_4">E( LCE) = LCE + i 1 2 zi(1 -zi)V ar( hi) (4)</formula><p>More details of the derivation can be found in Appendix. As shown in Equation <ref type="formula">4</ref>, random dropping methods on graphs introduce an extra regularization to the objective function. For binary classification tasks, this regularization enforces the classification probability approach to 0 or 1, thus a clearer judgment can be obtained. By reducing the variance of hi , random dropping methods motivate the model to extract more essential high-level representations. Therefore, the robustness of the models is enhanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Advantages of DropMessage</head><p>We give two additional analysis to demonstrate the advantages of DropMessage on two aspects: stabilizing the training process and keeping diverse information. Reducing sample variance. All random dropping methods are challenged by the problem of unstable training process. As existing works suggest, it is caused by the random noises introduced into each training epoch. These noises then add the difficulty of parameter coverage and the unstability of training process. Generally, sample variance can be used to measure the degree of stability. According to Table <ref type="table" target="#tab_0">1</ref>, the input of each training epoch can be regarded as a random sample of the whole graph, and the sample variance is calculated by the average difference of every two independent samples. Compared with other random dropping methods, DropMessage effectively alleviates the aforementioned problem by reducing the sample variance. Theorem 2. DropMessage presents the smallest sample variance among existing random dropping methods on messagepassing GNNs with the same dropping rate ?.</p><p>We leave the proof in Appendix. Intuitively, DropMessage independently determines whether an element in the message matrix is masked or not, which is exactly the smallest Bernoulli trail for random dropping on the message matrix. By reducing the sample variance, DropMessage diminishes the difference of message matrices among distinct training epochs, which stabilizes the training process and expedites the convergence. The reason why DropMessage has the minimum sample variance is that it is the finest-grained random dropping method for GNN models. When applying DropMessage, each element M i,j will be independently judged that whether it should be masked. Keeping diverse information. In the following, we compare different random dropping methods with their degree of losing information diversity, from the perspective of information theory. Definition 1. The information diversity consists of feature diversity and topology diversity. Feature diversity is defined as the total number of preserved feature dimensions from distinct source nodes; topology diversity is defined as the total number of directed edges propagating at least one dimension message.</p><p>With the above definition, we claim that a method possesses the ability of keeping information diversity only under the condition where neither the feature diversity nor the topology diversity decreases after random dropping. Lemma 2. None of Dropout, DropEdge, and DropNode is able to keep information diversity.</p><p>According to Definition 1, when we drop an element of the feature matrix X, all corresponding elements in the message matrix are masked and the feature diversity is decreased by 1. When we drop an edge in adjacency matrix, the corresponding two rows for undirected graphs in the message matrix are masked and the topology diversity is decreased by 2. Similarly, when we drop a node, i.e., a row in the feature matrix, elements in the corresponding rows of the message matrix are all masked. Both the feature diversity and the topology diversity are therefore decreased. Thus, for all of these methods, their feature and topology information cannot be completely recovered by propagated messages, leading to the loss of information diversity. Theorem 3. DropMessage can keep information diversity in expectation when</p><formula xml:id="formula_5">? i ? 1 -min( 1 di , 1 c )</formula><p>, where ? i is the dropping rate for node v i , d i is the out-degree of v i , and c is the feature dimension.</p><p>Proof. DropMessage conducts random dropping directly on message matrix M. To keep the diversity of the topology information, we expect that at least one element of each row in message matrix M can be preserved in expectation:</p><formula xml:id="formula_6">E(|M f |) ? 1 ? (1 -?)c ? 1 ? ? ? 1 - 1 c (5)</formula><p>To keep the diversity of the feature information, we expect that for every element in the feature matrix X, at least one of its corresponding elements in the message matrix M is preserved in expectation:</p><formula xml:id="formula_7">E(|Me|) ? 1 ? (1 -?i)di ? 1 ? ?i ? 1 - 1 di (6)</formula><p>Therefore, to keep the information diversity, the dropping rate ? i should satisfy both Equation 5 and Equation <ref type="formula">6</ref>as</p><formula xml:id="formula_8">?i ? 1 -min( 1 di , 1 c ) (7)</formula><p>From the perspective of information theory, a random dropping method with the capability of keeping information diversity can preserve more information and theoretically perform better than those without such capability. Thus, it can explain why our method performs better than those existing dropping methods. Actually, we may only set one dropping rate ? for the whole graph rather than for each node in practice. Consequently, both DropMessage and other methods may lose some information. However, DropMessage still preserves more information than other methods with the same dropping rate even under this circumstance. It is demonstrated that DropMessage remains its advantage in real-world scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>We empirically validate the effectiveness and adaptability of our proposed DropMessage in this section. In particular, we explore the following questions: 1) Does DropMessage outperform other random dropping methods on GNNs? 2) Could DropMessage further improve the robustness and training efficiency of GNNs? 3) Does information diversity (described in Definition 1) matter in GNNs? Datasets. We employ 7 graph datasets in our experiments, including 5 public datasets Cora, CiteSeer, PubMed, ogbnarxiv, Flickr and 2 industrial datasets FinV, Telecom. See data descriptions and statistics in Appendix. Baseline methods. We compare our proposed DropMessage with other existing random dropping methods, including Dropout <ref type="bibr" target="#b19">Hinton et al. [2012]</ref>, DropEdge <ref type="bibr" target="#b48">Rong et al. [2019]</ref>, and DropNode <ref type="bibr" target="#b14">Feng et al. [2020]</ref>. We adopt these dropping methods on various GNNs as the backbone model, and compare their performances on different datasets. Backbone models.</p><p>In this paper, we mainly consider three mainstream GNNs as our backbone models: GCN Kipf and Welling <ref type="bibr">[2017]</ref>, <ref type="bibr">GAT Velickovic et al. [2018]</ref>, and APPNP <ref type="bibr" target="#b27">Klicpera et al. [2019]</ref>. We take the official practice of these methods while make some minor modifications. More implementation details can be found in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison Results</head><p>Table <ref type="table" target="#tab_1">2</ref> summarizes the overall results. For the node classification task, the performance is measured by accuracy on five public datasets as their label distributions are rather balanced. As for the two imbalanced industrial datasets, we employ F1 scores. When it comes to the link prediction task, we calculate the AUC values for comparisons. Effect of random dropping methods. It is observed that random dropping methods consistently outperform GNNs without random dropping in both node classification and link prediction. Besides, we see that the effects of random dropping methods vary over different datasets, backbone models, and downstream tasks. For example, random dropping methods on APPAP obtain an average accuracy improvement of 1.4% on CiteSeer, while 0.1% on PubMed. Meanwhile, random dropping methods achieve 2.1% accuracy improvement for GCN on Cora, while only 0.8% for GAT. Comparison of different dropping methods.</p><p>Our proposed DropMessage works well in all settings, exhibiting its strong adaptability to various scenarios. Overall, we have 21 settings under the node classification task, each of which is a combination of different backbone models and datasets (e.g., GCN-Cora). It is showed that DropMessage achieves the optimal results in 15 settings, and gets sub-optimal results in the rest. As to 9 setttings under the link prediction task, DropMessage achieves the optimal results in 5 settings, and sub-optimal results in the rest. Moreover, the stable performance of DropMessage over all datasets compared to other methods is clearly presented. Taking DropEdge as the counterexample, it appears strong performance on industrial datasets but demonstrates a clear drop on public ones. A reasonable explanation is that the message matrix patterns reserved by distinct mask methods vary from each other as presented in Table <ref type="table" target="#tab_0">1</ref>. With the favor of its finest-grained dropping strategy, DropMessage obtains smaller inductive bias. Thus, compared with other methods, DropMessage is more applicable in most scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness analysis.</head><p>We study the robustness of dropping methods through measuring their ability of handling perturbed graphs. To guarantee that the initial data is comparatively clean, we conduct experiments on three citation networks: Cora, CiteSeer, and PubMed. We randomly add a certain ratio of edges into these datasets and perform the node classification. We find that all the random dropping methods have positive effects when the perturbation rate increases from 0% to 30%. The average improvement in the case of 30% perturbation reached 37% compared to that without perturbation, which indicates that the random dropping methods strengthen the robustness of GNN models. Besides, our proposed DropMessage shows its versatility and outperforms other dropping methods in noisy situations. Detailed results are exhibited in Appendix. Over-smoothing analysis. Over-smoothing is a common issue on GNNs <ref type="bibr">Li et al. [2018b]</ref>, which implies that the node representations become indistinguishable as the net-  work depth increases. In this part, we evaluate the effects that various random dropping methods exert on this issue, and measure the degree of over-smoothing by MADGap <ref type="bibr">Chen et al. [2020]</ref>. It should be noted that here a smaller value indicates the more indistinguishable node representations and vice versa. Experiments are conducted on Cora with GCNs serving as backbone models. Figure <ref type="figure">2</ref> shows the relative increase of MADGap values and test accuracies of the final node representations compared to the original model without any random dropping techniques. The results indicate that all these random dropping methods can alleviate over-smoothing by increasing the MADGap values and test accuracies when the depth of the model increases. Among all random dropping methods, our proposed DropMessage exhibits a superiority of consistency. It obtains an average improvement of 3.3% on MADGap values and an average improvement of 4.9% on test accuracies compared to other random dropping methods when the layer number l ? 3. This result can be explained by the fact that DropMessage can generate more various messages than other methods, which prevents the nodes from converging to the same representations to some extent.</p><p>Training process analysis. We conduct experiments to analyze the loss during the training process when employing different random dropping methods. The experimental results (see details in Appendix) suggest that DropMessage presents the smallest sample variance among all methods, thus achieving the fastest convergence and the most stable performance. This is consistent with the theoretical results in Section 4.2. Information diversity analysis. We conduct experiments to evaluate the importance of information diversity for messagepassing GNN models. We set Cora as our experimental dataset, which contains 2708 nodes and 5429 edges. The average node degree of Cora is close to 4. According to Equation 7, the upper bound of dropping rate is calculated from the node degree and the feature dimension. The feature dimension number of Cora is 1433, which is much larger than the number of node degree. Therefore, the upper bound is only determined by the degree of the node. In (backbone)nodewise settings, we set the dropping rate to be equal to its upper bound ? i = 1 -1 di for each node. In (backbone)average settings, we set the dropping rate ? i = 0.75 + i , where i ? U nif orm <ref type="bibr">(-0.15, 0.15)</ref>. Both of these settings employ DropMessage. The average random dropping rate of all nodes is almost identical under these two settings, but only the former one can keep the information diversity in expectation. Table <ref type="table" target="#tab_2">3</ref> presents the results. The (backbone)-nodewise settings outperform (backbone)-average settings regardless of which backbone model is selected. The results indicate the importance of keeping information diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose DropMessage, a general random dropping method for message-passing GNN models. We first unify all random dropping methods to our framework via performing dropping on the message matrix and analyzing their effects. Then we illustrate the superiority of DropMessage theoretically in stabilizing the training process and keeping information diversity. Due to its fine-grained dropping operations on the message matrix, DropMessage shows greater applicability in most cases. By conducting experiments for multiple tasks on five public datasets and two industrial datasets, we demonstrate the effectiveness and generalization of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Derivation Details</head><p>Detailed proof of Theorem 1. Theorem 1 Unbiased random dropping methods introduce an extra regularization term into the objective functions, which make the models more robust.</p><p>We give more derivation details of Theorem 1. When we use cross-entropy as the loss function, the objective function can be expressed as follows:</p><formula xml:id="formula_9">LCE = - j,y j =1 log(zj) - k,y k =0 log(1 -z k ) = j,y j =1 log(1 + e -h j ) + k,y k =0 log(1 + e h k )</formula><p>According to above equation, the initial objective function is L CE = j,yj =1 log(1 + e -hj ) + k,y k =0 log(1 + e h k ). When we perturb the message matrix, the objective function can be regarded as a process of adding a bias to the original function, expressed as follows:</p><formula xml:id="formula_10">E( LCE) = j,y j =1 [log(1 + e -h j ) + E(f ( hj, hj))] + k,y k =0 [log(1 + e h k ) + E(g( hk , h k ))]</formula><p>where f ( hj , h j ) = log(1 + e -hj ) -log(1 + e -hj ), and g( hk , h k ) = log(1 + e hk ) -log(1 + e h k ). We can approximate it with the second-order Taylor expansion of f (.) and g(.) around h i . Thus, the objective function in expectation can be expressed as bellow:</p><formula xml:id="formula_11">E( LCE) = LCE + E( j,y j =1 [(-1 + zj)( hj -hj) + 1 2 zj(1 -zj)( hj -hj) 2 ]) + E( k,y k =0 [z k ( hk -h k ) + 1 2 z k (1 -z k )( hk -h k ) 2 ]) = LCE + i 1 2 zi(1 -zi)V ar( hi)</formula><p>Proof of Theorem 2. Theorem 2 DropMessage presents the smallest sample variance among all existing random dropping methods on message-passing GNNs with the same dropping rate ?.</p><p>Proof. As stated in Lemma 1, all random dropping methods on graphs can be converted to masking operations on the message matrix M. We can measure the difference of message matrices in different epochs by the way of comparing the sample variance of random dropping methods, which can be measured via the norm variance of the message matrix |M| F . Without loss of generality, we assume the original message matrix M is 1 n?n , i.e., every element is 1. Thus, we can calculate its sample variance via the 1-norm of the message matrix.</p><p>We consider that the message-passing GNNs do not possess the node-sampler or the edge-sampler, which means every directed edge corresponds to a row vector in the message matrix M. For analytical simplicity, we assume that the graph is undirected and the degree of each node is d. In this case, k = 2|E| = nd rows of the message matrix counts in total. All random dropping methods can be considered as multiple independent Bernoulli samplings. The whole process conforms to a binomial distribution, and so we can calculate the variance of |M|. Dropout. Perform nc times of Bernoulli sampling. Dropping an element in the feature matrix leads to masking d elements in the message matrix. Its variance can be calculated by V ar do (|M|) = (1 -?)?ncd 2 .</p><p>DropEdge. Perform nd 2 times of Bernoulli sampling. Dropping an element in the adjacency matrix leads to masking 2c elements in the message matrix. Its variance can be calculated by V ar de (|M|) = 2(1 -?)?nc 2 d. DropNode. Perform n times of Bernoulli sampling. Dropping an element in the node set leads to masking cd elements in the message matrix. Its variance can be calculated by V ar dn (|M|) = (1 -?)?nc 2 d 2 . DropMessage. Perform ncd times of Bernoulli sampling. Dropping an element in the message matrix leads to masking 1 elements in the message matrix. Its variance can be calculated by V ar dm (|M|) = (1 -?)?ncd.</p><p>Therefore, the variances of the random dropping methods are sorted as follows:</p><p>V ar dm (|M|) ? V ar do (|M|) ? V ar dn (|M|)</p><p>V ar dm (|M|) ? V ar de (|M|)</p><p>Our DropMessage has the smallest sample variance among all existing random dropping methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Experiment Details</head><p>Dataset descriptions. We give more details about the 7 datasets that are applied in our experiments.</p><p>? Cora, CiteSeer, PubMed, ogbn-arxiv: These 4 different citation networks are widely used as graph benchmarks <ref type="bibr" target="#b20">Hu et al. [2020]</ref>; <ref type="bibr" target="#b50">Sen et al. [2008]</ref>. We conduct node classification tasks on each dataset to determine the research area of papers/researchers. We also consider link prediction on the first three graphs to predict whether one paper cites another. ? Flickr: It is provided by Flickr, the largest photo-sharing website <ref type="bibr" target="#b65">Zeng et al. [2020]</ref>. One node in the graph represents one image uploaded to Flickr. If two images share some common properties (e.g., same geographic location, same gallery, or comments by the same user), an edge between the nodes of these two images will appear. We conduct the node classification task that aims to categorize these images into 7 classes determined by their tags. ? FinV, Telecom: These are two real-world mobile communication networks provided by FinVolution Group <ref type="bibr" target="#b62">Yang et al. [2019]</ref> and China Telecom <ref type="bibr" target="#b63">Yang et al. [2021]</ref>, respectively. In the two datasets, nodes represent users, and edges indicate the situation where two users have communicated with each other at a certain frequency. The task is to identify whether a user is a default borrower or a telecom fraudster.</p><p>Dataset statistics. Table <ref type="table" target="#tab_3">4</ref> shows the statistics of datasets. Backbone models. All these backbone models have random dropping modules for different steps in their model implementation. For instance, GAT models perform random dropping after self-attention calculation, while APPNP models perform random dropping at the beginning of each iteration. For a fair comparison, we unify the implementation of random dropping modules in the same step for different backbone models. We fix Dropout, DropEdge and DropNode on the initial input and fix DropMessage on the start point of the message propagation process. Implementation details. We conduct 20 independent experiments for each setting and obtain the average results. We adjust the dropping rate from 0.1 to 0.9 in steps of 0.1 and select the optimal one for each setting. On the five public datasets, we continue to employ the same hyper-parameter settings as previous works have proposed. And on the two real-world datasets, we obtain the best parameters through careful tuning.</p><p>As for public datasets Cora, CiteSeer, PubMed, and Flickr, we apply two-layer models. However, when it comes to the public dataset ogbn-arxiv and two industrial datasets, Telecom and FinV, we employ three-layer models with two batch normalization layers between the network layers. These experimental settings are identical for node classification tasks and link prediction tasks. The number of hidden units on GCNs is 16 for Cora, CiteSeer, PubMed, and is 64 for others. For GATs, we apply eight-head models with 8 hidden units for Cora, CiteSeer, PubMed, and use single-head models with 128 hidden units for the other datasets. As for APP-NPs, we use the teleport probability ? = 0.1 and K = 10 power iteration steps. The number of hidden units on APP-NPs is always 64 for all datasets. In all cases, we use Adam optimizers with learning rate of 0.005 and L 2 regularization of 5 ? 10 -4 , and train each model 200 epochs. Results of robustness analysis. Table <ref type="table">5</ref> summarizes the classification accuracy of robustness analysis. Results of training process analysis.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Illustrations of various random dropping methods in the message passing process. In the figure, arrows indicate the direction of the message propagation. We present the source node (red circle) to deliver messages and its 1-hop neighbors (gray circles). Solid red grids represent reserved messages, while dotted white grids represent the masked ones. Solid lines represent the reserved edges, and dotted lines represent the masked ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Process Analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Overview of different random dropping methods in a view of Bernoulli sampling process.</figDesc><table><row><cell>Method</cell><cell>Formula</cell></row><row><cell>Dropout</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison results of different random dropping methods. The best results are in bold, while the second-best ones are underlined.</figDesc><table><row><cell>Task &amp; Dataset</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Node classification</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Link prediction</cell></row><row><cell>Model</cell><cell cols="6">Cora CiteSeer PubMed ogbn-arxiv Flickr Telecom</cell><cell>FinV</cell><cell>Cora</cell><cell cols="2">CiteSeer PubMed</cell></row><row><cell>GCN</cell><cell>80.68</cell><cell>70.83</cell><cell>78.97</cell><cell>70.08</cell><cell>51.88</cell><cell>0.6080</cell><cell>0.4220</cell><cell>0.9198</cell><cell>0.8959</cell><cell>0.9712</cell></row><row><cell>GCN-Dropout</cell><cell>83.16</cell><cell>71.48</cell><cell>79.13</cell><cell>71.16</cell><cell>52.22</cell><cell>0.6601</cell><cell>0.4526</cell><cell>0.9278</cell><cell>0.9107</cell><cell>0.9766</cell></row><row><cell>GCN-DropEdge</cell><cell>81.69</cell><cell>71.43</cell><cell>79.06</cell><cell>70.88</cell><cell>52.14</cell><cell>0.6650</cell><cell>0.4729</cell><cell>0.9295</cell><cell>0.9067</cell><cell>0.9762</cell></row><row><cell>GCN-DropNode</cell><cell>83.04</cell><cell>72.12</cell><cell>79.00</cell><cell>70.98</cell><cell>52.13</cell><cell>0.6243</cell><cell>0.4571</cell><cell>0.9238</cell><cell>0.9052</cell><cell>0.9748</cell></row><row><cell>GCN-DropMessage</cell><cell>83.33</cell><cell>71.83</cell><cell>79.20</cell><cell>71.27</cell><cell>52.23</cell><cell>0.6710</cell><cell>0.4876</cell><cell>0.9305</cell><cell>0.9071</cell><cell>0.9772</cell></row><row><cell>GAT</cell><cell>81.35</cell><cell>70.14</cell><cell>77.20</cell><cell>70.32</cell><cell>49.88</cell><cell>0.7050</cell><cell>0.4467</cell><cell>0.9118</cell><cell>0.8895</cell><cell>0.9464</cell></row><row><cell>GAT-Dropout</cell><cell>82.41</cell><cell>71.31</cell><cell>78.31</cell><cell>71.28</cell><cell>49.98</cell><cell>0.7382</cell><cell>0.4539</cell><cell>0.9182</cell><cell>0.9055</cell><cell>0.9536</cell></row><row><cell>GAT-DropEdge</cell><cell>81.82</cell><cell>71.17</cell><cell>77.70</cell><cell>70.67</cell><cell>50.04</cell><cell>0.7568</cell><cell>0.4896</cell><cell>0.9206</cell><cell>0.9037</cell><cell>0.9493</cell></row><row><cell>GAT-DropNode</cell><cell>82.08</cell><cell>71.44</cell><cell>77.98</cell><cell>70.96</cell><cell>49.92</cell><cell>0.7214</cell><cell>0.4647</cell><cell>0.9224</cell><cell>0.9104</cell><cell>0.9566</cell></row><row><cell>GAT-DropMessage</cell><cell>82.20</cell><cell>71.48</cell><cell>78.14</cell><cell>71.13</cell><cell>50.13</cell><cell>0.7574</cell><cell>0.4861</cell><cell>0.9216</cell><cell>0.9076</cell><cell>0.9553</cell></row><row><cell>APPNP</cell><cell>81.45</cell><cell>70.62</cell><cell>79.79</cell><cell>69.11</cell><cell>50.47</cell><cell>0.6217</cell><cell>0.3952</cell><cell>0.9058</cell><cell>0.8844</cell><cell>0.9531</cell></row><row><cell>APPNP-Dropout</cell><cell>82.23</cell><cell>71.93</cell><cell>79.92</cell><cell>69.36</cell><cell>50.55</cell><cell>0.6578</cell><cell>0.4023</cell><cell>0.9119</cell><cell>0.9071</cell><cell>0.9611</cell></row><row><cell>APPNP-DropEdge</cell><cell>82.75</cell><cell>72.10</cell><cell>79.83</cell><cell>69.15</cell><cell>50.61</cell><cell>0.6591</cell><cell>0.4149</cell><cell>0.9139</cell><cell>0.9131</cell><cell>0.9626</cell></row><row><cell>APPNP-DropNode</cell><cell>81.79</cell><cell>71.50</cell><cell>79.81</cell><cell>69.27</cell><cell>50.53</cell><cell>0.6412</cell><cell>0.4182</cell><cell>0.9068</cell><cell>0.8979</cell><cell>0.9561</cell></row><row><cell>APPNP-DropMessage</cell><cell>82.37</cell><cell>72.65</cell><cell>80.04</cell><cell>69.72</cell><cell>50.72</cell><cell>0.6619</cell><cell>0.4378</cell><cell>0.9165</cell><cell>0.9141</cell><cell>0.9634</cell></row><row><cell>1 2 3 4 5 6 7 8 Layer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Classification accuracy (%) for information diversity analysis (where AVG denotes average, and NW denotes nodewise).</figDesc><table><row><cell>Model</cell><cell cols="2">GCN</cell><cell cols="2">GAT</cell><cell cols="2">APPNP</cell></row><row><cell></cell><cell>AVG</cell><cell>NW</cell><cell>AVG</cell><cell>NW</cell><cell>AVG</cell><cell>NW</cell></row><row><cell cols="7">Accuracy (%) 81.62 82.67 80.81 81.61 80.71 81.56</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Dataset Statistics.</figDesc><table><row><cell>Dataset</cell><cell>Nodes</cell><cell>Edges</cell><cell cols="2">Feature Classes</cell></row><row><cell>Cora</cell><cell>2708</cell><cell>5429</cell><cell>1433</cell><cell>7</cell></row><row><cell>CiteSeer</cell><cell>3327</cell><cell>4732</cell><cell>3703</cell><cell>6</cell></row><row><cell>PubMed</cell><cell>19717</cell><cell>44338</cell><cell>500</cell><cell>3</cell></row><row><cell cols="3">ogbn-arxiv 169343 1166243</cell><cell>128</cell><cell>40</cell></row><row><cell>Flickr</cell><cell>89250</cell><cell>899756</cell><cell>500</cell><cell>7</cell></row><row><cell>FinV</cell><cell cols="2">340751 1575498</cell><cell>261</cell><cell>2</cell></row><row><cell>Telecom</cell><cell cols="2">509304 809996</cell><cell>21</cell><cell>2</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning from hints in neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yaser</surname></persName>
		</author>
		<author>
			<persName><surname>Abu-Mostafa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of complexity</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="192" to="198" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Rednemo: topology-based ppi network reconstruction via repeated diffusion with neighborhood modifications</title>
		<author>
			<persName><forename type="first">Ferhat</forename><surname>Alkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cesim</forename><surname>Erten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="537" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Outlier aware network embedding for attributed networks</title>
		<author>
			<persName><forename type="first">Sambaran</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lokesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murty</forename><surname>Narasimha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Beyond low-frequency information in graph convolutional networks</title>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">M</forename><surname>Bishop ; Deyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huawei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1995">1995. 2021</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3950" to="3957" />
		</imprint>
	</monogr>
	<note>Training with noise is equivalent to tikhonov regularization</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">How attentive are graph attention networks?</title>
		<author>
			<persName><forename type="first">Shaked</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<idno>ArXiv, abs/2105.14491</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Le-Cun</surname></persName>
		</author>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving the accuracy and speed of support vector machines</title>
		<author>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Minimum curvilinearity to enhance topological prediction of protein interactions by network embedding</title>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Vittorio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cannistraci</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Gregorio</forename><surname>Alanis-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Ravasi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="199" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Stochastic training of graph convolutional networks with variance reduction</title>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Measuring and relieving the over-smoothing problem for graph neural networks from the topological view</title>
		<author>
			<persName><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<title level="m">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised learning on graphs with generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="913" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph random neural networks for semi-supervised learning on graphs</title>
		<author>
			<persName><forename type="first">Wenzheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Kharlamov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Eric Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Missing and spurious interactions and the reconstruction of complex networks</title>
		<author>
			<persName><forename type="first">Roger</forename><surname>Guimer?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Sales-Pardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PNAS</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="22073" to="22078" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>abs/1207.0580</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Revealing missing parts of the interactome via link prediction</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec ; Yuriy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">W</forename><surname>Hulovatyy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tijana</forename><surname>Solava</surname></persName>
		</author>
		<author>
			<persName><surname>Milenkovi?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2014">2020. 2014</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">e90073</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Open graph benchmark: Datasets for machine learning on graphs</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Graph structure learning for robust graph neural networks</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianfeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10203</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Variational graph autoencoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno>abs/1611.07308</idno>
	</analytic>
	<monogr>
		<title level="m">ArXiv</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A novel link prediction algorithm for reconstructing protein-protein interaction networks by topological similarity</title>
		<author>
			<persName><forename type="first">Chengwei</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Ruan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deeprobust: A pytorch library for adversarial attacks and defenses</title>
		<author>
			<persName><forename type="first">Yaxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.06149</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semi-supervised embedding in attributed networks with outliers</title>
		<author>
			<persName><forename type="first">Jiongqian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiankai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivasan</forename><surname>Parthasarathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="153" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning to drop: Robust graph neural network via topological denoising</title>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenchao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingchao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.07057</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning with marginalized corrupted features</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Chris J Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><surname>Teh</surname></persName>
		</author>
		<title level="m">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Noise injection into inputs in backpropagation learning</title>
		<author>
			<persName><forename type="first">Kiyotoshi</forename><surname>Matsuoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="436" to="440" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Birds of a feather: Homophily in social networks</title>
		<author>
			<persName><forename type="first">Lynn</forename><surname>Miller Mcpherson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Smith-Lovin</surname></persName>
		</author>
		<author>
			<persName><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual review of sociology</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="415" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning graph neural networks with noisy labels</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Choong</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Tsuyoshi</forename><surname>Murata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.01591</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Robust graph embedding with noisy link weights</title>
		<author>
			<persName><forename type="first">Akifumi</forename><surname>Okuno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hidetoshi</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="664" to="673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
		<idno>arXiv: Learning</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Predicting missing links and identifying spurious links via likelihood analysis</title>
		<author>
			<persName><forename type="first">Liming</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linyuan</forename><surname>L?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chin-Kun</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzhen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Noise-resilient similarity preserving network embedding for social networks</title>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongzheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3282" to="3288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A distributed approach to node clustering in decentralized peerto-peer networks</title>
		<author>
			<persName><forename type="first">Lakshmish</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bugra</forename><surname>Gedik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPDS</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="814" to="829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The manifold tangent classifier</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Adding noise to the input of a model trained with a regularized objective</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<idno>ArXiv, abs/1104.3250</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dropedge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Graph sparsification approaches for laplacian smoothing</title>
		<author>
			<persName><forename type="first">Veeru</forename><surname>Sadhanala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1250" to="1259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="93" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Transformation invariance in pattern recognition-tangent distance and tangent propagation</title>
		<author>
			<persName><forename type="first">Patrice</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">A</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Victorri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: tricks of the trade</title>
		<imprint>
			<biblScope unit="page" from="239" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dropout training as adaptive regularization</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Network enhancement as a general method to denoise weighted biological networks</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armin</forename><surname>Pourshafeie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><forename type="middle">D</forename><surname>Bustamante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serafim</forename><surname>Batzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Tailin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12811</idno>
		<title level="m">Graph information bottleneck</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tomohiro Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Robust network enhancement from flawed networks</title>
		<author>
			<persName><forename type="first">Jiarong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Understanding default behavior in online lending</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Mining fraudsters and fraudulent strategies in large-scale mobile social networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="169" to="179" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Graphsaint: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prasanna</surname></persName>
		</author>
		<idno>ArXiv, abs/1907.04931</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Pairnorm: Tackling oversmoothing in gnns</title>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<idno>ArXiv, abs/1909.12223</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyang</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian-Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petter</forename><surname>Holme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Zhou</surname></persName>
		</author>
		<title level="m">Prediction of links and weights in networks by reliable routes</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Robust graph representation learning via neural sparsification</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingchao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenchao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11458" to="11468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Class noise vs. attribute noise: A quantitative study</title>
		<author>
			<persName><forename type="first">Xingquan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xindong</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif Intell Rev</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="177" to="210" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Robust graph convolutional networks against adversarial attacks</title>
		<author>
			<persName><forename type="first">Dingyuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1399" to="1407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Adversarial attacks on neural networks for graph data</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Z?gner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Akbarnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Proceedings of the 24th. Table 5: Classification accuracy (%) for robustness analysis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
