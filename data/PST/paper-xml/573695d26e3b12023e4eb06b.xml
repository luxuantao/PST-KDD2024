<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The direct extension of ADMM for multi-block convex minimization problems is not necessarily convergent</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Caihua</forename><surname>Chen</surname></persName>
							<email>chchen@nju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Bingsheng</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yinyu</forename><surname>Ye</surname></persName>
							<email>yyye@stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Xiaoming</forename><surname>Yuan</surname></persName>
							<email>xmyuan@hkbu.edu.hk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">International Centre of Management Science and Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Management and Engineering</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Department of Management Science and Engineering</orgName>
								<orgName type="department" key="dep2">School of Engineering</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">Hong Kong Baptist University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The direct extension of ADMM for multi-block convex minimization problems is not necessarily convergent</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B0D8D74969767D922C61688BC13EA7E9</idno>
					<idno type="DOI">10.1007/s10107-014-0826-5</idno>
					<note type="submission">Received: 22 January 2014 / Accepted: 4 October 2014</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Alternating direction method of multipliers</term>
					<term>Convergence analysis</term>
					<term>Convex programming</term>
					<term>Splitting methods Mathematics Subject Classification 90C25</term>
					<term>90C30</term>
					<term>65K13</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The alternating direction method of multipliers (ADMM) is now widely used in many fields, and its convergence was proved when two blocks of variables are alternatively updated. It is strongly desirable and practically valuable to extend the ADMM directly to the case of a multi-block convex minimization problem where its objective function is the sum of more than two separable convex functions. However, the convergence of this extension has been missing for a long time-neither an affirmative convergence proof nor an example showing its divergence is known in the literature. In this paper we give a negative answer to this long-standing open question:</p><p>C.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We consider the convex minimization model with linear constraints and an objective function which is the sum of three functions without coupled variables:</p><formula xml:id="formula_0">min θ 1 (x 1 ) + θ 2 (x 2 ) + θ 3 (x 3 ) s.t. A 1 x 1 + A 2 x 2 + A 3 x 3 = b, x 1 ∈ X 1 , x 2 ∈ X 2 , x 3 ∈ X 3 ,<label>(1.1)</label></formula><p>where A i ∈ p×n i (i = 1, 2, 3), b ∈ p , X i ⊂ n i (i = 1, 2, 3) are closed convex sets; and θ i : n i → (i = 1, 2, 3) are closed convex but not necessarily smooth functions. The solution set of (1.1) is assumed to be nonempty. The abstract model (1.1) captures many applications in diversifying areas-e.g. see the image alignment problem in <ref type="bibr" target="#b23">[24]</ref>, the robust principal component analysis model with noisy and incomplete data in <ref type="bibr" target="#b26">[27]</ref>, the latent variable Gaussian graphical model selection in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22]</ref> and the quadratic discriminant analysis model in <ref type="bibr" target="#b20">[21]</ref>. Our discussion is inspired by the scenario where each function θ i may have some specific properties and it deserves to explore them in algorithmic design. This is often encountered in some sparse and low-rank optimization models, such as the just-mentioned applications of (1.1). We thus do not consider the generic treatment that the sum of three functions is regarded as one general function and possible advantageous properties of each individual θ i are ignored or not fully used. The alternating direction method of multipliers (ADMM) was originally proposed in <ref type="bibr" target="#b11">[12]</ref> (see also <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref>), and it is now a benchmark for the following convex minimization model analogous to (1.1) but with only two blocks of functions and variables:</p><formula xml:id="formula_1">min θ 1 (x 1 ) + θ 2 (x 2 ) s.t. A 1 x 1 + A 2 x 2 = b, x 1 ∈ X 1 , x 2 ∈ X 2 .</formula><p>(1.2)</p><formula xml:id="formula_2">Let L A (x 1 , x 2 , λ) = θ 1 (x 1 ) + θ 2 (x 2 ) -λ T A 1 x 1 + A 2 x 2 -b + β 2 A 1 x 1 + A 2 x 2 -b 2 (1.3)</formula><p>be the augmented Lagrangian function of (1.2) with the Lagrange multiplier λ ∈ p and β &gt; 0 be a penalty parameter. Then, the iterative scheme of ADMM for (1.2) is (ADMM)</p><formula xml:id="formula_3">⎧ ⎪ ⎨ ⎪ ⎩ x k+1 1 = Argmin{L A (x 1 , x k 2 , λ k ) | x 1 ∈ X 1 }, (1.4a) x k+1 2 = Argmin{L A (x k+1 1 , x 2 , λ k ) | x 2 ∈ X 2 },<label>(1.4b</label></formula><p>)</p><formula xml:id="formula_4">λ k+1 = λ k -β(A 1 x k+1 1 + A 2 x k+1 2 -b).</formula><p>(1.4c)</p><p>The iterative scheme of ADMM embeds a Gaussian-Seidel decomposition into each iteration of the augmented Lagrangian method (ALM) in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25]</ref>; thus the functions θ 1 and θ 2 are treated individually and so easier subproblems could be generated. This feature is very advantageous for a broad spectrum of application such as partial differential equations, mechanics, image processing, statistical learning, computer vision, and so on. In fact, the ADMM has recently witnessed a "renaissance" in many application domains after a long period without too much attention. We refer to <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11]</ref> for some review papers on the ADMM.</p><p>Given the ADMM's advantage in using each θ i 's properties individually, it is natural to extend the original ADMM (1.4) for (1.2) directly to (1.1) and obtain the scheme (Direct Extension of ADMM)</p><formula xml:id="formula_5">⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ x k+1 1 = Argmin L A (x 1 , x k 2 , x k 3 , λ k ) | x 1 ∈ X 1 ,</formula><p>(1.5a)</p><formula xml:id="formula_6">x k+1 2 = Argmin L A (x k+1 1 , x 2 , x k 3 , λ k ) | x 2 ∈ X 2 ,</formula><p>(1.5b)</p><formula xml:id="formula_7">x k+1 3 = Argmin L A (x k+1 1 , x k+1 2 , x 3 , λ k ) | x 3 ∈ X 3 , (1.5c) λ k+1 = λ k -β(A 1 x k+1 1 + A 2 x k+1 2 + A 3 x k+1 3 -b),<label>(1.5d)</label></formula><p>where</p><formula xml:id="formula_8">L A (x 1 , x 2 , x 3 , λ) = 3 i=1 θ i (x i ) -λ T A 1 x 1 + A 2 x 2 + A 3 x 3 -b + β 2 A 1 x 1 + A 2 x 2 + A 3 x 3 -b 2 (1.6)</formula><p>is the augmented Lagrangian function of <ref type="bibr">(1.1)</ref>. This direct extension of ADMM is strongly desired and practically used by many users, see e.g. <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27]</ref>. The convergence of (1.5), however, has been ambiguous for a long time-there is neither an affirmative convergence proof nor an example showing its divergence in the literature. This convergence ambiguity has inspired an active research topic of developing such algorithms that are slightly twisted versions of (1.5) but with provable convergence and competitive numerical performance, see e.g. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20]</ref>. Since the direct extension of ADMM (1.5) does work well for some applications (e.g. <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27]</ref>), users have the inclination to imagine that this scheme seems to be convergent even though they are perplexed by the rigorous proof. In the literature, there was even very little hint for the difficulty in the convergence proof for (1.5), see <ref type="bibr" target="#b5">[6]</ref> for an insightful explanation.</p><p>The main result of this paper is to answer this long-standing open question negatively: The direct extension of ADMM (1.5) is not necessarily convergent. We organize the rest of this paper as follows. In Sect. 2, we present a sufficient condition to ensure the convergence of (1.5). Then, based on the analysis in Sect. 2, we construct an example to demonstrate the divergence of the direct extension of ADMM (1.5) in Sect. 3. Some extensions of the paper's main result are discussed in Sect. 4. Finally, some concluding remarks are given in Sect. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A sufficient condition ensuring the convergence of (1.5)</head><p>We first study a sufficient condition that can ensure the convergence for the direct extension of ADMM <ref type="bibr">(1.5)</ref>. This condition is of only theoretical interest, but our idea of constructing a counter example to show the divergence of (1.5) would be clearer via this study.</p><p>Our claim is that the convergence of (1.5) is guaranteed when any two coefficient matrices in (1.1) are orthogonal. We thus will discuss the cases:</p><formula xml:id="formula_9">A T 1 A 2 = 0, A T 2 A 3 = 0 and A T 1 A 3 = 0.</formula><p>This new condition does not impose any strong convexity on the objective function in (1.1), and it simply requires to check the orthogonality of the coefficient matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Case 1:</head><formula xml:id="formula_10">A T 1 A 2 = 0 or A T 2 A 3 = 0</formula><p>We remark that if two coefficient matrices of (1.1) in consecutive order are orthogonal, i.e., A T 1 A 2 = 0 or A T 2 A 3 = 0, then the direct extension of ADMM (1.5) reduces to a special case of the original ADMM <ref type="bibr">(1.4)</ref>. Thus the convergence of (1.5) under this condition is implied by well known results in ADMM literature.</p><p>To see this, let us first assume A T 1 A 2 = 0. According to the first-order optimality conditions of the minimization problems in (1.5), we have x k+1</p><formula xml:id="formula_11">⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ x k+1 1 , x k+1 2 = Argmin θ 1 (x 1 ) + θ 2 (x 2 ) -(λ k ) T (A 1 x 1 + A 2 x 2 ) + β 2 A 1 x 1 + A 2 x 2 + A 3 x k 3 -b 2 x 1 ∈ X 1 , x 2 ∈ X 2 ,</formula><p>(2.3a)</p><formula xml:id="formula_12">x k+1 3 = Argmin θ 3 (x 3 ) -(λ k ) T A 3 x 3 + β 2 A 1 x k+1 1 + A 2 x k+1 2 + A 3 x 3 -b 2 |x 3 ∈ X 3 , , (2.3b) λ k+1 = λ k -β A 1 x k+1 1 + A 2 x k+1 2 + A 3 x k+1 3 -b .</formula><p>(2.3c)</p><p>Clearly, (2.3) is a specific application of the original ADMM (1.4) to (1.1) by regarding (x 1 , x 2 ) as one variable, [A 1 , A 2 ] as one matrix and θ 1 (x 1 ) + θ 2 (x 2 ) as one function. Note that both x k 1 and x k 2 are not required to generate the (k + 1)-th iteration under the orthogonality condition A T  1 A 2 = 0 in (2.3). Existing convergence results for the original ADMM such as those in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref> thus hold for the special case of (1.5) with the orthogonality condition A T  1 A 2 = 0. Similar discussion can be carried out under the orthogonality condition A T 2 A 3 = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Case 2:</head><formula xml:id="formula_13">A T 1 A 3 = 0</formula><p>In the last subsection, we have discussed the cases where two consecutive coefficient matrices are orthogonal. Now, we pay attention to the case where A T 1 A 3 = 0 and show that it can also ensure the convergence of (1.5).</p><p>To prepare for the proof, we need to make something clear. First, note that the update order of (1.5) at each iteration is x 1 → x 2 → x 3 → λ and then it repeats cyclically. Equivalently, we can update the variables via the order x 2 → x 3 → λ → x 1 and thus have the following iterative form:</p><formula xml:id="formula_14">⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ x k+1 2 = Argmin L A (x k 1 , x 2 , x k 3 , λ k ) | x 2 ∈ X 2 ,</formula><p>(2.4a)</p><formula xml:id="formula_15">x k+1 3 = Argmin L A x k 1 , x k+1 2 , x 3 , λ k | x 3 ∈ X 3 ,<label>(2.4b</label></formula><p>)</p><formula xml:id="formula_16">λ k+1 = λ k -β A 1 x k 1 + A 2 x k+1 2 + A 3 x k+1 3 -b , (2.4c) x k+1 1 = Argmin L A x 1 , x k+1 2 , x k+1 3 , λ k+1 | x 1 ∈ X 1 . (2.4d)</formula><p>According to (2.4), there is a update for the variable λ between the updates for x 3 and x 1 . Thus, the case A T  1 A 3 = 0 requires discussion different from that in the last subsection. Moreover, when x k 1 is taken as x k+1</p><p>1 and x k+1 1 as x k+2 1 , the scheme (2.4) reduces exactly to the direct extension of ADMM <ref type="bibr">(1.5)</ref>. Therefore, the convergence analysis for the scheme (1.5) is equivalent to that for (2.4). For notational simplicity, we will focus on the representation of (2.4) within this subsection.</p><p>Second, it worths to mention that the variable x 2 is not involved in the iteration of (2.4), meaning the scheme (2.4) generating a new iterate only based on (x k 1 , x k 3 , λ k ). We thus follow the terminology in <ref type="bibr" target="#b2">[3]</ref> to call x 2 an intermediate variable; and correspondingly call (x 1 , x 3 , λ) essential variables because they are really necessary to execute the iteration of <ref type="bibr">(2.4)</ref>. Accordingly, we use the notations</p><formula xml:id="formula_17">w k = (x k 1 , x k 2 , x k 3 , λ k ), u k = w k \λ k = (x k 1 , x k 2 , x k 3 ), v k = w k \x k 2 = (x k 1 , x k 3 , λ k ), v = w\x 2 = (x 1 , x 3 , λ), V = X 1 × X 3 × p and V * := {v * = (x * 1 , x * 3 , λ * ) | w * = (x * 1 , x * 2 , x * 3 , λ * ) ∈ * },</formula><p>where * is the collection of the KKT points of (1.1). Third, it is useful to characterize the model (1.1) by a variational inequality. More specifically, finding a saddle point of the Lagrangian function of (1.1) is equivalent to solving the variational inequality problem: Finding w * ∈ such that</p><formula xml:id="formula_18">VI( , F, θ) : θ(u) -θ(u * ) + (w -w * ) T F(w * ) ≥ 0, ∀ w ∈ , (2.5a)</formula><p>where</p><formula xml:id="formula_19">u := ⎛ ⎝ x 1 x 2 x 3 ⎞ ⎠ , w := ⎛ ⎜ ⎜ ⎝ x 1 x 2 x 3 λ ⎞ ⎟ ⎟ ⎠ , θ(u) := θ 1 (x 1 )+θ 2 (x 2 )+θ 3 (x 3 ), (2.5b) F(w) := ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ -A T 1 λ -A T 2 λ -A T 3 λ A 1 x 1 + A 2 x 2 + A 3 x 3 -b ⎞ ⎟ ⎟ ⎟ ⎟ ⎠</formula><p>, and</p><formula xml:id="formula_20">:= X 1 ×X 2 ×X 3 × p . (2.5c)</formula><p>Obviously, the mapping F(•) defined in (2.5c) is monotone because it is affine with a skew-symmetric matrix. Last, let us take a deeper look at the output of (2.4) and investigate some of its properties. In fact, deriving the first-order optimality condition of the minimization problems in (2.4) and rewriting (2.4c) appropriately, we obtain</p><formula xml:id="formula_21">⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ θ 2 (x 2 )-θ 2 x k+1 2 + x 2 -x k+1 2 T -A T 2 λ k -β A 1 x k 1 + A 2 x k+1 2 + A 3 x k 3 -b ≥ 0, ∀x 2 ∈ X 2 ,</formula><p>(2.6a)</p><formula xml:id="formula_22">θ 3 (x 3 )-θ 3 x k+1 3 + x 3 -x k+1 3 T -A T 3 λ k -β(A 1 x k 1 + A 2 x k+1 2 + A 3 x k+1 3 -b) ≥ 0, ∀x 3 ∈ X 3 ,</formula><p>(2.6b)</p><formula xml:id="formula_23">A 1 x k 1 + A 2 x k+1 2 + A 3 x k+1 3 -b + 1 β λ k+1 -λ k = 0,</formula><p>(2.6c)</p><formula xml:id="formula_24">θ 1 (x 1 )-θ 1 x k 1 + x 1 -x k+1 1 T -A T 1 λ k+1 -β A 1 x k+1 1 + A 2 x k+1 2 + A 3 x k+1 3 -b ≥ 0, ∀x 1 ∈ X 1 .</formula><p>(2.6d) Then, substituting (2.6c) into (2.6a), (2.6b) and (2.6d); and using A T  1 A 3 = 0, we get</p><formula xml:id="formula_25">⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ θ 2 (x 2 ) -θ 2 x k+1 2 + x 2 -x k+1 2 T -A T 2 λ k+1 + β A T 2 A 3 x k 3 -x k+1 3 ≥ 0, ∀x 2 ∈ X 2 ,</formula><p>(2.7a)</p><formula xml:id="formula_26">θ 3 (x 3 ) -θ 3 x k+1 3 + x 3 -x k+1 3 T -A T 3 λ k+1 ≥ 0, ∀x 3 ∈ X 3 ,</formula><p>(2.7b)</p><formula xml:id="formula_27">A 1 x k+1 1 + A 2 x k+1 2 + A 3 x k+1 3 -b + A 1 x k 1 -x k+1 1 - 1 β λ k -λ k+1 = 0, (2.7c) θ 1 (x 1 ) -θ 1 x k+1 1 + x 1 -x k+1 1 T -A T 1 λ k+1 -β A T 1 A 1 x k 1 -x k+1 1 +A T 1 λ k -λ k+1 ≥ 0, ∀x 1 ∈ X 1 .</formula><p>(2.7d)</p><p>With the definitions of θ , F, , u k and v k , we can rewrite (2.7) as a compact form. We summarize it in the next lemma and omit its proof as it is just a compact reformulation of (2.7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 2.1 Let</head><formula xml:id="formula_28">w k+1 = (x k+1 1 , x k+1 2 , x k+1 3 , λ k+1 ) be generated by (2.4) from given v k = (x k 1 , x k 3 , λ k ).</formula><p>Then we have</p><formula xml:id="formula_29">w k+1 ∈ , θ(u) -θ u k+1 + w -w k+1 T × F(w k+1 ) + Q v k -v k+1 ≥ 0, ∀ w ∈ , (<label>2.8)</label></formula><p>where</p><formula xml:id="formula_30">Q = ⎛ ⎜ ⎜ ⎝ -β A T 1 A 1 0 A T 1 0 β A T 2 A 3 0 0 0 0 A 1 0 -1 β I ⎞ ⎟ ⎟ ⎠ .</formula><p>(2.9)</p><p>Note that the assertion (2.8) is useful for quantifying the accuracy of w k+1 to a solution point of VI( , F, θ), because of the variational inequality reformulation (2.5) of (1.1). Now, we are ready to prove the convergence for the direct extension of ADMM under the condition A T  1 A 3 = 0. We first refine the assertion (2.8) under this additional condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 2.2 Let</head><formula xml:id="formula_31">w k+1 = (x k 1 , x k+1 2 , x k+1 3 , λ k+1 ) be generated by (2.4) from given v k = (x k 1 , x k 3 , λ k ). If A T 1 A 3 = 0, then we have w k+1 ∈ , θ(u) -θ u k+1 + w -w k+1 T F w k+1 + β P A 3 x k 3 -x k+1 3 ≥ v -v k+1 T H v k -v k+1 , ∀ w ∈ , (<label>2.10)</label></formula><p>where</p><formula xml:id="formula_32">P = ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ A T 1 A T 2 A T 3 0 ⎞ ⎟ ⎟ ⎟ ⎟ ⎠ , v = ⎛ ⎝ x 1 x 3 λ ⎞ ⎠ and H = ⎛ ⎝ β A T 1 A 1 0 -A T 1 0 β A T 3 A 3 0 -A 1 0 1 β I ⎞ ⎠ . (2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11)</head><p>Proof Since A T 1 A 3 = 0, the following is an identity:</p><formula xml:id="formula_33">⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ x 1 -x k+1 1 x 2 -x k+1 2 x 3 -x k+1 3 λ -λ k+1 ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ T ⎛ ⎜ ⎜ ⎝ β A T 1 A 1 β A T 1 A 3 -A T 1 0 0 0 0 β A T 3 A 3 0 -A 1 0 1 β I ⎞ ⎟ ⎟ ⎠ ⎛ ⎜ ⎜ ⎝ x k 1 -x k+1 1 x k 3 -x k+1 3 λ k -λ k+1 ⎞ ⎟ ⎟ ⎠ = ⎛ ⎜ ⎜ ⎝ x 1 -x k+1 1 x 3 -x k+1 3 λ -λ k+1 ⎞ ⎟ ⎟ ⎠ T ⎛ ⎝ β A T 1 A 1 0 -A T 1 0 β A T 3 A 3 0 -A 1 0 1 β I ⎞ ⎠ ⎛ ⎜ ⎜ ⎝ x k 1 -x k+1 1 x k 3 -x k+1 3 λ k -λ k+1 ⎞ ⎟ ⎟ ⎠ .</formula><p>Adding the above identity to the both sides of (2.8) and using the notations of v and H , we obtain</p><formula xml:id="formula_34">w k+1 ∈ , θ(u) -θ u k+1 + w -w k+1 T F w k+1 + Q 0 v k -v k+1 ≥ v -v k+1 T H v k -v k+1 , ∀w ∈ , (<label>2.12)</label></formula><p>where (see Q in (2.9))</p><formula xml:id="formula_35">Q 0 = Q + ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ β A T 1 A 1 β A T 1 A 3 -A T 1 0 0 0 0 β A T 3 A 3 0 -A 1 0 1 β I ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ = ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ 0 β A T 1 A 3 0 0 β A T 2 A 3 0 0 β A T 3 A 3 0 0 0 0 ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠</formula><p>.</p><p>By using the structures of the matrices Q 0 and P (see <ref type="bibr">(2.11</ref>)), and the vector v, we have</p><formula xml:id="formula_36">(w -w k+1 ) T Q 0 (v k -v k+1 ) = (w -w k+1 ) T β P A 3 (x k 3 -x k+1 3 ).</formula><p>The assertion (2.10) is proved.</p><p>Let us define two auxiliary sequences which will only serve for simplifying our notation in convergence analysis:</p><formula xml:id="formula_37">wk = ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ xk 1 xk 2 xk 3 λk ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ = ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ x k+1 1 x k+1 2 x k+1 3 λ k+1 -β A 3 (x k 3 -x k+1 3 ) ⎞ ⎟ ⎟ ⎟ ⎟ ⎠ and ũk = ⎛ ⎜ ⎜ ⎝ xk 1 xk 2 xk 3 ⎞ ⎟ ⎟ ⎠ , (<label>2.13)</label></formula><p>where</p><formula xml:id="formula_38">(x k+1 1 , x k+1 2 , x k+1 3 , λ k+1</formula><p>) is generated by (2.4). In the next lemma, we establish an important inequality based on the assertion in Lemma 2.2, which will play a vital role in convergence analysis.</p><formula xml:id="formula_39">Lemma 2.3 Let w k+1 = (x k+1 1 , x k+1 2 , x k+1 3 , λ k+1 ) be generated by (2.4) from given v k = (x k 1 , x k 3 , λ k ). If A T 1 A 3 = 0, we have wk ∈ and θ(u) -θ( ũk ) + (w -wk ) T F( wk ) ≥ 1 2 v -v k+1 2 H -v -v k 2 H + 1 2 v k -v k+1 2 H , ∀ w ∈ , (2.14)</formula><p>where wk and ũk are defined in <ref type="bibr">(2.13)</ref>.</p><p>Proof According to the definition of wk and F(w) (see <ref type="bibr">(2.13</ref>) and (2.5c), respectively), (2.10) can be rewritten as</p><formula xml:id="formula_40">wk ∈ , θ(u) -θ( ũk ) + (w -w k+1 ) T F( wk ) ≥ (v -v k+1 ) T H (v k -v k+1 ), ∀w ∈ . (2.15) Note that w k+1 -wk = β ⎛ ⎜ ⎜ ⎝ 0 0 0 A 3 (x k 3 -x k+1 3 ) ⎞ ⎟ ⎟ ⎠ ,</formula><p>we further obtain that wk ∈ , and</p><formula xml:id="formula_41">θ(u) -θ( ũk ) + (w -wk ) T F( wk ) = θ(u) -θ( ũk ) + (w -w k+1 ) T F( wk ) + (w k+1 -wk ) T F( wk ) ≥ (v -v k+1 ) T H (v k -v k+1 ) + A 3 (x k 3 -x k+1 3 ) T β(A 1 x k+1 1 + A 2 x k+1 2 + A 3 x k+1 3 -b) , ∀w ∈ . (2.16) Setting x 3 = x k 3 in (2.7b), we obtain θ 3 (x k 3 ) -θ 3 (x k+1 3 ) + (x k 3 -x k+1 3 ) T {-A T 3 λ k+1 } ≥ 0.</formula><p>(2.17)</p><p>Note that (2.7b) is also true for the (k -1)th iteration. Thus, it holds that</p><formula xml:id="formula_42">θ 3 (x 3 ) -θ 3 x k 3 + x 3 -x k 3 T -A T 3 λ k ≥ 0. Setting x 3 = x k+1</formula><p>3 in the last inequality, we obtain</p><formula xml:id="formula_43">θ 3 x k+1 3 -θ 3 x k 3 + x k+1 3 -x k 3 T -A T 3 λ k ≥ 0,<label>(2.18)</label></formula><p>which together with (2.17) yields that</p><formula xml:id="formula_44">λ k -λ k+1 T A 3 x k 3 -x k+1 3 ≥ 0, ∀k ≥ 0. (2.19) By using the fact λ k -λ k+1 = β(A 1 x k 1 + A 2 x k+1 2 + A 3 x k+1 3 -b) (see (2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6c)) and the assumption A T</head><p>1 A 3 = 0, we get immediately that</p><formula xml:id="formula_45">β A 1 x k+1 1 + A 2 x k+1 2 + A 3 x k+1 3 -b T A 3 x k 3 -x k+1 3 ≥ 0,<label>(2.20)</label></formula><p>and hence</p><formula xml:id="formula_46">wk ∈ , θ(u)-θ ũk + w-wk T F wk ≥ v-v k+1 T H v k -v k+1 , ∀ w ∈ .</formula><p>(2.21) By substituting the identity</p><formula xml:id="formula_47">v -v k+1 T H v k -v k+1 = 1 2 v -v k+1 2 H -v -v k 2 H + 1 2 v k -v k+1 2</formula><p>H into the right-hand side of (2.21), we obtain (2.14). Now, we are able to establish the contraction property with respect to the solution set of VI( , F, θ) for the sequence {v k } generated by (2.4), from which the convergence of (2.4) can be easily established.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 2.4 Assume A T</head><p>1</p><formula xml:id="formula_48">A 3 = 0 for the model (1.1). Let {x k 1 , x k 2 , x k 3 , λ k }</formula><p>be the sequence generated by the direct extension of ADMM <ref type="bibr">(2.4)</ref>. Then, we have:</p><formula xml:id="formula_49">(i) The sequence {v k := (x k 1 , x k 3 , λ k )} is contractive with respective to the solution of VI( , F, θ), i.e., v k+1 -v * 2 H ≤ v k -v * 2 H -v k -v k+1 2 H . (2.22) (ii) If the matrices [A 1 , A 2 ]</formula><p>and A 3 are assumed to be full column rank, then the sequence {w k } converges to a KKT point of the model (1.1).</p><p>Proof (i) The first assertion is straightforward based on <ref type="bibr">(2.14)</ref>. Setting w = w * in (2.14), we get</p><formula xml:id="formula_50">1 2 v k -v * 2 H -v k+1 -v * 2 H - 1 2 v k -v k+1 2 H ≥ θ( ũk )-θ(u * )+( wk -w * ) T F( wk ).</formula><p>From the monotonicity of F and (2.5), it follows that </p><formula xml:id="formula_51">θ( ũk ) -θ(u * ) + ( wk -w * ) T F( wk ) ≥ θ( ũk ) -θ(u * ) + ( wk -w * ) T F(w * ) ≥ 0,</formula><formula xml:id="formula_52">v k -v k+1 2 H = β A 1 (x k 1 -x k+1 1 ) - 1 β (λ k -λ k+1 ) 2 + β A 3 (x k 3 -x k+1 3 ) 2 ,</formula><p>it follows that the sequences {A 1 x k 1 -1 β λ k } and {A 3 x k 3 } are both bounded. Since A 3 has full column rank, we deduce that {x k 3 } is bounded. Note that</p><formula xml:id="formula_53">A 1 x k 1 + A 2 x k 2 = A 1 x k 1 - 1 β λ k -A 1 x k-1 1 - 1 β λ k-1 -A 3 x k 3 + b. (2.23) Hence, {A 1 x k 1 + A 2 x k 2 } is bounded.</formula><p>Together with the assumption that [A 1 , A 2 ] has full column rank, we conclude that the sequences {x k 1 }, {x k 2 } and {λ k } are all bounded. Therefore, there exists a subsequence</p><formula xml:id="formula_54">{x n k +1 1 , x n k +1 2 , x n k +1 3 , λ n k +1 } that converges to a limit point, say (x ∞ 1 , x ∞ 2 , x ∞ 3 , λ ∞ ).</formula><p>Moreover, from (2.22), we see immediately that</p><formula xml:id="formula_55">∞ k=1 v k -v k+1 2 H &lt; +∞,<label>(2.24)</label></formula><p>which shows lim</p><formula xml:id="formula_56">k→∞ H v k -v k+1 = 0,<label>(2.25)</label></formula><p>and thus lim</p><formula xml:id="formula_57">k→∞ Q v k -v k+1 = 0. (2.26)</formula><p>Then, by taking the limits on the both sides of (2.8), using (2.26), one can immediately write</p><formula xml:id="formula_58">w ∞ ∈ , θ(u) -θ(u ∞ ) + (w -w ∞ ) T F(w ∞ ) ≥ 0, ∀ w ∈ , (2.27) which means w ∞ = (x ∞ 1 , x ∞ 2 , x ∞ 3 , λ ∞ ) is a KKT point of (1.1). Hence, the inequality (2.22) is also valid if (x * 1 , x * 2 , x * 3 , λ * ) is replaced by (x ∞ 1 , x ∞ 2 , x ∞ 3 , λ ∞ ). Then it holds that v k+1 -v ∞ 2 H ≤ v k -v ∞ 2 H , (2.28) which implies that lim k→∞ A 1 (x k 1 -x ∞ 1 ) - 1 β (λ k -λ ∞ ) = 0, lim k→∞ A 3 (x k 3 -x ∞ 3 ) = 0. (2.29)</formula><p>By taking limits to (2.23), using (2.29) and the assumptions, we know</p><formula xml:id="formula_59">lim k→∞ x k 1 = x ∞ 1 , lim k→∞ x k 2 = x ∞ 2 , lim k→∞ x k 3 = x ∞ 3 , lim k→∞ λ k = λ ∞ . (2.30)</formula><p>which completes the proof of this theorem.</p><p>Inspired by <ref type="bibr" target="#b17">[18]</ref>, we can also establish a worst-case convergence rate measured by the iteration complexity in the ergodic sense for the direct extension of ADMM <ref type="bibr">(2.4)</ref>. This is summarized in the following theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 2.5 Assume A T</head><p>1</p><formula xml:id="formula_60">A 3 = 0 for the model (1.1). Let {(x k 1 , x k 2 , x k 3 , λ k</formula><p>)} be the sequence generated by the direct extension of ADMM (2.4) and wk be defined in (2.13). After t iterations of (2.4), we take</p><formula xml:id="formula_61">wt = 1 t + 1 t k=0</formula><p>wk .</p><p>(2.31)</p><p>Then, w ∈ W and it satisfies</p><formula xml:id="formula_62">θ( ũt ) -θ(u) + ( wt -w) T F(w) ≤ 1 2(t + 1) v -v 0 2 H , ∀w ∈ . (2.32)</formula><p>Proof By the monotonicity of F and (2.14), it follows that</p><formula xml:id="formula_63">wk ∈ , θ(u) -θ( ũk ) + (w -wk ) T F(w) + 1 2 v -v k 2 H ≥ 1 2 v -v k+1 2 H , ∀w ∈ . (2.33)</formula><p>Together with the convexity of X 1 , X 2 and X 3 , (2.31) implies that wt ∈ . Summing the inequality (2.33) over k = 0, 1, . . . , t, we obtain</p><formula xml:id="formula_64">(t + 1)θ (u)- t k=0 θ( ũk )+ (t + 1)w- t k=0 wk T F(w)+ 1 2 v -v 0 2 H ≥ 0, ∀w ∈ .</formula><p>Use the notation of wt , it can be written as</p><formula xml:id="formula_65">1 t + 1 t k=0 θ( ũk ) -θ(u) + ( wt -w) T F(w) ≤ 1 2(t + 1) v -v 0 2 H , ∀w ∈ .</formula><p>(2.34)</p><p>Since θ(u) is convex and</p><formula xml:id="formula_66">ũt = 1 t + 1 t k=0 ũk , we have that θ( ũt ) ≤ 1 t + 1 t k=0 θ( ũk ).</formula><p>Substituting it into (2.34), the assertion of this theorem follows directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 2.6 For an arbitrarily given compact set</head><formula xml:id="formula_67">D ⊂ , let d = sup{ v-v 0 2 H } | v = w \ x 2 , w ∈ D}, where v 0 = (x 0 1 , x 0 3 , λ 0 ).</formula><p>Then, after t iterations of the extended ADMM (2.4) , the point wt defined in (2.31) satisfies</p><formula xml:id="formula_68">sup θ( ũt ) -θ(u) + ( wt -w) T F(w) ≤ d 2(t + 1) , (<label>2.35)</label></formula><p>which, according to the definition (2.5), means wt is an approximate solution of VI( , F, θ) with an accuracy of O(1/t). Thus a worst-case O(1/t) convergence rate in the ergodic sense is established for the direct extension of ADMM (2.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">An example showing the divergence of (1.5)</head><p>In the last section, we have shown that if it is additionally assumed that any two coefficient matrices in (1.1) be orthogonal, then the direct extension of ADMM (1.5) is convergent. Based on this study, we now give an example to show the divergence of (1.5) when such an orthogonality condition is missing. The analyses below also present a strategy for constructing more such examples. More specifically, we consider the following linear homogeneous equation with three variables:</p><formula xml:id="formula_69">A 1 x 1 + A 2 x 2 + A 3 x 3 = 0,<label>(3.1)</label></formula><p>where A i ∈ 3 (i = 1, 2, 3) are all column vectors and the matrix [A 1 , A 2 , A 3 ] is assumed to be nonsingular; and x i ∈ (i = 1, 2, 3). The unique solution of (3.1) is thus x 1 = x 2 = x 3 = 0. Clearly, (3.1) is a special case of (1.1) where the objective function is null, b is the all-zero vector in 3 , and X i = for i = 1, 2, 3. The direct extension of ADMM (1.5) is thus applicable to (3.1), and the corresponding optimal Lagrange multipliers are all 0. One will see next that the convergence of the direct extension of ADMM (1.5) applied to solving the linear equations with a null objective is independent of the selection of the penalty parameter β. That is, if the direct extension of ADMM (1.5) is convergent for a selected β &gt; 0, then it is convergent for every β &gt; 0. On the other hand, if (1.5) is not convergent for one selected β &gt; 0, then it is not convergent for any β &gt; 0. Hence, in our specific example to be developed below, one can think β = 1 without loss of generality.</p><p>3.1 The iterative scheme of (1.5) for (3.1) Now, we elucidate the iterative scheme when the direct extension of ADMM (1.5) is applied to solve the linear equation (3.1). In fact, as we will show, it can be represented as a matrix recursion.</p><p>Specifying the scheme (1.5) with any given β &gt; 0 by the particular setting in (3.1), we obtain</p><formula xml:id="formula_70">⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ -A T 1 λ k + β A T 1 A 1 x k+1 1 + A 2 x k 2 + A 3 x k 3 = 0, (3.2a) -A T 2 λ k + β A T 2 A 1 x k+1 1 + A 2 x k+1 2 + A 3 x k 3 = 0, (3.2b) -A T 3 λ k + β A T 3 A 1 x k+1 1 + A 2 x k+1 2 + A 3 x k+1 3 = 0, (3.2c) β A 1 x k+1 1 + A 2 x k+1 2 + A 3 x k+1 3 + λ k+1 -λ k = 0. (3.2d)</formula><p>By introducing a new variable μ k := λ k /β, we can recast the scheme (3.2) as</p><formula xml:id="formula_71">⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ -A T 1 μ k + A T 1 A 1 x k+1 1 + A 2 x k 2 + A 3 x k 3 = 0,<label>(3.3a)</label></formula><formula xml:id="formula_72">-A T 2 μ k + A T 2 A 1 x k+1 1 + A 2 x k+1 2 + A 3 x k 3 = 0, (3.3b) -A T 3 μ k + A T 3 A 1 x k+1 1 + A 2 x k+1 2 + A 3 x k+1 3 = 0,<label>(3.3c</label></formula><p>)</p><formula xml:id="formula_73">A 1 x k+1 1 + A 2 x k+1 2 + A 3 x k+1 3 + μ k+1 -μ k = 0. (3.3d)</formula><p>It follows from the first equation in (3.3) that </p><formula xml:id="formula_74">x k+1 1 = 1 A T 1 A 1 -A T 1 A 2 x k 2 -A T 1 A 3 x k 3 + A T 1 μ k . (<label>3</label></formula><formula xml:id="formula_75">⎛ ⎜ ⎝ A T 2 A 2 0 0 1×3 A T 3 A 2 A T 3 A 3 0 1×3 A 2 A 3 I 3×3 ⎞ ⎟ ⎠ ⎛ ⎜ ⎝ x k+1 2 x k+1 3 μ k+1 ⎞ ⎟ ⎠ = ⎡ ⎢ ⎣ ⎛ ⎜ ⎝ 0 -A T 2 A 3 A T 2 0 0 A T 3 0 3×1 0 3×1 I 3×3 ⎞ ⎟ ⎠ - 1 A T 1 A 1 ⎛ ⎜ ⎝ A T 2 A 1 A T 3 A 1 A 1 ⎞ ⎟ ⎠ -A T 1 A 2 , -A T 1 A 3 , A T 1 ⎤ ⎥ ⎦ ⎛ ⎜ ⎝ x k 2 x k μ k ⎞ ⎟ ⎠ . (3.5) Let L = ⎛ ⎜ ⎜ ⎝ A T 2 A 2 0 0 1×3 A T 3 A 2 A T 3 A 3 0 1×3 A 2 A 3 I 3×3 ⎞ ⎟ ⎟ ⎠ (3.6) and R = ⎛ ⎜ ⎝ 0 -A T 2 A 3 A T 2 0 0 A T 3 0 3×1 0 3×1 I 3×3 ⎞ ⎟ ⎠ - 1 A T 1 A 1 ⎛ ⎜ ⎝ A T 2 A 1 A T 3 A 1 A 1 ⎞ ⎟ ⎠ -A T 1 A 2 , -A T 1 A 3 , A T 1 . (3.7)</formula><p>Then the iterative formula (3.5) can be rewritten in the following fixed matrix mappings:</p><formula xml:id="formula_76">⎛ ⎜ ⎜ ⎝ x k+1 2 x k+1 3 μ k+1 ⎞ ⎟ ⎟ ⎠ = M ⎛ ⎜ ⎜ ⎝ x k 2 x k 3 μ k ⎞ ⎟ ⎟ ⎠ = • • • = M k+1 ⎛ ⎜ ⎜ ⎝ x 0 2 x 0 3 μ 0 ⎞ ⎟ ⎟ ⎠<label>(3.8)</label></formula><p>with</p><formula xml:id="formula_77">M = L -1 R. (3.9)</formula><p>Therefore, the direct extension of ADMM (1.5) is convergent for any starting point if the matrix mapping is a contraction, or equivalently, the spectral radius of M, denote by ρ(M), is strictly less than 1. Thus, to construct a divergent example, we would look for a A such that ρ(M) &gt; 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">A concrete example showing the divergence of (1.5)</head><p>Now we are ready to construct a concrete example to show the divergence of the direct extension of ADMM (1.5) for all β &gt; 0 when it is applied to solve the model (3.1).</p><p>Our previous analysis in Sect. 2 has shown that the scheme (1.5) is convergent whenever any two coefficient matrices are orthogonal. Thus, to show the divergence of (1.5) for (3.1), the columns A 1 , A 2 and A 3 in (3.1) should be chosen such that any two of them are non-orthogonal.</p><p>Specifically, we thus construct the matrix A as follows:</p><formula xml:id="formula_78">A = (A 1 , A 2 , A 3 ) = ⎛ ⎝ 1 1 1 1 1 2 1 2 2 ⎞ ⎠ . (<label>3</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.10)</head><p>Given this matrix A, the system of linear equations (3.5) can be specified as</p><formula xml:id="formula_79">⎛ ⎜ ⎜ ⎜ ⎜ ⎝ 6 0 0 0 0 7 9 0 0 0 1 1 1 0 0 1 2 0 1 0 2 2 0 0 1 ⎞ ⎟ ⎟ ⎟ ⎟ ⎠ ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ x k+1 2 x k+1 3 μ k+1 1 μ k+1 2 μ k+1 3 ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ = ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ 0 -7 1 1 2 0 0 1 2 2 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 ⎞ ⎟ ⎟ ⎟ ⎟ ⎠ - 1 3 ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ 4 5 1 1 1 ⎞ ⎟ ⎟ ⎟ ⎟ ⎠ -4, -5, 1, 1, 1 ⎤ ⎥ ⎥ ⎥ ⎥ ⎦ ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ x k 2 x k 3 μ k 1 μ k 2 μ k 3 ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ .</formula><p>Note with the specification in (3.10), the matrices L in (3.6) and R in (3.7) reduce to</p><formula xml:id="formula_80">L = ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ 6 0 0 0 0 7 9 0 0 0 1 1 1 0 0 1 2 0 1 0 2 2 0 0 1 ⎞ ⎟ ⎟ ⎟ ⎟ ⎠ and R = 1 3 ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ 16 -1 -1 -1 2 20 25 -2 1 1 4 5 2 -1 -1 4 5 -1 2 -1 4 5 -1 -1 2 ⎞ ⎟ ⎟ ⎟ ⎟ ⎠ .</formula><p>Thus we have From direct computation, M admits the following eigenvalue decomposition An important fact regarding d defined above is that</p><formula xml:id="formula_81">M = L -1 R = 1 162 ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ 144 -9 -9 -9<label>18</label></formula><formula xml:id="formula_82">M = V Diag(d)V -1 , (<label>3</label></formula><formula xml:id="formula_83">ρ(M) = |d 1 | = |d 2 | = 1.0278 &gt; 1, from which we can construct a divergent sequence {(x k 2 , x k 3 , λ k 1 , λ k 2 , λ k</formula><p>3 )} starting from certain initial points. The questions are: Can we find real-valued non-convergent starting points? Does the set of non-convergent starting points form a continuously dense set, that is, are they not isolated? We give affirmative answers below.</p><p>Indeed, for any initial</p><formula xml:id="formula_84">(x 0 2 , x 0 3 , μ 0 1 , μ 0 2 , μ 0 3 ), let ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ l 1 l 2 l 3 l 4 l 5 ⎞ ⎟ ⎟ ⎟ ⎟ ⎠ = V -1 ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ x 0 2 x 0 3 μ 0 1 μ 0 2 μ 0 3 ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ . (<label>3.13)</label></formula><p>From (3.8) and (3.12), we know that</p><formula xml:id="formula_85">⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ x k 2 x k 3 μ k 1 μ k 2 μ k 3 ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ = V Diag(d k )V -1 ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ x 0 2 x 0 3 μ 0 1 μ 0 2 μ 0 3 ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ = V Diag(d k ) ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ l 1 l 2 l 3 l 4 l 5 ⎞ ⎟ ⎟ ⎟ ⎟ ⎠ = V ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ l 1 ( 0.9836 + 0.2984i ) k l 2 ( 0.9836 -0.2984i ) k l 3 ( 0.8744 + 0.2310i ) k l 4 ( 0.8744 -0.2310i ) k 0 ⎞ ⎟ ⎟ ⎟ ⎟ ⎠ ,</formula><p>Thus, as long as (l 1 l 2 ) = 0, the sequence would be divergent and there is no way for it to converge to a solution point of (3.1).</p><p>There are many choices of the starting point</p><formula xml:id="formula_86">(x 0 2 , x 0 3 , μ 0 1 , μ 0 2 , μ 0 3 ) such that (l 1 l 2 ) = 0. For example, ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ x 0 2 x 0 3 μ 0 1 μ 0 2 μ 0 3 ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ = V ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ α 1 α 1 α 2 α 2 α 3 ⎞ ⎟ ⎟ ⎟ ⎟ ⎠ , (<label>3.14)</label></formula><p>where α i are any real numbers and α 1 = 0 (which implies that l 1 = l 2 = α 1 = 0). Furthermore, it is clear that the pair of V (1) and V (2) are two complex conjugate vectors, so are the pair of V (3) and V (4), where V (i) denotes the i-th column of V . Thus the starting point of (3.14) is real-valued.</p><p>Since the vectors (α 1 , α 2 , α 3 ) ∈ 3 with α 1 &gt; 0 form a continuously dense half space, the non-convergent starting points given by (3.14) with α 1 &gt; 0 also form a continuously dense half space. Thus, we conclude the main result of this paper as follows.</p><p>Theorem 3.1 For the three-block convex minimization problem (1.1), there is an example where the direct extension of ADMM (1.5) is divergent for any penalty parameter β &gt; 0 and for any starting-point in a certain continuously dense half space of dimension 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 3.2</head><p>The linear homogeneous Eq. (3.1) with the matrix given in (3.10), whose feasible region is a singleton, is already sufficient to show the divergence of the direct extension of ADMM <ref type="bibr">(1.5)</ref>. In fact, we can construct more sophisticated examples to demonstrate the same divergence. For example, we consider the quadratic programming model</p><formula xml:id="formula_87">min 1 2 x 2 1 s.t. ⎛ ⎝ 1 1 1 1 1 1 ⎞ ⎠ x 1 x 2 + ⎛ ⎝ 1 1 2 ⎞ ⎠ x 3 + ⎛ ⎝ 1 2 2 ⎞ ⎠ x 4 = 0.<label>(3.15)</label></formula><p>Obviously, the feasible region of (3.15) is not a singleton. Following the procedure in Sect. 3, it is easy to show that applying the scheme (1.5) with any β &gt; 0 to (3.15), the resulting linear iterative mapping shares the same non-zero eigenvalues as those of the matrix given in <ref type="bibr">(3.11)</ref>. Therefore, the corresponding spectral radius of the matrix involved in the linear iterative mapping is still 1.0278. Hence, the model (3.15) also shows the divergence of the direct extension of ADMM (1.5) with β &gt; 0 and a certain starting point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Strongly convex case of (1.1)</head><p>When all functions θ i 's in (1.1) are further assumed to be strongly convex and the penalty parameter β is restricted into a specific range determined by all the strong convex modulus of these functions, the direct extension of ADMM (1.5) is convergent as proved in <ref type="bibr" target="#b13">[14]</ref>. Then, it is interesting to ask whether the scheme (1.5) for a strongly convex minimization model is still convergent when the restriction on β in <ref type="bibr" target="#b13">[14]</ref> is removed. In other words, does the strong convexity of the objective function help the convergence of the direct extension of ADMM for the three block convex minimization problem (1.1)? A by-product of this paper is a negative answer to the question. Recall that the requirement ρ(M) &gt; 1 yields the divergence of the direct extension of ADMM (1.5) when it is applied to solve (3.1). Consider the following strongly convex minimization problem with three variables:</p><formula xml:id="formula_88">min 0.05x 2 1 + 0.05x 2 2 + 0.05x 2 3 s.t. ⎛ ⎝ 1 1 1 1 1 2 1 2 2 ⎞ ⎠ ⎛ ⎝ x 1 x 2 x 3 ⎞ ⎠ = 0. (<label>4.1)</label></formula><p>One can verify that each iteration of the direct extension of ADMM (1.5) applied to the problem remains a fixed matrix mapping. Based on a simple calculation, it is seen that for (4.1), the spectral radius of the matrix involved in (1.5) with β = 1 is 1.0087. Thus, by a similar discussion to that in Sect. 3.2, one can find a proper starting point such that the direct extension of ADMM (1.5) with β = 1 is divergent. The detail is omitted for succinctness.</p><p>4.2 A revisit to the ADMM variant with a small step-size in <ref type="bibr" target="#b19">[20]</ref> To tackle the convergence ambiguity of the direct extension of ADMM (1.5), it was recently proposed in <ref type="bibr" target="#b19">[20]</ref> <ref type="foot" target="#foot_1">1</ref> to attach a relaxation factor γ &gt; 0 to the Lagrangemultiplier updating step (1.5d). That is, the step (1.5d) was changed to</p><formula xml:id="formula_89">λ k+1 = λ k -γβ A 1 x k+1 1 + A 2 x k+1 2 + A 3 x k+1 3 -b , (<label>4.2)</label></formula><p>where the "step-size" relaxation factor γ is required to be sufficiently small to ensure that a certain error-bound condition is satisfied. As proved in <ref type="bibr" target="#b19">[20]</ref>, this ADMM variant with a small step-size could be even linearly convergent provided that certain additional assumptions are posed on the model (1.1). Indeed, the sufficiently small requirement on γ plays a significant theoretical role in the convergence analysis in <ref type="bibr" target="#b19">[20]</ref>; and the analysis in <ref type="bibr" target="#b19">[20]</ref> requires to know some model-dependent data to determine the value of γ . In fact, how to relax the Lagrange-multiplier updating steps for ALM-based iterative schemes has been well investigated in the literature. For instance, when there is only one block of variable and function in the model (1.1), the ADMM (1.4) reduces to the standard ALM <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25]</ref>; and it has been demonstrated in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref> that the convergence can still be ensured if we attach a relaxation factor γ ∈ (0, 2) to the Lagrangemultiplier updating step of the ALM. The key point is the fact elucidated in <ref type="bibr" target="#b25">[26]</ref> that the ALM is indeed an application of the proximal point algorithm in <ref type="bibr" target="#b22">[23]</ref>; and thus the relaxation idea in <ref type="bibr" target="#b12">[13]</ref> is applicable. When the model (1.2) with two blocks of variable and function is considered, the convergence can be ensured if a relaxation factor γ ∈ (0, √</p><p>2 ) is attached to the Lagrange-multiplier updating step (1.4c) in the ADMM scheme <ref type="bibr">(1.4)</ref>. We refer the reader to <ref type="bibr" target="#b9">[10]</ref> for the analysis and <ref type="bibr" target="#b27">[28]</ref> for some numerical experiments. Indeed, as well demonstrated in the literature, sometimes the ALM and ADMM schemes can be numerically accelerated with a relaxation factor γ &gt; 1 in their Lagrange-multiplier updating steps. According to these facts about the ALM and ADMM, we see that the upper bound for the factor γ 's allowable range is reduced from 2 to</p><formula xml:id="formula_91">√ 5+1 2</formula><p>when the ALM is splitted as the ADMM, i.e., when the block of variable and function is increased from 1 to 2; but these upper bounds do not depend on any model-dependent data. It is thus interesting to ask whether we can find such a model-data-independent range for the γ in (4.2) so that the convergence of the ADMM variant with a small step-size proposed in <ref type="bibr" target="#b19">[20]</ref> can be guaranteed with any value of γ in this range.</p><p>Recall that we are considering three blocks of variable and function in the model (1.1); and the ALM subproblem at each iteration is splitted as three subproblems. So, the steps (1.5a)-(1.5c) represent an approximation to the corresponding augmented Lagrangian function but less accurate than the steps (1.4a)-(1.4b) of the ADMM. Hence, based on the just mentioned facts about the ALM and ADMM, it seems reasonable to expect that even if such a range exists, very likely its upper bound should be smaller than</p><formula xml:id="formula_92">√ 5+1<label>2</label></formula><p>(in fact, by the counter example developed earlier, it could be even smaller than 1). In the following, we will construct some examples to study this fact numerically. Indeed, our numerical results strongly suggest that such a constant upper bound does not even exist, though rigorous theoretical analysis lacks.</p><p>We still consider the linear equation example (3.1) but the matrix A is given by</p><formula xml:id="formula_93">A = ⎛ ⎝ 1 1 1 1 1 1+ α 1 1 + α 1 + α ⎞ ⎠ (4.3)</formula><p>where the positive scalar α &gt; 0. Thus, the matrix in (3.10) used to show the divergence of the direct extension of ADMM (1.5) is a special case of (4.3) with α = 1. Now, we consider implementing the ADMM variant with a small step-size in <ref type="bibr" target="#b19">[20]</ref> to the problem (3.1) where the matrix A is given in (4.3) and the penalty parameter β is fixed as 1. In particular, we take the relaxation factor γ in (4.2) exactly as the scalar α in (4.3), i.e., γ = α; and test the convergence when the value of α (and also γ ) varies from 1 to some extremely small values. Let M(α) be the corresponding matrix of the resulting linear iterative mapping. Recall that the divergence occurs with a certain initial point if the spectral radius of M(α), denoted by ρ(M(α)), is greater than 1. In Table <ref type="table" target="#tab_3">1</ref>, we report the numerical values of ρ(M(α)) for several choices of α.</p><p>It is observed from Table <ref type="table" target="#tab_3">1</ref> that for the example (3.1) where the matrix A is given in (4.3) with the tested values of α, the ADMM variant with a small step-size in <ref type="bibr" target="#b19">[20]</ref> is still divergent even if γ is as small as 1e -8. In other words, even if the value of γ in (4.2) is extremely small (e.g., γ = 1e -8), the example (3.1) with α = γ in the matrix A given by (4.3) already shows the divergence of the ADMM variant with a small step-size. This numerical study thus inspires us to conjecture that it is not possible to find a model-data-independent allowable range for the factor γ in (4.2) such that the convergence of the ADMM variant with a small step-size can be guaranteed for any value of γ in this range. This is a significant difference between the ADMM variant with a small step-size and the ALM and ADMM schemes (Recall that the model-dataindependent ranges γ ∈ (0, 2) and γ ∈ (0, √</p><p>2 ) exist for the ALM and ADMM, respectively). Accordingly, with this numerical study, the rationale of proposing some model-data-dependent conditions on the factor γ , such as the one in <ref type="bibr" target="#b19">[20]</ref>, to ensure the convergence of the direct extension of ADMM for solving a multi-block convex minimization model is also verified empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have shown by an example that the direct extension of the alternating direction method of multiplier (ADMM) is not necessarily convergent for solving a convex minimization model with linear constraints and a separable objective function with three function blocks.</p><p>We first study a condition that can sufficiently ensure the convergence of the direct extension of the ADMM. This new condition requires the orthogonality of the given coefficient matrices in the model; but poses no restriction on strong convexity on the objective functions in the model or range restriction on the penalty parameter in the algorithmic implementation. This sufficient condition is only of theoretical interest, because it is not easily satisfied by the known applications in the literature. But, the study of this condition essentially inspires the idea and roadmap to construct the main counter example to show the divergence of the direct extension of ADMM.</p><p>We then extend the analysis to some other cases, including the strongly convex case considered in <ref type="bibr" target="#b13">[14]</ref> and an ADMM variant with a small step-size proposed in <ref type="bibr" target="#b19">[20]</ref>. Our main counter example can be easily extended to show that even with strong convexity on the objective functions in the model (1.1), the direct extension of ADMM is again not necessarily convergent with some specific β &gt; 0. This is a complementary result to the conclusion in <ref type="bibr" target="#b13">[14]</ref>. Moreover, slightly extending the main counter example, we can numerically show that it seems not possible to find a model-data-independent allowable range for the relaxation factor γ in the Lagrange-multiplier updating step (4.2) such that the convergence of the direct extension of ADMM can be guaranteed for any value of γ in this range. This further justifies the rationale in <ref type="bibr" target="#b19">[20]</ref> of considering some model-data-dependent conditions on the factor γ . Besides, the result in this paper also justifies the rationale of algorithmic design in some recent work such as the strategy of combining certain correction steps with the output of the direct extension of ADMM in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref>, and the idea in <ref type="bibr" target="#b15">[16]</ref> which suggests interchanging the order of variable updating appropriately and employing the proximal regularization for some decomposed subproblems, in order to produce a splitting algorithm with provable convergence under mild assumptions for multi-block convex minimization models. It has been shown that these strategies work for tackling the lack of convergence of the direct extension of ADMM.</p><p>Finally, we would mention that although our discussion focuses on the model (1.1) where there are three variable and function blocks, our analysis can be easily extended to the more general case where the number of variable and function is greater than 3. More specifically, we consider the more general multi-block convex minimization model min m i=1 θ i (x i ) s.t. m i=1 A i x i = b, x i ∈ X i , <ref type="bibr">(5.1)</ref> where m &gt; 3, A i ∈ p×n i (i = 1, 2, . . . , m), b ∈ p , X i ⊂ n i (i = 1, 2, . . . , m) are closed convex sets; and θ i : n i → (i = 1, 2, . . . , m) are closed convex but not necessarily smooth functions. Then, to ensure the convergence of the direct extension of ADMM for (5.1), a sufficient condition analogous to that in Sect. 2 is: There exist two integers i and j such that any two matrices in the sets {A i , A i+1 , . . . , A i+ j } and {A i+ j+1 , A i+ j+2 , . . . , A m , A 1 , A 2 , . . . , A i-1 } are orthogonal. This is an easy extension of the condition in Sect. 2; and based on this condition, some examples similar as that in Sect. 3 can be easily found to show the divergence of the direct extension of ADMM for (5.1). We omit the detail for succinctness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>. 4 )</head><label>4</label><figDesc>Substituting (3.4) into (3.3b), (3.3c) and (3.3d), we obtain a reformulation of(3.3)    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Theorem 4 . 1</head><label>41</label><figDesc>For the model (1.1) with the strong convex assumption on its objective function, the direct extension of ADMM (1.5) is not necessarily convergent for all β &gt; 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and thus(2.22) is proved. Clearly,(2.22)  indicates that the sequence {v k } is contractive with respect to the solution set of VI( , F, θ), see e.g.<ref type="bibr" target="#b1">[2]</ref>.(ii) To prove (ii), by the inequality (2.<ref type="bibr" target="#b21">22</ref>) and (see the definitions of v and H in (2.11))</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc>Spectral radius of the implementation of the ADMM variant with a small step-size (β = 1)</figDesc><table><row><cell>α = γ</cell><cell>1</cell><cell>0 . 1</cell><cell>1 e -2</cell><cell>1 e -3</cell><cell cols="2">1 e -4 1e-5 1e-6 1e-7 1e-8</cell></row><row><cell cols="6">ρ(M(γ )) 1.027839 1.002637 1.000105 1.000004 &gt;1</cell><cell>&gt;1</cell><cell>&gt;1</cell><cell>&gt;1</cell><cell>&gt;1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>ExtensionsIn this section, we extend our previous analysis to some relevant work in the literature.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>A more general model with m block of functions and variables was considered in<ref type="bibr" target="#b19">[20]</ref>. But here, for the convenience of notation, we only focus on the model (1.1) with m = 3 and the analysis can be trivially extended to the general case with a generic m.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Chen: This author was supported in part by the Natural Science Foundation of Jiangsu Province under project Grant No. BK20130550 and the NSFC Grant 11401300 and 11371192. B. He: This author was supported by the NSFC Grant 91130007 and 11471156. Y. Ye: This author was supported by AFOSR Grant FA9550-12-1-0396. X. Yuan: This author was supported partially by the General Research Fund from Hong Kong Research Grants Council: 203613.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Constrained Optimization and Lagrange Multiplier Methods</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<publisher>Academic Press</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mathematische Optimierung. Grundlagen und Verfahren. Ökonometrie und Unternehmensforschung</title>
		<author>
			<persName><forename type="first">E</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Oettli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin-Heidelberg-New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="122" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Finite Element Approximation and Iterative Solution of a Class of Mildly Non-linear Elliptic Equations</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Glowinski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978">1978</date>
			<pubPlace>Stanford, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent variable graphical model selection via convex optimization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Parrilo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1935" to="1967" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Augmented Lagrangian and alternating direction methods for convex optimization: A tutorial and some illustrative computational results</title>
		<author>
			<persName><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>manuscript</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the Douglas-Rachford splitting method and the proximal point algorithm for maximal monotone operators</title>
		<author>
			<persName><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="293" to="318" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Augmented Lagrangian Methods: Applications to the Solution of Boundary Problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fortin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Glowinski</surname></persName>
		</author>
		<editor>Fortin, M., Glowinski, R.</editor>
		<imprint>
			<date type="published" when="1983">1983</date>
			<pubPlace>North-Holland; Amsterdam</pubPlace>
		</imprint>
	</monogr>
	<note>On decomposition-coordination methods using an augmented Lagrangian</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A dual algorithm for the solution of nonlinear variational problems via finite element approximations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gabay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mercier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Math. Appl</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="17" to="40" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Numerical Methods for Nonlinear Variational Problems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Glowinski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On alternating directon methods of multipliers: a historical perspective</title>
		<author>
			<persName><forename type="first">R</forename><surname>Glowinski</surname></persName>
		</author>
		<editor>J. Periaux</editor>
		<imprint>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Approximation par èlèments finis d&apos;ordre un et rèsolution par pènalisation-dualitè d&apos;une classe de problémes non linèaires</title>
		<author>
			<persName><forename type="first">R</forename><surname>Glowinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marrocco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">R.A.I.R.O. R</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="41" to="76" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modified Lagrangian in convex programming and their generalizations</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Gol'shtein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Tret'yakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program. Studies</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="86" to="97" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A note on the alternating direction method of multipliers</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">M</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Optim. Theory Appl</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="227" to="238" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Alternating direction method with Gaussian back substitution for separable convex programming</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">M</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="313" to="340" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A splitting method for separable convex programming</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">M</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IMA J. Numer. Anal</title>
		<imprint/>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convergence rate and iteration complexity on the alternating direction method of multipliers with a substitution procedure for separable convex programming</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">M</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Oper. Res</title>
		<imprint/>
	</monogr>
	<note>under revision</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the O(1/n) convergence rate of the Douglas-Rachford alternating direction method</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">M</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Num. Anal</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="700" to="709" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multiplier and gradient methods</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Hestenes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Optim. Theory Appl</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="303" to="320" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">On the linear convergence of the alternating direction method of multipliers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-08">August 2012</date>
		</imprint>
	</monogr>
	<note>manuscript</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Mclachlan</surname></persName>
		</author>
		<title level="m">Discriminant Analysis and Statistical Pattern Recognition</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley-Interscience</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">544</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Node-based learning of multiple gaussian graphical models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fazel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1303.5145</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Regularization d&apos;inequations variationelles par approximations successives</title>
		<author>
			<persName><forename type="first">B</forename><surname>Martinet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Revue Francaise d&apos;Informatique et de Recherche Opérationelle</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="154" to="159" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust alignment by sparse and low-rank decomposition for linearly correlated images</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">G</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intel</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2233" to="2246" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A method for nonlinear constraints in minimization problems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J D</forename><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimization</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Fletcher</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1969">1969</date>
			<biblScope unit="page" from="283" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Augmented Lagrangians and applications of the proximal point algorithm in convex programming</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Rockafellar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="97" to="116" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recovering low-rank and sparse components of matrices from incomplete and noisy observations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">M</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="57" to="81" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Alternating direction augmented lagrangian methods for semidefinite programming</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goldfarb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program. Comput</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="203" to="230" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
