<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DialogCC: Large-scale Multi-Modal Dialogue Dataset</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-12-08">8 Dec 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Young-Jun</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Byungsoo</forename><surname>Ko</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">NAVER Vision</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Han-Gyu</forename><surname>Kim</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">NAVER Clova Speech github.com/passing2961</orgName>
								<address>
									<country>DialogCC</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ho-Jin</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DialogCC: Large-scale Multi-Modal Dialogue Dataset</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-12-08">8 Dec 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2212.04119v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As sharing images in an instant message is a crucial factor, there has been active research on learning a imagetext multi-modal dialogue model. However, training a well-generalized multi-modal dialogue model is challenging because existing multi-modal dialogue datasets contain a small number of data, limited topics, and a restricted variety of images per dialogue. In this paper, we present a multi-modal dialogue dataset creation pipeline that involves matching large-scale images to dialogues based on CLIP similarity. Using this automatic pipeline, we propose a large-scale multi-modal dialogue dataset, DialogCC, which covers diverse real-world topics and various images per dialogue. With extensive experiments, we demonstrate that training a multi-modal dialogue model with our dataset can improve generalization performance. Additionally, existing models trained with our dataset achieve state-of-theart performance on image and text retrieval tasks. The source code and the dataset will be released after publication.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>People share various images with each other when communicating via instant messaging tools. Such behavior increases social bonding (rapport) as well as engagement. The ability to share images is also necessary for a dialogue model for better bonding conversations. In the visual dialogue domain, the majority of previous works have focused on image-grounded dialogues, where two persons talk about given images <ref type="bibr">[2,</ref><ref type="bibr">6,</ref><ref type="bibr">18,</ref><ref type="bibr">27,</ref><ref type="bibr">29,</ref><ref type="bibr">31,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b50">51]</ref>. In practical situations, humans actively share images during conversations rather than merely talking about a given image, which is called image sharing behavior <ref type="bibr" target="#b48">[49]</ref>. Recent studies for image sharing have proposed multi-modal dialogue datasets, which are constructed either manually by crowd-sourcing (PhotoChat <ref type="bibr" target="#b48">[49]</ref>) or automatically by utilizing image-text similarity (MMDD <ref type="bibr">[20]</ref>  1 ). 1 Mutli-Modal Dialogue Dataset. We compare our proposed dataset DialogCC with existing multi-modal dialogue datasets: MMDD <ref type="bibr">[20]</ref> and PhotoChat <ref type="bibr" target="#b48">[49]</ref>. Compared to the existing datasets, DialogCC contains large-scale dialogues and images while covering more diverse words and hypernyms. Moreover, learning with various images for the same dialogue or utterance in DialogCC would benefit a model to obtain better generalization.</p><p>However, existing multi-modal dialogue datasets have three significant limitations; (1) Scalability. Recent years have witnessed the success of large-scale multi-modal pretraining models <ref type="bibr">[1,</ref><ref type="bibr">12,</ref><ref type="bibr">17,</ref><ref type="bibr">26,</ref><ref type="bibr" target="#b31">32]</ref>, which benefited from the power of large-scale image-caption datasets <ref type="bibr">[4,</ref><ref type="bibr">7,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42]</ref>. Nevertheless, as shown in Figure <ref type="figure" target="#fig_0">1</ref> (# unique dialogs and # unique images), existing image-dialogue datasets <ref type="bibr">[20,</ref><ref type="bibr" target="#b48">49]</ref> contain a small number of dialogues and images that are limited to training a large-scale multi-modal dialogue model. (2) Diversity. A large-scale multi-modal dialogue model should be able to cover open domain conversation. However, as illustrated in Figure <ref type="figure" target="#fig_0">1</ref> (# unique words and # unique hypernyms), existing datasets cover a limited number of words, topics, and domains. Such lack of diversity can also be found in conversational skills (e.g., empathizing with situations <ref type="bibr">[15,</ref><ref type="bibr">24,</ref><ref type="bibr" target="#b32">33]</ref> and understanding personal information <ref type="bibr">[14,</ref><ref type="bibr" target="#b49">50]</ref>), which is an important factor in human conversation. Existing datasets cover only few conversational skills (see Section 3.2). (3) Generalization. Given the same dialogue and context, people can share different types of images. For example, for an utterance of "I love pets," one can share an image of a dog, and the other can share an image of a cat. Nonetheless, as shown in Figure <ref type="figure" target="#fig_0">1</ref> (# images / dialog and # images / utterances), existing datasets consist of less than the average 3 images per dialogue and the average 1.7 images per utterance. A model trained with such datasets can be overfitted by memorizing those pairs of images and dialogues, resulting in a lack of generalization.</p><p>The objective of this work is to create a large-scale multimodal dialogue dataset in order to train a well-generalized multi-modal dialogue model for open-domain conversation.</p><p>To this end, we present an automatic pipeline for creating a multi-modal dialogue dataset and propose a large-scale multi-modal dialogue dataset, DialogCC, created by the automatic pipeline. The pipeline consists of two main filtering steps: source data filtering and multi-modal dialogue filtering. These filtering steps eliminate inappropriate images from large-scale images and dialogues based on CLIP similarity. As illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, DialogCC achieves better statistics compared to the existing datasets in terms of scalability, diversity, and generalization. In addition, extensive experiments demonstrate that a model trained with DialogCC achieves state-of-the-art performance in image and text retrieval tasks with enhanced generalization performance.</p><p>In summary, our main contributions are as follows: 1) We present an automatic pipeline for creating a multi-modal dialogue dataset that can create a large-scale dataset without human effort. 2) We propose a large-scale multi-modal dialogue dataset named DialogCC, which contains diverse images and dialogues consisting of various images per dialogue. 3) Extensive experiments demonstrate the effectiveness of our dataset, which achieves state-of-the-art performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Image-Dialogue Dataset. In the visual dialogue domain, most previous studies are divided into two categories depending on whether the image is grounded or sharing in the dialogue. The image-grounded dialogue task aims to answer questions <ref type="bibr">[2,</ref><ref type="bibr">6,</ref><ref type="bibr">18,</ref><ref type="bibr" target="#b35">36]</ref> or generate natural conversations <ref type="bibr">[27,</ref><ref type="bibr">29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b50">51]</ref> about given images. These datasets require machines to perceive and understand the given images, but we sometimes share images relevant to dialogue contexts in daily conversations. Hence, it is difficult to train dialogue agents to retrieve an appropriate image based on dialogue contexts in image-grounded dialogue task.</p><p>Recently the image-sharing dialogue task has been proposed to overcome such limitation, which predicts images semantically relevant to given dialogue contexts. Since there were no existing datasets for image-sharing task, previous studies have focused on construction of the dataset. One of the existing datasets, named PhotoChat <ref type="bibr" target="#b48">[49]</ref>, is manually constructed through a crowd-sourcing platform with Open Image Dataset V4 <ref type="bibr">[19]</ref> as source images. This dataset can provide a high-quality dialogue dataset, but the manual construction is time-consuming and expensive, which makes it hard to be expanded to a large-scale dataset. Another line of work <ref type="bibr">[20]</ref> creates a 45k multi-modal dialogue dataset through an automatic pipeline composed of the Visual Semantic Reasoning Network <ref type="bibr">[21]</ref> (VSRN). They replace utterances with semantically relevant images based on the similarity between the image and utterance obtained from VSRN. They also remove contextually incoherent images based on the threshold obtained through human annotation. However, both datasets are small and cover limited image objects and topics, as demonstrated in Figure <ref type="figure" target="#fig_0">1</ref>. To this end, we construct a large-scale multi-modal dialogue dataset through the automatic pipeline.</p><p>Multi-Modal Dialogue Model. The multi-modal dialogue model is mainly categorized into retrieval and generative models. The retrieval model is to retrieve proper texts or images from the candidates given the dialogue contexts. The generative model is to generate responses given the dialogue contexts. For the retrieval model, most existing studies have adopted the dual encoder architecture consisting of a text encoder and image encoder <ref type="bibr">[20,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b48">49]</ref>. For the generative model, many works are based on the encoderdecoder architecture <ref type="bibr">[25,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b46">47]</ref>. In this paper, for fair comparisons, we evaluate our dataset by adopting the text retrieval model <ref type="bibr">[20]</ref> and image retrieval model <ref type="bibr" target="#b48">[49]</ref> as our baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-Modal Dialogue Dataset Creation</head><p>In this section, we propose DialogCC, which is a largescale multi-modal dialogue dataset. In order to construct DialogCC, we introduce an automatic pipeline, which consists of two steps: (1) source data filtering, (2) multi-modal dialogue filtering. Besides, we conduct a comprehensive analysis of our dataset with respect to scalability, diversity, and generalization by comparing two existing datasets, MMDD <ref type="bibr">[20]</ref> and PhotoChat <ref type="bibr" target="#b48">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">CLIP-based Automatic Pipeline</head><p>We present an automatic pipeline for constructing Di-alogCC. The key idea of our pipeline is considering two types of similarities: utterance-image similarity and utterance-caption similarity. The overall pipeline is illus-Figure <ref type="figure" target="#fig_15">2</ref>. An overview of the automatic pipeline for multi-modal dialogue dataset creation. We collect source text-only dialogues and images, which survived by thresholding based on the similarity score between two modalities obtained from the CLIP model. Next, we combine two similarity types for each computed by utterance-image and utterance-caption. We then remove unrelated images from the matched results. trated in Figure <ref type="figure" target="#fig_15">2</ref>. In the following part of this section, we provide details about DialogCC construction pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Source Data Filtering</head><p>Source Dialogue. As a source data, we collect five multiturn text-only dialogue datasets, which are publicly available online. Five dialogue datasets are Persona-Chat <ref type="bibr" target="#b49">[50]</ref>, EmpatheticDialogues <ref type="bibr" target="#b32">[33]</ref>, Wizard-of-Wikipedia <ref type="bibr">[8]</ref>, Dai-lyDialog <ref type="bibr">[22]</ref>, and BlendedSkillTalk <ref type="bibr" target="#b39">[40]</ref>. They are manually constructed via a crowd-sourcing platform, and each dataset is specialized in specific conversational skills. Persona-Chat dataset contains the ability to get to know each other based on given personal information. Empa-theticDialogues dataset contains the ability to understand and interpret interlocutors' emotional situations and express emotional reactions adequately. Wizard-of-Wikipedia contains the ability to generate specific responses using knowledge or topic. DailyDialog contains daily life conversations with aspects, such as emotion, topic, and dialog acts. Lastly, in the BlendedSkillTalk, multiple skills (i.e., persona, empathy and knowledge) are integrated into one conversation, as humans do. We incorporate five dialogue datasets into one large dialogue dataset by removing duplicated dialogues.</p><p>Source Image-Caption Pairs. We choose Conceptual Captions 3M <ref type="bibr" target="#b36">[37]</ref> (CC3M), which is widely used in multi-modal modeling <ref type="bibr">[26,</ref><ref type="bibr" target="#b42">43]</ref> and creating multi-modality dataset <ref type="bibr">[30]</ref>. We obtain 2,712,320 image-caption pairs for the training and validation set. The detailed collection process is described in Appendix. The duplicated images and images with image-caption similarity smaller than the threshold of 0.185 are filtered out. After the filtering, 2,440,485 image-caption pairs are obtained, which are divided into the training / validation / test set with a ratio of 5:1:1, resulting in 1.7M / 0.3M / 0.3M of unique images. Note that our pipeline can work with any image-caption datasets, such as Conceptual Captions 12M <ref type="bibr">[4]</ref> and Red-Caps <ref type="bibr">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Multi-Modal Dialogue Filtering</head><p>CLIP-based Similarity Calculation. In order to find images semantically relevant to given utterance, we should get meaningful textual and visual features through a multimodal feature extractor f (?). The previous work <ref type="bibr">[20]</ref> used a pre-trained Visual Semantic Reasoning Network <ref type="bibr">[21]</ref> as f (?). In this work, we leverage CLIP <ref type="bibr" target="#b31">[32]</ref> model as f (?),  <ref type="bibr">[20]</ref> vs. DialogCC. In the first and second rows, both datasets contain semantically relevant images to the given utterances, while our dataset contains more and various images with different views or objects (e.g., dog breeds or backgrounds). In the last two rows, unlike the MMDD, our dataset is highly relevant for a given utterance because of matching from large-scale and diverse source data and considering image captions. Images in the green box are from MMDD, images in the red box are from DialogCC, and the blue box is utterances. More examples are in the Appendix.</p><p>which is widely used in previous studies <ref type="bibr">[3,</ref><ref type="bibr">5,</ref><ref type="bibr">10,</ref><ref type="bibr">11]</ref> because of a well-generalized open-domain model. We first extract utterance feature vector (v u = f (u)), caption feature vector (v c = f (c)), and image feature vector (v i = f (i)). We then calculate the utterance-image similarity following <ref type="bibr">[20]</ref> by computing the cosine similarity of v u and v i .</p><p>Besides, in order to enhance the quality of utteranceimage matching by additionally adopting the information provided by image captions, we also calculate the utterance-caption similarity. The results from CLIP-based similarity calculation are shown in Figure <ref type="figure" target="#fig_1">3</ref>. For example, in the last two rows of Figure <ref type="figure" target="#fig_1">3</ref>, MMDD overlooks "hawaiian" or "play" in the utterances, which are important words, and only focuses on the words "dance" or "keyboard". This is because MMDD does not consider the image captions.</p><p>However, there is one problem that we have to consider about how to combine these two similarity types. As reported in <ref type="bibr">[23,</ref><ref type="bibr" target="#b40">41]</ref>, there is a phenomenon called modality gap in multi-modal modeling, where two different modalities (e.g., image and text) are separately distributed in shared embedding space. Such phenomenon causes scale differences between utterance-image and utterance-caption similarities, so combining them directly would be biased to the larger scaled similarity. To alleviate this problem, the z-score normalization is conducted on both types of similarities where the mean and standard deviation values for each similarity type are calculated using training set. The normalized similarities are linearly combined as follows:</p><formula xml:id="formula_0">S = ?f Z (s c (v u , v i )) + (1 -?)f Z (s c (v u , v c )) ,<label>(1)</label></formula><p>where s c (x, y) denotes the cosine similarity and f Z represents z-score normalization. In this paper, we set ? as 0.5 to reflect two similarities equally. During the utterance-image matching process, the similarity matrix S of size of N ? M is computed, where N and M are the number of utterances and images respectively.</p><p>Filtering. We have found out that there still exist unsuitable cases among the matched images found by CLIP-based similarity. To improve the quality of our dataset, we present two more simple filtering after similarity-based matching. Figure <ref type="figure">5</ref>. Ablation study on the threshold ?2. We show the effect of the current turn prediction performance on DialogCC according to the threshold ?2.</p><p>In the first filtering stage, images whose scores are lower than a threshold ? 1 , where ? 1 is the median value of all scores. Median is used instead of heuristically determined threshold so that our pipeline can be applied to arbitrary datasets. Besides, we have observed that certain images are frequently matched with many utterances. As shown in Figure <ref type="figure" target="#fig_2">4</ref>, the frequently matched images mostly contain general semantics (e.g., meme, questions), which goes along with general utterances, rather than object-centric or eventcentric semantics (e.g., "playing a soccer game"). This phenomenon can result in the overfitting of the model to such frequently matched images, which is harmful to the generalization performance. Therefore, we propose to filter out such unsuitable images based on the frequency of being matched. In our method, a relatively determined threshold ? 2 is used to filter out a specific ratio of images. For example, using the threshold of ? 2 = p25 denotes that only bottom 25% of images by frequency of being matched are included in the constructed dataset. We conduct an ablation study on the text retrieval performance (i.e., current turn prediction task) on our dataset by differentiating the relative determined threshold ? 2 as shown in Figure <ref type="figure">5</ref>. The ablation study shows that the value of ? 2 = p75 is the best for both the efficient training and performance. Unless otherwise noted, all experiments on DialogCC are conducted with ? 2 = p75. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Analysis of DialogCC</head><p>(1) Scalability: Table <ref type="table" target="#tab_0">1</ref> shows the overall statistics of Di-alogCC compared to existing datasets MMDD <ref type="bibr">[20]</ref> and PhotoChat <ref type="bibr" target="#b48">[49]</ref>. In general, DialogCC comprises 92k unique dialogues in total, which is roughly 3.7? and 7.7? larger than MMDD and PhotoChat, respectively. While the average dialogue turn are shorter than that of other datasets, the average length of utterances is longer. Especially, Di-alogCC contains 651k unique images, which is approximately 54? larger than MMDD and PhotoChat. Our dataset is large-scale in both the number of dialogue and images.</p><p>(2) Diversity: In Table <ref type="table">2</ref>, we compare the diversity of datasets with the number of unique hypernyms from Word-Net <ref type="bibr">[28]</ref> and words in dialogues and image captions. As WordNet covers nouns, verbs, adjectives, and adverbs, we only count nouns with filtering out the hypernyms appearing less than ten times. In dialogue, compared to PhotoChat and MMDD, our dataset includes 3.7? and 1.4? more hy- Table <ref type="table">2</ref>. Diversity comparison. We count the number of unique hypernyms from WordNet <ref type="bibr">[28]</ref> and words in dialogues and image captions. We filter out a hypernym if it appears less than ten times in both dialogues and image captions. # hyp and # word denote the number of hypernyms and the number of unique words, respectively. pernyms; 3.4? and 3.2? more words, which implies that our dataset covers more variety of open-domain topics. In image captions, compared to PhotoChat and MMDD, our dataset includes 2.4? and 21.2? more hypernyms; 2.0? and 10.5? more words. Such statistics of image captions shows the diversity of images.</p><p>Moreover, our dataset also shows better diversity in terms of conversational skills. Conversational skill is a general ability to lead a conversation, which includes empathetic listening, giving knowledgable responses, getting to know each other, talking about daily life topics, and blending all of these skills. As shown in Figure <ref type="figure" target="#fig_4">7</ref>, MMDD covers three types of conversational skills mainly related to persona, while our dataset contains five types of conversational skills without overly focusing on one particular skill. Those skills enable a multi-modal dialogue model trained from our dataset to create engaging, vivid, and various dialogues.</p><p>(3) Generalization: In real-life scenarios, people can share images with different styles, views, or objects for the same dialogue and context. However, as shown in Table <ref type="table" target="#tab_0">1</ref>, the existing datasets include few images per dialogue and utterance. This does not reflect real-life scenarios and can cause an overfitting problem by forcing a model to memorize the pairs of images and dialogues. To handle this problem, our dataset has many and various images per dialogue and utterance, which is shown in Figure <ref type="figure" target="#fig_1">3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To explore how our dataset affects both text and image retrieval tasks, we evaluate two baseline models: text retrieval model used to evaluate MMDD <ref type="bibr">[20]</ref> and image retrieval model used to evaluate PhotoChat <ref type="bibr" target="#b48">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Task Definition</head><p>We explain the formulation of two main tasks -text retrieval <ref type="bibr">[20]</ref> and image retrieval <ref type="bibr" target="#b48">[49]</ref>. In the text retrieval task, two different sub-tasks exist which are current turn prediction and next turn prediction. Let us assume that we have a multi-modal dialogue D = {(u j , i j , c j )} N 1 where N denotes the number of dialogue turns, and j = t is the turn that an image sharing behavior occurs. Then, each task is formulated as follows. Current turn prediction is to predict the current response at turn t given the dialogue history ({u j } t-1 1 ) and image i t . Next turn prediction is to predict the next utterance at turn t + 1 given the dialogue history ({u j } t 1 ) and image i t . Image retrieval is to retrieve relevant image at turn t given the dialogue history ({u j } t-1 1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Baseline Model</head><p>As a baseline, we choose three models -BM25, text retrieval, and image retrieval. Followings are brief descriptions of each baseline model and more detailed information is provided in Appendix. Table <ref type="table">3</ref>. Text retrieval performance. We compare the Recall@1 (%) performance of text retrieval model trained on DialogCC (when the maximum number of image is 10) and MMDD.</p><formula xml:id="formula_1">Models ? Task ? Current Turn Prediction Next Turn Prediction Eval ? MMDD DialogCC MMDD DialogCC Train ? R@1 R@1 R@1 R@1 BM25 - 3</formula><p>BM25 <ref type="bibr" target="#b33">[34]</ref> retrieves response for the text retrieval task and image for the image retrieval task using captions.</p><p>Text retrieval <ref type="bibr">[20]</ref> consists of a dialogue encoder, response encoder, image encoder, and multi-modal encoder. We use the sum module that adds two vectors element-wise for the multi-modal encoder.</p><p>Image retrieval <ref type="bibr" target="#b48">[49]</ref> has a dual-encoder structure which consists of a dialogue encoder, photo description encoder, and image encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Datasets</head><p>DialogCC (ours) is a large-scale multi-modal dialogue dataset created by the CLIP-based automatic pipeline described in Sec. 3. MMDD <ref type="bibr">[20]</ref> contains 45k multi-modal dialogues, where each utterance is replaced into a relevant image matched by their automatic pipeline.</p><p>PhotoChat <ref type="bibr" target="#b48">[49]</ref> contains 10k multi-modal dialogues, where the dialogue is constructed via a crowd-sourcing platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Implementation Details</head><p>We implement baseline models based on PyTorch Lightning. All experiments are conducted on two A100 GPUs (40GB). To accelerate the training time, we apply distributed training to baselines. We follow the hyperparameter settings similar to the previous works <ref type="bibr">[20,</ref><ref type="bibr" target="#b48">49]</ref>, which are described as follows:</p><p>Text retrieval. For the text encoder, we use the BERTbased architecture (12 layers, 12 attention heads, 768 dimensions, uncased version). For the image encoder, we use the ResNeXt-101 model (2048 dimensions). We use a negative log likelihood loss with in-batch negative samples, same as <ref type="bibr">[20,</ref><ref type="bibr" target="#b37">38]</ref>. In our experiment, we set the batch size to 32, the learning rate to 5e-5, and the gradient clipping value to 2.0. We use the Adam optimizer <ref type="bibr">[16]</ref>  Table <ref type="table">4</ref>. Image retrieval performance. We report the Recall@1(%) performance on PhotoChat and DialogCC datasets. with the hardest negatives, and we set the margin parameter to 0.2. We set the batch size to 128. We also use the Adam optimizer with initial learning rate to 5e-5 and decaying 0.1% at every 1,000 steps. We truncate the length of dialogue context and photo description longer than 128.</p><p>Training. Since our dataset contains several images per utterance, we randomly choose one image in each batch.</p><p>For the text retrieval, we do not update the parameter of the image encoder as it helps achieve the best performance on the text retrieval task <ref type="bibr">[20]</ref>). On the other hand, we update the parameters of all encoders in the image retrieval model. In the validation steps, for the memory efficiency and fast computation speed, we constitute the number of candidates as 100 for all retrieval tasks, which is the same setting in <ref type="bibr">[20]</ref>.</p><p>Inference. The settings of inference stage is almost the same as those of validation steps, except that the inference stage uses the entire test set as candidates rather than using only 100 of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Text Retrieval Performance</head><p>We conduct an experiment by differentiating training and evaluation datasets to observe whether our dataset can boost performance in text retrieval tasks. As shown in Table <ref type="table">3</ref>, the model trained on MMDD performs poorly when evaluated on our dataset, implying that MMDD cannot help the model to understand various forms of images with similar semantic information. On the other hand, the model trained with our DialogCC achieves the best performance than the model trained with MMDD in all text retrieval tasks. This result indicates that our dataset improves the performance</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Current Turn Prediction</head><p>Next Turn Prediction  in open-domain conversation, which is benefited from large scalability, diversity, and images per dialogue.</p><formula xml:id="formula_2">Train ? MMDD DialogCC MMDD DialogCC Aug ? R@1 (?) R@1 (?) R@1 (?) R@</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Image Retrieval Performance</head><p>We also observe that training the image retrieval model with our dataset achieves the best performance on Pho-toChat dataset, as shown in Table <ref type="table">4</ref>. However, the model trained on the PhotoChat dataset achieves lower performance when evaluated on DialogCC. This result indicates that our dataset also improves the performance in the image retrieval task, which is benefited from the largest number of images per dialogue and utterance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Effect of Maximum Number of Images</head><p>We conduct an experiment to verify if learning with multiple images per utterance would be beneficial to model performance. Thus, we further evaluate the text retrieval performance by varying the maximum number of images to 1, 5, and 10. As shown in Figure <ref type="figure" target="#fig_6">8</ref>, we confirm that the overall tendency of the model performance to increase the maximum number of images mostly increases across metrics (R@{1,5,10}). This demonstrates that showing various images per utterance enables the model to learn the semantic relationship between images and utterances rather than memorizing a pair of an image and an utterance, resulting in better generalization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Robustness on Image Augmentation</head><p>To become a more generalized text retrieval model, the model should keep its performance as much as possible, even if the input image is distorted. Thus, we evaluate the performance of models trained on MMDD and DialogCC on the augmented MMDD dataset. We distort the input image of MMDD with several augmentation techniques, such as shearing and blurring provided by the imgaug <ref type="bibr">[13]</ref>. In Table 5, the model trained with our dataset shows more robust performance even with input image variants, with a lower Table <ref type="table">6</ref>. Robustness comparisons on text augmentation. We show the degree of decreased Recall@1(%) performance of the image retrieval model compared to the baseline score.</p><p>performance reduction compared to the model trained on MMDD dataset. This result indicates that our dataset makes the model robust to the input image distortion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5">Robustness on Text Augmentation</head><p>To augment the dialogue, we replace randomly selected words (except stopwords) with synonyms <ref type="bibr" target="#b47">[48]</ref>. Table <ref type="table">6</ref> shows the performance of the model trained on our dataset are reduced less than before applying augmentation to input dialogue history. This results indicates that even if our dataset contains some noisy samples due to the automatic pipeline, the model trained on our dataset is more robust to the text distortion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present the automatic pipeline for creating a multi-modal dialogue dataset that involves filtering with CLIP similarity. We also propose a large-scale multimodal dialogue dataset, DialogCC, which is constructed by leveraging the automatic pipeline with five text-only dialogue datasets and an image-text pair CC3M dataset. In a comprehensive analysis, compared to existing datasets MMDD and PhotoChat, DialogCC contains a larger number of unique hypernyms, words, and conversational skills, which indicates better diversity. Moreover, our dataset consists of many and various images per dialogue that can be beneficial in model generalization performance. Extensive experiments demonstrate that a model trained with Di-alogCC achieves state-of-the-art performance in image and text retrieval tasks while increasing model robustness.</p><p>Societal Impact. As reported in <ref type="bibr" target="#b44">[45]</ref>, even if we give the gender-neutral query to CLIP <ref type="bibr" target="#b31">[32]</ref> model, the CLIP model sometimes retrieves images causing gender-bias issues. We are concerned that this problematic issue may exist in our dataset because we use the CLIP model to match relevant images to given utterances. For example, most utterances related to the "hair designer" usually match images of women cutting hair. Therefore, the image retrieval model trained on our dataset may sometimes retrieve biased images. We should consider this problem important when building a multimodal search model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Details of Multi-Modal Dialogue Dataset Creation A.1. Source Data Collection</head><p>This section describe how we collect the source dialogue and image-caption pairs used in creating DialogCC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 Source Dialogue Collection</head><p>We collect five text-only dialogue datasets (i.e., Wizardof-Wikipedia <ref type="bibr">[6]</ref>, Persona-Chat <ref type="bibr">[31]</ref>, EmpatheticDialogues <ref type="bibr">[20]</ref>, DailyDialog <ref type="bibr">[13]</ref>, and BlendedSkillTalk <ref type="bibr">[25]</ref>) through the ParlAI <ref type="bibr">[16]</ref> framework, which provides many dialogue datasets online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2 Source Image-Caption Collection</head><p>We download the Conceptual Captions 3M <ref type="bibr">[24]</ref> (CC3M) dataset in here <ref type="foot" target="#foot_0">1</ref> . Since the CC3M dataset provides image URLs, we download images using img2dataset<ref type="foot" target="#foot_1">2</ref> library, which is a helpful library for quick downloading large-scale images based on URLs. We store downloaded images as a webdataset<ref type="foot" target="#foot_2">3</ref> format for efficiently extracting visual features by the CLIP <ref type="bibr">[19]</ref> model. Note that because each image URL has the copyright, we only use opened URLs as source image-caption data when we create DialogCC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Detailed Analysis of DialogCC</head><p>In this section, we provide a detailed analysis of Di-alogCC in terms of the scalability, diversity, and generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 Scalability</head><p>Table <ref type="table" target="#tab_6">A</ref> shows the full statistics of DialogCC compared to existing datasets MMDD <ref type="bibr">[12]</ref> and PhotoChat <ref type="bibr">[30]</ref>. As we aforementioned, DialogCC comprises of 92k unique dialogues in total, which is roughly 3.7? and 7.7? larger than MMDD and PhotoChat, respectively. Although the number of dialogue turn on average is shorter than existing datasets, the utterance length is longer than others, which indicates that our dataset contains more specific utterances that can increase the interestingness and engaingness <ref type="bibr">[23]</ref>. For example, given the utterance "how are you?", the response "I'm really good, because I'm going to paris today!" is more close to specific utterance than the response of "I'm good". Thus, the multi-modal dialogue generative model trained on our dataset can generate more specific responses, making conversation more attractive. Furthermore, we provide the detailed statistics of Di-alogCC according to the source dialogue datasets, as shown in Table <ref type="table">B</ref>. Overall, the KnowledgeCC dataset includes the largest number of images per utterance and dialogue than other source dialogue datasets and the EmpathyCC has the smallest number. This result indicates that the CLIP similarity between image and utterance, which contains more object information (e.g., dog, bus, soccer), is relatively higher than the similarity between image and utterance related to the emotional situation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 Diversity</head><p>As shown in Section 3.2, DialogCC contains the largest number of unique hypernyms and unique words in both dialogues and image captions. In addition, we show that our dataset covers various conversational skills with balanced distribution, as illustrated in Figure <ref type="figure" target="#fig_4">7</ref> in our main paper. We furthermore compare the diversity of datasets in terms of the part-of-speech (POS) by using the pos tagger provided by the spaCy <ref type="foot" target="#foot_3">4</ref> . In total, compared to PhotoChat and MMDD, our dataset includes 4.5? and 2.2? more noun words; 13.4? and 3.2? more verb words; 1.7? and 2.7? more adjective words, which suggests that our dataset covers more variety of words. Table <ref type="table">B</ref>. Detailed Statistics of DialogCC. We show the statistics on the results of matching each source dialogue dataset and CC3M. KnowledgeCC, PersonaCC, EmpathyCC, DailyCC, and BlendedCC denote the multi-modal dialogue dataset created from the Wizard-of-Wikipedia <ref type="bibr">[6]</ref>, Persona-Chat <ref type="bibr">[31]</ref>, EmpatheticDialogues <ref type="bibr">[20]</ref>, DailyDialog <ref type="bibr">[13]</ref>, and BlendedSkillTalk <ref type="bibr">[25]</ref>, respectively. Table <ref type="table">C</ref>. Part-of-Speech (POS) Comparison Results. We count the number of the unique noun, verb, and adjective words in dialogues and image captions. # noun, # verb, and # adj denote the number of the unique noun word, the number of the unique verb words, and the number of the unique adjective words, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.3 Generalization</head><p>In real-life conversations, various images can be shared even with the same utterance, depending on who shares the image. As shown in Table <ref type="table" target="#tab_6">A</ref>, DialogCC contains larger images per utterance and dialogue than existing datasets, which indicates that our dataset can successfully reflect this phenomenon. For example, as shown in Figure <ref type="figure" target="#fig_8">B</ref>, our dataset includes various images with similar semantic meanings to the given utterance, which can induce the model to be more robust than a model trained with MMDD. In addition, MMDD contains the same images on two different utterances, as illustrated in Figure <ref type="figure" target="#fig_7">A</ref>. For example, in the first row of Figure A, the image in our dataset is relevant to the keyword "disney races" in the utterance, while MMDD cannot. This result will induce the degradation of generalization performance because the model trained on MMDD may prefer to memorize specific images rather than  <ref type="bibr">[12]</ref> vs. DialogCC. DialogCC contains various forms of images that are semantically similar to a given utterance. For example, in the top three rows, our dataset includes different kinds of "landscape", "a pair of shoes", and "two horses". Besides, in the fifth row, unlike the MMDD, our dataset contains highly relevant images to the given utterance, which benefitted from the utterance-caption similarity. Images in the green box are from MMDD, images in the red box are from DialogCC, and the blue box is utterances.</p><p>understanding the dependency between image and utterance. We show that our dataset improves the generalization performance in Section 4.2.4, Section 4.2.5, Section C.3, and Section C.4. show that DialogCC covers a variety of images, including various objects or scenes which are relevant to the given utterance, regardless of whether the conversation is short or long. For example, there are diverse images of graduation ceremonies, christmas, traffic jams, and hiking with a dog.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Case Studies of DialogCC</head><p>However, we sometimes found poor-quality examples of DialogCC, as illustrated in Figure F. On the second turn in the red box with a dotted line, the images are related to the given utterance that includes the word or phrase "pretty animal" or "arctic". However, when we look at the previous utterance "Yes, I love huskys. They are a very pretty dog.", we can easily recognize that these matched images are inappropriate to the given utterance considering the whole dialogue context. This is because when we calculate two similarities between utterance-caption and utterance-image by using CLIP model, we do not extract dialogue feature vectors or calculate the similarity by considering the whole dialogue context. Such a problem also exists in MMDD, which can be regarded as a typical problem of automatic methods <ref type="bibr">[2,</ref><ref type="bibr">12,</ref><ref type="bibr">17]</ref> that can be utilized in creating several datasets. Nevertheless, recent studies have shown largescale multi-modal pretraining models <ref type="bibr">[1,</ref><ref type="bibr">9,</ref><ref type="bibr">11,</ref><ref type="bibr">15,</ref><ref type="bibr">19]</ref> have achieved enormous performance on various downstream tasks, which benefitted from the noisy large-scale imagecaption datasets <ref type="bibr">[3,</ref><ref type="bibr">4,</ref><ref type="bibr">21,</ref><ref type="bibr">22,</ref><ref type="bibr">26]</ref>. Therefore, this paper aims to create a large-scale multi-modal dialogue dataset, even if there is some noise, rather than creating a cleaned dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details of Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Baseline Models</head><p>As illustrated in Figure <ref type="figure">C</ref>, we present the architecture of baseline models, which is the text retrieval model <ref type="bibr">[12]</ref> and image retrieval model <ref type="bibr">[30]</ref>. We provide a detailed description of baseline models below.</p><p>Text Retrieval Model <ref type="bibr">[12]</ref>. The text retrieval model consists of the dialogue, response, image, and multi-modal encoder. The dialogue encoder encodes the whole dialogue history into a fixed-size representation using the BERT <ref type="bibr">[5]</ref> model. The response encoder converts the response into a fixed-size representation using the BERT model, the same BERT used in the dialogue encoder. They use the pooled vector from the BERT model to get the representations for dialogue history and response. When they encode the dialogue history, they use up to three turns just before the current turn by concatenating each turn with [SEP] special token. The image encoder is to extract image feature vectors using ResNeXt-101 <ref type="bibr">[29]</ref>. After extracting text and image features, these features are fed into the fully-connected layer with the ReLU <ref type="bibr">[18]</ref> activation function. They use the sum and attention modules to make a fused representation between the dialogue history and image. The sum module adds two vectors in an element-wise manner. They first concatenate the dialogue history vector and image feature vector for the attention module. They then pass the concatenated vector into the Transformer <ref type="bibr">[27]</ref> to obtain contextualized multi-modal representation. Lastly, they calculate the dot product between the response feature vector and the multi-modal feature vector to get the loss.</p><p>Image Retrieval Model <ref type="bibr">[30]</ref>. The image retrieval model consists of a dialogue encoder, a caption encoder <ref type="foot" target="#foot_4">5</ref> , and an image encoder. The dialogue encoder and caption encoder leverage two different BERT models, which means the parameters of the two BERT models are not shared. They use the contextualized representation vector corresponding to the position of [CLS] for dialogue history and a caption. For the image encoder, they use ResNet-152 <ref type="bibr">[8]</ref>. They concatenate two feature vectors for image and caption to obtain a meaningful multi-modal representation. They then pass the concatenated vector into the fully-connected layers with the ReLU function. They encode the dialogue history into a fixed-size representation and pass it to the fully-connected layers with the ReLU function. They calculate the cosine similarity by computing the dot product of L2 normalized features for dialogue history and multi-modal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Implementation Details</head><p>As we described the implementation details in our main paper, we explain additional implementation details for the extended experiments. Since our dataset is much larger than We show the architecture of the text retrieval model <ref type="bibr">[12]</ref> and image retrieval model <ref type="bibr">[30]</ref>, which are used in the experiment. We color the part corresponding to getting text representations as a light orange, the part corresponding to getting visual representation as a light green, and the part corresponding to getting a multi-modal representation as a light blue. The light red is the dot product.</p><p>the PhotoChat dataset, we use the cosine annealing schedule <ref type="bibr">[14]</ref> with the same step size of 1,000. In all extended experiments, we train the models longer to achieve the best performance. We use the RandomResized cropping technique for image augmentation in both image retrieval and text retrieval experiments.</p><p>Image Augmentation Techniques For the experiment of robustness on image augmentation, we use the imgaug <ref type="bibr">[10]</ref> library for image augmentation. We adopt eight image augmentation techniques, rotation, zoom-in, zoom-out, cutout, dropout, shearing, gaussian noise, and gaussian blur. We follow the same setting of each technique of the previous work <ref type="bibr">[7]</ref>.</p><p>Text Augmentation Techniques For the experiment of robustness on text augmentation, we use the synonym replacement techniques introduced in EDA <ref type="bibr">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Extended Experiments</head><p>In this section, we show the extended experiments of the text retrieval and image retrieval models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Text Retrieval</head><p>Table <ref type="table" target="#tab_9">D</ref> shows the text retrieval performance across all evaluation metrics, which is reported in our main paper. We train the model with more epochs to achieve improved performance. In addition, we conduct the ablation studies by applying different multi-modal encoders (i.e., sum and attention) and by changing model inputs, such as only providing the image or dialogue to the model, as shown in Table <ref type="table" target="#tab_12">G</ref>. Even we further training, we observe a similar tendency that the model trained on our dataset improves the performance of MMDD. However, training the model with MMDD achieves considerably poor performance on our dataset. This result indicates that our dataset is more challenging than MMDD due to the most significant number of images per utterance and dialogue. Moreover, such lack of diversity in MMDD induces the model to memorize seen images in the training process. The following paragraph explains the effect of different multi-modal encoders and model inputs.</p><p>Effect of Multi-Modal Encoder. In the model trained on MMDD, the sum module achieves better performance on two text retrieval tasks than the attention module, similar to the results reported in <ref type="bibr">[12]</ref>. However, in the model trained on DialogCC, the attention module performs better on current turn prediction task. This result indicates that the attention module benefitted from the scalability and diversity of our dataset. In addition, the next turn prediction task is more challenging than the current turn prediction task due to the overall lower performance.</p><p>Effect of Model Inputs. To understand which modality is important in the text retrieval task, we conduct another experiment by giving different model inputs, such as only the dialogue history, only the image, or both the dialogue history and the image. Overall, using both modalities as input achieves the best performance. We also observe that considering dialogue history is important in the multi-modal dialogue retrieval tasks due to the lower performance of "Image Only".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Image Retrieval</head><p>Table E shows the image retrieval performance across all evaluation metrics, which is reported in Table <ref type="table">4</ref> in our main paper. Since our dataset is larger and more diverse than PhotoChat, we need to train the image retrieval model with more epochs and vary the learning rate to achieve the best performance. Thus, we adopt the cosine annealing scheduler with a step size of 1,000. As shown in R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10  achieve the better performance than 7.58 (in Table <ref type="table" target="#tab_10">E</ref>), which indicates that our dataset can make the model's training difficult due to the scalability and diversity. It will be helpful to improve the generalization performance.</p><p>Models ? Eval ? PhotoChat DialogCC Train ? R@1 R@5 R@10 R@1 R@5 R@10 Train ? R@1 R@5 R@10 R@1 R@5 R@10 However, the model trained on PhotoChat achieves better on R@5 and R@10. There are two possible reasons.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Robustness on Image Augmentation</head><p>To explore whether our dataset can make a wellgeneralized model, we conduct a robust experiment by distorting image input. As shown in Table <ref type="table" target="#tab_13">H</ref>, the model trained on our dataset shows a more robust performance with a lower reduction than the model trained on the MMDD dataset. This result implies that our dataset induces the model to be more robust to image distortion, which benefitted from the diversity of our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Robustness on Text Augmentation</head><p>We evaluate the robustness of the model trained on our dataset by distorting input dialogue history. We replace randomly chosen words (except stopwords) with synonyms by adjusting the ratio of replacement ?. As shown in Figure <ref type="figure" target="#fig_13">E</ref>, the model trained on our dataset shows a more robust performance when the value of ? increases. This result indicates that while our dataset contains some noisy samples, our dataset makes the model more robust, which benefitted from the scalability and diversity of our dataset. Therefore, it is important to build a large-scale dataset to achieve improved generalization performance. In the red box with a dotted line, DialogCC contains diverse images that are relevant to the utterance. However, given the previous utterance, images should relate more to the "huskys" than many other animals, such as rabbits, polar bears, and penguins. Besides, it is more natural to share images related to "eskimo" or "arctic".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MME? Model Inputs?</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task?</head><p>Current Turn Prediction Next Turn Prediction Eval? MMDD DialogCC ACL DialogCC Train? R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 Train ? MMDD DialogCC Aug ? R@1 (?) R@5 (?) R@10 (?) R@1 (?) R@5 (?) R@10 (?)  </p><note type="other">Baseline</note></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure1. Dataset statistics comparison. We compare our proposed dataset DialogCC with existing multi-modal dialogue datasets: MMDD[20]  and PhotoChat<ref type="bibr" target="#b48">[49]</ref>. Compared to the existing datasets, DialogCC contains large-scale dialogues and images while covering more diverse words and hypernyms. Moreover, learning with various images for the same dialogue or utterance in DialogCC would benefit a model to obtain better generalization.</figDesc><graphic url="image-1.png" coords="1,383.94,197.79,91.63,270.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Comparison examples of relevant images in MMDD[20]  vs. DialogCC. In the first and second rows, both datasets contain semantically relevant images to the given utterances, while our dataset contains more and various images with different views or objects (e.g., dog breeds or backgrounds). In the last two rows, unlike the MMDD, our dataset is highly relevant for a given utterance because of matching from large-scale and diverse source data and considering image captions. Images in the green box are from MMDD, images in the red box are from DialogCC, and the blue box is utterances. More examples are in the Appendix.</figDesc><graphic url="image-4.png" coords="4,99.61,72.00,395.99,307.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Examples of frequently matched images. We show representative examples of images matched with various utterances by CLIP-based similarity. The number under each image indicates the count of how many utterances are matched.</figDesc><graphic url="image-5.png" coords="5,50.11,72.00,236.26,66.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. An example of DialogCC. We present an example of DialogCC with resized images. More examples are in Appendix.</figDesc><graphic url="image-7.png" coords="5,320.68,72.00,212.62,356.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Conversational Skills in DialogCC vs. MMDD[20]. We count the number of dialogues corresponding to the conversational skills. DialogCC covers two more various conversational skills (knowledge and blended skills) than MMDD.</figDesc><graphic url="image-8.png" coords="6,320.68,72.00,212.62,103.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>and Figure 6. There are an average of 34 images per dialogue and 4.1 images per utterance. Training a model with our dataset would enhance the generalization performance, which is experimentally shown in Section 4.2.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Effect of maximum number of images. We show the performance of the current turn prediction task by training the model with multiple images per utterance. In general, training with multiple images mostly improves performance.</figDesc><graphic url="image-9.png" coords="7,326.58,173.32,200.81,66.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure A .</head><label>A</label><figDesc>Figure A. Problematic Examples in MMDD [12]. Even if the two given utterances have different semantic meanings, MMDD matches the same image. Hence, training a multi-modal dialogue with MMDD will likely memorize this green image rather than a deep understanding of input utterances. Images in the green box are from MMDD, images in the red box are from DialogCC, and the blue box is utterances.</figDesc><graphic url="image-10.png" coords="12,320.68,71.85,212.73,193.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure B .</head><label>B</label><figDesc>Figure B. Comparison Examples of Relevant Images in MMDD[12]  vs. DialogCC. DialogCC contains various forms of images that are semantically similar to a given utterance. For example, in the top three rows, our dataset includes different kinds of "landscape", "a pair of shoes", and "two horses". Besides, in the fifth row, unlike the MMDD, our dataset contains highly relevant images to the given utterance, which benefitted from the utterance-caption similarity. Images in the green box are from MMDD, images in the red box are from DialogCC, and the blue box is utterances.</figDesc><graphic url="image-11.png" coords="14,74.75,71.85,445.69,554.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>A. 3 . 1</head><label>31</label><figDesc>Comparison examples to MMDDUnlike MMDD[12], we create DialogCC by using utterance-caption similarity as well as utterance-image similarity to improve the quality of DialogCC. To compare the different quality of multi-modal dialogue dataset, we present more comparison examples to MMDD in Figure B. In the last row of Figure B, our dataset includes more diverse images related to the "sneakers", which are in the given utterance. A.3.2 Examples of DialogCC We present more examples of DialogCC in Figure G and Figure D. Both Figure G and Figure D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Figure C. Architectures of Baseline Models. We show the architecture of the text retrieval model[12]  and image retrieval model[30], which are used in the experiment. We color the part corresponding to getting text representations as a light orange, the part corresponding to getting visual representation as a light green, and the part corresponding to getting a multi-modal representation as a light blue. The light red is the dot product.</figDesc><graphic url="image-13.png" coords="16,334.75,74.20,173.34,108.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure D. Case 1 :</head><label>1</label><figDesc>Figure D. Case 1: Examples of DialogCC. We present examples of DialogCC, and a gray dotted line separates each example. As illustrated in both examples, our dataset covers diverse images which are related to the utterance.</figDesc><graphic url="image-14.png" coords="17,49.99,205.41,236.36,381.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>The first reason is that the correlation between the training and test sets is larger in PhotoChat than in DialogCC. If the model trained on PhotoChat is well-generalized, it should have performed well in DialogCC, but it does not. On the other hand, training the model with our dataset shows high performance in both PhotoChat and DialogCC. The second reason is that the covered images in training, validation, and test set have similar domains in PhotoChat. PhotoChat only covers commonly shared objects, such as people, food, animal, and product. However, DialogCC is an open domain, which means that DialogCC covers many topics and conversational skills described in Section 3.2. This diversity would have decreased performance compared to the model trained on PhotoChat, which focuses on a specific domain. As shown in Figure E and Table F, the model trained on</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure E .</head><label>E</label><figDesc>Figure E. Performance Gap with Text Distortion. We show the robustness performance of both models trained on our dataset or trained on PhotoChat by increasing the value of ?. The y-axis denotes the performance gap between the baseline score (without augmentation) and the score (with augmentation). The higher the value of the y-axis, the less robust the trained model is. This graph shows that the model trained on our dataset mostly achieves better robustness across all ? values.</figDesc><graphic url="image-15.png" coords="18,55.90,71.85,224.54,84.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure F. Case 3 :</head><label>3</label><figDesc>Figure F. Case 3: Example of DialogCC that Unrelated to the Given Utterance.In the red box with a dotted line, DialogCC contains diverse images that are relevant to the utterance. However, given the previous utterance, images should relate more to the "huskys" than many other animals, such as rabbits, polar bears, and penguins. Besides, it is more natural to share images related to "eskimo" or "arctic".</figDesc><graphic url="image-16.png" coords="18,314.77,84.68,224.55,526.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure G. Case 2 :</head><label>2</label><figDesc>Figure G. Case 2: Examples of DialogCC. We present examples of DialogCC. Both examples also treat various images relevant to the utterance. For example, in the left figure, our dataset includes images related to an utterance in which the motion is revealed "go hiking with my dog".</figDesc><graphic url="image-17.png" coords="20,49.99,129.91,495.23,480.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-3.png" coords="3,50.11,72.00,495.01,292.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Statistics of Datasets. In total, DialogCC includes the largest number of unique dialogues and images than others. I./D. and I./U. denote images by an dialogue and images by an utterance, respectively. More detailed statistics are in the Appendix.</figDesc><table><row><cell>Datasets</cell><cell>Type</cell><cell># Unique Dialog</cell><cell>Avg. # Turns</cell><cell># Unique Image</cell><cell>Avg. # I./D.</cell><cell>Avg. # I./U.</cell></row><row><cell>MMDD [20]</cell><cell>train valid test</cell><cell>21,411 2,400 2,672</cell><cell>13.0 13.6 13.6</cell><cell>12,272 334 682</cell><cell>3.3 1.1 1.1</cell><cell>1.9 1.0 1.0</cell></row><row><cell></cell><cell>total</cell><cell>26,434</cell><cell>13.1</cell><cell>13,288</cell><cell>3.4</cell><cell>1.7</cell></row><row><cell>PhotoChat [49]</cell><cell>train valid test</cell><cell>10,286 1,000 1,000</cell><cell>12.7 12.7 12.8</cell><cell>8,899 1,000 1,000</cell><cell>1.0 1.0 1.0</cell><cell>1.0 1.0 1.0</cell></row><row><cell></cell><cell>total</cell><cell>12,286</cell><cell>12.7</cell><cell>10,889</cell><cell>1.2</cell><cell>1.1</cell></row><row><cell>DialogCC(ours)</cell><cell>train valid test</cell><cell>77,354 7,940 7,648</cell><cell>8.3 8.2 8.2</cell><cell>467,944 92,723 91,198</cell><cell>32.2 43.0 42.6</cell><cell>3.9 5.2 5.2</cell></row><row><cell></cell><cell>total</cell><cell>92,942</cell><cell>10.0</cell><cell>651,840</cell><cell>34.0</cell><cell>4.1</cell></row><row><cell></cell><cell></cell><cell>Dialog</cell><cell cols="2">Image caption</cell><cell cols="2">Total</cell></row><row><cell></cell><cell cols="6"># hyp # word # hyp # word # hyp # word</cell></row><row><cell cols="7">MMDD [20] PhotoChat [49] DialogCC(ours) 4,337 82,912 4,097 30,911 8,434 113,826 3,158 25,962 1,709 15,364 4,867 41,326 1,163 24,093 193 2,935 1,356 27,028</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Robustness comparisons on image augmentation. We show the degree of decreased Recall@1(%) performance of the text retrieval model compared to the baseline score. We present detailed information on applied augmentation techniques in Appendix.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table A .</head><label>A</label><figDesc>Full Statistics of Datasets. Compared to existing datasets MMDD and PhotoChat, DialogCC includes the largest number of unique dialogues and images.</figDesc><table><row><cell>Datasets</cell><cell>Type</cell><cell># Unique Dialog</cell><cell># Utter</cell><cell>Avg. # Utter/Dialog</cell><cell>Avg. # Token/Utter</cell><cell># Image</cell><cell># Unique Image</cell><cell>Avg. # Image/Dialog</cell><cell>Avg. # Image/Utter</cell><cell>Avg. # Utter/Image</cell></row><row><cell>MMDD [12]</cell><cell>train valid test</cell><cell>21,411 2,400 2,672</cell><cell>519,728 32,708 36,315</cell><cell>13.01 13.62 13.59</cell><cell>11.97 11.87 11.85</cell><cell>39,956 2,401 2,673</cell><cell>12,272 334 682</cell><cell>3.32 1.13 1.13</cell><cell>1.87 1.0 1.0</cell><cell>3.26 7.19 3.92</cell></row><row><cell></cell><cell>total</cell><cell>26,483</cell><cell>588,751</cell><cell>13.07</cell><cell>11.96</cell><cell>45,030</cell><cell>13,288</cell><cell>3.42</cell><cell>1.7</cell><cell>3.39</cell></row><row><cell>PhotoChat [30]</cell><cell>train valid test</cell><cell>10,286 1,000 1,000</cell><cell>130,459 12,695 12,841</cell><cell>12.68 12.7 12.84</cell><cell>6.33 6.31 6.29</cell><cell>10,286 1,000 1,000</cell><cell>8,889 1,000 1,000</cell><cell>1.0 1.0 1.0</cell><cell>1.0 1.0 1.0</cell><cell>1.16 1.0 1.0</cell></row><row><cell></cell><cell>total</cell><cell>12,286</cell><cell>155,995</cell><cell>12.7</cell><cell>6.33</cell><cell>12,286</cell><cell>10,889</cell><cell>1.0</cell><cell>1.0</cell><cell>1.13</cell></row><row><cell>DialogCC (ours)</cell><cell>train valid test</cell><cell>77,354 7,940 7,648</cell><cell>6,362,844 674,708 644,968</cell><cell>9.89 10.34 10.3</cell><cell>14.18 13.61 13.77</cell><cell cols="2">2,492,320 467,944 341,648 92,723 325,627 91,198</cell><cell>32.22 43.03 42.58</cell><cell>3.88 5.24 5.2</cell><cell>5.33 3.68 3.57</cell></row><row><cell></cell><cell>total</cell><cell>92,942</cell><cell>7,682,520</cell><cell>9.97</cell><cell>14.09</cell><cell cols="2">3,159,595 651,840</cell><cell>34.0</cell><cell>4.1</cell><cell>4.85</cell></row><row><cell>Datasets</cell><cell>Type</cell><cell># Unique Dialog</cell><cell># Utter</cell><cell>Avg. # Utter/Dialog</cell><cell>Avg. # Token/Utter</cell><cell># Image</cell><cell># Unique Image</cell><cell>Avg. # Image/Dialog</cell><cell>Avg. # Image/Utter</cell><cell>Avg. # Utter/Image</cell></row><row><cell>KnowledgeCC</cell><cell>train valid test</cell><cell>35,252 1,952 1,917</cell><cell>2,368,960 131,556 127,684</cell><cell>8.32 8.33 8.29</cell><cell>16.5 16.53 16.53</cell><cell cols="2">1,386,041 305,656 132,398 42,352 129,317 41,580</cell><cell>39.32 67.83 67.46</cell><cell>4.87 8.39 8.39</cell><cell>4.53 3.13 3.11</cell></row><row><cell></cell><cell>total</cell><cell>39,121</cell><cell>2,628,200</cell><cell>8.32</cell><cell>16.5</cell><cell cols="2">1,647,756 389,574</cell><cell>42.12</cell><cell>5.22</cell><cell>4.23</cell></row><row><cell>PersonaCC</cell><cell>train valid test</cell><cell>8,763 1,000 967</cell><cell>1,924,764 244,500 234,096</cell><cell>14.92 15.67 15.6</cell><cell>11.67 11.92 11.77</cell><cell>268,399 54,333 45,409</cell><cell>145,069 31,169 27,148</cell><cell>30.63 54.33 46.96</cell><cell>2.08 3.48 3.03</cell><cell>1.85 1.74 1.67</cell></row><row><cell></cell><cell>total</cell><cell>10,730</cell><cell>2,403,360</cell><cell>15.06</cell><cell>11.71</cell><cell>368,141</cell><cell>203,382</cell><cell>34.31</cell><cell>2.31</cell><cell>1.81</cell></row><row><cell>EmpathyCC</cell><cell>train valid test</cell><cell>11,830 2,151 1,993</cell><cell>209,280 38,280 35,412</cell><cell>4.26 4.27 4.27</cell><cell>13.73 14.62 15.59</cell><cell>130,441 27,739 25,991</cell><cell>84,072 19,597 18,438</cell><cell>11.03 12.9 13.04</cell><cell>2.65 3.1 3.14</cell><cell>1.55 1.42 1.41</cell></row><row><cell></cell><cell>total</cell><cell>15,974</cell><cell>282,972</cell><cell>4.26</cell><cell>14.08</cell><cell>184,171</cell><cell>122,104</cell><cell>11.53</cell><cell>2.77</cell><cell>1.51</cell></row><row><cell>DailyCC</cell><cell>train valid test</cell><cell>16,934 1,845 1,807</cell><cell>1,240,428 128,612 119,484</cell><cell>9.69 9.4 9.2</cell><cell>14.11 13.67 14.14</cell><cell>550,047 68,686 72,060</cell><cell>161,406 24,760 28,214</cell><cell>32.48 37.23 39.88</cell><cell>4.3 5.02 5.55</cell><cell>3.41 2.77 2.55</cell></row><row><cell></cell><cell>total</cell><cell>20,586</cell><cell>1,488,524</cell><cell>9.62</cell><cell>14.07</cell><cell>690,793</cell><cell>214,375</cell><cell>33.56</cell><cell>4.47</cell><cell>3.22</cell></row><row><cell>BlendedCC</cell><cell>train valid test</cell><cell>4,575 992 964</cell><cell>619,412 131,760 128,292</cell><cell>11.83 11.75 11.77</cell><cell>13.34 13.48 13.85</cell><cell>157,392 58,492 52,850</cell><cell>111,702 37,133 34,590</cell><cell>34.4 58.96 54.82</cell><cell>3.01 5.22 4.85</cell><cell>1.41 1.58 1.53</cell></row><row><cell></cell><cell>total</cell><cell>6,531</cell><cell>879,464</cell><cell>11.81</cell><cell>13.44</cell><cell>268,734</cell><cell>183,418</cell><cell>41.15</cell><cell>3.61</cell><cell>1.47</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table F</head><label>F</label><figDesc></figDesc><table><row><cell>Models?</cell><cell>Task ? Eval ? Train ?</cell><cell>Current Turn Prediction MMDD DialogCC</cell><cell>Next Turn Prediction MMDD DialogCC</cell></row><row><cell></cell><cell></cell><cell></cell><cell>, we</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table D .</head><label>D</label><figDesc>DialogCC 6.64 21.57 30.16 10.17 28.74 39.17 4.31 12.58 17.69 11.07 29.08 37.18 Text Retrieval Performance. We compare the performance of text retrieval model trained on DialogCC (when the maximum number of image is 10) and MMDD across all metrics.</figDesc><table><row><cell>BM25 Text Retrieval</cell><cell>-MMDD</cell><cell>3.62 5.28 16.18 23.34 7.91 10.46</cell><cell>0.72 2.65</cell><cell>4.48 7.73</cell><cell>8.05 11.29</cell><cell>3.72 2.93</cell><cell>8.34 9.6</cell><cell>12.58 12.13</cell><cell>0.83 1.56</cell><cell>4.61 4.93</cell><cell>8.53 7.23</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table E .</head><label>E</label><figDesc>Image Retrieval Performance. We report the image retrieval performance on PhotoChat and DialogCC datasets, which is reported in Table4in our main paper.</figDesc><table><row><cell>BM25 Image Retrieval</cell><cell cols="2">-PhotoChat 7.15 25.02 37.20 6.8 15.9 22.5 DialogCC 7.58 17.52 25.00 14.85 36.33 48.93 0.36 1.21 1.77 1.35 5.49 8.70</cell></row><row><cell>Eval ?</cell><cell>PhotoChat</cell><cell>DialogCC</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table F .</head><label>F</label><figDesc>Extended Image Retrieval Performance. We report the image retrieval performance on PhotoChat and DialogCC dataset, where we train the image retrieval model with DialogCC by adopting the cosine annealing schedule. The number in parenthesis denotes the maximum number of images used in the training process.</figDesc><table><row><cell>PhotoChat DialogCC (1) DialogCC (5) DialogCC (10) 7.44 18.08 26.34 8.06 23.76 38.02 7.03 18.70 26.86 8.68 18.70 26.55 DialogCC (15) 8.99 19.42 28.20 DialogCC (20) 8.68 18.70 26.14 DialogCC (30) 8.16 18.60 26.96</cell><cell>0.17 2.45 3.02 11.16 17.89 0.80 1.38 9.30 15.19 2.95 11.17 18.04 3.37 11.61 18.12 2.92 11.15 18.01 3.12 11.18 17.75</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table G .</head><label>G</label><figDesc>DialogCC 19.49 44.03 55.69 6.87 22.26 31.58 13.61 30.51 38.55 5.04 15.76 22.71 Extended Text Retrieval Performance. We report the text retrieval performance on MMDD and DialogCC. MME denotes the multi-modal encoder.</figDesc><table><row><cell></cell><cell>Image Only</cell><cell cols="2">MMDD DialogCC 3.90 15.67 24.90 5.37 20.88 33.29</cell><cell>0.25 1.01</cell><cell>1.20 4.27</cell><cell>2.16 6.94</cell><cell>0.86 1.67</cell><cell>3.12 5.82</cell><cell>5.48 9.07</cell><cell>0.09 0.23</cell><cell>0.37 0.92</cell><cell>0.65 1.61</cell></row><row><cell>Sum</cell><cell>Dialogue Only</cell><cell cols="2">MMDD DialogCC 16.79 35.92 45.55 8.55 19.53 26.53</cell><cell cols="6">2.64 5.22 20.28 29.47 16.26 32.65 40.52 9.47 14.91 7.02 15.79 21.22</cell><cell cols="2">3.52 10.45 14.96 7.02 21.94 30.15</cell></row><row><cell></cell><cell>Dialogue + Image</cell><cell cols="2">MMDD DialogCC 15.16 37.51 49.64 13.72 35.68 49.48</cell><cell cols="6">0.95 6.31 21.63 30.90 13.35 30.81 39.62 3.95 6.70 6.80 15.28 20.63</cell><cell cols="2">2.41 6.64 21.22 29.51 8.15 12.25</cell></row><row><cell></cell><cell>Image Only</cell><cell cols="2">MMDD DialogCC 4.97 16.99 26.93 5.01 20.13 33.37</cell><cell>0.21 1.08</cell><cell>1.12 2.94</cell><cell>2.10 6.74</cell><cell>0.86 1.37</cell><cell>3.47 6.29</cell><cell>5.73 9.76</cell><cell>0.09 0.25</cell><cell>0.40 1.13</cell><cell>0.72 2.04</cell></row><row><cell>Attention</cell><cell>Dialogue Only</cell><cell cols="2">MMDD DialogCC 15.39 33.17 42.64 8.83 19.81 26.93</cell><cell cols="6">1.90 5.40 20.03 29.80 16.09 34.28 42.88 7.57 12.01 6.76 15.32 20.92</cell><cell cols="2">3.28 10.45 15.55 7.17 22.76 31.34</cell></row><row><cell></cell><cell>Dialogue + Image</cell><cell>MMDD</cell><cell>12.97 33.13 45.55</cell><cell>0.92</cell><cell>3.57</cell><cell>6.23</cell><cell cols="3">4.66 12.11 17.37</cell><cell>2.24</cell><cell>7.23</cell><cell>10.54</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table H .</head><label>H</label><figDesc>Robustness Performance on Image Distortion. We report the robustness performance when input images are distorted.</figDesc><table><row><cell>13.72 10.46 (3.26) 30.27 (5.41) 35.68 7.52 (6.2) 22.43 (13.25) 31.5 (17.98) 49.48 41.89 (7.59) 13.25 (1.71) 14.96 9.83 (5.13) 25.18 (11.94) 34.05 (14.12) 37.12 48.17 32.62 (4.5) 43.16 (5.01) 10.98 (2.74) 31.07 (4.61) 42.32 (7.16) 13.6 (1.36) 32.7 (4.42) 43.28 (4.89) 11.14 (2.58) 31.11 (4.57) 43.56 (5.92) 12.85 (2.11) 32.54 (4.58) 43.44 (4.73) 9.03 (4.69) 25.54 (10.14) 36.2 (13.28) 10.5 (4.46) 27.73 (9.39) 37.79 (10.38) 9.67 (4.05) 27.69 (7.99) 38.27 (11.21) 12.25 (2.71) 31.19 (5.93) 41.25 (6.92) Gaussian noise 10.46 (3.26) Rotation Zoom-In Zoom-Out Cutout Dropout Shearing 28.4 (7.28) 39.62 (9.86) 11.54 (3.42) 30.11 (7.01) 40.37 (7.8) Gaussian blur 10.78 (2.94) 29.44 (6.24) 40.69 (8.79) 12.61 (2.35) 32.34 (4.78) 42.8 (5.37)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https : / / ai . google . com / research / ConceptualCaptions/download</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/rom1504/img2dataset</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/webdataset/webdataset</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://spacy.io/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>The original paper calls it a photo description encoder because Pho-toChat contains the photo description per photo (image). In this paper, we call it the caption encoder.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Flamingo: a visual language model for few-shot learning</title>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yana</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.14198</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Digbalay</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Hebbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Somandepalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kree</forename><surname>Cole-Mclaughlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huisheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shrikanth</forename><surname>Narayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.11065</idno>
		<title level="m">Movieclip: Visual scene recognition in movies</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Conceptual 12m: Pushing web-scale image-text pretraining to recognize long-tail visual concepts</title>
		<author>
			<persName><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seunghyun</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajinkya</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trung</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.13115</idno>
		<title level="m">Fine-grained image captioning with clip reward</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">and Dhruv Batra. Visual dialog</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khushi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deshraj</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Redcaps: Web-curated image-text data created by the people, for the people</title>
		<author>
			<persName><forename type="first">Karan</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Kaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zubin</forename><surname>Aysola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11431</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01241</idno>
		<title level="m">Wizard of wikipedia: Knowledge-powered conversational agents</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Vse++: Improving visual-semantic embeddings with hard negatives</title>
		<author>
			<persName><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05612</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Clipdraw: Exploring text-to-drawing synthesis through language-image encoders</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Frans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">B</forename><surname>Soros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Witkowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.14843</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Clipscore: A reference-free evaluation metric for image captioning</title>
		<author>
			<persName><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Le Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08718</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<idno>PMLR, 2021. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="4904" to="4916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">B</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kentaro</forename><surname>Wada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Crall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satoshi</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Graving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Reinders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarthak</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joy</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G?bor</forename><surname>Vecsei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kraft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jirka</forename><surname>Borovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Vallentin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Semen</forename><surname>Zhydenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Pfeiffer</surname></persName>
		</author>
		<idno>ac- cessed 01-Feb-2020. 8</idno>
		<ptr target="https://github.com/aleju/imgaug" />
		<title level="m">Ben Cook, Ismael Fern?ndez, Franc ?ois-Michel De Rainville</title>
		<editor>
			<persName><forename type="first">Abner</forename><surname>Weng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Raphael</forename><surname>Ayala-Acevedo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Matias</forename><surname>Meudec</surname></persName>
		</editor>
		<editor>
			<persName><surname>Laporte</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Will i sound like me? improving persona consistency in dialogues through pragmatic self-consciousness</title>
		<author>
			<persName><forename type="first">Hyunwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byeongchang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05816</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Perspective-taking and pragmatics for generating empathetic responses focused on emotion causes</title>
		<author>
			<persName><forename type="first">Hyunwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byeongchang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.08828</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Large-scale bilingual language-image contrastive learning</title>
		<author>
			<persName><forename type="first">Byungsoo</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geonmo</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.14463</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03166</idno>
		<title level="m">Clevr-dialog: A diagnostic dataset for multi-round reasoning in visual dialog</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The open images dataset v4</title>
		<author>
			<persName><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1956" to="1981" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Nyoungwoo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suwon</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ho-Jin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung-Hyun</forename><surname>Myaeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.08685</idno>
		<title level="m">Constructing multi-modal dialogue dataset by replacing text with semantically relevant images</title>
		<imprint>
			<date type="published" when="2007">2021. 1, 2, 3, 4, 5, 6, 7</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visual semantic reasoning for image-text matching</title>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Yanran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuzi</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><surname>Dailydialog</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03957</idno>
		<title level="m">A manually labelled multi-turn dialogue dataset</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning</title>
		<author>
			<persName><forename type="first">Weixin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongchan</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.02053</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Zhaojiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamin</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07687</idno>
		<title level="m">Moel: Mixture of empathetic listeners</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Hua</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chanjuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Bao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.03835</idno>
		<title level="m">Towards building an open-domain dialogue system incorporated with internet memes</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Openvidial: A large-scale, opendomain dialogue dataset with visual contexts</title>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15015</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Image-grounded conversations: Multimodal context for natural question and response generation</title>
		<author>
			<persName><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><forename type="middle">P</forename><surname>Spithourakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08251</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning audio-video modalities from image captions</title>
		<author>
			<persName><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Hongsuck Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anja</forename><surname>Hauth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Manen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.00679</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Game-based videocontext dialogue</title>
		<author>
			<persName><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04560</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno>PMLR, 2021. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">Michael</forename><surname>Hannah Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y-Lan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Boureau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00207</idno>
		<title level="m">Towards empathetic open-domain conversation models: A new benchmark and dataset</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The probabilistic relevance framework: Bm25 and beyond</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends? in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Laion-400m: Open dataset of clip-filtered 400 million image-text pairs</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Vencu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Kaczmarczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clayton</forename><surname>Mullis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aarush</forename><surname>Katta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Coombes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenia</forename><surname>Jitsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aran</forename><surname>Komatsuzaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.02114</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visual reference resolution using attention memory for visual dialog</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Hongsuck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seo</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Lehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Image chat: Engaging grounded conversations</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Humeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00945</idno>
		<imprint>
			<date type="published" when="2007">2018. 1, 2, 7</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">Michael</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01082</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Multi-modal open-domain dialogue. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Eric</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Smith</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08449</idno>
		<title level="m">Can you put it all together: Evaluating conversational agents&apos; ability to blend skills</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Multi-modal mixup for robust fine-tuning</title>
		<author>
			<persName><forename type="first">Junhyuk</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changdae</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minchul</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyungwoo</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.03897</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning</title>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiecao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2443" to="2449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08530</idno>
		<title level="m">Vl-bert: Pre-training of generic visuallinguistic representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Qingfeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiubo</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.08515</idno>
		<title level="m">Multimodal dialogue response generation</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Are genderneutral queries really gender-neutral? mitigating gender bias in image search</title>
		<author>
			<persName><forename type="first">Jialu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2109.05433</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Openvidial 2.0: A larger-scale, open-domain dialogue generation dataset with visual contexts</title>
		<author>
			<persName><forename type="first">Shuhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongbin</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.12761</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Modeling text-visual mutual dependency for multi-modal dialog generation</title>
		<author>
			<persName><forename type="first">Shuhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongbin</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14445</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11196</idno>
		<title level="m">Eda: Easy data augmentation techniques for boosting performance on text classification tasks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Photochat: A human-human dialogue dataset with photo sharing behavior for joint imagetext modeling</title>
		<author>
			<persName><forename type="first">Xiaoxue</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jindong</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.01453</idno>
		<imprint>
			<date type="published" when="2007">2021. 1, 2, 5, 6, 7</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Personalizing dialogue agents: I have a dog, do you have pets too</title>
		<author>
			<persName><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07243</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Yinhe</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><surname>Mmchat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07154</idno>
		<title level="m">Multi-modal chat dataset on social media</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Flamingo: a visual language model for few-shot learning</title>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yana</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.14198</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Digbalay</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Hebbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Somandepalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kree</forename><surname>Cole-Mclaughlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huisheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shrikanth</forename><surname>Narayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.11065</idno>
		<title level="m">Movieclip: Visual scene recognition in movies</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Conceptual 12m: Pushing web-scale image-text pretraining to recognize long-tail visual concepts</title>
		<author>
			<persName><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3558" to="3568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Redcaps: Web-curated image-text data created by the people, for the people</title>
		<author>
			<persName><forename type="first">Karan</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Kaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zubin</forename><surname>Aysola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11431</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01241</idno>
		<title level="m">Wizard of wikipedia: Knowledge-powered conversational agents</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Proxy synthesis: Learning with synthetic classes for deep metric learning</title>
		<author>
			<persName><forename type="first">Geonmo</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byungsoo</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han-Gyu</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1460" to="1468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<idno>PMLR, 2021. 5</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="4904" to="4916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">B</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kentaro</forename><surname>Wada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Crall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satoshi</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Graving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Reinders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarthak</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joy</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G?bor</forename><surname>Vecsei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kraft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jirka</forename><surname>Borovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Vallentin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Semen</forename><surname>Zhydenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Pfeiffer</surname></persName>
		</author>
		<idno>ac- cessed 01-Feb-2020. 6</idno>
		<ptr target="https://github.com/aleju/imgaug" />
		<title level="m">Ben Cook, Ismael Fern?ndez, Franc ?ois-Michel De Rainville</title>
		<editor>
			<persName><forename type="first">Abner</forename><surname>Weng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Raphael</forename><surname>Ayala-Acevedo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Matias</forename><surname>Meudec</surname></persName>
		</editor>
		<editor>
			<persName><surname>Laporte</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Large-scale bilingual language-image contrastive learning</title>
		<author>
			<persName><forename type="first">Byungsoo</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geonmo</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.14463</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">Nyoungwoo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suwon</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ho-Jin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung-Hyun</forename><surname>Myaeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.08685</idno>
		<title level="m">Constructing multi-modal dialogue dataset by replacing text with semantically relevant images</title>
		<imprint>
			<date type="published" when="2006">2021. 2, 3, 4, 5, 6</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">Yanran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuzi</forename><surname>Niu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03957</idno>
		<title level="m">Dailydialog: A manually labelled multi-turn dialogue dataset</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06476</idno>
		<title level="m">Parlai: A dialog research software platform</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Learning audio-video modalities from image captions</title>
		<author>
			<persName><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Hongsuck Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anja</forename><surname>Hauth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Manen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.00679</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Icml</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">Michael</forename><surname>Hannah Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y-Lan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Boureau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00207</idno>
		<title level="m">Towards empathetic open-domain conversation models: A new benchmark and dataset</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Laion-5b: An open large-scale dataset for training next generation image-text models</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Vencu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cade</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Cherti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Coombes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aarush</forename><surname>Katta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clayton</forename><surname>Mullis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.08402</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Laion-400m: Open dataset of clip-filtered 400 million image-text pairs</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Vencu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Kaczmarczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clayton</forename><surname>Mullis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aarush</forename><surname>Katta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Coombes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenia</forename><surname>Jitsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aran</forename><surname>Komatsuzaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.02114</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<author>
			<persName><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08654</idno>
		<title level="m">Douwe Kiela, and Jason Weston. What makes a good conversation? how controllable attributes affect human judgments</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<author>
			<persName><forename type="first">Eric</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Smith</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08449</idno>
		<title level="m">Can you put it all together: Evaluating conversational agents&apos; ability to blend skills</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning</title>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiecao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2443" to="2449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11196</idno>
		<title level="m">Eda: Easy data augmentation techniques for boosting performance on text classification tasks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Photochat: A human-human dialogue dataset with photo sharing behavior for joint imagetext modeling</title>
		<author>
			<persName><forename type="first">Xiaoxue</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jindong</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.01453</idno>
		<imprint>
			<date type="published" when="2006">2021. 2, 3, 5, 6</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Personalizing dialogue agents: I have a dog, do you have pets too</title>
		<author>
			<persName><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07243</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
