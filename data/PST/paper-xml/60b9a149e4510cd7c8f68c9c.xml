<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Real-Time Video Emotion Recognition Based on Reinforcement Learning and Domain Knowledge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-09">March 9, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ke</forename><surname>Zhang</surname></persName>
							<email>zhangke@nwpu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Key Laboratory of Aerospace Flight Dynamics</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuanqing</forename><surname>Li</surname></persName>
							<email>yuanqingli@mail.nwpu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Key Laboratory of Aerospace Flight Dynamics</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Jingyu</forename><surname>Wang</surname></persName>
							<email>jywang@nwpu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Key Laboratory of Aerospace Flight Dynamics</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Astronautics</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Artificial Intelligence, Optics and Electronics (iOPEN)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
							<email>cambria@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Key Laboratory of Aerospace Flight Dynamics</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">School of Computer Science and Engi-neering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<postCode>639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Xuelong</forename><surname>Li</surname></persName>
							<email>xuelong_li@nwpu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Astronautics</orgName>
								<orgName type="institution">Northwestern Poly-technical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">School of Artificial Intelligence, Optics and Elec-tronics (iOPEN)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Real-Time Video Emotion Recognition Based on Reinforcement Learning and Domain Knowledge</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-09">March 9, 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TCSVT.2021.3072412</idno>
					<note type="submission">received November 11, 2020; revised March 7, 2021; accepted April 1, 2021. Date of publication April 12, 2021; date of current version</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multimodal emotion recognition</term>
					<term>reinforcement learning</term>
					<term>domain knowledge</term>
					<term>real-time video conversation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multimodal emotion recognition in conversational videos (ERC) develops rapidly in recent years. To fully extract the relative context from video clips, most studies build their models on the entire dialogues which make them lack of real-time ERC ability. Different from related researches, a novel multimodal emotion recognition model for conversational videos based on reinforcement learning and domain knowledge (ERLDK) is proposed in this paper. In ERLDK, the reinforcement learning algorithm is introduced to conduct real-time ERC with the occurrence of conversations. The collection of history utterances is composed as an emotion-pair which represents the multimodal context of the following utterance to be recognized. Dueling deep-Q-network (DDQN) based on gated recurrent unit (GRU) layers is designed to learn the correct action from the alternative emotion categories. Domain knowledge is extracted from public dataset based on the former information of emotion-pairs. The extracted domain knowledge is used to revise the results from the RL module and is transformed into other dataset to examine the rationality. The experimental results on datasets show that ERLDK achieves the state-of-the-art results on weighted average and most of the specific emotion categories.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>researchers. In order to better adapt to daily application scenarios, like smart home, mental illness care, education aids and car-hailing services <ref type="bibr" target="#b0">[1]</ref>, many recent studies are not satisfied with simply classifying a single utterance, sentence or article using traditional emotion categorization models. The analysis of dialogues, especially conversational videos, has become increasingly popular <ref type="bibr" target="#b1">[2]</ref>. However, it is not appropriate to perform emotion recognition in conversational videos (ERC) by just transforming algorithms and results from original emotion recognition issues <ref type="bibr" target="#b2">[3]</ref>. The emotion states always change flexibly and frequently in ERC with contents and focuses of the present dialogue and speakers. People are inclined to express their emotions implicitly through some habitual ways during interpersonal conversations, which are different from the official circumstances <ref type="bibr" target="#b3">[4]</ref>. Misunderstandings can be caused even between humans if they were unfamiliar with each other, not to mention computers. Therefore, recognitions on article level or paragraph level are imprecisely and lack of meaning because of the too long span <ref type="bibr" target="#b4">[5]</ref>. Motivated by these challenges, a large number of methods and theories for ERC have emerged in recent years <ref type="bibr" target="#b5">[6]</ref>.</p><p>The unique characteristics of ERC are summarized as context dependence, persistence and contagiousness <ref type="bibr" target="#b6">[7]</ref> that the static and dynamic flows in a daily dialogue are both included. The context dependence emphasizes the reliance on contextual information other than single utterance or sentence. The persistence means that the interlocutors are inclined to maintain their original emotional states during the conversations and the third, contagiousness, describes the emotion states of interlocutors are interactive which can be influenced or be pushed into a different new state by others. However, it is not easy for computer algorithms to give most appropriate considerations to all these three characteristics of ERC like humans <ref type="bibr" target="#b7">[8]</ref>. To capture the contextual emotional changes as much as possible during actual research processes, majority studies choose to build models on whole dialogue range <ref type="bibr" target="#b8">[9]</ref> and extra layers are designed to track the flow of emotion states of each interlocutor throughout the conversations <ref type="bibr" target="#b9">[10]</ref>. However, rare researches pay the same attention on the real-time performance of ERC as on improving the recognition accuracy, despite the many benefits of real-time capability like helping the computer show better responses according to the emotion of speakers on current time spot.</p><p>Motivated by the issues mentioned above, a multimodal emotion recognition model for conversational videos based on reinforcement learning (RL) and domain knowledge (ERLDK) is proposed in this paper. Text, visual and audio are used in ERLDK as the multimodal input since they are the three most common expression modals in both videos and human daily life. Our model consists of three modules and the overview structure is depicted in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>Firstly, to maintain the real-time performance and include contextual information simultaneously <ref type="bibr" target="#b10">[11]</ref>, the concept of emotion-pair is defined as the smallest emotion stage unit of a conversation. The emotion-pair is combined of multiple continuous sentences sampled from both sides of interlocutors with a fixed window length and the emotion of the coming next sentence from one of the interlocutors becomes the target waiting to be recognized. At the same time, the correlations between the emotion-pair and the target are summarized as the domain knowledge during this step. For real-time ERC, the sentences after the target utterance are not considered, because the following dialogues all happen in the future which cannot be used to help the recognition of the present. Secondly, based on the definition of the emotion-pair, the reinforcement learning (RL) module learns the transformation of the emotion states. In this module, different modalities are fused at feature level so that the fusion of unimodal between utterances comes before the fusion with other modalities. To enhance the ability of ERC, the recognition results of the other states are taken into consideration while recognizing the emotion of the current state by utilizing the dueling deep-Q-network (DDQN) <ref type="bibr" target="#b11">[12]</ref>. The structure of DDQN in this RL module is depicted in Figure <ref type="figure" target="#fig_1">2</ref> and the sample window is set to be three as an example. Followed by the RL modules, the third module uses domain knowledge of emotion-pair and the target to revise the recognition results from the RL module. However, as emotion is closely linked to each human's character, the actual situation is impossible to summarize in total as commonsense knowledge is not absolute <ref type="bibr" target="#b12">[13]</ref>. Therefore, the first emotion-pair is utilized as the base domain knowledge of the current dialogue atmosphere to revise the following recognize outputs sentence by sentence. During this process, the revision effects of the domain knowledge will gradually decrease along with the accumulated recognize deviation brought by the RL module, but our method still achieves superiority over the baselines, especially on dialogues with the length less than 30 steps. The main contributions of this paper are listed as below:</p><p>• A new multimodal emotion recognition model for conversational videos based on reinforcement learning and domain knowledge (ERLDK) is proposed in this paper.</p><p>To the best of our knowledge, this is the first time that RL and domain knowledge are combined for real-time ERC.</p><p>The emotion of speakers are recognized on dialogue level as the conversations progress step by step. Text, visual and audio information are fused as multimodal inputs. • Human daily emotion transformation habits are innovatively extracted as the domain knowledge of the emotion recognition for conversational videos in ERLDK. Three window sizes for extraction in dialogues are experimented that the size of three turns out to be the most appropriate. • Experiments on two public datasets are carried out to examine the performance of ERLDK. The relationship between recognition ability and the changes of conversation lengths is also studied during the test. The results surpass the state-of-the-art baselines on most emotion categories. The rest of this paper is structured as follows: Section II briefly reviews important previous work in multimodal emotion recognition in conversations. Section III describes our method in detail. Section IV presents the experimental setups. Section V illustrates results and discussions. Finally, Section VI provides concluding remarks and directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>ERC research has become popular and attractive in both the scientific community and the business world. ERC studies can be mainly divided into two categories as unimodal emotion recognition and multimodal emotion recognition according to the number of the input modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Unimodal Emotion Recognition</head><p>Text is one of the most frequently unimodal used for ERC due to its advantages of expressing the conversation information clearly and continuously <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Peng et al. <ref type="bibr" target="#b15">[16]</ref> proposes a text based model which fuses the word-level and sentence-level features to learn emotional expression which focuses on human-machine dialogue systems. Majumder et al. <ref type="bibr" target="#b16">[17]</ref> proposes a Recurrent Neural Networks (RNN) based model named DialogueRNN that can not only analyze the contexts information but also keeps track of each individual party emotion state. Ghosal et al. <ref type="bibr" target="#b17">[18]</ref> presents a Dialogue Graph Convolutional Network (Dia-logueGCN) model based on former DialogueRNN model. DialogueGCN solves the context propagation issues of Dia-logueRNN by leveraging the dependency of all the interlocutors. DialogueRNN and DialogueGCN are superior methods that conversation information are computed on sentence level and interlocutor level to capture the emotion habits of each person. Both of them can be utilized on multispeakers dialogue scenes and achieves competitive experiments results. Some other unimodal approaches ERC include works that focus on audio <ref type="bibr" target="#b18">[19]</ref>, visual <ref type="bibr" target="#b19">[20]</ref> and electroencephalogram (EEG) <ref type="bibr" target="#b20">[21]</ref>. Dicé et al. <ref type="bibr" target="#b21">[22]</ref> transcribes and analyzes audio conversations with verona coding definitions of emotional sequences to establish a quantitative relationship between asymmetrical variables. Sisk et al. <ref type="bibr" target="#b22">[23]</ref> utilizes its proposed model based on audio features into health psychology field. Huang et al. <ref type="bibr" target="#b23">[24]</ref> focuses on nonverbal sounds which naturally exists in our daily conversation. Verbal and nonverbal segments within an utterance are extracted by a Prosodic Phrase (PPh) auto-tagger and an attentive long short-term memory (LSTM)-based sequence-to-sequence model. Unlike audio signals in which tone-related information are paid mainly attention <ref type="bibr" target="#b24">[25]</ref>, visual pictures are more intuitive <ref type="bibr" target="#b25">[26]</ref> and have better readability <ref type="bibr" target="#b26">[27]</ref>. Tao et al. <ref type="bibr" target="#b27">[28]</ref> proposes a two-stage module to find the low-dimensional tensor subspace and computing the spectral of the face tensors. Hofmann et al. <ref type="bibr" target="#b28">[29]</ref> investigates the elicitation of smiling and laughter and the role of facial display regulation markers in positive emotions during conversations. Emotion-related features are encoded by Ryu et al. <ref type="bibr" target="#b29">[30]</ref> that the robustness of edge patterns in the edge region are taken into consider with the smooth regions. Deep features and handcraft features of multiple views are innovatively combined in a simple but effective way by Tao et al. <ref type="bibr" target="#b30">[31]</ref> for person re-identification. Different strategies are utilized for both deep features and handcraft features. Both audio and visual signals mentioned above are common and frequently used in daily life <ref type="bibr" target="#b31">[32]</ref>, however, these information are likely to be contradictory or deceptive when they are analyzed on their own <ref type="bibr" target="#b32">[33]</ref>. On the contrary, EEG signals can make up for these ambiguities by recording the ongoing neuronal activities of the brain. Gupta et al. <ref type="bibr" target="#b33">[34]</ref> proposes an effective method based on flexible analytic wavelet transform (FAWT) for recognition of emotion through the investigation of the channel specific nature of EEG. EEG performs better accuracy in many cases, however, it is not easy to obtain which makes it hard to be applied to daily life conveniently.</p><p>Unimodal ERC method achieves a lot of remarkable results and has its own application fields. However, using unimodal for ERC is insufficient and unstable in several cases, for example, faces can be blocked <ref type="bibr" target="#b34">[35]</ref> and audio signals may mislead the results without the text <ref type="bibr" target="#b35">[36]</ref>. Hence, the research focus of ERC is attempting for multimodal methods now.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multimodal Emotion Recognition</head><p>The expression of human during conversations is the comprehensive result of multiple behavioral patterns <ref type="bibr" target="#b36">[37]</ref>, therefore, there are many benefits to use multimodal as the inputs <ref type="bibr" target="#b38">[38]</ref>. RNN and RNN-like networks are mainstream algorithms due to their significant advantages in processing sequence information <ref type="bibr" target="#b39">[39]</ref>. Zhang et al. <ref type="bibr" target="#b40">[40]</ref> builds a quantum-like multimodal network (QMN), which uses quantum theory (QT) and a LSTM network to analyze sentiment in conversations. Text and image are used as the multimodal inputs. Some novel methods also choose to improve the performances by integrating RNN with other algorithms and other relative subjects knowledge <ref type="bibr" target="#b41">[41]</ref>. Plaza-del Arco et al. <ref type="bibr" target="#b42">[42]</ref> recognizes emotions by integrating different affective lexical knowledge from Spanish social media with neural networks. Wang et al. <ref type="bibr" target="#b12">[13]</ref> proposes a multimodal deep regression Bayesian network (MMDRBN) to compute the relationship between audio and visual modalities for emotion recognition and domain knowledge from videos are incorporated. However, the contextual information and the habitual of human are not included in these domain knowledges. Besides combing domain knowledge, the relationship between speakers are also helpful during ERC. Xing et al. <ref type="bibr" target="#b10">[11]</ref> proposes an Adapted Dynamic Memory Network (A-DMN) in which regards the self and cross-speaker influence as two mainly points to the emotion flows of conversation. This method uses audio, visual and text inputs and receives great results on public datasets. However, these multimodal ERC methods rely deeply on using whole dialogue information to improving accuracy that only a few of recent models pays attention on the real-time capability of ERC.</p><p>Hazarika et al. <ref type="bibr" target="#b43">[43]</ref> introduces a conversational memory network (CMN) that comprises audio, visual and text features with gated recurrent units. The sample window of the dialogue is set to be eight that each speaker memories four sentences before recognition the next sentence. Lai et al. <ref type="bibr" target="#b44">[44]</ref> proposes a different contextual window sizes based recurrent neural networks (DCWS-RNNs) model. Four different RNNs are designed to compute the contexts separately that two for utterances before the target and the other two for utterances after the target. These kind of methods show competitive results with good real-time recognition capability for it does not need the whole dialogue information as assistance, but they still have a certain delay that three utterances after the target are required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Reinforcement Learning</head><p>To ensure the recognition accuracy as well as the real-time performance, RL is applied as the main module in this paper. Recognizing emotion using RL <ref type="bibr" target="#b45">[45]</ref> and knowledge-based system <ref type="bibr" target="#b46">[46]</ref> are not new things actually. Liu et al. <ref type="bibr" target="#b47">[47]</ref> proposes a Reinforcement Online Learning (ROL) method for real-time emotion state prediction by using EEG. This method applies the ROL to least square (LS) and support vector regression (SVR) for emotion prediction. Li and Xu <ref type="bibr" target="#b48">[48]</ref> proposes a RL model for pre-selecting (RLPS) useful images for facial emotion recognition, which is made up of an image selector and a rough emotion classifier.</p><p>RL has its unique advantages on imitating the real-time conversion of the conversations <ref type="bibr" target="#b49">[49]</ref> and the domain knowledge in conversations <ref type="bibr" target="#b50">[50]</ref> is also meaningful because the emotion of speakers is relatively following regular emotion inertia <ref type="bibr" target="#b51">[51]</ref>. However, few of them try to combine RL and domain knowledge in real-time ERC, even less using different modalities as inputs <ref type="bibr" target="#b52">[52]</ref>, <ref type="bibr" target="#b53">[53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>In this section, the ERLDK model for multimodal emotion recognition in video conversations is introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Preprocess Module</head><p>This module is the first module of the ERLDK model which is responsible for generalizing emotion-pair and corresponding domain knowledge. To recognize the emotion category of the target utterance inside a specific dialogue, the ability of properly combining contextual information is the key point. The concept of the emotion-pair is defined to represent the effective contextual information of the target utterance. The emotion-pair consists of several utterances before the target with a fixed sample length, in which the emotion persistence and contagious are potentially exposed. For example, when the sample length is set to be three, every three utterances are packed as an emotion-pair in the order of their appear sequences from the start to the end in a dialogue. During this step, both the multimodalities and the label of these three utterances are recoded in the new processed emotion-pair dataset. To find the best choice of this sample size, four different lengths are examined to make a balance between enough contexts and less redundancy. These four examined lengths are two utterances, three utterances, four utterances and 5 utterances before target respectively and the sampled corresponding emotion-pairs will construct the new trainsets and testsets.</p><p>The corresponding domain knowledge under each length is computed based on these new datasets under emotion-pair form. The labels of a emotion-pair with the original happening order is defined as its corresponding label-pair. The label-pair of a specific emotion-pair represent the current emotion atmosphere. On the basis of this atmosphere, not all emotion categories share the same probability of occurrence that some of the emotions are actually on the opposite side of each other. For example, with a label-pair as happy-excitedhappy, it is impossible for the next target become sad or angry.</p><p>On the contrary, happy and excited emotion enjoy very high probabilities and also neutral can happen in some cases. Such common regulations are summarized as the domain knowledge under different sample lengths to revise the recognition results of the next RL module at the final step. The widely accepted public dataset is applied as the hotbed to sum up the domain knowledge, because the emotion conversion rules are universal at most circumstances. The generalized domain knowledge will be transformed to apply on other dataset. The emotion categories are divided into six kinds: happy, sad, neutral, angry, excited and frustrated, each of which has different occurrence probability after giving the former label-pair. Each probability of six emotions after all kinds of label-pair is computed by counting the number of occurrences and normalize the probabilities by a sof tmax function. The domain knowledge with specific sample size is built. Each of the probability represents the correlation between an emotion and the corresponding label-pair.</p><p>Let L represents the specific label-pair and e denotes one of the six emotion categories. Num(e|L) represents the number of occurrences of the emotion e with the former label-pair as L. Num(L) represents the number of occurrence of label-pair L in the trainset of the dataset. P(e|L) represents the probability of occurrence of the emotion e under former label-pair as L. The C(e|L) is the final correlation between emotion e and label-pair L which is recorded as domain knowledge. The calculation formulas are as below.</p><formula xml:id="formula_0">P(e|L) = Num(e|L) Num(L)<label>(1)</label></formula><formula xml:id="formula_1">C(e|L) = so f tmax(P(e|L))<label>(2)</label></formula><p>The size of sample window will influence the revise performance of this domain knowledge. Short label-pair will not be able to support enough domain knowledge for the target and conversely, label-pair with a too long size will greatly restrict the flexibility and generalization ability of the domain knowledge. Long label-pair will bring unnecessary constraints to the recognition results with less fault tolerance. When the RL module misidentified the emotion type of an utterance, the exclusivity between classes in emotions becomes greater that domain knowledge with too long label-pair will not only fail to revise, but will make its own claim and produce completely wrong results to all following recognition steps of the same dialogue. In general, the domain knowledge should exclusive the impossible emotion results and give minor corrective support instead of becoming the main factor affecting the final results. The domain knowledge of the four different sample sizes will be presented in following section with other experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Reinforcement Learning Module</head><p>Emotion in conversations is inter-related that happens one after another step by step. This is similar to the sequence state transitions in RL and the action chosen according to the current emotion-state. The reward function is influenced by both of the current emotion-state and the chosen action, which represents the context in ERC and the recognition results of the target respectively. This reinforcement learning module is the core module to recognize the target emotion with features of the corresponding emotion-pair. Dueling deep-Q-network (DDQN) is used as the learning algorithm in RL module and separate process approaches are utilized on trainset and testset. The flow chart of the DDQN and the structure of the reinforcement learning module are depicted in Figure <ref type="figure" target="#fig_1">2</ref>. In Figure <ref type="figure" target="#fig_1">2</ref>, the multi-modalities of the emotion-state s(t) are the input of the Q(s(t), a(t)) at time step t. Text T (t), visual V (t) and audio A(t) are separately fused first before cross modal fusion. Bidirectional gated recurrent unit cell( ← −−−−− → G RUCell), a RNN based network, is utilized to capture the contextual relationships inside of each unimodal. The fused unimodal features, T f (t),V f (t) and A f (t), are cascaded as F usion (T f (t), V f (t), A f (t)) for cross modal fusion through a bidirectional multi-layer gated recurrent unit RNN ( ← − → G RU ). Three dense layers Dense are connected to the output of the ← − → G RU layer for utilizing the dueling mechanism to optimize the convergence speed. The specific calculation process is as follows.</p><p>At time step t, the features of an emotion-pair E pair (t) and the corresponding target T arget (t) are packed in pairs as an new integrated emotion-state s(t), s(t) = [E pair (t), T arget (t)] of the input of DDQN, regardless of the original conversation they belong to and all these kind of states construct the whole RL environment S, s ∈ S. During training, the Q net of DDQN is trained to output the right probabilities q eval (s(t), a(t)) of chosen action from the six alternative emotions, which is represented as a(t) ∈ A, A ct ion = [0, 1, 2, 3, 4, 5]. The numbers in A ct ion represent the happy, sad, neutral, angry, excited and frustrated respectively. The recognition result of the target utterance q action is achieved as below. q eval (s(t), a(t)) = Q(s(t), a(t))</p><p>(3)</p><formula xml:id="formula_2">q action = argmax(Log_So f tmax(q eval (s(t),a(t))))<label>(4)</label></formula><p>where Q(s(t), a(t)) is the Q net of DDQN. The detailed calculation formula of Q(s(t), a(t)) are as below.</p><formula xml:id="formula_3">T f (t) = ← −−−−− → G RUCell(T (t)) (5) V f (t) = ← −−−−− → G RUCell(V (t)) (6) A f (t) = ← −−−−− → G RUCell(A(t)) (7) F usion (t) = ← − → G RU(T f (t), V f (t), A f (t)) (8) V, A = Dense(F usion (t)) (9) q eval (s(t), a(t)) = Dueli ng(V + (A − 1 |A| a A)) (10)</formula><p>where V and A represent the original q eval (s(t), a(t)) from the Q(s(t), a(t)) and the average of advantages of each action on s(t). Dueli ng represents the update of the original q eval (s(t), a(t)) by dueling mechanism. The reward function R is computed as below. r is the value of reward and the label(t) means the right emotion label of the T arget (t).</p><formula xml:id="formula_4">R = q action = label(t), R = r else, R = −r<label>(11)</label></formula><p>Authorized licensed use limited to: Tsinghua University. Downloaded on September 06,2022 at 10:44:39 UTC from IEEE Xplore. Restrictions apply.</p><p>The features of the next state s(t + 1) at next time step of current time t is input to the target Q net, which is represented as Q to compute the loss function Loss(t) as below. q eval (s(t + 1), a(t + 1)) = Q (s(t + 1), a(t + 1)) <ref type="bibr" target="#b11">(12)</ref> q ex pect (s(t + 1), a(t + 1)) = R + γ max a(t +1) q eval (s(t + 1), a(t + 1)) ( <ref type="formula">13</ref>)</p><formula xml:id="formula_5">Loss(t) = E[q ex pect (s(t + 1), a(t + 1)) − q eval (s(t + 1), a(t + 1))]<label>(14)</label></formula><p>The Loss(t) is back propagated to optimize the Q net and the target Q net will updated using the parameters of the Q net after fixed step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The Domain Knowledge Revise Module</head><p>The domain knowledge revise module is applied during the test step. Unlike the random drawing states from the trainset with replacement strategy, the test is undertaken on the dialogue level. The emotion-states from the same dialogue are tested state by state to examine the recognition performance. The initialization state of the RL environment is one of the very beginning emotion-pair of a dialogue randomly sampled from the video clips of testset. The corresponding emotion label-pair of this emotion-pair will be given as the clue of the original emotion atmosphere. The third revise module use this clue to revise the recognized output from the RL module which recognizes the emotion states of the conversation step by step by just using the multimodal features until reaching the final signal and begins recognizing the other dialogue by same steps. The recognition results of a same dialogue on each step are recorded. The three recognized emotion types before the target utterances are used as the index to search for the corresponding C(e|L) in domain knowledge. According to Equation (1) and Equation (2), C(e|L) contains six probability of occurrence of the six alternative emotion types based on the former three recognized emotions which is in the same format of the RL module output. In order not to lose generality, we just simply add these two sets of probability values together to accomplish the revise instead of training extra layers on specific dataset for better improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL SETUP</head><p>In this section, the experimental setups and steps of our method are showed, including the datasets, baselines, metrics, training task and the process design of experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>Our model is evaluated on two widely accepted public datasets, IEMOCAP <ref type="bibr" target="#b54">[54]</ref> and MELD <ref type="bibr" target="#b55">[55]</ref>. These two datasets are multimodal datasets recording textual, visual and audio information of each video conversation on utterance level. Both of two datasets have been randomly partitioned into training set and testing set when they are released and share no same speakers. IEMOCAP dataset contains conversational videos between two interlocutors that ten unique speakers are included. Only the first eight speakers belong to the trainset. Each video contains a single dyadic dialogue and is labeled on utterance level with one of six emotion labels, which are happy, sad, neutral, angry, excited, and frustrated. Text, audio and visual information of each utterance are included.</p><p>MELD is a multimodal emotion and sentiment classification dataset which is annotated on utterance level as one of the seven emotion classes: anger, disgust, sadness, joy, surprise, fear and neutral. MELD is a multi-party conversational videos dataset that contains text, audio and visual modalities for more than 1400 dialogues and 13000 utterances from the Friends TV series.</p><p>In the pre-process module, these original utterance-level items are packed into the form of emotion-states, which include the current emotion-pair and target and the emotion-pair and target of next time step for DDQN to compute the q t +1 _ex pect (s(t + 1), a(t + 1)). To prevent overfitting, in trainset, either current emotion-pair and target or the next step emotion-pair and target are unique that will not exists in any other emotion-state twice. In testset, the q t +1 _ex pect (s(t + 1), a(t + 1)) for DDQN is not needed that the emotion-states just include the current emotion-pair and target. The test are carried out step by step on dialogue level. As a result, the generalized emotion-states for trainset and testset are less than the number of items in original datasets. The proportion between train and test is about 3 : 1. The Table <ref type="table">I</ref> shows the distributional statistics of the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Baselines</head><p>The experimental results are discussed with the following recent baselines.</p><p>c-LSTM + Att <ref type="bibr" target="#b56">[56]</ref>: To represent the efficient context information for the target utterance, utterances around the target are input to attention based bidiredectional LSTM at each time spot.</p><p>TFN <ref type="bibr" target="#b57">[57]</ref>: This multimodal method fuses information of both each unimodal and cross modals only from present target object.</p><p>MFN <ref type="bibr" target="#b58">[58]</ref>: Multi-view learning is utilized in this model that self-view and cross-view of multimodalities are fused. This method also only use information only from present target object.</p><p>CMN <ref type="bibr" target="#b43">[43]</ref>: This method samples the sentences from two interlocutors before the current target as its history. The sample size is set to be eight and two separate memory networks are utilized.  ICON <ref type="bibr" target="#b59">[59]</ref>: ICON is similar to CMN that separate memory networks are used for speakers. The ICON adds a extra memory network between interlocutors besides self-speakers and the sample size is set to be the overall dialogues.</p><p>BiDialogueRNN + Att <ref type="bibr" target="#b16">[17]</ref>: BiDialogueRNN + Att uses three GRUs to capture the target emotion, the context information and the changes of the overall dialogue. This method also needs the whole conversation features that has no capable of real-time performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Metrics</head><p>To evaluate the experimental performances, two classical parameters, the accuracy and macro-average F-score are calculated. F-score is computed as below.</p><formula xml:id="formula_6">F β = (1 + β 2 ) • precision • r ecall (β 2 • precision) + r ecall (<label>15</label></formula><formula xml:id="formula_7">)</formula><p>where β represents the weight between precision and recall. In this paper, β is set to be 1 which means precision and recall are regarded to share same weight. To evaluate the significance of our experimental results, paired T-test is conducted to calculate the P-value between our method and baselines on two datasets. The paired T-test is computed as below.</p><formula xml:id="formula_8">t = d − d 0 S d / √ n (16) d f = n − 1 (<label>17</label></formula><formula xml:id="formula_9">)</formula><p>where d represents the sample mean of differences, d 0 represents the hypothesized population mean difference, S d is the standard deviation of differences, d f is the degree of freedom. P-value is calculated with the significance level α set to be 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experimental Parameter Setting</head><p>The dimension of pre-processed text, audio and visual are 100, 100 and 512 respectively. The number of layer for both ← −−−−− → G RUCell and ← −→ G RU are set to be one with the number of hidden layer as 512 and drop probably as 0.3. The γ parameter for DDQN is 0.9 and the updated frequency of the target Q net is 100 with the learning rate as 0.00015 and the weight decay of the optimizer is set to be 0.00001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS AND DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Results</head><p>Firstly, the domain knowledge from the first module with four different sample window sizes from two utterances to five utterances are presented as form of heat map as Figure <ref type="figure" target="#fig_2">3</ref>, Figure <ref type="figure" target="#fig_3">4</ref>, Figure <ref type="figure" target="#fig_4">5</ref>, Figure <ref type="figure" target="#fig_5">6</ref> and Figure <ref type="figure" target="#fig_6">7</ref>. Figure <ref type="figure" target="#fig_2">3</ref>, Figure <ref type="figure" target="#fig_3">4</ref> and Figure <ref type="figure" target="#fig_4">5</ref> represented the domain knowledge of the sample window size as two, three and four respectively. The domain knowledge of window size as five are split into two figures to present which are Figure <ref type="figure" target="#fig_5">6</ref> and Figure <ref type="figure" target="#fig_6">7</ref>. The abscissa of the heat map is all the possible candidate emotion types. The ordinate of the heat map represents the current recognized emotion-pair. The virous heat colors means the different probability of occurrence of the coming candidate emotion types after the different known emotion-pair. The lighter color represents the higher probability of occurrence of the target emotion based on the current emotion-pair and vice versa. In Figure <ref type="figure" target="#fig_2">3</ref>, which the size of sample window is 2, the base emotion-pair cases are too little to give enough assistances for revise. On the contrary, there are too many probabilities of occurrence as near to 1.0 that the instructions from domain knowledge are too strong to have enough tolerance and generalize ability. Secondly, to test the performances of different sizes of sample window, experiments on IEMOCAP and MELD are carried out with total dialogues in testsets and the results are listed in Table II compared with the other results of baselines.</p><p>From the experimental results from Table <ref type="table">II</ref>, ERLDK with sample window size of three and four achieves best performances. However, ERLDK with sample window size of four has a much lower generalization ability than sample window size as three. As the result, the size of three is chosen as the sample window for the following testing.</p><p>To examine the real-time capabilities of our model step by step during recognizing the emotion of every utterance in dialogues with the original order, experiments on dialogue-level are conducted. The specific F1-scores of every step of the dialogues in test set are listed in Table <ref type="table" target="#tab_1">III</ref>. In Table <ref type="table" target="#tab_1">III</ref>, our ERLDK method has the real-time recognition ability, so the F1-scores are changing as the length of the dialogues increase. On the contrary, all the baselines has no real-time ability, so the F1-scores are all average results that summarized through the overall test set. The experimental results with sample window size as three on IEMOCAP are presented in Figure <ref type="figure" target="#fig_7">8</ref>. In Figure <ref type="figure" target="#fig_7">8</ref>, results of ERLDK and all baselines are depicted. The curves of ERLDK are varied as the steps of the dialogues and the curves of the baselines are straight dottted lines that keep in same on whole dialogue level. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Discussion</head><p>In ERC model, the span of contexts sampled for a specific utterance have a great influence on the final recognition ability. This is also the reason that many mainstream methods explore to improve their accuracies by capturing all related information from the total dialogue. However, this is not practical in many application scenarios without the real-time recognition ability. In our model, to find the best sample span, four sample window size are chosen to integrate the needed contexts. The emotions of these contexts provide reference information for the next appearing target which are then summed up as the domain knowledge. The domain knowledge under different sample lengths is captured which are presented in Figure <ref type="figure" target="#fig_2">3</ref> to Figure <ref type="figure" target="#fig_6">7</ref> by showing the correlation between emotion-pairs and the next six alternative emotion types through heat maps.</p><p>Compared to the few emotion-pairs in Figure <ref type="figure" target="#fig_2">3</ref>, the other four figures obviously make a more detailed distinction of emotional clues for the potential follow-ups. To find the best window size and the corresponding domain knowledge, we conduct experiments with four kinds of window sizes and -score for about 10% on happy emotion which is a big margin. Although the best results are not obtained in all six categories, the recognition efficiency is more balanced than the baselines instead of being completely biased to a certain emotion type. As for the difference performances between the four sampling sizes, ERLDK with sample size as three and four perform better that the other two sizes on almost all metrics. Longer sample size does not receive positive effects not only on the dataset it extracted from but also worse than the smallest sample size of the contexts. It shows that it is not more instructions suggesting better performances. This is because the lack of the fault tolerance ability that leads to an imbalance between the recognition ability of RL module and the revise ability of the domain knowledge. This imbalance brings about poor generalization ability that result in the lowest accuracy and F1-score while transforming to another dataset.</p><p>To test the real-time performance of ERLDK, we conduct dialogue level experiments on IEMOCAP dataset and the results are showed in Figure <ref type="figure" target="#fig_7">8</ref> and Table <ref type="table" target="#tab_1">III</ref>. Figure <ref type="figure" target="#fig_7">8</ref> shows the recognition performance changes with the conversation progresses and the Table III lists all ground truth of each recognition steps. In Figure <ref type="figure" target="#fig_7">8</ref>, the length of steps in X axis mean the number of utterances which accept recognition in occurring order after the very first given emotion-pair of each dialogue. The accuracy and F1-score in Y axis mean the test results of each length of step. Figure <ref type="figure" target="#fig_7">8</ref>(a) and 8(b) are weighted average accuracy and F1-score on all emotion classifications. The other subfigures are F1-score on six emotion classification respectively. The black thick solid curves are results of ERLDK and the green thick solid curves are the results of only RL module along with the steps of the dialogues without the domain knowledge revises for ablation study. It can be seen clearly that the results of only RL module perform less than the complete ERLDK from 2.21% to 7.08% which shows the effectiveness of the domain knowledge in improving the recognizing performance. The lines in other colors are the results of the baselines. The baselines have no real-time ability to recognize the emotion of the utterances as their appearances, so the results of baselines keep unchanged on whole dialogue level which is the biggest difference between ERLDK and other baselines. To show the performances in more detail, we list all specific F1-score values of each steps on average and six emotion types in Table <ref type="table" target="#tab_1">III</ref> for reference. In Table <ref type="table" target="#tab_1">III</ref>, SOTA means the state-of-the-art results which has already showed in Table II on each emotion types and the average. From these figures, most of them receive the highest recognition F1-score in the first step except the frustrated emotion because of the revise from the domain knowledge. This advantage becomes less conspicuous during around the fifteenth step and enters a plateau. ERLDK surpasses the baselines on most emotion types and the average from 1.21% to 6.64% at the plateau and surpasses the baselines more on the first fifteen steps. The frustrated emotion in Figure <ref type="figure" target="#fig_7">8</ref>(h) is still opposite exception that the recognition performance is not good enough until the plateau. We analysis this exception that frustrated emotion is easy to be confused by both recognition algorithms and domain knowledge which lead to this negative influences. After the step of around thirty to thirty-five, the recognition performance has a significant decline. This is because most dialogues in testsets do not have this long conversation length.</p><p>Only four dialogues contain more than fifty length of steps for testing. This makes the uneven distribution of emotion types that influence the results. From the ablation study which shows in Figure <ref type="figure" target="#fig_7">8</ref> and Table <ref type="table" target="#tab_1">III</ref>, domain knowledge can greatly improve the performance in the early stages and help our method stabilize at a better value than all the baselines in a relative long-term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND FUTURE WORK</head><p>In this paper, we proposed a multimodal emotion recognition model based on reinforcement learning and domain knowledge in video conversations. In particular, the reinforcement learning module is utilized to perform real-time recognition of conversation occurrence, while domain knowledge leverages emotion-pairs to revise recognition results. To the best of our knowledge, this is the first time that these two elements are combined for real-time ERC. We achieve the state-of-the-art results on average and most of specific emotion categories. As future work, we plan to continue optimize the RL module and the extraction of domain knowledge to improve recognition ability on all emotion classifications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The overview of the proposed ERLDK.</figDesc><graphic url="image-1.png" coords="2,48.95,58.61,513.02,267.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The structure of the dueling DQN of RL module in ERLDK with sample size as three.</figDesc><graphic url="image-2.png" coords="3,73.43,58.85,464.30,237.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The correlation between emotion and emtion-pire with sample window as two.</figDesc><graphic url="image-3.png" coords="7,99.95,58.49,411.74,61.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The correlation between emotion and emtion-pire with sample window as three.</figDesc><graphic url="image-4.png" coords="7,99.95,155.81,411.74,158.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The correlation between emotion and emtion-pire with sample window as four.</figDesc><graphic url="image-5.png" coords="8,99.95,58.25,411.62,306.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The correlation between emotion and emtion-pire with sample window as five.</figDesc><graphic url="image-6.png" coords="9,99.95,58.49,411.48,588.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The correlation between emotion and emtion-pire with sample window as five.</figDesc><graphic url="image-7.png" coords="10,99.95,58.01,411.48,589.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The comparation of accuracy and F1-score with baselines.</figDesc><graphic url="image-14.png" coords="12,75.95,478.01,226.34,106.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I DATASETS SPLIT</head><label>ISPLIT</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III THE</head><label>III</label><figDesc>SPECIFIC F1-SCORES OF EVERY STEP OF THE DIALOGUES IN TEST SET. BOLD FONT DENOTES THE BEST PERFORMANCES. P-VALUE IS LESS THAN 0.05 FOR PAIR T-TEST domain knowledge on two datasets. The experimental results are listed in TableII. In TableII, our method with four sample window sizes are compared to other baselines on two datasets by using the recognition results on total length of dialogues.</figDesc><table><row><cell>Experimental statistics on each classification of IEMOCAP</cell></row><row><cell>are listed in detailed for sufficient public data for reference.</cell></row><row><cell>From this table, the accuracy and F1-score of four sample</cell></row><row><cell>sizes which are showed in the last four rows surpass most</cell></row><row><cell>baselines. Particularly, our ERLDK with sample size as three</cell></row><row><cell>achieves the best F1-score performance on happy, excited,</cell></row><row><cell>frustrated and total average recognitions. ERLDK with sample</cell></row><row><cell>size as three achieves the best F1-score performance on sad</cell></row><row><cell>recognition. Especially, ERLDK model surpasses the sate-of-</cell></row><row><cell>the-art accuracy and F1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: Tsinghua University. Downloaded on September 06,2022 at 10:44:39 UTC from IEEE Xplore. Restrictions apply.</note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>. This work was supported in part by the National Natural Science Foundation of China under Grant 61871470, Grant U1801262, and Grant 61976179; and in part by the Fundamental Research Funds for the Central Universities under Grant 3102019HTXM005 and Grant 3102017HQZZ003. This article was recommended by Associate Editor Z. Li.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Driver drowsiness recognition via 3D conditional GAN and two-level attention bi-LSTM</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Video Technol</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4755" to="4768" />
			<date type="published" when="2020-12">Dec. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">video action clustering via motion-scene interaction constraint</title>
		<author>
			<persName><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="144" />
			<date type="published" when="2020-01">Jan. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dynamic emotion modelling and anomaly detection in conversation based on emotional transition tensor</title>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="11" to="22" />
			<date type="published" when="2019-03">Mar. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Video summarization with attention-based encoder-decoder networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1709" to="1717" />
			<date type="published" when="2020-06">Jun. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long-shortterm features for dynamic scene classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1038" to="1047" />
			<date type="published" when="2019-04">Apr. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">PCC Net: Perspective crowd counting via spatial convolutional network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3486" to="3498" />
			<date type="published" when="2020-10">Oct. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Interactive double states emotion cell model for textual dialogue emotion prediction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl.-Based Syst</title>
		<imprint>
			<biblScope unit="volume">189</biblScope>
			<date type="published" when="2020-02">Feb. 2020. 105084</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multimodal local-global attention network for affective video content analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCSVT.2020.3014889</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol., early access</title>
		<imprint>
			<date type="published" when="2021-08-07">Aug. 7, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An interaction-aware attention network for speech emotion recognition in spoken dialogs</title>
		<author>
			<persName><forename type="first">S.-L</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP-IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)</title>
				<meeting>ICASSP-IEEE Int. Conf. Acoust., Speech Signal ess. (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2019-05">May 2019</date>
			<biblScope unit="page" from="6685" to="6689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hybrid approach for emotion classification of audio conversation based on text and speech mining</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sruthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nedungadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="635" to="643" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adapted dynamic memory network for emotion recognition in conversation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TAFFC.2020.3005660</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput., early access</title>
		<imprint>
			<date type="published" when="2020-06-29">Jun. 29, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Identity-preserving face hallucination via deep reinforcement learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4796" to="4809" />
			<date type="published" when="2020-12">Dec. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Knowledge-augmented multimodal deep regression Bayesian networks for emotion video tagging</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1084" to="1097" />
			<date type="published" when="2020-04">Apr. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Conversational transfer learning for emotion recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2021-01">Jan. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Locality adaptive discriminant analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th Int. Joint Conf</title>
				<meeting>26th Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="2017-08">Aug. 2017</date>
			<biblScope unit="page" from="2201" to="2207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Human-machine dialogue modelling with the fusion of word-and sentence-level emotions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl.-Based Syst</title>
		<imprint>
			<biblScope unit="volume">192</biblScope>
			<date type="published" when="2020-03">Mar. 2020. 105319</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">DialogueRNN: An attentive RNN for emotion detection in conversations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf</title>
				<meeting>AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6818" to="6825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">DialogueGCN: A graph convolutional neural network for emotion recognition in conversation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chhaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.11540</idno>
		<ptr target="http://arxiv.org/abs/1908.11540" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Annotating and modeling empathy in spoken conversations</title>
		<author>
			<persName><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Danieli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Riccardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Speech Lang</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="40" to="61" />
			<date type="published" when="2018-07">Jul. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Linear disentangled representation learning for facial actions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3539" to="3544" />
			<date type="published" when="2018-12">Dec. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Identifying stable patterns over time for emotion recognition from EEG</title>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="417" to="429" />
			<date type="published" when="2019-09">Sep. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploring emotions in dialog between health provider, parent and child. an observational study in pediatric primary care</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dicé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dolce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maiello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Freda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pratiques Psychologiques</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="84" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Emotional communication in advanced pediatric cancer conversations</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Sisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Mack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Pain Symptom Manage</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="808" to="817" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Speech emotion recognition using deep neural network considering verbal and nonverbal speech sounds</title>
		<author>
			<persName><forename type="first">K.-Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-B</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP-IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)</title>
				<meeting>ICASSP-IEEE Int. Conf. Acoust., Speech Signal ess. (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2019-05">May 2019</date>
			<biblScope unit="page" from="5866" to="5870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sentiment analysis and topic recognition in video transcriptions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Stappen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning blur invariant binary descriptor for face recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">404</biblScope>
			<biblScope unit="page" from="34" to="40" />
			<date type="published" when="2020-09">Sep. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Face sketch synthesis by multidomain adversarial learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1419" to="1428" />
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tensor rank preserving discriminant analysis for facial recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="325" to="334" />
			<date type="published" when="2018-01">Jan. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Laughter and smiling in 16 positive emotions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ruch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="495" to="507" />
			<date type="published" when="2017-10">Oct. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Local directional ternary pattern for facial expression recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Rivera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="6006" to="6018" />
			<date type="published" when="2017-12">Dec. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep multi-view feature learning for person re-identification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2657" to="2666" />
			<date type="published" when="2018-10">Oct. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Heterogeneous face recognition: A common encoding feature discriminant approach</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2079" to="2089" />
			<date type="published" when="2017-05">May 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A multiview-based parameter free framework for group detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artif. Intell. (AAAI)</title>
				<meeting>AAAI Conf. Artif. Intell. (AAAI)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="4147" to="4153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cross-subject emotion recognition using flexible analytic wavelet transform from EEG signals</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Chopda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Pachori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sensors J</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2266" to="2274" />
			<date type="published" when="2019-03">Mar. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Robust Web image annotation via exploring multi-facet and structural knowledge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4871" to="4884" />
			<date type="published" when="2017-10">Oct. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning affective features with a hybrid deep model for audio-visual emotion recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3030" to="3043" />
			<date type="published" when="2018-10">Oct. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Social signal processing for evaluating conversations using emotion analysis and sentiment detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pisipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V N K</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Int. Conf</title>
				<meeting>2nd Int. Conf</meeting>
		<imprint>
			<date type="published" when="2019-02">Feb. 2019</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Authorized licensed use limited to: Tsinghua University</title>
		<imprint/>
	</monogr>
	<note>Downloaded on September 06,2022 at 10:44:39 UTC from IEEE Xplore. Restrictions apply</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Speech emotion recognition using convolutional neural network with audio word-based embedding</title>
		<author>
			<persName><forename type="first">K.-Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-B</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-R</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Int. Symp. Chin. Spoken Lang. Process. (ISCSLP)</title>
				<meeting>11th Int. Symp. Chin. Spoken Lang. ess. (ISCSLP)</meeting>
		<imprint>
			<date type="published" when="2018-11">Nov. 2018</date>
			<biblScope unit="page" from="265" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A dialogical emotion decoder for speech emotion recognition in spoken dialog</title>
		<author>
			<persName><forename type="first">S.-L</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP-IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)</title>
				<meeting>ICASSP-IEEE Int. Conf. Acoust., Speech Signal ess. (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2020-05">May 2020</date>
			<biblScope unit="page" from="6479" to="6483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A quantum-like multimodal network framework for modeling interaction dynamics in multiparty conversational sentiment analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="14" to="31" />
			<date type="published" when="2020-10">Oct. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Brain-inspired motion learning in recurrent neural network with emotion modulation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cognit. Develop. Syst</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1153" to="1164" />
			<date type="published" when="2018-12">Dec. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improved emotion recognition in Spanish social media through incorporation of lexical knowledge</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Plaza-Del-Arco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Martín-Valdivia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Ureña-López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mitkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Gener. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="1000" to="1008" />
			<date type="published" when="2020-09">Sep. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Conversational memory network for emotion recognition in dyadic dialogue videos</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. North Amer</title>
				<meeting>Conf. North Amer</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2122</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Different contextual window sizes based RNNs for multimodal emotion detection in interactive conversations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="119516" to="119526" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An active learning paradigm for online audio-visual emotion recognition</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kansizoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bampis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gasteratos</surname></persName>
		</author>
		<idno type="DOI">10.1109/TAFFC.2019.2961089</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput., early access</title>
		<imprint>
			<date type="published" when="2019-12-20">Dec. 20, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">ALSTM: An attention-based long short-term memory framework for knowledge base reasoning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">399</biblScope>
			<biblScope unit="page" from="342" to="351" />
			<date type="published" when="2020-07">Jul. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Reinforcement online learning for emotion prediction by using physiological signals</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="123" to="130" />
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for robust emotional classification in facial expression recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl.-Based Syst</title>
		<imprint>
			<biblScope unit="volume">204</biblScope>
			<date type="published" when="2020-09">Sep. 2020. 106172</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Connecting model-based and modelfree control with emotion modulation in learning systems</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSMC.2019.2933152</idno>
	</analytic>
	<monogr>
		<title level="j">Man, Cybern. Syst., early access</title>
		<imprint>
			<date type="published" when="2019-10-18">Oct. 18, 2019</date>
		</imprint>
	</monogr>
	<note>IEEE Trans. Syst.</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">ADRL: An attention-based deep reinforcement learning framework for knowledge graph reasoning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl.-Based Syst</title>
		<imprint>
			<biblScope unit="volume">197</biblScope>
			<date type="published" when="2020-06">Jun. 2020. 105910</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Informationdriven multirobot behavior adaptation to emotional intention in humanrobot interaction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hirota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cognit. Develop. Syst</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="647" to="658" />
			<date type="published" when="2018-09">Sep. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">How can we use knowledge about the neurobiology of emotion recognition in practice?</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Hunnikin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H M</forename><surname>Van Goozen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Criminal Justice</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<date type="published" when="2019-11">Nov. 2019. 101537</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Emotional editing constraint conversation content generation based on reinforcement learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="70" to="80" />
			<date type="published" when="2020-04">Apr. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">IEMOCAP: Interactive emotional dyadic motion capture database</title>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lang. Resour. Eval</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">335</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">MELD: A multimodal multi-party dataset for emotion recognition in conversations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 57th Annu. Meeting Assoc. Comput. Linguistics</title>
				<meeting>57th Annu. Meeting Assoc. Comput. Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="527" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Context-dependent sentiment analysis in user-generated videos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 55th Annu. Meeting Assoc. Comput. Linguistics</title>
				<meeting>55th Annu. Meeting Assoc. Comput. Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Tensor fusion network for multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Lang. Process</title>
				<meeting>Conf. Empirical Methods Natural Lang. ess</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1103" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Memory fusion network for multi-view sequential learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 32nd AAAI Conf</title>
				<meeting>32nd AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5634" to="5641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">ICON: Interactive conversational memory network for multimodal emotion detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Lang</title>
				<meeting>Conf. Empirical Methods Natural Lang</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2594" to="2604" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
