<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Artificial neural networks in hardware: A survey of two decades of progress</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2010-05-05">5 May 2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Janardan</forename><surname>Misra</surname></persName>
							<email>janardan.misra@honeywell.com</email>
							<affiliation key="aff0">
								<orgName type="institution">HTS Research</orgName>
								<address>
									<addrLine>151/1 Doraisanipalya, Bannerghatta Road</addrLine>
									<postCode>560076</postCode>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Indranil</forename><surname>Saha</surname></persName>
							<email>indranil@cs.ucla.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Artificial neural networks in hardware: A survey of two decades of progress</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2010-05-05">5 May 2010</date>
						</imprint>
					</monogr>
					<idno type="MD5">F638DA9549454CD24C01AB3EB4535F4D</idno>
					<idno type="DOI">10.1016/j.neucom.2010.03.021</idno>
					<note type="submission">Received 22 November 2009 Received in revised form 22 January 2010 Accepted 5 March 2010 Communicated by A. Prieto</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Hardware neural network Neurochip Parallel neural architecture Digital neural design Analog neural design Hybrid neural design Neuromorphic system FPGA based ANN implementation CNN implementation RAM based implementation Optical neural network</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article presents a comprehensive overview of the hardware realizations of artificial neural network (ANN) models, known as hardware neural networks (HNN), appearing in academic studies as prototypes as well as in commercial use. HNN research has witnessed a steady progress for more than last two decades, though commercial adoption of the technology has been relatively slower. We study the overall progress in the field across all major ANN models, hardware design approaches, and applications. We outline underlying design approaches for mapping an ANN model onto a compact, reliable, and energy efficient hardware entailing computation and communication and survey a wide range of illustrative examples. Chip design approaches (digital, analog, hybrid, and FPGA based) at neuronal level and as neurochips realizing complete ANN models are studied. We specifically discuss, in detail, neuromorphic designs including spiking neural network hardware, cellular neural network implementations, reconfigurable FPGA based implementations, in particular, for stochastic ANN models, and optical implementations. Parallel digital implementations employing bit-slice, systolic, and SIMD architectures, implementations for associative neural memories, and RAM based implementations are also outlined. We trace the recent trends and explore potential future research directions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Hardware devices designed to realize artificial neural network (ANN) architectures and associated learning algorithms especially taking advantage of the inherent parallelism in the neural processing are referred as hardware neural networks (HNN). Although most of the existing ANN applications in commercial use are often developed as software, there are specific applications such as streaming video compression, which demand high volume adaptive real-time processing and learning of large datasets in reasonable time and necessitate the use of energy-efficient ANN hardware with truly parallel processing capabilities. Specialized ANN hardware (which can either support or replace software) offers appreciable advantages in these situations as can be traced as follows <ref type="bibr" target="#b0">[1]</ref>: Speed: Specialized hardware can offer very high computational power at limited price and thus can achieve several orders of speed-up, especially in the neural domain where parallelism and distributed computing are inherently involved. For example, very large scale integration (VLSI) implementations for cellular neural networks (CNNs) can achieve speeds upto several teraflops <ref type="bibr" target="#b1">[2]</ref>, which otherwise is a very high speed for conventional DSPs, PCs, or even work stations.</p><p>Cost: A hardware implementation can provide margins for reducing system cost by lowering the total component count and decreasing power requirements. This can be important in certain high-volume applications, such as ubiquitous consumer-products for real-time image processing, that are very price-sensitive.</p><p>Graceful degradation: An intrinsic limitation of any sequential uni-processor based application is its vulnerability to stop functioning due to faults in the system (fail-stop operations). Primary reason is the lack of sufficient redundancy in the processor architecture. As some recent studies <ref type="bibr" target="#b2">[3]</ref> suggest, even with the advancement and introduction of the multi-core PC processors architectures, the need for having effective faulttolerant mechanisms is still present. In contrast to this parallel and distributed architectures allow applications to continue functioning though with slightly reduced performance (graceful degradation) even in the presence of faults in some components. For those ANN application which require complete availability or are safety critical, fault tolerance is of utmost importance and in this respect parallel hardware implementations offer considerable advantage. Mapping highly irregular and non-planar interconnection topology entailing complex computations and distributed communication on regular two dimensional surfaces poses significant challenge for the (VLSI) HNN designers. Also since hardware constraints (especially analog components) may introduce computational errors, degradation of learning and lack of accuracy in results become a major challenge while designing HNNs. These errors can divert the trajectory of the learning process, generally increasing the number of cycles required to achieve convergence. Non-linearity of activation functions poses yet another challenge while designing a compact hardware. To address these challenges wide spectrum of technologies and architectures have been explored in the past. These include digital <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>, analog <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, hybrid <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, FPGA based <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>, and (non-electronic) optical implementations <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>. At this point it is important to add that, for practical purposes, a HNN realizing an ANN model alone is not sufficient by itself and a fully operational system would demand many other components, e.g., for sensor acquisition, for pre and post processing of inputs and outputs etc.</p><p>Although not as widespread as ANNs in software, there do exist HNNs at work in real-world applications. Examples include optical character recognition, voice recognition (Sensory Inc. RSC Micro controllers and ASSP speech recognition specific chips), Traffic Monitoring (Nestor TrafficVision Systems), Experiments in High Energy Physics <ref type="bibr" target="#b16">[17]</ref> (Online data filter and Level II trigger in H1 electron-proton collision experiment using Adaptive Solutions CNAPS boards), adaptive control, and robotics. See Table <ref type="table" target="#tab_0">1</ref> for more examples.</p><p>With the advent of these technologies need of having timely surveys has also been felt. There are indeed several surveys which have appeared from time to time in the past. We will briefly discuss these surveys next.</p><p>Related surveys: Ref. <ref type="bibr" target="#b6">[7]</ref> by <ref type="bibr">Mead,</ref><ref type="bibr" target="#b3">[4,</ref><ref type="bibr"></ref> Part IV] by <ref type="bibr">Kung,</ref><ref type="bibr" target="#b49">and [49]</ref> by <ref type="bibr">Glesner and</ref> Poechmueller are some early references on the VLSI implementations of the ANN models. Lindsey and Lindbad <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b50">50]</ref> present one of earliest detailed overviews of the field covering most of electronic approaches as well as commercial hardware. Heemskerk <ref type="bibr" target="#b51">[51]</ref> presents an overview of Neurocomputers built from accelerator boards, general purpose processors, and neurochips coming out from both industries and academia upto mid 90s. Ienne et al. <ref type="bibr" target="#b52">[52]</ref> present a survey of digital implementations by considering two basic designs: Parallel systems with standard digital components and parallel systems with custom processors. They also discuss their experience with running a small ANN problem on two of the commercially available machines and conclude that most of the training times are actually slower or only moderately faster than on a serial workstation. Aybay et al. <ref type="bibr" target="#b53">[53]</ref> lay out a set of parameters, which can be used to classify and compare digital neurocomputers and neurochips. Moerland and Fiesler <ref type="bibr" target="#b54">[54]</ref> present an overview of some of the important issues encountered while mapping an ideal ANN model onto a compact and reliable hardware implementation, like quantization and associated weight discretizations, analog non-uniformities, and non-ideal responses etc. They also discuss hardware friendly learning algorithms. Sundararajan and Saratchandran <ref type="bibr" target="#b55">[55]</ref> discuss in detail various parallel implementation aspects of several ANN models (back propagation (BP) based NNs, ART NN, recurrent NN etc.) using various hardware architectures including scalable general purpose parallel computers and MIMD (multiple instruction multiple data) with MPI interface. Individual chapters discuss reviews, analysis, and experimental case studies, e.g., on implementations for BP based NNs and associated analysis of network and training set parallelisms. Burr <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b57">57]</ref> presents techniques for estimating chiparea, performance, and power consumption in the early stages of architectural exploration for HNN designs. These estimation techniques are further applied for predicting capacity and performance of some of the neuroarchitectures. Hammerstrom <ref type="bibr" target="#b58">[58]</ref> provides an overview of the research done in the digital implementations of ANNs till late 90s. Reyneri <ref type="bibr" target="#b59">[59]</ref> presents an annotated overview of the ANNs with ''Pulse Stream'' modulations including a comparative analysis of various existing modulations in terms of accuracy, response time, power, and energy requirements. Zhu and Sutton <ref type="bibr" target="#b60">[60]</ref> survey Field Programmable Gate Array (FPGA) based implementations of ANNs discussing different implementation techniques and design issues. Based upon the purpose of reconfiguration (prototyping and simulation, density enhancement, and topology adaptation) as well as data representation techniques (integer, floating point, and bit stream arithmetic) it provides taxonomy for classifying these implementations. Reyneri's survey on neurofuzzy hardware systems <ref type="bibr" target="#b61">[61]</ref> is an important paper discussing various technological aspects of hardware implementation technologies with a focus on hardware/ software co-design techniques. Diasa et al. <ref type="bibr" target="#b62">[62]</ref> is one of the latest surveys with specific focus to commercially available hardware. Schrauwen and D'Haene <ref type="bibr" target="#b10">[11]</ref> provide a brief overview of some of the recent FPGA based implementations of Spiking Neural Networks (SNN). Another more recent article by Maguire et al. <ref type="bibr" target="#b63">[63]</ref> also presents a detailed overview of FPGA based implementations of SNN models and brings out important challenges ahead. Bartolozzi and Indiveri in <ref type="bibr" target="#b64">[64]</ref> provide a comparative analysis of various hardware implementations for the spiking synaptic models. Smith <ref type="bibr" target="#b65">[65]</ref> surveys digital and analog VLSI implementation approaches for neuronal models with or without explicit time. Probably the most recent survey of the field with very interesting critical historical analysis of the major developments and limitations of digital, analog, and HNN approaches is presented by Hammerstrom and Waser in <ref type="bibr" target="#b66">[66]</ref>. Also Indiveri et al. <ref type="bibr" target="#b67">[67]</ref> present a survey of the recent progress in the field of neuromorphic designs and discusses challenges ahead for augmenting these systems with cognitive capabilities. Some of the HNN topics have found wider audience and there are specialized volumes on these topics. An edited volume by Austin <ref type="bibr" target="#b68">[68]</ref> provides a detailed glimpse on the RAM based HNN designs. Similarly another edited volume <ref type="bibr" target="#b69">[69]</ref> by Ormoindi and Rajapakse presents a recent update on FPGA based ANN implementations including foundational issues, various implementations, and lessons learned from a large scale project. An edited volume by Valle <ref type="bibr" target="#b70">[70]</ref> presents discussions on various approaches to build smart adaptive devices. Even though there exist several reviews and edited volumes on the subject, most of these either focus on specific aspects of HNN research or may not be so recent. This paper attempts to survey on all major HNN design approaches and models discussed in literature and in commercial use. Primary objective is to review the overall progress in the field of HNN over last two decades across all major ANN models, hardware design approaches, and applications. We cover these topics by including most of the important works which have appeared in the literature with an optimistic perspective. However, owing to space limitations, there are topics, which will not be covered in this survey including hardware friendly learning algorithms (e.g., perturbation learning <ref type="bibr" target="#b71">[71]</ref>, constructive learning <ref type="bibr" target="#b72">[72]</ref>, cascade error projection learning <ref type="bibr" target="#b73">[73,</ref><ref type="bibr" target="#b74">74]</ref>, and local learning <ref type="bibr" target="#b75">[75]</ref> with its special case of spike based Hebbian learning <ref type="bibr" target="#b76">[76]</ref>), HNN designs focused on specific ANN models (e.g., MLP with back propagation <ref type="bibr" target="#b77">[77,</ref><ref type="bibr" target="#b78">78]</ref>, radial basis function networks <ref type="bibr" target="#b79">[79,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b80">80]</ref>, and Neocognitron <ref type="bibr" target="#b81">[81]</ref>), and neurocomputers <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b82">82]</ref>.</p><p>Rest of the paper is organized as follows: Issues related to the parameters used for evaluating an HNN system are highlighted in Section 2. Section 2 also presents discussion on difficulties in HNN classification. Section 3 deals with different electronic approaches to implement a single neuron, whereas Section 4 provides a presentation on complete HNN models available as chips. CNN implementations are covered in Section 5. Neuromorphic systems including implementations for spiking NNs are covered in Section 6. A discussion on optical neurocomputers appears in Section 7. Finally Section 8 concludes the article by outlining some of the possible future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Evaluation parameters and classification</head><p>An ANN is generally specified in terms of the network topology, activation function (AF), learning algorithm, number and type of inputs/outputs, number of processing elements (neurons) and synaptic interconnections, number of layers etc. For a hardware implementation, in addition, specifications may include the technology used (analog, digital, hybrid, or FPGA), data representation (fixed/floating-point), weight storage, bits of precision, programmable or hardwired connections, on-chip learning or chip-in-the-loop training, on-chip or off-chip transfer function, e.g., look-up table, and degree of cascadability.</p><p>Based upon these parameters, various figures of merit are derived to indicate the resultant hardware performance. The most common performance ratings include Connections-per-second (CPS) for processing speed: Rate of multiplication/accumulate operations and transfer function computation during testing phase. This indicates how well the specific algorithm suits the architecture.</p><p>Connection-updates-per-second (CUPS) for learning speed: Rate of weight changes during learning, involving calculation and update of weights. This measures how fast a system is able to perform input-output mappings.</p><p>Synaptic energy: Average energy required to compute and update each synapse. Measured as WCPS (watt per connectionper-second) or J per connection <ref type="bibr" target="#b59">[59]</ref>. <ref type="bibr">Keulen et al. [83]</ref> propose an improved measure that also accounts for accuracy by defining bit connection primitives per second: CPPS¼b i Â b w Â CPS, with b i and b w denoting input and weight accuracy in bits, respectively. For RBF, instead of these, pattern presentation rate is actually used as a performance parameter.</p><p>Hardware constraints, such as weights/states precision, finite arithmetic/quantization effects caused by discrete values of the channel length, width of MOS transistor geometries, and AF realization play a major role in HNNs. Cornu and Ienne <ref type="bibr" target="#b84">[84]</ref> introduce the notion of algorithmic efficiency for performance measurement and evaluation of digital neurocomputers. Algorithmic efficiency is defined as a measure of the effect of the hardware constraints on the convergence properties of various ANN models to be simulated on a neurocomputer. They argue that comparing relative speeds in MCUPS is not sufficient and instead estimate global speedup of a neurocomputer as a product of its raw hardware speedup (corresponding to MCUPS) and the algorithmic efficiency (w.r.t. a specific ANN model).</p><p>The non-linearity associated with the AFs represents one of the major bottlenecks in digital VLSI implementation of ANNs, involving large overheads in time and silicon area. Possible solutions include use of look-up tables <ref type="bibr" target="#b85">[85,</ref><ref type="bibr" target="#b86">86]</ref> and piecewise linear approximating functions <ref type="bibr" target="#b87">[87]</ref>. In case of look-up table, table size again imposes an upper bound on the number of bits. A statistical study by Holt and Hwang <ref type="bibr" target="#b88">[88]</ref> on the precision requirements for a two layer MLP with BP learning showed that under certain assumptions (e.g., uniformly distributed input variables) a fixed point encoding of 16 bit is sufficient and at least 12 bits might be essential. Bieu <ref type="bibr" target="#b89">[89]</ref> presents several upper and lower bounds for the number-of-bits required for solving a classification problem using neural networks. These bounds are in turn used to devise ways for efficiently building the hardware implementations. Use of 1st and 2nd order Taylor interpolation also provides relatively high accuracy (up to 16-20 bits) even with very small look-up tables (256 words).</p><p>For large scale neural network, synaptic storage density is very important, and memory optimization plays an important role. However, there is a trade-off between the memory size and power consumption in the memory-one transistor DRAM has the highest density, but consumes more power than SRAM, as DRAM memory cells need to be refreshed due to leakage current, on the other hand six transistor SRAM consumes the least power, but achieves density which is factor 4 worse than one transistor DRAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Hardware neural network classification</head><p>Neural network hardware is becoming increasingly difficult to classify in a way that the classification yields useful comparative information for practical purposes. Primary source of difficulty arises from the multitude of characteristics associated with any such hardware implementation both arising from chosen hardware as well as underlying ANN model. As mentioned before, Aybay et al. <ref type="bibr" target="#b53">[53]</ref> list several classification attributes including transfer function characteristics: on-chip/off-chip, analog/digital, threshold/look-up table/computation; cascadibility, clock and data transfer rates. Based upon these attributes several HNN chips and designs were classified. Though such a classification covers wide range of attributes, extracting information for practical purposes using comparative analysis is relatively difficult. For these reasons, we do not attempt here to present another classification, though instead structure the discussion under several themes-starting with a discussion on basic neuronal level hardware designs, then progressing to the chip level approaches for various ANN models, followed by a discussion on several parallel implementation of specific ANN models including CNN, and finally focusing discussion on specific approaches including neuromorphic designs, and optical neurocomputers. In Table <ref type="table" target="#tab_1">2</ref>, we present an overview of the examples of various HNN implementations across wide range of ANN models. Forthcoming sections provide further details on these.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Hardware approaches to neuronal design</head><p>The transmission of signals in biological neurons through synapses is a complex chemical process in which specific neurotransmitter substances are released. Their effect is to change the electrical potential in the receiving cell by changing the Osmotic and ionic equilibrium across the cell membrane. If this potential reaches a threshold, the neuron fires. Artificial neuron models attempt to reproduce this phenomena at varying levels of abstractions <ref type="bibr" target="#b115">[115]</ref>.</p><p>In this section we describe the basic structure of digital and analog neurons used for HNN implementations and briefly discuss the implementations of spiking neurons and their synaptic dynamics. An analog implementation is usually efficient in terms of chip area and processing speed, but this comes at the price of a limited accuracy of the network components. In a digital implementation, on the other hand, accuracy is achieved at the cost of efficiency (e.g., relatively larger chip area, higher cost, and more power consumption). This amounts to a trade-off between the accuracy of the implementation and the efficiency of its performance.</p><p>It is also important to add at this point that the HW designs to be discussed throughout this paper involve significant manual adhoc steps, which is a time-consuming and expensive operation and a major factor in increasing ''time-to-market''. We will have bit more to say on this in the conclusion section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Digital neuron</head><p>In a digital neuron, synaptic weights are stored in shift registers, latches, or memories. Memory storage alternatives include one, two or three transistor dynamic RAM, or four or six transistor static RAM <ref type="bibr" target="#b49">[49]</ref>. Adders, subtracters, and multipliers are available as standard circuits, and non-linear AFs can be constructed using look-up tables or using adders, multipliers etc. A digital implementation entails advantages like simplicity, high signal-to-noise ratio, easily achievable cascadability and flexibility, and cheap fabrication, along with some demerits like slower operations (especially in the weight Â input multiplication). Also conversion of the digital representations to and from an analog form may be required since usually input patterns are available in analog form and control outputs also often required to be in analog form.</p><p>In a recent work Muthuramalingam et al. <ref type="bibr" target="#b116">[116]</ref> discuss in detail issues involved with the implementation of a single neuron in FPGA including serial versus parallel implementation of computational blocks, bit precision and use of look-up tables. Hikawa <ref type="bibr" target="#b102">[102]</ref> describes digital pulse-mode neuron which employs piecewise-linear function as its AF. The neuron is implemented on a FPGA rendering the piecewise-linear function programmable and robust against the changes in the number of inputs. In <ref type="bibr" target="#b117">[117,</ref><ref type="bibr" target="#b118">118]</ref>, Daalen et al. demonstrate through experiments how linear and sigmoid AFs can be generated in a digital stochastic bit stream neuron. The AF of the neuron is not built in the hardware explicitly, rather it is generated by the interaction of two probability distributions. Different AFs can be generated by controlling the distribution of the threshold values provided to each neuron.</p><p>Skrbek <ref type="bibr" target="#b119">[119]</ref> presents an architecture and overview of shift-add neural arithmetic, for an optimized implementation of multiplication, square root, logarithm, exponent and non-linear AFs at neuronal level for fast perceptron and RBF models. Functions are linearly approximated, for example, 2 x is be approximated as 2 int(x) (1+frac(x)) where int(x) calculates the integral part of x and frac(x) is its fractional part. Shift operation calculates 2 int(x) , whereas linear approximation (1+frac(x)) approximates remaining 2 frac(x) . Further an FPGA based implementation for the shift-add arithmetic is discussed involving only adders and barrel shifters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Analog neuron</head><p>In an analog neuron weights are usually stored using one of the following: resistors <ref type="bibr" target="#b120">[120]</ref>, charge-coupled devices <ref type="bibr" target="#b121">[121]</ref>, capacitors <ref type="bibr" target="#b122">[122]</ref>, and floating gate EEPROMS <ref type="bibr" target="#b123">[123]</ref>. In VLSI, a variable resistor as a weight can be implemented as a circuit involving two MOSFETs <ref type="bibr" target="#b124">[124]</ref>. However, discrete values of channel length and width of the MOS transistors may cause quantization effect in the values of the weight. The scalar product and subsequent nonlinear mapping is performed by a summing amplifier with saturation <ref type="bibr" target="#b125">[125]</ref>.</p><p>In the analog domain the characteristic non-linear functionality of neuronal AF can sometimes be captured directly (e.g., above saturation level current and voltage characteristics of transistors), yet a coherent set of all the basic elements is difficult to achieve. As the AFs used in software ANN implementations cannot be easily implemented in VLSI, some approximation functions are instead considered to act as AFs <ref type="bibr" target="#b124">[124]</ref>. Also analog neuron implementations benefit by exploiting simple physical effects to carry out some of the network functions <ref type="bibr" target="#b6">[7]</ref>. For example, the accumulator can be a common output line to sum currents. Analog elements are generally smaller and simpler than their digital counterparts. On the other hand, obtaining consistently precise analog circuits, especially to compensate for variations in temperature and control voltages, requires sophisticated design and fabrication.</p><p>In analog modeling, signals are typically represented by currents <ref type="bibr" target="#b79">[79]</ref> and/or voltages <ref type="bibr" target="#b123">[123]</ref> which work with real numbers. Current flow is preserved at each junction point by Kirchhoff's Current Law, and during multiplication various resistance values can be used for the weighting operation of the signal. Thus a network of resistors can simulate the necessary network connections and their resistances are the adaptive weights needed for learning. Besides, the non-linear voltagecurrent response curve of field effect transistors (FETs) makes them especially suitable for simulating neuronal AFs. However, the encoding of signals as voltages makes certain operations like addition rather difficult to implement as compared to multiplication and threshold activation. Also a major problem with this representation scheme is that before performing any operation a signal needs to be held constant for some time. The current which flows between the source and sink depends linearly on the potential difference between them, and the proportionality constant is determined by the stored charge. Learning involves weight updates corresponding to changes in the amount of charge stored. Even if the power source is disconnected, the magnitude of the weights remains unchanged. A different approach <ref type="bibr" target="#b25">[26]</ref>, with charged coupled devices (CCDs), is used to store the charge dynamically.</p><p>The main challenges for analog designs are the synapse multiplier over a useful range and the storage of the synapse weights. Moreover, there are some characteristics inherent to analog computation like the spatial non-uniformity of components (which are particularly troublesome when the training of the network is done off-chip, without taking these component variations into account) and non-ideal responses (that particularly affect the implementation of a linear multiplication and non-linear AF, like the standard sigmoid).</p><p>There are also attempts for designing digitally programmable analog building blocks for ANN implementations. Almeida and Franca <ref type="bibr" target="#b126">[126]</ref> propose a synapse architecture combining a quasipassive algorithmic digital to analog converter providing a 7-bit bipolar weight range and on-chip refreshing of the analog weight followed by a four quadrant analog-digital multiplier with extended linear range. Hamid et al. <ref type="bibr" target="#b127">[127]</ref> discuss an approach of including the effect of Deep Sub-Micrometer (DSM) noise in MOSFETs for circuit-level and architecture-level simulations. They show that that DSM noise has the potential to be exploited for probabilistic neural computation architecture hardware implementation. For example, they tested the effect of a noisy multiplier on the performance of Continuous Restricted Boltzmann Machine (CRBM) <ref type="bibr" target="#b128">[128,</ref><ref type="bibr" target="#b129">129]</ref> and result demonstrate that stochastic neuron implemented using noisy MOSFET can produce performance comparable with that of a ''perfect'' CRBM with explicit noise injected into it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Silicon implementation of spiking neuron and its synaptic dynamics</head><p>Actual communication between biological neurons happens by short electrical pulses, which are known as action potentials or spikes. Integrate and fire (I&amp;F) neuron model is among the simplest models with spiking dynamics. An I&amp;F neuron model can handle continuously time varying signals, support synchronization, and is computationally powerful as compared to non-spiking neuron models <ref type="bibr" target="#b130">[130]</ref>. Leaky I&amp;F model and its generalization as spike response model, non-linear I&amp;F model, Hodgkin-Huxley model, Mihalas-Niebur model, and Morris-Lecar model are among the better known extensions of the basic I&amp;F model. Networks of I&amp;F neurons exhibit a wide range of capabilities including feature binding, segmentation, pattern recognition, onset detection, and input prediction <ref type="bibr" target="#b131">[131]</ref>. We will next briefly discuss some of the representative hardware implementations (generally using mixed-mode circuits) for I&amp;F model and some of its extensions since they are often used while designing neuromorphic systems as discussed later in the Section 6.</p><p>For adequately realizing an I&amp;F model in hardware, it is necessary that the realized hardware can set at least an explicit threshold to define occurrence of a spike and implements spikefrequency adaptation. One of the early designs meeting these requirements was proposed by Schultz and Jabri in <ref type="bibr" target="#b132">[132]</ref> which can set an explicit threshold voltage and can realize spike-frequency adaptation. Because these spiking neuron models are capable of generating potentially varied functionalities, their detailed hardware realizations naturally tend to consume relatively larger silicon area and power. For example, Rasche and Douglas <ref type="bibr" target="#b133">[133]</ref> describe an analog implementation of Hodgkin-Huxley model with 30 adjustable parameters, which required 4 mm 2 area for a single neuron. Therefore, in order to be able to build larger neuromorphic systems using these models, it is necessary that these designs are optimized for area and power requirements. Schaik <ref type="bibr" target="#b98">[98]</ref> presents a circuit design for generating spiking activity. The circuit integrates charge on a capacitor such that when the voltage on the capacitor reaches a certain threshold, two consecutive feedback cycles generate a voltage spike and then bring the capacitor back to its resting voltage. The size of the presented circuit is small enough that it can be used in designing the larger systems on a single chip. Later, Indiveri and Fusi <ref type="bibr" target="#b76">[76]</ref> present a design employing 20 transistors and 3 capacitors for the leaky I&amp;F model with average power consumption in the range of ½0:321:5 mW.</p><p>Models by Izhikevich <ref type="bibr" target="#b134">[134]</ref> and Mihalas and Niebur <ref type="bibr" target="#b135">[135]</ref> are one of the recent attempts to define computationally simpler models of a spiking neuron having biological accuracy for spiking and bursting activity. The silicon realization of Izhikevich's model has been presented in a recent work by Wijekoon and Dudek <ref type="bibr" target="#b136">[136]</ref>. However, since Izhikevich's model does not land itself directly to parametric biological interpretation, it is bit difficult to integrate in larger neuromorphic designs. Folowosele et al. <ref type="bibr" target="#b137">[137]</ref> on the other hand present the hardware realization of a simplified Mihalas and Niebur's model in terms of switched capacitor circuits fabricated using 0.15 um CMOS technology, which could be used in larger neuromorphic systems.</p><p>There have also been concentrated efforts in modeling and implementing temporal dynamics of synaptic (ionic) current in a biological neuron enabling learning of neural codes and encoding of spatiotemporal spike patterns. Synaptic circuits implementing synaptic dynamics operate by translating presynaptic voltage pulses into postsynaptic currents injected in the membrane of the target neuron, with a gain corresponding to the synaptic weight. Briefly the implementations for the synaptic models can be classified as presented by Bartolozzi and Indiveri in <ref type="bibr" target="#b64">[64]</ref>:</p><p>Multiplier synapse: For models representing synaptic information in terms of mean firing rates, synapse is usually modeled as a multiplier circuit.</p><p>Pulsed current-source synapse: Synapse is implemented in analog form using transistors operating in subthreshold region such that an output pulsed current from the synapse circuit is generated for the duration of the input voltage spike given to it digitally. The underlying model represents synaptic information in terms of mean firing rates (see <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b138">138]</ref>).</p><p>Reset-and-discharge synapse: Using 3 p-EFT transistors and a capacitor such implementation can give rise to a postsynaptic excitatory current (EPSC), which can last longer then the input voltage spike and decays exponentially with time (see <ref type="bibr" target="#b139">[139]</ref>).</p><p>Linear charge-and-discharge synapse: It is a variant of resetand-discharge synapse, where first the input voltage spike decreases linearly as the postsynaptic excitatory current increases exponentially. After this, input voltage pulse increases to a reference power supply voltage and at the same time postsynaptic current decreases (see <ref type="bibr" target="#b140">[140]</ref>).</p><p>Current-mirror-integrator synapse: It is a variant of linear charge-and-discharge synapse where two transistors and a capacitor form a current mirror integrator circuit. In contrast to linear charge-and-discharge synapse, postsynaptic excitatory current increases in a sigmoidal fashion and later decreases in a hyperbolic fashion with respect to time (see <ref type="bibr" target="#b141">[141]</ref>).</p><p>Log-domain integrator (LDI) synapse: It is another variant of linear charge-and-discharge synapse which utilizes the logarithmic relationship between subthreshold MOSFET gate-tosource voltage and the channel current. The resultant synaptic circuit works like a linear low-pass filter. However, the circuit area is relatively larger as compared to other models (see <ref type="bibr" target="#b142">[142]</ref>).</p><p>Diff-pair integrator (DPI) synapse: Destexhe et al. <ref type="bibr" target="#b143">[143]</ref> proposed a macroscopic model for synaptic transmission and the linear summation property of postsynaptic currents, for which Bartolozzi and Indiveri <ref type="bibr" target="#b64">[64]</ref> propose a VLSI synaptic circuit-the diff-pair integrator-that implements this model as a log-domain linear temporal filter and supports synaptic properties including short-term depression to conductance based EPSC generation. The synaptic circuit uses six transistors and a capacitor and effectively works same as low-pass linear filter. However, unlike LDI synapse, DPI synapse can give rise to exponential dynamics for both excitatory as well as inhibitory postsynaptic currents.</p><p>For further details, reader is suggested to refer to <ref type="bibr" target="#b64">[64]</ref>, where authors present an overview and comparative analysis of existing synaptic circuits proposed in the literature, e.g., <ref type="bibr" target="#b144">[144,</ref><ref type="bibr" target="#b140">140,</ref><ref type="bibr" target="#b145">145]</ref>, including their own DPI circuit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Hardware neural network chips</head><p>This section provides an overview of HNNs implemented as chips, also known as neurochips, realizing complete ANN models. These include digital neurochips, analog neurochips, hybrids, neuromorphic implementations, FPGA-based neurochips, RAM based neurochips, and neurochips for neural associative memories. A general-purpose neurochip is capable of implementing more than one neural algorithm for a particular application, while a special-purpose neurochip models a particular neural algorithm for many applications.</p><p>Typically an activation block, performing the weight Â input multiplication and their summation, is always on the neurochip, whereas other blocks, involving neuron state, weights, and activation function, may be on/off the chip and some of these functions may even be performed by a host computer. Neuron states and weights can be stored in digital/analog form, and the weights can be loaded statically or updated dynamically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Digital neurochips</head><p>The majority of the available digital chips use CMOS technology. There are several categories of digital chips, like bit-slice, single instruction multiple data (SIMD), and systolic arrays. The advantages of digital technology include the use of well-understood fabrication techniques, RAM weight storage, and flexible design. The biggest challenge for designers is the synapse multiplier, which normally is the slowest element in the network processing.</p><p>In case of conventional bit-slice architectures, a processor is constructed from modules, each of which processes one bit-field or ''slice'' of an operand. They provide simple and cheap building blocks (typically single neurons) to construct networks of larger size and precision. An example is Micro Devices' MD1220 Neural Bit Slice <ref type="bibr" target="#b146">[146]</ref>, one of the first commercial HNN chips. It has eight neurons with hard-limiting thresholds and eight 16-bit synapses with 1-bit inputs. With bit-serial multipliers in the synapse, the chip provides a performance of about 9 MCPS. Other examples of slice architectures are the Philips' Lneuro chip <ref type="bibr" target="#b147">[147]</ref> and the Neuralogix' NLX-420 Neural Processor <ref type="bibr" target="#b148">[148]</ref>. Slice architectures generally include off-chip learning.</p><p>In case of SIMD, each of the multiple PEs run the same instruction simultaneously, but on different data sets <ref type="bibr" target="#b149">[149]</ref>. For a better match with ANN requirements one has to turn to programmable systems, and most such designs are SIMD with minor variations. Instructions are often horizontally encoded, that is, each field of the instruction word directly configures a part of the PE. 1 The two features, viz., no address/issue logic and reduced instruction decoding, render the implementation suitable for the resources required by general ANNs. In Adaptive Solutions' N64000 <ref type="bibr" target="#b85">[85]</ref> with 64 PEs, each PE holds a 9 Â 16-bit integer multiplier, a 32-bit accumulator, and 4 KB of on-chip memory for weight storage. Kim et al. <ref type="bibr" target="#b42">[42]</ref> propose a high performance neural network processor based on the SIMD architecture that is optimized for image processing. The proposed processor supports 24 instructions, and consists of 16 Processing Units (PUs) per chip. Each PU includes 24-bit 2K-word Local Memory and one PE.</p><p>In case of systolic array based designs, each PE does one step of a calculation synchronously with other PEs and then passes its result to the next processor in the pipeline, thus making the architecture very suitable for implementing efficient synapse multiplier. For example, in Siemens' MA-16 <ref type="bibr" target="#b151">[151]</ref>, fast matrixmatrix operations (multiplication, subtraction, or addition) are implemented with 16-bit elements for 4 Â 4 matrices. The multiplier outputs and accumulators have 48-bit precision. Weight storage is off-chip RAM and neuron transfer functions are off-chip via look-up tables. Generally systolic arrays are application specific processing arrays for problems displaying a large amount of fine-grained parallelism, and thus they are well matched to ANNs having low data bandwidth and potentially high utilization ratio of the processing units. Their disadvantage lies in the high complexity of the system controlling and interfacing the array with a host system. Some further examples of systolic architectures for HNNs include vector processor arrays <ref type="bibr" target="#b152">[152]</ref>, common bus architecture <ref type="bibr" target="#b49">[49]</ref>, ring architecture <ref type="bibr" target="#b4">[5]</ref>, and TORAN (two-in-one ring array network) architecture <ref type="bibr" target="#b153">[153]</ref>. Eppler et al. <ref type="bibr" target="#b92">[92]</ref> presented a cascadable, systolic processor array called simple applicable neural device (SAND), designed for fast processing of neural networks. The neurochip may be mapped on feed-forward networks, RBF, and Kohonen feature maps. The chip is optimized for an input data rate of 50 MHz, 16 bit data and could be considered having low cost at the time of its design. The performance of a single SAND chip that uses four parallel 16 bit multipliers and 40 bit adders in one clock cycle is 200 MCPS. In early nineties, Wang proposed an analog recurrent neural 1 In a horizontally encoded instruction set, each field in an instruction word controls some functional unit or gate directly, as opposed to vertical encoding where instruction fields are decoded (by hard-wired logic or microcode) to produce the control signals. A horizontally encoded instruction allows operation level parallelism by specifying more than one independent operations and thus in a single cycle multiple operations can be performed simultaneously. Because an architecture using horizontal encoding typically requires more instruction word bits it is sometimes known as a very long instruction word (VLIW) architecture <ref type="bibr" target="#b150">[150]</ref>. These architectures are especially suitable for HNN implementations.</p><p>network <ref type="bibr" target="#b47">[47]</ref> based on the deterministic annealing network for solving the assignment problem. However, that analog implementation required mapping the massive number of interconnections and programming the parameters. Later Hung and Wang <ref type="bibr" target="#b46">[46]</ref> presented the digital realization of the same by mapping it to a one dimensional systolic array with ring interconnection topology. A scaled down version was realigned using FPGA based devices. Interestingly, they demonstrate that regularities in the data for the assignment problem could be used to eliminate the need of multiplication and division operations.</p><p>Apart from the above, other digital HNN designs also exist. Bagging <ref type="bibr" target="#b154">[154]</ref> is a technique for improving classification performance by creating ensembles. Bagging uses random sampling with replacement from the original data set in order to obtain different training sets. It is observed that bagging significantly improves classifiers that are unstable in the sense that small perturbations in the training data may result in large changes in the generated classifier. Bermak and Martinez <ref type="bibr" target="#b5">[6]</ref> present a 3D circuit implementation of bagging ensembles for efficient pattern recognition tasks. Individual classifiers within the ensemble are decision trees specified as threshold networks having a layer of threshold logic units (TLUs) followed by combinatorial logic elements. The proposed architecture supports a variable precision computation (4/8/16-bit) and configurable network structure w.r.t. number of networks per ensemble or the number of TLUs and inputs per network.</p><p>In self-organizing feature map (SOFM) the capability of calculating the exact equation of the learning rule and the distance required by a PE has a direct bearing on the chip area. In particular, it becomes too large when large number of PEs are to be considered. Rueping et al. <ref type="bibr" target="#b93">[93]</ref> present a digital architecture based on the idea that restriction on the learning algorithm may simplify the implementation. In this architecture the Manhattan Distance and a special treatment of the adaptation factor are used to decrease the necessary chip area so that a high number of PEs can be accommodated on a single chip. The hardware is extendable and advantageous to realize map sizes of 10 Â 10 in one chip with only 28 pins. With binary data, even higher performance ( 425 GCPS for a 50 Â 50 map) can be achieved.</p><p>Recently, Dibazar et al. <ref type="bibr" target="#b43">[43]</ref> discuss Texas instrument's TMS320C6713 DSP Starter Kit (a floating point DSP processor) based implementation of a Dynamic Synapse Neural Network model for acoustic sound recognition in noisy environments. The developed hardware achieves an accuracy of 90% for classification and localization task for gunshot recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Analog neurochips</head><p>Some of early fully developed analog chips include Intel's ETANN and Synaptic's Silicon Retina. Intel's Electrically Trainable Analog Neural Network (ETANN) 80170NX <ref type="bibr" target="#b123">[123]</ref> is an elaborate analog chip with 64 fully connected neurons. It is a generalpurpose neurochip where analog non-volatile weights are stored on-chip as electrical charge on floating gates, and Gilbert-multiplier synapses provide four-quadrant multiplication. ETANN does not support on-chip learning and only a chip-in-the-loop mode using a host computer is used so that at the end of the learning phase weights could be downloaded on the chip. The chip is reported to achieve a calculation rate of 2 GCPS, accuracy of 4-bits with a 64-bit bus, and 10,240 programmable synapses. ETANN chips can be cascaded to form a network of upto 1024 neurons with upto 81,920 weights, by direct-pin/bus interconnection. The Mod2 Neurocomputer <ref type="bibr" target="#b155">[155]</ref> is an early design employing 12 ETANN chips for real-time image processing. Later many other systems utilized these ETANN chips including MBOX II <ref type="bibr" target="#b45">[45]</ref>, an analog audio synthesizer with 8 ETANN chips.</p><p>Competition based ANNs such as Kohonen SOFM often need calculating distances between input vectors and the weights. An analog implementation for an SOFM generally results into a compact circuit block that accurately computes the distances. In early 90s, Churcher et al. <ref type="bibr" target="#b156">[156]</ref> presented circuits for calculating Euclidean distance measure. Later, Gopalan and Titus <ref type="bibr" target="#b94">[94]</ref> provide an analog VLSI implementation of a wide range of Euclidean distance computation circuit which can be used as part of a highdensity hardware implementation of a SOFM.</p><p>Liu et al. <ref type="bibr" target="#b34">[35]</ref> present a mixed signal CMOS feed-forward chip with on-chip error-reduction hardware for real-time adaptation.</p><p>The chip was fabricated through MOSIS in Orbit 2 mm n-well process and weights were stored in capacitors targeting oscillating working conditions. The implemented learning algorithm is a genetic random search algorithm, known as Random Weight Change (RWC) algorithm, which does not require a known desired neural-network output for error calculation and is thus suitable for direct feedback control. In experiments, the RWC chip, as a direct feedback controller, could successfully suppress unstable oscillations modeling combustion engine instability in real time. Nonetheless, volatile weight storage remains an issue limiting the possible applications.</p><p>Ortiz and Ocasio <ref type="bibr" target="#b157">[157]</ref>, on the other hand, present a discrete analog hardware model for the morphological neural network, which replaces the classical operations of multiplication and addition by addition and maximum or minimum operations.</p><p>Milev and Hristov <ref type="bibr" target="#b33">[34]</ref> present an analog-signal synapse model using MOSFETs in a standard 0:35-mm CMOS fabrication process to analyze the effect of the synapse's inherent quadratic nonlinearity on learning convergence and on the optimization of vector direction. The synapse design is then used in a VLSI architecture consisting of 2176 synapses for a finger-print feature extraction application.</p><p>Brown et al. <ref type="bibr" target="#b7">[8]</ref> describe the implementation of a signal processing circuit for a Continuous-Time Recurrent Neural Network using subthreshold analog VLSI in mixed-mode (current and voltage) approach, where state variables are represented by voltages while neural signals are conveyed as currents. The use of current allows for the accuracy of the neural signals to be maintained over long distances, making this architecture relatively robust and scalable.</p><p>Bayraktaroglu et al. <ref type="bibr" target="#b158">[158]</ref> discuss-ANNSyS-a system for synthesizing analog ANN chips by approximating on-chip training to provide the starting point for ''chip-in-the-loop training''. The synthesis system is based on SPICE circuit simulator and a silicon assembler and designed for analog neural networks to be implemented in MOS technology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Hybrid neurochips</head><p>Hybrid Chips combine digital and analog technologies in an attempt to get the best of both. For example, one can use analog internal processing for speed with weights being set digitally. As an example, consider the hybrid Neuro-Classifier from the Mesa Research Institute at University of Twente <ref type="bibr" target="#b159">[159]</ref>, which uses 70 analog inputs, six hidden nodes, and one analog output with 5-bit digital weights achieving the feed-forward processing rate of 20 GCPS. The final output has no transfer function, so that multiple chips can be added to increase the number of hidden units. Similarly <ref type="bibr" target="#b9">[10]</ref> presents a hardware efficient matrix-vector multiplier architecture for ANNs with digitally stored synapse strengths.</p><p>Cortical neurons <ref type="bibr" target="#b160">[160,</ref><ref type="bibr" target="#b161">161]</ref> whose major mode of operation is analog can compute reliably even with the precision limitation of analog operations owing to their organization into populations in which a signal at each neuron is restored to an appropriate analog value according to some collective strategy. Douglas et al. <ref type="bibr" target="#b162">[162]</ref> describe a hybrid analog-digital CMOS architecture for constructing networks of cortical amplifiers using linear threshold transfer function.</p><p>A hybrid architecture with on-chip learning has been presented in <ref type="bibr" target="#b8">[9]</ref>. The overall circuit architecture is divided into two main parts with regard to their operating modes, viz., analog and digital. The analog ANN unit executes the neural function processing using a charge based circuit structure. It is composed of a 20 neuron layer, each with 10 bit vector inputs. The winnertakes-all unit is devoted to the task of selecting one neuron as the winner on the criterion of the best degree of match between the stored pixel pattern and the current input vector. On the other hand, the units for error correction, circuit control and clock generation are kept purely digital.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">FPGA based implementations</head><p>Reconfigurable FPGAs provide an effective programmable resource for implementing HNNs allowing different design choices to be evaluated in a very short time. They are low cost, readily available, and reconfigurable offering software like flexibility. Partial and online reconfiguration capabilities in the latest generation of FPGAs offer additional advantages. However, the circuit density using FPGAs is still comparably lower and is limiting factors in the implementation of large models with thousands of neurons.</p><p>Krips et al. <ref type="bibr" target="#b29">[30]</ref> present an FPGA implementation of a neural network meant for designing a real time hand detection and tracking system applied to video images. Yang and Paindavoine <ref type="bibr" target="#b30">[31]</ref> present an FPGA based hardware implemented on an embedded system with 92% success rates of face tracking and identity verification in video sequences.</p><p>Maeda and Tada <ref type="bibr" target="#b100">[100]</ref> describe an FPGA realization of a pulse density NN using the simultaneous perturbation method <ref type="bibr" target="#b163">[163,</ref><ref type="bibr" target="#b164">164]</ref> as the learning scheme. The simultaneous perturbation method is more amenable to a hardware realization than a gradient type learning rule, since the learning rule requires only forward operations of the network to modify weights unlike the BP present in the gradient type rule. Pulse density NN systems are also robust against noisy conditions.</p><p>In contrast to Custom VLSI, the FPGAs are readily available at a reasonable cost and have a reduced hardware development cycle. Moreover, FPGA-based systems can be tailored to specific ANN configurations. For example, Gadea et al. <ref type="bibr" target="#b91">[91]</ref> present the implementation of a systolic array for a multi-layer perceptron on a Xilinx Virtex XCV400 FPGA of a pipelined on-line BP learning algorithm. Huitzil and Girau <ref type="bibr" target="#b23">[24]</ref> map the integrate-and-fire LEGION (Local Excitatory Global Inhibitory Oscillator Network) spiking neural model for image segmentation <ref type="bibr" target="#b165">[165,</ref><ref type="bibr" target="#b21">22]</ref> onto Xilinx Virtex XC2V1500FF896-4 device. However, multiplication is bit costly using FPGAs since each synaptic connection in an ANN requires a single multiplier, and this number typically grows as the square of the number of neurons. Mordern FPGAs, e.g., Xilinx' Virtex II Pro <ref type="bibr" target="#b166">[166]</ref> with embedded IBM PowerPC cores and Altera's Stratix III <ref type="bibr" target="#b167">[167]</ref>, though can have 100s of dedicated multipliers.</p><p>In a relatively recent work Himavathi et al. <ref type="bibr" target="#b168">[168]</ref> have used layer multiplexing technique to implement multi-layer feedforward networks into Xilinx FPGA XCV400hq240. The suggested layer multiplexing involves implementing only the layer having the largest number of neurons. A separate control block is designed, which appropriately selects the neurons from this layer to emulate the behavior of any other layer and assigns the appropriate inputs, weights, biases, and excitation function for every neuron of the layer that is currently being emulated in parallel. Each single neuron is implemented as a look-up table. To assess the effectiveness of the design a flux estimator for sensorless drives <ref type="bibr" target="#b169">[169]</ref> was used for testing with reported 50% decrease in the number of neurons though adding an speed overhead of 17.7% because of the control block.</p><p>Another recent study Rice et al. <ref type="bibr" target="#b170">[170]</ref> reports that a FPGA based implementation of a neocortex inspired cognitive model can provide an average throughput gain of 75 times over software implementation on full Cray XD1 supercomputer. They use the hierarchical Bayesian network model based on the neocortex developed by George and Hawkins <ref type="bibr" target="#b171">[171]</ref>. Their hardwareaccelerated implementation on the Cray XD1 uses Xilinx Virtex II Pro FPGAs with off-chip SRAM memory and software implementation uses 5 dual core 2.0 GHz Opteron processors.</p><p>An important problem faced by designers of FPGA based HNNs is to select the appropriate ANN model for a specific problem to be implemented using optimal hardware resources. Simon Jothson and others provide interesting insights in <ref type="bibr" target="#b172">[172]</ref> for this purpose. They carried out a comparative analysis of hardware requirements for implementing four ANN models onto FPGA. The selected models include MLP with BP and RBF network as classical models, and two SNN models-leaky integrate and fire (LIF) and spike response model. These models were then analyzed on a benchmark classification problem for FPGA hardware resources. The results of the study suggest that LIF SNN model could be the most appropriate choice for implementation for non-linear classification tasks.</p><p>FPGA implementations of stochastic ANN models: Practical hardware implementations of large ANNs critically demand that the circuitry devoted to multiplication is significantly reduced. One way to reduce it is to use bit-serial stochastic computing <ref type="bibr" target="#b173">[173]</ref>. This uses relatively long, probabilistic bit-streams, where the numeric value is proportional to the density of ''1''s in it. For example, a real number r A ½À1,1 is represented as a binary sequence such that probability of a bit getting set to 1 is (r + 1)/2. The multiplication of two probabilistic bit-streams can be accomplished by a single two-input logic gate. This makes it feasible to implement large, dense, fully parallel networks with fault tolerance. Even though stochastic computation is simple, it may not always be efficient (see <ref type="bibr" target="#b59">[59]</ref> for comparison.)</p><p>Most of the stochastic ANN models have been implemented in hardware using FPGAs <ref type="bibr" target="#b113">[113,</ref><ref type="bibr" target="#b114">114,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b11">12]</ref>. Daalen et al. <ref type="bibr" target="#b113">[113]</ref> describe an FPGA based expandable digital architecture with bit serial stochastic computing to carry out the parallel synaptic calculations. Authors discuss that fully connected multi-layered networks can be implemented with time multiplexing using this architecture. FPGAs have also been used to implement stochastic computation with look-up table based architecture for computing AF <ref type="bibr" target="#b114">[114]</ref>. Li et al. <ref type="bibr" target="#b41">[41]</ref> discuss FPGA implementation of a feedforward network employing stochastic techniques for computing the non-linear sigmoid AFs. Further it is used to design a neuralnetwork based sensorless control of a small wind turbine system. Nedjah and Mourelle <ref type="bibr" target="#b11">[12]</ref> describe and compare the characteristics of two Xilinx VIRTEX-E family based FPGA prototype architectures implementing feed-forward fully connected ANNs with upto 256 neurons. The first prototype used traditional adders and multipliers of binary inputs while the second instead has stochastic representation of the inputs with corresponding stochastic computations. They compare both prototypes in terms of space requirements, network delays, and finally the time Â area factor. As expected, stochastic representation reduces space requirements to a good extent though resulting networks are slightly slower compared to binary models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Other implementations</head><p>Szabo et al. <ref type="bibr" target="#b174">[174]</ref> suggest a bit-serial/parallel neural network implementation method for pre-trained networks using bit-serial distributed arithmetic for implementing digital filters. Their implementation of a matrix-vector multiplier is based on an optimization algorithm, which utilizes CSD (Canonic Signed Digit) encoding and bit-level pattern coincidences. The resulting architecture can be realized using FPGA or ASIC and can be integrated into automatic neural network design environments. The suggested matrix multiplier structure is useful for both in MLP designs as well as cellular neural networks (CNNs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1.">Associative neural memories</head><p>Basic operation of an Associative Neural Memory (ANM) is to map between two (finite) pattern sets using threshold operation. Palm et al. <ref type="bibr" target="#b175">[175]</ref> studied a very simple model of a neural network performing this task efficiently, where the input, output, and connection weights are binary. Ruckert et al. <ref type="bibr" target="#b108">[108,</ref><ref type="bibr" target="#b109">109]</ref> thereafter designed VLSI architectures for this model using analog, digital, and mixed signal circuit techniques. Digital architecture is based on a 16-Kbit on-chip static RAM, a neural processing unit, a coding block including input/output logic, and an on-chip controller providing 12 instructions for synchronizing, controlling, and testing the modules. The learning rate estimated to be 0.48 GCUPS. The test chip contains a 16 neuron Â 16 synapse matrix using 1:2-m CMOS technology. The designed chips can be scaled up, for example, upto 4000 neurons, each having 16,000 inputs. Cascading such chips would further enlarge the design. Willshaw et al. <ref type="bibr" target="#b176">[176]</ref> define a type of ANM model called correlation matrix memory (CMM), where output pattern is a label associated with the most similar stored pattern to the input. Justin et al. <ref type="bibr" target="#b17">[18]</ref> discuss an FPGA based implementation of the pipelined binary-CMM with on-board training and testing for high-performance pattern recognition tasks. For an accessible reference on various ANM models the reader is referred to the edited volume by Hassoun <ref type="bibr" target="#b177">[177]</ref>-part IV discusses implementations of several ANM models including an optical implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2.">RAM based implementations</head><p>First introduced by Bledsoe and Browning <ref type="bibr" target="#b178">[178]</ref>, RAM based NN (RNN) (also known as weightless NN) <ref type="bibr" target="#b68">[68,</ref><ref type="bibr" target="#b179">179]</ref> consists of PEs (neurons), which have only binary inputs and outputs and no weight between nodes. Neuronal functions are stored into lookup tables, which can be implemented using commercially available RAMs. Unlike other neural network models, they can be trained very rapidly and can be implemented using simple hardware. Instead of adjusting weights in the conventional sense the RNNs are trained by changing the contents of the look-up tables. RNNs have found applications including as a class of methods for building pattern recognition systems. Refs. <ref type="bibr" target="#b68">[68,</ref><ref type="bibr" target="#b179">179]</ref> provide detailed overview on RNNs.</p><p>Aleksander et al. <ref type="bibr" target="#b18">[19]</ref> provide the first hardware realization of a general purpose image recognition system-WISARD, based on RAM circuits. In <ref type="bibr" target="#b180">[180]</ref>, hardware implementation of the probabilistic RAM networks is presented, as well as the learning algorithm. Kennedy and Austin <ref type="bibr" target="#b24">[25]</ref> describe a SAT (Sum And Threshold) processor; a dedicated hardware implementation of a binary neural image processor. The SAT processor is specifically aimed at supporting the Advanced Distributed Associative Memory (ADAM) model. ADAM essentially is a two layered binary weighted neural network aimed at recognizing and extracting features from images. Austin et al. further design C-NNAP (Cellular Neural Network Associative Processor) <ref type="bibr" target="#b19">[20]</ref>,</p><p>which is a MIMD array of ADAM based processors to provide a distributed solution to the object recognition problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CNN implementations</head><p>Chua and Yang <ref type="bibr" target="#b181">[181]</ref><ref type="bibr" target="#b182">[182]</ref><ref type="bibr" target="#b183">[183]</ref> introduced CNN as an regular array of locally interconnected analog processing elements, or cells, operating in parallel, whose dynamic behavior is determined by the cell connectivity pattern (neighborhood extent) and a set of configurable parameters. CNN by its very design is a circuit oriented architecture and is conceptually suitable for hardware implementation. After the inception of CNN, their implementations in hardware have attracted substantial interest covering different types of CNN models differing in interaction type (e.g., linear, non-linear, dynamic, or delay), modes of operation (e.g., dense time versus discrete time, oscillating type versus dynamic), and grid topology (e.g., planar, polygonal, circular etc.). There exist analog <ref type="bibr" target="#b104">[104,</ref><ref type="bibr" target="#b106">106]</ref>, digitally programmable <ref type="bibr" target="#b103">[103,</ref><ref type="bibr" target="#b105">105]</ref>, hybrid <ref type="bibr" target="#b37">[38]</ref>, FPGA <ref type="bibr" target="#b184">[184,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b185">185,</ref><ref type="bibr" target="#b107">107]</ref>, as well as optical <ref type="bibr" target="#b14">[15]</ref> implementations for CNN. CNN implementations can achieve speeds upto several tera flops and are ideal for the applications which require low power consumption, high processing speed, and emergent computation, e.g., real-time image processing <ref type="bibr" target="#b1">[2]</ref>. We will only briefly cover some of the recent representative implementations here. For further details readers may look into the detailed overview <ref type="bibr" target="#b186">[186]</ref> and monographs <ref type="bibr" target="#b187">[187,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b188">188]</ref>.</p><p>Rodriguez-Vazquez et al. <ref type="bibr" target="#b31">[32]</ref> discuss ACE16k, a mixed-signal SIMD-CNN ACE (Analogic Cellular Engine) chips as a vision system on chip realizing CNN Universal Machine (CNN-UM) <ref type="bibr" target="#b189">[189]</ref>. ACE16k is designed using 0:35 mm CMOS technology with 85% analog elements. Its design incorporates several advancements over its predecessor ACE4k chip <ref type="bibr" target="#b190">[190]</ref> including the use of local analog memories and ACE-BUS enabling it to process complex spatiotemporal images in parallel through a 32-bit data bus working at 120 MBPS with peak processing speed of 330 GOPS. The ACE16k chip consists of an array of 128 Â 128 locally connected mixedsignal processing units operating under SIMD mode. Yalcin et al. in <ref type="bibr" target="#b191">[191]</ref> discuss the spatio-temporal pattern formation in ACE16k and Carranza et al. <ref type="bibr" target="#b192">[192]</ref> present design of a programmable stand-alone system ACE16k-DB for real-time vision pre-processing tasks using ACE16k together with Xilinx XC4028XL FPGA. ACE16k chips have been used in commercial Bi-i <ref type="bibr" target="#b32">[33]</ref> speed vision system developed by AnaLogic Computers Ltd and MTA-SZTAKI. Also there exist many recent topographic, sensory, and Cellular Wave Architectures and corresponding hardware implementations based upon CNN-UM. Zarandy et al. <ref type="bibr" target="#b193">[193]</ref> present a brief overview of these implementations. An FPGA based emulated-digital CNN-UM implementation using GAPU (Global Analogic Programming Unit) as discussed by Voroshazi et al. <ref type="bibr" target="#b194">[194]</ref> is a recent work in this direction. They discuss design of an extended Falcon architecture using GAPU. Falcon was earlier proposed as a reconfigurable multi-layer FPGA based CNN-UM implementation employing systolic array architecture by Nagy and Szolgay in <ref type="bibr" target="#b195">[195]</ref>. In its original design, Falcon could compute result of only one iteration (e.g., only one image in a video sequence) so in <ref type="bibr" target="#b194">[194]</ref> a high level embedded control and arithmetic logic block (GAPU) is used which could support several interaction together. Actual design of the GAPU employs Xilinx MicroBlaze architecture. The comparative tests revealed that in comparison to software based implementation using Intel core2 Duo T7200 processor with optimized C++ code, the FPGA based hardware implementation could achieve 47 times speed up in time.</p><p>Arena et al. <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> discuss design of a CNN-based analog VLSI chip for real-time locomotion control in legged robots. The analog chip core solves the gait generation task whereas digital control modulates the behavior to deal with sensory feedback. An experimental six cell CNN chip is designed using a switched capacitors in CMOS AMS 0:8-mm technology.</p><p>One of the more recent works include the design of a stochastic bit-stream CNN model by Rak et al. <ref type="bibr" target="#b12">[13]</ref>, which is implemented using FPGA. Also Ho et al. <ref type="bibr" target="#b196">[196]</ref> suggest design of a CNN simulator using graphics processing unit (GPU) <ref type="bibr" target="#b197">[197]</ref> consisting of high performance parallel graphics accelerators, by parallelizing the CNN computations so that they can be executed concurrently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Neuromorphic HNNs</head><p>Neuromorphic refers to a circuit that closely emulates the biological neural design. The processing is mostly analog, although outputs can be digital. Examples include Silicon Retina <ref type="bibr" target="#b6">[7]</ref> and Synaptic Touchpad <ref type="bibr" target="#b198">[198]</ref>. Another important category of neuromorphic HNNs is Pulse Coupled Neural Networks (PCNNs) <ref type="bibr" target="#b99">[99,</ref><ref type="bibr" target="#b101">101]</ref>. These have been designed after the mammalian visual system, and further implemented in hardware. Like many other NN models, PCNNs can perform image preprocessing, such as edge finding and segmentation. The time series output is invariant to scaling, rotation and translation. A compact architecture for analog CMOS hardware implementation of voltagemode PCNNs is presented by Ota and Wilamowski in <ref type="bibr" target="#b99">[99]</ref>, which shows inherent fault tolerance and high speed compared to its software counterpart.</p><p>An important aspect of neuromorphic designs is the address event representation protocol (AER). There has been a considerable effort to create larger neuromorphic neural networks with point-to-point pulse/spike communication between neural assemblies. AER is used to emulate the point-to-point connections for SNNs of considerable size. They are now quite popular in the neuromorphic community. This work was initiated by Mahowald <ref type="bibr" target="#b199">[199]</ref> and Mortara <ref type="bibr" target="#b200">[200]</ref>. Over the last years AER has been perfected by Boahen <ref type="bibr" target="#b201">[201]</ref> and a large AER neuromorphic network system in hardware for visual processing has been presented in Serrano-Gotarredona et al. <ref type="bibr" target="#b202">[202]</ref>, claimed to be the most complex neuromorphic pulse communication network yet. In a more recent work Bamford et al. <ref type="bibr" target="#b203">[203]</ref> discuss design of a distributed and locally reprogrammable address event receiver, which could allow for arbitrarily large axonal fan-out.</p><p>Selective attention is a mechanism used to sequentially select and process only relevant subregions of the input space, while suppressing other irrelevant inputs arriving from other regions. By processing small amounts of sensory information in a serial fashion, rather than attempting to process all the sensory data in parallel, this mechanism overcomes the problem of flooding limited processing capacity systems with sensory inputs. Indiveri <ref type="bibr" target="#b141">[141,</ref><ref type="bibr" target="#b204">204]</ref> presents a 2-D neuromorphic hardware model called attention chip, which implements a real-time model of selective attention, for sequentially selecting the most salient locations of its inputs space. It is implemented on an analog VLSI chip using spike-based representations for receiving input signals, transmitting output signals and for shifting the selection of the attended input stimulus over time. Experiments were carried out using a 8 Â 8 grid, demonstrating how the chip's bias parameters could be used to impose different behaviors of the system. Also we should add the recent convolusion chip by Serrano Gotarredona et al. <ref type="bibr" target="#b205">[205]</ref>, which can implement many classical NN computations, specifically feature-maps.</p><p>The silicon retina are an important class of neuromorphic hardware with a potential to have commercial success beyond pure research. The earliest electronic retina was proposed by Fukushima et al. <ref type="bibr" target="#b206">[206]</ref> in 1970 itself and was subsequently integrated onto an ASIC by Mahowald <ref type="bibr" target="#b199">[199]</ref> in early nineties. Besides spatial contrast/derivative retina, later focus has been turned towards temporal contrast/derivative retina <ref type="bibr" target="#b207">[207,</ref><ref type="bibr" target="#b208">208]</ref>.</p><p>However, unlike the spatial contrast retina they do not communicate with their neighbors to attain a collective computation. Recent and relatively popular studies on the design of a neuromorphic model for mammalian retina include those by Boahen's group <ref type="bibr" target="#b209">[209]</ref><ref type="bibr" target="#b210">[210]</ref><ref type="bibr" target="#b211">[211]</ref><ref type="bibr" target="#b212">[212]</ref>. In these studies both outer and inner retina were modeled such that outer retina model performs linear band-pass spatiotemporal filtering and inner retina model performs high-pass temporal filtering and can realize non-linear temporal frequency adaptation as well as contrast gain control <ref type="bibr" target="#b209">[209]</ref>. The presented model was fabricated as actual chip having 90 Â 60 photoreceptor, 3.5 Â 3.3 mm 2 surface area using 0:35 mm-CMOS technology <ref type="bibr" target="#b210">[210]</ref>. As authors report, the chip has photoreceptor density only 2.5 times sparser that the human cone density. However, in contrast to actual mammalian retina, such designed retina chip does not respond at high temporal frequencies (10 Hz and above) <ref type="bibr" target="#b211">[211]</ref>.</p><p>Another important topic of Neuromorphic hardware are the silicon cochleae. The network aspect is somewhat weak for them, although the sensor nodes do have connections to one neighbor but more in the manner of a processing chain than a network. Initial work in this direction was reported by Lyon and Mead <ref type="bibr" target="#b213">[213]</ref> and Lazzaro and Mead <ref type="bibr" target="#b214">[214]</ref>. Recent improvements have been reported by Sarpeshkar et al. <ref type="bibr" target="#b215">[215]</ref> and Chan et al. <ref type="bibr" target="#b216">[216]</ref>.</p><p>Indiveri et al. <ref type="bibr" target="#b67">[67]</ref> present current state of the are in the field of neuromorphic engineering <ref type="bibr" target="#b217">[217]</ref> and discuss the challenges for designing cognitive-neuromorphic systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Spiking neural network hardware</head><p>Spiking (or pulsed) ANNs (SNNs), a class of ANNs, model neurons on a level relating more closely to biology and have attracted attention in many bio-sensing areas including image processing applications <ref type="bibr" target="#b97">[97]</ref> and olfactory sensing <ref type="bibr" target="#b48">[48]</ref>. They incorporate computation of membrane potentials, synaptic time delays, and dynamical thresholds, in addition to the prevalent synaptic weighting, postsynaptic summation, static threshold, and saturation. A SNN model synchronizes by taking into account the precise timing of spike events. A noteworthy characteristics of SNNs is that they have been proven to be computationally more powerful than classical ANN models with sigmoidal neurons <ref type="bibr" target="#b218">[218]</ref>. However, computing large networks of complex neuron models is a computationally expensive task and leads to longer execution delays even with high-performance workstations <ref type="bibr" target="#b219">[219]</ref>. Hardware implementations of a single spiking neuron model has been discussed in the Section 3.3. We next consider relatively recent efforts on designing low power compact VLSI architectures for large scale implementations of SNN models.</p><p>Schoenauer et al. <ref type="bibr" target="#b22">[23]</ref> present a neuroprocessor, called NeuroPipe-Chip, as part of an accelerator board, which approaches real-time computational requirements for SANNs in the order of 10 6 neurons. For a simple SNN benchmark network for image segmentation, the simulation of the accelerator suggested nearly two orders of magnitude faster computation time than a 500 MHz Alpha workstation and a performance comparable to dedicated accelerator architecture consisting of 64 high-performance DSPs. The NeuroPipe-Chip comprising 100 K gate equivalents is fabricated in an Alcatel five-metal layer 0:35-mm digital CMOS technology. To improve the speed of computations weight caches are used to accumulate all weighted spikes occurring in one time slot. To further speed up the performance, the NeuroPipe-Chip design was augmented with additional on-chip inhibition unit, which would apply equally distributed negative potential to a large set of spikes. Floriano et al. <ref type="bibr" target="#b35">[36]</ref> also demonstrate the usefulness of hardware SANNs in designing embedded microcontrollers for autonomous robots which can evolve the ability to move in a small maze without external support. More recently, Bellis et al. <ref type="bibr" target="#b36">[37]</ref> report using an FPGA based implementation of SNN for building collaborative autonomous agents.</p><p>Ros et al. <ref type="bibr" target="#b220">[220]</ref> present a HW/SW codesign approach, where the spike response model for a neuron is implemented in hardware and the network model of these neurons and the learning are implemented in software with a support for an incremental transition of the software components into hardware. Neuronal synapses are modeled as input-driven conductances and various stages of the temporal dynamics of the synaptic integration process are executed in parallel. Multiple PEs process different neurons concurrently. Effectiveness of the proposed architecture is tested with a prototype system using FPGA board and a host computer interacting with each other using PCI bus on a real-time visual data with a time resolution of 100 ms. Similarly Zou et al. <ref type="bibr" target="#b221">[221]</ref> also present real-time simulation architecture for networks of Hodgkin-Huxley spiking neurons using a mix of analog circuits and a host computer.</p><p>Vogelstein et al. in <ref type="bibr" target="#b222">[222]</ref> describe a mixed signal VLSI chip with on-chip learning for emulating larger SNN models. The experimentally designed chip consists of 60Â 40 array of I&amp;F neuron with reconfigurable synaptic connectivity allowing arbitrary number of synaptic connections to exist between neurons. The synaptic connections are actually implemented using digital RAM enabling reconfiguration of these connections and associated parameters (e.g., conductance value, post synaptic address) on-the-fly. The actual neuron and its membrane dynamics are implemented in an analog VLSI using a conductance based modeling. The chip has an area of 9 mm 2 with 645 mW of power consumption on 10 MCUPS activity.</p><p>The chip was demonstrated to emulate attractor dynamics observed in the neural activity in rat hippocampal ''place cells'' <ref type="bibr" target="#b223">[223]</ref>.</p><p>Koickal et al. <ref type="bibr" target="#b224">[224]</ref> present a spike-timing based reconfigurable single chip architecture for neuromorphic designs. The presented architecture uses only one type of event block designed as an analog circuit, which can be configured to model the functionality of a leaky I&amp;F neuron, a summing exponential synapse, a spike time dependent learning window, and for adaptively generating a compensating current at the neuron input so that neuron firing synchronizes with the timing of a target signal. The configurable event block uses a programmable capacitor array designed earlier by the same authors in <ref type="bibr" target="#b225">[225]</ref> together with an operational transconductor, and a comparator and occupies an area of 0.03 mm 2 .</p><p>Spiking models have also received a lot of attention in the context of learning rules. Traditional ANNs process real numbers that are inspired by average spiking frequency of real neurons. A fully represented spike train from a neuron, however, can potentially convey much more information content. Some neurophysiological experiments investigating synaptic change, i.e., learning, for example, indicate that relative spike timing of single spike pairs influences direction and magnitude of change of synaptic efficacy, i.e., average spiking frequencies are insufficient to describe the learning behavior of real neurons. This was implemented in neuromorphic on-chip learning synapses by Hafliger et al. <ref type="bibr" target="#b226">[226]</ref> and recently advanced by Fusi et al. <ref type="bibr" target="#b138">[138]</ref> and Chicca et al. <ref type="bibr" target="#b227">[227]</ref>. The latest publications by these groups <ref type="bibr" target="#b228">[228,</ref><ref type="bibr" target="#b229">229]</ref> also describe network experiments with those synapses.</p><p>Attempts to realize (multiplier-less) SNN models include works of Chen et al. <ref type="bibr" target="#b230">[230,</ref><ref type="bibr" target="#b231">231]</ref> and of Ghani et al. <ref type="bibr" target="#b232">[232]</ref>. A recent article by Maguire et al. <ref type="bibr" target="#b63">[63]</ref> presents a detailed overview of conventional simulation based approaches to implement SNNs and further details various FPGA based implementations of SNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Optical neural networks</head><p>In this section we provide a brief overview on optical neural networks (ONNs), designed on the principles of optical comput-ing. Optical technology (see <ref type="bibr" target="#b233">[233]</ref>) utilizes the effect of light beam processing that is inherently massively parallel, very fast, and without the side effects of mutual interference. Optical transmission signals can be multiplexed in time, space, and wavelength domains, and optical technologies may overcome the problems inherent in electronics. The results range from the development of special-purpose associative memory systems through various optical devices (e.g., holographic elements for implementing weighted interconnections) to optical neurochips. Optical techniques ideally match with the needs for the realization of a dense network of weighted interconnections.</p><p>Optical technology has a number of advantages for making interconnections, specifically with regard to density, capacity and 2D programmability. One of the early ONN design using optical vector matrix product processor or crossbar interconnection architecture is discussed in <ref type="bibr" target="#b234">[234,</ref><ref type="bibr" target="#b235">235]</ref>. Similarly a spatial coding method of dealing with input/output patterns as 2D information is used to develop a neural network system with learning capabilities in <ref type="bibr" target="#b111">[111]</ref>. However, the lack of efficient optical switches and high capacity erasable optical memories has been the cause of a bottleneck in the growth of ONNs. Typically such optical switch or spatial light modulator is designed as a set of movable mirrors, called a deformable mirror device (DMD), which is inherently difficult to design on large scale.</p><p>Hopfield networks are widely used for exemplifying optical implementations. An optical 2D NN has been developed <ref type="bibr" target="#b112">[112]</ref> using a liquid-crystal television (LCTV) and a lenslet array for producing multiple imaging under incoherent illumination. Multi-layer feedforward/feedback networks have also been optically implemented with the threshold function getting evaluated electronically <ref type="bibr" target="#b95">[95]</ref> or approximately realized by optical devices <ref type="bibr" target="#b96">[96]</ref>. In the second approach, the architecture employs LCTVs to implement the inputs and the weights, while liquid crystal light valves are used to implement the non-linear threshold <ref type="bibr" target="#b13">[14]</ref>.</p><p>An example of optical neurocomputer is the Caltech ''Holographic Associative Memory'' presented in <ref type="bibr" target="#b110">[110]</ref>. The goal of the system is to find the best match between an input image and a set of holographic images that represent its memory. Neurons are modeled by non-linear optical switching elements (optical transistors) that are able to change their transmittance properties as the brightness of a light beam changes. Weighted interconnections are modeled by holograms, which are able to record and reconstruct the intensity of light rays. A 1 in planar hologram, produced on a tiny photographic film, can fully interconnect 10,000 light sources with 10,000 light sensors making 100 million interconnections. The whole system, consisting of a set of lenses and mirrors, a pinhole array, two holograms and an ''optical transistor'', is realized as an optical loop.</p><p>As Lange et al. <ref type="bibr" target="#b27">[28]</ref> discuss, both electronic and optical technology could be useful to solve problems in real-time image processing applications. They fabricated an optical neurochip for fast analog multiplication with weight storage elements and on-chip learning capability. The chip can hold upto 128 fully interconnected neurons. They have also developed the ''artificial retina chip'', a device that can concurrently sense and process images for edge enhancement or for feature extraction. Applications of these optical devices are in the domains of image compression and character recognition.</p><p>Silveira <ref type="bibr" target="#b236">[236]</ref> presents recent review on various issues and design approaches related to these optoelectronic NN implementations.</p><p>Lack of effective programmability is one of the major limitation in optical implementations. Burns et al. <ref type="bibr" target="#b237">[237]</ref> describe an optoelectronic design to overcome this limitation using a combination of optics and electronics with high fan-in and temporal multiplexing of the weights. The layered network design consists of electronically controlled optical input layer using spatial light modulation with subsequent electronic processing, though the multiplication of input pattern with interconnection weight was still carried out using software. Their design significantly reduced the photo-induced charge leakage of the neuron activations stored dynamically on capacitors.</p><p>Tokes et al. discuss in <ref type="bibr" target="#b14">[15]</ref> an optical CNN device, also known as Programmable Optical Array/Analogic Computer (POAC), which is based on modified Joint Fourier Transform Correlator and Bacteriorhodopsin as a holographic optical memory. Later Moagar-Poladian and Bulinski <ref type="bibr" target="#b238">[238]</ref> presented a type of reconfigurable optical neuron, in which weights can be dynamically changed. Once a weight is set, it is memorized for a period of few days. The optical neuron comprises a photoelectret as the recording medium of the weights and an optical non-linear crystal with transverse Pockels effect. First described in 1906 by the German physicist Friedrich Pockels, Pockels effect is a linear electrooptical effect, in which, the application of an electric field produces a birefringence which is proportional to the field. To achieve, transverse Pockels effect, the electro-optic crystal is used in the transverse mode, i.e., the optic axis is set perpendicular to the direction of propagation of the light. Shortt et al. <ref type="bibr" target="#b239">[239]</ref> demonstrate a bipolar matrix vector multiplier based optical implementation of the Kak neural network <ref type="bibr" target="#b240">[240]</ref>. For this, the CC4 algorithm was modified on the training phase for implementing N-Parity problem. First proposed by Kak and Tang <ref type="bibr" target="#b241">[241]</ref>, CC4 is a corner classification training algorithm for three-layered feedforward neural networks. A very recent work in this direction include optical implementation of SNN using a thin film of electron-trapping material by Pashaie and Farhat <ref type="bibr" target="#b242">[242]</ref>.</p><p>Llata and Rivera <ref type="bibr" target="#b243">[243,</ref><ref type="bibr" target="#b244">244,</ref><ref type="bibr" target="#b15">16]</ref> have proposed design of vision system based upon a CMOS image sensor and a hybrid optoelectronic hardware architecture called optical broadcast neural network (OBNN). An OBNN processor classifies input patterns using Hamming classification using a set of reference patterns. The input signals are sent in their temporal order to an array of PEs for computing weight updates by means of an global optical broadcasting, thus taking advantage of fast optical communication as well as electronic computational processing. The downside of the architecture is that it is sensitive to rotation, translation, and scaling of the input images. To overcome these limitations, recently in <ref type="bibr" target="#b20">[21]</ref>, they extend the design by introducing PCNN preprocessor stage, which converts an 2D input image into a temporal pulsed pattern. These pulses are then applied as inputs to the OBNN processor. The combined system is reported to achieve the rate of 10 4 classifications per second on binary input images of size 128 Â 128 pixels.</p><p>Articles by Yu and Uang <ref type="bibr" target="#b245">[245]</ref> and by Ahmed et al. <ref type="bibr" target="#b246">[246]</ref> are interesting reviews on the later advancements in the design and implementation of ONNs. Ref. <ref type="bibr" target="#b246">[246]</ref> has additional discussion on the design of a portable POAC and optical template library.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions and discussion</head><p>HNN research and applications have witnessed a slow and incremental progress in last two decades. Even though ANN hardware has been there for more than last two decades, the rapid growth in general purpose hardware (microprocessors, DSPs, etc.) did not let most of these implementations to outperform to the extent of becoming commercially successful. Nonetheless, novel application areas have steadily started appearing, e.g., embedded microcontroller for autonomous robots <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b247">247]</ref>, autonomous flight control <ref type="bibr" target="#b248">[248,</ref><ref type="bibr" target="#b249">249]</ref>, proposed silicon model of the cerebral cortex-neurogrid <ref type="bibr" target="#b250">[250]</ref> (also see <ref type="bibr" target="#b251">[251]</ref>), and silicon retina <ref type="bibr" target="#b212">[212,</ref><ref type="bibr" target="#b211">211]</ref>. In recent years several special issues dedicated to HNN implementations have been published <ref type="bibr" target="#b252">[252]</ref><ref type="bibr" target="#b253">[253]</ref><ref type="bibr" target="#b254">[254]</ref> as well as a steady stream of Ph.D theses have appeared <ref type="bibr" target="#b255">[255]</ref><ref type="bibr" target="#b256">[256]</ref><ref type="bibr" target="#b257">[257]</ref> indicating the growing interest in the area.</p><p>However, in spite of the presence of expressive high-level hardware description languages and compilers, efficient neuralhardware designs still demand ingenious ways to optimally use the available resources for achieving high speed and low power dissipation. Judicious mapping of ANN models onto parallel architectures, entailing efficient computation and communication, is thus a key step in any HNN design and there is a need to design tools for automatically translating high level ANN models onto hardware <ref type="bibr" target="#b258">[258,</ref><ref type="bibr">Section 5.5]</ref>.</p><p>As noted in <ref type="bibr" target="#b259">[259]</ref>, digital neurohardware tends to be more algorithm specific requiring a good knowledge about algorithms as well as system design that eventually results into a high timeto-market as compared to conventional hardware. In this respect general-purpose hardware seems more user-friendly, offering more flexibility with uniform programming interfaces, and can therefore profit more from advances in technology and architectural revisions. However, many of the applications involving ANNs often demand computational capabilities exceeding of workstations or personal computers available today. For example, a typical real-time image processing task may demand 10 teraflops, <ref type="foot" target="#foot_1">2</ref> which is well beyond the current capacities of PCs or workstations today. In such cases neurohardware appears attractive choice and can provide a better cost-to-performance ratio even when compared to supercomputers because many aspects of user friendliness vanish for the supercomputers which are also relatively expensive.</p><p>Since currently many ANN applications use networks with less than 10 4 neurons and/or inputs and only need occasional training, software simulation is usually found to be sufficient in such situations. But when ANN algorithms develop to the point where useful things can only be done with 10 6 -10 8 of neurons and 10 10 -10 14 of synapses between them <ref type="bibr" target="#b260">[260,</ref><ref type="bibr" target="#b261">261]</ref>, high performance neural hardware will become essential for practical operations. It is important to add that such large scale neural network hardware designs might not be a distant reality as is apparent from the recent work of Schemmel et al. on wafer-scale integration of large SNN models <ref type="bibr" target="#b262">[262,</ref><ref type="bibr" target="#b263">263]</ref>.</p><p>It is observed that presently it is not always possible to exploit the entire parallelism inherent in the ANN topology along with a good cost-performance ratio, mainly due to the cost associated with the implementation of the numerous interconnections, control and mapping involved. In this scenario, optical implementations add a different dimension. Multi-Chip Modules or Wafer-Scale Integration hold further promise for implementing such large networks. IBM cell processor <ref type="bibr" target="#b264">[264]</ref> with nine processor cores or its recent variant QS22 PowerXCell 8i <ref type="bibr" target="#b265">[265]</ref> with their powerful vector processing capabilities hold good promise for highly parallel large scale ANN implementations or their fast emulations for comparative analysis. Also using 3D VLSI packaging technology <ref type="bibr" target="#b266">[266]</ref>, large number of synaptic connection could possibly be realized in small space. 3D VLSI classifier <ref type="bibr" target="#b5">[6]</ref> as discussed before in Section 4.1 is an example at hand. CMOS/nanowire/nanodevice (''CMOL'') technology <ref type="bibr" target="#b267">[267,</ref><ref type="bibr" target="#b268">268]</ref>, which combines both CMOS and nanotechnology, is one of the important emerging technologies with high potential for large scale HNN implementations. The basic idea of CMOL circuits is to combine the advantages of CMOS technology including its flexibility and high fabrication yield with the high potential density of molecular-scale two-terminal nanodevices. Neuro-morphic Mixed-Signal CMOL Circuits (known as ''CrossNets'') <ref type="bibr" target="#b269">[269]</ref><ref type="bibr" target="#b270">[270]</ref><ref type="bibr" target="#b271">[271]</ref><ref type="bibr" target="#b272">[272]</ref><ref type="bibr" target="#b273">[273]</ref> are the first results of an active research by K. Likharev's Nanotechnology Research Group at Stony Brook University. In a ''CrossNet'', CMOS subsystem realizes the neuron core, whereas crossbar nanowires play the roles of axons and dendrites (connections), and crosspoint latching switches serve as elementary (binary-weight) synapses enabling very high cell connectivity (e.g., 10 4 ) in quasi-2D electronic circuits.</p><p>Molecular technology is another relatively new approach for possible hardware implementation. It combines protein engineering, biosensors, and polymer chemistry in the efforts to develop a molecular computer <ref type="bibr" target="#b274">[274]</ref>. The computation uses the physical recognition ability of large molecules, like proteins, which can change their shape depending on the chemical interactions with other molecules. Molecular computing is still in its infancy. The major problem is to develop appropriate technology that would allow for construction of bio equivalents of transistors. However, inherently parallel generalization and adaptation capabilities perfectly match the needs of neural networks implementations. Research in this direction appears promising <ref type="bibr" target="#b275">[275,</ref><ref type="bibr" target="#b276">276]</ref> and, in the future, molecular computers with neural architectures appears to have a potential to become a reality. Dan Ventura <ref type="bibr" target="#b277">[277]</ref> presents an interesting discussion on the possibility designing quantum neural computing devices utilizing quantum entanglement effects. In a recent work Alibart et al. <ref type="bibr" target="#b278">[278]</ref> report feasibility of designing a hybrid nanoparticle-organic device, a nanoparticle organic memory FET, which uses the nanoscale capacitance of the nanoparticles and the transconductance gain of the organic transistor to mimic the short-term plasticity of a biological synapse.</p><p>HNN models hopefully will have the respected place in coming years when industry will face demands imposed by ubiquitous computing with learning and autonomous decision making capabilities, e.g., autonomous robotics and assistive technologies. These applications demand dealing with large amounts of real-time multimedia data from interacting environment, using lightweight hardware with strict power constraints, without letting the computational efficiencies go down. The DARPA initiative <ref type="bibr" target="#b261">[261]</ref>-''Systems of Neuromorphic Adaptive Plastic Scalable Electronics''-towards building cognitive-neuromorphic systems <ref type="bibr" target="#b67">[67]</ref> is indicative of such emerging directions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Contents lists available at ScienceDirect journal homepage: www.elsevier.com/locate/neucom Neurocomputing 0925-2312/$ -see front matter &amp; 2010 Elsevier B.V. All rights reserved. doi:10.1016/j.neucom.2010.03.021</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Examples of HNN applications.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>ANN-HNN table.    </figDesc><table><row><cell>ANN</cell><cell>HNN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">Digital Analog Hybrid Neuromorphic FPGA Optical</cell></row><row><cell>MLP</cell><cell>[90]</cell><cell></cell><cell></cell><cell>[91]</cell><cell></cell></row><row><cell>RBF</cell><cell>[92]</cell><cell>[79]</cell><cell></cell><cell>[31]</cell><cell></cell></row><row><cell></cell><cell></cell><cell>[80]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SOFM</cell><cell>[93]</cell><cell>[94]</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>[92]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Feed-forward [77]</cell><cell></cell><cell></cell><cell>[41]</cell><cell>[95]</cell></row><row><cell>Network</cell><cell>[92]</cell><cell></cell><cell></cell><cell>[12]</cell><cell>[96]</cell></row><row><cell>Spiking NN</cell><cell>[23]</cell><cell></cell><cell>[97]</cell><cell>[37]</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>[98]</cell><cell>[98]</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>[11]</cell><cell></cell></row><row><cell>Pulse</cell><cell></cell><cell>[99]</cell><cell>[99]</cell><cell>[100]</cell><cell></cell></row><row><cell>Coded NN</cell><cell></cell><cell></cell><cell>[101]</cell><cell>[102]</cell><cell></cell></row><row><cell>CNN</cell><cell>[103]</cell><cell>[104]</cell><cell>[38]</cell><cell>[29]</cell><cell>[15]</cell></row><row><cell></cell><cell>[105]</cell><cell>[106]</cell><cell></cell><cell>[107]</cell><cell></cell></row><row><cell></cell><cell></cell><cell>[39]</cell><cell></cell><cell>[13]</cell><cell></cell></row><row><cell>Associative</cell><cell></cell><cell>[108]</cell><cell>[109]</cell><cell>[18]</cell><cell>[110]</cell></row><row><cell>Memory</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[111]</cell></row><row><cell>Recurrent NN</cell><cell></cell><cell>[8]</cell><cell></cell><cell></cell><cell>[112]</cell></row><row><cell>Stochastic NN</cell><cell></cell><cell></cell><cell></cell><cell>[113]</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>[114]</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>[12]</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>J. Misra, I. Saha / Neurocomputing 74 (2010)<ref type="bibr" target="#b239">[239]</ref><ref type="bibr" target="#b240">[240]</ref><ref type="bibr" target="#b241">[241]</ref><ref type="bibr" target="#b242">[242]</ref><ref type="bibr" target="#b243">[243]</ref><ref type="bibr" target="#b244">[244]</ref><ref type="bibr" target="#b245">[245]</ref><ref type="bibr" target="#b246">[246]</ref><ref type="bibr" target="#b247">[247]</ref><ref type="bibr" target="#b248">[248]</ref><ref type="bibr" target="#b249">[249]</ref><ref type="bibr" target="#b250">[250]</ref><ref type="bibr" target="#b251">[251]</ref><ref type="bibr" target="#b252">[252]</ref><ref type="bibr" target="#b253">[253]</ref><ref type="bibr" target="#b254">[254]</ref><ref type="bibr" target="#b255">[255]</ref> </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Assuming the frame-rate of 100 fps, frame size of 1280 Â 1024 pixels with</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>bytes per pixel, and average number of basic imaging operation having computational complexity of OðNÞ, (N is the frame-size) with 10 5 such operations to be performed on each frame.J. Misra, I. Saha / Neurocomputing 74 (2010)<ref type="bibr" target="#b239">[239]</ref><ref type="bibr" target="#b240">[240]</ref><ref type="bibr" target="#b241">[241]</ref><ref type="bibr" target="#b242">[242]</ref><ref type="bibr" target="#b243">[243]</ref><ref type="bibr" target="#b244">[244]</ref><ref type="bibr" target="#b245">[245]</ref><ref type="bibr" target="#b246">[246]</ref><ref type="bibr" target="#b247">[247]</ref><ref type="bibr" target="#b248">[248]</ref><ref type="bibr" target="#b249">[249]</ref><ref type="bibr" target="#b250">[250]</ref><ref type="bibr" target="#b251">[251]</ref><ref type="bibr" target="#b252">[252]</ref><ref type="bibr" target="#b253">[253]</ref><ref type="bibr" target="#b254">[254]</ref><ref type="bibr" target="#b255">[255]</ref> </p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Review of hardware neural networks: a user&apos;s perspective</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lindsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lindblad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Third Workshop on Neural Networks: From Biology to High Energy Physics</title>
		<meeting>Third Workshop on Neural Networks: From Biology to High Energy Physics<address><addrLine>Isola d&apos;Elba, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="195" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Cellular Neural Networks: Analysis, Design, and Optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Änggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Moschytz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Norwell, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fault tolerance mechanism in chip many-core processors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tsinghua Science &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="169" to="174" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>Suppl. 1</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Kung</surname></persName>
		</author>
		<title level="m">Digital Neural Networks</title>
		<meeting><address><addrLine>Upper Saddle River, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Digital hardware architectures for neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ienne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speedup Journal</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="25" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A compact 3-D VLSI classifier using bagging threshold network ensembles</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bermak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1097" to="1109" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Mead</surname></persName>
		</author>
		<title level="m">Analog VLSI and Neural Systems</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mixed-mode analog VLSI continuous-time recurrent neural network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garverick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Circuits, Signals and Systems</title>
		<meeting>International Conference on Circuits, Signals and Systems</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="104" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A mixed analog digital artificial neural network with on chip learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Leblebici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mlynek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEE Proceedings-Circuits, Devices and Systems</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page">345</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mixed analog/digital matrix-vector multiplier for neural network synapses</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bruun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dietrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analog Integrated Circuits and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="63" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Compact digital hardware implementations of spiking neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>D'haene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth FirW Ph.D. Symposium</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Van Campenhout</surname></persName>
		</editor>
		<imprint>
			<publisher>CD</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<author>
			<persName><forename type="first">N</forename><surname>Nedjah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>De Macedo Mourelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Reconfigurable hardware for neural networks: binary versus stochastic</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="249" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stochastic bitstream-based CNN and its implementation on FPGA</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Adam Rak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Soos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Circuit Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="587" to="612" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Incorporation of liquid-crystal light valve nonlinearities in optical multilayer neural networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Moerland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fiesler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Optics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="5301" to="5307" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Bacteriorhodopsin as an analog holographic memory for joint fourier implementation of CNN computers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tokes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Orz O</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Roska</surname></persName>
		</author>
		<idno>DNS-3-2000</idno>
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>Budapest, Hungary</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Computer and Automation Research Institute of the Hungarian Academy of Sciences</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Optoelectronic neural processor for smart vision applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lamela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ruiz-Llata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Imaging Science Journal</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="197" to="205" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The use of neural networks in high energy physics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Denby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="505" to="549" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Developing hardware-based applications using PRESENCE-2</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weeks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moulds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Perspectives in Pervasive Computing, Savoy Place</title>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="469" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">WISARD: a radical step forward in image recognition</title>
		<author>
			<persName><forename type="first">I</forename><surname>Aleksander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensor Review</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="120" to="124" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Buckle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moulds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tumer</surname></persName>
		</author>
		<title level="m">The cellular neural network associative processor (C-NNAP), in: Associative Processing and Processors</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="284" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image identification system based on an optical broadcast neural network and a pulse coupled neural network preprocessor stage</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ruiz-Llata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lamela-Rivera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Optics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="10" to="47" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image segmentation based on oscillatory correlation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="805" to="836" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">NeuroPipe-Chip: a digital neuro-processor for spiking neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schoenauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Atasoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mehrtash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Klar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="205" to="213" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Massively distributed digital implementation of an integrate-and-fire legion network for visual scene segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Girau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Torres-Huitzil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">7-9</biblScope>
			<biblScope unit="page" from="1186" to="1197" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A hardware implementation of a binary neural image processor</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IV International Conference on Microelectronics for Neural Networks and Fuzzy Systems</title>
		<meeting>the IV International Conference on Microelectronics for Neural Networks and Fuzzy Systems<address><addrLine>Torino, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A CCD programmable image processor and its neural network applications</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid State Circuits</title>
		<imprint>
			<biblScope unit="page" from="1894" to="1901" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A low-power analog VLSI visual collision detector</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Harrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Saul</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Optical neural chips</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nitta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kyuma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>IEEE Micro</publisher>
			<biblScope unit="page" from="29" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Lopez-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moreno-Armendariz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riera-Babures</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Balsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Vilasis-Cardona</surname></persName>
		</author>
		<title level="m">Real time vision by FPGA implemented CNNs, Proceedings of the 2005 European Conference on Circuit Theory and Design</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">FPGA implementation of a neural network for a real-time hand tracking system</title>
		<author>
			<persName><forename type="first">M</forename><surname>Krips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lammert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kummert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of First IEEE</title>
		<meeting>First IEEE</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="313" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Implementation of an RBF neural network on embedded systems: real-time face tracking and identity verification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paindavoine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1162" to="1175" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">ACE16k: the third generation of mixed-signal SIMD-CNN ACE chips toward VSoCs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rodriguez-Vazquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Linan-Cembrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carranza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Roca-Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Carmona-Galan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jimenez-Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dominguez-Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Meana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems I: Regular Papers</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="851" to="863" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bi-i: a standalone ultra high speed cellular vision system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zarandy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rekeczky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Circuits and Systems Magazine</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="36" to="45" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Analog implementation of ANN with inherent quadratic nonlinearity of the synapses</title>
		<author>
			<persName><forename type="first">M</forename><surname>Milev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hristov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1187" to="1200" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A CMOS feed-forward neural network chip with on-chip parallel learning for oscillation cancellation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Brooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hirotsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1178" to="1186" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Evolutionary bits&apos;n&apos;spikes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Floreano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schoeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Caprari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blynel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Artificial Life</title>
		<meeting>the Eighth International Conference on Artificial Life<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="335" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Bellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Razeeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Delaney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mathuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pounds-Cornish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Colley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hagras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Callaghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Argyropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Karistianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nikiforidis</surname></persName>
		</author>
		<title level="m">FPGA implementation of spiking neural networks-an initial step towards building tangible collaborative autonomous agents</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="449" to="452" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE International Conference on Field-Programmable Technology</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A CNN-based chip for robot locomotion control</title>
		<author>
			<persName><forename type="first">P</forename><surname>Arena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fortuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Patane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems I: Regular Papers</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1862" to="1871" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">An autonomous minihexapod robot controlled through a CNN-based CPG VLSI chip</title>
		<author>
			<persName><forename type="first">P</forename><surname>Arena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fortuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Patane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollino</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m">International Workshop on Cellular Neural Networks and Their Applications</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Robot vision with cellular neural networks: a practical implementation of new algorithms</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pazienza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ponce-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Balsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Vilasis-Cardona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Circuit Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="449" to="462" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A stochastic digital implementation of a neural network controller for small wind turbine systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Foo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Power Electronics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1502" to="1507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A SIMD neural network processor for image processing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3497</biblScope>
			<biblScope unit="page" from="665" to="672" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hardware implementation of dynamic synapse neural networks for acoustic sound recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Dibazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bangalore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">O</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks</title>
		<meeting>the International Joint Conference on Neural Networks<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="2015" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cintia: a neuro-fuzzy real-time controller for low-power embedded systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chiaberge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Reyneri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="40" to="47" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Box 2-an analog audio synthesizer: architecture and procedures guide</title>
		<author>
			<persName><surname>Camalie</surname></persName>
		</author>
		<ptr target="http://www.camalie.com/MusicBox2/guide.docS" />
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Digital hardware realization of a recurrent neural network for solving the assignment problem</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="447" to="461" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">An analog neural network for solving the assignment problem</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Letters</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1047" to="1050" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Analog VLSI circuit implementation of an adaptive neuromorphic olfaction chip</title>
		<author>
			<persName><forename type="first">T</forename><surname>Koickal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pearce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems I: Regular Papers</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="60" to="73" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Neurocomputers: An Overview of Neural Networks in VLSI</title>
		<author>
			<persName><forename type="first">M</forename><surname>Glesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Poechmueller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Chapman and Hall</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Survey of neural network hardware</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lindsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lindblad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Conference on Applications and Science of Artificial Neural Networks</title>
		<meeting>the First International Conference on Applications and Science of Artificial Neural Networks</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">2492</biblScope>
			<biblScope unit="page" from="1194" to="1205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Overview of neural hardware</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heemskerk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neurocomputers for Brain-Style Processing, Design, Implementation and Application</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Special-purpose digital hardware for neural networks: an architectural survey</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ienne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cornu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of VLSI Signal Processing Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="25" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Classification of neural network hardware</title>
		<author>
			<persName><forename type="first">I</forename><surname>Aybay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cetinkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Halici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Network World</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="29" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Moerland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fiesler</surname></persName>
		</author>
		<title level="m">Neural network adaptations to hardware implementations</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Fiesler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Beale</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Institute of Physics Publishing and Oxford University Publishing</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="E1" />
		</imprint>
	</monogr>
	<note>Handbook of Neural Computation</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Parallel Architectures for Artificial Neural Networks: Paradigms and Implementations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Saratchandran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>IEEE Computer Society Press</publisher>
			<pubPlace>Los Alamitos, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Burr</surname></persName>
		</author>
		<title level="m">Parallel Digital Implementations of Neural Networks</title>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Przytula</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Prasanna</surname></persName>
		</editor>
		<meeting><address><addrLine>Upper Saddle River, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="223" to="281" />
		</imprint>
	</monogr>
	<note>Digital neurochip design</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Burr</surname></persName>
		</author>
		<title level="m">Energy, capacity, and technology scaling in digital VLSI neural networks, NIPS&apos;91 VLSI Workshop</title>
		<imprint>
			<date type="published" when="1991-05">May 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Digital VLSI for neural networks, in: The Handbook of Brain Theory and Neural Networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hammerstrom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="304" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">On the performance of pulsed and spiking neurons</title>
		<author>
			<persName><forename type="first">L</forename><surname>Reyneri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analog Integrated Circuits and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="101" to="119" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sutton</surname></persName>
		</author>
		<title level="m">FPGA implementations of neural networks-a survey of a decade of progress, Field-Programmable Logic and Applications</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2778</biblScope>
			<biblScope unit="page" from="1062" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Implementation issues of neuro-fuzzy hardware: going toward HW/SW codesign</title>
		<author>
			<persName><forename type="first">L</forename><surname>Reyneri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="176" to="194" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Artificial neural networks: a review of commercial hardware</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Diasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Antunesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Motab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="945" to="952" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Challenges for large-scale implementations of spiking neural networks on FPGAs</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Maguire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mcginnity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glackin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Belatreche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="13" to="29" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Synaptic dynamics in analog VLSI</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bartolozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Indiveri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2581" to="2603" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Implementing neural models in silicon</title>
		<author>
			<persName><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Nature-Inspired and Innovative Computing: Integrating Classical Models with Emerging Technologies</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">433</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A survey of bio-inspired and other alternative architectures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hammerstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Waser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nanotechnology: Information Technology-II</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="251" to="285" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Artificial cognitive systems: from VLSI networks of spiking neurons to neuromorphic cognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Indiveri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chicca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Douglas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="127" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">RAM-Based Neural Networks</title>
		<editor>J. Austin</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>World Scientific</publisher>
			<pubPlace>Singapore</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m">FPGA Implementations of Neural Networks</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Ormondi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Rajapakse</surname></persName>
		</editor>
		<meeting><address><addrLine>Dordrecht, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Valle</surname></persName>
		</author>
		<title level="m">Smart Adaptive Systems on Silicon</title>
		<meeting><address><addrLine>Dordrecht, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Weight perturbation: an optimal architecture and learning technique for analog VLSI feedforward and recurrent multilayer networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Flower</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="546" to="565" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Neural network constructive algorithms: trading generalization for learning efficiency?</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Smieja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Circuits, Systems, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="331" to="374" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Cascade error projection: an efficient hardware learning algorithm</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Duong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Neural Networks</title>
		<meeting>the IEEE International Conference on Neural Networks<address><addrLine>Perth, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="175" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Convergence analysis of cascade error projection: an efficient hardware learning algorithm</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Stubberud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Neural System</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Theory of monte carlo sampling-based alopex algorithms for neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Haykin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech, and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Spike-based learning in VLSI networks of integrate-andfire neurons</title>
		<author>
			<persName><forename type="first">G</forename><surname>Indiveri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fusi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<meeting>IEEE International Symposium on Circuits and Systems (ISCAS)</meeting>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="3371" to="3374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">A scalable parallel formulation of the backpropagation algorithm for hypercubes and related architectures</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Amin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1073" to="1090" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Back-propagation learning algorithm and parallel computers: the CLEPSYDRA mapping scheme</title>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="67" to="85" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">An analog processor architecture for a neural network classifier</title>
		<author>
			<persName><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Voz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Madrenas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="16" to="28" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Scalable closed-boundary analog neural networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Fakhraie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Farshbaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="492" to="504" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Parallel implementation of neocognitron on star topology: theoretical and experimental evaluation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="109" to="124" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Strey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Avellana</surname></persName>
		</author>
		<title level="m">A new concept for parallel neurocomputer architectures</title>
		<meeting><address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="470" to="477" />
		</imprint>
	</monogr>
	<note>Proceedings of EuroPar&apos;96</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Neural network hardware performance criteria</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Keulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Colak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Withagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hegt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Neural Networks</title>
		<meeting>the IEEE International Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="1885" to="1888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Performance of digital neuro-computers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cornu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ienne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Microelectronics for Neural Networks and Fuzzy Systems</title>
		<meeting>the Fourth International Conference on Microelectronics for Neural Networks and Fuzzy Systems</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="87" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">A VLSI architecture for high-performance, low-cost, onchip learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hammerstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks</title>
		<meeting>the International Joint Conference on Neural Networks<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="537" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">A reconfigurable approach to hardware implementation of neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Noory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Groza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Canadian Conference on Electrical and Computer Engineering</title>
		<meeting>Canadian Conference on Electrical and Computer Engineering</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1861" to="1864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Hayes-Gill</surname></persName>
		</author>
		<title level="m">IEE Proceedings of Circuits Devices and Systems</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="313" to="317" />
		</imprint>
	</monogr>
	<note>Piecewise linear approximation applied to nonlinear function of a neural network</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Finite precision error analysis of neural network hardware implementations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-N</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="281" to="290" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">How to build VLSI-efficient neural chips</title>
		<author>
			<persName><forename type="first">V</forename><surname>Beiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International ICSC Symposium on Engineering of Intelligent Systems</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Alpaydin</surname></persName>
		</editor>
		<meeting>the International ICSC Symposium on Engineering of Intelligent Systems<address><addrLine>Tenerife, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">An implementation of back-propagation learning on GF11, a large SIMD parallel computer</title>
		<author>
			<persName><forename type="first">M</forename><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zagha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="329" to="346" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Artificial neural network implementation on a single FPGA of a pipelined on-line back-propagation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gadea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cerda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ballester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Macholi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Symposium on System Synthesis</title>
		<meeting>the 13th International Symposium on System Synthesis</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="225" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Eppler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Becher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kock</surname></persName>
		</author>
		<title level="m">High speed neural network chip on PCI-board</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">A chip for self-organizing feature maps</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rueping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Ruckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Fourth International Conference on Microelectronics for Neural Networks and Fuzzy Systems</title>
		<meeting>Fourth International Conference on Microelectronics for Neural Networks and Fuzzy Systems</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="26" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">A new wide range Euclidean distance circuit for neural network hardware implementations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Titus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1176" to="1186" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Dynamic optical interconnections using holographic Lenslet arrays for adaptive neural networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Yuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="80" to="87" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Adaptive multilayer optical neural network with optical thresholding</title>
		<author>
			<persName><forename type="first">I</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fiesler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2435" to="2440" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Image Processing Using Pulse-Coupled Neural Networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lindblad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Kinser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York, Inc, Secaucus, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Building blocks for electronic spiking neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schaik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="617" to="628" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Analog implementation of pulse-coupled neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Wilamowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="539" to="544" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">FPGA implementation of a pulse density neural network with learning ability using simultaneous perturbation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="688" to="695" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Implementation of pulse-coupled neural networks in a CNAPS environment</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Kinser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lindblad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Neural Networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="584" to="597" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">A digital hardware pulse-mode neuron with piecewise linear activation function</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1028" to="1037" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">A fully digitally programmable CNN chip</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sargeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bonaiuto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="741" to="745" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">An analog implementation of discrete-time cellular neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Harrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nossek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stelzl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="466" to="476" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">A dedicated multi-chip programmable system for cellular neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Salerno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sargeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bonaiuto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analog Integrated Circuits and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="277" to="288" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">A programmable analog cellular neural network CMOS chip for high-speed image processing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kinget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steyaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid State Circuits</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="235" to="243" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Optimized cellular neural network universal machine emulation on FPGA</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pazienza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bellana-Camanes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riera-Babures</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Vilasis-Cardona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moreno-Armendariz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Balsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th European Conference on Circuit Theory and Design</title>
		<meeting>the 18th European Conference on Circuit Theory and Design</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="815" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">An associative memory with neural architecture and its VLSI implementation</title>
		<author>
			<persName><forename type="first">U</forename><surname>Ruckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Hawaii International Conference on System Sciences</title>
		<meeting>Hawaii International Conference on System Sciences</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="212" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Mixed mode VLSI implementation of a neural associative memory</title>
		<author>
			<persName><forename type="first">A</forename><surname>Heittmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Ruckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analog Integrated Circuits and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="172" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Optical neural computers</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Abumostafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Psaltis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<biblScope unit="volume">255</biblScope>
			<biblScope unit="page" from="88" to="95" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Optical association: a simple model for optical associative memory</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mukouzaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Toyoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Optics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="291" to="301" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Optical neural network with pocket sized liquid crystal televisions</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">T S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Gregory</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="863" to="865" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Daalen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jeavons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<title level="m">A stochastic neural architecture that exploits dynamically reconfigurable FPGAs</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Buell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Pocek</surname></persName>
		</editor>
		<meeting><address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
	<note>IEEE Workshop on FPGAs for Custom Computing Machines</note>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">FPGA-based stochastic neural networks-implementation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Bade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Hutchings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Workshop on FPGAs for Custom Computing Machines Workshop</title>
		<meeting>IEEE Workshop on FPGAs for Custom Computing Machines Workshop<address><addrLine>Napa, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Haykin</surname></persName>
		</author>
		<title level="m">Neural Networks: A Comprehensive Foundation</title>
		<meeting><address><addrLine>Upper Saddle River, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>third ed.</note>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><surname>Muthuramalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Himavathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural network implementation using FPGA: issues and application</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="2" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Probabilistic bit stream neural chip: theory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jeavons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daalen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="317" to="328" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Emergent activation functions from a stochastic bit stream neuron</title>
		<author>
			<persName><forename type="first">M</forename><surname>Daalen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kosel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jeavons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="331" to="333" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Fast neural network implementation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Skrbek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Network World</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="375" to="391" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">VLSI implementation of a neural network memory with several hundreds of neurons</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Straughn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tennant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIP Conference Proceedings 151 on Neural Networks for Computing</title>
		<meeting><address><addrLine>NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>American Institute of Physics Inc., Woodbury</publisher>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="182" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">A CCD based neural network integrated circuit with 64k analog programmable synapses</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Agranat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Neugebauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yariv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="551" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">A BiCMOS analog neural network with dynamically updated weights</title>
		<author>
			<persName><forename type="first">T</forename><surname>Morishita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Otsuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Transactions on Electronics</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="302" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">An electrically trainable artificial neural network (ETANN) with 10240 &apos;&apos;floating gate&apos;&apos; synapses</title>
		<author>
			<persName><forename type="first">M</forename><surname>Holler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Society Neural Networks Technology Series</title>
		<imprint>
			<biblScope unit="page" from="50" to="55" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">VLSI implementation of neural networks</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Wilamowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Binfet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Kaynak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Neural Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="191" to="197" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Analog implementation of neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zurada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Circuits and Devices Magazine</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="36" to="41" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Digitally programmable analog building blocks for the implementation of artificial neural networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Franca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="506" to="514" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Probabilistic computing with future deep sub-micrometer devices: a modeling approach</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Laurenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Circuits and Systems</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="2510" to="2513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">A continuous restricted Boltzmann machine with a hardware-amenable learning algorithm</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Neural Networks</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>the International Conference on Artificial Neural Networks</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="358" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Continuous restricted Boltzmann machine with an implementable training algorithm</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEE Proceedings-Vision Image and Signal Processing</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="page" from="153" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Gerstner</surname></persName>
		</author>
		<title level="m">Spiking Neuron Models: Single Neurons, Populations, Plasticity</title>
		<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<title level="m" type="main">Pulsed Neural Networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Analogue VLSI &apos;integrate-and-fire&apos; neuron with frequency adaptation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jabri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics Letters</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="1357" to="1358" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">An improved silicon neuron</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rasche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Douglas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analog Integrated Circuits and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="227" to="236" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Simple model of spiking neurons</title>
		<author>
			<persName><forename type="first">E</forename><surname>Izhikevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1569" to="1572" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">A generalized linear integrate-and-fire neural model produces diverse spiking behaviors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mihalas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="704" to="718" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Compact silicon neuron circuit with spiking and bursting behaviour</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wijekoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dudek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="524" to="534" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">A switched capacitor implementation of the generalized linear integrate-and-fire neuron</title>
		<author>
			<persName><forename type="first">F</forename><surname>Folowosele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Andreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Etienne-Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mihalas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<meeting>IEEE International Symposium on Circuits and Systems (ISCAS)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Spike-driven synaptic plasticity: theory, simulation, VLSI implementation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fusi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Annunziato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Badoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Amit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2227" to="2258" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Low-power silicon neurons, axons and synapses</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lazzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wawrzynek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Silicon Implementation of Pulse Coded Neural Networks</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="153" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Recurrently connected silicon neurons with active dendrites for one-shot learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Boahen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE International Joint Conference on Neural Networks</title>
		<meeting>the 2004 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1699" to="1704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Modeling selective attention using a neuromorphic analog VLSI device</title>
		<author>
			<persName><forename type="first">G</forename><surname>Indiveri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2857" to="2880" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">A recurrent model of orientation maps with simple and complex cells</title>
		<author>
			<persName><forename type="first">P</forename><surname>Merolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Boahen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="995" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Kinetic models of synaptic transmission</title>
		<author>
			<persName><forename type="first">A</forename><surname>Destexhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Mainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Methods in Neuronal Modeling</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="1" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Neuronal ion-channel dynamics in silicon</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hynna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Boahen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<meeting>the IEEE International Symposium on Circuits and Systems (ISCAS)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="3614" to="3617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Learning in silicon: timing is everything</title>
		<author>
			<persName><forename type="first">J</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Boahen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">75</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<monogr>
		<title level="m" type="main">MD1220 neural bit slice, data sheet, lake Mary</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Corp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990-03">March 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Lneuro 1.0: a piece of hardware LEGO for building neural network systems</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mauduit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Duration</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="414" to="422" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<monogr>
		<title level="m" type="main">NLX420 data sheet, Neurologix, Inc., 800 Charcot Av</title>
		<author>
			<persName><surname>Neurologix</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">112</biblScope>
			<pubPlace>San Jose, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Extensible linear floating point SIMD neurocomputer array processor</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Means</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lisenbee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Neural Networks</title>
		<meeting>International Joint Conference on Neural Networks<address><addrLine>Seattle, Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="587" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Very long instruction word architectures and the ELI-512</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Annual International Symposium on Computer Architecture</title>
		<meeting>the 10th Annual International Symposium on Computer Architecture<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1983">1983</date>
			<biblScope unit="page" from="140" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">MA16-programmable VLSI array processor for neuronal networks and matrix-based signal processing, user description</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bruels</surname></persName>
		</author>
		<idno>1.3</idno>
	</analytic>
	<monogr>
		<title level="m">Siemens AG, Corporate Research and Development Division</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-10">October 1993</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">A systolic array exploiting the inherent parallelisms of artificial neural networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Maeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Microprogramming and Microprocessors</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="159" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Hayes-Gill</surname></persName>
		</author>
		<title level="m">Two-ring systolic array network for artificial neural networks, in: IEE Proceedings of Circuits, Devices, and Systems</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="225" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">The Mod 2 neurocomputer system design</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mumford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Andes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Center</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="423" to="433" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Programmable analogue VLSI for radial basis function networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Churcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Reekie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Letters</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1603" to="1605" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Analog hardware model for morphological neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ocasio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IASTED International Conference on Neural Networks and Computational Intelligence</title>
		<meeting>the IASTED International Conference on Neural Networks and Computational Intelligence<address><addrLine>Anaheim, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACTA Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="40" to="044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">ANNSyS: an analog neural network synthesis system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bayraktaroglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Ogrenci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Balkr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Alpaydin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="325" to="338" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<monogr>
		<title level="m" type="main">A high-speed analog neural processor</title>
		<author>
			<persName><forename type="first">P</forename><surname>Masa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hoen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallinga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>IEEE Micro</publisher>
			<biblScope unit="page" from="40" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">A functional microcircuit for cat visual cortex</title>
		<author>
			<persName><forename type="first">R</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physiology</title>
		<imprint>
			<biblScope unit="volume">440</biblScope>
			<biblScope unit="page" from="735" to="769" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Polyneuronal innervation of spiny stellate neurons in cat visual cortex</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comparative Neurology</title>
		<imprint>
			<biblScope unit="volume">341</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="49" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Hybrid analog-digital architectures for neuromorphic systems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Mahowald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A C</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="1848" to="1853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">A learning rule of neural networks via simultaneous perturbation and its hardware implementation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hirano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kanata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="259" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Learning rules for neuro-controller via simultaneous perturbation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J P</forename><surname>De Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1119" to="1130" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Global competition and local cooperation in a network of neural oscillators</title>
		<author>
			<persName><forename type="first">D</forename><surname>Terman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="148" to="176" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<monogr>
		<title level="m" type="main">Virtex-ii pro and virtex-ii pro x platform fpgas: complete data sheet</title>
		<author>
			<persName><surname>Xilinx</surname></persName>
		</author>
		<ptr target="http://www.xilinx.com/support/documentation/data_sheets/ds083.pdfS" />
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<monogr>
		<author>
			<persName><surname>Stratix</surname></persName>
		</author>
		<ptr target="http://www.altera.com/products/devices/stratix3/overview/st3-overview.html#notesS" />
		<title level="m">Stratix III FPGA device family overview</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Feedforward neural network implementation in FPGA using layer multiplexing for effective resource utilization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Himavathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anitha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Muthuramalingam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="880" to="888" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Vas</surname></persName>
		</author>
		<title level="m">Sensorless Vector and Direct Torque Control</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Scaling analysis of a neocortex inspired cognitive model on the Cray XD1</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vutsinas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Supercomputing</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="43" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">A hierarchical bayesian model of invariant pattern recognition in the visual cortex</title>
		<author>
			<persName><forename type="first">D</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hawkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Joint Conference on Neural Networks</title>
		<meeting>the IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1812" to="1817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Comparative investigation into classical and spiking neuron implementations on FPGAs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Maguire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mcginnity</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICANN</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="269" to="274" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Stochastic computing systems</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Gaines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Information Systems Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="37" to="172" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">A full-parallel digital implementation for pre-trained NNs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Szabo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Horvath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Feher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE-INNS-ENNS International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">2049</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Palm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kurfess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schwenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Strey</surname></persName>
		</author>
		<title level="m">Neural associative memories, in: Associative Processing and Processors</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="284" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Non-holographic associative memory</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Willshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">P</forename><surname>Buneman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Longuet-Higgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">222</biblScope>
			<biblScope unit="page" from="960" to="962" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Hassoun</surname></persName>
		</author>
		<title level="m">Associative Neural Memories</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press, Inc</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Pattern recognition and reading by machine</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bledsoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Browning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eastern Joint Computer Conference</title>
		<meeting>Eastern Joint Computer Conference<address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1959">1959</date>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="225" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Weightless neural models: a review of current and past works</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Ludermiry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Carvalhoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Bragax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C P</forename><surname>De Souto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="41" to="61" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Learning probabilistic RAM nets using VLSI structures</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Clarkson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gorse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1552" to="1561" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Cellular neural networks: theory</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1257" to="1272" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Cellular neural networks: applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1273" to="1290" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">The CNN paradigm</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Roska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="147" to="156" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Nagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Szolgay</surname></persName>
		</author>
		<title level="m">Configurable multilayer CNN-UM emulator on FPGA, IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="774" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Nagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Voroshazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Szolgay</surname></persName>
		</author>
		<title level="m">Ninth International Workshop on Cellular Neural Networks and Their Applications</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="278" to="281" />
		</imprint>
	</monogr>
	<note>An emulated digital retina model implementation on FPGA</note>
</biblStruct>

<biblStruct xml:id="b186">
	<monogr>
		<title level="m" type="main">Cellular neural networks, The Circuits and Filters Handbook</title>
		<author>
			<persName><forename type="first">T</forename><surname>Roska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>CRC Press</publisher>
			<biblScope unit="page" from="1075" to="1092" />
			<pubPlace>Boca Raton, FL</pubPlace>
		</imprint>
	</monogr>
	<note>second ed.. Chapter 39</note>
</biblStruct>

<biblStruct xml:id="b187">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Roska</surname></persName>
		</author>
		<title level="m">Cellular Neural Networks and Visual Computing: Foundation and Applications</title>
		<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Roska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rodriguez-Vazquez</surname></persName>
		</author>
		<title level="m">Towards the Visual Microprocessor: VLSI Design and the Use of Cellular Neural Network Universal Machines</title>
		<meeting><address><addrLine>Chichester, England</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons Ltd</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">The CNN universal machine: an analogic array computer</title>
		<author>
			<persName><forename type="first">T</forename><surname>Roska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="163" to="173" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">ACE4k: an analog I/O 64 Â 64 visual microprocessor chip with 7-bit analog accuracy</title>
		<author>
			<persName><forename type="first">G</forename><surname>Linan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Espejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dominguez-Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rodriguez-Vazquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Circuit Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="89" to="116" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Yalcin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE International Symposium on Circuits and Systems</title>
		<meeting>the IEEE International Symposium on Circuits and Systems</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="5814" to="5817" />
		</imprint>
	</monogr>
	<note>Spatiotemporal pattern formation in the ACE16k CNN chip</note>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">ACE16k based stand-alone system for real-time preprocessing tasks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Carranza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jimenez-Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Linan-Cembrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Roca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Meana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rodriguez-Vazquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proceedings of SPIE</title>
		<imprint>
			<biblScope unit="volume">5837</biblScope>
			<biblScope unit="page">872</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Various implementations of topographic, sensory, cellular wave computers</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">´</forename><surname>Zara ´ndy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Öldesy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Szolgay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rekeczky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Roska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Circuits and Systems-ISCAS</title>
		<meeting><address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="5802" to="5805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Implementation of embedded emulated-digital CNN-UM global analogic programming unit on FPGA and its application</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Voroshazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Nagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Szolgay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Circuit Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="589" to="603" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Nagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Szolgay</surname></persName>
		</author>
		<title level="m">Configurable multilayer CNN-UM emulator on FPGA, IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="774" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Parallelization of cellular neural networks on GPU</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2684" to="2692" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<monogr>
		<title level="m" type="main">The Art of Multiprocessor Programming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Herlihy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shavit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<monogr>
		<title level="m" type="main">Synaptic touch pad</title>
		<author>
			<persName><surname>Synaptics</surname></persName>
		</author>
		<ptr target="http://www.synaptics.com/products/touchpad.cfmS" />
		<imprint>
			<date type="published" when="2009-11-22">November 22. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<monogr>
		<title level="m" type="main">An Analog VLSI System for Stereoscopic Vision</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mahowald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Norwell, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">A communication architecture tailored for analog VLSI artificial neural networks: intrinsic performance and limitations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mortara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Vittoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="459" to="466" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">A burst-mode word-serial address-event channel-i: transmitter design</title>
		<author>
			<persName><forename type="first">K</forename><surname>Boahen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems I</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1269" to="1280" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">AER building blocks for multi-layer multi-chip neuromorphic vision systems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Gotarredona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lichtsteiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Barranco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Rodraguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Riis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Delbrck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Sch Ölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1217" to="1224" />
			<date type="published" when="2006">2006</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">Large developing axonal arbors using a distributed and locally-reprogrammable address-event receiver</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Willshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Joint Conference on Neural Networks</title>
		<meeting>the IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1464" to="1471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">A neuromorphic VLSI device for implementing 2-D selective attention systems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Indiveri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1455" to="1463" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">An arbitrary kernel convolution AER-transceiver chip for real-time image filtering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Serrano-Gotarredona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serrano-Gotarredona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Acosta-Jime ´nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Linares-Barranco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<meeting>the IEEE International Symposium on Circuits and Systems (ISCAS)<address><addrLine>Kos, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="3145" to="3148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">An electronic model of the retina</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yasuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1950" to="1951" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">An on/off transient imager with event-driven, asynchronous readout</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Circuits and Systems</title>
		<meeting><address><addrLine>Phoenix, AZ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="165" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">An 128 Â 128 120 dB 15us-latency temporal contrast vision sensor</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lichtsteiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid State Circuits</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="566" to="576" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">Optic nerve signals in a neuromorphic chip I: outer and inner retina models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zaghloul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Boahen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="657" to="666" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">Optic nerve signals in a neuromorphic chip II: testing and results</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zaghloul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Boahen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="667" to="675" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">A silicon retina that reproduces signals in the optic nerve</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zaghloul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Boahen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neural Engineering</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="257" to="267" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main">Neuromorphic microchips</title>
		<author>
			<persName><forename type="first">K</forename><surname>Boahen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Special Editions</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="20" to="27" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">An analog electronic cochlea</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Lyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1119" to="1133" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">A silicon model of auditory localization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lazzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="41" to="70" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">A low-power wide-dynamic-range analog VLSI cochlea</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sarpeshkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analog Integrated Circuits and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="274" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">AER EAR: a matched silicon cochlea pair with address event representation interface</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Schaik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems I</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="59" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Lande</surname></persName>
		</author>
		<title level="m">Neuromorphic Systems Engineering: Neural Networks in Silicon</title>
		<meeting><address><addrLine>Norwell, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">Noisy spiking neurons with temporal coding have more computational power than sigmoidal neurons</title>
		<author>
			<persName><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Mozer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Petsche</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="211" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b219">
	<analytic>
		<title level="a" type="main">Simulation of spiking neural networks on different hardware platforms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jahnke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schoenauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mohraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Klar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICANN</title>
		<meeting>ICANN</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="1187" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">Real-time computing platform for spiking neurons (RT-spike)</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ortigosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Agis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Carrillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arnold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1050</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title level="a" type="main">Real-time simulations of networks of Hodgkin-Huxley neurons using analog circuits</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bornat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Renaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Destexhe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">10-12</biblScope>
			<biblScope unit="page" from="1137" to="1140" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<analytic>
		<title level="a" type="main">Dynamically reconfigurable silicon array of spiking neurons with conductance-based synapses</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vogelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Mallik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vogelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cauwenberghs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">253</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b223">
	<analytic>
		<title level="a" type="main">Interpreting neuronal population activity by reconstruction: unified framework with application to hippocampal place cells</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ginzburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcnaughton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1017</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b224">
	<analytic>
		<title level="a" type="main">A programmable spike-timing based circuit block for reconfigurable neuromorphic computing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Koickal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gouveia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">16-18</biblScope>
			<biblScope unit="page" from="3609" to="3616" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main">A programmable time event circuit block for reconfigurable neuromorphic computing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Koickal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gouveia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">4507</biblScope>
			<biblScope unit="page" from="430" to="437" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main">A spike based learning neuron in analog VLSI</title>
		<author>
			<persName><forename type="first">P</forename><surname>Äfliger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahowald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Watts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="692" to="698" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main">An adaptive silicon synapse</title>
		<author>
			<persName><forename type="first">E</forename><surname>Chicca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Indiveri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Douglas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<meeting>the IEEE International Symposium on Circuits and Systems (ISCAS)<address><addrLine>V, Bangkok</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="81" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main">A VLSI array of low-power spiking neurons and bistable synapses with spike-timing dependent plasticity</title>
		<author>
			<persName><forename type="first">G</forename><surname>Indiveri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chicca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Douglas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="211" to="221" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b229">
	<analytic>
		<title level="a" type="main">Adaptive WTA with an analog VLSI neuromorphic learning chip</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Äfliger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="551" to="572" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b230">
	<analytic>
		<title level="a" type="main">On the design of a low power compact spiking neuron cell based on charge coupled synapses</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mcdaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Buiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Joint Conference on Neural Networks (IJCNN)</title>
		<meeting>the IEEE International Joint Conference on Neural Networks (IJCNN)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1511" to="1517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b231">
	<analytic>
		<title level="a" type="main">A silicon synapse based on a charge transfer device for spiking neural network applications</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mcdaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Buiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Symposium on Neural Networks (ISNN)</title>
		<meeting>the Third International Symposium on Neural Networks (ISNN)<address><addrLine>Chengdu, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1366" to="1373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b232">
	<analytic>
		<title level="a" type="main">Area efficient architecture for large scale implementation of biologically plausible spiking neural networks on reconfigurable hardware</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mcginnity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maguire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Field Programmable Logic and Applications</title>
		<meeting>the International Conference on Field Programmable Logic and Applications<address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b233">
	<analytic>
		<title level="a" type="main">Optical computing</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Caulfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Encyclopedia of Optical Engineering</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Driggers</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1613" to="1620" />
			<date type="published" when="2003">2003</date>
			<publisher>CRC Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b234">
	<analytic>
		<title level="a" type="main">Optical implementations of associative networks with versatile adaptive learning capabilities</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Lippincott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Optics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="5039" to="5052" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b235">
	<analytic>
		<title level="a" type="main">Optoelectronic analogs of self-programming neural nets: architectures and methodologies for implementing fast stochastic learning by simulated annealing</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Farhat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Optics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="5093" to="5103" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b236">
	<analytic>
		<title level="a" type="main">Optoelectronic neural networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E X</forename><surname>Silveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Encyclopedia of Optical Engineering</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Driggers</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1887" to="1902" />
			<date type="published" when="2003">2003</date>
			<publisher>CRC Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b237">
	<analytic>
		<title level="a" type="main">An optoelectronic neural network with temporally multiplexed grey-scale weights</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Underwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Vass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Microelectronics for Neural Networks and Fuzzy Systems</title>
		<meeting>the Fourth International Conference on Microelectronics for Neural Networks and Fuzzy Systems</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="3" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b238">
	<analytic>
		<title level="a" type="main">Reconfigurable optical neuron based on the transverse Pockels effect</title>
		<author>
			<persName><forename type="first">G</forename><surname>Moagar-Poladian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bulinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Optoelectronics and Advanced Materials</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="929" to="936" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b239">
	<analytic>
		<title level="a" type="main">Optical implementation of the Kak neural network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shortt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Keating</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Moulinier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Pannell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences-Informatics and Computer Science</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="273" to="287" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b240">
	<analytic>
		<title level="a" type="main">New algorithms for training feedforward neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="295" to="298" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b241">
	<analytic>
		<title level="a" type="main">A new corner classification approach to neural network training</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Circuits, Systems, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="459" to="469" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b242">
	<analytic>
		<title level="a" type="main">Optical realization of bio-inspired spiking neurons in the electron trapping material thin film</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pashaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Farhat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Optics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">35</biblScope>
			<biblScope unit="page" from="8411" to="8418" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b243">
	<analytic>
		<title level="a" type="main">Optical broadcast interconnection neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lamela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ruiz-Llata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Warde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="2487" to="2488" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b244">
	<analytic>
		<title level="a" type="main">Image identification system based on an optical broadcast neural network processor</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ruiz-Llata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lamela-Rivera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Optics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2366" to="2376" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b245">
	<analytic>
		<title level="a" type="main">Optical neural networks</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">T S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Uang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Encyclopedia of Optical Engineering</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Driggers</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1763" to="1777" />
			<date type="published" when="2003">2003</date>
			<publisher>CRC Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b246">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Ayoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Orzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">´</forename></persName>
		</author>
		<title level="m">Proceedings of IEEE International Workshop on Cellular Neural Networks and their Applications</title>
		<meeting>IEEE International Workshop on Cellular Neural Networks and their Applications<address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="64" to="69" />
		</imprint>
	</monogr>
	<note>Evolution of the programmable optical array computer (POAC)</note>
</biblStruct>

<biblStruct xml:id="b247">
	<analytic>
		<title level="a" type="main">Reconfigurable hardware evolution platform for a spiking neural network robotics controller</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rocke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcginley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Maher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">ARC of Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Diniz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Marques</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Bertels</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Fernandes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">M P</forename><surname>Cardoso</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">4419</biblScope>
			<biblScope unit="page" from="373" to="378" />
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin, Heidelberg, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b248">
	<monogr>
		<title level="m" type="main">The intelligent flight control: advanced concept program final report</title>
		<author>
			<persName><surname>Annon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>The Boeing Company</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b249">
	<analytic>
		<title level="a" type="main">Development of an embedded intelligent flight control system for the autonomously flying unmanned helicopter skyexplorer</title>
		<author>
			<persName><forename type="first">W</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huanye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tiansheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Embedded Systems-Modeling Technology and Applications</title>
		<meeting><address><addrLine>Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="121" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b250">
	<analytic>
		<title level="a" type="main">Neurogrid: emulating a million neurons in the cortex</title>
		<author>
			<persName><forename type="first">K</forename><surname>Boahen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Grand Challenges in Neural Computation</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">6702</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b251">
	<analytic>
		<title level="a" type="main">Towards cortex sized artificial neural systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lansner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="61" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b252">
	<monogr>
		<title level="m">Special Issue on Micro-Electronic Hardware Implementation of Soft Computing: Neural and Fuzzy Networks with Learning</title>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Salam</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Yamakawa</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b253">
	<analytic>
		<title level="a" type="main">Special issue on neural networks hardware implementation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Linares-Barranco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Andreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Indiveri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shibata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b254">
	<analytic>
	</analytic>
	<monogr>
		<title level="m">Special issue on hardware implementations of soft computing techniques</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Anguita</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Baturone</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Miller</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b255">
	<monogr>
		<title level="m" type="main">Implementing neural hardware with on chip training on field programmable gate arrays</title>
		<author>
			<persName><forename type="first">D</forename><surname>Braendler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<pubPlace>Melbourne, Australia</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Swinburne University of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b256">
	<monogr>
		<title level="m" type="main">Devices and circuits for nanoelectronic implementation of artificial neural networks</title>
		<author>
			<persName><forename type="first">O</forename><surname>Turel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<pubPlace>NY</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Stony Brook University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b257">
	<monogr>
		<title level="m" type="main">Networks of spiking neurons and plastic synapses: implementation and control</title>
		<author>
			<persName><forename type="first">M</forename><surname>Giulioni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Italy</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Universit</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
	<note>degli Studi di Roma &apos;La Sapienza</note>
</biblStruct>

<biblStruct xml:id="b258">
	<analytic>
		<title level="a" type="main">Activity report-neuromimetic intelligence</title>
		<author>
			<persName><forename type="first">B</forename><surname>Girau</surname></persName>
		</author>
		<ptr target="http://www.inria.fr/rapportsactivite/RA2006/cortex/cortex.pdfS" />
	</analytic>
	<monogr>
		<title level="m">Project Team CORTEX</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>INRIA</note>
</biblStruct>

<biblStruct xml:id="b259">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Schoenauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jahnke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Klar</surname></persName>
		</author>
		<title level="m">Digital neurohardware: principles and perspectives</title>
		<meeting><address><addrLine>Magdeburg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="101" to="106" />
		</imprint>
	</monogr>
	<note>Proceedings of Neuronal Networks in Applications</note>
</biblStruct>

<biblStruct xml:id="b260">
	<analytic>
		<title level="a" type="main">Brain-scale simulation of the neocortex on the IBM Blue Gene/L supercomputer</title>
		<author>
			<persName><forename type="first">M</forename><surname>Djurfeldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lundqvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ekeberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lansner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="41" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b261">
	<monogr>
		<title level="m" type="main">Systems of neuromorphic adaptive plastic scalable electronics</title>
		<author>
			<persName><surname>Darpa</surname></persName>
		</author>
		<ptr target="http://www.darpa.mil/dso/solicitations/BAA08-28.pdfS" />
		<imprint>
			<date type="published" when="2010-01-20">January 20. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b262">
	<analytic>
		<title level="a" type="main">Realizing biological spiking network models in a configurable wafer-scale hardware system</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fieres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schemmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Meier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Joint Conference on Neural Networks (IJCNN)</title>
		<meeting>IEEE International Joint Conference on Neural Networks (IJCNN)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="969" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b263">
	<analytic>
		<title level="a" type="main">Wafer-scale integration of analog neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schemmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fieres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Meier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="431" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b264">
	<analytic>
		<title level="a" type="main">Introduction to the cell multiprocessor</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hofstee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maeurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shippy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">4/5</biblScope>
			<biblScope unit="page">589</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b265">
	<monogr>
		<author>
			<persName><surname>Ibm</surname></persName>
		</author>
		<ptr target="http://www-03.ibm.com/technology/cell/S" />
		<title level="m">IBM cell broadband engine technology</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b266">
	<analytic>
		<title level="a" type="main">A review of 3-D packaging technology</title>
		<author>
			<persName><forename type="first">S</forename><surname>Al-Sarawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Franzon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Components, Packaging, and Manufacturing Technology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="14" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note>Part B: Advanced Packaging</note>
</biblStruct>

<biblStruct xml:id="b267">
	<monogr>
		<title level="m" type="main">CMOL: Devices Circuits and Architectures</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Likharev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Strukov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="447" to="477" />
			<pubPlace>Berlin, Heidelberg, Germany</pubPlace>
		</imprint>
	</monogr>
	<note>Chapter 16</note>
</biblStruct>

<biblStruct xml:id="b268">
	<analytic>
		<title level="a" type="main">CMOL: second life for silicon</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Likharev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Microelectronics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="183" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b269">
	<analytic>
		<title level="a" type="main">Architectures for nanoelectronic implementation of artificial neural networks: new results</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">¨</forename><surname>Türel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Likharev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="271" to="283" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b270">
	<analytic>
		<title level="a" type="main">CMOL crossnets as pattern classifiers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Likharev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eighth International Work-Conference on Artificial Neural Networks: Computational Intelligence and Bioinspired Systems</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>Eighth International Work-Conference on Artificial Neural Networks: Computational Intelligence and Bioinspired Systems<address><addrLine>Berlin, Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3512</biblScope>
			<biblScope unit="page" from="446" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b271">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Likharev</surname></persName>
		</author>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks</title>
		<meeting>the International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="5026" to="5034" />
		</imprint>
	</monogr>
	<note>situ training of CMOL crossnets</note>
</biblStruct>

<biblStruct xml:id="b272">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Likharev</surname></persName>
		</author>
		<title level="m">CMOL crossnets: possible neuromorphic nanoelectronic circuits</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Sch Ölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="755" to="762" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b273">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<title level="m">CMOL crossnets ad defect-tolerant classifiers</title>
		<meeting><address><addrLine>NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>Stony Brook University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b274">
	<analytic>
		<title level="a" type="main">The lure of molecular computing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Conrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Spectrum</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="55" to="60" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b275">
	<analytic>
		<title level="a" type="main">Molecular computing with artificial neurons, Communications of the Korea Information</title>
		<author>
			<persName><forename type="first">M</forename><surname>Conrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-P</forename><surname>Zauner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Society</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="78" to="89" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b276">
	<analytic>
		<title level="a" type="main">Elements of a unique bacteriorhodopsin neural network architecture</title>
		<author>
			<persName><forename type="first">D</forename><surname>Haronian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Neural Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="191" to="197" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b277">
	<analytic>
		<title level="a" type="main">On the utility of entanglement in quantum neural computing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ventura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks</title>
		<meeting>the International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1565" to="1570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b278">
	<analytic>
		<title level="a" type="main">An organic nanoparticle transistor behaving as a biological spiking synapse</title>
		<author>
			<persName><forename type="first">F</forename><surname>Alibart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pleutin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gue ´rin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Novembre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lenfant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lmimouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gamrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vuillaume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advanced Functional Materials</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="330" to="337" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b279">
	<monogr>
		<title level="m">Lucknow, India in 1999 and M.Tech. degree in Computer Science from Indian Statistical Institute</title>
		<meeting><address><addrLine>Kolkata, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001 to 2002</date>
		</imprint>
		<respStmt>
			<orgName>Janardan Misra received his B.Tech. degree in Computer Science and Engineering from Institute of Engineering and Technology</orgName>
		</respStmt>
	</monogr>
	<note>he was associated with the Texas Instruments India. Pvt.</note>
</biblStruct>

<biblStruct xml:id="b280">
	<analytic>
		<title level="a" type="main">After a brief teaching tenure between 2005 and 2006, he has been associated with Research lab at HTS Research, Bangalore, India. His research interests include ANN hardware, machine learning, artificial life, enterprise security, and formal methods</title>
		<author>
			<persName><forename type="first">India</forename><forename type="middle">;</forename><surname>Ltd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">India in 2003 and M.Tech. degree in Computer Science from Indian Statistical Institute, Kolkata, India in 2005</title>
		<meeting><address><addrLine>Kalyani; Bangalore, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005 to 2008</date>
		</imprint>
		<respStmt>
			<orgName>Computer Science from National University of Singapore ; Computer Science Department of University of California, Los Angeles</orgName>
		</respStmt>
	</monogr>
	<note>Presently, he is a graduate student at. His research interests include hardware neural networks, formal methods, and embedded software reliability</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
