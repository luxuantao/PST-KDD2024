<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On Missing Labels, Long-tails and Propensities in Extreme Multi-label Classification</title>
				<funder ref="#_SjJjN4u">
					<orgName type="full">Academy of Finland</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-07-26">26 Jul 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Erik</forename><surname>Schultheis</surname></persName>
							<email>erik.schultheis@aalto.fi</email>
						</author>
						<author>
							<persName><forename type="first">Marek</forename><surname>Wydmuch</surname></persName>
							<email>mwydmuch@cs.put.poznan.pl</email>
						</author>
						<author>
							<persName><forename type="first">Rohit</forename><surname>Babbar</surname></persName>
							<email>rohit.babbar@aalto.fi</email>
						</author>
						<author>
							<persName><forename type="first">Krzysztof</forename><surname>Dembczy?ski</surname></persName>
							<email>kdembczynski@cs.put.poznan.pl</email>
							<affiliation key="aff4">
								<orgName type="department">Also with</orgName>
								<orgName type="institution">Poznan University of Technology</orgName>
								<address>
									<addrLine>KDD &apos;22, August 14-18</addrLine>
									<postCode>2022</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Aalto University Helsinki</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Poznan University of Technology</orgName>
								<address>
									<settlement>Poznan</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Aalto University Helsinki</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<settlement>Yahoo! Research New York</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">KDD &apos;22</orgName>
								<address>
									<addrLine>August 14-18</addrLine>
									<postCode>2022</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On Missing Labels, Long-tails and Propensities in Extreme Multi-label Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-07-26">26 Jul 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3534678.3539466</idno>
					<idno type="arXiv">arXiv:2207.13186v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>extreme classification</term>
					<term>multi-label classification</term>
					<term>propensity model</term>
					<term>missing labels</term>
					<term>long-tail labels</term>
					<term>recommendation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The propensity model introduced by Jain et al. <ref type="bibr" target="#b17">[18]</ref> has become a standard approach for dealing with missing and long-tail labels in extreme multi-label classification (XMLC). In this paper, we critically revise this approach showing that despite its theoretical soundness, its application in contemporary XMLC works is debatable. We exhaustively discuss the flaws of the propensity-based approach, and present several recipes, some of them related to solutions used in search engines and recommender systems, that we believe constitute promising alternatives to be followed in XMLC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Computing methodologies ? Supervised learning by classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Extreme multi-label classification (XMLC) is a supervised learning problem where only a few labels from an enormous label space, reaching orders of millions, are relevant per data point. Notable examples are tagging of text documents <ref type="bibr" target="#b0">[1]</ref>, content annotation for multimedia search <ref type="bibr" target="#b12">[13]</ref>, and diverse types of recommendation, including webpages-to-ads <ref type="bibr" target="#b5">[6]</ref>, ads-to-bid-words <ref type="bibr" target="#b24">[25]</ref>, users-toitems <ref type="bibr" target="#b41">[42]</ref>, queries-to-items <ref type="bibr" target="#b21">[22]</ref>, or items-to-queries <ref type="bibr" target="#b9">[10]</ref>. These practical applications pose statistical challenges, including: 1) longtailed distribution of labels-infrequent (tail) labels are much harder to predict than frequent (head) labels due to data imbalance, and a model completely ignoring the tail labels can get very high scores on standard performance metrics; 2) missing relevant labels in the observed training data-since it is nearly impossible to check the whole set of labels when it is so large.</p><p>To address the latter issue, propensity-scored versions of popular measures (i.e., precision@? and nDCG@?) were introduced by Jain et al. <ref type="bibr" target="#b17">[18]</ref>. Under the propensity model, it is assumed that an assignment of a label to an example is always correct, but the supervision may skip some positive labels, and propensity of a label refers to the probability of not skipping that label. Under the implicit assumption that the chance for a label to be missing is higher for tail than for head labels, the propensity-scored measures were used to evaluate the prediction performance on tail labels. Despite being originally introduced to study the phenomenon of missing labels in XMLC, over the years, they have found their way into the literature as default performance metrics on tail labels <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>In this work, we take a step back and thoroughly investigate the validity of the propensity model of Jain et al. <ref type="bibr" target="#b17">[18]</ref>, further referred to as JPV (from the first letters of authors' names), for the dual usage of missing and long-tail labels in XMLC. We start our discussion by recalling the definition of the XMLC problem, stating the problem of missing labels, and bringing closer the issues with long-tail labels (Section 2). We recall the JPV propensity model and highlight its shortcomings in Section 3, both in terms of the model itself and in regard to its current usage in XMLC. In particular, we demonstrate that this model: (i) does not fulfill natural conditions that may be desired of a reasonable propensity model, (ii) falls short on reliable and reproducible estimation of the model hyper-parameters, and (iii) leads to implausible results exceeding substantially the natural range of the metrics (e.g., precision@k &gt; 300%).</p><p>After formally studying the above short-comings of the JPV model, we propose a suite of alternatives (c.f. Section 4) which are promising to follow for a more principled approach in (i) evaluating machine learning systems trained on data with incomplete user feedback, and (ii) disentangling the individual contribution of missing and tail labels in XMLC. We suggest using unbiased sets for validating models designed to deal with missing labels. Alternatively, one should use a set with a controlled bias, on which one can obtain unbiased estimates of performance metrics. Thereafter, we discuss alternative propensity models, which possess desirable analytical properties, and compare them with the JPV model empirically, confirming its shortcomings. We also show the efficacy of a framework in which label propensities and parameters of the learning model are learned jointly. Towards disambiguating the phenomenon of missing and long-tail labels in XMLC, we finally highlight other metrics as possible options for measuring tail-label performance instead of conflating these with missing labels.</p><p>It should be noted that, unlike most contemporary advances in XMLC, our goal in this work is not algorithmic. Instead, we take a critical viewpoint and study the commonly-used propensity model, explicating the consequences when it is used in real-world production environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM STATEMENT</head><p>We first define the problem of XMLC, then the problem of missing labels, and finally the problem of long-tail labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Extreme multi-label classification</head><p>The goal of XMLC is to find a mapping between instances ? ? X and a finite set of ? non-mutually-exclusive class labels. <ref type="foot" target="#foot_0">1</ref> This means that any specific realization ? of ? is associated with a (possibly empty) subset L (?) ? [?] of the labels called the relevant or positive labels, with the complement, [?] \ L (?), of the irrelevant or negative ones. We identify the relevant labels with a binary vector ? ? Y through <ref type="foot" target="#foot_1">2</ref> where Y {0, 1} ? is called the label vector space. In the classical setting, we assume that observations (?, ? ) are generated independently and identically according to a probability distribution P on X ?Y. In case of XMLC we assume ? to be a large number (e.g., ? 10 5 ), and ?? ? 1 to be much smaller than ?, ?? ? 1 ? ?.</p><formula xml:id="formula_0">? ? = 1[ ? ? L (?)],</formula><p>The problem of XMLC can be defined as finding a classifier ? : X -? R ? which minimizes the task risk:</p><formula xml:id="formula_1">Risk ? task [?; ?, ? ] E[? task (? , ?(? ))] ,<label>(1)</label></formula><p>where ? task : Y ?R ? -? R ?0 is the (task) loss. The optimal (Bayes) classifier for a loss ? task is given by</p><formula xml:id="formula_2">? * (?) = arg min ? ?R ? E[? task (? , ?) | ? = ?] .<label>(2)</label></formula><p>The above definitions follow the standard statistical learning framework. Let us notice, however, that in XMLC, instead of loss functions, one often uses performance metrics, which are rather maximized than minimized. Moreover, these definitions correspond to the most natural setting in which a decision is made based on a single ?. Later in the paper, we also consider more general metrics that cannot be optimized with respect to individual instances. Typically, a task loss is hard to optimize and one chooses instead a surrogate loss that is easier to cope with, e.g., because it is differentiable and convex. Furthermore, instead of a probability distribution, a learning algorithm operates on a finite i.i.d. sample and minimizes the corresponding empirical risk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Missing labels</head><p>In XMLC, the observed data might not follow the distribution we want to learn about. As an illustrative example, take the Wikipedia-500k dataset. The content of a Wikipedia article should be matched with a set of categories the article belongs to. Such a dataset can be easily created by scraping existing Wikipedia annotations. However, there are about 500 000 categories on Wikipedia, and it is clear that the original authors and curators have never checked every single category for each article. <ref type="foot" target="#foot_2">3</ref> On the other hand, each category that has been assigned to an article has been verified by a human to be relevant. Therefore, the labeling error can be assumed to be strongly one-sided: There may be many missing labels, but spurious labels should be uncommon.</p><p>To contrast ground-truth labels ? with those actually available, we denote the observed labels ? using a tilde. <ref type="foot" target="#foot_3">4</ref> Mathematically, the setting studied in this paper is defined by</p><formula xml:id="formula_3">P ? ? ? | ? = 1 , P ? ?? ? | ? = 0 ,<label>(3)</label></formula><p>where ? ? ? means that ?? ? ? ? for all ? ? [?], and ? ?? ? means that there is at least one label for which ?? &gt; ? ? . Notice that the above equations cover also the no noise case, as we may have</p><formula xml:id="formula_4">P ? = ? | ? = 1.</formula><p>Reconstruction of the ground truth distribution from the observed one, in the general case, is not a trivial task from the statistical and computational perspective, as it requires an exponential number of parameters. Let ? ? (?)</p><formula xml:id="formula_5">P[? = ? | ? = ?] and ?? (?) P ? = ? | ? = ? . We have then ? ? (?) = ?? ? ? ? (?, ?)? ? (?) ,<label>(4)</label></formula><p>where ? ? (?, ?) P ? = ? | ? = ?, ? = ? is a propensity of observing ? for ground-truth labels ? and instance ?. Notice that from (3) we have ? ? (?, ?) = 0 for ? ?? ?. Furthermore, let ? Y (?) and ?Y (?) be vectors of ? ? (?) and ?? (?), respectively, for all ? ? Y given in some predefined order ?. Let C be a matrix containing all propensities ? ? (?, ?), with rows and columns corresponding to ? and ? , respectively, and organized according to ?. Then, we get:</p><formula xml:id="formula_6">?Y (?) = C? Y (?) ,<label>(5)</label></formula><p>and, finally:</p><formula xml:id="formula_7">? Y (?) = C -1 ?Y (?) ,<label>(6)</label></formula><p>where we need to assume that C is invertible.</p><p>Because of the practical reasons, a much simpler, label-wise, propensities are commonly used that are defined for each label separately:</p><formula xml:id="formula_8">? ? (? ) P ?? = 1 | ? ? = 1, ? .<label>(7)</label></formula><p>Let ?? (?) P ?? = 1 | ? = ? and ? ? (?) P ? ? = 1 | ? = ? . We have then:</p><formula xml:id="formula_9">?? (?) = ? ? (?)? ? (?) , ? ? (?) = ?? (?)/? ? (?) .<label>(8)</label></formula><p>If propensities are known, then they can be used to construct an unbiased, task or surrogate, loss l <ref type="bibr" target="#b34">[35]</ref> in the sense that The construction of the unbiased counterpart depends on the form of propensities, e.g., the label-wise propensities <ref type="bibr" target="#b6">(7)</ref> are sufficient for losses decomposable over labels <ref type="bibr" target="#b23">[24]</ref> like Hamming loss or binary cross-entropy, but might not be for more complex losses without additional assumptions <ref type="bibr" target="#b29">[30]</ref>. The unbiased losses can be used in training procedures <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26]</ref> or for estimating the performance of classifiers. For some losses, such as Hamming loss or precision@?, the Bayes classifier can be written as a function of the conditional label distributions ? ? (?). In this case, one can adjust existing inference procedures to use <ref type="bibr" target="#b7">(8)</ref> to obtain estimates of ? ? (?) from estimates of ?? (?) <ref type="bibr" target="#b36">[37]</ref>.</p><formula xml:id="formula_10">?? : Risk ? [?; ?, ? ] = Risk l [?; ?, ? ].<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Long-tailed label distribution</head><p>A defining characteristic of extreme classification data is that the label distribution is highly imbalanced. In the binary case, the amount of imbalance is completely determined by the imbalance ratio P[? =0] P[? =1] . In this sense, almost every binary problem corresponding to a label is highly imbalanced in XMLC, i.e., only a small fraction of training instances will be associated with that label. However, in XMLC, the data are also imbalanced when comparing different labels. In analogy to the binary case, we can define an inter-label imbalance ratio through ILIR = <ref type="foot" target="#foot_4">5</ref>Nevertheless, the imbalance factor does not cover an important property of the label distribution. It could be that most labels have a large number of positives, but some have very few, or vice-versa. The latter case is what happens in XMLC, where the label distribution is said to be long-tailed <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref>.</p><formula xml:id="formula_11">max{P[? ? =1]:? ? [?] } min{P[? ? =1]:? ? [?] } .</formula><p>In Table <ref type="table" target="#tab_0">1</ref>, these imbalance measures are shown for several XMLC datasets. We use min IR to denote the binary imbalance ratio of the head label, i.e., the label with the largest fraction of positive instances (therefore, its IR is the smallest), and Pos-80% to indicate the minimum fraction of class labels that retain 80% of positive labels (i.e., ? ? = 1) in the training set. For example, in Delicious-200K, only four percent of the class labels contain 80% of the positive labels.</p><p>In addition to the number of positive instances of a sparse label, their distribution within the feature space can be very important. If ? ? has few positives, but in a small pocket X + of the feature space it still fulfills P ? ? = 1 | ? ? X + ? 1, then a learning algorithm might still learn a reasonable decision boundary, especially if the overall number of samples is large enough <ref type="bibr">[15, p. 23]</ref>. In contrast, if</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CRITICAL VIEW ON THE CURRENT APPROACH TO SPARSE LABELS IN XMLC</head><p>In this section we present an overview on the current state of addressing the long-tail and missing-labels problems in XMLC. This is in large parts based on the work of Jain et al. <ref type="bibr" target="#b17">[18]</ref>, so we start by a recap of their findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Current approach to missing labels and long tails</head><p>The goal of Jain et al. <ref type="bibr" target="#b17">[18]</ref> was to develop loss functions for XMLC that (a) prioritize predicting the few relevant labels over the large number of irrelevant ones; (b) do not erroneously treat missing labels as irrelevant [...] (c) promote the accurate prediction of infrequently occurring, hard to predict, but rewarding tail labels.</p><p>There are two main contributions of the paper that are relevant for our discussion: First, the development of unbiased loss functions that allow compensating for missing labels if their propensities are known, and second, an empirical model to estimate these propensities on XMLC data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Propensity-scored losses.</head><p>Popular XMLC performance metrics focus on the highest scored labels by the prediction algorithm. Examples of such metrics are precision at k (P@?), recall at k (R@?), or (normalized) discounted cumulative gain nDCG@?. For these metrics, unbiased estimates in the sense of (9) can be calculated, which are called the propensity-scored (PS) variants of these metrics (more examples in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr">Table 1]</ref>). Table <ref type="table">2</ref> gives the formal definitions, where top ? maps a vector to the indices of its top-k components, r ? ( ?) gives the ranking of the ?-th element in the vector, and ? ? is the propensity for label ?. Let us notice that, in general, ? ? shall depend on ?, but Jain et al. <ref type="bibr" target="#b17">[18]</ref> practically assume ? ? to be a constant value for each label ?. Moreover, of the above unbiased estimates, only PSP and PSnDCG have found widespread use <ref type="bibr" target="#b6">[7]</ref>, because PSR still requires the knowledge of the total number of relevant labels ??? 1 .</p><p>Table <ref type="table">2</ref>: Definitions of popular XMLC performance metrics and their unbiased estimates on missing labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measure</head><p>Definition Unbiased estimate</p><formula xml:id="formula_12">P@? (?, ?) ? -1 ? ?top ? ( ?) ? ? ? -1 ? ?top ? ( ?) ?? /? ? , R@? (?, ?) ??? -1 1 ? ?top ? ( ?) ? ? ??? -1 1 ? ?top ? ( ?) ?? /? ? nDCG@? (?, ?) ? ?top ? ( ?) ? ? log(r ? ( ?) +1) ? ? =1 1 log( ? +1) ? ?top ? ( ?) ?? ? ? log(r ? ( ?) +1) ? ? =1 1 log( ?+1)</formula><p>Because Jain et al. <ref type="bibr" target="#b17">[18]</ref> observed that the unbiased estimates could results in values larger than one, they suggest a normalized version of these metrics to be reported (cf. Section 3.4). In subsequent literature, the distinction between the unbiased metrics and the normalized versions is not always preserved, e.g., Bhatia et al. <ref type="bibr" target="#b6">[7]</ref> present unbiased formulas but lists normalized values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Empirical propensity model.</head><p>In order to use the propensityscored loss functions, one needs to have the propensities available for the individual labels. Since true propensities are unknown for the XMLC benchmark datasets, Jain et al. <ref type="bibr" target="#b17">[18]</ref> proposed to model propensities as a function of labels frequencies, resulting in propensities being a constant value for each label.</p><p>Let ? denote a propensity model. The model defined in <ref type="bibr" target="#b17">[18]</ref> can be expressed via label priors ?? P ?? = 1 :</p><formula xml:id="formula_13">? ? =? JPV ( ?? ; ?, ?, ?) 1 1 + (log ? -1)(? + 1) ? ? -? log(? ?? +?) ,<label>(10)</label></formula><p>where ? is the number of training instances, and ? and ? are datasetdependent parameters.</p><p>In order to arrive at this model and determine values for ? and ?, Jain et al. <ref type="bibr" target="#b17">[18]</ref> investigated two datasets in which ancillary information could be used to identify some missing labels.</p><p>For a Wikipedia-based dataset, the parameters of the model have been estimated with the help of a label hierarchy. They assumed that if a label is relevant to an article, then all its ancestors in the hierarchy should also be relevant. If not present, they are counted as missing. This allows plotting the fraction of instances in which the label is missing over the number of instances in which it appears. This seems to follow a sigmoidal shape as described by <ref type="bibr" target="#b9">(10)</ref>, see Figure <ref type="figure" target="#fig_0">1</ref>. The parameters ? and ? were then determined by fitting the model against the estimated values, where only labels with more than 4 descendants were used to improve robustness. The obtained values are ? = 0.5, ? = 0.4.</p><p>For the Amazon data set, which is an item-to-item recommendation task, missing labels have been approximated using "also viewed" and "also bought" information. It was assumed that a label ? (an item) is relevant to all the items viewed along with items that were also bought with label ?, as proposed by McAuley et al. <ref type="bibr" target="#b20">[21]</ref>. The obtained values are ? = 0.6 and ? = 2.6.</p><p>For other data sets the authors propose, if there is no other possibility of estimating parameters ? and ?, to use averages of the values obtained for Wikipedia and Amazon data sets (which are ? = 0.55, ? = 1.5). This, in fact, has become a standard followed in many papers without questioning its rationality.</p><p>The above propensity model is then typically used in the metric of choice for model selection and evaluation. It has also been incorporated into training procedures. For example, decision tree methods can directly use the propensity-scored variants of metrics such as precision@? or nDCG@? <ref type="bibr" target="#b17">[18]</ref>. Alternatively, one can use unbiased or upper-bounded propensity-scored surrogate losses <ref type="bibr" target="#b25">[26]</ref>.</p><p>3.1.3 Propensity and long tails. The form of <ref type="bibr" target="#b9">(10)</ref> implies that tail labels are assigned lower propensities, which means that in metrics like those in Table <ref type="table">2</ref> these tail labels, if predicted correctly, will be weighted more strongly than head labels. In particular, the resulting weightings resemble existing weighting schemes used for longtailed learning tasks, leading the authors to conclude: Such weights arise naturally as inverse propensities in the unbiased losses developed in this paper. <ref type="bibr">[...]</ref> This not only provides a sound theoretical justification of label weighting heuristics for recommending rare items but also leads to a more principled setting of the weights. As a result, propensity-scored variants are also viewed as metrics in their own right, and are currently used both to counteract missing labels (as unbiased estimates) and to weigh tail labels (as independent metrics), becoming established performance metrics commonly used in XMLC. <ref type="foot" target="#foot_5">6</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Discussion of missing-labels assumptions</head><p>In order to derive unbiased loss functions, we need to impose assumptions on the process of how labels go missing, as initially discussed in Section 2.2. Unfortunately, Jain et al. <ref type="bibr" target="#b17">[18]</ref> sent a potentially misleading message in this regard. Their Theorem 4.1 proves</p><formula xml:id="formula_14">E[? (? , ?)] = E l ( ? , ?) ,<label>(11)</label></formula><p>for any fixed prediction ? without a clear dependence on ? . This also implies that the assumptions behind the propensities are unclear. Even if we assume the propensities to be constant for label ?, the exact form of this assumption is necessary to properly prove unbiasedness in the sense of <ref type="bibr" target="#b8">(9)</ref>. Notice that P ?? = 1 | ? ? = 1 = ? ? does not imply P ?? = 1 | ? ? = 1, ? = ? ? . Moreover, for more complex functions, such as recall@?, this assumption may take the form of P ?? = 1 | ? ? = 1, ? ?? , ? = ? ? , where ? ?? represents ground-truth labels without label ? (see Appendix for an example).</p><p>In general, we cannot expect the independence of missing labels from the instance's features to hold. Consider, for example, cases where the feature and label space are of a similar origin <ref type="bibr" target="#b11">[12]</ref>, such as matching Wikipedia titles or articles to categories. It seems unlikely that a label such as "Italy" would be missing for articles containing the word "Italy" in the subject, but it might be missing for articles that still pertain to Italy but do not feature the word "Italy" in the title. The assumption that propensities are constant for each label simplifies the model significantly and leads to much simpler computational procedures. Unfortunately, if this assumption is not satisfied, then one may get implausible results as discussed later.</p><p>The assumption that the propensities do not depend on other labels going missing does not need to hold in practice as well. For example, a user that tagged the article for "Italy" with "Member states of the European Union" might be primed to think of more examples of organizations in which Italy is a member, and thus e.g., "Current member states of the United Nations" might be less likely to be forgotten than if the EU membership had been forgotten. Fortunately, in many cases, the unbiased estimate does not actually require this dependence -if the loss function can be written as a sum over contributions from each label individually, then the labels do not interact with each other and the label-wise properties are sufficient to obtain unbiased losses. This is the case for the popular PSP and PSnDCG metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Shortcomings of the JPV propensity model</head><p>Let us discuss several issues of the JPV model, concerning theoretical and empirical shortcomings, as well as some problems in the way the parameters of the model have been established.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scaling behavior.</head><p>Let us first observe that <ref type="bibr" target="#b9">(10)</ref> does not preserve propensity estimates if the amount of data is changed, without changing its characteristics, e.g., by sub-or over-sampling the dataset. In particular, if one increases the amount of available data by making multiple copies of the dataset, which should not change the estimates of label priors ?? given by ?? /? (with ?? being the number of positive instances of label ? in the observed, noisy training set), the JPV model will estimate propensities to be equal one, i.e., no missing labels, as the amount of data goes to infinity:</p><formula xml:id="formula_15">lim ??? ? JPV ( ?? , ?) = 1 1 + (? + 1) ? lim ??? (log ? -1)? -? log( ?? ?+?) = 1 1 + (? + 1) ? lim ??? (log ?)( ?? ?) -? = 1. (12)</formula><p>This means that we cannot interpret ? and ? as parameters of some underlying (unknown) process that describes the labeling process. As we cannot even have fixed ? and ? when the data come from the same process, this very much calls into question the approach of using values for ? and ? across datasets as is current practice.</p><p>Estimation process. Setting aside structural concerns about <ref type="bibr" target="#b9">(10)</ref>, the estimation of the parameters ? and ? still remains an issue. First, by identifying missing labels based on meta-data as described in Section 3.1.2, only an upper-bound on the propensity is estimated, since labels may also be missing in other ways. For example, we tried to reproduce the procedure of propensity estimation on the Wikipedia dataset. We have found that only around 40 000 out of 500 000 labels meet the criteria of the sufficient number of descendants selected by the authors, and around 300 000 labels are without descendants, so they would never be considered missing by this protocol.</p><p>Further, one might argue that in cases in which missing labels can be identified by some side-channel information such as label hierarchies, then one can directly impute these missing labels and need not worry about training with missing labels.</p><p>Propensity as a function of frequency. This still leaves the question of whether such estimates are sensible. Even though there is clearly a trend that labels within a given range of frequency have -on average -a certain propensity, for each individual label the actual propensity can fluctuate widely around this mean, as shown in Figure <ref type="figure" target="#fig_0">1</ref> that we obtained following the original procedure for estimating propensities.</p><p>Reproducibility. The description of the process of propensity estimation in <ref type="bibr" target="#b17">[18]</ref> is rather sparse on details. While meta-data for Wikipedia is easily obtainable, it is not clear what is the source of ancillary information that has been used for the Amazon dataset. Additionally, depending on the preprocessing steps and criteria, such as the number of descendants in the label hierarchy, one can achieve very different estimates of parameters ? and ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implausible results and normalization</head><p>Despite being unable to verify the correctness of the assumptions and the JPV model without actual clean ground truth data, we are still able to show that the approach of Jain et al. <ref type="bibr" target="#b17">[18]</ref> leads to implausible results. For example, PSP@?, as an unbiased estimate of P@? on the ground-truth data, should be bounded between zero and one. However, when calculating this measure for a real classifier, the result may exceed this range substantially. Of course, for an individual instance or a small subset of them, the unbiased estimate does not need to fall into that range, but a large deviation from the true value becomes exceedingly unlikely when averaging over the entire dataset.</p><p>To circumvent this issue, Jain et al. <ref type="bibr" target="#b17">[18]</ref> suggest to report a normalized version of PSP@?, also calling this measure "propensityscored precision". The normalization is realized by dividing the metrics value by the largest possible value that any prediction could have achieved on that data:</p><formula xml:id="formula_16">Norm PSP@? = ? ?=1 PSP@? ( ?? , ?? ) ? ?=1 max z PSP@? ( ?? , z) . (<label>13</label></formula><formula xml:id="formula_17">)</formula><p>The normalization introduces a factor that is constant over the entire dataset, and thus does not influence model selection. However, it removes the interpretation of the received value as an unbiased estimate of the metric on clean data, and it hides the model misspecification. Table <ref type="table">3</ref> reports the values of both variants of PSP@?, showing how severe this issue is.</p><p>Table <ref type="table">3</ref>: Normalized and unnormalized propensity-scored precision of PfastreXML <ref type="bibr" target="#b17">[18]</ref>, when using the JPV model, with ? = 0.5, ? = 0.4 for WikiLSHTC-325K and ? = 0.6, ? = 2.6 for Amazon-670K.</p><p>WikiLSTHC-325K Amazon-670K PSP(%) @1 @3 @5 @1 @3 @5 Normalized </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">The current use of propensity metrics</head><p>It seems that the current use of propensity metrics mixes up, in a not entirely clear way, two different issues, missing and tail labels. As mentioned in Section 3.1.3, these metrics might be used for the purpose of giving more weight to tail labels. In this case, the normalization step seems to be a valid procedure. However, a propensity metric loses its original interpretation, and it is just one way of accounting for tail labels, without any concrete justification. For this use case, it would be preferable to have a metric that treats tail labels in a principled way. As a first step towards that goal, Section 4.4 provides some discussion on alternative task losses.</p><p>Only in the interpretation as a tail-performance promoting loss, it does make sense to speak of a trade-off in performance between vanilla and propensity-scored metrics, as these are conceptually different. In the missing-labels interpretation, taking the propensities into account is not calculating a different conceptual metric, but instead, the correct way of calculating the unweighted, but the true performance of a classifier. Of course, in XMLC, both interpretations can be combined, i.e., one would like to have a task loss that is adapted to tail labels, but calculate it in a way that takes missing labels into account. The closest to this in the literature is <ref type="bibr" target="#b25">[26]</ref>, where training uses a loss that combines unbiased estimates and classrebalancing, but still, evaluation is performed using vanilla and propensity-scored metrics, instead of a propensity-scored variant of a tail-weighted metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RECIPES TO FOLLOW</head><p>In this section, we present several recipes on how to conduct research on missing and tail labels in XMLC. We start our discussion with a recommendation of using an additional dataset which is either unbiased or its bias is under control. We then discuss several alternatives for the JPV model, show how to fit the models to unbiased data, and how to compare them empirically. Next, we introduce methods that jointly train the prediction and the propensity model. Despite our critical remarks, we consider in this section only propensities being constant for each label. Finally, we discuss performance metrics for long-tail labels that might be a better choice than propensity-scored metrics. <ref type="foot" target="#foot_6">7</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Bias-controlled validation and test sets</head><p>If indeed our training data are biased by missing labels, the best way to test, validate, and estimate the propensity model is to use unbiased data or data with controlled bias. In the latter case, the bias is controlled in such a way that unbiased estimates can be easily computed. Such data are definitely costly to get, but even a small set can be beneficial, helping in selecting the right model to be used in production and in estimating its real performance. This is a standard approach used in recommendation systems <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b37">38]</ref> and search engines <ref type="bibr" target="#b18">[19]</ref>. Even if, in a given application, it is not possible to obtain an unbiased data due to some constraints, we should investigate algorithms on benchmarks with unbiased or bias-controlled sets associated. Without this investigation, it is hard to verify which methods are indeed working and which are misleading. In many real-world applications, the final evaluation of the model is performed in A/B tests on real, unbiased data. We should avoid situations in which the results of A/B tests are against our expectations coming from offline experiments.</p><p>To follow the above recommendation, we have prepared several datasets of different types, which can be used for experimentation with missing labels in XMLC. The first type contains fully synthetic datasets. The second one is a modification of the standard XMLC benchmarks. The final dataset is a variant of the Yahoo R3 dataset, <ref type="foot" target="#foot_7">8</ref>transformed from a recommendation problem to multi-label classification. Statistics of these datasets along with additional information are given in Appendix.</p><p>The synthetic datasets are generated in a similar way as in <ref type="bibr" target="#b33">[34]</ref>. They are parameterized by the number ? of labels. In the experiments reported below, we use ? = 100, which is not very extreme but suffices for investigating the propensity models. Each label ? is represented by a ?-dimensional hyper-ball ? ? , whose radius and center are generated randomly. All those hyper-balls lay in a feature space being itself a ?-dimensional hyper-ball ?, big enough to contain all hyper-spheres ? ? . Instances are uniformly generated in ? and each instance ? is associated with labels whose hyperballs contain ?, i.e., L (?) = { ? ? [?] : ? ? ? ? }. The popularity (or priors) of the labels is directly determined by the radius of ? ? . We separately generate training, validation, and test sets from the above model. We then apply a propensity model of choice to the training set to generate missing labels. For some experiments, we also generate missing labels in validation and test sets.</p><p>For the original XMLC benchmark datasets, we assume that there are no missing labels. We then merge the original train and test sets, and take labels having at least ? positive instances. We perform this step to select labels for which one can apply the noise models without removing all positive labels. We then split the data again into training and test sets, and apply, similarly as above, a propensity model of choice to the training set to generate missing labels. For some experiments, we extract a validation set from the test set.</p><p>In the original Yahoo R3 dataset, records are organized in a format of user-item ratings, and each record contains a user ID, an item ID, and the user's rating for the item (from 1 to 5). The training set contains over 300K ratings from 15.4K users to 1K items. This set is biased as users select items from a limited list of options recommended by some algorithm. The bias-controlled test set is obtained by collecting ratings from a subset of 5.4K users to rate ? = 10 randomly selected items. To create a multi-label dataset we treat each item as a label (? = 1?). We consider ratings greater or equal to 4 as positive feedback and others as irrelevant. We take users unique to the original training set (10K of users) to create a biased multi-label training set. For each user, we randomly split positive feedback into equal halves. We take the first half as features ? and the later half as labels ?. Next, we use users present in both the original training and test set to create a test set with a controlled bias. We again randomly select half of the positive feedbacks from the training set as features ? (to keep the same distribution of features) and all positive feedback from the test set as labels ?. We can also extract a validation set from the test set, usually consisting of a half of users from the test set. For a dataset created in such a way, we can calculate estimates of training-set propensities ? tr ? as:</p><formula xml:id="formula_18">ptr ? = ? direct = ?tr ? ? c ? ?val ? -1 ,<label>(14)</label></formula><p>where ?tr ? and ?val ? are, respectively, training-and validation-set estimates of the prior probability of label ?, and ? c ? is the controlled propensity used for the validation and test set. To estimate the label priors, one can use relative frequencies of labels in training and validation sets. For ? c ? we use a ratio of ? labels used for labelling to all ? labels, i.e., ? c ? = ? ? = 0.01 for the Yahoo R3 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Alternative propensity models</head><p>The JPV propensity model <ref type="bibr" target="#b9">(10)</ref> is not the only one to consider. In fact, many different forms have been introduced in other domains <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38]</ref>. We express the propensity models as functions of observed label priors ?? P ?? = 1 , without direct relation to ?, which by construction avoids convergence issues discussed in Section 3.3. <ref type="foot" target="#foot_8">9</ref>A propensity model used frequently in recommendation systems is given by the following power-law formulation:</p><formula xml:id="formula_19">? ? = ? P ( ?? ; ?, ?) ? ?? ? .<label>(15)</label></formula><p>With ? = max ? ?? /? we receive a model used, for example, in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b37">38]</ref>, while for ? = ? = 1 we get a very simple model which might be used, if estimation of the parameters is infeasible due to lack of unbiased or bias-controlled data. Another solution could be to use the generalized logistic function, also called Richard's curve <ref type="bibr" target="#b27">[28]</ref>, which is very flexible and its shape resembles <ref type="bibr" target="#b9">(10)</ref>:</p><formula xml:id="formula_20">? ? = ? R ( ?? ; ?, ?, ?, ? , ?, ?) ? + ? -? ? + ? exp(-? ?? ) 1/? . (<label>16</label></formula><formula xml:id="formula_21">)</formula><p>The parameters of the models can be either set up according to a domain knowledge or fit using an additional unbiased or biascontrolled dataset. In the latter case, one can use standard non-linear optimization methods <ref type="bibr" target="#b3">[4]</ref>. We fit the models to inverse propensities, minimizing squared errors ? p-1 ? -? ( ?? ) -1 ? 2 using the Levenberg-Marquardt method <ref type="bibr" target="#b22">[23]</ref>. The errors are reported in the Appendix. Table <ref type="table">4</ref>: Actual precision@{1,3,5} (and their standard errors) on the Yahoo R3 dataset. The best results are marked in bold. The last row presents the results of the PEJL method from Section 4.3. Each experiment was repeated 25 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>P@1 (%) P@3 (%) P@5 (%) Figure <ref type="figure" target="#fig_1">2</ref> illustrates the results of fitting the different propensity models for the Yahoo R3 dataset. The actual training-set propensities have been obtained using <ref type="bibr" target="#b13">(14)</ref>. The plot clearly shows that the JPV model with ? = 0.55 and ? = 1.5, suggested as default values, is not a good fit to the actual propensities. The same model, but with ? and ? fitted to the data gives a degenerated solution because many values are out of codomain of (10) when ? is that small. On the other hand, ? P and ? R seem to give a good fit, but still actual propensities are widely spread, suggesting that a model solely depending on label priors might not be the best choice.</p><formula xml:id="formula_22">?</formula><p>We have also trained prediction models using the above propensities to see whether they help in improving (actual) precision@? on the unbiased test set. We use the one-vs-all approach in which probabilistic model ? ? (?), for label ?, is obtained by minimizing the unbiased variant of logistic loss <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref>:</p><formula xml:id="formula_23">? ( ?? , ? ? , ? ? (?)) = - ?? ? ? log(? ? (?)) -1 - ?? ? ? log(1 -? ? (?)) .<label>(17)</label></formula><p>The results are given in Table <ref type="table">4</ref>. As a baseline we also use a vanilla logistic loss which corresponds to ? 1 ( ?? ) = 1. We can observe that all propensities models, except the degenerated variant of JPV, give slightly better results than the baseline, with ? R being clearly the best among them. On the other hand, ? direct , which directly estimates propensity for each label using <ref type="bibr" target="#b13">(14)</ref>, significantly improves the performance (particularly for P@3 and P@5). Nevertheless, ? direct can only work well if the unbiased or bias-controlled data are substantial. If this is not the case, one might need to use a parametric model, but the above results suggest that the dependence on label priors might not be sufficient.</p><p>Finally, we illustrate a problem of propensity mismatch on synthetic and modified benchmark datasets. We introduce noise to training data according to either the ? JPV or ? P model, train prediction functions using both propensities models, and report actual precision@?, computed on the unbiased test set, along with propensity-based precision@? for the same ? JPV or ? P model, computed on the biased test set (i.e., with the noise model applied). The results in Table <ref type="table">5</ref> show that relying on propensity-based metrics can be misleading. As it should be expected, in the majority of cases, models compatible with the metric are obtaining the best performance. However, selecting a model based on a chosen propensitybased metric can be wrong as the actual precision might be driven by a completely different propensity model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Propensity estimation via joint learning</head><p>To minimize an unbiased loss function, such as the unbiased logistic loss <ref type="bibr" target="#b16">(17)</ref>, one needs to know propensities in advance. However, estimating them might be difficult in practice. As demonstrated above, the use of inaccurate estimates can lead to results being far away from the optimal ones. Therefore it would be useful if propensities could be estimated directly from a biased training set. Unfortunately, this is an ill-defined problem because the absence of a label can be explained by either a small conditional probability of the label or a low propensity or both. The additional assumption needed for the propensity to be identifiable were studied before, in the areas of learning from positive and unlabeled data <ref type="bibr" target="#b13">[14]</ref>, and novelty detection <ref type="bibr" target="#b8">[9]</ref>. The overview of the possible assumption is given by Bekker and Davis <ref type="bibr" target="#b4">[5]</ref>, where the weakest of the assumptions requires that the true distribution of negative samples for a given label cannot contain the positive distribution <ref type="bibr" target="#b8">[9]</ref>. In these areas and under compatible assumptions, many methods for estimating the error ratio or labels priors, both directly related to propensity estimates, were proposed <ref type="bibr" target="#b4">[5]</ref>. Recently, <ref type="bibr" target="#b40">[41]</ref> and <ref type="bibr" target="#b32">[33]</ref> have introduced methods for estimating the unbiased conditional label probabilities and propensities jointly on the biased training set. We refer to such methods as Propensity Estimation via Joint Learning (PEJL).</p><p>Table <ref type="table">5</ref>: Mismatch of propensity models: actual P@{1, 3, 5} (computed on unbiased test set) and PSP@1 (computed on biased test set) of prediction models trained on data biased by ? JPV or ? P models. Green highlights PSP@k compatible with the used propensity model, while red highlights incompatible PSP@k. The best value in each column for a given dataset is marked in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Method P@k (%) PSP@k(%) @1 @3 @5 (? JPV ) @1 (? P ) @ Let us briefly describe the method of Teisseyre et al. <ref type="bibr" target="#b32">[33]</ref> (cf. Appendix for description of the method of Zhu et al. <ref type="bibr" target="#b40">[41]</ref>). It uses the fact that minimization of logistic loss leads to estimation of the posterior probability. Therefore, we can define the loss in the following way: ? ( ?? , ? ? , ? ? (?)) = -?? log(? ? ? ? (?)) -1-?? log(1-? ? ? ? (?)) , <ref type="bibr" target="#b17">(18)</ref> where ? ? ? ? (?) can be seen as an estimate of the actual, groundtruth, conditional probability ? ? (?), with ? ? being the propensity and ? ? (?) the estimate of the observed conditional probability, analogously to <ref type="bibr" target="#b7">(8)</ref>. This function can be optimized not only with respect to ? ? (?), but also to ? ? . The outline of the alternative method of Zhu et al. <ref type="bibr" target="#b40">[41]</ref> can be found in the appendix.</p><p>We evaluate this approach on Yahoo R3 dataset. The estimated values of ? ? are plotted on the Figure <ref type="figure" target="#fig_1">2</ref> and the last row of Table <ref type="table">4</ref> presents the promising results of this approach. While the obtained estimates are overestimated, they capture the true trend. The predictive performance also looks promising, being only slightly worst than the best propensity model ? R . This is indeed encouraging as this method does not have access to the unbiased or bias-controlled data. Figure <ref type="figure" target="#fig_1">2</ref> also plots the obtained propensities for each label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Task losses for long-tails</head><p>It seems that Jain et al. <ref type="bibr" target="#b17">[18]</ref> have introduced the propensity-scored losses rather to "promote" long-tail labels than to deal with missing labels. As such, the propensities can be seen as a kind of weighing approach that gives higher importance to less popular labels. Unfortunately, it is not clear why the weighing scheme used in <ref type="bibr" target="#b17">[18]</ref> should be preferred over other ones. Moreover, a weighing scheme does not have to be interpreted in terms of propensities. Let us consider a weighted variant of P@?:</p><formula xml:id="formula_24">P@? (?, ?) = ? -1 ?? ? ?top ? ( ?) ? ? ?? .<label>(19)</label></formula><p>This boils down to PSP@? when ? ? = 1 ? ? which also implies ? ? ? 1. But one can use any weights that would represent the importance or gain of labels. In such a case, the weighted P@? has a natural interpretation of being an unbiased estimate of the expected gain. If tail labels are of our interest, then they should get higher weights, but actual values are rather domain-specific without a direct relation to propensities.</p><p>To finalize our discussion, let us mention several other task losses that can be used as metrics that pay special attention to long-tail labels. The macro ? ? -measure defined as:</p><formula xml:id="formula_25">F macro ? {? ? , ?? } ? 1 = 1 ? ?? ? (1 + ? 2 ) ? ? ? ? ?? ? ? 2 ? ? ? ? + ? ?? ? ,<label>(20)</label></formula><p>puts the same weight to each label and focuses on true positives. Therefore, positive predictions on long-tail labels are important to obtain high values on this metric. One can also consider an @? version of this metric, in which only top ? predictions are taken into account. Another interesting metric, originally proposed for search engines <ref type="bibr" target="#b26">[27]</ref>, is abandonment@? defined as:</p><formula xml:id="formula_26">abandonment@? (?, ?) = 1 ?? ? top ? ( ?) : ? ? ? 1 ,<label>(21)</label></formula><p>which encounters no error if there is at least one correctly predicted label among the ? ones in the predicted set. This untypical formulation enforces diversity in the predicted set. Always predicting the two most popular but correlated, labels might be less beneficial than predicting less popular but also non-overlapping labels. Finally, we mention the coverage metric, which directly reflects the diversity of correctly predicted labels. It is defined as a fraction of labels with at least one correct positive prediction:</p><formula xml:id="formula_27">cov {? ? , ?? } ? 1 = ? -1 { ? ? [?] : ?? ? [?] s.t. ? ? ? = ?? ? = 1} . (<label>22</label></formula><p>) This metric has already been suggested in the literature as an alternative <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">36]</ref>. It has also been used in the original paper of Jain et al. <ref type="bibr" target="#b17">[18]</ref>, but only to motivate the propensity-based metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>Despite our critical comments regarding <ref type="bibr" target="#b17">[18]</ref>, we still appreciate this contribution. It was the first paper that brought direct attention to the problem of missing and long-tail labels in XMLC. The original theoretical results concerning the propensity model have motivated a lot of research in this direction. Nevertheless, we believe that missing labels and long-tail labels are rather orthogonal problems that should be solved with different tools. Obviously, labels gone missing may cause labels to be sparse, but it does not mean that a blind propensity model may solve any of these problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A LABEL FREQUENCY IN XMLC DATASETS</head><p>We show in Figure <ref type="figure" target="#fig_2">3</ref> a log-log plot of the distribution of label frequencies in popular benchmark datasets from the XMLC repository <ref type="bibr" target="#b6">[7]</ref>. As also noted in other works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref>, the label frequencies are characterized by the long-tail. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B STATISTICS OF MISSING-LABEL DATASETS</head><p>We present in Table <ref type="table">6</ref> the statistics of datasets created to experiment with propensities models and missing labels. The description of these datasets is given in Section 4. Because the process of generating the biased training sets contains randomness, for the mean number of labels per example, we report the average value from all generated variants of the datasets.</p><p>Table <ref type="table">6</ref>: Characteristics of datasets used in the experiments. We report the size of the biased train set (? tr. ) and the size of the test set (? ts. ), the total number of labels (?), and the mean number of labels per example in the biased train set and the test set. Symbol * denotes the average value over all generated variants of the dataset, and ? the value corrected by ? ? ? = ? /?, where ? is a number of labels sampled for labeling for each datapoint. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ESTIMATION ERRORS OF PROPENSITY MODELS</head><p>Table <ref type="table" target="#tab_5">7</ref> presents the mean squared errors (MSE) of inverse propensity estimates (? p-1 ? -? ( ?? ) -1 ? 2 ) on the Yahoo R3 dataset obtained by different propensity models described in Section 4. Models marked as (fit.) have been fitted to the same data the error has been calculated on. Since we repeated the experiment with the PEJL model several times, we report the average error. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D DETAILS OF MODEL TRAINING</head><p>We describe here the implementation and training procedures of classifiers used in the experiments in Section 4. For all the experiments, except the one with the PEJL model described in Section 4.3, we train a linear model using the unbiased binary-cross entropy loss <ref type="bibr" target="#b16">(17)</ref> with propensities coming from different models ?. The weights of the linear models are initialized from the uniform distribution U (-? ?, ? ?), where ? = 1/? with d being the number of features.</p><p>In the case of the PEJL approach, the same linear architecture is used but trained using <ref type="bibr" target="#b17">(18)</ref>. To assure that estimated values of ? ? are in [0, 1], they are modeled as a sigmoid transformation (? (?)) of ? ? ? parameters (one per label). Parameters ? ? ? are initialized from the uniform distribution U (-?, ?).</p><p>For each experiment, 10% of the biased training set serves as a validation set for the selection of hyperparameters and early stopping. All methods are implemented using PyTorch <ref type="bibr">[44]</ref>. Optimization is performed with the Adam optimizer <ref type="bibr">[43]</ref>. Only two hyperparameters are tuned on the biased validation test, the learning rate (from set {0.005, 0.01, 0.05, 0.1}) and the weight decay (from set {0, 1e-8, 1e-7, 1e-6}). Experiments have been performed on a single machine with Intel Xeon Gold 5115 and NVIDIA V100 16GB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E AN ALTERNATIVE PEJL METHOD</head><p>Below we briefly describe the main idea behind the method of Zhu et al. <ref type="bibr" target="#b40">[41]</ref>, which can serve as an alternative to the approach of Teisseyre et al. <ref type="bibr" target="#b32">[33]</ref> describe in Section 4.3.</p><p>Let ? ? {0, 1} ? be a mask random variable that expresses relation between ? and ? , i.e., ? = ? ? ? . We then have P ?? = 1|? = P ? ? = 1 ? P[? = 1|?] and P ? ? = 1 = ? ? and P[? = 1|?] = ? ? (?) <ref type="bibr" target="#b29">[30]</ref>. Because ? ? is a Bernoulli variable as well as ? ? and ?? , we end up with two models, the first one for ? ? with known ? ? , and the second for ? ? with known ? ? (?). The first one can be obtained</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Reproduced estimates of propensities for Wikipedia-500K dataset using labels hierarchy and propensity function ? JPV with ? = 0.5 and ? = 0.4 as estimated by Jain et al.<ref type="bibr" target="#b17">[18]</ref> for this dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Propensity models on the Yahoo R3 dataset. Annotation (fit.) denotes that the parameters have been fitted to the actual training-set propensities. The ? ?? ? ? model is described in Section 4.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Label frequency in XMLC datasets. The X-axis shows the label rank when sorted by the frequency of positive instances and the Y-axis gives the number of the positive instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Imbalance characteristics of typical XMLC datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Instances min IR</cell><cell cols="2">ILIR Pos-80%</cell></row><row><cell>Eurlex-4K</cell><cell>1.55 ? 10 4</cell><cell cols="2">15.0 1.01 ? 10 3</cell><cell>19.9</cell></row><row><cell>AmazonCat-13K</cell><cell>1.19 ? 10 6</cell><cell cols="2">3.3 3.55 ? 10 5</cell><cell>4.8</cell></row><row><cell>Wiki10-31K</cell><cell>1.41 ? 10 4</cell><cell cols="2">1.2 1.14 ? 10 4</cell><cell>19.6</cell></row><row><cell>Delicious-200K</cell><cell>1.97 ? 10 5</cell><cell cols="2">3.0 6.45 ? 10 4</cell><cell>4.0</cell></row><row><cell cols="2">WikiLSHTC-325K 1.78 ? 10 6</cell><cell cols="2">6.1 2.94 ? 10 5</cell><cell>20.7</cell></row><row><cell>Wikipedia-500K</cell><cell>1.81 ? 10 6</cell><cell cols="2">6.5 2.80 ? 10 5</cell><cell>25.1</cell></row><row><cell>Amazon-670K</cell><cell>4.90 ? 10 5</cell><cell cols="2">268.0 1.83 ? 10 3</cell><cell>54.3</cell></row><row><cell>Amazon-3M</cell><cell>1.72 ? 10 6</cell><cell cols="2">143.0 1.20 ? 10 4</cell><cell>26.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>MSE on inverse propensity estimates of different propensity models for the Yahoo-R3 dataset. Symbol * denotes the average value over several runs.</figDesc><table><row><cell>Model</cell><cell>MSE</cell></row><row><cell>? 1</cell><cell>1663.94</cell></row><row><cell>? JPV</cell><cell>1557.04</cell></row><row><cell cols="2">? JPV (fit.) 1259.78</cell></row><row><cell>? P (fit.)</cell><cell>512.94</cell></row><row><cell>? R (fit.)</cell><cell>516.85</cell></row><row><cell>? PEJL</cell><cell>? 1236  *</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We use capital letters for random variables, and calligraphic letters for sets.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>1[?]  is the indicator function.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>If it took a human one second to check a category for an article, then annotating a single article fully would take almost 6 days.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Note that other papers, including<ref type="bibr" target="#b17">[18]</ref>, use often a slightly different notation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>This concept is also used in multiclass classification, e.g. the imbalance factor of Cui et al.<ref type="bibr" target="#b10">[11]</ref>.P ? ? = 1 | ? ? 1 everywhere (called uniform class imbalance by Singh and Khim<ref type="bibr" target="#b30">[31]</ref>), learning to recognize the given class might be infeasible.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>We list several examples of references to propensity-scored losses: "We examined the performance on tail labels by PSP@k"<ref type="bibr" target="#b39">[40]</ref>; "We achieve high precision and propensity scores, thus demonstrating the effectiveness of our method even on infrequent tail labels."<ref type="bibr" target="#b15">[16]</ref>; "capture prediction accuracy of a learning algorithm at top-k slots of prediction, and also the diversity of prediction by giving higher score for predicting rarely occurring tail-labels"<ref type="bibr" target="#b2">[3]</ref>; "propensity scored precision@k which has recently been shown to be an unbiased, and more suitable, metric"<ref type="bibr" target="#b16">[17]</ref>; "which leads to better performance on tail labels."<ref type="bibr" target="#b38">[39]</ref>; "propensity scored variant which is unbiased and assigns higher rewards for accurate tail label predictions", "evaluate prediction performance on tail labels using propensity scored variants"<ref type="bibr" target="#b19">[20]</ref>; "replacing the nDCG loss with its propensity scored variant and using additional classifiers designed for tail labels"<ref type="bibr" target="#b31">[32]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>Repository with the code to reproduce all experiments: https://github.com/ mwydmuch/missing-labels-long-tails-and-propensities-in-xmlc</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>https://webscope.sandbox.yahoo.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>The simplest estimate of the priors are relative frequencies of labels, i.e., ?? = ?? /?. As we deal with many very sparse labels, we should rather use more robust estimates, for example, ?? = ( ?? + ?)/(? + ?).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p><rs type="funder">Academy of Finland</rs> grant: Decision No. <rs type="grantNumber">347707</rs>. Computational experiments have been performed in Poznan Supercomputing and Networking Center.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_SjJjN4u">
					<idno type="grant-number">347707</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>by minimizing <ref type="bibr" target="#b16">(17)</ref>. For the second one, a parametric model ? (? ? ) for ? ? can be learned by minimizing the following logistic loss: ? ( ?? , ? ? (?), ? (? ? )) = -?? ? ? (?) log(? (? ? ))-1-??</p><p>log(1-? (? ? )) .</p><p>(23) We can learn ? ? (?) and ? (? ? ) jointly by replacing ? ? (?) by ? ? (?) in <ref type="bibr" target="#b22">(23)</ref>, and ? ? by ? (? ? ) in <ref type="bibr" target="#b16">(17)</ref>. Since both ? ? (?) and ? (? ? ) can be updated on a single example ?, to avoid estimation-training overlap problem, the training data is split into two parts, and the training is performed with ? (? ? ) fixed on one part and with ? ? (?) fixed on the second one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F ADDITIONAL ASSUMPTIONS FOR COMPLEX METRICS</head><p>Here we show an example which demonstrates that for a nondecomposable metric, additional assumptions on the process of labels going missing are required. Consider a setting with two class labels. Let the label-wise propensities for both labels be ? 1 = ? 2 = 0.5. The desired unbiased loss function of ? can be parametrized for each prediction ? by four real numbers, ? ? l ( ?, ?). We can consider two different scenarios. First, both labels always go missing at the same time, second they go missing complementarily, corresponding to the probability distributions P and P ? . As the unbiasedness needs to hold for all potential distributions of true labels, it needs to hold in particular in the four cases in which the true label distribution is concentrated on a single point. By explicitly calculating expectations through reading off the probabilities from Table <ref type="table">8</ref>, we can state the unbiasedness requirement which is E l ( ? , ? ) = E ? (? , ? ) : ? ((1, 1), ?) = 0.5(? 11 + ? 00 ) = 0.5(? 10 + ? 01 ) , ? ((1, 0), ?) = 0.5? 00 + 0.5? 10 , ? ((0, 1), ?) = 0.5? 00 + 0.5? 01 , ? ((0, 0), ?) = ? 00 . <ref type="bibr" target="#b23">(24)</ref> These are five linear equations with only four variables, so in general, there is no solution. If we additionally assume that the labels go missing independently from each other, then the marginal propensities uniquely determine the full distribution. In that case, unbiased estimates can be derived for general loss functions <ref type="bibr" target="#b29">[30]</ref>. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multilabel learning with millions of labels: Recommending advertiser bid phrases for web pages</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Archit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashoteja</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manik</forename><surname>Varma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">DiSMEC: Distributed Sparse Machines for Extreme Multi-label Classification</title>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Babbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="721" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Data scarcity, robustness and extreme multi-label classification</title>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Babbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">09</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Mokhtar</surname></persName>
		</author>
		<author>
			<persName><surname>Bazaraa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hanif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitharanjan</forename><forename type="middle">M</forename><surname>Sherali</surname></persName>
		</author>
		<author>
			<persName><surname>Shetty</surname></persName>
		</author>
		<title level="m">Nonlinear Programming: Theory and Algorithms</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning from positive and unlabeled data: a survey</title>
		<author>
			<persName><forename type="first">Jessa</forename><surname>Bekker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="719" to="760" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Conditional Probability Tree Estimation Analysis and Algorithms</title>
		<author>
			<persName><forename type="first">Alina</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yury</forename><surname>Lifshits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">B</forename><surname>Sorkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">L</forename><surname>Strehl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="51" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The extreme classification repository: Multi-label datasets and code</title>
		<author>
			<persName><surname>Kush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Himanshu</forename><surname>Dahiya</surname></persName>
		</author>
		<author>
			<persName><surname>Jain</surname></persName>
		</author>
		<ptr target="http://manikvarma.org/downloads/XC/XMLRepository.html" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Anshul Mittal, Yashoteja Prabhu, and Manik Varma</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sparse Local Embeddings for Extreme Multi-label Classification</title>
		<author>
			<persName><forename type="first">Kush</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Purushottam</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manik</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="730" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-Supervised Novelty Detection</title>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gyemin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clayton</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2973" to="3009" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Taming Pretrained Transformers for Extreme Multi-label Text Classification</title>
		<author>
			<persName><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3163" to="3171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Classbalanced loss based on effective number of samples</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">SiameseXML: Siamese Networks meet Extreme Classifiers with 100M Labels</title>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Dahiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananye</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gururaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumeet</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Purushottam</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manik</forename><surname>Varma</surname></persName>
		</author>
		<idno>ICML. 2330- 2340</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>In NeurIPS. 567-575</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning classifiers from only positive and unlabeled data</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Elkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Noto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="213" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning from imbalanced data sets</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salvador</forename><surname>Garc?a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Galar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ronaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bartosz</forename><surname>Prati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Krawczyk</surname></persName>
		</author>
		<author>
			<persName><surname>Herrera</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Breaking the Glass Ceiling for Embedding-Based Classifiers for Large Output Spaces</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satyen</forename><surname>Daniel N Holtmann-Rice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sashank</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Slice: Scalable Linear Extreme Classifiers Trained on 100 Million Labels for Related Searches</title>
		<author>
			<persName><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatesh</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhanu</forename><surname>Chunduri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manik</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="528" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Extreme Multi-Label Loss Functions for Recommendation, Tagging, Ranking and Other Missing Label Applications</title>
		<author>
			<persName><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashoteja</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manik</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="935" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unbiased Learning-to-Rank with Biased Feedback</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adith</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5284" to="5288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bonsai: diverse and shallow trees for extreme multi-label classification</title>
		<author>
			<persName><forename type="first">Sujay</forename><surname>Khandagale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Babbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="2099" to="2119" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Inferring Networks of Substitutable and Complementary Products</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Extreme Classification in Log Memory using Count-Min Sketch: A Case Study of Amazon Search with 50M Products</title>
		<author>
			<persName><forename type="first">Tharun</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reddy</forename><surname>Medini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qixuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijai</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshumali</forename><surname>Shrivastava</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13265" to="13275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The Levenberg-Marquardt algorithm: Implementation and theory</title>
		<author>
			<persName><forename type="first">Jorge</forename><forename type="middle">J</forename><surname>Mor?</surname></persName>
		</author>
		<editor>Numerical Analysis, G. A. Watson</editor>
		<imprint>
			<date type="published" when="1978">1978</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="105" to="116" />
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cost-sensitive learning with noisy labels</title>
		<author>
			<persName><forename type="first">Nagarajan</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambuj</forename><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5666" to="5698" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">FastXML: a fast, accurate and stable tree-classifier for extreme multi-label learning</title>
		<author>
			<persName><forename type="first">Yashoteja</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manik</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Priyanshu Gupta, and Rohit Babbar. 2021. Convex Surrogates for Unbiased Loss Functions in Extreme Classification With Missing Labels</title>
		<author>
			<persName><forename type="first">Mohammadreza</forename><surname>Qaraei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Schultheis</surname></persName>
		</author>
		<idno>WWW. 3711-3720</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning diverse rankings with multi-armed bandits</title>
		<author>
			<persName><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="784" to="791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Flexible Growth Function for Empirical Use</title>
		<author>
			<persName><forename type="first">J</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName><surname>Richards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Botany</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="290" to="301" />
			<date type="published" when="1959-06">1959. 06 1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suguru</forename><surname>Yaginuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Nishino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hayato</forename><surname>Sakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuhide</forename><surname>Nakata</surname></persName>
		</author>
		<title level="m">Unbiased Recommender Learning from Missing-Not-At-Random Implicit Feedback. In WSDM</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="501" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Unbiased Loss Functions for Multilabel Classification with Missing Labels</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Schultheis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Babbar</surname></persName>
		</author>
		<idno>CoRR abs/2109.11282</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Shashank</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Khim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.01777</idno>
		<title level="m">Statistical Theory for Imbalanced Binary Classification</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>math.ST</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">AnnexML: Approximate Nearest Neighbor Search for Extreme Multi-label Classification</title>
		<author>
			<persName><forename type="first">Yukihiro</forename><surname>Tagami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="455" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Different Strategies of Fitting Logistic Regression for Positive and Unlabelled Data</title>
		<author>
			<persName><forename type="first">Pawe?</forename><surname>Teisseyre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Mielniczuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ma?gorzata</forename><surname>?az?cka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Framework to Generate Synthetic Multi-label Datasets</title>
		<author>
			<persName><forename type="first">Jimena</forename><surname>Tom?s</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Newton</forename><surname>Spola?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Everton</forename><surname>Cherman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria-Carolina</forename><surname>Monard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Electronic Notes in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">302</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page" from="155" to="176" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A theory of learning with corrupted labels</title>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Van Rooyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="8501" to="8550" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Does Tail Label Help for Large-Scale Multi-Label Learning?</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Feng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2315" to="2324" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Propensity-Scored Probabilistic Label Trees</title>
		<author>
			<persName><forename type="first">Marek</forename><surname>Wydmuch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalina</forename><surname>Jasinska-Kobus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Babbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Dembczynski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2252" to="2256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unbiased Offline Recommender Evaluation for Missing-Not-at-Random Implicit Feedback</title>
		<author>
			<persName><forename type="first">Longqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deborah</forename><surname>Estrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="279" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">PPDsparse: A Parallel Primal-Dual Sparse Method for Extreme Classification</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>En-Hsu Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangru</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno>KDD. 545-553</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">AttentionXML: Label Tree-based Attention-Aware Deep Model for High-Performance Extreme Multi-Label Text Classification</title>
		<author>
			<persName><forename type="first">Ronghui</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziye</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Mamitsuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanfeng</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In NeurIPS. 5812-5822</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unbiased Implicit Recommendation and Propensity Estimation via Combinational Joint Learning</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Caverlee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="551" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning Optimal Tree Models under Beam Search</title>
		<author>
			<persName><forename type="first">Jingwei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziru</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11650" to="11659" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
