<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Describing Clothing by Semantic Attributes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Huizhong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<settlement>Stanford</settlement>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Gallagher</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Kodak Research Laboratories</orgName>
								<address>
									<settlement>Rochester</settlement>
									<region>New York</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bernd</forename><surname>Girod</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<settlement>Stanford</settlement>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Describing Clothing by Semantic Attributes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DA91F1E35BB8B54A18A02FEF1CEDB997</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Describing clothing appearance with semantic attributes is an appealing technique for many important applications. In this paper, we propose a fully automated system that is capable of generating a list of nameable attributes for clothes on human body in unconstrained images. We extract low-level features in a pose-adaptive manner, and combine complementary features for learning attribute classifiers. Mutual dependencies between the attributes are then explored by a Conditional Random Field to further improve the predictions from independent classifiers. We validate the performance of our system on a challenging clothing attribute dataset, and introduce a novel application of dressing style analysis that utilizes the semantic attributes produced by our system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over recent years, computer vision algorithms that describe objects on the semantic level have attracted research interest. Compared to conventional vision tasks such as object matching and categorization, learning meaningful attributes offers a more detailed description about the objects. One example is the FaceTracer search engine <ref type="bibr" target="#b0">[1]</ref>, which allows the user to perform face queries with a variety of descriptive facial attributes.</p><p>In this paper, we are interested in learning the visual attributes for clothing items. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, a set of attributes is generated to describe the visual appearance of clothing on the human body. This technique has a great impact on many emerging applications, such as customer profile analysis for shopping recommendations. With a collection of personal or event photos, it is possible to infer the dressing style of the person or the event by analyzing the attributes of clothes, and subsequently make shopping recommendations. The application of dressing style analysis is shown in Fig. <ref type="figure">9</ref>.</p><p>Another important application is context-aware person identification, where many researchers have demonstrated superior performance by incorporating clothing information as a contextual cue that complements facial features <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. Indeed, within a certain time frame (i.e., at a given event), people are unlikely to change their clothing. By accurately describing the clothing, person identification accuracy can be improved over conventional techniques that only rely on faces. In our study, we also found that clothing carry significant information to infer the gender of the wearer. This observation is consistent with the prior work of <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, which exploits human body information to predict gender. Consequently, a better gender classification system can be developed by combining clothing information with traditional face-based gender recognition algorithms. We present a fully automatic system that learns semantic attributes for clothing on the human upper body. We take advantage of the recent advances in human pose estimation <ref type="bibr" target="#b6">[7]</ref>, by adaptively extracting image features from different human body parts. Due to the diversity of the clothing attributes that we wish to learn, a single type of feature is unlikely to perform well on all attributes. Consequently, for each attribute, the prediction is obtained by aggregating the classification results from several complementary features. Last, but not the least, since the clothing attributes are naturally correlated, we also explore the mutual dependencies between the attributes. Essentially, these mutual dependencies between various clothing attributes capture the Rules of Style. For example, neckties are rarely worn with T-shirts. To model the style rules, a conditional random field (CRF) is employed on top of the classification predictions from individual attribute classifiers, and the final list of attributes will be produced as the inference result of the CRF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>List of attributes</head><p>To evaluate the descriptive ability of our system, we have created a dataset with annotated images that contain clothed people in unconstrained settings. Learning clothing attributes in unconstrained settings can be extremely difficult due to the wide variety of clothing appearances, such as geometric and photometric distortions, partial occlusions, and different lighting conditions. Even under such challenging conditions, our system demonstrates very good performance. Our major contributions are as follows: 1) We introduce a novel clothing feature extraction method that is adaptive to human pose; 2) We exploit the natural rules of fashion by learning the mutual relationship between different clothing attributes, and show improved performance with this modeling; 3) We propose a new application that predicts the dressing style of a person or an event by analyzing a group of photos, and demonstrate gender classification from clothing that advocate similar findings from other researchers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The study of clothing appearance has become a popular topic because many techniques in context-aware person identification use clothing as an important contextual cue. Anguelov et. al. <ref type="bibr" target="#b1">[2]</ref> constructed a Markov Random Field to incorporate clothing and other contexts with face recognition. They extract clothing features by collecting color and textual information from a rectangular bounding box under the detected face, but this approach suffers from losing clothing information by neglecting the shape and variable pose of the human body. To overcome this problem, Gallagher and Chen <ref type="bibr" target="#b3">[4]</ref> proposed clothing cosegmentation from a set of images and demonstrated improved performance on person identification. However, clothing is described only by low-level features for the purpose of object matching, which differs from our work that learns mid-level semantic attributes for clothes.</p><p>Very recently, attribute learning has been widely applied to many computer vision tasks. In <ref type="bibr" target="#b7">[8]</ref>, Ferrari and Zisserman present a probabilistic generative model to learn visual attributes such as "red", "stripes" and "dots". With the challenge of training models on noisy images returned from Google search, their algorithm has shown very good performance. Farhadi et. al. <ref type="bibr" target="#b8">[9]</ref> learn discriminative attributes and effectively categorize objects using the compact attribute representation. Similar work has been performed by Russakovsky and Fei-Fei <ref type="bibr" target="#b9">[10]</ref>, by utilizing attributes and exploring transfer learning for object classification on large-scale datasets. Also, for the task of object classification, Parikh and Grauman <ref type="bibr" target="#b10">[11]</ref> build a set of attributes that is both discriminative and nameable by displaying categorized object images to a human in the loop. In <ref type="bibr" target="#b11">[12]</ref>, Kumar et. al. propose attribute and simile classifiers that describe face appearances, and have demonstrated very competitive results for the application of face verification. Siddiquie et. al. <ref type="bibr" target="#b12">[13]</ref> explore the co-occurrence of attributes for image ranking and retrieval with multi-attribute queries. One of the major challenges of attribute learning is the lack of training data, since the acquisition of labels can be both labor-intensive and time-consuming. To overcome this limitation, Berg et. al. <ref type="bibr" target="#b13">[14]</ref> proposed mining both the catalog images and their descriptive text from the Internet and perform language processing to discover attributes. Another interesting application is to incorporate the discovered attributes with language models to generate a sentence for an image, in a similar manner that a human might describe an image <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. Most work in the attribute learning literatures either assumes a pre-detected bounding box that contains the object of interest, or uses the image as a whole for feature extraction. Our work is different from previous work not simply because we perform attribute learning on clothing, but also due to the fact that we extract features that are adaptive to unconstrained human poses. We show that with the prior knowledge of human poses, features can be collected in a more sensible way to make better attribute predictions.</p><p>Recently, Bourdev et. al. <ref type="bibr" target="#b5">[6]</ref> proposed a system that describes the appearance of people, using 9 binary attributes such as "is male", "has T-shirt", and "long hair". They used a set of parts from Poselets <ref type="bibr" target="#b17">[17]</ref> for extracting low-level features and perform subsequent attribute learning. In comparison, we propose a system that comprehensively describes the clothing appearance, with 23 binary attributes and 3 multi-class attributes. We not only consider high-level attributes such as clothing categories, but also deal with some very detailed attributes like "collar presence", "neckline shape", "striped", "spotted" and "graphics". In addition, we explicitly exploit human poses by selectively changing sampling positions of our model and experimentally demonstrate the benefits of modeling pose. Further, <ref type="bibr" target="#b5">[6]</ref> applies an additional layer of SVMs to explore the attribute correlations, whereas in our system, the attribute correlations are modeled with a CRF which benefits from computation efficiency. Besides the system of <ref type="bibr" target="#b5">[6]</ref> that learns descriptive attributes for people, some limited work has been done to understand the clothing appearance on the semantic level. Song et. al. <ref type="bibr" target="#b18">[18]</ref> proposed an algorithm that predicts job occupation via human clothes and contexts, showing emerging applications by investigating clothing visual appearance. Yang and Yu <ref type="bibr" target="#b19">[19]</ref> proposed a clothing recognition system that identifies clothing categories such as "suit" and "T-shirt". In contrast, our work learns a much broader range of attributes like "collar presence" and "sleeve length". The work that explores similar attributes to ours is <ref type="bibr" target="#b20">[20]</ref>. However, their system is built for images taken in a well controlled fitting room environment with constrained human poses in frontal view, whereas our algorithm works for unconstrained images with varying poses. In addition, their attributes predictions use hand-designed features, while we follow a more disciplined learning approach that more easily allows new attributes to be added to the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Clothing Attributes and Image Data</head><p>By surveying multiple online catalogs, we produced a list of common attributes to describe clothing. As shown in Table <ref type="table" target="#tab_0">1</ref>, some of these attributes are binary like "collar presence", while some are multi-class such as "clothing category". Although we tried to include a full set of attributes that comprehensively describes clothing, we dropped a few attributes like "contains logo" due to very small number of positive examples.</p><p>Clothing images and labels are required to train attribute classifiers. We have collected images from Sartorialist<ref type="foot" target="#foot_0">1</ref> and Flickr, by applying an upper body detector <ref type="bibr" target="#b21">[21]</ref> to select pictures with people. Altogether, we harvested 1856 images that contain clothed people (mostly pedestrians on the streets). Some image samples from our dataset are shown in Fig. <ref type="figure">8</ref>. We then use Amazon Mechanical Turk (AMT) to collect ground truth labels. When labeling a piece of clothing on the human body, the AMT workers were asked to make a choice for each attribute. For instance, the worker may select one answer from "1) no sleeve; 2) short sleeve; 3) long sleeve" for the attribute "sleeve length". Note that for the "gender" attribute, in order to avoid the ambiguity of judging whether this piece of clothing is "men's" or "women's", workers were explicitly told to label the gender of the wearer. To eliminate noisy labels, every image was labeled by 6 workers. Only labels that have 5 or more agreements are accepted as the ground truth. We gathered 283,107 labels from the AMT workers, and the ground truth statistics of our clothing attribute dataset are summarized in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning Clothing Attributes</head><p>The flowchart of our system is illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. For an input image, human pose estimation is performed to find the locations of the upper torso and arms. We then extract 40 features from the torso and arm regions, and subsequently quantize them. For each attribute, we perform SVM classification using the combined feature computed from the weighted sum of the 40 features. Each attribute classifier outputs a probability score that reflects the confidence of the attribute prediction. Next, a CRF is employed to learn the stylistic relationships between various attributes. By feeding the CRF with the probability scores from the attribute classifiers, the attribute relations are explored, which leads to better predictions than independently using the attribute classifiers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Human Pose Estimation</head><p>Thanks to the recent progress in human pose estimation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23]</ref>, the analysis of complex human poses has been made possible. With the knowledge of the physical pose of the person, clothing attributes can be learned more effectively, e.g., features from the arm regions offer a strong clue for sleeve length.</p><p>Estimating the full body pose from 2-D images still remains a challenging problem, partly because the lower body is occasionally occluded or otherwise not visible in some images. Consequently, we only consider the clothing items on the upper body. We apply the work in <ref type="bibr" target="#b6">[7]</ref> for human pose estimation and briefly review the technique here for the completeness of the paper. Given an input image, the upper body of the person is firstly located by using complementary results of an upper body detector <ref type="bibr" target="#b21">[21]</ref> and Viola-Jones face detector <ref type="bibr" target="#b24">[24]</ref>. The bounding box of the detected upper body is then enlarged, and the GrabCut algorithm <ref type="bibr" target="#b25">[25]</ref> is used to segment the person from the background. Personspecific appearance models for different body parts are estimated within the detected upper body window. Finally, an articulated pose is formed within the segmented person area, by using the person-specific appearance models and generic appearance models. The outputs of the pose estimation algorithm are illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, which include a stick-man model and the posterior probability map of the six upper body regions (head, torso, upper arms and lower arms). We threshold the posterior probability map to obtain binary masks for the torso and arms, while ignoring the head region since it is not related to clothing attribute learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Feature Extraction</head><p>Due to the large number of attributes that we wish to learn, the system is unlikely to achieve optimal performance with a single type of feature. For example, while texture descriptors are useful for analyzing the clothing patterns such as "striped" and "dotted", they are not suitable for describing clothing colors. Therefore, in our implementation we use 4 types of base features, including SIFT <ref type="bibr" target="#b26">[26]</ref>, texture descriptors from the Max- A Conditional Random Field that captures the mutual dependencies between the attributes is employed to make attribute predictions. The final output of the system is a list of nameable attributes that describe the clothing appearance. imum Response Filters <ref type="bibr" target="#b27">[27]</ref>, color in the LAB space, and skin probabilities from our skin detector.</p><p>As mentioned in Section 1, the features are extracted in a pose-adaptive way. The sampling location, scale and orientation of the SIFT descriptors depend on the estimated human body parts and the stick-man model. Fig. <ref type="figure" target="#fig_2">3</ref> illustrates the extraction of the SIFT descriptors over the person's torso region. The sampling points are arranged according to the torso stick and the boundary of the torso. The configuration of the torso sampling points is a 2-D array, whose size is given by the number of samples along the stick direction times the number of samples normal to the stick direction. The scale of the SIFT descriptor is determined by the size of the torso mask, while the orientation is simply chosen as the direction of the torso stick. The extraction of SIFT features over the 4 arm regions is done in a similar way, except that the descriptors are only sampled along the arm sticks. In our implementation, we sample SIFT descriptors at 64 × 32 locations for the torso, and 32 locations along the arm stick for each of the 4 arm segments. The remaining three of our base features, namely textures descriptors, color and skin probabilities, are computed for each pixel in the five body regions.</p><p>Once the base features are computed, they are quantized by soft K-means with 5 nearest neighbors. As a general design guideline, features with larger dimensionality should have more quantization centroids. Therefore we allocate 1000 visual words for SIFT, 256 centroids for texture descriptor, and 128 centroids for color and skin probability. We employed a Kd-tree for efficient feature quantization. The quantized features are then aggregated by performing average pooling or pooling over the torso and arm regions. Note that the feature aggregation for torso is done by constructing a 4 × 4 spatial pyramid <ref type="bibr" target="#b28">[28]</ref> over the torso region and then average or max pooled.</p><p>Table <ref type="table" target="#tab_1">2</ref> summarizes the features that we extract. In total, we have 40 different features from 4 feature types, computed over 5 body regions with 2 aggregation methods.</p><p>Last, but not the least, we extract one more feature that is specifically designed for learning clothing color attributes, which we call skin-excluded color feature. Obviously, the exposed skin color should not be used to describe clothing colors. Therefore we mask out the skin area (generated by our skin detector) to extract the skin-excluded color feature. As the arms may contain purely skin pixels when the person is wearing no-sleeve clothes, it may not be feasible to aggregate skin-excluded color feature for the arm regions. Hence we perform aggregation over the whole upper body area (torso + 4 arm segments), excluding the skin region. Also, since the maximum color responses are subject to image noise, only average pooling is adopted. So compared to the set of 40 features described above, we only use 1 feature for learning clothing color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Attribute Classification</head><p>We utilize Support Vector Machines (SVMs) <ref type="bibr" target="#b29">[29]</ref> to learn attributes since they demonstrate state-of-the-art performance for classification. For clothing color attributes like "red" and "blue", we simply use the skin-excluded color feature to train one SVM per color attribute. For each of the other clothing attributes such as "wear necktie" and "collar presence", we have 40 different features and corresponding weak classifiers but it is uncertain which feature offers the best classification performance for the attribute that we are currently learning. A naive solution would be to concatenate all features into a single feature vector and feeding to an SVM for attribute classification. However, this approach suffers from three major drawbacks: 1) the model is likely to overfit due to the extremely high dimensional feature vector; 2) due to the high feature dimension, classification can be slow; and most importantly, 3) within the concatenated feature vector, high dimensional features will dominate over low dimensional ones so <ref type="figure">4</ref>. A CRF model with M attribute nodes pairwise connected. Fi denotes the feature for inferring attribute i the classifier performance is similar to the case when only high dimensional features are used. Indeed, in our experiments we found that the concatenated feature vector offers negligible improvements over the SVM that uses only the SIFT features.</p><formula xml:id="formula_0">A 1 A M F 1 F M A 2 F 2 … … Fig.</formula><p>Another attribute classification approach is to train a set of 40 SVMs, one for each feature type, and pick the best performing SVM as the attribute classifier. This method certainly works, but we are interested in achieving a even better performance with all the available features. Using the fact that SVM is a kernel method, we form a combined kernel from the 40 feature kernels by weighted summation, where the weights correspond to the classification performance of the features. Intuitively, better feature kernels are assigned heavier weights than weaker feature kernels. The combined kernel is then used to train an SVM classifier for the attribute. This method is inspired by the work in <ref type="bibr" target="#b30">[30]</ref>, where they demonstrated significant scene classification improvement by combining features. Our experimental results in Section 5.1 also show the advantage offered by feature combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Multi-attribute Inference</head><p>Due to the functionality and fashion of clothing, it is common to see correlations between clothing attributes. As an example, in our ground truth dataset there is only 1 instance when the person wears a necktie but does not have a collar. It should be noted that the dependencies between the attributes are not symmetric, e.g., while having a necktie strongly suggest collar presence, the presence of a collar does not indicate that the person should wear a necktie.</p><p>We explore the mutual dependencies between attributes by applying a CRF with the SVM margins from the previous attribute classification stage. Each attribute functions as a node in the CRF, and the edge connecting every two attribute nodes reflects the joint probability of these two attributes. We build a fully connected CRF with all attribute nodes pairwise connected, as shown in Fig. <ref type="figure">4</ref>.</p><p>Let us consider the relation between two attributes, A 1 and A 2 . F 1 and F 2 are the features that we use to infer A 1 and A 2 respectively. Our goal is to maximize the conditional probability P (A 1 , A 2 |F 1 , F 2 ):  Equation 3 is consistent with our CRF model in Fig. <ref type="figure">4</ref>, assuming that the observed feature F i is independent of all other features once the attribute A i is known. From the ground truth of the training data, we can estimate the joint probability P (A 1 , A 2 ) as well as the priors P (A 1 ) and P (A 2 ). The conditional probabilities P (A 1 |F 1 ) and P (A 2 |F 2 ) are given by the SVM probability outputs for A 1 and A 2 respectively. We define the unary potential Ψ (A i ) = -log P (Ai|Fi)</p><formula xml:id="formula_1">P (A 1 , A 2 |F 1 , F 2 ) = P (F 1 , F 2 |A 1 , A 2 )P (A 1 , A 2 ) P (F 1 , F 2 )<label>(1)</label></formula><formula xml:id="formula_2">∝ P (F 1 , F 2 |A 1 , A 2 )P (A 1 , A 2 ) (2) = P (F 1 |A 1 )P (F 2 |A 2 )P (A 1 , A 2 )<label>(3)</label></formula><formula xml:id="formula_3">∝ P (A 1 |F 1 ) P (A 1 ) P (A 2 |F 2 ) P (A 2 ) P (A 1 , A 2 )<label>(4)</label></formula><formula xml:id="formula_4">P (Ai)</formula><p>and the edge potential Φ(A i , A j ) = -log(P (A i , A j )). Following Equation <ref type="formula" target="#formula_1">1</ref>, the optimal inference for (A 1 , A 2 ) is achieved by minimizing Ψ (A 1 )+Ψ (A 2 )+Φ(A 1 , A 2 ), where the first two terms are the unary potentials associated with the nodes, and the last term is the edge potential that describes the relation between the attributes.</p><p>For a fully connected CRF with a set of nodes S and a set of edges E, the optimal graph configuration is obtained by minimizing the graph potential, given by:</p><formula xml:id="formula_5">Ai∈S Ψ (A i ) + λ (Ai,Aj)∈E Φ(A i , A j )<label>(5)</label></formula><p>where λ assigns relative weights between the unary potentials and the edge potentials.</p><p>It is typically less than 1 because a fully connected CRF normally contains more edges than nodes. The actual value of λ can be optimized by cross validation. In our implementation, we use belief propagation <ref type="bibr" target="#b31">[31]</ref> to minimize the attribute label cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We have performed extensive evaluations of our system on the clothing attribute dataset. In Section 5.1, we show that our pose-adaptive features offer better classification accuracies compared to the features extracted without a human pose model. The results from section 5.2 demonstrates that the CRF improves attribute predictions since it explores relations between the attributes. Finally, we show some potential applications that directly utilize the output of our system. Fig. <ref type="figure">6</ref>. Multi-class confusion matrices. Predictions are made using the combined feature extracted with the pose model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Attribute Classification</head><p>We use the chi-squared kernel for the SVM attribute classifiers since it outperforms both the linear and the RBF kernels in our experiments. To examine the classification accuracy, we partition the data such that each attribute has equal number of examples in all classes. For example, we balance the data of the "wearing necktie" attribute so that it has the same number of positive and negative examples. We use leave-1-out cross validation to report the classification accuracy for each attribute.</p><p>Fig. <ref type="figure" target="#fig_3">5</ref> compares the SVM performances with three types of feature inputs: 1) With pose model, using the best feature out of our 40 features; 2) With pose model, combining the 40 features with the method described in Section 4.3; 3) No pose model, same experiment settings as case 2 but with features extracted within a scaled clothing mask <ref type="bibr" target="#b3">[4]</ref> under the face. Note that the single best feature accuracies for the the color attributes are not displayed in Fig. <ref type="figure" target="#fig_3">5</ref>, because color attribute classifiers only use the skin-excluded color feature, which we regard as the "combined" feature for color attributes.</p><p>First of all, we observe that the SVM classifiers with combined features always perform better than those using the single best feature. This is because the combined feature utilizes the complementary information of all extracted features. More importantly, we are interested to know whether considering human pose helps the classification of clothing attributes. As can be seen in Fig. <ref type="figure" target="#fig_3">5</ref>, pose-adaptive features offer improved performance for most attributes, except a slight decrease in performance for 4 attributes in colors and patterns. In particular, for those attributes like "sleeve length" that heavily depend on human poses, we observe a significant boost in performance with poseadaptive features, compared to the classifier performance of using features without prior knowledge of the human pose.</p><p>We also show the confusion matrices of the multi-class attributes in Fig. <ref type="figure">6</ref>, where the attributes are predicted using the combined feature extracted with the pose model. The misclassification patterns of the confusion matrices are plausible. For example, "no sleeve" is misclassified more often as "short sleeve" than "long sleeve", and "tank top" is more often confused with "dress" than with other clothing categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Exploiting Attribute Relations with CRF</head><p>We apply a CRF as described in Section 5.2 to perform multi-attribute inference. As shown in Equation <ref type="formula" target="#formula_1">1</ref>, the CRF uses the prior probability of each attribute, and the prior Fig. <ref type="figure">7</ref>. Comparison of G-means before and after the CRF reflects the proportion of each attribute class in the dataset. Therefore, instead of testing the classifier performance on the balanced data, we evaluate the classification performance on the whole clothing attribute dataset. Since this is an unbalanced classification problem, classification accuracy is no longer a proper evaluation metric. We use the Geometric Mean (G-mean), which is a popular evaluation metric for unbalanced data classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G-mean</head><formula xml:id="formula_6">= N i=1 R i 1 N (6)</formula><p>where R i is the retrieval rate for class i, and N is the number of classes.</p><p>The G-means before and after applying the CRF are shown in Fig. <ref type="figure">7</ref>. It can be seen that the CRF offers better predictions for 19 out of 26 attributes, sometimes providing large margins of improvement. For those attributes that the CRF fails to improve the performance, only very minor degradations are observed. Overall, better classification performance is achieved by applying the CRF on top of our attribute classifiers. Fig. <ref type="figure">8</ref> shows the attribute predictions of 4 images sampled from our dataset. As can be seen in Fig. <ref type="figure">8</ref>, the CRF corrects some conflicting attributes that are generated by independent classifiers. More clothing attribute illustration examples can be found here<ref type="foot" target="#foot_1">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Applications</head><p>Dressing Style Analysis With a collection of customer photos, our system can be used to analyze the customer's dressing style and subsequently make shopping recommendations. For each attribute, the system can tell the percentage of its occurrences in the group of photos. Intuitively, in order for an attribute class to be regarded as a personal style, this class has to be observed much more frequently compared to the prior of the general public. As an example, if a customer wears plaid-patterned clothing three days a week, then wearing plaid is probably his personal style, because the frequency that he than that of using each feature alone. Interestingly, clothing-only gender recognition outperforms face-only gender recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We propose a fully automated system that describes the clothing appearance with semantic attributes. Our system demonstrates superior performance on unconstrained images, by incorporating a human pose model during the feature extraction stage, as well as by modeling the rules of clothing style by observing co-occurrences of the attributes. We also show novel applications where our clothing attributes can be directly utilized. In the future, we expect to observe even more improvements on our system, by employing the (almost ground truth) human pose estimated by Kinect sensors <ref type="bibr" target="#b23">[23]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. With the estimated human pose, our attribute learning algorithm generates semantic attributes for the clothing</figDesc><graphic coords="2,94.56,59.40,61.15,91.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Flowchart of our system. Several types of features are extracted based on the human pose. These features are combined based on their predictive power to train a classifier for each attribute.A Conditional Random Field that captures the mutual dependencies between the attributes is employed to make attribute predictions. The final output of the system is a list of nameable attributes that describe the clothing appearance.</figDesc><graphic coords="6,78.81,103.71,57.85,86.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Extraction of SIFT over the torso region. For better visualization, we display fewer descriptors than the actual number. The left figure shows the location, scale and orientation of the SIFT descriptors in green circles. The right figure depicts the relative positions between the sampling points (red dots), the torso region (white mask) and the torso stick (green bar).</figDesc><graphic coords="6,159.75,271.41,73.45,102.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Comparison of attribute classification under 3 scenarios</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Statistics of the clothing attribute dataset. There are 26 attributes in total, including 23 binary-class attributes (6 for pattern, 11 for color and 6 miscellaneous attributes) and 3 multi-class attributes (sleeve length, neckline shape and clothing category).</figDesc><table><row><cell>Clothing pattern</cell><cell>Solid (1052 / 441), Floral (69 / 1649), Spotted (101 / 1619)</cell></row><row><cell>(Positive / Negative)</cell><cell>Plaid (105 / 1635), Striped (140 / 1534), Graphics (110 / 1668)</cell></row><row><cell>Major color (Positive / Negative)</cell><cell>Red (93 / 1651), Yellow (67 / 1677), Green (83 / 1661), Cyan (90 / 1654) Blue (150 / 1594), Purple (77 / 1667), Brown (168 / 1576), White (466 / 1278) Gray (345 / 1399), Black (620 / 1124), &gt; 2 Colors (203 / 1541)</cell></row><row><cell>Wearing necktie</cell><cell>Yes 211, No 1528</cell></row><row><cell>Collar presence</cell><cell>Yes 895, No 567</cell></row><row><cell>Gender</cell><cell>Male 762, Female 1032</cell></row><row><cell>Wearing scarf</cell><cell>Yes 234, No 1432</cell></row><row><cell>Skin exposure</cell><cell>High 193, Low 1497</cell></row><row><cell>Placket presence</cell><cell>Yes 1159, No 624</cell></row><row><cell>Sleeve length</cell><cell>No sleeve (188), Short sleeve (323), Long sleeve (1270)</cell></row><row><cell>Neckline shape</cell><cell>V-shape (626), Round (465), Others (223)</cell></row><row><cell>Clothing category</cell><cell>Shirt (134), Sweater (88), T-shirt (108), Outerwear (220) Suit (232), Tank Top (62), Dress (260)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Summary of feature extraction</figDesc><table><row><cell>Feature type</cell><cell>Region</cell><cell>Aggregation</cell></row><row><cell>SIFT</cell><cell>Torso</cell><cell>Average pooling</cell></row><row><cell cols="3">Texture descriptor Left upper arm Max pooling</cell></row><row><cell>Color</cell><cell>Right upper arm</cell><cell></cell></row><row><cell cols="2">Skin probability Left lower arm</cell><cell></cell></row><row><cell></cell><cell>Right lower arm</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www.thesartorialist.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://sites.google.com/site/eccv2012clothingattribute/</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>V-shape neckline Tank top (outerwear) Fig. <ref type="figure">8</ref>. For each image, the attribute predictions from independent classifiers are listed. Incorrect attributes are highlighted in red. We describe the abnormal attributes generated by independent classifiers for the above images, from left to right: man in dress, suit with high skin exposure, no sleeves but wearing scarf, T-shirt with placket. By exploring attribute dependencies, the CRF corrects some wrong attribute predictions. Attributes that are changed by the CRF are shown in parentheses, and override the independent classifier result to the left.</p><p>is observed in plaid is much higher than the general public's prior of wearing plaid (the prior of plaid is 6.4% according to our clothing attribute dataset). In our analysis, we regard an attribute class as a dressing style if it is 20% higher than its prior. Of course, this threshold can be flexibly adjusted based on specific application requirements.</p><p>We perform dressing style analysis on both personal and event photos, as shown in Fig. <ref type="figure">9</ref>. Firstly, we analyze the dressing style of Steve Jobs, who is well known for wearing his black turtlenecks. Using 35 photos of Jobs from Flickr, our system summarizes his dressing style as "solid pattern, men's clothing, black color, long sleeves, round neckline, outerwear, wear scarf". Most of the styles predicted by our system are very sensible. The wrong inferences of "outerwear" and "wearing scarf" are not particularly surprising, since Steve Job's high-collar turtlenecks share visual similarities with outerwears and the presence of scarfs. Similarly, our system predicts the dressing style of Mark Zuckerberg as "gray, brown, T-shirt, outerwear", which is in agreement with his actual dressing style.</p><p>Apart from personal clothing style analysis, we can also analyze the dressing style for events. We downloaded 54 western-style wedding photos from Flickr, and consider the dressing styles for men and women using the "gender" attribute predicted by our system. The dressing style for men at wedding is predicted as: "solid pattern, suit, longsleeves, V-shape neckline, wearing necktie, wear scarf, has collar, has placket", while the dressing style for women is "high skin exposure, no sleeves, dress, other neckline shapes (i.e. neither v-shape nor round), white, &gt;2 colors, floral pattern". Most of these predictions agree well with the dressing style of weddings, except "wearing scarf" for men, and "&gt;2 colors" and "floral pattern" for women. These abnormal predictions can be understood, by considering the similarity between men's scarfs and neckties, as well as the visual confusion for including the wedding flowers in women's hands when describing the color and pattern of their dresses. Similar analysis was done  to predict the clothing style of the event of basketball. Using 61 NBA photos, the basketball players' dressing style is predicted as "no sleeves, high skin exposure, other neckline shapes, tank top". Gender Classification. Although there exist clothes that are unisex style, visual appearance of clothes often carries significant information about gender. For example, men are less likely to wear floral-patterned clothes, and women rarely wear a tie. Motivated by this observation, we are interested in combining the gender prediction from clothing information, with traditional gender recognition algorithms that use facial features. We adopt the publicly available gender-from-face implementation of <ref type="bibr" target="#b32">[32]</ref>, which outputs gender probabilities by projecting aligned faces in the Fisherface space. For the clothing-based gender classification, we use the graph potential in Equation 5 as the penalty score for assigning gender to a testing instance. The male and female prediction penalties are simply given by the CRF potentials, by assigning male or female to the "gender" node, while keeping all other nodes unchanged. An RBF-kernel SVM is trained to give gender predictions that combine both the gender probabilities from faces and penalties from clothing.</p><p>We evaluate the gender classification algorithms on our clothing attribute dataset. As shown Table <ref type="table">3</ref>, the combined gender classification algorithm offers a better performance </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">FaceTracer: A Search Engine for Large Collections of Images with Faces</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2008, Part IV</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5305</biblScope>
			<biblScope unit="page" from="340" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Contextual identity recognition in personal photo albums</title>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Gokturk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sumengen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint People, Event, and Location Recognition in Personal Photo Collections Using Cross-Domain Context</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2010, Part I</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Maragos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6311</biblScope>
			<biblScope unit="page" from="243" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Clothing cosegmentation for recognizing people</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Gender recognition from body</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dikmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>ACM Multimedia</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Describing people: Poselet-based attribute classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Articulated human pose estimation and search in (almost) unconstrained still images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marin-Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<idno>272</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>ETH Zurich, D-ITET</publisher>
		</imprint>
		<respStmt>
			<orgName>BIWI</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning visual attributes</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Attribute learning in large-scale datasets</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Interactively building a discriminative vocabulary of nameable attributes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attribute and simile classifiers for face verification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image ranking and retrieval based on multi-attribute queries</title>
		<author>
			<persName><forename type="first">B</forename><surname>Siddiquie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic Attribute Discovery and Characterization from Noisy Web Data</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2010, Part I</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Maragos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6311</biblScope>
			<biblScope unit="page" from="663" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Baby talk: Understanding and generating image descriptions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Every Picture Tells a Story: Generating Sentences from Images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hejrati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2010, Part IV</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Maragos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">6314</biblScope>
			<biblScope unit="page" from="15" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Poselets: Body part detectors trained using 3d human pose annotations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Predicting occupation via human clothing and contexts</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Real-time clothing recognition in surveillance videos</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real-time clothes comparison based on multi-view vision</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Begole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICDSC</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part based models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Robust real-time object detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Grabcut -interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>SIGGRAPH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A statistical approach to texture classification from single images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Intel. Sys. and Tech</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Sun database: Large scale scene recognition from abbey to zoo</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Comparison of graph cuts with belief propagation for stereo, using identical mrf parameters</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Understanding images of groups of people</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
