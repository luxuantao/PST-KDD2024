<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Name Disambiguation in AMiner: Clustering, Maintenance, and Human in the Loop</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yutao</forename><surname>Zhang</surname></persName>
							<email>yt-zhang13@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fanjin</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peiran</forename><surname>Yao</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>jietang@tsinghua.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Name Disambiguation in AMiner: Clustering, Maintenance, and Human in the Loop</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3219819.3219859</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Information systems → Information integration</term>
					<term>Entity resolution</term>
					<term>Clustering</term>
					<term>Name Disambiguation, Entity Resolution, Clustering, Metric Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>AMiner 1 is a free online academic search and mining system, having collected more than 130,000,000 researcher pro les and over 200,000,000 papers from multiple publication databases <ref type="bibr" target="#b33">[25]</ref>.</p><p>In this paper, we present the implementation and deployment of name disambiguation, a core component in AMiner. The problem has been studied for decades but remains largely unsolved. In AMiner, we did a systemic investigation into the problem and propose a comprehensive framework to address the problem. We propose a novel representation learning method by incorporating both global and local information and present an end-to-end cluster size estimation method that is signi cantly better than traditional BIC-based method. To improve accuracy, we involve human annotators into the disambiguation process. We carefully evaluate the proposed framework on real-world large data and experimental results show that the proposed solution achieves clearly better performance (+7-35% in terms of F1-score) than several state-of-the-art methods including GHOST [5], Zhang et al. [33], and Louppe et al. <ref type="bibr" target="#b25">[17]</ref>.</p><p>Finally, the algorithm has been deployed in AMiner to deal with the disambiguation problem at the billion scale, which further demonstrates both e ectiveness and e ciency of the proposed framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>There are 151,671 di erent last names and 5,163 di erent rst names in common use in the United States 2 . This means that if you happen to have a common rst and last names, according to the birthday paradox theory <ref type="bibr" target="#b35">[27]</ref>, the probability that some other researcher in your university shares the same name with you is close to 100%! In practice, it is estimated that the 300 most common male names are used by more than 115 million people (taking about 78.74 percent) in the United States. This poses a big challenge to many personcentric applications such as scienti c literature management and people search engine.</p><p>The problem of disambiguating who is who is referred to as name disambiguation, also named as entity resolution <ref type="bibr" target="#b11">[3,</ref><ref type="bibr" target="#b12">4]</ref>, web appearance disambiguation <ref type="bibr">[1,</ref><ref type="bibr" target="#b19">11]</ref>, name identi cation <ref type="bibr" target="#b24">[16]</ref>, and object distinction <ref type="bibr" target="#b39">[31]</ref> from a broader viewpoint, and has been extensively studied for decades by di erent communities. It has many real applications, for example, matching records between enterprise databases with di erent schema <ref type="bibr" target="#b12">[4]</ref>, aligning protein-protein interaction networks to transfer biological knowledge across di erent species <ref type="bibr" target="#b38">[30]</ref>, constructing canonicalized knowledge base based on facts extracted from texts <ref type="bibr" target="#b14">[6]</ref>, and identifying users across multiple online social networks <ref type="bibr" target="#b42">[34]</ref>. However, despite much work that has been done, the problem remains largely unsolved. Most of the aforementioned methods are more or less ad-hoc and the performance becomes unpredictable when scale up to handle real large data.</p><p>In this paper, employing AMiner as the basis for our experimental data, we explain how we deal with the name ambiguity problem with large data in an online fashion. AMiner is a free online academic search and mining system <ref type="bibr" target="#b33">[25]</ref>. The system extracts researchers' pro les automatically from the Web <ref type="bibr" target="#b32">[24]</ref> and integrates them with published papers after name disambiguation <ref type="bibr" target="#b31">[23]</ref>. To date, it has collected more than 130,000,000 researcher pro les and over 200,000,000 papers from multiple publication databases, with a growth rate of over 500,000 per month. AMiner can be viewed as an author-centric search system, where one can nd domain experts, rising stars, collaborators, reviewers, potential readers, and so on.</p><p>Clearly, name disambiguation is a cornerstone of the system. Dealing with the name ambiguity problem in a large online system poses several unique challenges:</p><p>(1) How to quantify the similarity between entities from di erent data sources? As documents and authors are from di erent sources and may have no overlapping information, it is necessary to design a principled way to quantify the similarity between (di erent) entities. (2) How to determine the number of persons with the same name? There is no answer for question like how many people having the same name with you. However, this is usually a pre-speci ed parameter for existing name disambiguation algorithms (using clustering). (3) How to integrate data continuously? To ensure user experience, we need to minimize the delay between the arrival of a document and the time it displays in its authors' pro les and maintain the consistency after each update. (4) How to involve human e orts in the loop? A completely automatic disambiguation system without any human interactions is far from su cient. It is necessary to involve human e orts in the loop to achieve high disambiguation accuracy.</p><p>To this end, in AMiner, we design a uni ed framework to address the above challenges. For quantifying the similarity, we propose a global metric and local linkage learning algorithm, which projects each entity into a low-dimensional latent common space. This o ers a way to directly compute the similarity of entities from di erent sources. For determining the number of persons who share the same name, we propose an end-to-end model that directly estimates the number of persons (clusters) in a dataset using a recurrent neural network. For involving human into disambiguation, we formally de ne six potential feedbacks from users/annotators. The feedbacks are then incorporated into di erent components of the framework to improve the disambiguation accuracy.</p><p>We evaluate the proposed framework on real-world large data. Our experiments show that the proposed solution achieves significantly better performance than several state-of-the-art methods including GHOST <ref type="bibr" target="#b41">[33]</ref>, Zhang et al. <ref type="bibr" target="#b41">[33]</ref>, and Louppe et al. <ref type="bibr" target="#b25">[17]</ref> (+7-35% in terms of F1-score). The automatically estimated number of persons by our proposed model is close to the actual number.</p><p>The framework has been deployed in AMiner to deal with the disambiguation problem at the billion scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Name disambiguation is usually viewed as a clustering problem [1, <ref type="bibr" target="#b17">9,</ref><ref type="bibr" target="#b18">10,</ref><ref type="bibr" target="#b20">12,</ref><ref type="bibr" target="#b30">22,</ref><ref type="bibr" target="#b40">32]</ref>. As many other clustering tasks, there are two main challenges that need to be addressed how to quantify the similarity and how to determine cluster size. Most existing literature mainly focuses on addressing the rst sub-problem, while ignore the second. The state-of-the-art solutions for name disambiguation problem can be divided into two categories: feature-based and linkage-based.</p><p>Feature-based methods. Feature-based methods <ref type="bibr" target="#b16">[8,</ref><ref type="bibr" target="#b18">10,</ref><ref type="bibr" target="#b30">22,</ref><ref type="bibr" target="#b40">32]</ref> leverage supervised learning method to learn a pairwise distance function between documents based on their feature vectors. Huang et al. <ref type="bibr" target="#b18">[10]</ref> rst use blocking <ref type="bibr" target="#b30">[22]</ref> technique to group candidate documents with similar names together. Then it learns distance between documents by an e cient Support Vector Machines (SVM) and nally employs DBSCAN to cluster documents. Yoshida et al. <ref type="bibr" target="#b40">[32]</ref> propose a two-stage clustering method to learn better feature representation via the rst clustering step. Han et al. <ref type="bibr" target="#b16">[8]</ref> present supervised disambiguation methods based on SVM and Naïve Bayes. Moreover, Louppe et al. <ref type="bibr" target="#b25">[17]</ref> use a classi er to learn pairwise similarity and perform semi-supervised hierarchical clustering to generate results.</p><p>Linkage-based methods. There are also several linkage/graphbased methods [1, <ref type="bibr" target="#b17">9,</ref><ref type="bibr" target="#b20">12]</ref> to deal with name disambiguation. These methods are capable of utilizing graph topology and aggregate information from neighbors. GHOST <ref type="bibr" target="#b13">[5]</ref> builds document graph for each ambiguous name by co-authorship only. It uses carefully-designed similarity function and uses a nity propagation algorithm to generate clustering results. Tang et al. <ref type="bibr" target="#b31">[23]</ref> employ Hidden Markov Random Fields to model node features and edge features in a uni ed probabilistic framework. Zhang et al. <ref type="bibr" target="#b41">[33]</ref> solve this problem by learning graph embedding from three constructed graphs based on document similarity and coauthor relationship.</p><p>Our method proposed in this paper combines the advantages of above two methods by learning a global embedding using supervised metric learning and re ning the embedding using local linkage structures.</p><p>Cluster size estimation. The second challenge is how to determine the number K of clusters (name identities). Most previous literature assumes the number is known beforehand and ignores this problem in the solution. Several existing works claim to use clustering methods such as DBSCAN to avoid specifying K; however several density-based hyperparameters are still needed to be pre-speci ed. Tang et al. <ref type="bibr" target="#b31">[23]</ref> use a variation of X-means <ref type="bibr" target="#b27">[19]</ref> algorithm to iteratively estimates the optimal K by measuring clustering quality based on Bayesian Information Criterion (BIC). However, in our empirical study on large data, we found that the BIC-based methods are inclined to merge clusters together, thus result in low accuracy. In this paper, we seek to learn an end-to-end model that takes a set of document embeddings as input and directly predict the number of clusters.</p><p>There is also a thread of research <ref type="bibr" target="#b29">[21,</ref><ref type="bibr" target="#b37">29]</ref> solving this problem in a hierarchical tree model instead of making pairwise comparisons using clustering. For example, Wick et al. <ref type="bibr" target="#b37">[29]</ref> build a discriminative hierarchical factor graph model for coreference, and the hierarchical model can also address the scalability issue. Furthermore, human edits can be incorporated <ref type="bibr" target="#b36">[28]</ref> by running MCMC inference on the hierarchical model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>In this section, we present the formulation of the problem with preliminaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Let a be a given name reference, and D a = {D a 1 , D a 2 , ..., D a N } be a set of N documents associated with the author name a. We call D a as the candidate set of a. Each document D a i ∈ D a is represented by a set of features D a i = {x 1 , x 2 , ...} including title, abstracts, coauthor names, venue names, etc. We use I(D a i ) to denote the identity (corresponding real-world person) of D a i . Thus if D a i and D a j are authored by the same author, we have I(D a i ) = I(D a j ). Given this, we de ne the problem of author disambiguation as follows.</p><p>De nition 3.1. Name Disambiguation. The task of author disambiguation is to nd a function Φ to partition D a into a set of disjoint clusters, i.e., Φ(D a ) → C a , where C a = {C a 1 , C a 2 , ..., C a K }, such that each cluster only contains documents of the same identityi.e., I(D a i ) = I(D a j ), ∀(D a i , D a j ) ∈ C a k ×C a k , and di erent clusters contains documents of di erent identities-i.e., I(D a i )</p><formula xml:id="formula_0">I(D a j ), ∀(D a i , D a j ) ∈ C a k × C a k , k k .</formula><p>We call C a a disambiguation (clustering) solution of D a for name a. We omit the superscript a in the following description if there is no ambiguity. In general, the problem is very di cult, if there is not any supervision information. Human annotators (or users) can provide useful constraints. We mainly consider two kinds of constraints: identity constraints S I and pairwise constraints S P , with S = S I ∪ S P . The identity constraint (D i , C k , ik ∈ {0, 1}) indicates that D i should belong to (or not belong to) the cluster C k</p><formula xml:id="formula_1">(D i , C k , 0) ∈ S I → D i C k , (D i , C k , 1) ∈ S I → D i ∈ C k .</formula><p>The pairwise constraint (D i , D j , i j ∈ {0, 1}) indicates that the two documents D i and D j should (or not) belong to the same author.</p><formula xml:id="formula_2">(D i , D j , 0) ∈ S P → I(D i ) I(D j ), (D i , D j , 1) ∈ S P → I(D i ) = I(D j ).</formula><p>Please note that identity constraints imply pairwise constraints since</p><formula xml:id="formula_3">         (D i , C k , 1) ∈ S I ∧ (D j , C k , 1) ∈ S I → (D i , D j , 1) ∈ S P , (D i , C k , 1) ∈ S I ∧ (D j , C k , 1) ∈ S I ∧ C k C k → (D i , D j , 0) ∈ S P , (D i , C k , 1) ∈ S I ∧ (D j , C k , 0) ∈ S I → (D i , D j , 0) ∈ S P .</formula><p>(1)</p><p>For simplicity, we assume there is no logical con ict in S.  </p><formula xml:id="formula_4">f f f D i D i+ D i x i x i+ x i y i y i+ y i (y i , y i ) (y i , y i+ ) Triplet Loss</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">AMiner</head><p>AMiner is a free online academic search and mining system and also the second generation of ArnetMiner <ref type="bibr" target="#b33">[25]</ref>, with the emphasis to o er approaches to gain a deeper understanding of the large and heterogeneous information networks (authors, papers, venues, topics, etc.) formed in the scienti c literature data. The system has been in operation since 2006 and has attracted more than 8,000,000 independent IP accesses from over 220 countries/regions. It has become strategic partner of Microsoft Academic Search and the o cial content provider of Sogou 3 Scholar.</p><p>AMiner automatically extracts researchers' pro les from the Web and integrates the publication data from online databases such as DBLP, ACM Digital Library, CiteSeer, and SCI. In the integration, we inevitably have to face the challenge of name ambiguity. The problem is becoming more and more challenging as the volume of publication data is growing at a rapid rate, with more than 500,000 new documents needed to be integrated into the system per month. In this paper, we systematically discuss how to solve this problem at the billion scale and in an incremental fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">FRAMEWORK</head><p>In this section, we discuss the design and implementation of our deployed solution to author disambiguation problem in detail. We rst propose our representation learning method and introduce how to estimate cluster size in the disambiguation process. To deploy the algorithm in a large scale online system, we need to minimize the delay of the integration and maintain the consistency after each update. We discuss our solution for continuous integration in Section 4.3 and present how to leverage human annotations in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Representation Learning</head><p>In order to e ectively quantify the similarity between di erent documents, we rst transform documents into an embedding space such that D i is close to D j if I(D i ) = I(D j ). There are two ways to embed the data. We can either learn a global embedding function that encodes all the documents in a uni ed space or learn a local embedding function for each candidate set separately. In our framework, we rst learn a supervised global embedding function, then re ne the global embeddings for each candidate set based on the local contexts. We rst present the implementation of our method and then discuss the intuition of our design in Section 4.5.</p><p>4.1.1 Global Metric Learning. Input document D i is represented as a varied-length set of features D i = {x 1 , x 2 , ...}, where features are words in title and abstract, coauthor names, venue names, a liations, etc. Each feature is a one-hot vector. We rst transform the feature set into a continuous low-dimensional space. Inspired by the unsupervised representation learning techniques, we use Word2Vec <ref type="bibr" target="#b26">[18]</ref> to obtain an embedding xn ∈ R d for each feature x n . We de ne the feature embedding of document D i as x i = x n ∈D i α n xn , which is a weighted sum of the embedding of each feature in D i . In the equation, α n is the inverted document frequency of feature x n and x i captures the correlations between features based on the co-occurrence statistics within each individual documents <ref type="bibr" target="#b26">[18]</ref>. However, the ability of x i to distinguish documents with di erent authors is limited. Thus, we seek to leverage labeled data to ne-tune the embedding.</p><p>Contrastive Loss. Given a set of constraints</p><formula xml:id="formula_5">S P = {(D i , D j , )} where = 1 if I(D i ) = I(D j ) (i.e. positive pairs) and I(D i ) I(D j ) if</formula><p>= 0 (i.e. negative pairs). The idea of metric learning is to enforce positive pairs to be close in the embedding space and negative pairs to be far away. To this end, we introduce another embedding function f :</p><formula xml:id="formula_6">x i ∈ R d → R d f , and y i = f (x i ) as a new embedding of D i . A straightforward idea is to optimize the contrastive loss L f = (D i , D j , )∈S P L f (D i , D j , ) where L f (D i , D j , ) = δ (y i , y j ), = 1 max{0, m − δ (y i , y j )}, = 0 where δ ( 1 , 2 ) = || 1 − 2 || 2</formula><p>is the euclidean distance in the embedding space, m &gt; 0 is a margin. L f encourages all documents of the same author to be projected onto a single point in the embedding space which can be potentially troublesome since a single author might work on di erent topics and collaborate with di erent communities. Thus, we adopt Learning to Rank and optimize a triplet loss function.</p><formula xml:id="formula_7">Triplet Loss. Let (D i , D i+ , D i− ) be a triplet where I(D i ) = I(D i+ ) and I(D i ) I(D i− ), we have δ (y i , y i+ ) + m &lt; δ (y i , y i− ), ∀(D i , D i+ , D i− ) ∈ T, (<label>2</label></formula><formula xml:id="formula_8">)</formula><p>where T is the set of all possible triplets in the training set, m is a margin enforced between positive pairs and negative pairs. The objective L f is then replaced as</p><formula xml:id="formula_9">L f = (D i , D i + , D i − )∈ T max{0, δ (y i , y i+ ) − δ (y i , y i− ) + m}.<label>(3)</label></formula><p>Instead of projecting to a single point, triplet loss enables documents with the same identity to reside on a manifold <ref type="bibr" target="#b28">[20]</ref>, and at the same time maintain a distance from other documents. The architecture of global metric learning with triplet loss is shown in Figure <ref type="figure" target="#fig_1">2</ref>.</p><formula xml:id="formula_10">Y 2 R N ⇥df Z 2 R N ⇥dg Encoder Decoder g2 A 2 R N ⇥N Ã 2 R N ⇥N Local Embedding Reconstruction Error G = (D, E) g1</formula><p>We call {y i } global embeddings because documents in di erent candidate sets are embedded in a uni ed space. However, since the clustering is conducted separately for each name reference, local contexts within a candidate set can be leveraged to further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Local Linkage</head><p>Learning. We seek to re ne the global embeddings by leveraging ne-grained information within a candidate set. Since most candidate sets do not have any labeled data, an unsupervised method is preferred in this step. Following the idea of linkage based disambiguation methods proposed in [1, 9, 12, 33], we construct a graph for each candidate set.</p><p>De nition 4.1. Local Linkage Graph. For a given name reference a, we construct a local linkage graph G a = (D a , E a ), where D a = {D a i } is the set of documents authored by a person named a, E a = {(D a i , D a j )} is a set of edges capturing the similarity between the documents.</p><p>We measure the similarity of two documents based on the common features shared by the two documents. Let the common feature set of D i and D j be the intersection between their feature sets D i ∩ D j , we de ne the linkage weight W (D i , D j ) = x ∈D i ∩D j w x between D i and D j as a weighted sum of all the common features, where w x is the weight of feature x. The weights {w x } can be learned by a supervised model, but for simplicity, we de ne feature weight as its inverted document frequency. We construct an edge in G between D i and D j if W (D i , D j ) is above a threshold 4 .</p><p>The intuition for constructing G is that D i and D j are likely to have the same identity if they share a lot of unique features (i.e. W (D i , D j ) is large). Thus, we try to leverage the structure of G to improve the global embedding.We use an unsupervised autoencoder architecture <ref type="bibr" target="#b22">[14,</ref><ref type="bibr" target="#b34">26]</ref> to learn from the local linkage graph.</p><p>Graph Auto-encoder. Let Y = [y 1 , ..., y N ] denotes an embedding matrix of D generated by f , A ∈ R N ×N is the adjacency matrix of G. A graph auto-encoder is comprised of a node encoder model Z = 1 (Y, A) and an edge decoder model Ã = 2 (Z), where Z = [z 1 , ..., z N ] is a node embedding matrix, Ã is a predicted 4 We empirically set the threshold as 10. adjacency matrix. The objective is to minimize the reconstruction error between the predicted Ã and the original adjacency matrix A.</p><p>We instantiate 1 as a two-layer graph convolution network (GCN) <ref type="bibr" target="#b21">[13]</ref> due to its e ectiveness for modeling networked data:</p><formula xml:id="formula_11">1 (Y, A) = AReLU( AYW 0 )W 1 ,<label>(4)</label></formula><p>where A is symmetrically normalized adjacency matrix (i.e. A = D 1 2 AD − 1 2 , D is the degree matrix of G), ReLU(•) = max(0, •), W 0 and W 1 are the parameters of the rst and second layer respectively.</p><p>The decoder 2 is de ned as</p><formula xml:id="formula_12">2 (Z) = sigmoid(Z Z)<label>(5)</label></formula><p>Thus, the probability of predicting the existence of an edge between D i and D j is given by</p><formula xml:id="formula_13">p( Ãij = 1|z i , z j ) = sigmoid(z i z j )<label>(6)</label></formula><p>The objective is then de ned as minimizing the cross entropy:</p><formula xml:id="formula_14">L = − D i , D j ∈ D A i j log p( Ãij )<label>(7)</label></formula><p>We take the latent variables Z = [z 1 , ..., z N ] as new document embeddings. Z incorporates information from both global and local context. In our implementation, we use a variational version of graph auto-encoder <ref type="bibr" target="#b22">[14]</ref> by assuming Z is generated from a latent Gaussian distribution, hence, Equation ( <ref type="formula" target="#formula_14">7</ref>) is extended as the sum of a reconstruction loss and the KL divergence between the learned latent distribution and the prior distribution. Our empirical results show that variational graph auto-encoder outperforms the original one. The binary adjacency matrix A in the encoder model can be replaced with the weight matrix calculated by W (D i , D j ). However, in our experiments, we found that it leads to slower convergence and does not signi cantly improve the performance.</p><p>The local linkage learning model is unsupervised since we construct the local linkage graph G purely based on the overlap of features between documents. We can re ne G based on the pairwise constraints S P to support semi-supervised learning (Cf. § 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Cluster Size Estimation</head><p>Based on the learned embeddings, the nal partition of a candidate set is determined by a clustering algorithm. We use hierarchical agglomerative clustering algorithm (HAC) as our main clustering method. One common issue for clustering problems is how to estimate the number of clusters K. There are existing clustering methods such as DBSCAN that do not take an explicit K as input, but it requires preset hyperparameters to determine density which changes signi cantly in di erent candidate sets. The most popular way to determine the number of clusters is X-means <ref type="bibr" target="#b27">[19]</ref>, which iteratively splitting the centroids and searching for an optimal K based on the quality of the proposed clustering. There are mainly two drawbacks of X-means algorithm for our task. First, the clustering quality is scored based on a prede ned measurement such as Bayesian Information Criterion which cannot handle a complex mixture of data (hence tend to over merge the data when the number of clusters is large) in high dimension <ref type="bibr" target="#b15">[7]</ref>. Second, it involves an iterative trial of potential splits which is not e cient enough on large datasets. To overcome the above issues, we seek to learn an end-to-end model h(D) → R that takes a set of documents as input and directly estimates the number of di erent identities in the candidate set.</p><p>Recent progress in deep learning community shows the capacity of recurrent neural networks in modeling discrete sequences and sets of data. Bello et al. <ref type="bibr" target="#b10">[2]</ref> shows that RNN is able to act as an encoder in the solution of combinatorial optimization problems such as TSP. Inspired by these works, we adopt RNN as an encoder and try to map a set of embedding vectors to the true number of clusters in the set. To achieve this goal, there are two challenges. First, the size of a candidate set varies in a wide range from one to tens of thousands. Although RNN is able to handle variable sized input via padding and truncation, those operations may introduce unwanted bias. Second, it is hard to construct a training set for this problem. It is infeasible to manually label a large number of candidate sets with true cluster size. To resolve these issues, we propose a sampling strategy to construct a pseudo-training set.</p><p>Let C = {C 1 , C 2 , ...} be a set of clean clusters (as discussed in Section 4.4) where each cluster only contains documents of a single author. The clusters may be from di erent candidate sets. For each t th training step, we rst uniformly sample the number of clusters</p><formula xml:id="formula_15">K t from [K min , K max ]. Then, we sample K t clusters from C to construct a pseudo-candidate set C t . Let D C t = C i ∈C t {D ∈ C i }</formula><p>denotes all the documents within C and z denotes a xed number of sample size. We then sample a set of z documents D t from D C t with replacement. We note that D t may contain duplicate documents and the order of D t is arbitrary. In this way, we can construct in nite amount of pseudo-training examples from C. We use a neural network architecture h(D t ) → R with a bi-directional LSTM as encoder and a one-dimensional fully-connected layer as decoder. The model takes the raw feature embedding x i of each document D i ∈ D t as input. We optimize the Mean Squared Logarithmic Error as follows:</p><formula xml:id="formula_16">L h = 1 N a t =1 [log(1 + h(D t )) − log(1 + K t )] 2<label>(8)</label></formula><p>Algorithm 1 summarizes the pseudo-training data generation strategy for cluster size estimation described above. Our experimental results (in Section 5.6) suggest that in our task the proposed method is signi cantly better than traditional X-means approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Continuous Integration</head><p>Our system integrates new scholarly documents in a streaming fashion. Based on our statistics, there are more than 500,000 new documents being integrated into the system every month. Thus, it is crucial for us the e ciently handle the ever-growing data volume. Although e ciency is an important objective for designing our disambiguation method, it is still impossible for us to re-compute the clustering from scratch for every single new document. The main time cost comes from local linkage learning, clustering and the IO overhead of retrieving all the documents associated with a candidate set from the database. Thus, instead, we maintain a ALGORITHM 1: Pseudo-training data generation strategy for cluster size estimation. (D t , K t ) is a training example for RNN model h(D) → R.</p><p>Input: Clean clusters C, K min , K max , sample size z, step t;</p><formula xml:id="formula_17">Output: Pseudo-training example (D t , K t ); K t ← Sample from [K min , K max ]; C t ← Sample K t clusters from C; D t ← Sample z documents from C i ∈C t {D ∈ C i } with replacement; return (D t , K t );</formula><p>priority queue of every candidate sets and update each candidate set in an iterative manner. However, a full update of all the candidate sets usually takes weeks which is not acceptable for an online system.</p><p>Real-time Update. To overcome this issue, each new document D * will rst be greedily assigned to an existing pro le in the following way. We search for a set of pro les Hits(D * ) = {C k } based on the author name and a liation using an inverted index of all the pro les in the system where each pro le is corresponding to a cluster of documents. If there are multiple hits, we retrieve the global embeddings {y i } of documents {D i ∈ C k } and construct a local kNN classi er to nd the best assignment where each C k is a class and {(y i , C k )|D i ∈ C k } is a set of data points (documents) with their labels (corresponding pro les). This simple strategy enables us to update the documents almost in real-time. Although the assignment may be sub-optimal, it will be corrected by the next iteration of clustering re-computation.</p><p>Data Consistency. Another issue we are facing with in an online system is how to maintain the consistency between each iteration of the updates. Due to the unsupervised nature of clustering, the index of each cluster is anonymous. Thus, after re-computing the clustering, the index of clusters might be inconsistent with the former ones. This is critical for an online system as each cluster is associated with an individual pro le, herein a change may signi cantly harm user experience. Hence, after obtaining a new clustering C t +1 , we search for an optimal matching between it and its previous version C t . We de ne the optimal matching MC t C t +1 as:</p><formula xml:id="formula_18">MC t C t +1 = arg max M C t C t +1 (C i ,C j )∈M C t C t +1 sim(C i , C j ),<label>(9)</label></formula><p>subject to one-to-one matching constraint, where sim(C i , C j ) is de ned as the Jaccard coe cient. An optimal one-to-one mapping is found using Kuhn-Munkres algorithm <ref type="bibr" target="#b23">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Human in the Loop</head><p>The automatically generated clusterings are far from being perfect. Thus, to improve the accuracy we need to involve human in the loop of author disambiguation. In our system, we allow both users and professional annotators to provide feedback based on the clustering results. The supported feedbacks include:</p><p>(1) Delete: removing a document D i from a pro le C k and adding an identity constraint (D i , C k , 0) into S I .</p><p>(2) Insert: adding a document D i in to a pro le C k and adding an identity constrain (D i , C k , 1) into S I . (3) Split: annotating a pro le C k as over-merged and requesting clustering within the pro le. (4) Merge: merge all the documents from C k to C k .</p><p>(5) Create: creating an empty pro le C k . (6) Con rm: label a pro le C k as a clean cluster and adding</p><formula xml:id="formula_19">{(D i , C k , 1)|D i ∈ C k } into S I .</formula><p>To leverage user feedbacks in the algorithm, we translate the identity constraints S I to pairwise constraints S P according to Equation (1). Pairwise constraints are used in both of the two embedding learning phases.</p><p>In global metric learning phrase (Section 4.1.1), the training set T is generated from S P in the following way: 1) Sample a constraint (D i , D j , i j ) from S P . 2) If i j = 0, sample a constraint (D i , D l , 1) from S P and generate a triplet (D i , D l , D j ). 3) Otherwise, sample a random document D l from the whole document space and generate a triplet (D i , D j , D l ).</p><p>In local linkage learning phrase (Section 4.1.2), we re ne the local linkage graph G = (D, E) based on S P . We add an edge</p><formula xml:id="formula_20">(D i , D j ) into E if (D i , D j , 1) ∈ S P ∧ (D i , D j ) E ∧ (D i , D j ) ∈ D × D,</formula><formula xml:id="formula_21">and remove edge (D i , D j ) from E if (D i , D j , 0) ∈ S P ∧ (D i , D j ) ∈ E.</formula><p>Identity constraints S I are also used to generate pseudo-training set for clustering size estimation described in Section 4.2. The clean cluster set C is constructed by:</p><formula xml:id="formula_22">C = {C k |∃(•, C k , 1) ∈ S I }, where C k = {D i |(D i , C k , 1) ∈ S I }</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Discussion</head><p>The global metric learning model f is inductive in the sense that it is able to be employed by a new document that has not been seen in the training time, while the local linkage model is transductive, meaning that it is not able to generalize to unseen instances.</p><p>Empirically, with limited labeled data, the local model performs better since it leverages ne-grain information within a candidate set. However, the global model is particularly useful in our system for two reasons. First, most candidate sets do not have any labeled data since the human annotations are concentrated on a small fraction of the pro les. The global model is able to transfer external supervision from other candidate sets to improve the performance of the local model. Second, as we described in Section 4.3, constructing and learning from the local linkage graph is time-consuming. It is infeasible for us to re-train the local model for every single new document. The inductive nature of the global model enables us to instantly determine a fairly good temporary assignment for an incoming document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>All codes and data used in this work are publicly available.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data Summarization</head><p>To systematically evaluate the proposed method, we construct a benchmark based on AMiner. We sampled 100 author names from a well-labeled subset of AMiner database. The benchmark consists of 70,258 documents from 12,798 authors. The labeling process was carried out based on the publication lists on the authors' homepages and the a liations, e-mail addresses in the web databases (e.g. Scopus, ACM Digital Library, etc). We applied "majority voting" to cope with the disagreements in the annotation process. Comparing to existing benchmarks for name disambiguation 6,7 , our benchmark is signi cantly larger (in terms of the number of documents) and more challenging (since each candidate set contains much more clusters).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baseline Comparison Methods</head><p>To validate the performance of our proposed approach, we compare it against three state-of-the-art name disambiguation methods. For a fair comparison, the number of clusters is set to the true value.</p><p>Zhang et al. <ref type="bibr" target="#b41">[33]</ref>: This method constructs three local graphs for a candidate set based on coauthors and document similarity. A graph embedding is learned for each candidate set by sampling triplets from the graph. The nal result is generated by agglomerative hierarchical clustering.</p><p>GHOST <ref type="bibr" target="#b13">[5]</ref>: The second method is purely based on coauthor names. For each query name, it constructs a graph by collapsing all the coauthors with identical names to one single node. The distance between two nodes is measured based on the number of valid paths. The nal clustering result is generated by a nity propagation algorithm.</p><p>Louppe et al. <ref type="bibr" target="#b25">[17]</ref>: This method rst trains a pairwise distance function base on a set of carefully designed similarity features. A semi-supervised HAC algorithm is used to determine clusters. 6 https://aminer.org/disambiguation 7 http://clgiles.ist.psu.edu/data/ Rule: Rule-based method constructs local linkage graphs by connecting two documents when their co-authors, a liations or publication venues are strictly matched. The clustering is obtained by simply partitioning the graph into connected components. Our method is indicated by AMiner. In order to analyze the contribution of each component in our solution, we also present our performance at di erent stages in Table <ref type="table" target="#tab_4">2</ref>.</p><p>Embedding: This is the clustering result based on the original feature space, where each document D i is represented as its feature embedding x i de ned in Section 4.1.1.</p><p>Global: This shows the result after global metric learning (Section 4.1.1). The embedding space is ne-tuned by optimizing a triplet loss. We use the global embeddings {y i } as the of clustering.</p><p>Local: This method uses orthogonal one-hot vectors as the input node features of local linkage learning (Section 4.1.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Results</head><p>Table <ref type="table" target="#tab_2">1</ref> shows the performance of di erent disambiguation methods on some sampled names. We use pairwise Precision, Recall, and F1score to evaluate our method against the alternative ones. A macro averaged score of each metric is calculated according to all test names. A holdout sample of the dataset is used for training.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Contribution Analysis</head><p>In In our system, a higher precision is preferred for the sake of user experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Embedding Analysis</head><p>In order to further evaluate the e ect of global metric learning and local linkage learning, we project the embedding generated at di erent stages into 2-dimensional Euclidean space which can be easily visualized. Figure <ref type="figure" target="#fig_4">4</ref> shows the t-SNE plot of the embedding of a candidate set where each point is a document. In the rst row, the color of a point denotes the corresponding ground truth cluster, while in the second row, the color denotes the cluster predicted by hierarchical agglomerative clustering. Figure <ref type="figure" target="#fig_4">4a</ref> shows the original feature embedding space without global metric learning and local linkage learning. We can easily observe that points from di erent clusters are not well separated in Figure <ref type="figure" target="#fig_4">4a</ref>, where there are a signi cant amount of overlaps between di erent clusters. The dashed black ellipses circle the points of a green cluster which are scattered over a wide area in the space. Figure <ref type="figure" target="#fig_4">4b</ref> shows the embedding improved by global metric learning where the green points form two clusters in the embedding space (represented by two dashed black ellipses). Figure <ref type="figure" target="#fig_4">4c</ref> shows the nal embedding after local linkage learning. We can see that the green cluster is well separated from other points in the embedding space.</p><p>Figure <ref type="figure" target="#fig_4">4d</ref>, 4e and 4f demonstrate the clustering results generated by K-means algorithm. We achieve an F1 of 61.11% with both global  metric learning and local linkage learning which signi cantly outperforms the result with the original feature embedding and global metric learning only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Cluster Size Estimation</head><p>We compare our proposed cluster size estimation method with the traditional X-means <ref type="bibr" target="#b27">[19]</ref> algorithm with Bayesian Information Criterion as the measurement of clustering quality. We set the K min = 1 and K max = 300. Our RNN is trained according to the pseudo-training data sampling method described in Section 4.2.</p><p>Half of the labeled candidate sets are used for training and the other half are used for evaluation. There is no overlap between training and test data. We evaluate the performance of both methods using Root Mean Squared Logarithmic Error. Table <ref type="table" target="#tab_6">3</ref> summarizes the results. From Table <ref type="table" target="#tab_6">3</ref> our method achieves a RMSLE of 0.2493 which signi cantly outperforms the baseline. To provide a more comprehensive view of the performance, we also demonstrate results of some individual candidate sets in Table <ref type="table" target="#tab_6">3</ref> comparing to the actual number of clusters. It is easy to see that the maximum estimation from X -means is around 10 which is not usable since there can be hundreds of clusters in each candidate set. Our method provides an estimation within a reasonable error range for most cases.</p><p>To further illustrate the e ectiveness of RNN encoder, we test the same method except changing the RNN to a DNN with two fully connected layers that take the summation of the feature embeddings as input. The result is labeled as Regression in Table <ref type="table" target="#tab_6">3</ref>. We can see that without RNN encoder, the estimation is almost random. This result veri es the ability of RNN to encode a discrete set of data points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">E ect of Constraints</head><p>In this experiment, we show the e ect of adding labeled constraints into local linkage learning. As we described in Section 4.4, pairwise constraints S P can be leveraged in local linkage learning by re ning the local linkage graph G. The experiment is conducted by sampling pairs of documents within a candidate set and adding them into the pairwise constraint set based on their ground truth clusters. Figure <ref type="figure" target="#fig_5">5</ref> shows the performance by labeling di erent amount of pairs. The X-axis is the proportion of pairs that are labeled and the Y-axis is the corresponding F1-score. We can see that the performance improves signi cantly with a few pairs being labeled. Active learning can be leveraged here to improve the labeling e ciency, which we intend to explore as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>To conclude, in this paper, we propose a novel representation learning framework by leveraging both global supervision and local contexts. Experimental results verify the advantage of our method over state-of-the-art name disambiguation methods. We address the problem of estimating cluster size by learning a recurrent neural network on sampled pseudo-training set. We discuss the lessons learned from deploying the name disambiguation framework in a large-scale online system and present how to leverage human annotations to improve disambiguation accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An overview of the author disambiguation framework in AMiner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The architecture of global metric learning with triplet loss. Each training example is a triplet consisting of an anchor D i , a positive sample D i+ and a negative sample D i− . The objective is to distinguish positive pairs (D i , D i+ ) from negative pairs (D i , D i+ ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>3 http://sogou.com, the second largest search engine in China.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The architecture of local linkage learning with graph auto-encoder. G is a local linkage graph, Y is a feature matrix, A is an adjacency matrix, Z is a learned latent local embedding matrix, Ã is a reconstructed adjacency matrix. The objective is to minimize the reconstruction error between A and Ã.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: t-SNE Visualization of embedding spaces on a candidate set. Each color in (a), (b), (c) denotes an individual ground truth cluster, while each color in (d), (e), (f) denotes a predicted cluster by hierarchical agglomerative clustering. Emb indicates the original feature embedding. Global and Local represent the use of global metric learning and local linkage learning respectively. The dashed black ellipses in (a), (b), (c) circle the points of the same ground truth cluster.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The performance of with pairwise constraints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>5  </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results of Author Name Disambiguation.</figDesc><table><row><cell></cell><cell></cell><cell>AMiner</cell><cell></cell><cell></cell><cell>Zhang et al.</cell><cell></cell><cell></cell><cell>GHOST</cell><cell></cell><cell cols="2">Louppe et al.</cell><cell></cell><cell></cell><cell>Rule</cell><cell></cell></row><row><cell>Name</cell><cell>Prec</cell><cell>Rec</cell><cell>F1</cell><cell>Prec</cell><cell>Rec</cell><cell>F1</cell><cell>Prec</cell><cell>Rec</cell><cell>F1</cell><cell>Prec</cell><cell>Rec</cell><cell>F1</cell><cell>Prec</cell><cell>Rec</cell><cell>F1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Louppe et al. use carefully designed features to learn a supervised pairwise similarity function, instead, we avoid hand-crafted features by learning from raw input features. Both GHOST and Zhang et al. leverage the structure of coauthor graphs. GHOST directly partitions the coauthor graph by a nity propagation. Zhang et al. transforms coauthor graphs into local embeddings by sampling edges from the graph, which is closely related to our local linkage learning method. However, by incorporating both global supervision and local linkage structure, our method (AMiner) outperforms the baselines in terms of F1-score (+7.93% over Zhang et al., +34.96% over GHOST and +7.43% over Louppe et al. relatively). We also tested a baseline method that directly partitions local linkage graphs into</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Contribution of Each Component.</figDesc><table><row><cell></cell><cell>Pre.</cell><cell>Rec.</cell><cell>F1</cell></row><row><cell cols="4">Embedding 66.85 42.04 49.79</cell></row><row><cell>Global</cell><cell cols="3">68.40 47.42 54.56</cell></row><row><cell>Local</cell><cell cols="3">68.97 67.68 66.55</cell></row><row><cell>Overall</cell><cell cols="3">77.96 63.03 67.79</cell></row><row><cell cols="4">connected components which yields low precision, hence veri es</cell></row><row><cell cols="3">the e ectiveness of local linkage learning.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>, some incremental results of our method are presented.</cell></row><row><cell>Global outperforms Embedding by +9.58% which veri es the e ec-</cell></row><row><cell>tiveness of leveraging global supervision. Local achieves a much</cell></row><row><cell>better performance than Global (+18.01% in terms of F1) which</cell></row><row><cell>shows the advantage of the local model over the global model with</cell></row><row><cell>limited training data. AMiner outperforms Local by +1.86% in terms</cell></row><row><cell>of F1-score and +13.03% in terms of Precision which veri es the</cell></row><row><cell>e ectiveness of incorporating global supervision in local models.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Results of Clustering Size Estimation.</figDesc><table><row><cell></cell><cell>Actual</cell><cell>RNN</cell><cell cols="2">Regression X-means</cell></row><row><cell>RMSLE</cell><cell>-</cell><cell>0.2493</cell><cell>1.6006</cell><cell>2.1065</cell></row><row><cell>Song Chen</cell><cell>125</cell><cell>101.39</cell><cell>173.80</cell><cell>10</cell></row><row><cell>Jian Du</cell><cell>87</cell><cell>62.89</cell><cell>110.21</cell><cell>5</cell></row><row><cell>Fosong Wang</cell><cell>4</cell><cell>5.71</cell><cell>184.75</cell><cell>5</cell></row><row><cell>J Yu</cell><cell>346</cell><cell>74.06</cell><cell>24.92</cell><cell>7</cell></row><row><cell>Yang Shen</cell><cell>157</cell><cell>153.77</cell><cell>89.52</cell><cell>7</cell></row><row><cell>Xiaobing Luo</cell><cell>13</cell><cell>11.01</cell><cell>143.44</cell><cell>3</cell></row><row><cell>Jian Feng</cell><cell>102</cell><cell>149.73</cell><cell>113.88</cell><cell>8</cell></row><row><cell>Lu Han</cell><cell>129</cell><cell>114.51</cell><cell>173.16</cell><cell>7</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_0">https://github.com/neozhangthe1/disambiguation/ Applied Data Science Track Paper KDD 2018, August 19-23, 2018, London, United Kingdom</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>Jie Tang is the corresponding author of this paper. The work is supported by the National High-tech R&amp;D Program (2015AA124102), Development Program of China (2016QY01W0200), National Basic Research Program of China (2014CB340506), National Natural Science Foundation of China (61631013,61561130160), National Social Science Foundation of China (13&amp;ZD190), a research fund supported by MSRA, and the Royal Society-Newton Advanced Fellowship Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Rong</forename><surname>Yu</surname></persName>
		</author>
		<idno>89.13 46.51 61.12 65.48 40.85 50.32 92.00 36.41 52.17 38.85 91.43 54.53 30.81 97.79 46.86</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yong</forename><surname>Tian</surname></persName>
		</author>
		<idno>76.32 51.95 61.82 70.74 56.85 63.04 86.94 54.58 67.06 32.08 63.71 42.67 10.37 93.79 18.67</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Lu</forename><surname>Han</surname></persName>
		</author>
		<idno>51.78 28.05 36.39 47.88 20.62 28.82 69.72 17.39 27.84 30.25 46.65 36.70 13.66 89.16 23.69</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><surname>Wei</surname></persName>
		</author>
		<idno>Quan 53.88 39.02 45.26 64.45 47.66 54.77 86.42 27.80 42.07 37.86 63.41 47.41 28.16 93.80 43.32</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Li</surname></persName>
		</author>
		<idno>77.20 69.21 72.99 54.66 53.05 53.84 56.29 29.12 38.39 19.48 85.96 31.77 13.25 96.41 23.30</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Meiling Chen</title>
		<idno>74.93 44.70 55.99 59.36 28.80 38.79 86.11 23.85 37.35 58.32 47.14 52.14 59.55 82.07 69.02</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yanqing</forename><surname>Wang</surname></persName>
		</author>
		<idno>71.52 75.33 73.37 60.40 51.97 55.87 80.79 40.39 53.86 29.64 79.08 43.11 25.72 62.47 36.44</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Shi</surname></persName>
		</author>
		<idno>52.20 36.15 42.72 43.84 36.94 40.10 53.72 26.80 35.76 35.31 47.18 40.39 28.79 93.89 44.06</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<idno>57.65 22.35 32.21 54.76 19.70 28.98 80.50 15.21 25.58 25.86 32.67 28.87 15.41 98.72 26.66</idno>
		<title level="m">Min Zheng</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Disambiguating Web Appearances of People in a Social Network</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Bekkerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno>WWW&apos;05. 463-470</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Neural Combinatorial Optimization with Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR abs/1611.09940</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Swoosh: a generic approach to entity resolution</title>
		<author>
			<persName><forename type="first">Omar</forename><surname>Benjelloun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hector</forename><surname>Garcia-Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Menestrina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">Euijong</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="255" to="276" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Collective entity resolution in relational data</title>
		<author>
			<persName><forename type="first">Indrajit</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On graph-based name disambiguation</title>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JDIQ</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Canonicalizing open knowledge bases</title>
		<author>
			<persName><forename type="first">Luis</forename><surname>Galárraga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geremy</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM&apos;14</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1679" to="1688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Introduction to high-dimensional statistics</title>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Giraud</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CRC Press</publisher>
			<biblScope unit="volume">138</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Two supervised learning approaches for name disambiguation in author citations</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Tsioutsiouliklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JCDL&apos;04</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="296" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Entity disambiguation in anonymized graphs using graph kernels</title>
		<author>
			<persName><forename type="first">Linus</forename><surname>Hermansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Kerola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinay</forename><surname>Jethava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devdatt</forename><surname>Dubhashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM&apos;13</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1037" to="1046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">E cient name disambiguation for large-scale databases</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyda</forename><surname>Ertekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C Lee</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PKDD&apos;06</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="536" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Grape: A graph-based framework for disambiguating people appearances in web search</title>
		<author>
			<persName><forename type="first">Lili</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lian</forename><surname>Li</surname></persName>
		</author>
		<idno>ICDM&apos;09. 199-208</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving Author Coreference by Resource-Bounded Information Gathering from the Web</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Pallika H Kanani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="429" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classi cation with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The Hungarian method for the assignment problem</title>
		<author>
			<persName><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics (NRL)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955">1955. 1955</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Identi cation and tracing of ambiguous names: Discriminative and generative approaches</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Morie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI&apos;04</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="419" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ethnicity sensitive author disambiguation using semi-supervised learning</title>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Louppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hussein</forename><forename type="middle">T</forename><surname>Al-Natsheh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Susik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eamonn</forename><forename type="middle">James</forename><surname>Maguire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KESW&apos;16</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="272" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Je</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;13</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">X-means: Extending K-means with E cient Estimation of the Number of Clusters</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Pelleg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
		<idno>ICML&apos;00. 727-734</idno>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Facenet: A uni ed embedding for face recognition and clustering</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Schro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;15</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Large-scale cross-document coreference using distributed inference and hierarchical models</title>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language</title>
				<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="793" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Comparison of Blocking Methods for Record Linkage</title>
		<author>
			<persName><forename type="first">Rebecca</forename><forename type="middle">C</forename><surname>Steorts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">L</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mauricio</forename><surname>Sadinle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Fienberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PSD&apos;14</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="253" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A Uni ed Probabilistic Framework for Name Disambiguation in Digital Library</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C M</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="975" to="987" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Combination Approach to Web User Pro ling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TKDD</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="44" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Arnet-Miner: Extraction and Mining of Academic Social Networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;08</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="990" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning deep representations for graph clustering</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI&apos;14</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1293" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A generalized birthday problem</title>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Crypto&apos;17</title>
				<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="288" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Probabilistic Reasoning about Human Edits in Information Integration</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Kobren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop: Machine Learning Meets Crowdsourcing</title>
				<meeting><address><addrLine>Atlanta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A discriminative hierarchical model for fast coreference at large scale</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
				<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="379" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Integrated network analysis platform for proteinprotein interactions</title>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tea</forename><surname>Vallenius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Ovaska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jukka</forename><surname>Westermarck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomi</forename><forename type="middle">P</forename><surname>Mäkelä</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sampsa</forename><surname>Hautaniemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="75" to="77" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Object distinction: Distinguishing objects with identical names</title>
		<author>
			<persName><forename type="first">Xiaoxin</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE&apos;07</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1242" to="1246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Person name disambiguation by bootstrapping</title>
		<author>
			<persName><forename type="first">Minoru</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masaki</forename><surname>Ikeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shingo</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Issei</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR&apos;10</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Name disambiguation in anonymized graphs using network embedding</title>
		<author>
			<persName><forename type="first">Baichuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Al Hasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM&apos;17</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1239" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cosnet: Connecting heterogeneous social networks with local and global consistency</title>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;15</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1485" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
