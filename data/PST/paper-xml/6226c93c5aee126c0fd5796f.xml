<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GEODIFF: A GEOMETRIC DIFFUSION MODEL FOR MOLECULAR CONFORMATION GENERATION</title>
				<funder ref="#_uT73fb2">
					<orgName type="full">ONR</orgName>
				</funder>
				<funder ref="#_E8md38H">
					<orgName type="full">AFOSR</orgName>
				</funder>
				<funder ref="#_t2rMqcQ">
					<orgName type="full">Natural Sciences and Engineering Research Council</orgName>
					<orgName type="abbreviated">NSERC</orgName>
				</funder>
				<funder>
					<orgName type="full">Microsoft Research and Mila</orgName>
				</funder>
				<funder ref="#_Wy7npuQ">
					<orgName type="full">NRC Collaborative R&amp;D Project</orgName>
				</funder>
				<funder ref="#_W58U68b">
					<orgName type="full">ARO</orgName>
				</funder>
				<funder>
					<orgName type="full">Amazon Faculty Research Award, Tencent AI Lab Rhino-Bird Gift Fund</orgName>
				</funder>
				<funder ref="#_Nu8WMYY">
					<orgName type="full">IVADO Fundamental Research Project</orgName>
				</funder>
				<funder ref="#_jwfkA9G #_FSPdEU6 #_5aEqcKn">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder>
					<orgName type="full">Canada CIFAR AI Chair Program</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-06">6 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Minkai</forename><surname>Xu</surname></persName>
							<email>minkai.xu@umontreal.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">Mila -Qu?bec AI Institute</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Universit? de Montr?al</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lantao</forename><surname>Yu</surname></persName>
							<email>lantaoyu@cs.stanford.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
							<email>yangsong@cs.stanford.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chence</forename><surname>Shi</surname></persName>
							<email>chence.shi@umontreal.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">Mila -Qu?bec AI Institute</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Universit? de Montr?al</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
							<email>ermon@cs.stanford.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
							<email>jian.tang@hec.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">Mila -Qu?bec AI Institute</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">HEC Montr?al</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">CIFAR AI Research Chair</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GEODIFF: A GEOMETRIC DIFFUSION MODEL FOR MOLECULAR CONFORMATION GENERATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-06">6 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.02923v1[q-bio.QM]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Predicting molecular conformations from molecular graphs is a fundamental problem in cheminformatics and drug discovery. Recently, significant progress has been achieved with machine learning approaches, especially with deep generative models. Inspired by the diffusion process in classical non-equilibrium thermodynamics where heated particles will diffuse from original states to a noise distribution, in this paper, we propose a novel generative model named GEODIFF for molecular conformation prediction. GEODIFF treats each atom as a particle and learns to directly reverse the diffusion process (i.e., transforming from a noise distribution to stable conformations) as a Markov chain. Modeling such a generation process is however very challenging as the likelihood of conformations should be rototranslational invariant. We theoretically show that Markov chains evolving with equivariant Markov kernels can induce an invariant distribution by design, and further propose building blocks for the Markov kernels to preserve the desirable equivariance property. The whole framework can be efficiently trained in an end-toend fashion by optimizing a weighted variational lower bound to the (conditional) likelihood. Experiments on multiple benchmarks show that GEODIFF is superior or comparable to existing state-of-the-art approaches, especially on large molecules. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph representation learning has achieved huge success for molecule modeling in various tasks ranging from property prediction <ref type="bibr" target="#b10">(Gilmer et al., 2017;</ref><ref type="bibr" target="#b6">Duvenaud et al., 2015)</ref> to molecule generation <ref type="bibr" target="#b18">(Jin et al., 2018;</ref><ref type="bibr" target="#b36">Shi et al., 2020)</ref>, where typically a molecule is represented as an atom-bond graph. Despite its effectiveness in various applications, a more intrinsic and informative representation for molecules is the 3D geometry, also known as conformation, where atoms are represented as their Cartesian coordinates. The 3D structures determine the biological and physical properties of molecules and hence play a key role in many applications such as computational drug and material design <ref type="bibr" target="#b46">(Thomas et al., 2018;</ref><ref type="bibr" target="#b9">Gebauer et al., 2021;</ref><ref type="bibr" target="#b19">Jing et al., 2021;</ref><ref type="bibr" target="#b2">Batzner et al., 2021)</ref>. Unfortunately, how to predict stable molecular conformation remains a challenging problem. Traditional methods based on molecular dynamics (MD) or Markov chain Monte Carlo (MCMC) are very computationally expensive, especially for large molecules <ref type="bibr" target="#b13">(Hawkins, 2017)</ref>.</p><p>Recently, significant progress has been made with machine learning approaches, especially with deep generative models. For example, <ref type="bibr" target="#b39">Simm &amp; Hernandez-Lobato (2020)</ref>; <ref type="bibr">Xu et al. (2021b)</ref> studied predicting atomic distances with variational autoencoders (VAEs) <ref type="bibr" target="#b22">(Kingma &amp; Welling, 2013)</ref> and flow-based models <ref type="bibr" target="#b5">(Dinh et al., 2017)</ref> respectively. <ref type="bibr" target="#b26">Shi et al. (2021)</ref> proposed to use denoising score matching <ref type="bibr" target="#b44">(Song &amp; Ermon, 2019;</ref><ref type="bibr">2020)</ref> to estimate the gradient fields over atomic distances, through which the gradient fields over atomic coordinates can be calculated. <ref type="bibr" target="#b8">Ganea et al. (2021)</ref> studied generating conformations by predicting both bond lengths and angles. As molecular conformations are roto-translational invariant, these approaches circumvent directly modeling atomic coordinates by leveraging intermediate geometric variables such as atomic distances, bond and torsion angles, which are roto-translational invariant. As a result, they are able to achieve very compelling performance. However, as all these approaches seek to indirectly model the intermediate geometric variables, they have inherent limitations in either training or inference process (see Sec. 2 for a detailed description). Therefore, an ideal solution would still be directly modeling the atomic coordinates and at the same time taking the roto-translational invariance property into account.</p><p>In this paper, we propose such a solution called GEODIFF, a principled probabilistic framework based on denoising diffusion models <ref type="bibr" target="#b42">(Sohl-Dickstein et al., 2015)</ref>. Our approach is inspired by the diffusion process in nonequilibrium thermodynamics <ref type="bibr" target="#b4">(De Groot &amp; Mazur, 2013)</ref>. We view atoms as particles in a thermodynamic system, which gradually diffuse from the original states to a noisy distribution in contact with a heat bath. At each time step, stochastic noises are added to the atomic positions. Our high-level idea is learning to reverse the diffusion process, which recovers the target geometric distribution from the noisy distribution. In particular, inspired by recent progress of denoising diffusion models on image generation <ref type="bibr" target="#b15">(Ho et al., 2020;</ref><ref type="bibr">Song et al., 2020)</ref>, we view the noisy geometries at different timesteps as latent variables, and formulate both the forward diffusion and reverse denoising process as Markov chains. Our goal is to learn the transition kernels such that the reverse process can recover realistic conformations from the chaotic positions sampled from a noise distribution. However, extending existing methods to geometric generation is highly non-trivial: a direct application of diffusion models on the conformation generation task lead to poor generation quality. As mentioned above, molecular conformations are roto-translational invariant, i.e., the estimated (conditional) likelihood should be unaffected by translational and rotational transformations <ref type="bibr" target="#b23">(K?hler et al., 2020)</ref>. To this end, we first theoretically show that a Markov process starting from an roto-translational invariant prior distribution and evolving with roto-translational equivariant Markov kernels can induce an roto-translational invariant density function. We further provide practical parameterization to define a roto-translational invariant prior distribution and a Markov kernel imposing the equivariance constraints. In addition, we derive a weighted variational lower bound of the conditional likelihood of molecular conformations, which also enjoys the rototranslational invariance and can be efficiently optimized.</p><p>A unique strength of GEODIFF is that it directly acts on the atomic coordinates and entirely bypasses the usage of intermediate elements for both training and inference. This general formulation enjoys several crucial advantages. First, the model can be naturally trained end-to-end without involving any sophisticated techniques like bilevel programming <ref type="bibr">(Xu et al., 2021b)</ref>, which benefits from small optimization variances. Besides, instead of solving geometries from bond lengths or angles, the one-stage sampling fashion avoids accumulating any intermediate error, and therefore leads to more accurate predicted structures. Moreover, GEODIFF enjoys a high model capacity to approximate the complex distribution of conformations. Thus, the model can better estimate the highly multi-modal distribution and generate structures with high quality and diversity.</p><p>We conduct comprehensive experiments on multiple benchmarks, including conformation generation and property prediction tasks. Numerical results show that GEODIFF consistently outperforms existing state-of-the-art machine learning approaches, and by a large margin on the more challenging large molecules. The significantly superior performance demonstrate the high capacity to model the complex distribution of molecular conformations and generate both diverse and accurate molecules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Recently, various deep generative models have been proposed for conformation generation. Among them, CVGAE <ref type="bibr" target="#b27">(Mansimov et al., 2019)</ref> first proposed a VAE model to directly generate 3D atomic coordinates, which fails to preserve the roto-translation equivariance property of conformations and suffers from poor performance. To address this problem, the majority of subsequent models are based on intermediate geometric elements such as atomic distances and torsion angles. A favorable property of these elements is the roto-translational invariance, (e.g. atomic distances does not change when rotating the molecule), which has been shown to be an important inductive bias for molecular geometry modeling <ref type="bibr" target="#b23">(K?hler et al., 2020)</ref>. However, such a decomposition suffers from several drawbacks for either training or sampling. For example, GRAPHDG <ref type="bibr" target="#b39">(Simm &amp; Hernandez-Lobato, 2020)</ref> and CGCF <ref type="bibr">(Xu et al., 2021a)</ref> proposed to predict the interatomic distance matrix by VAE and Flow respectively, and then solve the geometry through the Distance Geometry (DG) technique <ref type="bibr" target="#b24">(Liberti et al., 2014)</ref>, which searches reasonable coordinates that matches with the predicted distances. CONFVAE further improves this pipeline by designing an end-to-end framework via bilevel optimization <ref type="bibr">(Xu et al., 2021b)</ref>. However, all these approaches suffer from the accumulated error problem, meaning that the noise in the predicted distances will misguide the coordinate searching process and lead to inaccurate or even erroneous structures. To overcome this problem, CONFGF <ref type="bibr" target="#b26">(Shi et al., 2021;</ref><ref type="bibr">Luo et al., 2021)</ref> proposed to learn the gradient of the log-likelihood w.r.t coordinates. However, in practice the model is still aided by intermediate geometric elements, in that it first estimates the gradient w.r.t interatomic distances via denoising score matching (DSM) <ref type="bibr" target="#b44">(Song &amp; Ermon, 2019;</ref><ref type="bibr">2020)</ref>, and then derives the gradient of coordinates using the chain rule. The problem is, by learning the distance gradient via DSM, the model is fed with perturbed distance matrices, which may violate the triangular inequality or even contain negative values. As a consequence, the model is actually learned over invalid distance matrices but tested with valid ones calculated from coordinates, making it suffer from serious out-of-distribution <ref type="bibr" target="#b14">(Hendrycks &amp; Gimpel, 2016)</ref> problem. Most recently, another concurrent work <ref type="bibr" target="#b8">(Ganea et al., 2021)</ref> proposed a highly systematic (rule-based) pipeline named GEOMOL, which learns to predict a minimal set of geometric quantities (i.e. length and angles) and then reconstruct the local and global structures of the conformation in a sophisticated procedure. Besides, there has also been efforts to use reinforcement learning for conformation search <ref type="bibr" target="#b11">Gogineni et al. (2020)</ref>. Nevertheless, this method relies on rigid rotor approximation and can only model the torsion angles, and thus fundamentally differs from other approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">NOTATIONS AND PROBLEM DEFINITION</head><p>Notations. In this paper each molecule with n atoms is represented as an undirected graph G = V, E , where V = {v i } n i=1 is the set of vertices representing atoms and E = {e ij | (i, j) ? |V| ? |V|} is the set of edges representing inter-atomic bonds. Each node v i ? V describes the atomic attributes, e.g., the element type. Each edge e ij ? E describes the corresponding connection between v i and v j , and is labeled with its chemical type. In addition, we also assign the unconnected edges with a virtual type. For the geometry, each atom in V is embedded by a coordinate vector c ? R 3 into the 3-dimensional space, and the full set of positions (i.e., the conformation) can be represented as a matrix</p><formula xml:id="formula_0">C = [c 1 , c 2 , ? ? ? , c n ] ? R n?3 .</formula><p>Problem Definition. The task of molecular conformation generation is a conditional generative problem, where we are interested in generating stable conformations for a provided graph G. Given multiple graphs G, and for each G given its conformations C as i.i.d samples from an underlying Boltzmann distribution <ref type="bibr" target="#b29">(No? et al., 2019)</ref>, our goal is learning a generative model p ? (C|G), which is easy to draw samples from, to approximate the Boltzmann function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">EQUIVARIANCE</head><p>Equivariance is ubiquitous in machine learning for atomic systems, e.g., the vectors of atomic dipoles or forces should rotate accordingly w.r.t. the conformation coordinates <ref type="bibr" target="#b46">(Thomas et al., 2018;</ref><ref type="bibr" target="#b47">Weiler et al., 2018;</ref><ref type="bibr" target="#b7">Fuchs et al., 2020;</ref><ref type="bibr" target="#b28">Miller et al., 2020;</ref><ref type="bibr" target="#b40">Simm et al., 2021;</ref><ref type="bibr" target="#b2">Batzner et al., 2021)</ref>. It has shown effectiveness to integrate such inductive bias into model parameterization for modeling 3D geometry, which is critical for the generalization capacity <ref type="bibr" target="#b23">(K?hler et al., 2020;</ref><ref type="bibr">Satorras et al., 2021a)</ref>. Formally, a function F : X ? Y is equivariant w.r.t a group G if:</p><formula xml:id="formula_1">F ? T g (x) = S g ? F(x),<label>(1)</label></formula><p>where T g and S g are transformations for an element g ? G, acting on the vector spaces X and Y, respectively. In this work, we consider the SE(3) group, i.e., the group of rotation, translation in 3D space. This requires the estimated likelihood unaffected with translational and rotational transformations, and we will elaborate on how our method satisfy this property in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GEODIFF METHOD</head><p>In this section, we elaborate on the proposed equivariant diffusion framework. We first present a high level description of our 3D diffusion formulation in Sec. 4.1, based on recent progress of denoising diffusion models <ref type="bibr" target="#b42">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b15">Ho et al., 2020)</ref>. Then we emphasize several</p><formula xml:id="formula_2">? ! ? " ? !"# ? ? ! ? "#$ ?, ? " ) ? ? " ? "#$ ) ? $ ? Figure 1:</formula><p>Illustration of the diffusion and reverse process of GEODIFF. For diffusion process, noise from fixed posterior distributions q(C t |C t-1 ) is gradually added until the conformation is destroyed. Symmetrically, for generative process, an initial state C T is sampled from standard Gaussian distribution, and the conformation is progressively refined via the Markov kernels p ? (C t-1 |G, C t ).</p><p>non-trivial challenges of building diffusion models for geometry generation scenario, and show how we technically tackle these issues. Specifically, in Sec. 4.2, we present how we parameterize p ? (C|G) so that the conditional likelihood is roto-translational invariant, and in Sec. 4.3, we introduce our surgery of the training objective to make the optimization also invariant of translation and rotation. Finally, we briefly show how to draw samples from our model in Sec. 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">FORMULATION</head><p>Let C 0 denotes the ground truth conformations and let C t for t = 1, ? ? ? , T be a sequence of latent variables with the same dimension, where t is the index for diffusion steps. Then a diffusion probabilistic model <ref type="bibr" target="#b42">(Sohl-Dickstein et al., 2015)</ref> can be described as a latent variable model with two processes: the forward diffusion process, and the reverse generative process. Intuitively, the diffusion process progressively injects small noises to the data C 0 , while the generative process learns to revert the diffusion process by gradually eliminating the noise to recover the ground truth. We provide a high-level schematic of the processes in Fig. <ref type="figure">1</ref>.</p><p>Diffusion process. Following the physical insight, we model the particles C as an evolving thermodynamic system. With time going by, the equilibrium conformation C 0 will gradually diffuse to the next chaotic states C t , and finally converge into a white noise distribution after T iterations. Different from typical latent variable models, in diffusion model this forward process is defined as a fixed (rather than trainable) posterior distribution q(C 1:T |C 0 ). Specifically, we define it as a Markov chain according to a fixed variance schedule ? 1 , . . . , ? T :</p><formula xml:id="formula_3">q(C 1:T |C 0 ) = T t=1 q(C t |C t-1 ), q(C t |C t-1 ) = N (C t ; 1 -? t C t-1 , ? t I).<label>(2)</label></formula><p>Note that, in this work we do not impose specific (invariance) requirement upon the diffusion process, as long as it can efficiently draw noisy samples for training the generative process p ? (C 0 ).</p><formula xml:id="formula_4">Let ? t = 1 -? t and ?t = t s=1 ? s , a special property of the forward process is that q(C t |C 0 ) of arbitrary timestep t can be calculated in closed form q(C t |C 0 ) = N (C t ; ? ?t C 0 , (1 -?t )I) 2 .</formula><p>This indicates with sufficiently large T , the whole forward process will convert C 0 to whitened isotropic Gaussian, and thus it is natural to set p(C T ) as a standard Gaussian distribution.</p><p>Reverse Process. Our goal is learning to recover conformations C 0 from the white noise C T , given specified molecular graphs G. We consider this generative procedure as a reverse dynamics of the above diffusion process, starting from the noisy particles C T ? p(C T ). We formulate this reverse dynamics as a conditional Markov chain with learnable transitions:</p><formula xml:id="formula_5">p ? (C 0:T -1 |G, C T ) = T t=1 p ? (C t-1 |G, C t ), p ? (C t-1 |G, C t ) = N (C t-1 ; ? ? (G, C t , t), ? 2 t I). (3)</formula><p>Herein ? ? are parameterized neural networks to estimate the means, and ? t can be any user-defined variance. The initial distribution p(C T ) is set as a standard Gaussian. Given a graph G, its 3D structure is generated by first drawing chaotic particles C T from p(C T ), and then iteratively refined through the reverse Markov kernels p ? (C t-1 |G, C t ).</p><p>Having formulated the reverse dynamics, the marginal likelihood can be calculated by p ? (C 0 |G) = p(C T )p ? (C 0:T -1 |G, C T )dC 1:T . Herein a non-trivial problem is that the likelihood should be invariant w.r.t translation and rotation, which has proved to be a critical inductive bias for 3D object generation <ref type="bibr" target="#b23">(K?hler et al., 2020;</ref><ref type="bibr">Satorras et al., 2021a)</ref>. In the following subsections, we will elaborate on how we parameterize the Markov kernels p ? (C t-1 |G, C t ) to achieve this desired property, and also how to maximize this likelihood by taking the invariance into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">EQUIVARIANT REVERSE GENERATIVE PROCESS</head><p>Instead of directly leveraging existing methods, we consider building the density p ? (C 0 ) that is invariant to rotation and translation transformations. Intuitively, this requires the likelihood to be unaffected by translations and rotations. Formally, let T g be some roto-translational transformations of a group element g ? SE(3), then we have the following statement:</p><formula xml:id="formula_6">Proposition 1. Let p(x T ) be an SE(3)-invariant density function, i.e., p(x T ) = p(T g (x T )). If Markov transitions p(x t-1 |x t ) are SE(3)-equivariant, i.e., p(x t-1 |x t ) = p(T g (x t-1 )|T g (x t )), then we have that the density p ? (x 0 ) = p(x T )p ? (x 0:T -1 |x T )dx 1:T is also SE(3)-invariant.</formula><p>This proposition indicates that the dynamics starting from an invariant standard density along an equivariant Gaussian Markov kernel can result in an invariant density. Now we provide a practical implementation of GEODIFF based on the recent denoising diffusion framework <ref type="bibr" target="#b15">(Ho et al., 2020)</ref>.</p><p>Invariant Initial Density p(C T ). We first introduce the invariant distribution p(C T ), which will also be employed in the equivariant Markov chain. We borrow the idea from <ref type="bibr" target="#b23">K?hler et al. (2020)</ref> to consider systems with zero center of mass (CoM), termed CoM-free systems. We define p(C T ) as a "CoM-free standard density" ?(C), built upon an isotropic normal density ?(C): for evaluating the likelihood ?(C) we can firstly translate C to zero CoM and then calculate ?(C), and for sampling from ?(C) we can first sample from ?(C) and then move the CoM to zero.</p><p>We provide a formal theoretical analysis of ?(C) in Appendix A. Intuitively, the isotropic Gaussian is manifestly invariant to rotations around the zero CoM. And by considering CoM-free system, moving the particles to zero CoM can always ensure the translational invariance. Consequently, ?(C) is constructed as a roto-transitional invariant density.</p><p>Equivariant Markov Kernels p(C t-1 |G, C t ). Similar to the prior density, we also consider equipping all intermediate structures C t as CoM-free systems. Specifically, given mean ? ? (G, C t , t) and variance ? t , the likelihood of C t-1 will be calculated by ?</p><formula xml:id="formula_7">( C t-1 -? ? (G,C t ,t) ?t</formula><p>). The CoM-free Gaussian ensures the translation invariance in the Markov kernels. Consequently, to achieve the equivariant property defined in Proposition 1, we focus on the rotation equivariance.</p><p>Then in general, the key requirement is to ensure the means ? ? (G, C t , t) to be roto-translation equivariant w.r.t C t . Following <ref type="bibr" target="#b15">Ho et al. (2020)</ref>, we consider the following parameterization of ? ? :</p><formula xml:id="formula_8">? ? (C t , t) = 1 ? ? t C t - ? t ? 1 -?t ? (G, C t , t) ,<label>(4)</label></formula><p>where ? are neural networks with trainable parameters ?. Intuitively, the model ? learns to predict the noise necessary to decorrupt the conformations. This is analogous to the physical force fields <ref type="bibr" target="#b34">(Sch?tt et al., 2017;</ref><ref type="bibr" target="#b50">Zhang et al., 2018;</ref><ref type="bibr" target="#b16">Hu et al., 2021;</ref><ref type="bibr" target="#b38">Shuaibi et al., 2021)</ref>, which also gradually push particles towards convergence around the equilibrium states.</p><p>Now the problem is transformed to constructing ? to be roto-translational equivariant. We draw inspirations from recent equivariant networks <ref type="bibr" target="#b46">(Thomas et al., 2018;</ref><ref type="bibr">Satorras et al., 2021b)</ref> to design an equivariant convolutional layer, named graph field network (GFN). In the l-th layer, GFN takes node embeddings h l ? R n?b (b denotes the feature dimension) and corresponding coordinate embeddings x l ? R n?3 as inputs, and outputs h l+1 and x l+1 as follows:</p><formula xml:id="formula_9">m ij = ? m h l i , h l j , x l i -x l j 2 , e ij ; ? m (5) h l+1 i = ? h h l i , j?N<label>(i)</label></formula><p>m ij ; ? h (6)</p><formula xml:id="formula_10">x l+1 i = j?N (i) 1 d ij (c i -c j ) ? x (m ij ; ? x )<label>(7)</label></formula><p>where ? are feed-forward networks and d ij denotes interatomic distances. N (i) denotes the neighborhood of i th node, including both connected atoms and other ones within a radius threshold ? , which enables the model to explicitly capture long-range interactions and support molecular graphs with disconnected components. Initial embeddings h 0 are combinations of atom and timestep embeddings, and x 0 are atomic coordinates. The main difference between proposed GFN and other GNNs lies in equation 7, where x is updated as a combination of radial directions weighted by ? x : R b ? R. Such vector field x L enjoys the roto-translation equivariance property. Formally, we have: Proposition 2. Parameterizing ? (G, C, t) as a composition of L GFN layers, and take the x L after L updates as the output. Then the noise vector field ? is SE(3) equivariant w.r.t the 3D system C.</p><p>Intuitively, given h l already invariant and x l equivariant, the message embedding m will also be invariant since it only depends on invariant features. Since x is updated with the relative differences c i -c j weighted by invariant features, it will be translation-invariant and rotation-equivariant. Then inductively, composing ? with L GFN layers enables equivariance with C t . We provide the formal proof of equivariance properties in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">IMPROVED TRAINING OBJECTIVE</head><p>Having formulated the generative process and the model parameterization, now we consider the practical training objective for the reverse dynamics. Since directly optimizing the exact log-likelihood is intractable, we instead maximize the usual variational lower bound (ELBO)<ref type="foot" target="#foot_2">3</ref> :</p><formula xml:id="formula_11">E log p ? (C 0 |G) = E log E q(C 1:T |C 0 ) p ? (C 0:T |G) q(C 1:T |C 0 ) ? -E q T t=1 D KL (q(C t-1 |C t , C 0 ) p ? (C t-1 |C t , G)) := -L ELBO<label>(8)</label></formula><p>where q(C t-1 |C t , C 0 ) is analytically tractable as N ( <ref type="bibr" target="#b15">Ho et al. (2020)</ref> showed that under the parameterization in equation 4, the ELBO of the diffusion model can be further simplified by calculating the KL divergences between Gaussians as weighted L 2 distances between the means ? and 3 . Formally, we have: Proposition 3. <ref type="bibr" target="#b15">(Ho et al., 2020)</ref> Under the parameterization in equation 4, we have:</p><formula xml:id="formula_12">? ?t-1?t 1-?t C 0 + ? ?t(1-?t-1) 1-?t C t , 1-?t-1 1-?t ? t ) 3 . Most recently,</formula><formula xml:id="formula_13">L ELBO = T t=1 ? t E {C 0 ,G}?q(C 0 ,G), ?N (0,I) -? (G, C t , t) 2 2 (9)</formula><p>where</p><formula xml:id="formula_14">C t = ? ?t C 0 + ? 1 -?t . The weights ? t = ?t 2?t</formula><p>(1-?t-1) for t &gt; 1, and ? 1 = 1 2?1 .</p><p>The intuition of this objective is to independently sample chaotic conformations of different timesteps from q(C t-1 |C t , C 0 ), and use ? to model the noise vector . To yield a better empirical performance, <ref type="bibr" target="#b15">Ho et al. (2020)</ref> suggests to set all weights ? t as 1, which is in line with the the objectives of recent noise conditional score networks <ref type="bibr" target="#b44">(Song &amp; Ermon, 2019;</ref><ref type="bibr">2020)</ref>.</p><p>As ? is designed to be equivariant, it is natural to require its supervision signal to be equivariant with C t . Note that once this is achieved, the ELBO will also become invariant. However, the in the forward diffusion process is not imposed with such equivariance, violating the above properties.</p><p>Here we propose two approaches to obtain the modified noise vector ? , which, after replacing in the L 2 distance calculation in equation 9, achieves the desired equivariance:</p><p>Alignment approach. Considering the fact that can be calculated by . Since the aligned conformation ?0 is equivariant with C t , the processed ? will also enjoy the equivariance. Specifically, the alignment is implemented by first translating C 0 to the same CoM of C t and then solve the optimal rotation matrix by Kabsch alignment algorithm <ref type="bibr" target="#b21">(Kabsch, 1976)</ref>.</p><p>Chain-rule approach. Another meaningful observation is that by reparameterizing the Gaussian distribution q(C t |C 0 ) as C t = ? ?t C 0 + ? 1 -?t , can be viewed as a weighted score function ? 1 -?t ? C t q(C t |C 0 ). <ref type="bibr" target="#b26">Shi et al. (2021)</ref> recently shows that generally this score function ? C t q(C t |?) can be designed to be equivariant by decomposing it into ? C t d t ? d t q(C t |?) with the chain rule, where d t can be any invariant features of the structures C t such as the inter-atomic distances. We refer readers to <ref type="bibr" target="#b26">Shi et al. (2021)</ref> for more details. The insight is that as gradient of invariant variables w.r.t equivariant variables, the partial derivative ? C t d t will always be equivalent with C t . In this work, under the common assumption that d also follows a Gaussian distribution (Kingma &amp; Welling, 2013), our practical implementation is to first approximately calculate</p><formula xml:id="formula_15">? d t q(C t |C 0 ) as d t - ? ?td 0 1-?t</formula><p>, and then compute the modified noise vector ? as</p><formula xml:id="formula_16">? 1 -?t ? C t d t ( d t - ? ?td 0 1-?t ) = ? C t d t ?(d t - ? ?td 0 ) ? 1-?t .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">SAMPLING</head><p>Algorithm 1 Sampling Algorithm of GEODIFF.</p><p>Input: the molecular graph G, the learned reverse model ? .</p><p>Output: the molecular conformation C. </p><formula xml:id="formula_17">1: Sample C T ? p(C T ) = N (0, I) 2: for s = T, T -1, ? ? ? , 1 do 3: Shift C s to zero CoM 4: Compute ? ? (C s , G, s) from ? (C s , G, s) using equation 4 5: Sample C s-1 ? N (C s-1 ; ? ? (C s , G, s),</formula><formula xml:id="formula_18">C t-1 ? p ? (C t-1 |G, C t ) for t = T, T - 1, ? ? ? , 1.</formula><p>This process is Markovian, which gradually shifts the previous noisy positions towards equilibrium states. We provide the pseudo code of the whole sampling process in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENT</head><p>In this section, we empirically evaluate GEODIFF on the task of equilibrium conformation generation for both small and drug-like molecules. Following existing work <ref type="bibr" target="#b26">(Shi et al., 2021;</ref><ref type="bibr" target="#b8">Ganea et al., 2021)</ref>, we test the proposed method as well as the competitive baselines on two standard benchmarks: Conformation Generation (Sec. 5.2) and Property Prediction (Sec. 5.3). We first present the general experiment setups, and then describe task-specific evaluation protocols and discuss the results in each section. The implementation details are provided in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">EXPERIMENT SETUP</head><p>Datasets. Following prior works <ref type="bibr">(Xu et al., 2021a;</ref><ref type="bibr">b)</ref>, we also use the recent GEOM-QM9 (Ramakrishnan et al., 2014) and GEOM-Drugs <ref type="bibr" target="#b1">(Axelrod &amp; Gomez-Bombarelli, 2020)</ref> datasets. The former one contains small molecules while the latter one are medium-sized organic compounds. We borrow the data split produced by <ref type="bibr" target="#b26">Shi et al. (2021)</ref>. For both datasets, the training split consists of 40, 000 molecules with 5 conformations for each, resulting in 200, 000 conformations in total. The valid split share the same size as training split. The test split contains 200 distinct molecules, with 22, 408 conformations for QM9 and 14, 324 ones for Drugs.</p><p>Baselines. We compare GEODIFF with 6 recent or established state-of-the-art baselines. For the ML approaches, we test the following models with highest reported performance: CVGAE <ref type="bibr" target="#b27">(Mansimov et al., 2019)</ref>, GRAPHDG <ref type="bibr" target="#b39">(Simm &amp; Hernandez-Lobato, 2020)</ref>, CGCF <ref type="bibr">(Xu et al., 2021a)</ref>, CONF-VAE <ref type="bibr">(Xu et al., 2021b)</ref> and CONFGF <ref type="bibr" target="#b26">(Shi et al., 2021)</ref>. We also test the classic RDKIT <ref type="bibr" target="#b31">(Riniker &amp; Landrum, 2015)</ref> method, which is arguably the most popular open-source software for conformation generation. We refer readers to Sec. 2 for a detailed discussion of these models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">CONFORMATION GENERATION</head><p>Evaluation metrics. The task aims to measure both quality and diversity of generated conformations by different models. We follow <ref type="bibr" target="#b8">Ganea et al. (2021)</ref> to evaluate 4 metrics built upon root-mean-square  <ref type="bibr">et al. (2021)</ref>. The results of GEOMOL are borrowed from a most recent study <ref type="bibr" target="#b51">Zhu et al. (2022)</ref>. Other results are obtained by our own experiments. The results of all models for the GEOM-QM9 dataset (summarized in Tab. 5) are collected in the same way. deviation (RMSD), which is defined as the normalized Frobenius norm of two atomic coordinates matrices, after alignment by Kabsch algorithm <ref type="bibr" target="#b21">(Kabsch, 1976)</ref>. Formally, let S g and S r denote the sets of generated and reference conformers respectively, then the Coverage and Matching metrics <ref type="bibr">(Xu et al., 2021a)</ref> following the conventional Recall measurement can be defined as:</p><formula xml:id="formula_19">COV-R(S g , S r ) = 1 |S r | C ? S r | RMSD(C, ?) ? ?, ? ? S g ,<label>(10)</label></formula><formula xml:id="formula_20">MAT-R(S g , S r ) = 1 |S r | C?Sr min ??Sg RMSD(C, ?), (<label>11</label></formula><formula xml:id="formula_21">)</formula><p>where ? is a pre-defined threshold. The other two metrics COV-P and MAT-P inspired by Precision can be defined similarly but with the generated and reference sets exchanged. In practice, S g is set as twice of the size of S r for each molecule. Intuitively, the COV scores measure the percentage of structures in one set covered by another set, where covering means the RMSD between two conformations is within a certain threshold ?. By contrast, the MAT scores measure the average RMSD of conformers in one set with its closest neighbor in another set. In general, higher COV rates or lower MAT score suggest that more realistic conformations are generated. Besides, the Precision metrics depend more on the quality, while the Recall metrics concentrate more on the diversity. Either metrics can be more appealing considering the specific scenario. Following previous works <ref type="bibr">(Xu et al., 2021a;</ref><ref type="bibr" target="#b8">Ganea et al., 2021)</ref>, ? is set as 0.5? and 1.25? for QM9 and Drugs datasets respectively.</p><p>Results &amp; discussion. The results are summarized in Tab. 1 and Tab. 5 (left in Appendix. D). As noted in Sec. 4.3, GEODIFF can be trained with two types of modified ELBO, named alignment and chain-rule approaches. We denote models learned by these two objectives as GEODIFF-A and GEODIFF-C respectively. As shown in the tables, GEODIFF consistently outperform the state-of-theart ML models on all datasets and metrics, especially by a significant margin for more challenging large molecules (Drugs dataset). The results demonstrate the superior capacity of GEODIFF to model the multi modal distribution, and generative both accurate and diverse conformations. We also notice that in general GEODIFF-C performs slightly better than GEODIFF-A, which suggests that chain-rule approach leads to a better optimization procedure. We thus take GEODIFF-C as the representative in the following comparisons. We visualize samples generated by different models in Fig. <ref type="figure" target="#fig_0">2</ref> to provide a qualitative comparison, where GEODIFF is shown to capture better both local and global structures.</p><p>On the more challenging Drugs dataset, we further test RDKIT. As shown in Tab. 2, our observation is in line with previous studies <ref type="bibr" target="#b26">(Shi et al., 2021)</ref> that the state-of-the-art ML models (shown in Tab. 1) perform better on COV-R and MAT-R. However, for the new Precision-based metrics we found that ML models are still not comparable. This indicates that ML models tend to explore more possible representatives while RDKIT concentrates on a few most common ones, prioritizes quality over diversity. Previous works <ref type="bibr" target="#b27">(Mansimov et al., 2019;</ref><ref type="bibr">Xu et al., 2021b)</ref> suggest that this is because RDKIT involves an additional empirical force field (FF) <ref type="bibr" target="#b12">(Halgren, 1996)</ref> to optimize the structure, and we follow them to also combine GEODIFF with FF to yield a more fair comparison. Results in  Tab. 2 demonstrate that GEODIFF +FF can keep the superior diversity (Recall metrics) while also enjoy significantly improved accuracy ((Precision metrics)). Evaluation metrics. This task estimates the molecular ensemble properties (Axelrod &amp; Gomez-Bombarelli, 2020) over a set of generated conformations. This can provide an direct assessment on the quality of generated samples. In specific, we follow <ref type="bibr" target="#b26">Shi et al. (2021)</ref> to extract a split from GEOM-QM9 covering 30 molecules, and generate 50 samples for each. Then we use the chemical toolkit PSI4 <ref type="bibr" target="#b41">(Smith et al., 2020)</ref> to calculate each conformer's energy E and HOMO-LUMO gap , and compare the average energy E, lowest energy E min , average gap ? , minimum gap ? min , and maximum gap ? max with the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">PROPERTY PREDICTION</head><p>Results &amp; discussions. The mean absolute errors (MAE) between calculated properties and the ground truth are reported in Tab. 3. CVGAE is excluded due to the poor performance, which is also reported in Simm &amp; Hernandez-Lobato (2020); <ref type="bibr" target="#b26">Shi et al. (2021)</ref>. The properties are highly sensitive to geometric structure, and thus the superior performance demonstrate that GEODIFF can consistently predict more accurate conformations across different molecules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We propose GEODIFF, a novel probabilistic model for generating molecular conformations. GEODIFF marries denoising diffusion models with geometric representations, where we parameterize the reverse generative dynamics as a Markov chain, and novelly impose roto-translational invariance into the density with equivariant Markov kernels. We derive a tractable invariant objective from the variational lower bound to optimize the likelihood. Comprehensive experiments over multiple tasks demonstrate that GEODIFF is competitive with the existing state-of-the-art models. Future work includes further improving or accelerating the model with other recent progress of diffusion models, and extending our method to other challenging structures such as proteins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PROOFS</head><p>A.1 PROPERTIES OF THE DIFFUSION MODEL</p><p>We include proofs for several key properties of the probabilistic diffusion model here to be selfcontained. For more detailed discussions, please refer to <ref type="bibr" target="#b15">Ho et al. (2020)</ref>. Let {? 0 , ..., ? T } be a sequence of variances, and ? t = 1 -? t and ?t = t s=1 ? s . The two following properties are crucial for deriving the final tractable objective in equation 9. Property 1. Tractable marginal of the forward process:</p><formula xml:id="formula_22">q(C t |C 0 ) = q(C 1:t |C 0 ) dC 1:(t-1) = N (C t ; ? ?t C 0 , (1 -?t )I).</formula><p>Proof. Let i 's be independent standard Gaussian random variables. Then, by definition of the Markov kernels q(C t |C t-1 ) in equation 2, we have</p><formula xml:id="formula_23">C t = ? ? t C t-1 + ? ? t t = ? ? t ? t-1 C t-2 + ? t ? t-1 t-1 + ? ? t t = ? ? t ? t-1 ? t-1 C t-3 + ? t ? t-1 ? t-2 t-2 + ? t ? t-1 t-1 + ? ? t t = ? ? ? = ? ?t C 0 + ? t ? t-1 ? ? ? ? 2 ? 1 1 + ? ? ? + ? t ? t-1 t-1 + ? ? t t<label>(12)</label></formula><p>Therefore q(C t |C 0 ) is still Gaussian, and the mean of C t is ? ?t C 0 , and the variance matrix is</p><formula xml:id="formula_24">(? t ? t-1 ? ? ? ? 2 ? 1 + ? ? ? + ? t ? t-1 + ? t )I = (1 -?t )I. Then we have: q(C t |C 0 ) = N (C t ; ? ?t C 0 , (1 -?t )I).</formula><p>This property provides convenient closed-form evaluation of C t knowing C 0 :</p><formula xml:id="formula_25">C t = ? ?t C 0 + ? 1 -?t ,</formula><p>where ? N (0, I).</p><p>Besides, it is worth noting that,</p><formula xml:id="formula_26">q(C T |C 0 ) = N (C T ; ? ?T C 0 , (1 -?T )I),</formula><p>where ?T = T t=1 (1 -? t ) approaches zero with large T , which indicates the diffusion process can finally converge into a whitened noisy distribution.</p><p>Property 2. Tractable posterior of the forward process:</p><formula xml:id="formula_27">q(C t-1 |C t , C 0 ) = N (C t-1 ; ? ?t-1 ? t 1 -?t C 0 + ? ? t (1 -?t-1 ) 1 -?t C t , (1 -?t-1 ) 1 -?t ? t I).</formula><p>Proof. Let ?t = 1-?t-1 1-?t ? t , then we can derive the posterior by Bayes rule:</p><formula xml:id="formula_28">q(C t-1 |C t , C 0 ) = q(C t |C t-1 ) q(C t-1 |C 0 ) q(C t |C 0 ) = N (C t ; ? ? t C t-1 , ? t I) N (C t-1 ; ? ?t-1 C 0 , (1 -?t-1 )I) N (C t ; ? ?t C 0 , (1 -?t )I) = (2?? t ) -d 2 (2?(1 -?t-1 )) -d 2 (2?(1 -?t )) d 2 ? exp - C t - ? ? t C t-1 2 2? t - C t-1 - ? ?t-1 C 0 2 2(1 -?t-1 ) + C t - ? ?t C 0 2 2(1 -?t ) = (2? ?t ) -d 2 exp - 1 2 ?t C t-1 - ? ?t-1 ? t 1 -?t C 0 - ? ? t (1 -?t-1 ) 1 -?t C t 2 (13)</formula><p>Then we have the posterior q(C t-1 |C t , C 0 ) as the given form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 PROOF OF PROPOSITION 1</head><p>Let T g be some roto-translational transformations of a group element g ? SE(3), and let p(x T ) be a density which is SE(3)-invariant, i.e., p(x T ) = p(T g (x T )). If the Markov transitions p(x t-1 |x t ) are SE(3)-equivariant, i.e., p(x t-1 |x t ) = p(T g (x t-1 )|T g (x t )), then we have that the density p ? (x 0 ) = p(x T )p ? (x 0:T -1 |x T )dx 1:T is also SE(3)-invariant.</p><p>Proof.</p><formula xml:id="formula_29">p ? (T g (x 0 )) = p(T g (x T ))p ? (T g (x 0:T -1 )|T g (x T ))dx 1:T = p(T g (x T ))? T t=1 p ? (T g (x t-1 )|T g (x t ))dx 1:T = p(x T )? T t=1 p ? (T g (x t-1 )|T g (x t ))dx 1:T (invariant prior p(x T )) = p(x T )? T t=1 p ? (x t-1 |x t )dx 1:T (equivariant kernels p(x t-1 |x t )) = p(x T )p ? (x 0:T -1 |x T )dx 1:T = p ? (x 0 )<label>(14)</label></formula><p>A.3 PROOF OF PROPOSITION 2</p><p>In this section we prove that the output x of GFN defined in equation 5, 6 and 7 is translationally invariant and rotationally equivariant with the input C. Let g ? R 3 denote any translation transformations and orthogonal matrices R ? R 3?3 denote any rotation transformations. let Rx be shorthand for (Rx 1 , ? ? ? , Rx N ). Formally, we aim to prove that the model satisfies:</p><formula xml:id="formula_30">Rx l+1 , h l+1 = GFN(Rx l , RC + g, h l ).<label>(15)</label></formula><p>This equation indicates that, given x l already rotationally equivalent with C, and h l already invariant, then such property can propagate through a single GFN layer to x l+1 and h l+1 .</p><p>Proof. Firstly, given that h l already invariant to SE(3) transformations, we have that the messages m ij calculated from equation 5 will also be invariant. This is because it sorely relies on the distance between two atoms, which are manifestly invariant to rotations Rx</p><formula xml:id="formula_31">l i -Rx l j 2 = (x l i - x l j ) R R(x l i -x l j ) = (x l i -x l j ) I(x l i -x l j ) = x l i -x l j 2 .</formula><p>Formally, the invariance of messages in equation 5 can be written as:</p><formula xml:id="formula_32">m i,j = ? m h l i , h l j , Rx l i -Rx l j 2 , e ij = ? m h l i , h l j , x l i -x l j 2 , e ij . (<label>16</label></formula><formula xml:id="formula_33">)</formula><p>And similarly, the h t+1 updated from equation 6 will also be invariant.</p><p>Next, we prove that the vector x updated from equation 7 preserves rotational equivariance and translational invariance. Given m ij already invariant as proven above, we have that:</p><formula xml:id="formula_34">j?N (i) 1 d ij (Rc i + g -Rc j -g) ? x (m i,j ) = R j?N (i) 1 d ij (c i -c j ) ? x (m i,j ) = Rx l+1 i . (17)</formula><p>Therefore, we have that rotating and translating c results in the same rotation and no translation on x l+1 by updating through equation 7.</p><p>Thus we can conclude that the property defined in equation 15 is satisfied.</p><p>Having proved the equivariance property of a single GFN layer, then inductively, we can draw conclusion that a composition of L GFN layers will also preserve the same equivariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 PROOF OF PROPOSITION 3</head><p>We first derive the variational lower bound (ELBO) objective in equation 8. The ELBO can be calculated as follows:</p><formula xml:id="formula_35">E log p ? (C 0 |G) = E log E q(C 1:T |C 0 ) p ? (C 0:T -1 |G, C T ) ? p(C T ) q(C 1:T |C 0 ) ? E q log p ? (C 0:T -1 |G, C T ) ? p(C T ) q(C 1:T |C 0 ) = E q log p(C T ) - T t=1 log p ? (C t-1 |G, C t ) q(C t |C t-1 ) = E q log p(C T ) -log p ? (C 0 |G, C 1 ) q(C 1 |C 0 ) - T t=2 log p ? (C t-1 |G, C t ) q(C t-1 |C t , C 0 ) + log q(C t-1 |C 0 ) q(C t |C 0 ) = E q log p(C T ) q(C T |C 0 ) -log p ? (C 0 |G, C 1 ) - T t=2 log p ? (C t-1 |G, C t ) q(C t-1 |C t , C 0 ) = -E q KL q(C T |C 0 ) p(C T ) + T t=2 KL q(C t-1 |C t , C 0 ) p ? (C t-1 |G, C t ) -log p ? (C 0 |G, C 1 ) .<label>(18)</label></formula><p>It can be noted that the first term KL q(C T |C 0 ) p(C T ) is a constant, which can be omitted in the objective. Furthermore, for brevity, we also merge the final term log p ? (C 0 |G, C 1 ) into the second term (sum over KL divergences), and finally derive that</p><formula xml:id="formula_36">L ELBO = T t=1 D KL (q(C t-1 |C t , C 0 ) p ? (C t-1 |G, C t ))</formula><p>as in equation 8. Now we consider how to compute the KL divergences as the proposition 3. Since both q(C t-1 |C t , C 0 ) and p ? (C t-1 |G, C t ) are Gaussian share the same covariance matrix ?t I, the KL divergence between them can be calculated by the squared 2 distance between their means weighed by a certain weights 1 2 ?t</p><p>. By the expression of q(C t |C 0 ), we have the reparameterization that C t = ? ?t C 0 + ? 1 -?t . Then we can derive:</p><formula xml:id="formula_37">E q KL q(C t-1 |C t , C 0 ) p ? (G, C t-1 |C t ) = 1 2 ?t E C 0 ? ?t-1 ? t 1 -?t C 0 + ? ? t (1 -?t-1 ) 1 -?t C t - 1 ? ? t C t - ? t ? 1 -?t ? (C t , G, t) 2 = 1 2 ?t E C 0 , ? ?t-1 ? t 1 -?t ? C t - ? 1 -?t ? ?t + ? ? t (1 -?t-1 ) 1 -?t C t - 1 ? ? t C t - ? t ? 1 -?t ? (C t , G, t) 2 = 1 2 ?t ? ? 2 t ? t (1 -?t ) E C 0 , 0 ? C t + -? (C t , G, t) 2 = ? 2 t 2 1-?t-1 1-?t ? t ? t (1 -?t ) E C 0 , -? (C t , G, t) 2 = ? t E C 0 , -? (C t , t) 2 ,<label>(19)</label></formula><p>where ? t represent the wights ?t 2?t(1-?t-1) . And we finish the proof.</p><p>A.5 ANALYSIS OF THE INVARIANT DENSITY IN SEC. 4.2 Given a geometric system x ? R N ?3 , we obtain the CoM-free x by subtracting its CoM. This can be considered as a linear transformation:</p><p>x = Qx, where</p><formula xml:id="formula_38">Q = I 3 ? I N -1 N 1 N 1 T N (<label>20</label></formula><formula xml:id="formula_39">)</formula><p>where I k denotes the k ? k identity matrix and 1 k denotes the k-dimensional vector filled with ones.</p><p>It can be noted that Q is a symmetric projection operator, i.e., Q 2 = Q and Q T = Q. And we also have that rank[Q] = (N -1) ? 3. Furthermore, let U represent the space of CoM-free systems, we can easily have that Qy = y for any y ? U since the CoM of y is already zero.</p><p>Formally, let n = N ? 3 and set R n with an isotropic normal distribution ? = N (0, I n ), then the CoM-free density can be formally written as ? = N (0, QI n Q T ) = N (0, QQ T ). Thus, sampling from ? can be trivially achieved by sampling from ? and then projecting with Q. And ?(y) can be calculated by ?(y) since for any y ? U we have y 2 2 = Qy 2 2 , and thus ?(y) = ?(y). And in this paper, with the SE(3)-equivariant Markov kernels of the reverse process, any CoM-free system will transit to another CoM-free system. And thus we can induce a well-defined Markov chain on the subspace spanned by Q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B OTHER RELATED WORK</head><p>Protein structure generation. There has also been many recent works working on protein structure folding. For example, Boltzmann generators <ref type="bibr" target="#b29">No? et al. (2019)</ref> use flow-based models to generate the structure of protein main chains. AlQuraishi (2019) uses recurrent networks to model the amino acid sequences. <ref type="bibr" target="#b17">Ingraham et al. (2019)</ref> proposed neural networks to learn an energy simulator to infer the protein structures. Most recently, AlphaFold Senior et al. (2020); <ref type="bibr" target="#b20">Jumper et al. (2021)</ref> has significantly improved the performance of protein structure generation. Nevertheless, proteins are mainly linear backbone structures while general molecules are highly branched with various rings, making protein folding approaches unsuitable for our setting.</p><p>Point cloud generation. Recently, some other works <ref type="bibr" target="#b25">(Luo &amp; Hu, 2021;</ref><ref type="bibr" target="#b3">Chibane et al., 2020)</ref> has also been proposed for 3D structure generation with diffusion-based models, but focus on the point cloud problem. Unfortunately, in general, point clouds are not considered as graphs with various atom and bond information, and equivariance is also not widely considered, making these methods fundamentally different from our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EXPERIMENT DETAILS</head><p>In this section, we introduce the details of our experiments. In practice, the means ? are parameterized as compositions of both typical invariant MPNNs <ref type="bibr" target="#b34">(Sch?tt et al., 2017)</ref> and the proposed equivariant GFNs in Sec. 4.2. As a default setup, the MPNNs for parameterizing the means ? are all implemented with 4 layers, and the hidden embedding dimension is set as 128. After the MPNNs, we can obtain the informative invariant atom embeddings, which we denote as h 0 . Then the embeddings h 0 are fed into equivariant layers and updated with equation 5, equation 6, and equation 7 to obtain the equivariant output. For the training of GEODIFF, we train the model on a single Tesla V100 GPU with a learning rate of 0.001 until convergence and Adam <ref type="bibr" target="#b22">(Kingma &amp; Welling, 2013)</ref> as the optimizer. The practical training time is around 48 hours. The other hyper-parameters of GEODIFF are summarized in Tab. 4, including highest variance level ? T , lowest variance level ? T , the variance schedule, number of diffusion timesteps T , radius threshold for determining the neighbor of atoms ? , batch size, and number of training iterations. The results on the GEOM-QM9 dataset are reported in Tab. 5.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples of generated structures from Drugs dataset. For every model, we show the conformation best-aligned with the ground truth. More examples are provided in Appendix E.</figDesc><graphic url="image-7.png" coords="9,137.30,93.44,91.64,104.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results on the GEOM-Drugs dataset, without FF optimization. The COV-R and MAT-R results of CVGAE, GRAPHDG, CGCF, and CONFGF are borrowed from Shi</figDesc><table><row><cell></cell><cell cols="2">COV-R (%) ?</cell><cell>MAT-R (?) ?</cell><cell cols="2">COV-P (%) ?</cell><cell cols="2">MAT-P (?) ?</cell></row><row><cell>Models</cell><cell cols="7">Mean Median Mean Median Mean Median Mean Median</cell></row><row><cell>CVGAE</cell><cell>0.00</cell><cell>0.00</cell><cell>3.0702 2.9937</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GRAPHDG</cell><cell>8.27</cell><cell>0.00</cell><cell>1.9722 1.9845</cell><cell>2.08</cell><cell>0.00</cell><cell cols="2">2.4340 2.4100</cell></row><row><cell>CGCF</cell><cell>53.96</cell><cell>57.06</cell><cell cols="2">1.2487 1.2247 21.68</cell><cell>13.72</cell><cell cols="2">1.8571 1.8066</cell></row><row><cell>CONFVAE</cell><cell>55.20</cell><cell>59.43</cell><cell cols="2">1.2380 1.1417 22.96</cell><cell>14.05</cell><cell cols="2">1.8287 1.8159</cell></row><row><cell>GEOMOL</cell><cell>67.16</cell><cell>71.71</cell><cell>1.0875 1.0586</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CONFGF</cell><cell>62.15</cell><cell>70.93</cell><cell cols="2">1.1629 1.1596 23.42</cell><cell>15.52</cell><cell cols="2">1.7219 1.6863</cell></row><row><cell cols="2">GEODIFF-A 88.36</cell><cell>96.09</cell><cell cols="2">0.8704 0.8628 60.14</cell><cell>61.25</cell><cell cols="2">1.1864 1.1391</cell></row><row><cell cols="2">GEODIFF-C 89.13</cell><cell>97.88</cell><cell cols="2">0.8629 0.8529 61.47</cell><cell>64.55</cell><cell cols="2">1.1712 1.1232</cell></row></table><note><p>*</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results on the GEOM-Drugs dataset, with FF optimization.</figDesc><table><row><cell></cell><cell cols="2">COV-R (%) ?</cell><cell cols="2">MAT-R (?) ?</cell><cell cols="2">COV-P (%) ?</cell><cell>MAT-P (?) ?</cell></row><row><cell>Models</cell><cell cols="2">Mean Median</cell><cell>Mean</cell><cell cols="3">Median Mean Median</cell><cell>Mean</cell><cell>Median</cell></row><row><cell>RDKIT</cell><cell>60.91</cell><cell>65.70</cell><cell cols="2">1.2026 1.1252</cell><cell>72.22</cell><cell>88.72</cell><cell>1.0976 0.9539</cell></row><row><cell cols="5">GEODIFF + FF 92.27 100.00 0.7618 0.7340</cell><cell>84.51</cell><cell>95.86</cell><cell>0.9834 0.9221</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>MAE of predicted ensemble properties in eV.</figDesc><table><row><cell>Method</cell><cell>E</cell><cell>E min</cell><cell>?</cell><cell>? min</cell><cell>? max</cell></row><row><cell>RDKIT</cell><cell cols="5">0.9233 0.6585 0.3698 0.8021 0.2359</cell></row><row><cell>GRAPHDG</cell><cell cols="5">9.1027 0.8882 1.7973 4.1743 0.4776</cell></row><row><cell>CGCF</cell><cell cols="5">28.9661 2.8410 2.8356 10.6361 0.5954</cell></row><row><cell>CONFVAE</cell><cell cols="5">8.2080 0.6100 1.6080 3.9111 0.2429</cell></row><row><cell>CONFGF</cell><cell cols="5">2.7886 0.1765 0.4688 2.1843 0.1433</cell></row><row><cell>GEODIFF</cell><cell cols="5">0.25974 0.1551 0.3091 0.7033 0.1909</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Additional hyperparameters of our GEODIFF.</figDesc><table><row><cell>Task</cell><cell>? 1</cell><cell>? T</cell><cell>? scheduler</cell><cell>T</cell><cell>?</cell><cell cols="2">Batch Size Train Iter.</cell></row><row><cell cols="3">QM9 1e-7 2e-3</cell><cell>sigmoid</cell><cell cols="2">5000 10?</cell><cell>64</cell><cell>1M</cell></row><row><cell cols="3">Drugs 1e-7 2e-3</cell><cell>sigmoid</cell><cell cols="2">5000 10?</cell><cell>32</cell><cell>1M</cell></row><row><cell cols="4">D ADDITIONAL EXPERIMENTS</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">D.1 RESULTS FOR GEOM-QM9</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Code is available at https://github.com/MinkaiXu/GeoDiff.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Detailed derivations are provided in the Appendix A.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The detailed derivations and full proofs are provided in Appendix A.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENT</head><p>Minkai thanks <rs type="person">Huiyu Cai</rs>, <rs type="person">David Wipf</rs>, <rs type="person">Zuobai Zhang</rs>, and <rs type="person">Zhaocheng Zhu</rs> for their helpful discussions and comments. This project is supported by the <rs type="funder">Natural Sciences and Engineering Research Council (NSERC)</rs> <rs type="grantName">Discovery Grant</rs>, the <rs type="funder">Canada CIFAR AI Chair Program</rs>, collaboration grants between <rs type="funder">Microsoft Research and Mila</rs>, <rs type="institution">Samsung Electronics Co., Ltd.</rs>, <rs type="funder">Amazon Faculty Research Award, Tencent AI Lab Rhino-Bird Gift Fund</rs> and a <rs type="funder">NRC Collaborative R&amp;D Project</rs> (<rs type="grantNumber">AI4D-CORE-06</rs>). This project was also partially funded by <rs type="funder">IVADO Fundamental Research Project</rs> grant <rs type="grantNumber">PRF-2019-3583139727</rs>. The Stanford team is supported by <rs type="funder">NSF</rs>(#<rs type="grantNumber">1651565</rs>, #<rs type="grantNumber">1522054</rs>, #<rs type="grantNumber">1733686</rs>), <rs type="funder">ONR</rs> (<rs type="grantNumber">N000141912145</rs>), <rs type="funder">AFOSR</rs> (<rs type="grantNumber">FA95501910024</rs>), <rs type="funder">ARO</rs> (<rs type="grantNumber">W911NF-21-1-0125</rs>) and Sloan Fellowship.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_t2rMqcQ">
					<orgName type="grant-name">Discovery Grant</orgName>
				</org>
				<org type="funding" xml:id="_Wy7npuQ">
					<idno type="grant-number">AI4D-CORE-06</idno>
				</org>
				<org type="funding" xml:id="_Nu8WMYY">
					<idno type="grant-number">PRF-2019-3583139727</idno>
				</org>
				<org type="funding" xml:id="_jwfkA9G">
					<idno type="grant-number">1651565</idno>
				</org>
				<org type="funding" xml:id="_FSPdEU6">
					<idno type="grant-number">1522054</idno>
				</org>
				<org type="funding" xml:id="_5aEqcKn">
					<idno type="grant-number">1733686</idno>
				</org>
				<org type="funding" xml:id="_uT73fb2">
					<idno type="grant-number">N000141912145</idno>
				</org>
				<org type="funding" xml:id="_E8md38H">
					<idno type="grant-number">FA95501910024</idno>
				</org>
				<org type="funding" xml:id="_W58U68b">
					<idno type="grant-number">W911NF-21-1-0125</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We also test our method with fewer diffusion steps. Specifically, we test the setting with T = 1000, ? 1 =1e-7 and ? T =9e-3. The results on the more challenging Drugs dataset are shown in Tab. 6. Compared with the results in Tab. 1, we can observe that when setting the diffusion steps as 1000, though slightly weaker than the performance with 5000 decoding steps, the model can already outperforms all existing baselines. Note that, the most competitive baseline CONFGF <ref type="bibr" target="#b26">(Shi et al., 2021)</ref> also requires 5000 sampling steps, which indicates that our model can achieve better performance with fewer computational costs compared with the state-of-the-art method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E MORE VISUALIZATIONS</head><p>We provide more visualization of generated structures in Fig. <ref type="figure">3</ref>. The molecules are chosen from the test split of GEOM-Drugs dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Conformations</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">End-to-end differentiable learning of protein structure</title>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Alquraishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="292" to="301" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Geom: Energy-annotated molecular conformations for property prediction and molecular generation</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Gomez-Bombarelli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05531</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Se (3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Batzner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tess</forename><forename type="middle">E</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">P</forename><surname>Mailoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mordechai</forename><surname>Kornbluth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Molinari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Kozinsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03164</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Implicit functions in feature space for 3d shape reconstruction and completion</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Chibane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thiemo</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6970" to="6981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Non-equilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">Sybren</forename><surname>Ruurds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De</forename><surname>Groot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Mazur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>Courier Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Density estimation using Real NVP</title>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al?n</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Se(3)-transformers: 3d rototranslation equivariant attention networks</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volker</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Octavian-Eugen</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lagnajit</forename><surname>Pattanaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><forename type="middle">W</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Klavs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">H</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><surname>Geomol</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07802</idno>
		<title level="m">Torsional geometric generation of molecular 3d conformer ensembles</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Inverse design of 3d molecular structures with conditional generative neural networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Niklas Wa Gebauer</surname></persName>
		</author>
		<author>
			<persName><surname>Gastegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Stefaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Hessmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristof</forename><forename type="middle">T</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><surname>Sch?tt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04824</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Torsionnet: A reinforcement learning approach to sequential conformer search</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gogineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziping</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Exequiel</forename><surname>Punzalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runxuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">A</forename><surname>Kammeraad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambuj</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zimmerman</surname></persName>
		</author>
		<idno>ArXiv, abs/2006.07078</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Merck molecular force field. v. extension of mmff94 using experimental data, additional computational data, and empirical rules</title>
		<author>
			<persName><surname>Thomas A Halgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Chemistry</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="616" to="641" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Conformation generation: the state of the art</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><surname>Hawkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1747" to="1756" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02136</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11239</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Forcenet: A graph neural network for large-scale quantum chemistry simulation</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammed</forename><surname>Shuaibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anuroop</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning protein structure with a differentiable simulator</title>
		<author>
			<persName><forename type="first">John</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">J</forename><surname>Riesselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debora</forename><forename type="middle">S</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Junction tree variational autoencoder for molecular graph generation</title>
		<author>
			<persName><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04364</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning from protein structure with geometric vector perceptrons</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Eismann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Suriana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">Lamarre</forename><surname>Townshend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Dror</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Highly accurate protein structure prediction with alphafold</title>
		<author>
			<persName><forename type="first">John</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><surname>Tunyasuvunakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustin</forename><surname>??dek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Potapenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">596</biblScope>
			<biblScope unit="issue">7873</biblScope>
			<biblScope unit="page" from="583" to="589" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A solution for the best rotation to relate two sets of vectors</title>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Kabsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Crystallographica Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="922" to="923" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Equivariant flows: Exact likelihood generative learning for symmetric densities</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Noe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Euclidean distance geometry and applications</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Liberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlile</forename><surname>Lavor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Maculan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Mucherino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="69" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Diffusion probabilistic models for 3d point cloud generation</title>
		<author>
			<persName><forename type="first">Shitong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<idno>ArXiv, abs/2103.01458</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Predicting molecular conformation via dynamic graph score matching</title>
		<author>
			<persName><forename type="first">Shitong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chence</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Molecular geometry prediction using a deep generative graph neural network</title>
		<author>
			<persName><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seokho</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00314</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Relevance of rotationally equivariant convolutions for predicting molecular properties</title>
		<author>
			<persName><forename type="first">B</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>No?</surname></persName>
		</author>
		<idno>ArXiv, abs/2008.08461</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>No?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">365</biblScope>
			<biblScope unit="issue">6457</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Quantum chemistry structures and properties of 134 kilo molecules</title>
		<author>
			<persName><forename type="first">Raghunathan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pavlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Dral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Von</forename><surname>Anatole</surname></persName>
		</author>
		<author>
			<persName><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Better informed distance geometry: Using what we know to improve conformation generation</title>
		<author>
			<persName><forename type="first">Sereina</forename><surname>Riniker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">A</forename><surname>Landrum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2562" to="2574" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Victor</forename><surname>Garcia Satorras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emiel</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><forename type="middle">B</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingmar</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.09016</idno>
		<title level="m">E (n) equivariant normalizing flows for molecule generation in 3d</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">E(n) equivariant graph neural networks</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Garcia Satorras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emiel</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Schnet: A continuous-filter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName><forename type="first">Kristof</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huziel</forename><surname>Enoc Sauceda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="991" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improved protein structure prediction using potentials from deep learning</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Andrew W Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongli</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><surname>??dek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Alexander Wr Nelson</surname></persName>
		</author>
		<author>
			<persName><surname>Bridgland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">577</biblScope>
			<biblScope unit="issue">7792</biblScope>
			<biblScope unit="page" from="706" to="710" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Graphaf: a flow-based autoregressive model for molecular graph generation</title>
		<author>
			<persName><forename type="first">Chence</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09382</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning gradient fields for molecular conformation generation</title>
		<author>
			<persName><forename type="first">Chence</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shitong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Muhammed</forename><surname>Shuaibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adeesh</forename><surname>Kolluru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anuroop</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Ulissi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitnick</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09575</idno>
		<title level="m">Rotation invariant graph neural networks using spin convolutions</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A generative model for molecular distance geometry</title>
		<author>
			<persName><forename type="first">Gregor</forename><surname>Simm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Miguel Hernandez-Lobato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Hal</forename><surname>Daum?</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Iii</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="8949" to="8958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Symmetryaware actor-critic for 3d molecular design</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Simm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G?bor</forename><surname>Pinsler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jos?</forename><surname>Cs?nyi</surname></persName>
		</author>
		<author>
			<persName><surname>Miguel Hern?ndez-Lobato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Psi4 1.4: Open-source software for high-throughput quantum chemistry</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Simmonett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Parrish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raimondas</forename><surname>Schieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Galvelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName><surname>Di Remigio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Asem Alenaizan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><forename type="middle">P</forename><surname>Lehtola</surname></persName>
		</author>
		<author>
			<persName><surname>Misiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of chemical physics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03585</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02502</idno>
		<title level="m">Denoising diffusion implicit models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11918" to="11930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Improved techniques for training score-based generative models</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lusann</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Kohlhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Riley</surname></persName>
		</author>
		<title level="m">Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds</title>
		<imprint>
			<publisher>ArXiv</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">3d steerable cnns: Learning rotationally equivariant features in volumetric data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Boomsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning neural generative dynamics for molecular conformation generation</title>
		<author>
			<persName><forename type="first">Minkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shitong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">An end-to-end framework for molecular conformation generation via bilevel programming</title>
		<author>
			<persName><forename type="first">Minkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wujie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shitong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chence</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Gomez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.07246</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep Potential Molecular Dynamics: A Scalable Model with the Accuracy of Quantum Mechanics</title>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiequn</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Car</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">143001</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Direct molecular conformation generation</title>
		<author>
			<persName><forename type="first">Jinhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shufang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.01356</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
