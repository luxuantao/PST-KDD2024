<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring Principles-of-Art Features For Image Emotion Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yue</forename><surname>Gao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaolei</forename><surname>Jiang</surname></persName>
							<email>xljiang@hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongxun</forename><surname>Yao</surname></persName>
							<email>h.yao@hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoshuai</forename><surname>Sun</surname></persName>
							<email>xiaoshuaisun@hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring Principles-of-Art Features For Image Emotion Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E7DC670A20F493D347A1A2633BE81657</idno>
					<idno type="DOI">10.1145/2647868.2654930</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.1 [Information storage and retrieval]: Content Analysis and Indexing</term>
					<term>I.4.7 [Image processing and computer vision]: Feature Measurement</term>
					<term>J.5 [Computer Applications]: Arts and Humanities Algorithms, Human Factors, Experimentation, Performance Image Emotion</term>
					<term>Affective Image Classification</term>
					<term>Image Features</term>
					<term>Art Theory</term>
					<term>Principles of Art</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Emotions can be evoked in humans by images. Most previous works on image emotion analysis mainly used the elements-of-artbased low-level visual features. However, these features are vulnerable and not invariant to the different arrangements of elements. In this paper, we investigate the concept of principles-of-art and its influence on image emotions. Principles-of-art-based emotion features (PAEF) are extracted to classify and score image emotions for understanding the relationship between artistic principles and emotions. PAEF are the unified combination of representation features derived from different principles, including balance, emphasis, harmony, variety, gradation, and movement. Experiments on the International Affective Picture System (IAPS), a set of artistic photography and a set of peer rated abstract paintings, demonstrate the superiority of PAEF for affective image classification and regression (with about 5% improvement on classification accuracy and 0.2 decrease in mean squared error), as compared to the stateof-the-art approaches. We then utilize PAEF to analyze the emotions of master paintings, with promising results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Humans are able to perceive and understand images only at high level semantics (including cognitive level and affective level <ref type="bibr" target="#b10">[10]</ref>), rather than at low level visual features. Most previous works on image content analysis focus on understanding the cognitive aspects of images, such as object detection and recognition. Little research effort has been dedicated to the understanding of images at the affective level, due to the subjective evaluation on emotions and the "affective gap", which can be defined as "the lack of coincidence between the measurable signal properties, commonly referred to as features, and the expected affective state in which the user is brought by perceiving the signal" ( <ref type="bibr" target="#b10">[10]</ref>, p. 91). However, with the increasing use of digital photography technology by the public and users' high requirement for image understanding, the analysis of image content at higher semantic levels, in particular the affective level, is becoming increasingly important.</p><p>For affective level analysis, how to extract emotion related features is the key problem. Most existing works target low level visual features based on the elements-of-art, such as color, texture, shape, line, etc. Obviously, these features are not invariant to their different arrangements and their link to emotions is weak, while different element arrangements share different meanings and evoke different emotions. Therefore, elements must be carefully arranged and orchestrated into meaningful regions and images to describe specific semantics and emotions. The rules, tools or guidelines of arranging and orchestrating the elements-of-art in an artwork are known as the principles-of-art, which consider various artistic aspects including balance, emphasis, harmony, variety, gradation, movement, rhythm, and proportion <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b12">12]</ref>. Different combinations of these principles can evoke different emotions. For example, symmetric and harmonious images tend to express positive emotions, while images with strong color contrast may evoke negative emotions <ref type="bibr" target="#b31">[31]</ref> (see Section 5.2). Further, the artistic principles are more interpretable by humans than elements <ref type="bibr" target="#b5">[5]</ref>.</p><p>Inspired by these observations, we propose to study, formulate, and implement the principles-of-art systematically, based on the related art theory and computer vision research. After quantizing each principle, we combine them together to construct image emotion features. Different from previous low level visual features, PAEF take the arrangements and orchestrations of different elements into account, and it can be used to classify and score image emotions evoked in humans. The framework of our proposed method is shown in Figure <ref type="figure" target="#fig_0">1</ref>. We then apply the proposed PAEF to predict the emotions implied in famous artworks to capture the masters' emotional status.</p><p>The rest of this paper is organized as follows. Section 2 introduces related work on affective content analysis, aesthetics, composition and photo quality assessment. We summarize the elementsof-art-based low level emotion features (EAEF) and their limitations in emotion prediction as a preliminary in Section 3. The proposed PAEF are described in Section 4. Experimental evaluation, analysis and applications are presented in Section 5, followed by conclusion and future work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification image dataset</head><p>Principles-of-art Emotion Features  </p><note type="other">Resizing</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Affective content analysis. Some research efforts have been made recently to improve the accuracy of affective understanding in images and videos. Table <ref type="table" target="#tab_0">1</ref> presents the related works, which can be divided into different types, according to the analyzed multimedia type, the adopted emotion model and the extracted features.</p><p>Generally, there are two typical models to represent emotions: categorical emotion states (CES), and dimensional emotion space (DES). CES methods <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b13">13]</ref> consider emotions to be one of a few basic categories, such as fear, contentment, sadness, etc. DES methods mostly employ the 3-D valence-arousal-control emotion space <ref type="bibr" target="#b32">[32]</ref>, 3-D natural-temporalenergetic connotative space <ref type="bibr" target="#b3">[3]</ref>, 3-D activity-weight-heat emotion factors <ref type="bibr" target="#b33">[33]</ref>, and 2-D valence-arousal emotion space <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b41">41]</ref> for affective representation and modeling. CES in the classification task is easier for users to understand and label, while DES in the regression task is more flexible and richer in the descriptive power. Similar to <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b42">42]</ref>, we adopt CES to classify emotions into eight categories defined in a rigorous psychological study <ref type="bibr" target="#b27">[27]</ref>, including anger, disgust, fear, sadness as negative emotions, and amusement, awe, contentment, excitement as positive emotions. We also use valence-arousal DES to predict the scores of image emotions as in <ref type="bibr" target="#b24">[24]</ref>.</p><p>From a feature's view point, most methods extract low level visual and audio features. Lu et al. <ref type="bibr" target="#b24">[24]</ref> investigated the computability of emotion through shape features. Machajdik and Hanbury <ref type="bibr" target="#b26">[26]</ref> exploited theoretical and empirical concepts from psychology and art theory to extract image features that are specific to the domain of artworks. In their method, color and texture are used as low level features, composition are used as mid level features, while image semantic content including human faces and skin are used as high level features. Besides color features, Jia et al. <ref type="bibr" target="#b15">[15]</ref> also extracted social correlation features for social network images. Solli and Lenz <ref type="bibr" target="#b33">[33]</ref> classified emotions using emotion-histogram features and bag-of-emotion features derived for patches surrounding each interest point. Irie et al. <ref type="bibr" target="#b13">[13]</ref> extracted mid level features based on affective audio-visual words and proposed a latent topic driving model for video classification task. Borth et al. proposed to infer emotions based on the understanding of visual concepts <ref type="bibr" target="#b4">[4]</ref>. A large-scale visual sentiment ontology composed of adjective noun pairs (ANPs) is constructed and SentiBank is proposed to detect the presence of ANPs. Popular features in previous works on image emotion analysis are mainly based on elements-of-art, such as color, texture, shape, etc. Machajdik and Hanbury <ref type="bibr" target="#b26">[26]</ref> extracted composition features, some of which can be considered as principles. However, there is still no systematic study on the use of principles-of-art for image emotion analysis.</p><p>Aesthetics, composition and photo quality assessment. Aesthetics, composition in images and the quality of photos are strongly related to humans' emotions. Joshi et al. <ref type="bibr" target="#b16">[16]</ref> and Datta et al. <ref type="bibr" target="#b7">[7]</ref> discussed key aspects of the problem of computational inference of aesthetics and emotions from natural images. Liu et al. <ref type="bibr" target="#b21">[21]</ref> evaluated the composition aesthetics of a given image based on measuring composition guidelines and changed the relative position of salient regions using a compound operator of crop-andretarget. Aesthetics and interestingness are predicted through high level describable attributes, including compositional, content and sky-illumination attributes <ref type="bibr">[8]</ref>. Compositional features are also exploited for scene recognition <ref type="bibr" target="#b30">[30]</ref> and category-level image classification <ref type="bibr" target="#b37">[37]</ref>. Based on professional photography techniques, Luo and Tang <ref type="bibr" target="#b25">[25]</ref> extracted the subject region from a photo and formulated a number of high-level semantic features based on this subject and background division. Sun et al. <ref type="bibr" target="#b35">[35]</ref> presented a computational visual attention model to assess photos by using the rate of focused attention. In this work, we expand related research on computer vision and multimedia to measure the artistic principles for affective image classification and score prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ELEMENTS OF ART: A PRELIMINARY</head><p>Low level features extracted for emotion recognition are mostly based on the elements-of-art (EAEF), including color, value, line, texture, shape, form and space <ref type="bibr" target="#b12">[12]</ref>, as shown in Figure <ref type="figure" target="#fig_2">2</ref>. In this section, we briefly introduce EAEF and their limitations in image emotion prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Elements-of-art-based Low Level Features</head><p>Color. An element of art which has three properties: hue, intensity, and value, representing the name, brightness and lightness or darkness of a color. Color is often used effectively by artists to induce emotional effects, such as saturation, brightness, hue, and colorfulness <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b15">15]</ref>.</p><p>Value. An element of art that describes the lightness or darkness of a color. Value is usually found to be an important element in works of art. This is true with drawings, prints, photographs, most sculpture, and architecture. The description of lightness or darkness is often used as value features <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b38">38]</ref>.</p><p>Line. An element of art which is a continuous mark made on some surface by a moving point. There are mainly two types of lines, emphasizing lines and de-emphasizing lines. Emphasizing lines, better known as contour lines, show and outline the edges or contours of an object. When artists stress contours or outlines in their work, the pieces are usually described as lines. Not all artists emphasize lines in their works. Some even try to hide the outline of objects in their works. De-emphasizing lines are used to describe works that do not stress the contours or outlines of objects.</p><p>Lines can be used to suggest movement in some direction. They are also used in certain ways to give people different feelings. For example, horizontal lines suggest calmness and usually make people feel relaxed, vertical lines suggest strength and stability, diagonal lines suggest tension, and curved lines suggest flowing movement <ref type="bibr" target="#b26">[26]</ref>. Usually, the amounts and lengths of static and dynamic lines are calculated by Hough transform to describe lines <ref type="bibr" target="#b26">[26]</ref>.</p><p>Texture. An element of art which is used to describe the surface quality of one object. It refers to how things feel, or look as if they might feel if you were able to touch it. Some artists paint carefully to give their paintings a smooth appeal, while others use a lot of paint to produce a rough texture. The most frequently used texture features are wavelet-based features, Tamura features, graylevel co-occurrence matrix <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b20">20]</ref> and LBP features.</p><p>Shape and Form. Shape is flat and has only 2 dimensions, height and width. The descriptions of roundness, angularity, simplicity, and complexity are used as shape features <ref type="bibr" target="#b24">[24]</ref>. Form is 3 dimensional with height, width and depth, thus having mass and volume.</p><p>Space. An element of art which refers to the distance or area between, around, above, below or within things.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Limitations of EAEF</head><p>These elements-of-art based low level visual features are easy to extract based on current computer vision and multimedia research. However, there are several disadvantages using them to model image emotions:</p><p>(1) Weak link to emotions <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b26">26]</ref>. EAEF suffer from the greatest "affective gap" and are vulnerable and not invariant to the different arrangements of elements, resulting in the poor performance on image emotion recognition. These low level features cannot represent high level emotions well.</p><p>(2) Not interpretable by humans <ref type="bibr" target="#b1">[1]</ref>. As EAEF are extracted from low level view point. Humans cannot understand the meanings of these features and why such a set of features induce a particular emotion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">PROPOSED EMOTION FEATURES</head><p>In this section, we systematically study and formulize 6 artistic principles. For each principle, we first explain the concepts and meanings, under the art theory in <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b12">12]</ref>, and then translate  these concepts into mathematical formulae for quantization measurement. As rhythm and proportion are ambiguously defined, we do not take them into account here.</p><note type="other">Shade Tint</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Balance</head><p>Balance (symmetry) refers to the feeling of equilibrium or stability of an art work. The artists arrange balance to set the dynamics of a composition. There are three types of balances: symmetrical, asymmetrical and radial. Symmetrical balance is the most visually stable, and characterized by an exact or nearly exact compositional design on both sides of the horizontal, vertical or any axis of the picture plane. If the two halves of an image are identical or very similar, it is symmetrical balance. Asymmetry uses compositional elements that are offset from each other, creating a visually unstable balance. Asymmetrical balance is the most dynamic because it creates a more complex design construction. Radial balance refers to balance within a circular shape or object, offering stability and a point of focus at the center of the composition <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b12">12]</ref>.</p><p>Since the asymmetrical balance is difficult to measure mathematically, in this paper we only consider symmetry, including bilateral symmetry, rotational symmetry <ref type="bibr" target="#b22">[22]</ref> and radial symmetry <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b28">28]</ref>. Symmetry can be seen as the reverse measurement of asymmetry.</p><p>To detect bilateral and rotational symmetry, we use the symmetry detection method in <ref type="bibr" target="#b22">[22]</ref>, which is based on matching symmetrical pairs of feature points. The method for determining feature points should be rotationally invariant, so SIFT is a good choice, although scale-invariance is not necessary. Each feature can be represented by a point vector describing its location in x, y coordinates, its orientation and (optionally) scale. Every pair of feature points is a potential candidate for a symmetrical pair. In the case of bilateral symmetry, each pair of matching points defines a potential axis of symmetry passing perpendicularly through the mid-point of the line joining these two points. Unlike bilateral symmetry detection, detecting rotational symmetry does not require the development of additional feature descriptors. It can be simply detected by matching the features against each other. Given a pair of non-parallel feature point vectors, there exists a point about which feature vector can be rotated to precisely align with another feature vector. The Hough transform <ref type="bibr" target="#b2">[2]</ref> is used to find dominant symmetry axes or centers. Each potential symmetrical pair casts a vote in Hough space weighted by their symmetry magnitude. The rotational symmetry magnitude may be set to unity, while the bilateral symmetry magnitude may involve the discrepancy between the orientation of one feature point and the mirrored orientation of another feature point. Finally the symmetries exhibited by all individual pairs in a voting space are accumulated to determine the dominant symmetries present in the image. The result is blurred with a Gaussian and   the maxima are identified as dominant axes of bilateral symmetry or centres of rotational symmetry. We compute symmetry number, radius, angle and strength of the maximum symmetry for bilateral symmetry, symmetry number, center and strength of the maximum symmetry for rotational symmetry, as shown in Figure <ref type="figure" target="#fig_3">3</ref>.</p><p>Based on the symmetry detection method in <ref type="bibr" target="#b23">[23]</ref>, we compute the distribution of symmetry map after radial symmetry transformation for radial symmetry (see Section 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Emphasis</head><p>Emphasis, also known as contrast, is used to stress the difference of certain elements. It can be accomplished by using sudden and abrupt changes in elements. Emphasis is usually used to direct and focus viewers' attention to the most important area or centers of interests of a design, because it catches your attention <ref type="bibr" target="#b12">[12]</ref>. We adopt Itten's color contrasts <ref type="bibr" target="#b14">[14]</ref> and Sun's rate of focused attention (RFA) <ref type="bibr" target="#b35">[35]</ref> to measure the principle of emphasis.</p><p>Itten defined and identified strategies for successful color combinations <ref type="bibr" target="#b14">[14]</ref>. Seven methodologies were devised to coordinate colors using hue's contrasting properties. Itten contrasts include contrast of saturation, light and dark, extension, complements, hue, warm and cold and the simultaneous contrast. We calculate six color contrasts by the mathematical expressions in <ref type="bibr" target="#b26">[26]</ref> and represent the contrast of extension as the standard deviation of the pixel amount of 11 basic colors as in Section 4.4.</p><p>RFA was proposed to measure the focus rate of an image when people watch it <ref type="bibr" target="#b35">[35]</ref>. FRA is defined as the attention focus on some predefined aesthetic templates or some statistical distributions according to image's saliency map. Here we adopt Sun's response map method <ref type="bibr" target="#b34">[34]</ref> to estimate saliency. Besides the statistic subject mask coincidence with "Rule of the third" composition method, defined in <ref type="bibr" target="#b35">[35]</ref>, we use another two diagonal aesthetic templates <ref type="bibr" target="#b21">[21]</ref>. A 3 dimensional RFA vector is obtained by computing, RF A(i) = , <ref type="bibr" target="#b1">(1)</ref> where W id and Hei denote the width and height of image I, while Saliency(x, y) and M aski(x, y) are the saliency value and mask value at pixel (x, y), respectively. In Eq. ( <ref type="formula">1</ref>), i = 1, 2, 3, representing different aesthetic templates. Illustrations of different masks are shown in Figure <ref type="figure" target="#fig_4">4</ref>, 5, and 6, together with related images, saliency maps and RFA scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Harmony</head><p>Harmony, also known as unity, refers to a way of combining similar elements (such as line, shape, color, texture) in an artwork to accent their similarities. It can be accomplished by using repetition and gradual changes when the components of an image are perceived as harmonious. Pieces that are in harmony give the work a sense of completion and have an overall uniform appearance <ref type="bibr" target="#b12">[12]</ref>.</p><p>Inspired by Kass' idea of smoothed filters for local histogram <ref type="bibr" target="#b18">[18]</ref>, we compute the harmony intensity of each pixel on its hue and gradient direction in a neighborhood. We divide the circular hue or gradient direction equally into eight parts, which are separated into two adjacent groups c = {i1, i2, . . . , i k |0 ≤ ij ≤ 7, j = 1, 2, . . . , k} and I\c (see Figure <ref type="figure">7(a)</ref>), where i k+1 ≡ i k +1(mod8), I = {0, 1, . . . , 7}. The harmony intensity at pixel p(x, y) is defined as</p><formula xml:id="formula_0">H(x, y) = min c e -|h m (c)-h m (I\c)| |i m (c) -i m (I \ c)|,<label>(2)</label></formula><p>where</p><formula xml:id="formula_1">h m (c) = max i∈c h i (c) i m (c) = arg max i∈c h i (c),<label>(3)</label></formula><p>where hi(c) is the hue or gradient direction in groups c. The harmony intensity of the whole image is the sum of all pixels' harmony intensity, that is   </p><formula xml:id="formula_2">H = (x,y) H(x, y).<label>(4</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Variety</head><p>Variety is used to create complicated images by combining different elements. A picture made up of many different hues, values, lines, textures, or shapes would be described as a complex picture, which increases humans' visual interestingness <ref type="bibr" target="#b12">[12]</ref>.</p><p>However, harmony and variety are not opposites. A careful blend of the two principles is essential to the success of almost any work of art. Artists who focus only on harmony and ignore variety might find it easier to achieve balance and unity; but the visual interest in the piece could be lost. On the other hand, artists who focus only on variety and not harmony would make their works too complex; and consequently, the overall unity of the piece could be lost, which makes viewers confused <ref type="bibr" target="#b12">[12]</ref>.</p><p>Each color has a special meaning and is used in certain ways by artists. We count how many basic colors (black, blue, brown, green, gray, orange, pink, purple, red, white, and yellow) are present and the pixel amount of each color using the algorithm proposed by Weijer et al. <ref type="bibr" target="#b36">[36]</ref>. Image examples of different color variety are shown in Figure <ref type="figure" target="#fig_9">8</ref>. Gradient depicts the changes of values and directions of pixels in an image. We calculate the distribution of gradient statistically (Figure <ref type="figure">7(b)</ref>). For directions, we count the number of pixels in the eight regions equally divided of the circle. For lengths, we divide the relative maximum length (RML) into 8 parts equally, by computing RML as,</p><formula xml:id="formula_3">RM L = µ + 5σ,<label>(5)</label></formula><p>where µ and σ are respectively the mean and standard deviation of the gradient matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Gradation</head><p>Gradation refers to a way of combining elements by using a series of gradual changes. For example, gradation may be a gradual change of a dark value to a light value <ref type="bibr" target="#b12">[12]</ref>.</p><p>We adopt the concepts of pixel-wise windowed total variation and windowed inherent variation proposed by Xu et al. <ref type="bibr" target="#b40">[40]</ref> and their combination to measure gradation for each pixel. The windowed total variation for pixel p(x, y) in image I is defined as  where q ∈ R(p), R(q) is a rectangular region centered at p, Dx(p) and Dy(p) are windowed total variations in the x and y directions for pixel p, which count the absolute spatial difference within the window R(q). gp,q is a weighting function</p><formula xml:id="formula_4">Dx(p) = q gp,q|(∂xI)q|, Dy(p) = q gp,q|(∂yI)q|,<label>(6)</label></formula><formula xml:id="formula_5">gp,q = exp - (xp -xq) 2 + (yp -yq) 2 2σ 2 , (<label>7</label></formula><formula xml:id="formula_6">)</formula><p>where σ controls the spatial scale of the window. The windowed inherent variation for pixel p(x, y) in image I is defined as Lx(p) = | q gp,q(∂xI)q|, Ly(p) = | q gp,q(∂yI)q|. <ref type="bibr">(8)</ref> Different from Dx(p) and Dy(p), Lx(p) and Ly(p) capture the overall spatial variation, without incorporating modules.</p><p>It has been proven that in the relative total variation (RTV) defined in Equ. <ref type="bibr" target="#b9">(9)</ref>, opposite gradients in a window cancel out each other (Figure <ref type="figure" target="#fig_8">10</ref>), regardless whether the pattern is isotropic or not. We adopt the sum of RTV, the sum of windowed total variation and the sum of windowed inherent variation to measure an image's relative gradation and absolute gradation, respectively. </p><formula xml:id="formula_7">RG = p RT V (p) = p Dx(p) Lx(p) + ε + Dy(p) Ly(p) + ε ,<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Movement</head><p>Movement is used to create the look and feel of action. It guides and moves the viewers' eye throughout the work of art. Movement is achieved through placement of elements so that the eye follows a certain path, like the curve of a line, the contours of shapes, or the repetition of certain colors, textures, or shapes <ref type="bibr" target="#b12">[12]</ref>.</p><p>Based on Super Gaussian Component analysis, Sun et al.</p><p>[34] obtained a response map by filtering the original image and adopted the winner-takes-all (WTA) principle to select and locate the simulated fixation point and estimate a saliency map. We calculate the distribution of eye scan path obtained using Sun's method (see Figure <ref type="figure" target="#fig_10">9</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Application to Emotion Classification and Score Prediction</head><p>From the above six subsections describing the measurements for each principle, we can see that: (1) PAEF are more interpretable and semantic than EAEF; and are easier for humans to understand. For example, humans can understand symmetry and variety better than texture and line. (2) PAEF take the arrangements and orchestrations of elements into consideration and are more relevant to image emotions and more robust in image emotion prediction, as demonstrated in Sections 5.2 and 5.3.</p><p>We then apply the proposed PAEF to image emotion classification and score prediction. Firstly, we combine the representation of the six principles into one feature vector consistently. The dimensions of these principles are 60, 18, 2, 60, 9 and 16 respectively. The measurements for each principle are summarized in Table <ref type="table" target="#tab_3">2</ref>. Secondly, we adopt Support Vector Machine (SVM) and Support Vector Regression (SVR) both with radial basis function (RBF) kernel to classify categorial emotions and predict dimensional emotion </p><p>Pixel-wise windowed total variation, windowed inherent variation in x and y direction respectively, and relative total variation Movement Gaze scan path 16 The distribution of gaze vector scores, respectively. We use the LIBSVM 1 to conduct the emotion classification and score prediction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>To evaluate the effectiveness of the proposed PAEF, we carried out two experiments, affective image classification and emotion score prediction. PAEF were then applied to predict emotions of masterpieces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>IAPS dataset. The International Affective Picture System (IAP-S) is a standard emotion evoking image set in psychology <ref type="bibr" target="#b19">[19]</ref>. It consists of 1,182 documentary-style natural color images depicting complex scenes, such as portraits, babies, animals, landscapes, etc. Each image is associated with an empirically derived mean and standard deviation of valance, arousal, and dominance ratings in a 9-point rating scale. In this rating setting, rating score 9 represents a high rating on each dimension (i.e. high pleasure, high arousal, high dominance), and 1 represents a low rating on each dimension (low pleasure, low arousal, low dominance). This dataset and related emotion ratings were used for DES modeling. Similar to <ref type="bibr" target="#b24">[24]</ref>, we only modelled on the valence and arousal dimension, without considering the dominance dimension for its relatively small contributing scope on emotions <ref type="bibr" target="#b11">[11]</ref>.</p><p>Subset A of the IAPS dataset (IAPSa). Mikels et al. <ref type="bibr" target="#b27">[27]</ref> selected 395 pictures from IAPS and categorized them into eight discreet categories: Anger, Disgust, Fear, Sadness, Amusement, Awe, Contentment, and Excitement.</p><p>Artistic dataset (ArtPhoto). This dataset consists of 806 artistic photographs from a photo sharing site searched by emotion categories <ref type="bibr" target="#b26">[26]</ref>.</p><p>Abstract dataset (Abstract). This dataset includes 228 peer rated abstract paintings without contextual content <ref type="bibr" target="#b26">[26]</ref>.</p><p>The latter three datasets (IAPSa, ArtPhoto, Abstract for short) were used for CES modeling. The summary of these datasets is listed in Table <ref type="table" target="#tab_4">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Affective Image Classification</head><p>We compared our emotion classification method with Wang et al. <ref type="bibr" target="#b38">[38]</ref>, Machajdik et al. <ref type="bibr" target="#b26">[26]</ref> and Yanulevskaya et al. <ref type="bibr" target="#b42">[42]</ref>. We </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Average true positive rate</head><p>Machajdik <ref type="bibr" target="#b26">[26]</ref> Yanulevskaya <ref type="bibr" target="#b42">[42]</ref> Wang <ref type="bibr" target="#b38">[38]</ref> Ours</p><p>Figure <ref type="figure" target="#fig_0">11</ref>: Classification performance on the IAPSa dataset compared to Machajdik et al <ref type="bibr" target="#b26">[26]</ref>, Yanulevskaya et al <ref type="bibr" target="#b42">[42]</ref> and Wang et al <ref type="bibr" target="#b38">[38]</ref>.</p><p>adopted a "one category against all" strategy for experimental setup. The data was separated into a training set and a test set using K-fold Cross Validation (K=5) for 10 runs. Similar to <ref type="bibr" target="#b26">[26]</ref>, we optimized for the true positive rate per class averaged over the positive and negative classes, to overcome the limit of unbalanced data distribution of each category. We utilized PCA to perform dimensionality reduction on the feature vectors. Figures 11 to 13 illustrate the comparison of average classification performance and standard deviation between the proposed method and those of Machajdik et al. <ref type="bibr" target="#b26">[26]</ref>, Wang et al. <ref type="bibr" target="#b38">[38]</ref> and Yanulevskaya et al. <ref type="bibr" target="#b42">[42]</ref> on the IAPSa dataset, the Abstract dataset and the Artistic dataset, respectively.</p><p>From the results, it is clear that our method outperforms the state-of-the-art methods, achieving an improvement of about 5% on classification accuracy on average. This improvement arises because the state-of-the-art methods only consider the value of different low-level visual features, without considering the relationships of elements, while our proposed PAEF takes the elements' arrangements and orchestrations into account. The classification improvement demonstrates that principles-of-art are important in expressing image emotions. From the results of standard deviation, we can conclude that the proposed features are more robust for affective image classification than the use of low-level visual features.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Average true positive rate</head><p>Machajdik <ref type="bibr" target="#b26">[26]</ref> Yanulevskaya <ref type="bibr" target="#b42">[42]</ref> Wang <ref type="bibr" target="#b38">[38]</ref> Ours  Comparing different datasets, we can also observe that the classification accuracy on the Abstract and ArtPhoto datasets is better than that on the IAPSa dataset. This is because in the IAPSa dataset, the emotions are usually evoked by certain objects in the images, while in the other two datasets, the images are taken by artists who understand and utilize the principles-of-art better.</p><p>The 8-class confusion matrix of our final results is shown in Fig. <ref type="figure" target="#fig_4">14(a)</ref>. Some pair-wise emotions are difficult to classify, such as amusement and contentment, fear and disgust. This is easy to understand, because one image can evoke different emotions. For example, for the image shown in Fig. <ref type="figure" target="#fig_4">14(b)</ref>, some people may feel amusement while others may feel contentment.</p><p>In order to evaluate the effectiveness of the measurements for each principle and its contribution for affective image classification, we built classifiers for each measurement. We sorted the measurements based on the classification accuracy in a descending order with the results in Table <ref type="table" target="#tab_5">4</ref>. The letters from 'a' to 'l' represent the measurements of Bilateral symmetry, Rotational symmetry, Radial symmetry, Itten color contrast, RFA, Rangeability of hue and gradient direction, Color names, Distribution of gradient, Absolute variation, Relative variation, Relative total variation and Gaze s- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Average true positive rate</head><p>Machajdik <ref type="bibr" target="#b26">[26]</ref> Yanulevskaya <ref type="bibr" target="#b42">[42]</ref> Wang <ref type="bibr" target="#b38">[38]</ref> Ours .  can path, respectively. Readers can refer to Table <ref type="table" target="#tab_3">2</ref> for the detailed meanings of each measurement.</p><p>From the results and the visulization results for different principles, we draw the following conclusions: (1) The best features for affective image classification are dependent on the emotion category, which means that different combinations of principles express different emotions. <ref type="bibr" target="#b2">(2)</ref> The best features for affective image classification are dependent on the dataset, this is because the three datasets vary greatly from each other. Hence, based on the above two observations, we use all the principle features instead of selecting optimal feature combinations for different datasets and different emotions. (3) In terms of roles of different principles, symmetry (balance) and harmony tend to express positive emotions more often, while emphasis (contrast) and variety play an important role in classifying all the 8 categories of emotions. (4) Relative variation performs better than absolute variation, the eye scan path (movement) mainly focuses on the emphasis area, while RFA is extremely effective for emotion classification in the Abstract dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Emotion Score Prediction</head><p>We used SVR with RBF kernel to model the VA dimensions on the IAPS dataset, and computed the mean squared error (MSE) of each dimension as the evaluation measurement. The lower the MSE is, the better the regression is. We compared our method with Machajdik's features <ref type="bibr" target="#b26">[26]</ref> and the combination, using 5-cross validation for 10 runs. From Table <ref type="table" target="#tab_7">5</ref>, we can see that: (1) both valence and arousal are more accurately modeled by our principles-of-art features than Machajdik's features; (2) both our principles-of-art features and Machajdik features predict arousal better than valence; and (3) there is little improvement (3.05% and 3.53% decrease in MSE for valence and arousal) by combining them together, indicating that the principle features provide a strong enough ability in understanding image emotions. Some regression results are given in Fig. <ref type="figure" target="#fig_18">15</ref>, which demonstrates the effectiveness of our image emotion prediction method.</p><p>We also conducted the VA emotion regression task using each of the six principles. From the MSE results in Table <ref type="table" target="#tab_8">6</ref>, we find that variety, emphasis, gradation and balance have higher correlations with valence, while emphasis, variety, harmony and movement are more correlated with arousal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Inferring Masters' Moods</head><p>Masters have strong abilities to capture scenes or subjects into artworks which evoke strong emotional responses <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b15">15]</ref>. Inferring the emotions implied in the masterpieces can immensely help in understanding the essential moods that the masters intended to express at that time.</p><p>Here we gathered 1,029 paintings and 158 watercolors of Vincent van Gogh, a famous Post-Impressionist painter, to infer his moods at different life periods, including early years (1881-1883), Nuenen (1883-1886), Antwerp (1883-1886), Paris (1886-1888), Arles (1888-1889), Saint-Remy (1889-1890) and Auvers (1890).</p><p>We used PAEF to predict the implied image emotions from van Gogh's artworks based on the training results of the three differ-  ent datasets, IAPSa, Abstract and ArtPhoto, respectively. Some representational paintings and our predicted emotions are shown in Figure <ref type="figure" target="#fig_19">16</ref>. We can observe that training result in ArtPhoto dataset performs best. So we used this training result to predict all the artworks. The prediction result is shown in Table <ref type="table" target="#tab_9">7</ref>, from which we can see the distribution of the number of his paintings and watercolors. Note that one image can evoke different emotions.</p><p>For each life period of van Gogh in Table <ref type="table" target="#tab_9">7</ref>, the first and second rows of each entry represent the numbers of paintings and watercolors, respectively. Take the painting "Wheat Field with Crows" (van Gogh's last painting) as example, the comments from vangoghgallery (www.vangoghgallery.com) are heavy, lonely, gloom, and melancholy, and our prediction emotion is sadness, clearly indicting the emotional status of his final days. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Discussion</head><p>From the classification results in Section 5.2 and the regression results in Section 5.3, we can conclude that PAEF can indeed help to improve the performance of image emotion recognition. The results demonstrate that the principles-of-art features can model image emotions better and are more robust in image emotion recognition than the elements-of-art features. PAEF are especially helpful and accurate to handle the abstract and artistic images, the emotions of which are mainly determined by the composition. However, as our method does not consider the semantics of images, it does not work so well for the images whose emotions are dominated by some specific objects, concepts or scenes; and the emotion recognition performance is relatively low for these images. For example, in one image containing snakes, the emotion of fear may directly be evoked by the presence of snakes. In such cases, our method may fail. Combining the visual concept detection method, such as SentiBank <ref type="bibr" target="#b4">[4]</ref>, may help to tackle this problem and further improve the emotion recognition performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION AND FUTURE WORK</head><p>In this paper, we proposed to extract emotion features based on principles-of-art (PAEF) for image emotion classification and scoring task. Different from previous works that mainly extract low level visual features based on elements-of-art, we drew inspirations from the concept of principles-of-art for higher level understanding of images. Experimental results on affective image classification and regression have demonstrated that the performance of the proposed features is superior over the state-of-the-art approaches. The application of PAEF in emotion prediction of masterpieces is also interesting and has much potential for future research. PAEF can also be used to develop other emotion based applications, such as image musicalization <ref type="bibr" target="#b44">[44]</ref> and affective image retrieval <ref type="bibr" target="#b45">[45]</ref>.</p><p>For further studies, we will continue our efforts to quantize the principles using more effective measurements and to improve the efficiency for real time implementation. Applying high level content detection and recognition methods may improve the performance of emotion recognition. In addition, we will consider using social network (e.g., Flickr) data, combining the descriptions and images to jointly learn the expected emotion of specified image based on visual-textual-social features <ref type="bibr" target="#b9">[9]</ref> and analyzing the comments to distinguish expected emotion and actual emotion. How to analyze videos using visual features together with acoustic signals from an emotional perspective is also worth studying.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The framework of our proposed method. The main contributions, principles-of-art-based emotion features (PAEF), lie in the central feature extraction part in blue solid rectangle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Low-level representation features of emotions based on elements-of-art.</figDesc><graphic coords="3,329.22,101.65,50.24,50.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Symmetrical gray scale images. The first row is images in bilateral symmetry with symmetry axis and symmetrical feature points. The second row is images in rotational symmetry with symmetry center and symmetrical feature points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Images with high RFA based on statistic subject mask in [35]. The first column is "Rule of the third" composition and this mask. The three rows on the right of the black line are related images, saliency maps and RFA scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>y=1</head><label></label><figDesc>Saliency(x, y)M aski(x, y) W id x=1 Hei y=1 Saliency(x, y)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Images with high RFA based on diagonal mask [21], shown in the first column. The three rows on the right of the black line are related images, saliency maps and RFA scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Images with high RFA based on back diagonal mask [21], shown in the first column. The three rows on the right of the black line are related images, saliency maps and RFA scores.</figDesc><graphic coords="5,85.77,342.77,72.23,84.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Images of different texture gradations, but with similar content meanings and emotions.</figDesc><graphic coords="5,439.22,343.26,115.72,87.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Images of different color variety. The first row shows images of high color variety with their color distributions in terms of 11 basic colors shown in row 2. The third and fourth rows respectively show images of low color variety and related distributions of the 11 basic colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Eye scan path for measuring the principle of movement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>1 http://www.csie.ntu.edu.tw/~cjlin/libsvm/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Classification performance on the Abstract dataset compared to Machajdik et al [26], Yanulevskaya et al [42] and Wang et al [38].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Classification performance on the ArtPhoto dataset compared to Machajdik et al [26], Yanulevskaya et al [42] and Wang et al [38].</figDesc><graphic coords="8,356.63,354.80,68.82,68.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Emotion prediction results of our method. The black plus signs and blue circles represent the ground truth and our predicted values of image emotions, respectively.</figDesc><graphic coords="9,54.19,125.98,238.55,152.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: van Gogh's masterpieces, and our predicted emotions. The paintings are "Skull with burning cigarette", "Starry night", "Still life vase with fourteen sunflowers" and "Wheat field with crows" from left to right. The three rows of predicted emotions below the paintings are based on the training results in IAPSa, Abstract and ArtPhoto datasets, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Related works on affective content analysis</figDesc><table><row><cell>Category</cell><cell>Classification</cell><cell>Publications</cell></row><row><cell>Data</cell><cell>Still images</cell><cell></cell></row></table><note><p><p><ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b33">33</ref></p>] type Dynamic videos [10, 17, 41, 43, 39, 3, 13] Emotion Categorical [26, 38, 42, 43, 24, 20, 15, 17, 39, 13] model Dimensional [10, 24, 29, 41, 3, 33] Features Generality Generic [42, 29, 4] Specific [10, 26, 38, 43, 24, 20, 15, 17, 41, 39, 3, 13, 33] Level Low [10, 26, 38, 42, 24, 20, 15, 29, 17, 41, 39, 3] Mid [26, 13, 33] High [26, 4, 43] Art theory Elements [26, 38, 42, 24, 20, 15, 33] Principles [26]</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>)</figDesc><table><row><cell>0.6990</cell><cell>0.6877</cell><cell>0.6838</cell><cell>0.6795</cell><cell>0.6463</cell><cell>0.6373</cell><cell>0.6362</cell><cell>0.6266</cell><cell>0.6132</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Summary of the measurements for principles of art. '#' indicates the dimension of each measurement.</figDesc><table><row><cell>Principles</cell><cell>Measurement</cell><cell>#</cell><cell>Short Description</cell></row><row><cell>Balance</cell><cell>Bilateral symmetry</cell><cell cols="2">12 Symmetry number, Maximum symmetry radius, angle and strength</cell></row><row><cell></cell><cell>Rotational symmetry</cell><cell cols="2">12 Symmetry number, Maximum symmetry center (x and y), strength</cell></row><row><cell></cell><cell>Radial symmetry</cell><cell cols="2">36 Distribution of symmetry map after radial symmetry transformation</cell></row><row><cell>Emphasis</cell><cell>Itten color contrast</cell><cell cols="2">15 Average contrast of saturation, contrast of light and dark, contrast of extension, con-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>trast of complements, contrast of hue, contrast of warm and cold, simultaneous con-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>trast</cell></row><row><cell></cell><cell>RFA</cell><cell>3</cell><cell>Rate of focused attention based on saliency map and subject mask</cell></row><row><cell>Harmony</cell><cell>Rangeability of hue and</cell><cell>2</cell><cell>The first and second maximums of local maximum hues and gradient directions in</cell></row><row><cell></cell><cell>gradient direction</cell><cell></cell><cell>relative histograms of an image patch, and their differences; the combination of all</cell></row><row><cell></cell><cell></cell><cell></cell><cell>patches of an image</cell></row><row><cell>Variety</cell><cell>Color names</cell><cell cols="2">12 Color types of black, blue, brown, gray, green, orange, pink, purple, red, white,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>yellow and each color's amount</cell></row><row><cell></cell><cell>Distribution of gradient</cell><cell cols="2">48 The distribution of gradient on eight scales of direction and eight scales of length</cell></row><row><cell cols="2">Gradation Absolute and relative vari-</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ation</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Summary of the three datasets with discrete emotion categories for affective image classification.</figDesc><table><row><cell>Dataset</cell><cell cols="9">Amusement Anger Awe Contentment Disgust Excitement Fear Sadness Sum</cell></row><row><cell>IAPSa</cell><cell>37</cell><cell>8</cell><cell>54</cell><cell>63</cell><cell>74</cell><cell>55</cell><cell>42</cell><cell>62</cell><cell>395</cell></row><row><cell>ArtPhoto</cell><cell>101</cell><cell>77</cell><cell>102</cell><cell>70</cell><cell>70</cell><cell>105</cell><cell>115</cell><cell>166</cell><cell>806</cell></row><row><cell>Abstract</cell><cell>25</cell><cell>3</cell><cell>15</cell><cell>63</cell><cell>18</cell><cell>36</cell><cell>36</cell><cell>32</cell><cell>228</cell></row><row><cell>Combined</cell><cell>163</cell><cell>88</cell><cell>171</cell><cell>196</cell><cell>162</cell><cell>196</cell><cell>193</cell><cell>260</cell><cell>1429</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Measurements ranking list for the contribution to affective image classification.</figDesc><table><row><cell></cell><cell>IAPSa</cell><cell>Abstract</cell><cell>ArtPhoto</cell></row><row><cell cols="4">Amusement jbeghacdkifl egdhkbfacjli gedjkhfcaibl</cell></row><row><cell>Anger</cell><cell cols="3">bgkdfacjlehi efgjklabcidh befgjklacdih</cell></row><row><cell>Awe</cell><cell cols="3">bjefgldchkai edkclbghafji cbkhfldegaji</cell></row><row><cell cols="4">Contentment bfhjegkdilac ebfkhacljdig fbcekhagdjli</cell></row><row><cell>Disgust</cell><cell cols="3">ecdajhfbkigl eklghbcadfji gelbadhcfkji</cell></row><row><cell>Excitement</cell><cell cols="3">fghdbcjkleia gjcdhkafiebl cbejdgkahifl</cell></row><row><cell>Fear</cell><cell cols="3">dcghkaejfbli bgdhakcejfil cgdkhejlabif</cell></row><row><cell>Sadness</cell><cell cols="3">fbecljhkagdi efkbcdgahjli fbhjdkglciea</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>69 .01 .06 .21 .00 .02 .01 .00 .06 .63 .23 .00 .05 .02 .02 .00 .03 .00 .68 .02 .00 .12 .13 .02 .26 .06 .05 .56 .01 .04 .01 .02 .02 .03 .00 .02 .65 .00 .22 .06 .04 .01 .21 .05 .03 .66 .00 .01 .04 .10 .00 .01 .13 .00 .68 .05 .05 .02 .04 .00 .05 .02 .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparison of MSE (standard deviation) for VA dimensions in the IAPS dataset.</figDesc><table><row><cell></cell><cell>Machajdik [26]</cell><cell>PAEF</cell><cell>Combination</cell></row><row><cell>Valence</cell><cell>1.49(0.21)</cell><cell>1.31(0.15)</cell><cell>1.27(0.13)</cell></row><row><cell>Arousal</cell><cell>1.06(0.13)</cell><cell>0.85(0.10)</cell><cell>0.82(0.09)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>MSE of each principle for VA dimensions in the IAPS dataset. Ban Emp Har Var Gra Mov Valence 1.85 1.72 2.16 1.67 1.78 2.37 Arousal 1.52 0.98 1.12 1.07 1.61 1.15</figDesc><table><row><cell>Awe Excitement</cell><cell>Disgust</cell><cell>Sadness</cell><cell>Contentment</cell></row><row><cell>Awe</cell><cell>Awe Excitement</cell><cell>Neutral</cell><cell>Excitement</cell></row><row><cell>Fear</cell><cell cols="2">Awe Contentment Contentment</cell><cell>Sadness</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Emotion prediction result of van Gogh's artworks.</figDesc><table><row><cell>Period</cell><cell>Am</cell><cell>An</cell><cell>Aw</cell><cell>Co</cell><cell>Di</cell><cell>Ex</cell><cell>Fe</cell><cell>Sa</cell><cell>Ne</cell><cell>Sum</cell></row><row><cell>Early</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>4</cell><cell>3</cell><cell>9</cell><cell>22</cell><cell>8</cell><cell>35</cell></row><row><cell></cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>7</cell><cell>3</cell><cell>15</cell><cell>22</cell><cell>45</cell><cell>88</cell></row><row><cell>Nuenen</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>3</cell><cell>41</cell><cell>26</cell><cell>73</cell><cell>75</cell><cell>25</cell><cell>200</cell></row><row><cell></cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>3</cell><cell>1</cell><cell>3</cell><cell>10</cell><cell>9</cell><cell>24</cell></row><row><cell>Antwerp</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>3</cell><cell>0</cell><cell>5</cell><cell>2</cell><cell>1</cell><cell>7</cell></row><row><cell></cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Paris</cell><cell>11</cell><cell>1</cell><cell>3</cell><cell>7</cell><cell>35</cell><cell>47</cell><cell>44</cell><cell>45</cell><cell>53</cell><cell>224</cell></row><row><cell></cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>3</cell><cell>2</cell><cell>4</cell><cell>10</cell></row><row><cell>Arles</cell><cell>11</cell><cell>5</cell><cell>1</cell><cell>19</cell><cell>45</cell><cell>69</cell><cell>52</cell><cell>49</cell><cell>87</cell><cell>304</cell></row><row><cell></cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>6</cell><cell>7</cell><cell>3</cell><cell>1</cell><cell>3</cell><cell>21</cell></row><row><cell>SaintRemy</cell><cell>8</cell><cell>12</cell><cell>3</cell><cell>11</cell><cell>36</cell><cell>36</cell><cell>22</cell><cell>11</cell><cell>57</cell><cell>177</cell></row><row><cell></cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>6</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>11</cell></row><row><cell>Auvers</cell><cell>6</cell><cell>0</cell><cell>5</cell><cell>3</cell><cell>7</cell><cell>22</cell><cell>6</cell><cell>8</cell><cell>29</cell><cell>82</cell></row><row><cell></cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>2</cell><cell>4</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGEMENTS</head><p>This work was supported by National Natural Science Foundation of China (No. 61071180) and Key Program (No. 61133003), and partially supported by the Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme Office. Sicheng Zhao was also supported by the Ph.D. Short-Term Overseas Visiting Scholar Program of Harbin Institute of Technology.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Art and visual perception: A psychology of the creative eye</title>
		<author>
			<persName><forename type="first">R</forename><surname>Arnheim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1954">1954</date>
			<publisher>University of California Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generalizing the hough transform to detect arbitrary shapes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="122" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A connotative space for supporting movie affective recommendation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Benini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Canini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leonardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1356" to="1370" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large-scale visual sentiment ontology and detectors using adjective noun pairs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Storytelling through lighting: a computer graphics perspective</title>
		<author>
			<persName><forename type="first">S</forename><surname>Calahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH course notes</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">96</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The principles of art</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Collingwood</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1958">1958</date>
			<publisher>Oxford University Press</publisher>
			<biblScope unit="volume">62</biblScope>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Algorithmic inferencing of aesthetics and emotion in natural images: An exposition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">High level describable attributes for predicting aesthetics and interestingness</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual-textual joint relevance learning for tag-based social image search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="363" to="376" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Extracting moods from pictures and sounds: Towards truly personalized tv</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="90" to="100" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Affective video content representation and modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="143" to="154" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The visual experience</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hobbs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vieth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Davis Publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Affective audio-visual words and latent topic driving model for realizing movie affective scene classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Satou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="523" to="535" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The art of color: the subjective experience and objective rationale of color</title>
		<author>
			<persName><forename type="first">J</forename><surname>Itten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974">1974</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Can we understand van gogh&apos;s mood? learning to infer affects from images in social networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Aesthetics and emotions in images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fedorovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="94" to="115" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Affective content detection using hmms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Smoothed local histogram filters</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">100</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cuthbert</surname></persName>
		</author>
		<title level="m">International affective picture system (IAPS): Affective ratings of pictures and instruction manual. NIMH, Center for the Study of Emotion &amp; Attention</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Context-aware affective images classification based on bilayer sparse representation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Optimizing photo composition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Detecting symmetry and symmetric constellations of features</title>
		<author>
			<persName><forename type="first">G</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eklundh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast radial symmetry for detecting points of interest</title>
		<author>
			<persName><forename type="first">G</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zelinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="959" to="973" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On shape and the computability of emotions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Suryanarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Adams</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Photo and video quality evaluation: Focusing on the subject</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Affective image classification using features inspired by psychology and art theory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Machajdik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Emotional category data on images from the international affective picture system</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mikels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fredrickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Larkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lindberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maglio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Reuter-Lorenz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior research methods</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="626" to="630" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast radial symmetry detection under affine transformations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bahlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A multi-layer hybrid framework for dimensional emotion classification</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Enhancing semantic features with compositional analysis for scene recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Redi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Merialdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Emotion and Art: Mastering the Challenges of the Artist&apos;s Path</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ruskan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>R. Wyler &amp; Co</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Three dimensions of emotion</title>
		<author>
			<persName><forename type="first">H</forename><surname>Schlosberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">81</biblScope>
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Color based bags-of-emotions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Solli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lenz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CAIP</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">What are we looking for: Towards statistical modeling of saccadic eye movements and visual saliency</title>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Photo assessment based on computational visual attention model</title>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning color names from real-world images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Van De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploiting photographic style for category-level image classification by generalizing the spatial pyramid</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Image retrieval by emotional semantics: A study of emotional space and feature extraction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<editor>IEEE SMC</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Affect-based adaptive presentation of home videos</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Structure extraction from texture via relative total variation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">139</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hierarchical movie affective content analysis based on arousal and valence features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Emotional valence categorization using holistic image features</title>
		<author>
			<persName><forename type="first">V</forename><surname>Yanulevskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Geusebroek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Video indexing and recommendation based on affective analysis of viewers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Emotion based image musicalization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICMEW</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Affective image retrieval via multi-graph learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
