<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This paper is included in the Proceedings of the 16th USENIX Symposium on Networked Systems Design and Implementation (NSDI &apos;19)</title>
				<funder>
					<orgName type="full">Samsung</orgName>
				</funder>
				<funder>
					<orgName type="full">Google</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kostis</forename><surname>Kaffes</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Timothy</forename><surname>Chong</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jack</forename><forename type="middle">Tigar</forename><surname>Humphries</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adam</forename><surname>Belay</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Mazi?res</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">David Mazi?res and Christos Kozyrakis</orgName>
								<orgName type="institution" key="instit1">Massachusetts Institute of Technology</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">This paper is included in the Proceedings of the 16th USENIX Symposium on Networked Systems Design and Implementation (NSDI &apos;19)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open access to the Proceedings of the 16th USENIX Symposium on Networked Systems</head><p>Design and Implementation (NSDI '19</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Popular cloud applications such as web search, social networks, and machine translation fan out requests to hundreds of communicating services running on thousands of machines. End-to-end response times are then dominated by the slowest machine to respond <ref type="bibr" target="#b21">[23]</ref>. Reacting to user actions within tens of milliseconds requires that each participating service process requests with tail latency in the range of ten to a few hundred microseconds <ref type="bibr" target="#b12">[14]</ref>. Unfortunately, thread management in modern operating systems such as Linux is not designed for microsecond-scale tasks and frequently produces long, unpredictable scheduling delays resulting in millisecond-scale tail latencies <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b35">37]</ref>.</p><p>To compensate, researchers have developed network stacks, dataplanes, and full applications that bypass the operating system <ref type="bibr" target="#b42">[44,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b20">22]</ref>. Most of these systems operate in a similar way: the NIC uses receive-side scaling (RSS) <ref type="bibr" target="#b19">[21]</ref> to distribute incoming requests across multiple queues in a flowconsistent manner; a polling thread serves requests in each queue in a first-come-first-served manner (FCFS) without scheduling interruptions; optimizations such as zero copy, run-to-completion, adaptive batching, and cache-friendly and thread-private data structures reduce overheads. The resulting request scheduling is known as distributed queuing and FCFS scheduling, or d-FCFS.</p><p>d-FCFS is effective when request service times exhibit low dispersion <ref type="bibr" target="#b55">[57]</ref>, as is the case for get/put requests to simple in-memory key-value stores (KVS) such as Memcached <ref type="bibr" target="#b41">[43]</ref>. d-FCFS fares poorly under high dispersion or heavy-tailed request distributions (e.g., bimodal, log-normal, Zipf, or Pareto distributions), as short requests get stuck behind older long ones assigned to the same queue. d-FCFS is also not workconserving, an effect exacerbated by implementations based on RSS's flow-consistent hashing, which approximates true d-FCFS only with high numbers of client connections spreading requests out evenly over queues.</p><p>ZygOS <ref type="bibr" target="#b44">[46]</ref> improved on d-FCFS by implementing low-overhead task stealing: threads that complete short requests steal work from threads tied up by longer ones. It approximates centralized FCFS scheduling (c-FCFS), in which all threads serve a single queue. Work stealing is not free. It requires scanning queues cached on non-local cores and forwarding system calls back to a request's home core. However, if service times exhibit low dispersion and there are enough client connections for RSS to spread requests evenly across queues, stealing happens infrequently.</p><p>Unfortunately, c-FCFS is also inefficient for workloads with request times that follow heavy-tailed distributions or even light-tailed distributions with high dispersion. These workloads include search engines that score and rank a number of items depending on the popularity of search terms <ref type="bibr" target="#b11">[13]</ref>; microservices and functionas-a-service (FaaS) frameworks <ref type="bibr" target="#b15">[17]</ref>; and in-memory stores or databases, such as RocksDB <ref type="bibr" target="#b24">[26]</ref>, Redis <ref type="bibr" target="#b33">[35]</ref>, and Silo <ref type="bibr" target="#b52">[54]</ref>, that support both simple get/put requests and complex range or SQL queries, and use slower nonvolatile memory in addition to fast DRAM. Theory tells us that such workloads do best in terms of tail latency under processor sharing (PS) <ref type="bibr" target="#b55">[57]</ref>, where all requests receive a fine-grain, fair fraction of the available processing capacity.</p><p>To approximate PS, we need preemption, as built into any modern kernel scheduler including Linux. However, any service that uses one thread per request or connection and allows Linux to manage threads will experience millisecond-scale tail latencies, because the kernel employs preemption at millisecond granularities and its policies are not optimized for microsecond-scale tail latency <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b35">37]</ref>. User-level libraries for cooperative threading can avoid the overheads of kernel scheduling <ref type="bibr" target="#b54">[56]</ref>. However, it is difficult to yield often enough during requests with longer processing times and without many blocking I/O calls, which are precisely the requests impacting tail latency.</p><p>This paper presents Shinjuku, a single-address space operating system that implements preemptive scheduling at the microsecond-scale and improves tail latency and throughput for both light-and heavy-tailed service time distributions. Shinjuku deemphasizes RSS in favor of true centralized scheduling by one or more dedicated dispatcher threads with centralized knowledge of load and service time distribution. It leverages hardware support for virtualization-specifically posted interruptsto achieve preemption overheads of 298 cycles in the dispatcher core and 1,212 cycles in worker cores. The single address space architecture allows us to optimize context switches down to 110 cycles.</p><p>Fast preemption enables scheduling policies that switch between requests as often as every 5 ?sec when needed. We developed two policies. The first assumes no prior knowledge of request service times and uses preemption to select at fine granularity between FCFS or PS based on observed service times. The second policy assumes we can segregate requests with different service level objectives (SLO) in order to ensure good tail latency for both short and long requests. Both policies are work conserving and work well across multiple distributions of service times (light-tailed, heavy-tailed, bimodal, or multimodal). The two policies make Shinjuku the first system to support microsecond-scale tail latency for workloads beyond those with fixed or low-dispersion service time distributions.</p><p>We compare Shinjuku with IX <ref type="bibr" target="#b14">[16]</ref> and ZygOS <ref type="bibr" target="#b44">[46]</ref>, two state-of-the-art dataplane operating systems. Using synthetic workloads, we show that Shinjuku matches IX's and ZygOS' performance for light-tailed workloads while it supports up to 5x larger load for heavy-tailed and multi-modal distributions. Using RocksDB, a popular key-value store that also supports range queries, we show that Shinjuku improves upon ZygOS by up to 6.6? in terms of throughput at a given 99th percentile latency. We show that Shinjuku scales well with the number of cores available, can saturate high speed network links, and is efficient even with small connection counts.</p><p>The rest of the paper is organized as follows. ?2 motivates the need for preemptive scheduling at microsecond-scale. ?3 discusses the design and implementation of Shinjuku. ?4 presents a thorough quantitative evaluation while ?5 discusses related work.</p><p>Shinjuku is open-source software. The code is available at https://github.com/ stanford-mast/shinjuku.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation</head><p>Background: We aim to improve the SLO of latencycritical services on a single server. For cloud services and microservices with high fan-out, Shinjuku must achieve low tail latency at the microsecond scale <ref type="bibr" target="#b12">[14]</ref>. Low average or median latency is not sufficient <ref type="bibr" target="#b21">[23]</ref>. While tail latency can be improved through overprovisioning, doing so is not economical for services with millions of users. To be cost-effective, Shinjuku must maintain low tail latency in the face of high request throughput. Finally, it must be practical for a wide range of workloads and support intuitive APIs that simplify development and maintenance of large code bases.</p><p>The key to achieving low tail latency and high throughput is effective request scheduling, which requires both good policies and low-overhead mechanisms that operate at the microsecond scale. Good policies are easy to achieve in isolation. Linux already supplies approximations of the optimal policies for workloads we target. Unfortunately, the Linux kernel scheduler operates at the millisecond scale because of preemption and context switch overheads and the complexity of simultaneously accommodating batch, background, and interactive tasks at different time scales <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b35">37]</ref>.</p><p>Recent proposals for user-level networking stacks, dataplanes, RPC protocols, and applications <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b30">32]</ref> sidestep the bloated kernel networking and thread management stacks in order to optimize tail latency and throughput. Most of these systems use RSS to approximate a d-FCFS scheduling policy <ref type="bibr" target="#b19">[21]</ref>, the IX dataplane being a canonical example <ref type="bibr" target="#b14">[16]</ref>. Zy-gOS improves on IX by using work stealing to approximate c-FCFS <ref type="bibr" target="#b44">[46]</ref>. Linux applications built with libevent <ref type="bibr" target="#b45">[47]</ref> or libuv [5] also implement c-FCFS, but at much higher overheads due to the use of interrupts for request distribution instead of RSS and polling.</p><p>Policy comparison: In order to quantify the differences between different scheduling policies, we developed a discrete event simulator. The simulator allowed us to configure parameters such as scheduling policy, number of host cores, system load, service and inter-arrival time distributions as well as various systemrelated overheads. Figure <ref type="figure">1</ref> compares idealized versions of scheduling policies-i.e., no stealing or preemption overhead-using the simulator. Plot (a) shows a light-tailed exponential distribution of service times with mean ? = 1 ?sec, representative of workloads such as the get/set requests of in-memory key-value stores. d-FCFS is arguably tolerable under such simple workloads, but suffers at moderate and high load as requests are not perfectly distributed across workers. c-FCFS is optimal under such workloads, while PS is slightly worse because it preempts even short requests. The PS time slice used for all simulations is 0.1 ?sec. d-FCFS is a poor option for heavy-tailed request distributions <ref type="bibr" target="#b40">[42]</ref>, as found in search engines <ref type="bibr" target="#b36">[38]</ref> or induced by activities such as garbage collection or compaction <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b3">6,</ref><ref type="bibr" target="#b24">26]</ref>. Plot (b) shows performance under a heavy-tailed log-normal distribution with mean ? = 1 ?sec and standard deviation ? = 10 ?sec. Any long request blocks every short request assigned to the same queue in d-FCFS. c-FCFS performs significantly better as a worker can service any request; short requests are only delayed when most workers simultaneously process older long requests, which is uncommon for the log-normal distribution. c-FCFS performs significantly worse under a lighttailed bimodal distribution, commonly found in object stores and databases that mix simple get/put requests with complex range or relational queries <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b52">54]</ref>. Plot (c) shows such a distribution in which 99.5% of requests take 0.5?sec and 0.5% take 500?sec. Compared to a heavy-tailed case, the bimodal distribution's long requests are not as long but far more frequent. PS han-dles both cases in Plots (b) and (c) well by preempting long requests to interleave execution of short ones.</p><p>Figure <ref type="figure">2</ref> provides further insights by separating the performance of short and long requests in a bimodal workload with service times evenly split between 1 ?sec and 100 ?sec. This approximates a KVS in which half of the requests are get/put requests and the other half are range queries. The tail latency for the two request types is drawn separately in Plots (a) and (b), and Plot (c) shows the 99th percentile of the request slowdown for all requests, which is the ratio of a request's overall latency to its service time. This ratio is a useful metric for measuring how well we achieve our goal of reducing queuing time for all request types: if this ratio is small, it means that queuing time is small for all types and no requests are affected by the requests of different types.</p><p>Plot 2a shows that both d-FCFS and c-FCFS heavily penalize 1-?sec requests. Plot 2b shows that c-FCFS is marginally better than PS for 100 ?sec requests, as it effectively prioritizes older, long requests that would be preempted by PS. Plot 2c shows that, in relative terms, the penalty c-FCFS inflicts on short requests dwarfs any benefit to long requests.</p><p>Shinjuku approach: Shinjuku implements the c-PRE policies (see ?3.4) that achieve the best of both worlds between PS and c-FCFS as shown in Figures <ref type="figure">1</ref> and<ref type="figure">2</ref>. The reason other recent systems cannot implement similar policies is that these policies require preemption at arbitrary execution points. Preemption typically involves interrupts and kernel threads whose overheads are incompatible with microsecond-scale latencies. Therefore, Shinjuku aims to achieve the following goals: 1) Implement low-overhead preemption and context switching mechanisms for user-level threads. 2) Use these mechanisms to build scheduling policies that work well across all possible distributions of service times for microsecond-scale workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Shinjuku</head><p>Shinjuku 1 is a single-address space operating system for low-latency applications. Shinjuku is a significant departure from the common pattern in IX <ref type="bibr" target="#b14">[16]</ref> and Zy-gOS <ref type="bibr" target="#b44">[46]</ref>, which rely heavily on RSS to distribute incoming requests to workers that process them without interruption. Instead, Shinjuku uses a centralized queuing and scheduling architecture and relies on low overhead and frequent preemption in order to ensure low tail latency for a wide variety of service time distributions.</p><p>1 Shinjuku (???)is a major train station in Tokyo that serves millions traveling on 12 lines of various types and speeds. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Design Overview</head><p>Figure <ref type="figure" target="#fig_1">3</ref> summarizes the key components in Shinjuku and the typical request flow. Incoming requests are first processed by the networking subsystem that handles all network protocol processing and identifies request boundaries . The networking subsystem can be implemented using one or more dedicated cores or hyperthreads <ref type="bibr" target="#b20">[22]</ref>, a smartNIC <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b51">53]</ref>, or a combination of the two. By separating network processing from request scheduling, Shinjuku can be combined with a range of networking protocols that optimize for different conditions (UDP, TCP, ROCE <ref type="bibr" target="#b50">[52]</ref>, TIMELY <ref type="bibr" target="#b39">[41]</ref>, etc.) and various optimized network stacks <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b20">22]</ref>. The networking subsystem passes requests to a centralized dispatcher thread that will queue and schedule them to worker threads. The dispatcher generates a context for each incoming request in order to support preemption and rescheduling. In its simplest form, the dispatcher maintains a single queue for all pending requests. The dispatcher sends requests to worker threads , each using a dedicated hardware core or hyperthread. Most requests will complete their execution without interruption. Network processing for any replies can take place either at the networking subsystem or the worker thread itself to optimize for latency. At a minimum, the worker thread notifies the networking subsystem to free any buffer space allocated for the incoming request .</p><p>The dispatcher uses timestamps to identify long running requests that should be preempted based on the scheduling policy. Assuming there are queued requests, we preempt running requests after 5?sec to 15?sec for the workloads we studied (see ?4), which is extremely frequent compared to the time slice in the Linux kernel. For example, the CFS scheduler has a target preemption latency of 6ms and a minimum one of 0.75ms. The dispatcher sends an interrupt to the worker thread , which performs a context switch and receives a different request to run from the dispatcher. The long request is re-enqueued in the dispatcher and processed later us- Sender/receiver cost refers to cycles consumed in the sending/receiving core, including the receiver overhead of invoking an empty interrupt handler. Total cost includes interrupt propagation through the system bus. Hence, it is not equal to the sum of sender and receiver overhead. For "IPI Sender-only Exit" and "Vanilla IPI," the receiver starts interrupt processing before the sender returns from its VM exit.</p><p>ing stepsas many times as needed.</p><p>Since Shinjuku is a single-address space operating system, communication between its components occurs over shared memory. We use dedicated pairs of cache lines for each pair of communicating threads (see ?3.5).</p><p>Similar to IX and ZygOS, Shinjuku leverages the Dune system for process virtualization <ref type="bibr" target="#b13">[15]</ref>. With Dune, Linux and the Dune kernel module run in VMX root mode ring 0, where a hypervisor would run in a virtualized system. Shinjuku runs in VMX non-root mode ring 0, where a guest OS would run. This allows it to use very low overhead interrupts while separating the control from the data plane. The application context that uses Shinjuku can run in VMX non-root mode ring 0 or ring 3. For the results in ?4, we run applications in VMX non-root mode ring 0 to avoid the address space crossings between Shinjuku and the application code. There is a separate instance of Shinjuku for each low-latency application running on the server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fast Preemption</head><p>To use preemptive scheduling at microsecond latencies, Shinjuku requires fast preemption. A naive approach would be for the dispatcher to notify workers using Linux signals. As we show in Table <ref type="table" target="#tab_0">1</ref>, however, signals incur high overheads for both the sender and the receiver (roughly 2.5 ?sec on a 2GHz machine). They require user-to kernel-space transitions plus some kernel processing.</p><p>Preemption through interrupts. Direct use of interprocessor interrupts (IPIs) is potentially faster than signals. x86 processors implement IPIs using the Advanced Programmable Interrupt Controller (APIC).</p><p>Each core has a local APIC and an I/O APIC is attached to the system bus. To send an IPI, the sending core writes registers in its local APIC which propagates the interrupt via the I/O APIC to the destination core's APIC, which in turn vectors execution to an interrupt handler.</p><p>We extended Dune to support IPIs by virtualizing the local APIC registers. When a non-root thread on core A writes its virtual APIC to send interrupt number V to core B, this causes a VM exit to Dune running in root mode. Dune writes V to core B's posted interrupt descriptor, and then uses the real APIC to send interrupt 242 to core B. That causes core B to perform a VM exit to an interrupt handler in Dune, which injects interrupt number V into non-root mode on resuming the application.</p><p>As Table <ref type="table" target="#tab_0">1</ref> shows, this vanilla implementation of preemption using IPIs is slightly faster than Linux signals but still suffers from significant overheads due to the cost of VM exits in both the sender and the receiver.</p><p>Optimized interrupt delivery. We first focus on removing the VM exit on the receiving core B (the Shinjuku worker) using posted interrupts, an x86 feature for receiving interrupts without a VM exit. To enable posted interrupts, Dune on B configures its hardwaredefined VM control structure (VMCS) to recognize interrupt 242 as the special posted interrupt notification vector. B also registers its posted interrupt descriptor with the VMCS. Core A still performs a VM exit upon writing the virtual APIC. Dune code on A writes V into B's posted interrupt descriptor and sends interrupt 242 to B. However, B then directly injects interrupt V without a VM exit. Table <ref type="table" target="#tab_0">1</ref> shows that eliminating the receiverside VM exit reduces receiver overhead by 54% (from 2662 to 1212 cycles). This allows frequent preemption of worker threads without significant reduction in useful worker throughput. This receiver overhead consists of modifications to hardware structures, and it cannot be significantly improved without hardware changes, such as support for lightweight user-level interrupts <ref type="bibr" target="#b49">[51]</ref>.</p><p>Optimized interrupt sending. Finally, we remove the VM exit on the sending core (dispatcher thread) by trusting the Shinjuku dispatcher with direct access to the real (non-virtual) APIC. Using the extended page table (EPT), we map both the posted interrupt descriptors of other cores and the local APIC's registers into the guest physical address space of the Shinjuku dispatcher. Hence, the dispatcher can directly send an IPI without incurring a VM exit. Table <ref type="table" target="#tab_0">1</ref> shows that eliminating the sender-side VM exit reduces sender overheads down to 298 cycles (149ns in a 2GHz system). This improves dispatcher scalability and allows it to serve more requests per second and/or more worker threads (cores). Table <ref type="table" target="#tab_0">1</ref> presents the result of combining the senderside and receiver-side optimization for the interrupt delivery used to support preemption in Shinjuku. The low sender-side overhead (298 cycles) makes it practical to build a centralized, preemptive dispatcher that handles millions of scheduling actions per second. The low receiver-side overhead (1212 cycles) makes it practical to preempt requests as often as every 5?sec in order to schedule longer requests without wasting more than 10% of the workers' throughput.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Low-overhead Context Switch</head><p>When a request is scheduled to an idling core or upon preemption, we context switch between the main context in each worker and the request handling context.</p><p>The direct approach would be to use the swapcontext function in the Linux ucontext library. According to Table 2, the overhead is significant in an ordinary Linux process and doubles when used in a Dune process. swapcontext requires a system call to set the signal masks during the switch, which requires a VM exit in Dune. The rest of the work in swapcontext-i.e., saving/restoring register state and the stack pointer-does not require system calls.</p><p>Table <ref type="table" target="#tab_1">2</ref> evaluates context switch optimizations. First, we skip setting the signal mask which eliminates the system call and brings Dune to parity with ordinary Linux. This introduces the limitation that all tasks belonging to the same application need to share the same signal mask. Next, we exploit that the main worker context does not use floating (FP) instructions. When switching from a request context to the worker context, we must save FP registers as they may have been used in request processing, but we do not need to restore them for the worker context. When switching from the worker context to a request context, we skip saving FP registers and just restore them for the request context. Shinjuku uses the last two options in Table <ref type="table" target="#tab_1">2</ref> for context switching in worker cores. The overall cost ranges from 36 to 109 cycles (18 to 55ns for a 2GHz system).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Preemptive Scheduling</head><p>The centralized dispatcher and fast preemption and context switch mechanisms allow Shinjuku to implement preemptive scheduling policies. We developed two policies that differ on whether we can differentiate a priori between requests types. The policies rely on frequent preemption to provide near-optimal tail latency for any workload, approximating c-FCFS for low dispersion workloads and PS for all other cases.</p><p>Single queue (SQ) policy: This policy assumes that we do not differentiate a priori between request types and that there is a single service-level agreement (SLO) for tail latency. This is the case, for example, in a search service where we cannot know a priori which requests will have longer service times. All incoming requests are placed in a single FCFS queue. When a worker is idle, the dispatcher assigns to it the request at the head of the queue. If requests are processed quickly, this policy operates as centralized FCFS. The dispatcher uses timestamps to identify any request running for more than a predefined quantum (5 to 15?sec in our experiments) and, assuming the queue is not empty, preempts it. The request is placed back in the queue and the worker is assigned the request at the current head of the queue. The c-PRE-SQ policy evaluated through simulation in Figure <ref type="figure">1</ref> is this single queue policy.</p><p>Multi queue (MQ) policy: This policy assumes that the network subsystem can identify different request types. For example, it can parse the request header for KVS like Redis and RocksDB and separate simple get/put requests from complex range query requests <ref type="bibr" target="#b31">[33]</ref> or use different ports for different request types. Linux already supports peeking into packets with eBPF [2]. Each request type can have a different tail latency SLO. The dispatcher maintains one queue per request type. If only one queue has pending requests, this policy operates just like the single queue policy described above. If more than one queue is non empty, the dispatcher must select a queue to serve when a worker becomes idle or a request is preempted. Once the queue is selected, the dispatcher always takes the request at the head.</p><p>The queue selection algorithm is inspired by BVT <ref type="bibr" target="#b22">[24]</ref>, a process scheduling algorithm for latency sensitive tasks. In BVT, each process has a warp factor that quantifies its priority compared to other processes. For Shinjuku, we need a similar warp factor that favors requests with smaller target latency in the short term, but also considers aging of requests with longer latency targets. Since Shinjuku schedules requests and not long running processes with priorities like BVT, the selec-tion algorithm shown below uses as input the target SLO latency for each queue (e.g., target 99th percentile latency). For the request at the head of each queue, the algorithm uses timestamps to calculate the ratio of the time it has already spent in the system (queuing time) to the SLO target latency for this request type. The queue with the highest such ratio is selected. The algorithm initially favors short requests that can only tolerate short queuing times, but eventually selects long requests that may have been waiting for a while. The per-queue SLO is a user-set parameter. In our experiments, we set it by running each request type individually using the single queue policy and use the observed 99% latency. This captures the requirement that the performance of a request type should not be affected by the existence of requests with different service time distributions.</p><p>1 Queue Selection Policy if cur ratio &gt; max then return max queue A preempted request can be placed either at the tail of its queue to approximate PS or at the head of the queue to approximate c-FCFS. This choice can be set by the application or based on online measurements of service time statistics. The rule of thumb we use is that for multi-modal or heavy-tailed workloads, the requests should be placed at the tail of the queue, while for light-tailed ones at the head. Frequent preemption is needed even with light-tailed distributions in order to allow Shinjuku to serve the queues for other request types. The c-PRE-MQ policy evaluated through simulation in Figure <ref type="figure">2</ref> is this multi-queue policy, where both request types are placed at the head of their corresponding queues when preempted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Implementation</head><p>The current Shinjuku implementation is based on Dune and requires the VT-x virtualization features in x86-64 systems <ref type="bibr" target="#b18">[20]</ref>. Dune can be ported to other architectures with similar virtualization support. Our modifications to Dune involve 1365 SLOC. The Shinjuku dispatcher and worker code are 2535 SLOC. The network subsystem we used in ?4 is based on IX <ref type="bibr" target="#b14">[16]</ref>. All the aforementioned codebases are in C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>API:</head><p>To use Shinjuku, applications need to register three callback functions: the init() function that initializes global application state; the init per core(int core num) function that initializes application state for each worker thread (e.g. local variables or configuration options); the reply * handle request(request * ) function that handles a single application-level request and returns a pointer to the reply data.</p><p>Context management: We use a modified version of the Linux ucontext library for context management. The context structure consists of a machine-specific representation of the saved state, the signal mask, a pointer to the context stack, and a pointer to the context that will be resumed when this context finishes execution. The dispatcher allocates context objects and stack space for each request from a memory pool. They are freed by the dispatcher when the request context completes execution and is returned by a worker thread.</p><p>Inter-thread communication: In adition to preemption, we use a low-overhead, shared memory communication scheme similar to that used in <ref type="bibr" target="#b48">[50]</ref>. Each pair of threads, running on dedicated cores or hyperthreads, communicates over shared pairs of cache lines, one for each direction of communication. The sending thread fills the cache line with the data it wants to send, e.g. request or context pointers, as shown in Figure <ref type="figure" target="#fig_1">3</ref>. Then, it sets the value of the byte the receiver polls to notify it that the cache line is ready for reading. This approach requires two cache line state transitions, one from shared to exclusive state which takes place when the sender writes the data and one from exclusive to shared state when the receiver reads the data. The average roundtrip latency for a message sent and received over a cache line is 211 cycles. The dispatcher's minimum work for sending a message is approximately 70 cycles, i.e. 35ns in a 2GHz machine. This sets a theoretical upper bound of 28 MRPS for the number of requests the dispatcher can handle, assuming all it has to do is to place pointers to the requests in shared memory locations and notify idling workers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Discussion</head><p>Hardware constraints: ?4 shows that a single dispatcher thread can process at least 5M requests per second and comfortably saturate a full socket with 12 cores and 24 hyperthreads. To scale a single application to higher core and/or socket counts, we must improve the dispatcher throughput. The approach we use is to have each dispatcher thread handle a subset of the worker threads and steer requests to different dispatchers using the NIC RSS feature. A relatively simple hardware fea- ture that would vastly improve the dispatcher scalability would be a low-overhead message passing mechanism among different cores <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b49">51]</ref>. Ideally, such a mechanism would offer two variations, a preemptive one that would be used for scheduling and a non-preemptive one where messages are added to per core queues and would be used for work assignment.</p><p>Connection counts: IX and ZygOS use RSS to distribute requests to workers. Using a Monte-Carlo simulation, we calculate the connection count needed for RSS to keep imbalance below 10% with high probability as we increase the number of cores. As shown in Figure <ref type="figure" target="#fig_4">4</ref>, they need 16,000 connections (clients or flows) to avoid imbalance on a server with 24 hyperthreads. High connection counts are common for public facing services (e.g., public load balancer or HTTP server), but not for internal ones. The DCTCP project <ref type="bibr" target="#b7">[9]</ref> found at most a few hundred connections to back-end servers over each 1 msec window. In contrast, Shinjuku uses RSS to distribute requests to dispatchers. Since each dispatcher can manage tens of cores, Shinjuku is not subject to the requirement of high connection (clients or flows) counts discussed in ?2. For example, 300 connections are sufficient to load balance across 2 dispatchers. When a single dispatcher suffices, Shinjuku will operate efficiently even with a single connection.</p><p>Alternative scheduling policies: Shinjuku can support more scheduling policies in addition to the two we presented. In future work, we will explore integrating Shinjuku with datacenter-wide profiling tools <ref type="bibr" target="#b47">[49]</ref> and online experimentation tools <ref type="bibr" target="#b53">[55]</ref> in order to dynamically infer the service time distributions and adjust the policy accordingly. We will also explore microsecond-scale scheduling policies that are localityand heterogeneity-aware <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b25">27]</ref>. For example, consider an application which creates a large memory footprint before responding to a client request. In such cases, we will want to avoid preempting and context switching as multiple cache lines will have to move to a different core, which can be very expensive.</p><p>Control plane: Online services experience load variations, such as a diurnal load patterns and spikes. Hence, it makes sense to adjust over time the number of workers a Shinjuku process uses. Shenango <ref type="bibr" target="#b4">[7]</ref> solves this problem by adjusting core allocation between applications in microsecond timescales. We plan to explore the possibility of integrating the two systems.</p><p>Security model: The Dune kernel module <ref type="bibr" target="#b13">[15]</ref> uses hardware support for virtualization to isolate a Shinjuku process from the Linux kernel and any other process, ordinary Linux or Shinjuku based. Linux can also remove cores and network queues from a Shinjuku process at any time. Within a Shinjuku process, the application code must trust the Shinjuku runtime and, if the application contexts execute in VMX non-root ring 0, the Shinjuku runtime must trust the application code. For example, the fact that APIC registers are mapped in the process address space means that one process could launch a denial-of-service attack on another process by issuing a large number of interrupts to a specific core.</p><p>We measured the cost of a ring 3 ? ring 0 ? ring 3 transition to be only 84 cycles. Future versions of Shinjuku will run application code in ring 3 while the Shinjuku runtime will be running in VMX non-root ring 0 eliminating this attack vector with very small overhead. Moreover, with this approach, bugs in application code will only cause contexts to crash, not affecting the runtime system.</p><p>Synchronization in user code: Online services are designed to run well on multiple cores. They synchronize across requests, but synchronization is short and infrequent to achieve scalability. Scalable applications will perform with Shinjuku regardless of whether we disable or allow preemption around read/write locks. We currently disable interrupts during any non threadsafe code, using a call safe(fn) API call to simplify application porting. The runtime overhead of the instructions that are used to disable interrupts is only a few clock cycles and they do not affect the Linux kernel's abilities to reclaim the cores. Memory allocation code is a special case that often optimizes away locks using thread-local storage. We preload our own version of the C and C++ libraries that disable interrupts (and hence preemption) during the execution of allocation functions. If these functions take a long time, it will affect the tail latency observed with Shinjuku.</p><p>Any application that frequently uses coarse-grain or contested locks within requests will scale poorly regard-less of scheduling policy on any system, including Shinjuku.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We compare Shinjuku to IX <ref type="bibr" target="#b14">[16]</ref> and ZygOS <ref type="bibr" target="#b44">[46]</ref>, two recent systems that use d-FCFS and approximate c-FCFS respectively to improve tail latency. All three systems are built on top of Dune <ref type="bibr" target="#b13">[15]</ref>. We use the latest IX and ZygOS versions available at <ref type="bibr" target="#b2">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Methodology</head><p>Machines: We use a cluster of 6 client and one server machines, connected through an Arista 7050-S switch with 48 10GbE ports. The client machines each include two Intel Xeon E5-2630 CPUs operating at 2.3GHz. Their NICs are a mixture of Intel 82599ES and Solarflare SFC9020 10GbE NICs. The server machine that runs IX, ZygOS, or Shinjuku includes two Intel E5-2658 CPUs operating at 2.3GHz, 128GB of DRAM, and an Intel 82599ES 10Gb NIC. All machines run Ubuntu LTS 16.0.4 with the 4.4.0 Linux kernel. Hyperthreading is always enabled unless noted. NICs are configured as half-duplex by the IX and ZygOS drivers and we use the same setting for Shinjuku. To perform scalability experiments, we also use the server machine with a 40Gb Intel XL710-QDA2 NIC and an identical E5-2658 twosocket machine as the client.</p><p>Each of the two server CPUs has 12 cores and 24 hyperthreads. However, ZygOS and IX can only support up to 16 hyperthreads as their network drivers are limited to 16 RSS RX queues. Hence, we use an 8-core (16-hyperthread) configuration for most experiments. Shinjuku always uses two of the available hyperthreads for the networking subsystem and dispatcher. Hence, our results use the notation Shinjuku(x) to specify that Shinjuku uses x-2 hyperthreads for workers and a total of x hyperthreads. The notation IX(x) and ZygOS(x) specify that IX and ZygOS use x hyperthreads, all for d-FCFS or c-FCFS processing respectively.</p><p>Networking: We use the following networking subsystem with Shinjuku. A single hyperthread, co-located on the same physical core with the dispatcher, polls the NIC queue and processes raw packets. It performs UDP processing, identifies requests, and optionally parses the request header to identify types. The Shinjuku workers process network replies. This simple subsystem is sufficient to evaluate Shinjuku. Since Shinjuku decouples network processing from request scheduling, we can combine Shinjuku in the future with alternative systems that implement other transport protocols and use optimizations such as multithreaded stacks <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b20">22]</ref> or stacks that offload networking to a SmartNIC <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b17">19]</ref>. The latter will free x86 hyperthreads for Shinjuku workers. If the SmartNIC is connected to the processor chip through a coherent interconnect like Intel's UPI, we can also offload the Shinjuku dispatcher to the NIC cores.</p><p>IX supports both UDP and TCP networking. We use it with UDP and a batch size of 64. ZygOS supports only TCP networking <ref type="bibr" target="#b2">[4]</ref>, but is configured to use exactly one TCP segment per request and reply. Hence, ZygOS requests have some additional service time for TCP processing ( &lt; 0.25?sec), but are otherwise similar to UDP-based IX and Shinjuku requests.</p><p>Workloads: We use one synthetic and one real workload. The synthetic workload is a server application where requests perform dummy work that we can control in order to emulate any target distribution of service times. This synthetic server allows us to derive insights about how the three systems compare across a large application space.</p><p>We also use RocksDB version 5.13 <ref type="bibr" target="#b24">[26]</ref>, a popular and widely deployed key-value store developed by Facebook. The IX, ZygOS, and Shinjuku servers handle RocksDB queries that may be simple get/put requests or range scans. We configure RocksDB to keep all data in DRAM in order to evaluate all three systems under the lowest latency requirements possible. If some RocksDB requests had to access data in Flash, the variability of service times would be even higher, and the preemptive Shinjuku would perform even better than the nonpreemptive IX and ZygOS.</p><p>We developed an open loop load generator similar to mutilate <ref type="bibr" target="#b34">[36]</ref> that transmits requests over either TCP or UDP. The load generator starts a large number of connections in a set of client machines, while it measures latency from a single unloaded machine. Unless otherwise noted, we use 1920 persistent TCP connections (ZygOS) and 1920 distinct UDP 5-tuples (IX and Shinjuku). Using fewer connections significantly affected the performance of IX and ZygOS (see ?3.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Synthetic Workload Comparison</head><p>Figure <ref type="figure">5</ref> compares Shinjuku to IX and ZygOS for three service time distributions. Figure <ref type="figure">5a</ref> uses a fixed service time of 1?sec, while Figure <ref type="figure">5b</ref> uses an exponential distribution with a mean of 1?sec. These two cases are ideal for IX that uses d-FCFS. IX benefits further from its ability to batch similarly sized requests. Shinjuku (SQ) performs close to IX, despite exclusively using two hyperthreads for networking and the dispatcher and despite preempting requests that exceed 5?sec. In this case, Shinjuku places preempted requests at the head of the queues. Moreover, preemption is fast and for light-IX( <ref type="formula">16</ref>) ZygOS( <ref type="formula">16</ref>) Shinjuku( <ref type="formula">16</ref> IX <ref type="bibr" target="#b14">(16)</ref> ZygOS( <ref type="formula">16</ref>) Shinjuku( <ref type="formula">16</ref> tailed workloads only a few requests will be preempted allowing Shinjuku to outperform ZygOS for both scenarios. ZygOS also has a high stealing rate (60%) even for homogeneous workloads which exacerbates its stealing overheads. A similar performance drop was also observed in the original ZygOS paper <ref type="bibr" target="#b44">[46]</ref>. Figure <ref type="figure">5c</ref> uses a Bimodal(99.5 -0.5, 0.5 -500) service time distribution where 99.5% of the requests have a 0.5?sec service time and 0.5% 500?sec. Shinjuku with the single queue policy is vastly better than both IX and ZygOS, achieving up to 50% lower tail latency at low load and 5x better throughput for a given 300?s tail latency target. IX and ZygOS lack preemption, hence the 0.5% of long requests determine the overall 99th percentile tail latency as short requests are frequently blocked behind them. The task stealing in ZygOS improves upon IX but is not sufficient to deal with the high dispersion in service times. In contrast, Shinjuku preempts long requests and places them at the tail of the single queue to allow short requests to complete quickly.</p><p>Figure <ref type="figure" target="#fig_6">6</ref> evaluates the three systems with multiple request types, a key experiment that was missing from the original IX and ZygOS papers. We use Shinjuku's multiqueue policy which assumes knowledge of the request types (e.g., from packet inspection). Figure <ref type="figure" target="#fig_6">6a</ref> uses a Bimodal(50 -1, 50 -100) workload, while Figure <ref type="figure" target="#fig_6">6b</ref> uses a Trimodal(33 -1, 33 -10, 33 -100) workload. In all cases, Shinjuku places preempted requests to the head of their corresponding queues. Both figures show the 99th percentile of request slowdown (overall request latency / service time) with a logarithmic y-axis. The preemptive, multi-queue policy allows Shinjuku to outperform IX and Zygos by having 94% lower slowdown at low load and over 2x higer throughput (RPS). In addition to the frequent preemption that avoids head-ofline blocking, Shinjuku benefits from its ability to select the type of requests (long vs. short) to serve next based on their ratio of queuing time to target latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Shinjuku Analysis</head><p>How important is frequent preemption? Figure 7a ZygOS <ref type="bibr" target="#b14">(16)</ref> Shinjuku( <ref type="formula">16</ref>) IX( <ref type="formula">16</ref>)</p><p>ZygOS( <ref type="formula">16</ref>) Shinjuku( <ref type="formula">16</ref> varies the preemption interval for a Bimodal(50 -1, 50 -100) synthetic workload. Shinjuku uses the multi-queue policy. The shorter the preemption interval, the better Shinjuku performs as the impact of 100?sec requests on 1?sec is reduced. Shinjuku performs well even at the very frequent 5?sec preemption interval.</p><p>How does Shinjuku scale? Figures 7{b,c} examine how Shinjuku scales with more workers. We issue short requests with 1?sec fixed service time to stress the dispatcher. We also use the Intel XL710-QDA2 40Gb NICs so that networking is not a bottleneck. Since each worker thread can saturate its core, we turn off hyperthreading and pin each worker thread to a physical core. We use the two hyperthreads in the 12th physical core for the dispatcher and the networking threads. Figure <ref type="figure" target="#fig_7">7b</ref> shows that a single dispatcher thread scales almost linearly to 11 worker cores, which is the socket size in our server. A second dispatcher thread allows Shinjuku to schedule across the 22 worker cores on both sockets for a single application. Shinjuku can schedule 5M and 9.5M RPS with 1 and 2 dispatchers respectively. Figure <ref type="figure" target="#fig_7">7c</ref> measures the outgoing network throughput of Shinjuku using two dispatchers. Shinjuku saturates the 40Gb NIC when reply frames are as short as 258 bytes.</p><p>These two figures validate that a single Shinjuku application can scale to high core counts and high line rates even with short 1?sec service times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">RocksDB Comparison</head><p>We use RocksDB with a simple server we ported to IX, ZygOS, and Shinjuku. Client requests are looked up in a RocksDB database created on an in-memory file system (/tmpfs/) with random key-value pairs. We use two request types: GET requests for a single key-value pair that execute within 6?sec; SCAN requests that scan 1,000 or 5,000 key-value pairs and require 240?sec or 1, 200?sec respectively. We use memory-mapped plain tables as the backing files to avoid memory copies and access to block devices. Shinjuku uses a preemption time slice of 15?sec and places preempted requests at the head of their corresponding queues for the multiqueue policy and at the tail for the single-queue policy.</p><p>Figure <ref type="figure" target="#fig_8">8a</ref> compares IX, ZygOS, and Shinjuku with the single queue policy for a 99.5-0.5 mix of GET and SCAN(1000) requests. Shinjuku provides a vast improvement over ZygOS in tail latency (88% decrease) and throughput (6.6x improvement). Frequent preemption in Shinjuku allows GET requests to avoid long queuing times due to SCAN requests. IX performs even worse due to the combination of highly imbalanced request service times and d-FCFS scheduling.</p><p>Preemption and queue selection policy matter: Figures 8b and 8c use a 50-50 workload between GET and SCAN(5000) requests. In addition to comparing with ZygOS, we modified the Shinjuku dispatcher to show the impact of using Shinjuku without preemption, the single-queue preemptive policy, and the multi-queue preemptive policy. IX is omitted because its latency is outside the range of our plot. The results show that Shinjuku without preemption (SQ without preemption) favors the longer SCAN requests over the shorter GET requests. The addition of preemption (SQ) fixes this problem and allows both request types to achieve fair throughput and low tail latency. The multi-queue policy (MQ) improves SCAN requests as it avoids excessive queuing for them as well. ZygOS performs significantly worse even than Shinjuku without preemption. ZygOS uses distributed queuing and is susceptible to head-ofline blocking for requests within the same connection. This supports our decision to decouple network processing and request scheduling in Shinjuku.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Optimized network stacks: There is significant work in optimizing network stacks, including polling based processing (DPDK <ref type="bibr" target="#b1">[3]</ref>), multi-core scalability (mTCP <ref type="bibr" target="#b29">[31]</ref>), modularity and specialization (Sandstorm <ref type="bibr" target="#b38">[40]</ref>), and OS bypass (Andromeda <ref type="bibr" target="#b20">[22]</ref>). Shinjuku is orthogonal to this work as it optimizes request scheduling after network protocol processing.</p><p>Dataplane operating systems: Several recent systems optimize for throughput and tail latency by separating the OS dataplane from the OS control plane, an idea originating in Exokernel <ref type="bibr" target="#b23">[25]</ref>. IX <ref type="bibr" target="#b14">[16]</ref>, Arrakis <ref type="bibr" target="#b43">[45]</ref>, MICA <ref type="bibr" target="#b37">[39]</ref>, Chronos <ref type="bibr" target="#b30">[32]</ref>, and ZygOS <ref type="bibr" target="#b44">[46]</ref> fall in this category. Shinjuku improves on these systems by introducing preemptive scheduling that allows short requests to avoid excessive queuing.</p><p>Task scheduling: Li et al. <ref type="bibr" target="#b36">[38]</ref> control tail latency by reducing the amount of resources dedicated to longrunning requests that violate the SLO. Haque et al. <ref type="bibr" target="#b27">[29]</ref> take the opposite approach and devote more resources to stragglers so that they finish faster. Interestingly, both approaches work well. However, these approaches are applicable to millisecond-scale workloads and require workloads that are dynamically parallelizable. Shinjuku allows the development of efficient scheduling policies for requests 3 orders of magnitude shorter than what this line of work can handle.</p><p>Flow scheduling: PIAS <ref type="bibr" target="#b10">[12]</ref> is a network flow scheduling mechanism that uses hardware priority queues available in switches to approximate the Shortest Job First (SJF) scheduling policy and prioritize short flows over longs ones. We do not follow a similar approach in Shinjuku as SJF is optimal in terms of minimizing average but not tail latency <ref type="bibr" target="#b55">[57]</ref>. Moreover, in order to be effective, PIAS requires some form of congestion control to keep the queue length short. This is not practical in non-networked settings where the runtime does not control the application.</p><p>Exit-less interrupts: The idea of safe, low-overhead interrupts was introduced in ELI for fast delivery of interrupts to VMs <ref type="bibr" target="#b8">[10]</ref>. ZygOS <ref type="bibr" target="#b44">[46]</ref> uses inter-processor interrupts for work stealing but does not implement preemptive scheduling. Shinjuku uses Dune <ref type="bibr" target="#b13">[15]</ref> to optimize processor-to-processor interrupts.</p><p>User-space thread management: Starting with scheduler activations <ref type="bibr" target="#b9">[11]</ref>, there have been several efforts to implement efficient, user-space thread libraries <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b54">56,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b0">1]</ref>. They all focus on cooperative scheduling. Shinjuku shows that preemptive scheduling is practical at microsecond-scales and leads to low tail latency and high throughput.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Shinjuku uses hardware support for virtualization to make frequent preemption practical at the microsecond scale. Hence, its scheduling policies can avoid the common pitfall of non-preemptive policies where short requests are blocked behind long requests. Shinjuku provides low tail latency and high throughput for a wide range of distributions and request service times regardless of the number of client connections. For the RocksDB KVS, we show that Shinjuku improves upon the recently published ZygOS system by 6.6x in throughput and 88% in tail latency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Simulation results for different workloads and scheduling policies for a 16-core system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Shinjuku system design.</figDesc><graphic url="image-3.png" coords="5,323.64,72.00,210.59,127.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Number of concurrent connections needed for load imbalance among queues to be less than 10% with probability greater than 90%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Bimodal(99.5 -0.5, 0.5 -500) Figure5: Systems comparison with synthetic workloads. Shinjuku uses the single queue policy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Systems comparison with multi-modal synthetic workloads. Shinjuku uses the multi-queue policy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>(a) Two request types Bimodal(50-1, 50-100) with varying preemption time slice. (b) Shinjuku throughput (Million RPS) as we scale the worker cores. (c) Shinjuku throughput (Gbps) as we scale the worker cores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>RocksDB (a) Shinjuku, IX, and ZygOS 99.5% GET 0.5% SCAN(1000). (b + c) Shinjuku Performance -50% GET(b) 50% SCAN(5000)(c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="1,-9.00,-10.14,630.00,272.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-2.png" coords="1,-9.00,553.00,630.00,249.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Average preemption overhead in cycles.</figDesc><table><row><cell>Preemption</cell><cell cols="2">Sender Receiver</cell><cell>Total</cell></row><row><cell>Mechanism</cell><cell>Cost</cell><cell cols="2">Cost Latency</cell></row><row><cell>Linux Signal</cell><cell>2084</cell><cell>2523</cell><cell>4950</cell></row><row><cell>Vanilla IPI</cell><cell>2081</cell><cell>2662</cell><cell>4219</cell></row><row><cell>IPI Sender-only Exit</cell><cell>2081</cell><cell>1212</cell><cell>2768</cell></row><row><cell>IPI Receiver-only Exit</cell><cell>298</cell><cell>2662</cell><cell>3433</cell></row><row><cell>IPI No Exits</cell><cell>298</cell><cell>1212</cell><cell>1993</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Average overhead in clock cycles of different context-switch mechanisms for both an ordinary Linux process and the Dune process used by Shinjuku.</figDesc><table><row><cell>Mechanism</cell><cell cols="2">Linux process Dune process</cell></row><row><cell>swapcontext</cell><cell>985</cell><cell>2290</cell></row><row><cell>No signal mask</cell><cell>140</cell><cell>140</cell></row><row><cell>No FP-restore</cell><cell>36</cell><cell>36</cell></row><row><cell>No FP-save</cell><cell>109</cell><cell>109</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank our shepherd, <rs type="person">Irene Zhang</rs>, and the anonymous NSDI reviewers for their helpful feedback. We also thank <rs type="person">John Ousterhout</rs>, <rs type="person">Adam Wierman</rs>, and <rs type="person">Ana Klimovic</rs> for providing feedback on early versions of this paper. This work was supported by the <rs type="institution">Stanford Platform Lab</rs> and by gifts from <rs type="funder">Google</rs>, <rs type="person">Huawei</rs>, and <rs type="funder">Samsung</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://github.com/brianwatling/libfiber" />
		<title level="m">A user space threading library supporting multi-core systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="http://www.dpdk.org/" />
		<title level="m">Data plan development kit</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="https://github.com/ix-project/" />
		<title level="m">Ix-project: Protected dataplane for low latency and high performance</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><surname>Memcached</surname></persName>
		</author>
		<ptr target="https://memcached.org/" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Achieving high CPU efficiency for latency-sensitive datacenter workloads</title>
		<author>
			<persName><surname>Shenango</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th USENIX Symposium on</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m">Networked Systems Design and Implementation (NSDI 19)</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cooperative task management without manual stack management</title>
		<author>
			<persName><forename type="first">Atul</forename><surname>Adya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Howell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Theimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Bolosky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">R</forename><surname>Douceur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the General Track of the Annual Conference on USENIX Annual Technical Conference, ATC &apos;02</title>
		<meeting>the General Track of the Annual Conference on USENIX Annual Technical Conference, ATC &apos;02</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="289" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Data center tcp (dctcp)</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Maltz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Padhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parveen</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaji</forename><surname>Prabhakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudipta</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murari</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCOMM 2010 Conference, SIGCOMM &apos;10</title>
		<meeting>the ACM SIGCOMM 2010 Conference, SIGCOMM &apos;10<address><addrLine>New Delhi, India</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="63" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bare-metal performance for virtual machines with exitless interrupts. Commun</title>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Har'el</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muli</forename><surname>Ben-Yehuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Landau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Tsafrir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-12">December 2015</date>
			<publisher>ACM</publisher>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="108" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scheduler activations: Effective kernel support for the user-level management of parallelism</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">E</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">N</forename><surname>Bershad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">D</forename><surname>Lazowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><forename type="middle">M</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, SOSP &apos;91</title>
		<meeting>the Thirteenth ACM Symposium on Operating Systems Principles, SOSP &apos;91<address><addrLine>Pacific Grove, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="95" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Information-agnostic flow scheduling for commodity data centers</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Networked Systems Design and Implementation (NSDI 15)</title>
		<meeting><address><addrLine>Oakland, CA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="455" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Web search for a planet: The google cluster architecture</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Holzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="22" to="28" />
			<date type="published" when="2003-03">March 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attack of the killer microseconds</title>
		<author>
			<persName><forename type="first">Luiz</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Marty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parthasarathy</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="48" to="54" />
			<date type="published" when="2017-03">March 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dune: Safe user-level access to privileged cpu features</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Belay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Bittau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Mashtizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Terei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mazi?res</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th USENIX Conference on Operating Systems Design and Implementation, OSDI&apos;12</title>
		<meeting>the 10th USENIX Conference on Operating Systems Design and Implementation, OSDI&apos;12<address><addrLine>Hollywood, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="335" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ix: A protected dataplane operating system for high throughput and low latency</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Belay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Prekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Klimovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Bugnion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation, OSDI&apos;14</title>
		<meeting>the 11th USENIX Conference on Operating Systems Design and Implementation, OSDI&apos;14<address><addrLine>Broomfield, CO</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="49" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Putting the &quot;micro&quot; back in microservice</title>
		<author>
			<persName><forename type="first">Sol</forename><surname>Boucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anuj</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kaminsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 USENIX Annual Technical Conference (USENIX ATC 18)</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="645" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Ps225</title>
		<author>
			<persName><surname>Broadcom</surname></persName>
		</author>
		<ptr target="https://www.broadcom.com/products/ethernet-connectivity/network-adapters/ps225" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><surname>Cavium</surname></persName>
		</author>
		<author>
			<persName><surname>Liquidio</surname></persName>
		</author>
		<ptr target="https://www.cavium.com/product-liquidio-adapters.html" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Intel</forename><surname>Corp</surname></persName>
		</author>
		<ptr target="https://www.intel.com/content/www/us/en/virtualization/virtualization-technology/intel-virtualization-technology.html" />
		<title level="m">Intel virtualization technology</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<ptr target="http://msdn.microsoft.com/library/windows/hardware/ff556942.aspx" />
		<title level="m">Receive side scaling</title>
		<imprint>
			<publisher>Microsoft Corp</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Andromeda: Performance, isolation, and velocity at scale in cloud network virtualization</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Adriaens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahsan</forename><surname>Arefin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshuman</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Fahs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dima</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrique</forename><forename type="middle">Cauich</forename><surname>Zermeno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Rubow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Alexander Docauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Alpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Decabooter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>De Kruijf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Kasinadhuni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Crepaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivas</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subbaiah</forename><surname>Venkata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uday</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th USENIX Symposium on Networked Systems Design and Implementation (NSDI 18)</title>
		<meeting><address><addrLine>Renton, WA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="373" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The tail at scale. Communications of the ACM</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luiz</forename><surname>Andr?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barroso</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="74" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Borrowed-virtualtime (bvt) scheduling: Supporting latency-sensitive threads in a general-purpose scheduler</title>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">J</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Cheriton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth ACM Symposium on Operating Systems Principles, SOSP &apos;99</title>
		<meeting>the Seventeenth ACM Symposium on Operating Systems Principles, SOSP &apos;99<address><addrLine>Charleston, South Carolina, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="261" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exokernel: An operating system architecture for application-level resource management</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Engler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>O'toole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jr</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, SOSP &apos;95</title>
		<meeting>the Fifteenth ACM Symposium on Operating Systems Principles, SOSP &apos;95<address><addrLine>Copper Mountain, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="251" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><surname>Facebook</surname></persName>
		</author>
		<author>
			<persName><surname>Rocksdb</surname></persName>
		</author>
		<ptr target="http://rocksdb.org/" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Slaw: A scalable locality-aware adaptive work-stealing scheduler for multi-core systems</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jisheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Cave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIG-PLAN Symposium on Principles and Practice of Parallel Programming, PPoPP &apos;10</title>
		<meeting>the 15th ACM SIG-PLAN Symposium on Principles and Practice of Parallel Programming, PPoPP &apos;10<address><addrLine>Bangalore, India</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="341" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Softnic: A software nic to augment hardware</title>
		<author>
			<persName><forename type="first">Sangjin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keon</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurojit</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoumik</forename><surname>Palkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvia</forename><surname>Ratnasamy</surname></persName>
		</author>
		<idno>UCB/EECS-2015-155</idno>
		<imprint>
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
		<respStmt>
			<orgName>EECS Department, University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Few-to-many: Incremental parallelism for reducing tail latency in interactive services</title>
		<author>
			<persName><forename type="first">E</forename><surname>Md</surname></persName>
		</author>
		<author>
			<persName><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>Yong Hun Eom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameh</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Elnikety</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><forename type="middle">S</forename><surname>Bianchini</surname></persName>
		</author>
		<author>
			<persName><surname>Mckinley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;15</title>
		<meeting>the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;15<address><addrLine>Turkey</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="161" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploiting heterogeneity for tail latency and energy efficiency</title>
		<author>
			<persName><forename type="first">E</forename><surname>Md</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameh</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thu</forename><forename type="middle">D</forename><surname>Elnikety</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><forename type="middle">S</forename><surname>Bianchini</surname></persName>
		</author>
		<author>
			<persName><surname>Mckinley</surname></persName>
		</author>
		<idno>MICRO-50 &apos;17</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 50th Annual IEEE/ACM International Symposium on Microarchitecture<address><addrLine>Cambridge, Massachusetts</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="625" to="638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">mtcp: a highly scalable user-level TCP stack for multicore systems</title>
		<author>
			<persName><forename type="first">Eunyoung</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinae</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Jamshed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haewon</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunghwan</forename><surname>Ihm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoungsoo</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th USENIX Symposium on Networked Systems Design and Implementation (NSDI 14)</title>
		<meeting><address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="489" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Chronos: Predictable low latency for data center applications</title>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malveeka</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">M</forename><surname>Voelker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third ACM Symposium on Cloud Computing, SoCC &apos;12</title>
		<meeting>the Third ACM Symposium on Cloud Computing, SoCC &apos;12<address><addrLine>San Jose, California</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">High performance packet processing with flexnic</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Kr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;16</title>
		<meeting>the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;16<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="67" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Carbon: Architectural support for fine-grained parallelism on chip multiprocessors</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th Annual International Symposium on Computer Architecture, ISCA &apos;07</title>
		<meeting>the 34th Annual International Symposium on Computer Architecture, ISCA &apos;07<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="162" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Redis</forename><surname>Labs</surname></persName>
		</author>
		<author>
			<persName><surname>Redis</surname></persName>
		</author>
		<ptr target="https://redis.io/" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reconciling high server utilization and sub-millisecond quality-of-service</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Leverich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth European Conference on Computer Systems, EuroSys &apos;14</title>
		<meeting>the Ninth European Conference on Computer Systems, EuroSys &apos;14<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Tales of the tail: Hardware, os, and application-level sources of tail latency</title>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Kr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><forename type="middle">R K</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">D</forename><surname>Ports</surname></persName>
		</author>
		<author>
			<persName><surname>Gribble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on Cloud Computing, SOCC &apos;14</title>
		<meeting>the ACM Symposium on Cloud Computing, SOCC &apos;14<address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Work stealing for interactive services to meet target latency</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameh</forename><surname>Elnikety</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I-Ting Angelina</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP &apos;16</title>
		<meeting>the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP &apos;16<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mica: A holistic approach to fast in-memory keyvalue storage</title>
		<author>
			<persName><forename type="first">Hyeontaek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kaminsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th USENIX Conference on Networked Systems Design and Implementation, NSDI&apos;14</title>
		<meeting>the 11th USENIX Conference on Networked Systems Design and Implementation, NSDI&apos;14<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="429" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Network stack specialization for performance</title>
		<author>
			<persName><forename type="first">Ilias</forename><surname>Marinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><surname>Handley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 ACM Conference on SIGCOMM, SIGCOMM &apos;14</title>
		<meeting>the 2014 ACM Conference on SIGCOMM, SIGCOMM &apos;14<address><addrLine>Chicago, Illinois, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="175" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Timely: Rtt-based congestion control for the datacenter</title>
		<author>
			<persName><forename type="first">Radhika</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">The</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nandita</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dukkipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Blem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monia</forename><surname>Wassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Ghobadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaogong</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wetherall</surname></persName>
		</author>
		<author>
			<persName><surname>Zats</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication, SIGCOMM &apos;15</title>
		<meeting>the 2015 ACM Conference on Special Interest Group on Data Communication, SIGCOMM &apos;15<address><addrLine>London, United Kingdom</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="537" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The fundamentals of heavy-tails: Properties, emergence, and identification</title>
		<author>
			<persName><forename type="first">Jayakrishnan</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Wierman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bert</forename><surname>Zwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGMETRICS/International Conference on Measurement and Modeling of Computer Systems, SIGMETRICS &apos;13</title>
		<meeting>the ACM SIGMETRICS/International Conference on Measurement and Modeling of Computer Systems, SIGMETRICS &apos;13<address><addrLine>Pittsburgh, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="387" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scaling memcache at facebook</title>
		<author>
			<persName><forename type="first">Rajesh</forename><surname>Nishtala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans</forename><surname>Fugal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herman</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><forename type="middle">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcelroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Paleczny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Peek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Stafford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkateshwaran</forename><surname>Venkataramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Presented as part of the 10th USENIX Symposium on Networked Systems Design and Implementation (NSDI 13)</title>
		<meeting><address><addrLine>Lombard, IL</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="385" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The ramcloud storage system</title>
		<author>
			<persName><forename type="first">John</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankita</forename><surname>Kejriwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Montazeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Ongaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mendel</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Rumble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Stutsman</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">55</biblScope>
			<date type="published" when="2015-08">August 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Arrakis: The operating system is the control plane</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><forename type="middle">R K</forename><surname>Ports</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Woos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Roscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting><address><addrLine>Broomfield, CO</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Zygos: Achieving low tail latency for microsecond-scale networked tasks</title>
		<author>
			<persName><forename type="first">George</forename><surname>Prekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marios</forename><surname>Kogias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Bugnion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Symposium on Operating Systems Principles, SOSP &apos;17</title>
		<meeting>the 26th Symposium on Operating Systems Principles, SOSP &apos;17<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Provos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mathewson</surname></persName>
		</author>
		<ptr target="http://libevent.org" />
		<title level="m">libevent: An event notification library</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Arachne: Core-aware thread management</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Henry Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacqueline</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Speiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Kraft</surname></persName>
		</author>
		<author>
			<persName><surname>Ousterhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)</title>
		<meeting><address><addrLine>Carlsbad, CA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="145" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Google-wide profiling: A continuous profiling infrastructure for data centers</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Gang Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tipp</forename><surname>Tune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvius</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName><surname>Hundt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="65" to="79" />
			<pubPlace>Micro</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Ffwd: Delegation is (much) faster than you think</title>
		<author>
			<persName><forename type="first">Sepideh</forename><surname>Roghanchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nilanjana</forename><surname>Basu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Symposium on Operating Systems Principles, SOSP &apos;17</title>
		<meeting>the 26th Symposium on Operating Systems Principles, SOSP &apos;17<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="342" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Flexible architectural support for fine-grain scheduling</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">M</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Edition of ASPLOS on Architectural Support for Programming Languages and Operating Systems, ASPLOS XV</title>
		<meeting>the Fifteenth Edition of ASPLOS on Architectural Support for Programming Languages and Operating Systems, ASPLOS XV<address><addrLine>Pittsburgh, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="311" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Mellanox</forename><surname>Technologies</surname></persName>
		</author>
		<ptr target="http://www.mellanox.com/related-docs/whitepapers/roce_in_the_data_center.pdf" />
		<title level="m">Rdma over converged ethernet</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<ptr target="http://www.mellanox.com/related-docs/npu-multicore-processors/PB_Bluefield_SoC.pdf" />
		<title level="m">Mellanox Technologies. Bluefield multicore system on chip</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Speedy transactions in multicore in-memory databases</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenting</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Liskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Madden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles, SOSP &apos;13</title>
		<meeting>the Twenty-Fourth ACM Symposium on Operating Systems Principles, SOSP &apos;13<address><addrLine>Farminton, Pennsylvania</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="18" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Leveraging live traffic tests to identify and resolve resource utilization bottlenecks in large scale web services</title>
		<author>
			<persName><forename type="first">Kaushik</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Meza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonia</forename><surname>Margulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Michelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajesh</forename><surname>Nishtala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Obenshain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Jiun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><surname>Kraken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)</title>
		<meeting><address><addrLine>Savannah, GA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="635" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Capriccio: Scalable threads for internet services</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Rob Von Behren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Condit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Necula</surname></persName>
		</author>
		<author>
			<persName><surname>Brewer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth ACM Symposium on Operating Systems Principles, SOSP &apos;03</title>
		<meeting>the Nineteenth ACM Symposium on Operating Systems Principles, SOSP &apos;03<address><addrLine>Bolton Landing, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="268" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Is tail-optimal scheduling possible?</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Wierman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bert</forename><surname>Zwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oper. Res</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1249" to="1257" />
			<date type="published" when="2012-09">September 2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
