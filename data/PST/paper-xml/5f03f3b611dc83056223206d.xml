<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">XGNN: Towards Model-Level Explanations of Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-03">3 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Hao</forename><surname>Yuan</surname></persName>
							<email>hao.yuan@tamu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University College Station</orgName>
								<address>
									<region>Texas</region>
									<country>United States Jiliang Tang</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Michigan State University East Lansing</orgName>
								<address>
									<region>Michigan</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Xia Hu</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Texas A&amp;M University College Station</orgName>
								<address>
									<region>Texas</region>
									<country>United States Shuiwang ji</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Texas A&amp;M University College Station</orgName>
								<address>
									<region>Texas</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">XGNN: Towards Model-Level Explanations of Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-03">3 Jun 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394486.3403085</idno>
					<idno type="arXiv">arXiv:2006.02587v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep learning</term>
					<term>Interpretability</term>
					<term>Graph Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graphs neural networks (GNNs) learn node features by aggregating and combining neighbor information, which have achieved promising performance on many graph tasks. However, GNNs are mostly treated as black-boxes and lack human intelligible explanations. Thus, they cannot be fully trusted and used in certain application domains if GNN models cannot be explained. In this work, we propose a novel approach, known as XGNN, to interpret GNNs at the model-level. Our approach can provide high-level insights and generic understanding of how GNNs work. In particular, we propose to explain GNNs by training a graph generator so that the generated graph patterns maximize a certain prediction of the model. We formulate the graph generation as a reinforcement learning task, where for each step, the graph generator predicts how to add an edge into the current graph. The graph generator is trained via a policy gradient method based on information from the trained GNNs. In addition, we incorporate several graph rules to encourage the generated graphs to be valid. Experimental results on both synthetic and real-world datasets show that our proposed methods help understand and verify the trained GNNs. Furthermore, our experimental results indicate that the generated graphs can provide guidance on how to improve the trained GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Computing methodologies → Neural networks; Artificial intelligence; • Mathematics of computing → Graph algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph Neural Networks (GNNs) have shown their effectiveness and obtained the state-of-the-art performance on different graph tasks, such as node classification <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b36">37]</ref>, graph classification <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b46">47]</ref>, and link prediction <ref type="bibr" target="#b45">[46]</ref>. In addition, extensive efforts have been made towards different graph operations, such as graph convolution <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref>, graph pooling <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b43">44]</ref>, and graph attention <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>. Since graph data widely exist in different real-world applications, such as social networks, chemistry, and biology, GNNs are becoming increasingly important and useful. Despite their great performance, GNNs share the same drawback as other deep learning models; that is, they are usually treated as black-boxes and lack human-intelligible explanations. Without understanding and verifying the inner working mechanisms, GNNs cannot be fully trusted, which prevents their use in critical applications pertaining to fairness, privacy, and safety <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b39">40]</ref>. For example, we can train a GNN model to predict the effects of drugs where we treat each drug as a molecular graph. Without exploring the working mechanisms, we do not know what chemical groups in a molecular graph lead to the predictions. Then we cannot verify whether the rules of the GNN model are consistent with real-world chemical rules, and hence we cannot fully trust the GNN model. This raises the need of developing interpretation techniques for GNNs.</p><p>Recently, several interpretations techniques have been proposed to explain deep learning models on image and text data. Depending on what kind of interpretations are provided, existing techniques can be categorized into example-level <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b47">48]</ref> or model-level <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> methods. Example-level interpretations explain the prediction for a given input example, by determining important features in the input or the decision procedure for this input through the model. Common techniques in this category include gradient-based methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43]</ref>, visualizations of intermediate feature maps <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b47">48]</ref>, and occlusion-based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b44">45]</ref>. Instead of providing input-dependent explanations, model-level interpretations aim to explain the general behavior of the model by investigating what input patterns can lead to a certain prediction, without respect to any specific input example. Input optimization <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref> is the most popular model-level interpretation method. These two categories of interpretation methods aim at explaining deep models in different views. Since the ultimate goal of interpretations is to verify and understand deep models, we need to manually check the interpretation results and conclude if the deep models work in our expected way. For example-level methods, we may need to explore the explanations for a large number of examples before we can trust the models. However, it is timeconsuming and requires extensive expert efforts. For model-level methods, the explanations are more general and high-level, and hence need less human supervision. However, the explanations of model-level methods are less precise compared with examplelevel interpretations. Overall, both model-level and example-level methods are important for interpreting and understanding deep models.</p><p>Interpreting deep learning models on graph data become increasingly important but is still less explored. To the best of our knowledge, there is no existing study on interpreting GNNs at the model-level. The existing study <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b39">40]</ref> only provides example-level explanations for graph models. As a radical departure from existing work, we propose a novel interpretation technique, known as XGNN, for explaining deep graph models at the model-level. We propose to investigate what graph patterns can maximize a certain prediction. Specifically, we propose to train a graph generator such that the generated graph patterns can be used to explain deep graph models. We formulate it as a reinforcement learning problem that at each step, the graph generator predicts how to add an edge to a given graph and form a new graph. Then the generator is trained based on the feedback from the trained graph models using policy gradient <ref type="bibr" target="#b34">[35]</ref>. We also incorporate several graph rules to encourage the generated graphs to be valid. Note that the graph generation part in our XGNN framework can be generalized to any suitable graph generation method, determined by the dataset at hand and the GNNs to be interpreted. Finally, we trained GNN models on both real-world and synthetic datasets which can yield good performance. Then we employ our proposed XGNN to explain these trained models. Experimental results show that our proposed XGNN can find the desired graph patterns and explains these models. With our generated graph patterns, we can verify, understand, and even improve the trained GNN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Graph Neural Networks</head><p>Graphs are wildly employed to represent data in different realworld domains and graph neural networks have shown promising performance on these data. Different from image and text data, a graph is represented by a feature matrix and an adjacency matrix. Formally, a graph G with n nodes is represented by its feature matrix X ∈ R n×d and its adjacency matrix A ∈ {0, 1} n×n . Note that we assume each node has a d-dimension vector to represent its features. Graph neural networks learn node features based on these matrices. Even though there are several variants of GNNs, such as graph convolution networks (GCNs) <ref type="bibr" target="#b18">[19]</ref>, graph attention networks (GATs) <ref type="bibr" target="#b36">[37]</ref>, and graph isomorphism networks (GINs) <ref type="bibr" target="#b38">[39]</ref>, they share a similar feature learning strategy. For each node, GNNs update its node features by aggregating the features from its neighbors and combining them with its own features. We take GCNs as an example to illustrate the neighborhood information aggregation scheme. The operation of GCNs is defined as</p><formula xml:id="formula_0">X i+1 = f (D − 1 2 ÂD − 1 2 X i W i ),<label>(1)</label></formula><p>where X i ∈ R n×d i and X i+1 ∈ R n×d i +1 are the input and output feature matrices of the i th graph convolution layer. In addition, Â = A + I is used to add self-loops to the adjacency matrix, D denotes the diagonal node degree matrix to normalize Â. The matrix W i ∈ R d i ×d i +1 is a trainable matrix for layer i and is used to perform linear feature transformation and f (•) denotes a non-linear activation function. By stacking j graph convolution layers, the j-hop neighborhood information can be aggregated. Due to its superior performance, we incorporate the graph convolution in Equation (1) as our graph neural network operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model-level Interpretations</head><p>Next, we briefly discuss popular model-level interpretation techniques for deep learning models on image data, known as input optimization methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref>. These methods generally generate optimized input that can maximize a certain behavior of deep models. They randomly initialize the input and iteratively update the input towards an objective, such as maximizing a class score.</p><p>Then such optimized input can be regarded as the explanations for the target behavior. Such a procedure is known as optimization and is similar to training deep neural networks. The main difference is that in such input optimization techniques, all network parameters are fixed while the input is treated as trainable variables. While such methods can provide meaningful model-level explanations for deep models on images, they cannot be directly applied to interpret GNNs due to three challenges. First, the structural information of a graph is represented by a discrete adjacency matrix, which cannot be directly optimized via back-propagation. Second, for images, the optimized input is an abstract image and the visualization shows high-level patterns and meanings. In the case of graphs, the abstract graph is not meaningful and hard to visualize. Third, the obtained graphs may not be valid for chemical or biological rules since non-differentiable graph rules cannot be directly incorporated into optimization. For example, the node degree of an atom should not exceed its maximum chemical valency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Graph Model Interpretations</head><p>To the best of our knowledge, there are only a few existing studies focusing on the interpretability of deep graph models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b39">40]</ref>. The recent GNN interpretation tool GNN Explainer <ref type="bibr" target="#b39">[40]</ref> proposes to explain deep graph models at the example-level by learning soft masks. For a given example, it applies soft masks to graph edges and node features and updates the masks such that the prediction remains the same as the original one. Then some graph edges and node features are selected by thresholding the masks, and they are treated as important edges and features for making the prediction for the given example. The other work [4] also focuses on the example-level interpretations of deep graph models. It applies several well-known image interpretation methods to graph models, such as sensitivity analysis (SA) <ref type="bibr" target="#b11">[12]</ref>, guided backpropagation (GBP) <ref type="bibr" target="#b32">[33]</ref>, and layer-wise relevance propagation (LRP) <ref type="bibr" target="#b2">[3]</ref>. The SA and GBP methods are based on the gradients while the LRP method computes the saliency maps by decomposing the output While input-dependent explanations are important for understanding deep models, model-level interpretations should not be ignored. However, none of the existing work investigates the modellevel interpretations of deep graph models. In this work, we argue that model-level interpretations can provide higher-level insights and a more general understanding in how a deep learning model works. Therefore, we aim at providing model-level interpretations for GNNs. We propose a novel method, known as XGNN, to explain GNNs by graph generation such that the generated graphs can maximize a certain behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">XGNN: EXPLAINABLE GRAPH NEURAL NETWORKS 3.1 Model-Level GNN Interpretation</head><p>Intuitively, given a trained GNN model, the model-level interpretations for it should explain what graph patterns or sub-graph patterns lead to a certain prediction. For example, one possible type of patterns is known as network motifs that represent simple building blocks of complex networks (graphs), which widely exist in graphs from biochemistry, neurobiology, ecology, and engineering <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30]</ref>. Different motif sets can be found in graphs with different functions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref>, which means different motifs may directly relate to the functions of graphs. However, it is still unknown whether GNNs make predictions based on such motifs or other graph information. By identifying the relationships between graph patterns and the predictions of GNNs, we can better understand the models and verify whether a model works as expected. Therefore, we propose our XGNN, which explains GNNs using such graph patterns. Specifically, in this work, we investigate the model-level interpretations of GNNs for graph classification tasks and the graph patterns are obtained by graph generations. The obtained patterns can be treated as model-level interpretations with respect to c i . Formally, the task can be defined as</p><formula xml:id="formula_1">G * = argmax G P(f (G) = c i ),<label>(2)</label></formula><p>where G * is the optimized input graph we need. A popular way to obtain such optimized input for interpreting image and text models is known as input optimization <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b42">43]</ref>. However, as discussed in Section 2.2, such optimization method cannot be applied to interpret graph models because of the special representations of graph data. Instead, we propose to obtain the optimized graph G * via graph generation. The general illustration of our proposed method is shown in Figure <ref type="figure">1</ref>. Given a pre-trained graph classification model, we interpret it by providing explanations for its third class. We may manually conclude the graph patterns from the graph dataset. By evaluating all graph examples in the dataset, we can obtain the graphs that are predicted to be the third class. Then we can manually check what are the common graph patterns among these graphs. For example, the left part of Figure <ref type="figure">1</ref> shows that a set of four graphs are classified into the third class. Based on human observations, we know that the important graph pattern leading to the prediction is the triangle pattern consisting of a red node, a yellow node, and a blue node. However, such manual analysis is time-consuming and not applicable for large-scale and complex graph datasets. As shown in the right part, we propose to train a graph generator to generate graph patterns that can maximize the prediction score of the third class. In addition, we incorporate graph rules, such as the chemical valency check, to encourage valid and human-intelligible explanations. Finally, we can analyze the generated graphs to obtain model-level explanations for the third class.</p><p>Compared with directly manual analysis on the original dataset, our proposed method generates small-scale and less complex graphs, which can significantly reduce the cost for further manual analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Interpreting GNNs via Graph Generation</head><p>Recent advances in graph generation lead to many successful graph generation models, such as GraphGAN <ref type="bibr" target="#b37">[38]</ref>, ORGAN <ref type="bibr" target="#b13">[14]</ref>, Junction Tree VAE <ref type="bibr" target="#b16">[17]</ref>, DGMG <ref type="bibr" target="#b21">[22]</ref>, and Graph Convolutional Policy Network (GCPN) <ref type="bibr" target="#b40">[41]</ref>. </p><formula xml:id="formula_2">Inspired</formula><formula xml:id="formula_3">G t +1 that X t +1 , A t +1 = д θ (X t , A t ).<label>(3)</label></formula><p>Then the generator is trained with the guidance from the pretrained GNNs f (•). Since generating the new graph G t +1 from G t is non-differentiable, we formulate the generation procedure as a reinforcement learning problem. Specifically, assuming there are k types of nodes in the dataset, we define a candidate set C = {s 1 , s 2 , • • • , s k } denoting these possible node types. For example, in a chemical molecular dataset, the candidate set can be C = {Carbon, N itroдen, • • • , Oxyдen, Fluorine}. In a social network dataset where nodes are not labeled, the candidate set only contains a single node type. Then at each step t, based on the partially generated graph G t , the generator д(•) generates G t +1 by predicting how to add an edge to the current graph G t . Note that the generator may add an edge between two nodes in the current graph G t or add a node from the candidate set C to the current graph G t and connect it with an existing node in G t . Formally, we formulate it as a reinforcement learning problem, which consists of four elements: state, action, policy, and reward.</p><p>State: The state of the reinforcement learning environment at step t is the partially generated graph G t . The initial graph at the first step can be either a random node from the candidate set C or manually designed based on prior domain knowledge. For example, for the dataset describing organic molecules, we can set the initial graph as a single node labeled with carbon atom since any organic compound contains carbon generally <ref type="bibr" target="#b27">[28]</ref>.</p><p>Action: The action at step t, denoted as a t , is to generate the new graph G t +1 based on the current graph G t . Specifically, given the current state G t , the action a t is to add an edge to G t by determining the starting node and the ending node of the edge. Note that the starting node a t,st ar t can be any node from the current graph G t while the ending node a t,end is selected from the union of the current graph G t and the candidate set C excluding the selected starting node a t,st ar t , denoted as (G t C)\a t,st ar t . Note that with the predefined maximum action step and maximum node number, we can control the termination of graph generation.</p><p>Policy: We employ graph neural networks to serve as the policy. The policy determines the action a t based on the state G t . Specifically, the policy is the graph generator д θ (•), which takes G t and C as the input and outputs the probabilities of possible actions. With the reward function, the generator д θ (•) can be trained via policy gradient <ref type="bibr" target="#b34">[35]</ref>.</p><p>Reward: The reward for step t, denoted as R t , is employed to evaluate the action at step t, which consists of two parts. The first part is the guidance from the trained GNNs f (•), which encourages the generated graph to maximize the class score of class c i . By feeding the generated graphs to f (•), we can obtain the predicted probabilities for class c i and use them as the feedback to update д θ (•). The second part encourages the generated graphs to be valid in terms of certain graph rules. For example, for social network datasets, it is may not allowed to add multiple edges between two nodes. In addition, for chemical molecular datasets, the degree of an atom cannot exceed its chemical valency. Note that for each step, we include both intermediate rewards and overall rewards to evaluate the action.</p><p>While we formulate the graph generation as a reinforcement learning problem, it is noteworthy that our proposed XGNN is a novel and general framework for interpreting GNNs at the modellevel. The graph generation part in this framework can be generalized to any suitable graph generation method, determined by the dataset at hand and the GNNs to be interpreted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Graph Generator</head><p>For step t, the graph generator д θ (•) incorporates the partially generated graph G t and the candidate set C to predict the probabilities of different actions, denoted as p t,st ar t and p t,end . Assume there are n t nodes in G t and k nodes in C, then both p t,st ar t and p t,end are with n t + k dimensionality. Then the action a t = (a t,st ar t , a t,end ) is sampled from the probabilities p t = (p t,st ar t , p t,end ). Next, we can obtain the new graph G t +1 based on the action a t . Specifically, in our generator, we first employ several graph convolutional layers to aggregate neighborhood information and learn node features. Mathematically, it can be written as</p><formula xml:id="formula_4">X = GCNs(G t , C),<label>(4)</label></formula><p>where X denotes the learnt node features. Note that the graph G t and the candidate set C are combined as the input of GCNs.</p><p>We merge all nodes in C to G t without adding any edge and then obtain the new node feature matrix and adjacency matrix. Then Multilayer Perceptrons (MLPs) are used to predict the probabilities of the starting node, p t,st ar t and the action a t,st ar t is sampled from this probabilty distribution. Mathematically, it can be written as p t,st ar t = Softmax(MLPs( X )), </p><p>where [•, •] denotes broadcasting and concatenation. In addition, m t,end is the mask consisting of all 1s except the position indicating a t,st ar t . Note that the same graph generator д θ (•) is shared by different time steps, and our generator is capable to incorporate graphs with variable sizes. We illustrate our graph generator in Figure <ref type="figure">2</ref> where we show the graph generation procedure for one step. The current graph G t consists of 4 nodes and the candidate set has 3 available nodes. They are combined together to serve as the input of the graph generator. The embeddings of candidate nodes are concatenated to the feature matrix of G t while the adjacency matrix of G t is expanded accordingly. Then multiple graph convolutional layers are employed to learn features for all nodes. With the first MLPs, we obtain the probabilities of selecting different nodes as the starting node, and from which we sample the node 1 as the starting node. Then based on the features of node 1 and all node features, the second MLPs predict the ending node. We sample from the probabilities and select the node 7 as the ending node, which corresponds to the red node in the candidate set. Finally, a new graph is obtained by including a red node and connecting it with node 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training the Graph Generator</head><p>The graph generator is trained to generate specific graphs that can maximize the class score of class c i and be valid to graph rules. Since such guidance is not differentiable, we employ policy gradient <ref type="bibr" target="#b34">[35]</ref> to train the generator. According to <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b41">42]</ref>, the loss function for the action a t at step t can be mathematically written as</p><formula xml:id="formula_7">L д = −R t (L C E (p t,st ar t , a t,st ar t ) + L C E (p t,end , a t,end )),<label>(9)</label></formula><p>where L C E (•, •) denotes the cross entropy loss and R t means the reward function for step t. Intuitively, the reward R t indicates whether a t has a large chance to generate graph with high class score of class c i and being valid. Hence, the reward R t consists of two parts. The first part R t, f is the feedback from the trained model f (•) and the second part R t,r is from the graph rules. Specifically, for step t, the reward R t, f contains both an intermediate reward and a final graph reward for graph</p><formula xml:id="formula_8">G t +1 that R t, f = R t, f (G t +1 ) + λ 1 m i=1 R t,f (Rollout(G t +1 )) m ,<label>(10)</label></formula><p>where λ 1 is a hyper-parameter, and the first term is the intermediate reward which can be obtained by feeding G t +1 to the trained GNNs f (•) and checking the predicted probability for class c i . Mathematically, it can be computed as</p><formula xml:id="formula_9">R t, f (G t +1 ) = p(f (G t +1 ) = c i ) − 1/ℓ, (<label>11</label></formula><formula xml:id="formula_10">)</formula><p>where ℓ denotes the number of possible classes for f (•). In addition, the second term in Equation ( <ref type="formula" target="#formula_8">10</ref>) is the final graph reward for G t +1 which can be obtained by performing Rollout <ref type="bibr" target="#b41">[42]</ref> m times on the intermediate graph G t +1 . Each time, a final graph is generated based on G t +1 until termination and then evaluated by f (•) using Equation <ref type="bibr" target="#b10">(11)</ref>. Then the evaluations for m final graphs are averaged to serve as the final graph reward. Overall, R t,f is positive when the obtained graph tends to yield high score for class c i , and vice versa.</p><p>In addition, the reward R t,r is obtained from graphs rules and is employed to encourage the generated graphs to be valid and human-intelligible. The first rule we employ is that only one edge is Algorithm 1 The algorithm of our proposed XGNN.</p><p>1: Given the trained GNNs for graph classification, denoted as f (•), we try to interpret it and set the target class as c i .</p><p>2: Let C define the candidate node set and д(•) mean our graph generator. We predefine the maximum generation step as S max and the number of Rollout as m. 3: Define the initial graph as G 1 . 4: for step t in S max do 5:</p><p>Merge the current graph G t and the candidate set C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Obtain the action a t from the generator д(•) that a t = (a t,st ar t , a t,end ) with Equation (4-8). end if 12: end for allowed to be added between any two nodes. Second, the generated graph cannot contain more nodes than the predefined maximum node number. In addition, we incorporate dataset-specific rules to guide the graph generation. For example, in a chemical dataset, each node represents an atom so that its degree cannot exceed the valency of the corresponding atom. When any of these rules is violated, a negative reward will be applied for R t,r . Finally, by combining the R t,f and R t,r , we can obtain the reward for step t that</p><formula xml:id="formula_11">R t = R t,f (G t +1 ) + λ 1 m i=1 R t, f (Rollout(G t +1 )) m + λ 2 R t,r ,<label>(12)</label></formula><p>where λ 1 and λ 2 are hyper-parameters. We illustrate the training procedure in Algorithm 1. Note that we roll back the graph G t +1 to G t when the action a t is evaluated as not promising that R t &lt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL STUDIES 4.1 Dataset and Experimental Setup</head><p>We evaluate our proposed XGNN on both synthetic and real-world datasets. We report the summary statistics of these datasets in Table <ref type="table" target="#tab_4">1</ref>. Since there is no existing work investigating model-level interpretations of GNNs, we have no baseline to compare with. Note that existing studies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b39">40]</ref> only focus on interpreting GNNs at example-level while ignoring the model-level explanations. Comparing with them is not expected since these example-level and model-level are two totally different interpretation directions. Synthetic dataset: Since our XGNN generates model-level explanations for Deep GNNs, we build a synthetic dataset, known as Is_Acyclic, where the ground truth explanations are available. The graphs are labeled based on if there is any cycle existing in the graph. The graphs are obtained using Networkx software package <ref type="bibr" target="#b14">[15]</ref>. The first class refers to cyclic graphs, including grid-like graphs, cycle graphs, wheel graphs, and circular ladder graphs. The second class denotes acyclic graphs, containing star-like graphs, binary tree graphs, path graphs and full rary tree graphs <ref type="bibr" target="#b33">[34]</ref>. Note that all nodes in this dataset are unlabeled and we focus on investigating the ability of GNNs to capture graph structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real-world dataset:</head><p>We conduct experiments on the real-world The graphs are labeled into two different classes according to their mutagenic effect on a bacterium <ref type="bibr" target="#b5">[6]</ref>. Each node is labeled based on its type of atom and there are seven possible atom types: Carbon, Nitrogen, Oxygen, Fluorine, Iodine, Chlorine, Bromine. Note that the edge labels are ignored for simplicity. For this dataset, we investigate the ability of GNNs to capture both graph structures and node labels.</p><p>Graph classification models: We train graph classification models using these datasets and then try to explain these models. These models share a similar pipeline that first learns node features using multiple layers of GCNs, then obtain graph level embeddings by averaging all node features, and finally employs fully-connected layers to perform graph classification. For the synthetic dataset Is_Acyclic, we use the node degrees as the initial features for all nodes. Then we apply two layers of GCNs with output dimensions equal to 8, 16 respectively and perform global averaging to obtain the graph representations. Finally, we employ one fully-connected layer as the classifier. Meanwhile, for the real-world dataset MU-TAG, since all nodes are labeled, we employ the corresponding one-hot representations as the initial node features. Then we employ three layers of GCNs with output dimensions equal to 32, 48, 64 respectively and average all node features. The final classifier contains two fully-connected layers in which the hidden dimension is set to 32. Note that for all GCN layers, we apply the GCN version shown in Equation <ref type="bibr" target="#b0">(1)</ref>. In addition, we employ Sigmoid as the nonlinear function in GCNs for dataset Is_Acyclic while we use Relu for dataset MUTAG. These models are implemented using Pytorch <ref type="bibr" target="#b26">[27]</ref> and trained using Adam optimizer <ref type="bibr" target="#b17">[18]</ref>. The training accuracies of these models are reported in Table <ref type="table" target="#tab_4">1</ref>, which show that the models we try to interpret are models with reasonable performance.</p><p>Graph generators: For both datasets, our graph generators share the same structure. Our generator first employs a fully-connected layer to map node features to the dimension of 8. Then three layers of GCNs are employed with output dimensions equal to 16, 24, 32 respectively. The first MLPs consist of two fully-connected layers with the hidden dimension equal to 16 and a ReLU6 non-linear function. The second MLPs also have two fully-connected layers that the hidden dimension is set to 24 and ReLU6 is applied. The initial features for input graphs are the same as mentioned above.</p><p>For dataset Is_Acyclic, we set λ 1 = 1, λ 2 = 1, and R t,r = −1 if the generated graph violates any graph rule. For dataset MUTAG, we set λ 1 = 1, λ 2 = 2, and the total reward R t = −1 if the generated graph violates any graph rule. In addition, we perform rollout m = 10 times each step to obtain final graph rewards. The models are implemented using Pytorch <ref type="bibr" target="#b26">[27]</ref> and trained using Adam optimizer <ref type="bibr" target="#b17">[18]</ref> with β 1 = 0.9 and β 2 = 0.999.  In each row, from left to right, we report the generated graphs with increasing maximum node number limits. In addition, we feed each generated graph to the pre-trained GCNs and report the predicted probability for the corresponding class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results on Synthetic Data</head><p>We first conduct experiments on the synthetic dataset Is_Acyclic where the ground truth is available. As shown in Table <ref type="table" target="#tab_4">1</ref>, the trained GNN classifier can reach a promising performance. Since the dataset is manually and synthetically built based on if the graph contains any circle, we can check if the trained GNN classifier makes predictions in such a way. We explain the model with our proposed XGNN and report the generated interpretations in Figure <ref type="figure" target="#fig_1">3</ref>. We show the explanations for the class "cyclic" in the first row and the results for the class "acyclic" in the second row. In addition, we also report different generated explanations by setting different maximum graph node limits. First, by comparing the graphs generated for different classes, we can easily conclude the difference that the explanations for the class "cyclic" always contain circles while the results for the class "acyclic" have no circle at all. Second, to verify whether our explanations can maximize the class probability for a certain class, as shown in Equation ( <ref type="formula" target="#formula_1">2</ref>), we feed each generated graph to the trained GNN classifier and report the predicted probability for the corresponding class. The results show that our generated graph patterns can consistently yield high predicted probabilities. Note that even though the graph obtained for the class "cyclic" with maximum node number equal to 3 only leads to p = 0.7544, it is still the highest probability for all possible graphs with 3 nodes. Finally, based on these results, we can understand what patterns can maximize the predicted probabilities for different classes. In our results, we know the trained GNN classifier very likely distinguishes different classes by detecting circular structures, which is consistent with our expectations. Hence, such explanations help understand and trust the model, and increase the trustworthiness of this model to be used as a circular graph detector. In addition, it is noteworthy that our generated graphs are easier to analyze compared with the graphs in the datasets. Our generated graphs have significantly fewer numbers of nodes and simpler structures, and yield higher predicted probabilities while the graphs from the dataset have an average of 28 nodes and 30 edges, as shown in Table <ref type="table" target="#tab_4">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results on Real-World Data</head><p>We also evaluate our proposed XGNN using real-world data. For dataset MUTAG, there is no ground truth for the interpretations. Since all nodes are labeled as different types of atoms, we investigate whether the trained GNN classifier can capture both graph structures and node labels. We interpret the trained GNN with our proposed method and report selected results in Figure <ref type="figure" target="#fig_2">4</ref> and Figure <ref type="figure" target="#fig_4">5</ref>. Note that the generated graphs may not represent real chemical compounds because, for simplicity, we only incorporate a simple chemical rule that the degree of an atom cannot exceed its maximum chemical valency. In addition, since nodes are labeled, we can set the initial graphs as different types of atoms.</p><p>We first set the initial graph as a single carbon atom and report the results in Figure <ref type="figure" target="#fig_2">4</ref>, since generally, any organic compound contains carbon <ref type="bibr" target="#b27">[28]</ref>. The first row reports explanations for the class "non-mutagenic" while the second row shows the results for the class "mutagenic". We report the generated graphs with different node limits and the GNN predicted probabilities. For the class "mutagenic", we can observe that carbon circles and NO 2 are some common patterns, and this is consistent with the chemical fact that carbon rings and NO 2 chemical groups are mutagenic <ref type="bibr" target="#b5">[6]</ref>. Such observations indicate that the trained GNN classifier may capture these key graph patterns to make predictions. In addition, for the class "non-mutagenic", we observe the atom Chlorine is widely existing in the generated graphs and the combination of Chlorine, Bromine, and Fluorine always leads to "non-mutagenic" predictions. By analyzing such explanations, we can better understand the trained GNN model.</p><p>We also explore different initial graphs and report the results in Figure <ref type="figure" target="#fig_4">5</ref>. We fix the maximum node limit as 5 and generate explanations for the class "mutagenic". First, no matter how we set the initial graph, our proposed method can always find graph   We fix the maximum node number limit as 5 and explore different initial graphs. Note that all graphs are generated for explaining the mutagenic class. For each generated graph, we show its predicted probability and corresponding initial graph at the bottom.</p><p>patterns maximizing the predicted probability of class "mutagenic".</p><p>For the first 5 graphs, which means the initial graph is set to a single node of Carbon, Nitrogen, Oxygen, Iodine, or Fluorine, some generated graphs still have common patterns like carbon circle and NO 2 chemical groups. Our observations further confirm that these key patterns are captured by the trained GNNs. In addition, we notice that the generator can still produce graphs with Chlorine which are predicted as "mutagenic", which is contrary to our conclusion above. If all graphs with Chlorine should be identified as "non-mutagenic", such explanations show the limitations of trained GNNs. Then these generated explanations can provide guidance for improving the trained GNNs, for example, we may place more emphasis on the graphs Chlorine when training the GNNs. Furthermore, the generated explanations may also be used to retrain and improve the GNN models to correctly capture our desired patterns. Overall, the experimental results show that our proposed interpretation method XGNN can help verify, understand, and even help improve the trained GNN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>Graphs neural networks are widely studied recently and have shown great performance for multiple graph tasks. However, graph models are still treated as black-boxes and hence cannot be fully trustable. It raises the need of investigating the interpretation techniques for graph neural networks. It is still a less explored area where existing methods only focus on example-level explanations for graph models. However, none of the existing work investigates the model-level interpretations of graph models which is more general and high-level. Hence, in this work, we propose a novel method, XGNN, to interpret graph models in the model-level. Specifically, we propose to find graph patterns that can maximize a certain prediction via graph generation. We formulate it as a reinforcement learning problem and generate graph pattern iteratively. We train a graph generator and for each step, it predicts how to add an edge into the current graph. In addition, we incorporate several graph rules to encourage the generated graphs to be valid and humanintelligible. Finally, we conduct experiments on both synthetic and real-world datasets to demonstrate the effectiveness of our proposed XGNN. Experimental results show that the generated graphs help discover what patterns will maximize a certain prediction of the trained GNNs. The generated explanations help verify and better understand if the trained GNNs make a prediction in our expected way. Furthermore, our results also show that the generated explanations can help improve the trained models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Formally, let f (•) denote a trained GNN classification model, and y ∈ {c 1 , • • • , c ℓ } denote the classification prediction. Given f (•) and a chosen class c i , i ∈ {1, • • • , ℓ}, our goal is to investigate what input graph patterns maximize the predicted probability for this class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Experimental results for the synthetic dataset Is_Acyclic. Each row shows our explanations for a certain class that the first row corresponds to the class cyclic while the second row explains the class acyclic.In each row, from left to right, we report the generated graphs with increasing maximum node number limits. In addition, we feed each generated graph to the pre-trained GCNs and report the predicted probability for the corresponding class.</figDesc><graphic url="image-7.png" coords="7,354.40,171.32,103.21,68.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Experimental results for the MUTAG dataset. The first row reports the explanations for the class non-mutagenic while the second row shows results for the class mutagenic. Note that different node colors denote different types of atoms and the legend is shown at the bottom of the figure. All graphs are generated with the initial graph as a single Carbon atom.</figDesc><graphic url="image-11.png" coords="8,57.83,169.93,99.67,66.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Experimental results for the MUTAG dataset. We fix the maximum node number limit as 5 and explore different initial graphs. Note that all graphs are generated for explaining the mutagenic class. For each generated graph, we show its predicted probability and corresponding initial graph at the bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Illustrations of our proposed XGNN for graph interpretation via graph generation. The GNNs represent a trained graph classification model that we try to explain. All graph examples in the graph set are classified to the third class. The left part shows that we can manually conclude the key graph patterns for the third class but it is challenging. The right part shows that we propose to train a graph generator to generate graphs that can maximize the class score and be valid according to graph rules.</figDesc><table><row><cell>Graph set</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Prediction</cell><cell></cell></row><row><cell></cell><cell cols="2">GNNs</cell><cell></cell><cell>Maximize</cell></row><row><cell>Human observation</cell><cell></cell><cell></cell><cell>Rewards</cell><cell>Graph rules</cell></row><row><cell>Step 0</cell><cell>Step 1</cell><cell>Step 2</cell><cell>Step 3</cell><cell>Valid</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Generated</cell></row><row><cell></cell><cell cols="2">Graph generator</cell><cell></cell><cell>graph</cell></row><row><cell>Figure 1: prediction into a combination of its inputs. In addition, both of</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>these studies generate input-dependent explanations for individual</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>examples. To verify and understand a deep model, humans need to check explanations for all examples, which is time-consuming or even not feasible.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>by these methods, we propose to train a graph generator which generates G * step by step. For each step, the graph generator generates a new graph based on the current graph. Formally, we define the partially generated graph at step t as G t , which contains n t nodes. It is represented as a feature matrix X</figDesc><table /><note>t ∈ R n t ×d and an adjacency matrix A t ∈ {0, 1} n t ×n t , assuming each node has a d-dimensional feature vector. Then we define a θ -parameterized graph generator as д θ (•), which takes G t as input, and outputs a new graph</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>a t,st ar t ∼ p t,st ar t • m t,st ar t , An Illustration of our graph generator for processing a single step. Different colors denote different types of node. Given a graph with 4 nodes and a candidate set with 3 nodes, we first combine them together to obtain the feature matrix and the adjacency matrix. Then we employ several GCN layers to aggregate and learn node features. Next, the first MLPs predict a probability distribution from which we sample the starting node. Finally, the second MLPs predict the ending node conditioned on the starting node. Note that the black crosses indicates masking out nodes.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Node Features</cell><cell>Starting Node</cell><cell></cell><cell></cell></row><row><cell cols="2">Current Graph</cell><cell></cell><cell></cell><cell></cell><cell>Ending Node</cell><cell></cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4</cell><cell>3</cell><cell>Combine</cell><cell>GCNs</cell><cell>MLPs</cell><cell>MLPs</cell><cell>1 New Graph</cell></row><row><cell cols="2">Candidates</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell>5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell>3</cell></row><row><cell>Figure 2:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(6)</cell></row></table><note>where • means element-wise product and m t,st ar t is to mask out all candidate nodes since the starting node can be only selected from the current graph G t . Let x st ar t denote the features of the node selected by the start action a t,st ar t . Then conditioned on the selected node, we employ the second MLPs to compute the probability distribution of the ending node p t,end from which we sample the ending node action a t,end . Note that since the starting node and the ending node cannot be the same, we apply a mask m t,end to mask out the node selected by a t,st ar t . Mathematically, it can be written as p t,end = Softmax(MLPs([ X , x st ar t ])), (7) a t,end ∼ p t,end • m t,end ,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>7 :</head><label>7</label><figDesc>Obtain the new graph G t +1 based on a t . +1 with Equation (10-12) and obtain R t . if R t &lt; 0 then roll back and set G t +1 = G t .</figDesc><table><row><cell>8: Evaluate G t 9: Update the generator д(•) with Equation (9).</cell></row><row><cell>10:</cell></row></table><note>11:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Statistics and properties of datasets. Note that the edge number and node number are averaged numbers.</figDesc><table><row><cell>Dataset</cell><cell cols="4">Classes # of Edges # of Nodes Accuracy</cell></row><row><cell>Is_Acyclic</cell><cell>2</cell><cell>30.04</cell><cell>28.46</cell><cell>0.978</cell></row><row><cell>MUTAG</cell><cell>2</cell><cell>19.79</cell><cell>17.93</cell><cell>0.963</cell></row><row><cell cols="5">dataset MUTAG. The MUTAG dataset contains graphs represent-</cell></row><row><cell cols="5">ing chemical compounds where nodes represent different atoms</cell></row><row><cell cols="3">and edges represent chemical bonds.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>The learning rate for graph generator training is set to 0.01.</figDesc><table><row><cell>Max node: 3</cell><cell>Max node: 4</cell><cell>Max node: 5</cell><cell>Max node: 6</cell><cell>Max node: 7</cell></row><row><cell>p=0.7544</cell><cell>p=0.9993</cell><cell>p=0.9992</cell><cell>p=0.9998</cell><cell>p=0.9999</cell></row><row><cell>p=0.9999</cell><cell>p=0.9985</cell><cell>p=0.9965</cell><cell>p=0.9760</cell><cell>p=0.9634</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported in part by National Science Foundation grants DBI-2028361, IIS-1714741, IIS-1715940, IIS-1845081, IIS-1900990 and Defense Advanced Research Projects Agency grant N66001-17-2-4031.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An introduction to systems biology: design principles of biological circuits</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Chapman and Hall/CRC</publisher>
			<pubPlace>Uri Alon</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Network motifs: theory and experimental approaches</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Genetics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">450</biblScope>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Explainability Techniques for Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Baldassarre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Azizpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML) Workshops, 2019 Workshop on Learning and Reasoning with Graph-Structured Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Real time image saliency for black box classifiers</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dabkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6967" to="6976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity</title>
		<author>
			<persName><forename type="first">Asim</forename><surname>Kumar Debnath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosa</forename><forename type="middle">L</forename><surname>Lopez De Compadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gargi</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">J</forename><surname>Shusterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corwin</forename><surname>Hansch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="786" to="797" />
			<date type="published" when="1991">1991. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Towards a rigorous science of interpretable machine learning</title>
		<author>
			<persName><forename type="first">Finale</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Velez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08608</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Visualizing higher-layer features of a deep network</title>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Montreal 1341</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Interpretable explanations of black boxes by meaningful perturbation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ruth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3429" to="3437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph representation learning via hard and channel-wise attention networks</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="741" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph U-Net</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Review and comparison of methods to study the contribution of variables in artificial neural network models</title>
		<author>
			<persName><forename type="first">Muriel</forename><surname>Gevrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Dimopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sovan</forename><surname>Lek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecological modelling</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="249" to="264" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Objective-reinforced generative adversarial networks (ORGAN) for sequence generation models</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Lima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guimaraes</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Sanchez-Lengeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Outeiral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><forename type="middle">Cunha</forename><surname>Farias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alán</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10843</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Exploring network structure, dynamics, and function using NetworkX</title>
		<author>
			<persName><forename type="first">Aric</forename><surname>Hagberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Swart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Chult</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Los Alamos, NM (United States</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Los Alamos National Lab.(LANL)</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Junction Tree Variational Autoencoder for Molecular Graph Generation</title>
		<author>
			<persName><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
				<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2323" to="2332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
				<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
				<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-Attention Graph Pooling</title>
		<author>
			<persName><forename type="first">Junhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3734" to="3743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rationalizing Neural Predictions</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning deep generative models of graphs</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03324</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Network motifs: simple building blocks of complex networks</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shen-Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shalev</forename><surname>Itzkovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Kashtan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Chklovskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">298</biblScope>
			<biblScope unit="page" from="824" to="827" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Plug &amp; play generative networks: Conditional iterative generation of images in latent space</title>
		<author>
			<persName><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4467" to="4477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feature visualization</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">e7</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
				<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Spencer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Seager</surname></persName>
		</author>
		<author>
			<persName><surname>Slabaugh</surname></persName>
		</author>
		<title level="m">Chemistry for today: General, organic, and biochemistry</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Cengage learning</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Grad-CAM: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ramprasaath R Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Network motifs in the transcriptional regulation network of Escherichia coli</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Shai S Shen-Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shmoolik</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Mangan</surname></persName>
		</author>
		<author>
			<persName><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature genetics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">64</biblScope>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernanda</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03825</idno>
		<title level="m">Smoothgrad: removing noise by adding noise</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<title level="m">Striving for simplicity: The all convolutional net</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">An introduction to data structures and algorithms</title>
		<author>
			<persName><forename type="first">James</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Storer</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Richard S Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satinder</forename><forename type="middle">P</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yishay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1057" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Attention-based graph neural network for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Kiran K Thekumparampil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewoong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03735</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJXMpikCZ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">GraphGAN: Graph representation learning with generative adversarial nets</title>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2508" to="2515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryGs6iA5Km" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">GNNExplainer: Generating Explanations for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="9244" to="9255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graph convolutional policy network for goal-directed molecular graph generation</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6410" to="6421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Seqgan: sequence generative adversarial nets with policy gradient</title>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2852" to="2858" />
		</imprint>
	</monogr>
	<note>AAAI-17: Thirty-First AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Interpreting Deep Models for Text Analysis via Optimization and Regularization Methods</title>
		<author>
			<persName><forename type="first">Yongjun</forename><surname>Hao Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Third AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5717" to="5724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">StructPool: Structured Graph Pooling via Conditional Random Fields</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJxg_hVtwH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An endto-end deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4438" to="4445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
