<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A New Method for the Nonlinear Transformation of Means and Covariances in Filters and Estimators</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Simon</forename><surname>Julier</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jeffrey</forename><surname>Uhlmann</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hugh</forename><forename type="middle">F</forename><surname>Durrant-Whyte</surname></persName>
						</author>
						<title level="a" type="main">A New Method for the Nonlinear Transformation of Means and Covariances in Filters and Estimators</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">474A589C2EBD74ECD2E338FE69BFD898</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Covariance matrices</term>
					<term>estimation</term>
					<term>filtering</term>
					<term>missile detection and tracking</term>
					<term>mobile robots</term>
					<term>nonlinear filters</term>
					<term>prediction methods</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes a new approach for generalizing the Kalman filter to nonlinear systems. A set of samples are used to parameterize the mean and covariance of a (not necessarily Gaussian) probability distribution. The method yields a filter that is more accurate than an extended Kalman filter (EKF) and easier to implement than an EKF or a Gauss second-order filter. Its effectiveness is demonstrated using an example.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The problem of generalizing the Kalman filter paradigm for nonlinear applications is considered. This work is motivated by the wellknown limitations of the extended Kalman filter (EKF), which simply linearizes all nonlinear models so that the traditional linear Kalman filter can be applied. Although the EKF (in its many forms) is a widely used filtering strategy <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, over 30 years of experience with it has led to a general consensus within the tracking and control community that it is difficult to implement, difficult to tune, and only reliable for systems that are almost linear on the time scale of the update intervals.</p><p>As is well known, the optimal solution to the nonlinear filtering problem is infinite dimensional <ref type="bibr" target="#b2">[3]</ref> and a large number of suboptimal approaches have been developed <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. These methods can be broadly classed as numerical Monte Carlo <ref type="bibr" target="#b5">[6]</ref> methods or analytical approximations <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b8">[9]</ref>. However, the application of these methods to high-dimensioned systems is rarely practical, and it is a testament to the conceptual simplicity of the EKF that it is still widely used.</p><p>In this paper, a new linear estimator <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref> is developed. It yields performance equivalent to the Kalman filter for linear systems, yet generalizes elegantly to nonlinear systems without the linearization steps required by the EKF. We show analytically that the expected performance of the new approach is superior to that of the EKF and, in fact, lies between those of the modified, truncated second-order filter <ref type="bibr" target="#b12">[13]</ref> and the Gaussian second-order filter <ref type="bibr" target="#b13">[14]</ref>. However, the algorithm is not restricted to Gaussian distributions. We demonstrate the performance benefits in an example application, and we argue that the ease of implementation and more accurate estimation features of the new filter recommend its use over the EKF in virtually all applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head><p>We seek the minimum-mean-squared error (MMSE) estimate of the state vector of the nonlinear discrete time system</p><p>x(k + 1) = f [x(k); u(k); v(k); k] Manuscript received <ref type="bibr">November 16, 1994</ref>; revised December 1, 1995, September 1, 1996, and June 1, 1998. Recommended by Associate Editor, V. Solo.</p><p>S. Julier is with IDAK Industries, Jefferson City, MO USA (e-mail: sjulier@idak.com).</p><p>J. Uhlmann is with the Robotics Research Group, Oxford University, U.K. H. F. Durrant-Whyte is with the Department of Mechanical and Mechatronic Engineering, Sydney, Australia (e-mail: hugh@mech.eng.usyd.edu.au).</p><p>Publisher Item Identifier S 0018-9286(00)02145-0.</p><formula xml:id="formula_0">z(k) = h[x(k); u(k); k] + w(k)<label>(1)</label></formula><p>where x(k) is the state of the system at timestep k; u(k+1) is the input vector, v(k) is the noise process caused by disturbances and modelling errors, z(k) is the observation vector, and w(k) is additive measurement noise. It is assumed that the noise vectors v(k) and w(k) are zero mean and</p><formula xml:id="formula_1">E[v(i)v T (j)] = ij Q(i) E[w(i)w T (j)] = ij R(i); 8 i; j E[v(i)w T (j)] = 0:</formula><p>The MMSE estimate of x(k) is the conditional mean. Let x(ijj) be the mean of x(i) conditioned on all of the observations up to and including time j</p><formula xml:id="formula_2">x(ijj) = E[x(i)jZ j ]</formula><p>where Z j = [z(1); 11 1; z(j)]: The covariance of this estimate is denoted P(ijj):</p><p>The Kalman filter propagates the first two moments of the distribution of x(k) recursively and has a distinctive "predictor-corrector" structure. Given an estimate x(kjk); the filter first predicts what the future state of the system will be using the process model. Ideally, the predicted quantities are</p><formula xml:id="formula_3">x(k + 1jk) = E[f [x(k); u(k); v(k); k]jZ k ] P(k + 1jk) = E[fx(k + 1) 0 x(k + 1jk)g 2 fx(k + 1) 0 x(k + 1jk)g T jZ k ]:</formula><p>The expectations can be calculated only if the distribution of x(k) conditioned on Z k is known. In general, the distribution cannot be described by a finite number of parameters and most practical systems employ an approximation of some kind. It is conventionally assumed that the distribution of x(k) is Gaussian at any time k: Two justifica- tions are made. First, only the mean and covariance need to be maintained. Second, given just the first two moments the Gaussian distribution is the entropy maximizing or least informative distribution <ref type="bibr" target="#b14">[15]</ref>.</p><p>The estimate at time k + 1 is given through updating the prediction by the linear update rule</p><formula xml:id="formula_4">x(k + 1jk + 1) = x(k + 1jk) + W(k + 1) (k + 1) P(k + 1jk + 1) = P(k + 1jk) 0 W(k + 1)P (k + 1jk) 1 W T (k + 1) (k + 1) = z(k + 1) 0 ẑ(k + 1jk) W(k + 1) = Px(k + 1jk)P 01 (k + 1jk):</formula><p>The EKF exploits the fact that the error in the prediction, x(ijj) =</p><p>x(i)0 x(ijj); can be attained by expanding (1) as a Taylor Series about the estimate x(kjk): Truncating this series at the first order yields the approximate linear expression for the propagation of state errors as</p><p>x(k + 1jk) rf x x(kjk) + rf v v(k) where rf x is the Jacobian of f <ref type="bibr" target="#b0">[1]</ref> with respect to x(k); and rf v is that with respect to v(k): Using this approximation, the state prediction equations are</p><formula xml:id="formula_5">x(k + 1jk) = f [x(kjk); u(k); 0]</formula><p>0018-9286/00$10.00 © 2000 IEEE P(k + 1jk) = rfxP(kjk)r T fx + rfvQ(k + 1)r T fv: <ref type="bibr" target="#b1">(2)</ref> This approximation is valid if the contributions of the truncated higher order terms are negligible. However, in many practical situations (such as converting from polar to Cartesian coordinate systems <ref type="bibr" target="#b16">[17]</ref>), linearization introduces significant biases or errors. The EKF can also be prohibitively difficult to implement because deriving Jacobians can be cumbersome and time consuming (especially when the system is complicated and of high order), and they must be reevaluated at every prediction step of the filter.</p><p>This paper focuses on the central problem of predicting the mean and covariance. We present an alternative algorithm that is more accurate, convenient, and uses approximately the same number of calculations as an EKF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE NEW FILTER</head><p>We use the intuition that it is easier to approximate a probability distribution than it is to approximate an arbitrary nonlinear function or transformation <ref type="bibr" target="#b17">[18]</ref>. Following this intuition, we generate a set of points whose sample mean and sample covariance are x(kjk) and P(kjk), respectively. The nonlinear function is applied to each of these points in turn to yield a transformed sample, and the predicted mean and covariance are calculated from the transformed sample.</p><p>Although this superficially resembles a Monte Carlo method, the samples are not drawn at random. Rather, the samples are deterministically chosen so that they capture specific information about the distribution. In general, this intuition can be applied to capture many kinds of information about many types of distribution <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. In this paper, we consider the special case of i) capturing the mean and covariance of an ii) assumed Gaussian distribution.</p><p>The n-dimensional random variable x(k) with mean x(kjk) and covariance P(kjk) is approximated by 2n+1 weighted samples or sigma points selected by the algorithm</p><formula xml:id="formula_6">X 0 (kjk) = x(kjk) W0 = =(n + ) Xi(kjk) = x(kjk) + (n + )P(kjk) i W i = 1=f2(n + )g X i+n (kjk) = x(kjk) 0 (n + )P(kjk) i W i+n = 1=f2(n + )g: (3)</formula><p>2 &lt;; ( (n + )P(kjk)) i is the ith row or column 1 of the matrix square root of (n + )P(kjk), and W i is the weight that is associated with the ith point.  Appendix I sketches the principle of the argument for the higher moments, and a full proof can be found in <ref type="bibr" target="#b20">[21]</ref>.</p><p>Remark 1: The above properties hold for any choice of the matrix square root. Efficient and stable methods, such as the Cholesky decomposition, should be used.</p><p>Remark 2: κ can be any number (positive or negative) providing that (n + ) 6 = 0:</p><p>Given the set of samples generated by (3), the prediction procedure is as follows.</p><p>1) Each sigma point is instantiated through the process model to yield a set of transformed samples</p><formula xml:id="formula_7">X i (k + 1jk) = f[X i (kjk); u(k); k]:</formula><p>2) The predicted mean is computed as</p><formula xml:id="formula_8">x(k + 1jk) = 2n i=0 W i X i (k + 1jk):<label>(4)</label></formula><p>3) The predicted covariance is computed as</p><formula xml:id="formula_9">P(k + 1jk) = 2n i=0 W i fX i (k + 1jk) 0 x(k + 1jk)g 2 fX i (k + 1jk) 0 x(k + 1jk)g T : (5)</formula><p>The mean and covariance are calculated using standard vector and matrix operations, which means that the algorithm is suitable for any choice of process model, and implementation is extremely convenient because it is not necessary to evaluate the Jacobians, which are needed in an EKF. The method has a further advantage: it yields more accurate predictions than those determined through linearization.</p><p>Theorem 2: The prediction algorithm introduces errors in estimating the mean and covariance at the fourth and higher orders in the Taylor series. These higher order terms are a function of κ and the matrix square root used.</p><p>Proof: Appendix II shows that evaluating the mean and covariance correctly up to the mth order requires approximating the moments of x(k) up to the mth order. From Theorem 1, the approximation is correct up to the third order, and so errors are introduced in the fourth and higher orders. These are a function of the matrix square root and κ.</p><p>Remark 3: This is the same order of accuracy as the second-order Gaussian filter <ref type="bibr" target="#b13">[14]</ref>, but without the need to calculate Jacobians or Hessians.</p><p>provides an extra degree of freedom to "fine tune" the higher order moments of the approximation, and can be used to reduce the overall prediction errors. When x(k) is assumed Gaussian, Appendix I shows that a useful heuristic is to select n + = 3: If a different distribution is assumed for x(k), then a different choice of might be more appropriate.</p><p>Although can be positive or negative, Appendix III shows that when is negative it is possible to calculate a nonpositive, semidefinite P(k + 1jk): This problem is not uncommon for methods that approximate higher order moments or probability density distributions (such as those described in <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b3">[4]</ref>, and <ref type="bibr" target="#b7">[8]</ref>). In this situation, it is possible to use a modified form of the prediction algorithm. The mean is still calculated using (4), but the "covariance" is evaluated about X0(k+1jk):</p><p>Appendix III shows that the modified form ensures positive semi-definiteness, and, in the limit (n + ) ! 0; the prediction is the same as that of the modified, truncated second-order filter <ref type="bibr" target="#b3">[4]</ref>.</p><p>Remark 4: The method can be generalized to include the effects of process and observation noise by appending the noise vectors to the state vector <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXAMPLE APPLICATION</head><p>This section applies and compares the performance of the new filter against an EKF for a tracking problem that was presented in <ref type="bibr" target="#b13">[14]</ref>. This example was chosen because it has significant nonlinearities in the process and observation models and has been analyzed extensively in the literature.</p><p>We wish to estimate the position x 1 (t); velocity x 2 (t), and constant ballistic coefficient x3(t) of a body as it reenters the atmosphere at a very high altitude at a very high velocity. Its motion is determined by altitude-and velocity-dependent drag terms, and it is constrained to fall vertically. The position of the body is measured at discrete points in time using a radar capable of measuring range corrupted by Gaussian measurement noise. The radar is at an altitude of H (100 000 ft), and the horizontal range between the body and the radar, M; is (100 000 ft).</p><p>The continuous time dynamics of this system are</p><formula xml:id="formula_10">_ x 1 (t) =0x 2 (t) + w 1 (t) (6) _ x2(t) =0e 0x (t) x2(t) 2 x3(t) + w2(t) (7) _ x 3 (t) =w 3 (t)<label>(8)</label></formula><p>where w 1 (t); w 2 (t), and w 3 (t) are zero-mean, uncorrelated noises with covariances given by Q(t) and is a constant (5 2 10 05 ) that relates the air density with altitude. The range at time t; z(t); is z(t) = (M 2 + [x 1 (t) 0 H] 2 ) + r(t);</p><p>where r(t) is the uncorrelated observation noise with covariance R(t) = 4 ft . The measurements are made with a frequency of 1</p><p>Hz.</p><p>Tracking systems were implemented using the new filter and the EKF. The nonlinearities of the process model and the high velocities required the numerical integration of ( <ref type="formula">6</ref>)-( <ref type="formula" target="#formula_10">8</ref>) to be carried out using extremely small time steps. In accordance with <ref type="bibr" target="#b13">[14]</ref>, a fourth-order Runge-Kutta scheme was employed with 64 steps between each observation. For the EKF, it was necessary to recalculate the Jacobian 64 times between each update. For the new filter, the trajectory of each sigma point was calculated using the small time steps, but it was only necessary to calculate the mean and covariance just before an observation was made. Because n = 3; was chosen to be zero in accordance with the heuristic n + = 3:</p><p>The initial true state of the system is x(0) = [3210 5 2210 4 10 03 ],</p><p>and the initial estimates and covariances of these states are </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>:</head><p>Although the initial estimates of altitude and velocity are correct, x3 (0j0) is very bad. The body is assumed to be "heavy," whereas in reality it is "light." The behavior of the two filters differs if the second and higher order terms are significant. Because process noise can be used to mask linearization errors, we adopt the practice from <ref type="bibr" target="#b13">[14]</ref> and do not introduce any process noise into the simulation-Q(k) = 0 for both filters.</p><p>In Fig. <ref type="figure" target="#fig_0">1</ref>, we show the average magnitude of the state errors committed by each filter across a Monte Carlo simulation consisting of 50  runs. At high altitude, the drag effects are minimal, and the body falls approximately linearly. However, after about 10 s, drag becomes significant, motion becomes noticeably nonlinear, and the two filters differ significantly. The velocity estimates are shown in Fig. <ref type="figure" target="#fig_4">2</ref> and indicate large error spikes in both filters. These occur when the altitude of the body is the same as that of the radar and range information provides less data about body motion.The new filter recovers quickly, but the EKF has a larger error spike and only slowly converges. Fig. <ref type="figure" target="#fig_5">3</ref> shows the errors in estimating x 3 (1): The error in the EKF estimate is biased and is an order of magnitude larger than that for the new filter.</p><p>In Figs. <ref type="figure" target="#fig_6">4</ref> and<ref type="figure" target="#fig_7">5</ref>, we show the errors in the position estimates made by the EKF and the new filter and the associated estimates of the two standard deviation bounds. These bounds are given by twice the square root of the diagonals of the covariance matrix, and, if the filter is consistent, the state errors should lie within these bounds 95% of the time. However, the error in the EKF drifts outside of these bounds after 30 s, showing that it does not yield consistent estimates. However, the errors in the new filter always lie well within the two standard deviations, implying that the new filter is consistent.  Therefore, we conclude that in this example the new filter has substantial advantages over the EKF both in implementation and performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>Motivated by the deficiencies of the EKF, we have examined a completely new approach for applying linear estimation theory to nonlinear systems. Rather than approximate the Taylor series to an arbitrary order, we approximate the first three moments of the prior distribution accurately using a set of samples. The algorithm predicts the mean and covariance accurately up to the third order and, because the higher order terms in the series are not truncated, it is possible to reduce the errors in the higher order terms as well. We have provided empirical evidence that supports the theoretical conclusion and have demonstrated that the new filter is far easier to implement because it does not involve any linearization steps, eliminating the derivation and evaluation of Jacobian matrices. This algorithm has found a number of applications in high-order, nonlinear coupled systems, including navigation systems for high-speed road <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b15">[16]</ref> vehicles, public transportation systems <ref type="bibr" target="#b21">[22]</ref>, and underwater vehicles <ref type="bibr" target="#b22">[23]</ref>. Square root filters can be formulated by discrete sets of points (as demonstrated in <ref type="bibr" target="#b23">[24]</ref>), and iterated filters can be constructed using the predictions <ref type="bibr" target="#b24">[25]</ref>. The algorithm has been extended to capture the first four moments of a Gaussian distribution <ref type="bibr" target="#b11">[12]</ref> and the first three moments of an arbitrary distribution <ref type="bibr" target="#b19">[20]</ref>.</p><p>Given its performance and implementation advantages, we conclude that the new filter should be preferred over the EKF in virtually all nonlinear estimation applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX I HIGHER MOMENTS OF THE APPROXIMATION</head><p>This appendix analyzes the properties of the higher moments of the sigma point selection scheme of (3). The selection process consists of three stages. First, a set of sigma points are drawn to approximate an n-dimensional standard Gaussian with mean 0 and covariance I: A linear transformation [a matrix square root P(kjk)] is applied to each point so that the transformed sample still has mean 0, but the covariance is P(kjk): Finally, x(kjk) is added to each transformed sigma point to ensure the correct sample mean. affects the first stage of the approximation, and the matrix square root affects the second.</p><p>The sigma points, which are assumed approximate the standard Gaussian, lie on the coordinate axes. <ref type="foot" target="#foot_0">2</ref> The orthogonality and the symmetry of these points means that the only nonzero sample moments are those that are an even order power of a single coefficient. The 2kth-order moment for any coefficient is (n + ) (k01) : Therefore, the higher order moments scale geometrically by a factor determined by . However, the moments for the standard Gaussian are different from those of the sigma points, which approximate it. Because the covariance matrix is I, the different components are independent of one another and</p><formula xml:id="formula_12">E[x 1 2 1 1 1 2 x n ] = n i=1 E[x i ]</formula><p>where the ith component in the moment is raised to the power i: It can be shown that <ref type="bibr" target="#b12">[13]</ref> E[x i ] = 0;</p><p>i is odd:</p><p>1 2 3 2 111 2 (i 0 1); i is even:</p><p>Two differences exist in the approximation by the sigma points. First, the "scaling" of nonzero moments are different. Second, a number of moment terms are "missed out." These differences become more marked as the order increases, but, as explained in Appendix II, they only affect the higher order terms of the Taylor Series of the process model/nonlinear function. Assuming that the contribution of terms diminishes as the order increases, the emphasis is to minimize the errors in the lowest order terms. Because E[x 4 i ] = 3 for the Gaussian distribution, choosing (n + ) = 3 minimizes the difference between the moments of the standard Gaussian and the sigma points up to the fourth order.</p><p>Because the fourth and higher order moments are not precisely matched, the choice of matrix square root affects the errors in the higher order terms by adjusting the way in which the errors are distributed among the different states. However, in general, this information cannot be exploited because it would require knowledge of the higher order derivatives of the process model equation. In the absence of this information, the choice of matrix square root is governed by other issues, such as numerical stability or computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX II MOMENT APPROXIMATION AND PERFORMANCE</head><p>In this appendix we provide a justification for approximating a probability distribution by a set of samples that match its moments. We show that an approximation is only correct to the mth order if its moments up to that order are correctly approximated.</p><p>Consider a Gaussian-distributred random variable x with mean x and covariance Pxx: We wish to calculate the mean y and covariance P yy of the random variable y, which is related to x through the nonlinear (analytic) function y = f[x]:</p><p>Noting that x can be written as x = x + x, where x is a zero-mean Gaussian random variable with covariance P xx ; the nonlinear transformation can be expanded as a Taylor Series about x y</p><formula xml:id="formula_13">= f[x + x] = f[x] + D 1x f + D 2 1x f 2! + D 3 1x f 3! + D 4 1x f 4!</formula><p>+ 111 (10)   where the D 1x f operator evaluates the total differential of f <ref type="bibr" target="#b0">[1]</ref> when perturbed around a nominal value x by x: The ith term in the Taylor series for f <ref type="bibr" target="#b0">[1]</ref> is given by D</p><formula xml:id="formula_14">i 1x f i! = 1 i! n j=1 xj @ @x j i f[x] x=X (<label>11</label></formula><formula xml:id="formula_15">)</formula><p>where xj is the jth component of x: Therefore, the ith term in the series is an ith-order polynomial in the coefficients of x; whose coefficients are given by derivatives of f[1]: y is the expected value of (10)</p><formula xml:id="formula_16">y = E[f[x + x]] = f[x] + E D1xf + D 2 1x f 2! + D 3 1x f 3! + D 4 1x f 4!</formula><p>+ 1 11 (12)   where the ith term in the series is given by</p><formula xml:id="formula_17">E D i 1x f i! = 1 i! E n j=1 xj @ @xj i f[x] = 1 i! m 111111 @ i f @x i 1 + m 111112 @ i f @x i01 1 @x2 + 111 (13)</formula><p>and mc c 111c is the ith-order moment mc c 111c = E[xc xc 111 xc ]:</p><p>The ith term is a function of the ith central moments of the distribution of x: Therefore, for an approximation to be accurate up to the ith order, it must be able to approximate the moments correctly up to the ith order. The EKF truncates this series after the first term, and so its error in predicting the mean is in the second and higher orders. The new filter matches the mean and covariance correctly, and so it is correct up to the second order. Because the new filter does not truncate the filter at any order, it can be expected that the errors in the fourth and higher order terms are smaller than those committed by the EKF.</p><p>The covariance is given by P yy = E[fy 0 ygfy 0 yg T ]: Now</p><formula xml:id="formula_18">y 0 y = f[x + x] 0 E[f[x + x]] = D1xf + D 2 1x f 2! + D 3 1x f 3! + D 4 1x f 4! 0 E D 2 1x f 2! + D 4 1x f 4! + 11 1<label>(14)</label></formula><p>with substitutions from ( <ref type="formula">10</ref>) and ( <ref type="formula">12</ref>). The true covariance is found by postmultiplying the state error by the transpose of itself and taking expectations. Exploiting the fact that x is symmetric, the odd terms all evaluate to zero and the covariance is given by P yy = rf x P xx rf T where E[D 1x f(D 1x f) T ] = rf x P xx rf T</p><p>x has been used. Comparing this with (2), it can be seen that the EKF truncates this series after the first term. The new filter does not truncate the series at any arbitrary order, and, applying the analysis from above, it is correct up to the second order with errors in the fourth and higher order terms. Therefore, both the EKF and the new filter predict the covariance correctly up to the third order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX III THE MODIFIED FORM OF THE ALGORITHM</head><p>When is negative, it is possible that the predicted covariance will not be positive semidefinite. This can be demonstrated by taking the limit of ( <ref type="formula">15</ref>) as (n + ) ! 0: As shown in Appendix I, the fourth and higher order moments tend to zero and It can be seen that a fourth-order, positive-semidefinite matrix is subtracted. However, this term originates from the outer product of two second-order expected terms and does not scale with .</p><p>The modified form of the algorithm evaluates the covariance about the projected mean. Taking the sigma points for x are Xi and those for y are Y i ; the modified form for calculating P yy x :</p><p>These are the values calculated by the modified, truncated secondorder filter <ref type="bibr" target="#b3">[4]</ref>, but without the need to evaluate Jacobians or Hessians. <ref type="foot" target="#foot_2">4</ref></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Theorem 1 :1</head><label>1</label><figDesc>The set of samples chosen by<ref type="bibr" target="#b2">(3)</ref> have the same sample mean, covariance, and all higher odd-ordered central moments as the distribution of x(k): The matrix square root and κ affect the fourth and higher order sample moments of the sigma points.Proof: The matching of the mean, covariance, and all odd-ordered moments can be readily demonstrated. Because the points are symmetrically distributed and chosen with equal weights about x; the sample mean is obviously x and all odd-ordered moments are zero. The sample covariance P is P = 2n i=0 WifXi(kjk) 0 x(kjk)gfXi(kjk) 0 x(kjk)g T = n i=1 2W i (n + ) P(kjk) i P(kjk) T i If the matrix square root A of P is of the form P = A A, then the sigma points are formed from the rows of A: However, for a root of the form P = AA , the columns of A are used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Absolute mean error position error.</figDesc><graphic coords="3,301.14,62.28,257.04,207.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Absolute mean velocity error.</figDesc><graphic coords="3,301.14,305.10,254.88,207.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Absolute mean error in x :</figDesc><graphic coords="4,39.60,62.28,256.08,207.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. EKF mean position error.</figDesc><graphic coords="4,39.60,310.38,255.12,207.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. New filter mean position error.</figDesc><graphic coords="4,303.90,62.28,248.64,207.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>is 3 P</head><label>3</label><figDesc>MOD yy = 2n i=1 Wi[Yi 0 Y0][Yi 0 Y0] T :Positive semidefiniteness is guaranteed by the fact that the covariance matrix is evaluated as the sum of outer products of vectors. It can be shown that the calculated covariance is P yy = rf x P xx rf T in the fourth and higher orders. The modified form has the advantage that, irrespective of the choice of the matrix square root lim (n+)!0 y = f[x] + E D</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>In effect, these points are drawn from the orthogonal matrix square root of I:</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>This is equivalent to calculating the mean and covariance using (4) and (5) and adding a term fy = Y gfy 0 Y g :</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>This can be contrasted with an alternative approach of the initial intuition, which was explored in<ref type="bibr" target="#b25">[26]</ref>. Under that scheme, no copies of the previously estimated mean are included in the sample set and the sigma points are scaled using a parameter . In the limit, as tends to infinity, this algorithm predicts the same mean and covariance as the EKF. However, when = 1; this method estimates the same mean and covariance as the new filter, but with = 0:</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank R. Bellaire at Georgia Tech for insightful comments on an early draft of this paper and the referees for their many constructive remarks.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>An</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Algorithms for multiple target tracking</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Uhlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. Sci</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="128" to="141" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Sorenson</surname></persName>
		</author>
		<title level="m">Kalman Filtering: Theory and Application</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dynamical equations for optimum non-linear filtering</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kushner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Differential Equations</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="179" to="190" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Maybeck</surname></persName>
		</author>
		<title level="m">Stochastic Models, Estimation, and Control</title>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Maybeck</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="1982">1982</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Jazwinski</surname></persName>
		</author>
		<title level="m">Stochastic Processes and Filtering Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Novel approach to nonlinear/non-Gaussian Bayesian state estimation</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Salmond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F M</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Inst. Elect. Eng. F</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="113" />
			<date type="published" when="1993-04">Apr. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Approximations to optimal nonlinear filters</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kushner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Contr</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="546" to="556" />
			<date type="published" when="1967-10">Oct. 1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Non-linear filtering approximation of a posteriori density</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Sorenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Contr</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="51" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">New exact nonlinear filters</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Daum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bayesian Analysis of Time Series and Dynamic Models</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Spall</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Marcel Dekker</publisher>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="199" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A new approach for filtering nonlinear systems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Julier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Uhlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Durrant-Whyte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Am. Contr. Conf</title>
		<meeting>Am. Contr. Conf<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="1628" to="1632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A general method for approximating nonlinear transformations of probability distributions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Julier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Uhlmann</surname></persName>
		</author>
		<ptr target="http://www.robots.ox.ac.uk/~siju" />
		<imprint>
			<date type="published" when="1994-08">1994. Aug</date>
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A new extension of the Kalman filter to nonlinear systems</title>
	</analytic>
	<monogr>
		<title level="m">Proc. AeroSense: 11th Int. Symp. Aerosp./Defense Sensing</title>
		<meeting>AeroSense: 11th Int. Symp. Aerosp./Defense Sensing<address><addrLine>Orlando, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Maybeck</surname></persName>
		</author>
		<title level="m">Stochastic Models, Estimation, and Control</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Acdemic</publisher>
			<date type="published" when="1979">1979</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Suboptimal state estimation for continuous-time nonlinear systems from discrete noisy measurements</title>
		<author>
			<persName><forename type="first">M</forename><surname>Athans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Wishner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bertolini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Contr</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="504" to="518" />
			<date type="published" when="1968-10">Oct. 1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Estimation, control and the discrete Kalman filter</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Catlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Applied Mathematical Sciences</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">84</biblScope>
			<date type="published" when="1989">1989</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Autonomous land vehicle navigation using millimetre wave radar</title>
		<author>
			<persName><forename type="first">S</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>University of Sydney, Australia</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tracking with debiased consistent converted measurements vs. EKF</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lerro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Bar-Shalom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Aerosp. Electron. Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1015" to="1022" />
			<date type="published" when="1993-07">July 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Simultaneous map building and localization for real time applications</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Uhlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Univ. Oxford, U.K., Tech. Rep</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A consistent, debiased method for converting between polar and Cartesian coordinate systems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Julier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Uhlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AeroSense: 11th Int. Symp. Aerosp./Defense Sensing</title>
		<meeting>AeroSense: 11th Int. Symp. Aerosp./Defense Sensing<address><addrLine>Orlando, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A skewed approach to filtering</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Julier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AeroSense: 12th Int. Symp. Aerosp./Defense Sensing</title>
		<meeting>AeroSense: 12th Int. Symp. Aerosp./Defense Sensing<address><addrLine>Orlando, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Comprehensive process models for high-speed navigation</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<pubPlace>U.K.</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Univ. Oxford</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Sperimentazione ed affinamento di un localizzatore</title>
		<author>
			<persName><forename type="first">A</forename><surname>Montobbio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<pubPlace>Torino, Italy</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">B.S. thesis</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Navigation of an underwater remote operated vehicle</title>
		<author>
			<persName><forename type="first">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Univ. Oxford, Tech. Rep</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A finite difference approach to linearization in nonlinear estimation algorithms</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Schei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Am. Contr. Conf</title>
		<meeting>Am. Contr. Conf<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="114" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A new nonlinear iterated filter with applications to target tracking</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Bellaire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Kamen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Zabin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AeroSense: 8th Int. Symp. Aerosp./Defense Sensing</title>
		<meeting>AeroSense: 8th Int. Symp. Aerosp./Defense Sensing<address><addrLine>Orlando, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">2561</biblScope>
			<biblScope unit="page" from="240" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Implicit Jacobians for linearized state estimation in nonlinear systems</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Quine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Uhlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Durrant-Whyte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Amer. Contr. Conf</title>
		<meeting>Amer. Contr. Conf<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="1645" to="1646" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
