<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">c o m p u t e r m e t h o d s a n d p r o g r a m s i n b i o m e d i c i n e 1 0 7 ( 2 0 1 2 ) 497-512</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Christoph</forename><surname>Bergmeir</surname></persName>
							<email>c.bergmeir@decsai.ugr.es</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Artificial Intelligence, E.T.S. de Ingenierías Informática y de Telecomunicación</orgName>
								<orgName type="institution">University of Granada</orgName>
								<address>
									<postCode>18071</postCode>
									<settlement>Granada</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Miguel</forename><forename type="middle">García</forename><surname>Silvente</surname></persName>
							<email>m.garcia-silvente@decsai.ugr.es</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Artificial Intelligence, E.T.S. de Ingenierías Informática y de Telecomunicación</orgName>
								<orgName type="institution">University of Granada</orgName>
								<address>
									<postCode>18071</postCode>
									<settlement>Granada</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">José</forename><forename type="middle">Manuel</forename><surname>Benítez</surname></persName>
							<email>j.m.benitez@decsai.ugr.es</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Artificial Intelligence, E.T.S. de Ingenierías Informática y de Telecomunicación</orgName>
								<orgName type="institution">University of Granada</orgName>
								<address>
									<postCode>18071</postCode>
									<settlement>Granada</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">c o m p u t e r m e t h o d s a n d p r o g r a m s i n b i o m e d i c i n e 1 0 7 ( 2 0 1 2 ) 497-512</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BDB4E30B53A3FBACC4645A6A5C633D81</idno>
					<idno type="DOI">10.1016/j.cmpb.2011.09.017</idno>
					<note type="submission">Received 15 September 2010 Received in revised form 17 August 2011 Accepted 11 September 2011</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Cervical cell imaging Nucleus segmentation High-resolution microscopic imaging</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In order to automate cervical cancer screening tests, one of the most important and longstanding challenges is the segmentation of cell nuclei in the stained specimens. Though nuclei of isolated cells in high-quality acquisitions often are easy to segment, the problem lies in the segmentation of large numbers of nuclei with various characteristics under differing acquisition conditions in high-resolution scans of the complete microscope slides. We implemented a system that enables processing of full resolution images, and proposes a new algorithm for segmenting the nuclei under adequate control of the expert user. The system can work automatically or interactively guided, to allow for segmentation within the whole range of slide and image characteristics. It facilitates data storage and interaction of technical and medical experts, especially with its web-based architecture. The proposed algorithm localizes cell nuclei using a voting scheme and prior knowledge, before it determines the exact shape of the nuclei by means of an elastic segmentation algorithm. After noise removal with a mean-shift and a median filtering takes place, edges are extracted with a Canny edge detection algorithm. Motivated by the observation that cell nuclei are surrounded by cytoplasm and their shape is roughly elliptical, edges adjacent to the background are removed.</p><p>A randomized Hough transform for ellipses finds candidate nuclei, which are then processed by a level set algorithm. The algorithm is tested and compared to other algorithms on a database containing 207 images acquired from two different microscope slides, with promising results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Within the struggle against cervical cancer, well organized screening with high coverage plays an important role, as it can significantly reduce the number of both new cases and deaths <ref type="bibr" target="#b0">[1]</ref>. The most common screening procedure is the Papanicolaou (Pap) smear, for which cells are gathered in the cervix supervision of a pathologist. A main part of the diagnostic process normally consists of categorizing the slide according to the Bethesda 2001 system <ref type="bibr" target="#b0">[1]</ref>. Many of the anomalies a cytotechnician looks for and bases this categorization on are characteristics of the cell nuclei (i.e. their shape, color, size, proportion to cytoplasm, etc.). So, an automatic analysis system of cervical cell images has to perform a process of segmentation, feature extraction, classification, validation, and error management.</p><p>Many literature can be found on feature extraction and classification of cervical cell images <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>, but without the ability of segmenting the nuclei in a robust and accurate way, automation of the whole process is not possible <ref type="bibr" target="#b9">[10]</ref>.</p><p>The slides typically contain thousands of cells. They are scanned with a maximum magnification level of 40×, which results in large images with a size around 80,000 pixels in each dimension. Different magnification levels are used for different tasks, such as background and overall slide quality identification, analysis of cell groupings and clusters, or determination of single cell characteristics (see Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>There are many cells where even the most basic methods (e.g. global threshold) yield satisfactory results. The difficulty lies in producing accurate results on an extremely high number of cells, under different slide conditions and with a variety of artifacts and other clinically relevant or irrelevant phenomena (see Fig. <ref type="figure">2</ref>). Also, as the staining method used during Pap tests stains both cytoplasm and nuclei, the contrast and the gradient between these two structures often are low <ref type="bibr" target="#b9">[10]</ref>.</p><p>Because of these difficulties, much of the previous works have been done on images showing individual cells, where the main task is to find the contours of the nucleus and/or the whole cell. Walker <ref type="bibr" target="#b5">[6]</ref> segments nuclei by a global threshold, followed by closing and opening operations. Bamford and Lovell <ref type="bibr" target="#b9">[10]</ref> use a dual active contour algorithm with an inner and an outer contour to segment the nuclei. Wu et al. <ref type="bibr" target="#b10">[11]</ref> present a parametric cell representation, and optimize a cost function that measures the difference between this parametric representation and the image. Mat-Isa et al. <ref type="bibr" target="#b11">[12]</ref> use a region-growing algorithm. Tsai et al. <ref type="bibr" target="#b12">[13]</ref> propose a method which uses a median filter and a bi-group enhancer for smoothing and contour strengthening, a K-means algorithm to segment the cell in the background, and maximal color difference to segment the nucleus. In similar approaches, Yang-Mao et al. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> use a trim-meaning filter, a bi-group enhancer, and gradient direction enhancing for segmentation of both the nucleus and the cytoplasm. Malm and Brun <ref type="bibr" target="#b15">[16]</ref> use anisotropic dilation for curve closing and Riemannian dilation. Lin et al. <ref type="bibr" target="#b16">[17]</ref> use a two group object enhancement technique.</p><p>By using images showing single cells the problem of localizing the nuclei is considerably simplified, as many assumptions on number and location can be made. So extension of these methods to images showing clusters of cells with an unknown amount of nuclei at unknown locations of large images is not straightforward (also see Figs. <ref type="figure" target="#fig_0">1</ref> and<ref type="figure">2</ref>). In Section 3, we evaluate basic methods such as thresholding or region growing which are not robust enough for a fully automated approach. The active contour methods bear the problem that the nuclei have to be localized before the contours can be accurately found. We also evaluate different localization approaches such as basic thresholding, adaptive thresholding, and a Hough transform for ellipses (Sections 2 and 3).</p><p>When images showing more than an individual cell are addressed, approaches exist in related fields. Mouroutis et al. <ref type="bibr" target="#b17">[18]</ref> use a compact Hough transform (CHT) to find centers of the nuclei in laryngeal tissue sections. The images are globally thresholded in order to guide the search and to reduce Fig. <ref type="figure">2</ref> -Various phenomena hindering a reliable segmentation: heavy overlap of the cells, large artifacts, air bubbles, clinically irrelevant findings. computational cost. For boundary detection, they use a maximum likelihood method that combines information on the gradient, the intensity and the relative position to the center. Other work considers blood cells. Won et al. <ref type="bibr" target="#b18">[19]</ref> use a Markov random field to improve a segmentation obtained from histogram thresholding. Furthermore, they detect and solve concavities, i.e. overlapping of cells. Dorini et al. <ref type="bibr" target="#b19">[20]</ref> use morphological operators and a scale-space toggle operator, which uses a decision rule to determine whether to apply dilation or erosion. The algorithm is iterative. Liu et al. <ref type="bibr" target="#b20">[21]</ref> use an iterative threshold method on gray level images and in the HSI color space. The contours of both nucleus and cytoplasm are determined using the morphological gradient. Li et al. <ref type="bibr" target="#b21">[22]</ref> segment nuclei using a gradient diffusion procedure, gradient flow tracking and grouping, and local adaptive thresholding. By using 3D microscopic images, their algorithm has much more input information available than with processing 2D data. Chang et al. <ref type="bibr" target="#b22">[23]</ref> use an iterative radial voting scheme, Voronoi tesselation and a level set method to segment blob-like structures. Recently, Ta et al. <ref type="bibr" target="#b23">[24]</ref> proposed a graph-based framework for segmentation of cellular images, which is used for automatic and semi-automatic segmentation of cell images from pleural and peritoneal effusions. Though these works are related to ours, due to different cell types and often different staining methods, appearance and characteristics of the nuclei are different in the images. As the staining method used within our work stains both nuclei and cytoplasm in a similar way, contrast between them is often poor. Furthermore, the cells usually are very large compared to their nuclei, and have irregular shapes with large overlapping regions. These regions and other artifacts often appear as dark structures with clear-cut borders, which makes reliable segmentation without taking the shape of the objects into account difficult.</p><p>Few works that we are aware of are directly related to our problem of segmenting cervical cell nuclei in images that show more than an individual cell. Edwards et al. <ref type="bibr" target="#b24">[25]</ref> present an approach using a mean-shift filtering and a morphological analysis. However, their aim is to use the resulting segmentation for sonification, i.e. generating acoustical guidance for the cytopathologist. They do not present an evaluation of the segmentation step. In recent work, Harandi et al. <ref type="bibr" target="#b25">[26]</ref> present a segmentation method for nuclei and cells in images of Thin-Prep slides, which works on two resolution levels in order to find regions of interest on lower resolution levels and segment these later in higher resolution images. However, they do not work with whole slide scans, and the method for choosing regions of interest does not cover e.g. artifacts and inflammation, so that parts of the problem remain unaddressed.</p><p>The algorithm and software system are designed to specifically address the following shortcomings in the stateof-the-art: (i) the full complexity of the segmentation process (artifacts, overlap) is not currently covered; (ii) no software framework with such segmentation algorithms is freely available that handles full slide scans and facilitates interaction with the expert; (iii) as such a software framework is not available, the experts have no assistance for quality estimation of the methods.</p><p>These problems are tackled by our semi-automatic, interactive, web-based software system in a robust way: It is both possible to fully automatically process the slides and to interactively choose regions for segmentation, if automatic processing fails. Later, the final and all intermediate results can be inspected and controlled visually over the internet.</p><p>In other fields of medical imaging, interactive segmentation is an established tool when larger structures are segmented <ref type="bibr" target="#b26">[27]</ref>. By the possibility of interaction, the experts can discover and treat errors in the segmentation process effectively. As a high number of objects have to be segmented within our work, corrections on the level of single cell nuclei are not feasible. So, interaction is possible by the choice of regions and algorithms, with availability of all former results of the segmentation processes as overlap to the original data.</p><p>Web-based architectures have several advantages in the research field of medical imaging, and therewith become increasingly popular as well in teleradiology and PACS systems <ref type="bibr" target="#b27">[28]</ref>, as in 2D and 3D applications <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>: deployment is as easy as possible, and updated versions of the software accounting for feedback of the experts are visible to them immediately. So, the experts can see results of their work directly, which might increase their interest in the project. Furthermore, as the program does not need to be installed, concerns regarding integrity of the expert computer systems (e.g. with respect to personalized patient data) are handled appropriately. Particularly, the system thereby can be tested easily, safely, and securely by experts not directly involved in the project yet, and cooperation with various and variable groups of experts is possible.</p><p>We presented first a more basic algorithm using adaptive thresholding and morphological operators together with an early version of the software system in <ref type="bibr" target="#b30">[31]</ref>. Within this paper, we compare the results of that algorithm with the algorithm presented here and introduce the software framework in detail. Its source code can be found at http://sci2s.ugr.es/dicits/software/CIP2.</p><p>The rest of the article is structured as follows. Section 2 presents the proposed algorithm to locate the nuclei and to find their exact boundaries. Section 3 is devoted to an experimental study performed to assess the algorithm performance. Section 4 presents the web application that facilitates administration of the image data and interaction with the expert users. Finally, Section 5 gives the conclusions and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed algorithm</head><p>Taking into account the large size of the original images, direct processing of the whole image is not feasible. The images are stored in the Aperio SVS file format (Aperio Technologies, Inc.), which is basically TIFF with different compression techniques.</p><p>In the web application presented in Section 4, regions of interest can be marked, which are extracted from the large image for processing. Currently, the regions of interest have a fixed size of 1950 pixel in each dimension. Besides the background identification, all further processing is done at the full magnification level of 40×. An overview of the algorithm is shown in Fig. <ref type="figure" target="#fig_3">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Preprocessing</head><p>The aim of the preprocessing is to identify relevant edges in the images, from which the nuclei are extracted later. For this purpose, Otsu thresholding, mean-shift filtering, median filtering and Canny edge detection are used. Where the methods require gray scale input, the images are converted as follows:</p><formula xml:id="formula_0">gr = 0.114 • b + 0.587 • g + 0.299 • r</formula><p>Here, gr is the value of the current pixel in the gray scale image, and r, g, b are the values of the current pixel in the original color image. This is a standard way to perform this kind of conversion <ref type="bibr" target="#b31">[32]</ref>, as proceeding this way ensures that the gray scale images have the same luminance as the color images <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Background identification</head><p>We apply an Otsu multiple thresholding <ref type="bibr" target="#b33">[34]</ref> (with two thresholds) on the inner region of the image at magnification level 2× (see Fig. <ref type="figure" target="#fig_1">3</ref>). The method finds a partition of the intensity values into three classes in the histogram so that the variance within each of the classes is minimized. The output are two thresholds on the intensity values, t 1 , t 2 . We assume that the first class (i.e. the interval from the minimum of intensity values to t 1 ) are the nuclei, darker parts of the cytoplasm (e.g. where cells overlap) and some kinds of artifacts. The second class are brighter parts of the cytoplasm and the third class is the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Mean-shift filtering</head><p>We apply mean-shift filtering as presented by Comaniciu and Meer <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>, which eliminates noise effectively and preserves edges in the images. It has a solid theoretical base in statistics and has already been used successfully in the field of cell image segmentation <ref type="bibr" target="#b36">[37]</ref>.</p><p>The mean-shift algorithm finds maxima of density functions. Unlike other optimization techniques that use the gradient, it makes no assumption on the underlying density distribution. It uses the Parzen window method to perform a nonparametric estimation of the density gradient. The Parzen window is iteratively moved according to the estimated density gradient until a maximum is reached (convergence of the algorithm can be shown).</p><p>For mean-shift filtering of RGB color images, this method is used in a five dimensional space (x, y, c r , c g , c b ), whereby the size of the window has to be chosen properly with respect to the different nature of spatial and color parameters.</p><p>Then, for each point in the image, the window center is initialized (in the 5D space), the mean is computed over all points lying in the window, and the window center is shifted to the mean point. This computation is continued iteratively until a local maximum is reached, i.e. the mean does not change anymore. Finally, the color values of the initialization point are substituted by the color values of the local maximum (see Fig. <ref type="figure" target="#fig_2">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3.">Finding the relevant edges</head><p>Before we apply the Canny edge detection algorithm, we perform median filtering with a mask size of 5 × 5 pixels on the mean-shift filtered images in order to smooth the curvature of the contours.</p><p>Motivated by the assumption that a nucleus is always surrounded by a cell, and therefore edges directly adjacent to the background are borders of cells or artifacts, we erode the segmentation obtained from the Otsu thresholding and use it as an image mask for the image obtained from the edge detection (see Fig. <ref type="figure" target="#fig_2">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Localizing the nuclei</head><p>Approximating cells or their nuclei using ellipses is an approach employed in biomedical image processing <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>Though the nuclei vary widely in color, intensity, texture, sharpness of the contour, and also shape, we observe that they all lie within a certain range of size. Their shape is always compact and mostly smooth, with considerable parts of their contours having an elliptic shape. These preconditions make possible the use of a method for ellipse finding.</p><p>A straightforward approach would be the use of a standard Hough transform. Within Hough transform, instances of geometric objects (such as lines, circles or ellipses) are detected in images. The objects are defined by various parameters. Each point present in the (preprocessed) input image votes for a set of points in a parameter space. As an ellipse is defined by five parameters, the parameter space is five dimensional, which leads to high computational cost <ref type="bibr" target="#b39">[40]</ref>.</p><p>Many methods for ellipse finding have been proposed in the literature, an overview is given by Hahn et al. <ref type="bibr" target="#b40">[41]</ref>. The majority of these methods are based on the randomized Hough transform (RHT) as presented by Xu et al. <ref type="bibr" target="#b41">[42]</ref>. Instead of mapping a point in the image to a set of points in a parameter space, a set of points from the image is mapped to one point in the parameter space. Randomized Hough transform renders the ellipse detection method both faster and more accurate as standard Hough transform <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43]</ref>. The application of methods using contours instead of points during ellipse detection, such as presented by Hahn et al. <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b42">43]</ref>, is hindered within our approach by the fact that it might be the case that a single contour forms part of different ellipses and other edges present in the image (e.g. borders of the cytoplasm). So we use the RHT for ellipses as presented by McLaughlin <ref type="bibr" target="#b37">[38]</ref>. To focus the search, we compute the Hough transform in regions of interest (ROIs) with a size of 3 • d max pixels in each direction. To avoid missing nuclei that happen to lie on the border of the ROIs, we overlap the regions by d max pixels (see Fig. <ref type="figure" target="#fig_4">6</ref>).</p><p>We define a maximal and a minimal diameter d max and d min of the ellipses we are looking for. These values have to be adjusted according to the magnification level.</p><p>Three points P 1 , P 2 , P 3 are randomly sampled from the image, and lines are fitted in a neighborhood (we choose a squared neighborhood of 10 pixels). Then the points T 1 , T 2 are computed as two of the intersections of these three lines, and M 1 , M 2 are computed as two of the mid points of the lines directly connecting P 1 , P 2 , P 3 , see Fig. <ref type="figure" target="#fig_5">7</ref>. The two lines passing through M 1 , T 1 , and M 2 , T 2 , respectively, intersect in the candidate center O.</p><p>Points P = (x, y) lying on an ellipse verify the equation with (x i , y i ) : = P i -O, for i = 1, 2, 3 (i.e. the ellipse is moved to the coordinate origin), the ellipse equation parameters can be computed by solving the system of equations</p><formula xml:id="formula_1">ax 2 + 2bxy + cy 2 = 1.</formula><formula xml:id="formula_2">⎛ ⎝ x 2 1 2x 1 y 1 y 2 1 x 2 2 2x 2 y 2 y 2 2 x 2 3 2x 3 y 3 y 2 3 ⎞ ⎠ ⎛ ⎝ a b c ⎞ ⎠ = ⎛ ⎝ 1 1 1 ⎞ ⎠ ,</formula><p>for which we use a singular value decomposition (SVD).</p><p>The algebraic ellipse parameters a, b, c are converted to geometric parameters r 1 , r 2 , Â (i.e. the ellipse axes and the rotation angle) with the following formulas <ref type="bibr" target="#b43">[44]</ref>:</p><formula xml:id="formula_3">r 1 = 2 (c + a) -(c -a) 2 + 4b 2 r 2 = 2 (c + a) + (c -a) 2 + 4b 2 Â = 1 2 tan -1 2b a -c .</formula><p>The chosen points and the candidate ellipse have to pass various checks before they are inserted into the accumulator. The points are discarded if one of the following conditions are met: the points do not give a solution for the ellipse equation (because of a negative argument in the square root). The points are more distant from each other than d max /2. The ellipse center lies outside of the ROI. is bigger than e. The inner product of the gradient direction at a point P i and the respective line from P i to M is negative. By checking the direction of the gradient, it is ensured that the inner pixels of the ellipse are darker than the outside.</p><p>If the ellipse passes all checks, it is added to an accumulator list. We define the amount of samples to take as a multiple m of the amount of points present in the ROI. When all samples are taken, we choose the cluster that received the maximal number of votes. The accumulator list is partitioned into clusters with the help of maximal distances on the geometric parameters of the ellipses. The maximal distances control the resolution of the accumulator. By choosing a lower resolution, the algorithm is able to also detect objects that are not exact ellipses. The list is sorted by the first ellipse parameter, i.e. the x value of the center. Then, the list is divided into sublists by the maximal distance on the first parameter. The resulting sublists are then sorted for the second parameter and so on.</p><p>Finally, an average ellipse is computed from the cluster containing most ellipses by calculating the median of each parameter in the cluster.</p><p>With the randomized Hough transform the number of votes is difficult to normalize and therefore the number of votes for an ellipse does not directly express the probability that the ellipse exists in the image. However, by taking a sufficient amount of samples from the ROI, we can assume that if there is at least one ellipse in the image, it will receive a certain minimum number of votes. Otherwise, if the ROI does not contain an ellipse, it might be the case that all candidate ellipses are discarded before insertion into the accumulator.  The number of votes an ellipse receives depends on both the amount of times voted and on the number of points present, so it has to be related to m. If the ellipse receives at least 0.15 • m votes, we use it as initialization for the level set, otherwise it is discarded. The weighting factor was estimated empirically.</p><p>Then, the accumulator is cleared, the pixels lying on the ellipse are deleted from the image, and the search starts anew. If already two ellipses were discarded, search stops in this ROI, and continues in the next ROI.</p><p>In order to compensate for the fact that larger ellipses will receive more votes it is reasonable to divide the number of votes by the ellipse perimeter. However, we omit this correction factor as it did not lead to better results in empirical tests. This might be due to the fact that we aim to detect objects that are not exact ellipses, and larger objects have more possibilities to deviate from the shape of an exact ellipse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Fitting the shape of the nucleus</head><p>With the use of a level set function, active contours can be modeled in an elegant way. The level set function is a higher dimensional function calculating the signed distance to a contour. The contours are represented implicitly by zero crossings in the level set function, so no parametric representations of the curve are needed, and all computations can be done in a fixed (higher dimensional) coordinate system. The level set function is modified using a differential equation to fit it to image features. We use a common approach for the differential equation which includes an advection term, a propagation term, and a curvature term <ref type="bibr" target="#b44">[45]</ref>. The propagation term is defined directly as the distance D of the level set to the detected edges (see Fig. <ref type="figure" target="#fig_9">10</ref>). The advection term is computed as the product of the gradient and the distance (i.e. the squared distance is minimized), and the curvature term is the mean curvature:</p><formula xml:id="formula_4">d dt = D • ∇D • ∇ + D • |∇ | + Ä • |∇ |</formula><p>The computation of D is done with the help of a distance map. By using the Hough transform, the initialization problem inherent to active contour approaches is solved. As the initial level set, we use a circle with radius 0.2 • r 2 at the position of the candidate ellipse center. Initializing the level set with a smaller circle and not with the detected ellipse has the advantage, that if the detected ellipse is too big, the level set can fit correctly to the borders of the nucleus. Furthermore, we can establish another criterion for the detected ellipse being a nucleus or not.</p><p>After performing level set segmentation, a distance measure dist between the resulting zero set and the candidate ellipse is calculated. The nucleus is finally assumed to exist, if dist is smaller than a distance dist a , or if the number of votes is bigger than 0.35 • m (weighting factor estimated empirically) and dist is smaller than dist b (see Figs. 8, 9 and 10). Many ways to compare different segmentations exist <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref>. As we are only interested in the difference between the ellipse and the output of the level set method, without considering the input image itself, a rather basic measure similar to the one proposed by Yasnoff et al. <ref type="bibr" target="#b47">[48]</ref> is used. We compute dist by dividing the squared distance of the resulting zero set and the candidate ellipse by r 2 1 + r 2 2 , in order to compensate for different ellipse perimeters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Empirical evaluation</head><p>The purpose of this section is to analyze the performance of the proposed method. To achieve this, the algorithm has to be tested on a sufficiently big dataset. The Hospital Clínico San Cecilio de Granada and the Hospital General de Ciudad Real were kind enough to supply us with full-slide scans, from which a database of ROI-images was composed using the web application presented in Section 4. The image database contains 207 images acquired from two different full-slide scans. In total, there are 549 nuclei present in the images. for the ellipse angle. The level set algorithm is stopped after 300 iterations, or if the root mean square of changes from one iteration to the next is smaller than 0.01. Fig. <ref type="figure" target="#fig_10">11</ref> shows results on edge detection. By using Canny's edge detection algorithm without further preprocessing, it is not possible to effectively eliminate noise and to keep important edges at the same time. Preprocessing the images with the mean-shift filter improves the results considerably. The median filter leads to smoother edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Used methods and parameters</head><p>Our aim is to determine the behavior of isolated components of the algorithm. Since performance measures of the subsystems are difficult to define (e.g. performance of filtering or edge detection), the whole algorithm is compared to various similar methods, in which components of the algorithm are substituted with corresponding other methods from literature:</p><p>After mean-shift and median filtering, a threshold of 100 is applied (parameters empirically estimated on the experimentation dataset). Furthermore, in a second, more sophisticated step, an adaptive threshold is applied, followed by morphological operations, so that regions of an area smaller than r 2 min or larger than r 2 max are rejected. Also, regions with roundness less than 1.35 are rejected. The roundness is computed by dividing the perimeter of a circle with the same area as the region by the regions area. The same algorithm was tested with smoothing by curvature anisotropic diffusion instead of mean-shift filtering. Furthermore, the thresholding method was extended with the used level set method. Fig. <ref type="figure" target="#fig_16">18</ref> shows some results of the comparison. We already presented the algorithm MS-Med-Ath-M in <ref type="bibr" target="#b30">[31]</ref>, together with a preliminary evaluation on a smaller image database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Qualitative result inspection</head><p>At first, the results were visually inspected. The amount of correctly detected nuclei (true positives, TP), the amount of segmentations where no nucleus was present (false positives, FP), and the amount of missed nuclei (false negatives, FN) were determined in the images. Nuclei where the border was found not to be detected correctly (e.g. where the segmentation leaked out) are counted as false negatives.</p><p>Table <ref type="table" target="#tab_0">1</ref> shows the results of the different methods, indicating positive predictive value (PPV), true positive rate (TPR), and F-measure, which is calculated as the harmonic mean of PPV and TPR:</p><formula xml:id="formula_5">PPV = TP TP + FP TPR = TP TP + FN F-measure = 2 • TPR • PPV TPR + PPV</formula><p>The use of these measures is motivated by the fact that the amount of true negatives (i.e. nuclei that are not present  in the image and were not detected by the algorithm) cannot be defined in a meaningful way. By using the harmonic mean for calculation, the F-measure is always closer to the smallest value of PPV and TPR, ensuring that both values need to be large to result in a large F-measure <ref type="bibr" target="#b48">[49]</ref>. A low PPV means that the method tends to over-segmentation, a low TPR means that many nuclei are missed.</p><p>Using the threshold after preprocessing does not lead to a robust method, as many nuclei are only partially captured, whereas other dark structures are segmented as well. The morphologically based method improves results, but also tends to segment structures that have a blob-like shape after adaptive thresholding. It also looses nuclei adjacent to other dominant structures, leading to a low TPR of 79.96%. The mean-shift filter is robust and outperforms other filtering methods such as curvature anisotropic diffusion, indicated by F-measures of 87.43%, and 84.88%. The level set initialized with a mean-shift filtered, median filtered and thresholded image does not lead to satisfactory results. The proposed method outperforms the other methods as it leads to both the least number of false positives, i.e. detections of a nucleus, where there is no nucleus present in the image, as to the least number of false negatives, i.e. missed nuclei. This is shown by high values for both PPV and TPR, resulting in a high Fmeasure of 96.15%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Quantitative result analysis using manual segmentations</head><p>We generated manual segmentations of the image database using the following method: the user defines points on the border of the nucleus, which are then automatically connected using splines. Later, the user has the possibility to correct the result until it is satisfactory.</p><p>The manual segmentations are compared to the automatic segmentation in the following way: labeled connected component maps are generated from the manual segmentations. Then, for every connected component (i.e. for every nucleus), a corresponding region in the automatic segmentation is determined by overlap. If no such region is found, the nucleus is omitted. Then, a measure MS, which determines the fit of the both regions is defined as the amount of pixels that differ in both regions, normalized by division of the amount of pixels in the region of the manual segmentation. Mean (MMS) and median (MedMS) of this measure are computed. By performing this way, neither the false positives nor the false negatives   <ref type="table" target="#tab_0">1</ref>. Furthermore, Fig. <ref type="figure" target="#fig_11">12</ref> shows a boxplot of the results.</p><p>The quantitative evaluation complements the findings of the qualitative evaluation. The proposed method has an MMS of 0.205 and a MedMS of 0.120, the lowest errors. Also MS-Med-Ath-M performs well on finding the shape of the nuclei. So, when the nucleus is detected, the proposed method does this accurately and therewith it not only performs well in finding the nuclei, but also in finding their shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The web application</head><p>The implemented application tackles several problems. Because of the varying complexity of the segmentation process (see Section 1), regions of interest have to be chosen to allow for reliable segmentation within the whole range of segmentation complexity. The application allows to choose regions manually by the physician, but it is also prepared for automatic region choosing and comparison of manual and automatic methods. Furthermore, the system solves practical problems of the workflow: it enables the storage and display of algorithm results (e.g. segmentations), as well as of diagnoses and comments of the physicians for evaluation purposes. Its source code can be found at http://sci2s.ugr.es/dicits/software/CIP2. The application was implemented using standard web development tools and techniques such as PHP, JavaScript, and MySQL. As the application is additionally based on the Google Maps API 1 (see Fig. <ref type="figure" target="#fig_12">13</ref>), tiled versions of the original images 1 http://code.google.com/apis/maps/.</p><p>with tiles of the size 256 × 256 pixels are generated for each zoom level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Region extraction procedures</head><p>Manual ROI-finding was performed by a cytopathologist by marking regions of interest in different slides. The normal processing of the slide was performed using the web application.</p><p>The cytopathologist examined the whole slides at a magnification level of 10×, and marked regions that he "processes", i.e. that attract his attention. Regions, where it was necessary to zoom to the maximal level were marked in another color (see Fig. <ref type="figure" target="#fig_2">14</ref>). Currently, our work is based on the manual extraction of regions. However, as we plan to automate the whole screening process, it is necessary to detect the ROIs automatically. These algorithms then can be evaluated against the manually defined regions with the help of our software.</p><p>A first version of such an algorithm is currently implemented in the software system. The algorithm uses an Otsu thresholded version of the image in a low magnification level, and performs a tiling on it, partitioning the image into tiles of 18 × 18 pixels in this zoom level. Then, it computes the percentage of pixels segmented by the thresholding technique. Upper and lower thresholds th min , th max for this percentage are defined manually. Next, the algorithm marks all regions where the percentage of segmented pixels lies within these thresholds. The automatically generated markers shown in Fig. <ref type="figure" target="#fig_3">15</ref> were generated using this algorithm, with a lower threshold of 15% and an upper threshold of 100%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Result storage and data management</head><p>All intermediary results are stored, and can be displayed together with the results as overlays onto the original images within the web application (see Fig. <ref type="figure" target="#fig_14">16</ref>). For this purpose, the ROIs are temporarily inserted into the original image and the tiles affected are recalculated and stored for each zoom level. All specimens are presented within the web application as a list, containing an overview image, the scanned bar code, comments inserted by the physician, the date the slide was inserted into the database and a name (see Fig. <ref type="figure" target="#fig_15">17</ref>).</p><p>The bar code allows for identification of the slide within the hospital's database, so that the physician can look up the diagnosis and other slide characteristics, and add them as a comment. These data are planned to be used in future evaluation and development of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and future work</head><p>Cervical screening concerns the health of thousands of women in the world, thus it is a problem of the utmost importance from a social point of view. And commercially it also would render high profits. Furthermore, with the presence of (semi-)automatic systems, more women could be screened, the screening potentially could be accelerated, and quality of the screening could be improved, as screening is an exhausting procedure, and humans tend to errors after having processed a great amount of slides without breaks. A possible solution for the automation of the problem would include the following steps: region of interest selection, cell nuclei and cytoplasm segmentation, and classification into different classes of diagnoses. Segmentation of the nuclei is the most challenging step. This is due to the fact that the difficulty for performing accurate segmentation of the nuclei varies widely. Especially because in an automated system thousands of cells have to be segmented reliably, no satisfactory solutions to the problem have been published, to the best of our knowledge. Hence, a methodology for nuclei segmentation has to be developed. By effectively combining available tools, we have proposed an algorithm for this problem. The proposed algorithm addresses successfully the segmentation of nuclei in cell images with differing characteristics. The noise present in the images due to the acquisition modality and the staining is eliminated with mean-shift filtering and median filtering. Nuclei are localized with a randomized Hough transform, using prior knowledge. Their exact shape is determined with a level set segmentation, which results in smooth contours, finding the borders of the nuclei accurately. The algorithm is more robust against the segmentation of artifacts and the miss of cell nuclei in the proposed combination of methods than the single components. It leads to promising results on the evaluation database.</p><p>Although a previous selection of the ROIs is generally not necessary for application of the algorithm (the algorithm could be applied to tiled versions of the images), it is important because quality of slides and the type and appearance of artifacts and nuclei vary widely. Therefore, an application has been developed for interactive region extraction by cytopathologists. Furthermore, the whole-slide scans at the maximal magnification level of 40× are huge images of 60,000-80,000 pixels in each dimension, that have an uncompressed size of around 22 GB each (with jpeg compression ca. 1 GB), and they typically contain thousands of cells, so that each cell only provides around 100 × 100 pixels of information. These problems yield several consequences. As no publicly available databases exist to the best of our knowledge, in order to obtain data, cooperations with experts are indispensable. Obtaining gold-standard, manual segmentations for even one full-slide scan is a time consuming process that has to be performed by the experts. Moreover, the algorithms have to work robustly and fast on these huge amounts of data, and tools for managing the data are not readily available. In order to tackle these problems, our software system enables besides region extraction and processing of high-resolution full-slide scans the administration of the original image data, intermediate and final algorithmical results, and additional medical information.</p><p>Our aim is to build a publicly available database with gold-standard segmentations, which would ease greatly standardization and comparability of methods in the field. Besides this, future work includes extracting features from the nuclei and using an expert system for their classification. Furthermore, a robust algorithm for region extraction should be developed and evaluated against the regions manually chosen by the expert. Also, as it will be possible to take into account characteristics of the nuclei identified within classification during segmentation (e.g. as stopping criteria for the search for new ellipses during Hough transform), the segmentation and classification steps do not have to be performed strictly sequentially, but a method could be developed that feeds classification results back to the segmentation process, potentially leading to better results.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 -</head><label>1</label><figDesc>Fig. 1 -Example of a microscope slide at different magnification levels: (a) the whole slide is shown at magnification level 1×; (b) at a magnification level of 20×, groups of cells can be examined; (c) single cells can be analyzed in detail at the highest magnification level, 40×.</figDesc><graphic coords="2,103.44,66.30,384.12,137.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 -</head><label>3</label><figDesc>Fig. 3 -Otsu thresholding with two thresholds is performed on images of magnification level 2× in order to get a separation between background and foreground. (a and c) Example input images. (b and d) Logarithmic histograms of the images. The dashed line indicates t 1 , which gives a rough separation into cells and nuclei. The continuous line indicates t 2 , which is used for separation between foreground and background.</figDesc><graphic coords="5,96.06,66.52,408.24,433.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 -</head><label>4</label><figDesc>Fig. 4 -(a) Original image. (b) Mean-shift filtered image. Noise is removed effectively while the edges are preserved. (c) The threshold obtained by the Otsu thresholding on the overview image applied to the image. The resulting mask is eroded. (d) Edges obtained by a Canny edge detection from the mean-shift and median filtered image. (e) The edges of the cytoplasm are masked out.</figDesc><graphic coords="6,91.44,66.77,408.24,438.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 -</head><label>5</label><figDesc>Fig. 5 -Flowchart of the proposed algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 -</head><label>6</label><figDesc>Fig. 6 -The Hough transform is computed sequentially in regions of interest that overlap.</figDesc><graphic coords="7,69.54,553.74,204.12,166.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 -</head><label>7</label><figDesc>Fig. 7 -Three randomly sampled points P 1 , P 2 , P 3 are used to compute a candidate ellipse.</figDesc><graphic coords="7,326.57,66.42,204.12,140.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 -</head><label>8</label><figDesc>Fig. 8 -Candidate nuclei found by the randomized Hough transform. The ellipses discarded before application of the level set are shown in green color. The ellipses discarded after the level set segmentation with the help of a distance measure are shown in blue color. Red ellipses are the ellipses whose level set segmentation is finally assumed to be a nucleus. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of the article.)</figDesc><graphic coords="8,58.93,66.27,216.00,302.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 -</head><label>9</label><figDesc>Fig. 9 -The method is capable of detecting nuclei that are only roughly ellipse-shaped. (a) Part of an original image. (b) Detected edges. (c) Candidate ellipses (colors as in Fig. 8). (d) Resulting segmentation. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of the article.)</figDesc><graphic coords="8,303.96,66.39,240.12,269.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>For</head><label></label><figDesc>mean-shift filtering, we used a window size of 40 for the intensity values, and a window size of 20 on the spatial parameters. During Canny edge detection, as parameters for the hysteresis thresholding, we use a lower threshold of 5 and an upper threshold of 10, furthermore, a variance of 2.0 is used for the Gaussian kernel. The Hough transform is used with the parameters d min = 30, d max = 150, e = 0.99, m = 300, dist a = 50, and dist b = 200. The resolution of the accumulator is 10 pixels for the coordinates of the ellipse center and the axes, and 20 •</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 -</head><label>10</label><figDesc>Fig. 10 -The level set segmentation. (a) Speed image: a distance map of the detected edges. (b) The segmentation resulting from the level set algorithm.</figDesc><graphic coords="9,174.56,521.47,250.92,200.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 -</head><label>11</label><figDesc>Fig. 11 -Four example images for the edge detection process. The first row shows the original images. The second and third rows show the result of Canny edge detection with parameters (variance, lower threshold, upper threshold) of (2.0, 3, 7), and (2.0, 5, 10), respectively. It is not possible to effectively eliminate noise and keep all nuclei. The fourth and fifth rows show Canny edge detection with parameters (2.0, 5, 10) applied to the mean-shift and median filtered images. In the fifth row, edges adjacent to the background have been masked out.</figDesc><graphic coords="10,91.44,66.45,408.24,334.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 -</head><label>12</label><figDesc>Fig.12-Boxplot showing the comparison of the methods with a manual segmentation. Methods are as in Table1. Boxes are from first to third quartile, middle line is the median, whiskers indicate 5% and 95% quantile.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 -</head><label>13</label><figDesc>Fig. 13 -Regions are defined with the help of markers (green), for navigation purposes, an overview map is present. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of the article.)</figDesc><graphic coords="12,103.44,66.62,384.12,223.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 14 -Fig. 15 -</head><label>1415</label><figDesc>Fig. 14 -Regions marked by a cytopathologist. The yellow labels show regions that were inspected and on the lower magnification level 10× could be determined that they are normal. The regions marked with a green marker are regions that were examined at the full magnification level by the cytopathologist for the diagnosis. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of the article.)</figDesc><graphic coords="12,115.44,516.87,360.00,182.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 16 -</head><label>16</label><figDesc>Fig. 16 -The processed regions are displayed in the application as overlays. Here, an example of an intermediary adaptive threshold result is shown.</figDesc><graphic coords="13,326.57,66.92,204.12,131.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 17 -</head><label>17</label><figDesc>Fig. 17 -The specimens are presented as a list including overview maps and bar code images for identification by the physician (anonymized in this image).</figDesc><graphic coords="13,96.06,563.92,408.24,158.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 18 -</head><label>18</label><figDesc>Fig. 18 -Four example images of the results, from two different specimens. The first row shows the original images. The following rows show segmentation results superimposed in black color on the original images for the methods (in this order): MS-Med-Th-M, MS-Med-Ath-M, CAD-Med-Ath-M, MS-Med-Th-LS-M, and the proposed method MS-Med-Hough-LS. The indicated methods are combinations of: mean-shift (MS), median (Med), threshold (Th), morphological analysis (M), adaptive threshold (Ath), curvature anisotropic diffusion (CAD), and the Canny edge-based level set method (LS). Using the thresholding techniques, often parts of the nuclei are lost, and other dark structures are captured as well. The methods using morphological operators also miss nuclei in the neighborhood of dominant structures. The presented method is able to overcome these shortcomings.</figDesc><graphic coords="14,79.44,66.71,432.00,428.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 -Segmentation results. For the qualitative evaluation, positive predictive value (PPV), true positive rate (TPR), and F-measure are shown, calculated over 549 nuclei in total. For the quantitative evaluation, mean and median of the measure MS are shown. The indicated methods are combinations of: mean-shift (MS), median (Med), threshold (Th), morphological analysis (M), adaptive threshold (Ath), curvature anisotropic diffusion (CAD), and the Canny edge-based level set method (LS).</head><label>1</label><figDesc></figDesc><table><row><cell>Method</cell><cell>MS-Med-Th-M</cell><cell>MS-Med-Ath-M</cell><cell>CAD-Med-Ath-M</cell><cell>MS-Med-Th-LS-M</cell><cell>Proposed method</cell></row><row><cell>PPV (%)</cell><cell>89.78</cell><cell>88.74</cell><cell>90.68</cell><cell>95.50</cell><cell>96.69</cell></row><row><cell>TPR (%)</cell><cell>79.96</cell><cell>86.16</cell><cell>79.78</cell><cell>73.41</cell><cell>95.63</cell></row><row><cell>F-measure (%)</cell><cell>84.59</cell><cell>87.43</cell><cell>84.88</cell><cell>83.01</cell><cell>96.15</cell></row><row><cell>MMS</cell><cell>0.516</cell><cell>0.327</cell><cell>0.418</cell><cell>0.355</cell><cell>0.205</cell></row><row><cell>MedMS</cell><cell>0.128</cell><cell>0.122</cell><cell>0.141</cell><cell>0.114</cell><cell>0.120</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>in Table 1. Boxes are from first to third quartile, middle line is the median, whiskers indicate 5% and 95% quantile. already</head><label></label><figDesc>discussed in Section 3.2 affect the evaluation, and accuracy of the segmentation where the nucleus was correctly detected can be assessed.Mean and median values for MS over all nuclei are shown in Table</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The used image material and medical assistance were provided by Doctors Esquivias and García-Rojo. We acknowledge the help of Jose Rubén Sánchez Iruela regarding web application programming tasks. C. Bergmeir holds a scholarship from the Spanish Ministry of Education (MEC) of the "Programa de Formación del Profesorado Universitario (FPU)". r e f e r e n c e s</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of interest</head><p>None declared.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Comprehensive cervical cancer control: a guide to essential practice, World Health Organization, Department of Reproductive Health and Research and Department of Chronic Diseases and Health Promotion</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>World Health Organization</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Comparison of fluid-based, thin-layer processing and conventional Papanicolaou methods for uterine cervical cytology</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-C</forename><surname>Tsou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Formosan Medical Association</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="500" to="505" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cytology: Diagnostic Principles and Clinical Correlates</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Cibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Ducatman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Expert Consult Series</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic classification of cervical cells using a binary tree classifier</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="80" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Some new color features and their application to cervical cell classification</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Poulsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Louis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="401" to="411" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adaptive multi-scale texture analysis with application to automated cytology</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Walker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>The University of Queensland</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Pap-smear classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>Technical University of Denmark (DTU</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s Thesis</note>
	<note>Oersted, Dept. of Automation</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Classification of pap-smear data by transductive neuro-fuzzy methods</title>
		<author>
			<persName><forename type="first">J</forename><surname>Norup</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Technical University of Denmark (DTU</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s Thesis</note>
	<note>Oersted, Dept. of Automation</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Particle swarm optimization for pap-smear diagnosis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Marinakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marinaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dounias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1645" to="1656" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised cell nucleus segmentation with active contours</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lovell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="213" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Optimal segmentation of cell images</title>
		<author>
			<persName><forename type="first">H.-S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEE Proceedings: Vision, Image and Signal Processing</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="50" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Seeded region growing features extraction algorithm; its potential use in improving screening for cervical cancer</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Mat-Isa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Mashor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Othman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of the Computer, the Internet and Management</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="70" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nucleus and cytoplast contour detector of cervical smear image</title>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-K</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Yang-Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-C</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1441" to="1453" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Edge enhancement nucleus and cytoplast contour detector of cervical smear images</title>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Yang-Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-K</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-P</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="366" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gradient direction edge enhancement based nucleus and cytoplasm contour detector of cervical smear images</title>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Yang-Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-K</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-P</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS)</title>
		<imprint>
			<biblScope unit="volume">4901</biblScope>
			<biblScope unit="page" from="290" to="297" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Closing curves with Riemannian dilation: application to segmentation in automated cervical cancer screening</title>
		<author>
			<persName><forename type="first">P</forename><surname>Malm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brun</surname></persName>
		</author>
		<idno>ISVC &apos;09</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Symposium on Advances in Visual Computing</title>
		<meeting>the 5th International Symposium on Advances in Visual Computing</meeting>
		<imprint>
			<publisher>Berlin/Heidelberg</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detection and segmentation of cervical cell cytoplast and nucleus</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-K</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Imaging Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="260" to="270" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust cell nuclei segmentation using statistical modelling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mouroutis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Bharath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioimaging</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="79" to="91" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Segmenting cell images: a deterministic relaxation approach</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">3117</biblScope>
			<biblScope unit="page" from="281" to="291" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">White blood cell segmentation using morphological operators and scale-space analysis</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Dorini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Minetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Leite</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIBGRAPI 2007 -20th Brazilian Symposium on Computer Graphics and Image Processing</title>
		<meeting>SIBGRAPI 2007 -20th Brazilian Symposium on Computer Graphics and Image Processing</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="294" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic segmentation on cell image fusing gray and gradient information</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference Proceedings: Annual International Conference of the IEEE Engineering in Medicine and Biology Society</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="5624" to="5627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3D cell nuclei segmentation based on gradient flow tracking</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tarokh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Holley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T C</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Cell Biology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Segmentation of heterogeneous blob objects through voting and level set formulation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Parvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1781" to="1787" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graph-based tools for microscopic cellular image segmentation</title>
		<author>
			<persName><forename type="first">V.-T</forename><surname>Ta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lézoray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elmoataz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schüpp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1113" to="1125" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Segmentation of biological cell images for sonification</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D N</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -1st International Congress on Image and Signal Processing</title>
		<meeting>-1st International Congress on Image and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="128" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An automated method for segmentation of epithelial cervical cells in images of ThinPrep</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Amirfattahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medical Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1043" to="1058" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Interactive segmentation framework of the medical imaging interaction toolkit</title>
		<author>
			<persName><forename type="first">D</forename><surname>Maleike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nolden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Meinzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods and Programs in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="72" to="83" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A web-based approach to distributing medical images for referring physicians outside hospitals without specialised user management</title>
		<author>
			<persName><forename type="first">H</forename><surname>Muench</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schroeter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Meinzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Engelmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Assisted Radiology and Surgery</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="130" to="132" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Suppl. 7</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">High performance medical image processing in client/server-environments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Meinzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods and Programs in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="207" to="217" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Web-based interactive 2D/3D medical image processing and visualization software</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Mahmoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akhondi-Asl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Faghih-Roohi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Taimouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sabouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Soltanian-Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods and Programs in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="172" to="182" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Segmentation of cervical cell images using mean-shift filtering and morphological operators</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bergmeir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>García Silvente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Esquivias</forename><surname>López-Cuervo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Benítez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPIE</title>
		<imprint>
			<biblScope unit="volume">7623</biblScope>
			<biblScope unit="page">76234</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning OpenCV: Computer Vision with the OpenCV Library</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kaehler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>O&apos;Reilly, Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Pratt</surname></persName>
		</author>
		<title level="m">Digital Image Processing</title>
		<imprint>
			<publisher>PIKS Inside, Wiley</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Threshold selection method from gray-level histograms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics SMC</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="62" to="66" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mean shift analysis and applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1197" to="1203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mean shift: a robust approach toward feature space analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Advanced Algorithmic Approaches to Medical Image Segmentation: State-of-the-art Applications in Cardiology</title>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurology, Mammography and Pathology</title>
		<imprint>
			<biblScope unit="page" from="541" to="558" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>Cell image segmentation for diagnostic pathology</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Randomized Hough transform: better ellipse detection</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Mclaughlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Region 10 Annual International Conference, Proceedings/TENCON</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="409" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Splitting touching cells based on concave points and ellipse fitting</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2434" to="2446" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Aguado</surname></persName>
		</author>
		<title level="m">Feature Extraction &amp; Image Processing</title>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="2008-01">January 2008</date>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A new algorithm for ellipse detection by curve segments</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1836" to="1841" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kultanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A new curve detection method: randomized Hough transform (RHT)</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="331" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Ellipse detection using a randomized Hough transform based on edge segment merging scheme</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hahn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>ISPRA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Detecting partially occluded ellipses using the Hough transform</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Illingworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="37" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">The ITK Software Guide</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ibanez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cates</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Kitware, Inc</publisher>
			<biblScope unit="page" from="1" to="930934" />
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Image segmentation evaluation: a survey of unsupervised methods</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Fritts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="260" to="280" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A multidimensional segmentation evaluation for medical image data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cárdenes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>De Luis-García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bach-Cuadra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods and Programs in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="108" to="124" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Error measures for scene segmentation</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Yasnoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Mui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Bacus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="217" to="231" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cost-sensitive boosting for classification of imbalanced data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K C</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3358" to="3378" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
