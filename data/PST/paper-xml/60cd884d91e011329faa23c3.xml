<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledgeable or Educated Guess? Revisiting Language Models as Knowledge Bases</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-06-17">17 Jun 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Boxi</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
							<email>hongyu@iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xianpei</forename><surname>Han</surname></persName>
							<email>xianpei@iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">State Key Laboratory of Computer Science Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Le Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">State Key Laboratory of Computer Science Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lingyong</forename><surname>Yan</surname></persName>
							<email>lingyong2014@iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meng</forename><surname>Liao</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Data Quality Team</orgName>
								<address>
									<addrLine>WeChat</addrLine>
									<region>Tencent Inc</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tong</forename><surname>Xue</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Data Quality Team</orgName>
								<address>
									<addrLine>WeChat</addrLine>
									<region>Tencent Inc</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jin</forename><surname>Xu</surname></persName>
							<email>jinxxu@tencent.com</email>
							<affiliation key="aff3">
								<orgName type="department">Data Quality Team</orgName>
								<address>
									<addrLine>WeChat</addrLine>
									<region>Tencent Inc</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">London Chicago Capital</orgName>
								<address>
									<addrLine>1 Milan Big City 2 City 3 Area 3 Area 1.0 City 1.0</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledgeable or Educated Guess? Revisiting Language Models as Knowledge Bases</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-06-17">17 Jun 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2106.09231v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Previous literatures show that pre-trained masked language models (MLMs) such as BERT can achieve competitive factual knowledge extraction performance on some datasets, indicating that MLMs can potentially be a reliable knowledge source. In this paper, we conduct a rigorous study to explore the underlying predicting mechanisms of MLMs over different extraction paradigms. By investigating the behaviors of MLMs, we find that previous decent performance mainly owes to the biased prompts which overfit dataset artifacts. Furthermore, incorporating illustrative cases and external contexts improve knowledge prediction mainly due to entity type guidance and golden answer leakage. Our findings shed light on the underlying predicting mechanisms of MLMs, and strongly question the previous conclusion that current MLMs can potentially serve as reliable factual knowledge bases 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, pre-trained language models <ref type="bibr" target="#b28">(Peters et al., 2018;</ref><ref type="bibr" target="#b6">Devlin et al., 2019;</ref><ref type="bibr" target="#b2">Brown et al., 2020)</ref> have achieved promising performance on many NLP tasks. Apart from utilizing the universal representations from pre-trained models in downstream tasks, some literatures have shown the potential of pretrained masked language models (e.g., BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> and RoBERTa <ref type="bibr" target="#b25">(Liu et al., 2019b)</ref>) to be factual knowledge bases <ref type="bibr" target="#b30">(Petroni et al., 2019;</ref><ref type="bibr" target="#b1">Bouraoui et al., 2020;</ref><ref type="bibr" target="#b16">Jiang et al., 2020b;</ref><ref type="bibr" target="#b37">Shin et al., 2020;</ref><ref type="bibr" target="#b15">Jiang et al., 2020a;</ref><ref type="bibr" target="#b44">Wang et al., 2020;</ref><ref type="bibr" target="#b18">Kassner and Schütze, 2020a;</ref><ref type="bibr" target="#b17">Kassner et al., 2020)</ref>. For example, to extract the birthplace of Steve Jobs, we can query MLMs like BERT with "Steve Jobs was born in [MASK]", where Steve Jobs is the subject of the fact, "was born in" is a prompt string for the relation "place-of-birth" and [MASK] is a placeholder for the object to predict. Then MLMs are expected to predict the correct answer "California" at the [MASK] position based on its internal knowledge. To help MLMs better extract knowledge, the query may also be enriched with external information like illustrative cases (e.g., (Obama, Hawaii)) <ref type="bibr" target="#b2">(Brown et al., 2020)</ref> or external context (e.g., Jobs lives in California) <ref type="bibr" target="#b29">(Petroni et al., 2020)</ref>. Some literatures have shown that such paradigms can achieve decent performance on some benchmarks like LAMA <ref type="bibr" target="#b30">(Petroni et al., 2019)</ref>.</p><p>Despite some reported success, currently there is no rigorous study looking deeply into the underlying mechanisms behind these achievements. Besides, it is also unclear whether such achievements depend on certain conditions (e.g., datasets, domains, relations). The absence of such kind of studies undermines our trust in the predictions of MLMs. We could neither determine whether the predictions are reliable nor explain why MLMs make a specific prediction, and therefore significantly limits MLMs' further applications and improvements.</p><p>To this end, this paper conducts a thorough study on whether MLMs could be reliable factual knowledge bases. Throughout our investigations, we analyze the behaviors of MLMs, figure out the critical factors for MLMs to achieve decent performance, and demonstrate how different kinds of external information influence MLMs' predictions. Specifically, we investigate factual knowledge extraction from MLMs<ref type="foot" target="#foot_0">2</ref> over three representative factual knowledge extraction paradigms, as shown in Figure <ref type="figure" target="#fig_0">1:</ref> • Prompt-based retrieval <ref type="bibr" target="#b30">(Petroni et al., 2019;</ref><ref type="bibr" target="#b16">Jiang et al., 2020b;</ref><ref type="bibr" target="#b37">Shin et al., 2020)</ref>, which queries MLM for object answer only given the subject and the corresponding relation prompt as input, e.g., "Jobs was born in <ref type="bibr">[MASK]</ref>."</p><p>• Case-based analogy <ref type="bibr" target="#b2">(Brown et al., 2020;</ref><ref type="bibr" target="#b26">Madotto et al., 2020;</ref><ref type="bibr" target="#b10">Gao et al., 2020)</ref>, which enhances the prompt-based retrieval with several illustrative cases, e.g., "Obama was born in Hawaii.</p><p>[SEP] Jobs was born in <ref type="bibr">[MASK]</ref>."</p><p>• Context-based inference <ref type="bibr" target="#b29">(Petroni et al., 2020;</ref><ref type="bibr" target="#b0">Bian et al., 2021)</ref>, which augments the prompt-based retrieval with external relevant contexts, e.g., "Jobs lives in California.</p><p>[SEP] Jobs was born in <ref type="bibr">[MASK]</ref>."</p><p>Surprisingly, the main conclusions of this paper somewhat diverge from previous findings in published literatures, which are summarized in Figure 1. For prompt-based paradigm ( § 3), we find that the prediction distribution of MLMs is significantly prompt-biased. Specifically, we find that prompt-based retrieval generates similar predictions on totally different datasets. And predictions are spuriously correlated with the applied prompts, rather than the facts we want to extract. Therefore, previous decent performance mainly stems from the prompt over-fitting the dataset answer distribution, rather than MLMs' knowledge extraction ability. Our findings strongly question the conclusions of previous literatures, and demonstrate that current MLMs can not serve as reliable knowledge bases when using prompt-based retrieval paradigm.</p><p>For case-based paradigm ( § 4), we find that the illustrative cases mainly provide a "type guidance" for MLMs. To show this, we propose a novel algorithm to induce the object type of each relation based on Wikidata<ref type="foot" target="#foot_1">3</ref> taxonomy. According to the induced types, we find that the performance gain brought by illustrative cases mainly owes to the improvement on recognizing object type. By contrast, it cannot help MLMs select the correct answer from the entities with the same type: the rank of answer within its entity type is changed randomly after introducing illustrative cases. That is to say, under the case-based paradigm, although MLMs can effectively analogize between entities with the same type, they still cannot well identify the exact target object based on their internal knowledge and the provided illustrative cases.</p><p>For context-based paradigm ( § 5), we find that context can help the factual knowledge extraction mainly because it explicitly or implicitly leaks the correct answer. Specifically, the knowledge extraction performance improvement mainly happens when the introduced context contains the answer. Furthermore, when we mask the answer in the context, the performance still significantly improves as long as MLMs can correctly reconstruct the masked answer in the remaining context. In other words, in these instances, the context itself servers as a delegator of the masked answer, and therefore MLMs can still obtain sufficient implicit answer evidence even the answer doesn't explicitly appear.</p><p>All the above findings demonstrate that current MLMs are not reliable in factual knowledge extraction. Furthermore, this paper sheds some light on the underlying predicting mechanisms of MLMs, which can potentially benefit many future studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The great success of Pre-trained Language Models (PLMs) raises the question of whether PLMs can be directly used as reliable knowledge bases. <ref type="bibr" target="#b30">Petroni et al. (2019)</ref> propose the LAMA benchmark, which probes knowledge in PLMs using prompt-based retrieval. <ref type="bibr" target="#b15">Jiang et al. (2020a)</ref> build a multilingual knowledge probing benchmark based on LAMA. There are many studies focus on probing specific knowledge in PLMs, such as linguistic knowledge <ref type="bibr" target="#b23">(Lin et al., 2019;</ref><ref type="bibr" target="#b41">Tenney et al., 2019;</ref><ref type="bibr" target="#b24">Liu et al., 2019a;</ref><ref type="bibr">Htut et al., 2019;</ref><ref type="bibr" target="#b12">Hewitt and Manning, 2019;</ref><ref type="bibr" target="#b11">Goldberg, 2019;</ref><ref type="bibr" target="#b45">Warstadt et al., 2019)</ref>, semantic knowledge <ref type="bibr" target="#b41">(Tenney et al., 2019;</ref><ref type="bibr" target="#b43">Wallace et al., 2019;</ref><ref type="bibr" target="#b8">Ettinger, 2020)</ref> and world knowledge <ref type="bibr" target="#b5">(Davison et al., 2019;</ref><ref type="bibr" target="#b1">Bouraoui et al., 2020;</ref><ref type="bibr" target="#b9">Forbes et al., 2019;</ref><ref type="bibr" target="#b46">Zhou et al., 2019;</ref><ref type="bibr" target="#b33">Roberts et al., 2020;</ref><ref type="bibr">Lin et al., 2020;</ref><ref type="bibr" target="#b40">Tamborrino et al., 2020)</ref>. Recently, some studies doubt the reliability of PLMs as knowledge base by discovering the the spurious correlation to surface forms <ref type="bibr" target="#b27">(McCoy et al., 2019;</ref><ref type="bibr" target="#b31">Poerner et al., 2020;</ref><ref type="bibr" target="#b38">Shwartz et al., 2020)</ref>, and their sensitivity to "negation" and "mispriming" (Kassner and Schütze, 2020b).</p><p>Currently, there are three main paradigms for knowledge extraction from PLMs: prompt-based retrieval <ref type="bibr" target="#b36">(Schick and Schütze, 2021;</ref><ref type="bibr" target="#b21">Li and Liang, 2021)</ref>, case-based analogy <ref type="bibr">(Schick and Schütze, 2020a,b)</ref>, and context-based inference. For promptbased retrieval, current studies focus on seeking better prompts by either mining from corpus <ref type="bibr" target="#b16">(Jiang et al., 2020b)</ref> or learning using labeled data <ref type="bibr" target="#b37">(Shin et al., 2020)</ref>. For case-based analogy, current studies mostly focus on whether good cases will lead to good few-shot abilities, and many tasks are tried <ref type="bibr" target="#b2">(Brown et al., 2020;</ref><ref type="bibr" target="#b26">Madotto et al., 2020;</ref><ref type="bibr" target="#b10">Gao et al., 2020)</ref>. For context-based inference, current studies focus on enhancing the prediction by seeking more informative contexts, e.g., for knowledge extraction <ref type="bibr" target="#b29">(Petroni et al., 2020)</ref> and <ref type="bibr">Com-monsenseQA (Bian et al., 2021)</ref>. However, there is no previous work which focuses on systematically study the underlying predicting mechanisms of MLMs on these paradigms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Prompt-based Retrieval</head><p>The prompt-based retrieval extracts factual knowledge by querying MLMs with (subject, prompt, [MASK]). For example, to extract the "place-of-birth" of Steve Jobs, we could query BERT with "Steve Jobs was born in <ref type="bibr">[MASK]</ref>." and the predicted "California" would be regarded as the answer. We consider three kinds of prompts: the manually prompts T man created by <ref type="bibr" target="#b30">Petroni et al. (2019)</ref>, the mining-based prompts T mine by <ref type="bibr" target="#b16">Jiang et al. (2020b)</ref> and the automatically searched prompts T auto from <ref type="bibr" target="#b37">Shin et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overall Conclusion</head><p>Conclusion 1. Prompt-based retrieval is promptbiased. As a result, previous decent performance actually measures how well the applied prompts fit the dataset answer distribution, rather than the factual knowledge extraction ability from MLMs.  Specifically, we conduct studies and find that 1) Prompt-based retrieval will generate similar responses given quite different datasets. To show this, we construct a new dataset from Wikidata -WIKI-UNI, which have a totally different answer distribution from the widely-used LAMA<ref type="foot" target="#foot_2">4</ref> dataset <ref type="bibr" target="#b30">(Petroni et al., 2019)</ref>. However, we find that the prediction distributions on WIKI-UNI and LAMA are highly correlated, and this spurious correlation holds across different prompts. Such results reveal that there is just a weak correlation between the predictions of MLMs and the factual answer distribution of the dataset. 2) The prediction distribution is dominated by the prompt, i.e., the prediction distribution using only (prompt, <ref type="bibr">[MASK]</ref>) is highly correlated to the prediction distribution using <ref type="bibr">(subject, prompt, [MASK]</ref>). This indicates that it is the applied prompts, rather than the actual facts, determine the predictions of MLMs. 3) The performance of the prompt can be predicted by the divergence between the prompt-only distribution and the answer distribution of the dataset. All these findings reveal that previous decent performance in this field actually measures the degree of prompt-dataset fitness, rather than the universal factual knowledge extraction ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Different Answers, Similar Predictions</head><p>Finding 1. Prompt-based retrieval will generate similar responses to quite different datasets.</p><p>A reliable knowledge extractor should generate  different responses to different knowledge queries.</p><p>To verify whether MLMs meet this standard, we manually construct a new dataset -WIKI-UNI, which has a comparable size but totally different answer distribution to LAMA, and then compare the prediction distributions on them. For a fair comparison, we follow the construction criteria of LAMA: we use the same 41 relations, filter out the queries whose objects are not in the MLMs' vocabulary. Compared with LAMA, the major difference is that WIKI-UNI has a uniform answer distribution, i.e., for each relation, we keep the same number of instances for each object. Please refer to Appendix for more construction details.</p><p>Figure <ref type="figure" target="#fig_2">2a</ref> shows the answer distributions of LAMA and WIKI-UNI on relation "place-of-birth". We can see that the answers in LAMA are highly concentrated on the head object entities, while the answers in WIKI-UNI follow a uniform distribution.</p><p>Given LAMA and WIKI-UNI, we investigate the predicting behaviors of MLMs. Surprisingly, the prediction distributions on these two totally different datasets are highly correlated. Figure <ref type="figure" target="#fig_2">2b</ref> shows an example. We can see that the prediction distribution on WIKI-UNI is very similar to that on LAMA. And these two distributions are both close to the answer distribution of LAMA but far away from the answer distribution of WIKI-UNI.</p><p>To investigate whether this spurious correlation is a common phenomenon, we analyze the Pearson correlation coefficient between prediction distributions on LAMA and WIKI-UNI across different relations and three kinds of prompts. The boxplot in Figure <ref type="figure" target="#fig_3">3</ref> shows the very significant correlation between the prediction distributions on LAMA and WIKI-UNI: on all three kinds of prompts, the correlation coefficients exceed 0.8 in more than half of relations. These results demonstrate that promptbased retrieval will lead to very similar prediction distributions even when test sets have vastly different answer distributions. Furthermore, we find that the prediction distribution obviously doesn't correspond to the answer distribution of WIKI-UNI. From Table <ref type="table" target="#tab_0">1</ref>, we can see that on average, the top-5 answers of each relation in WIKI-UNI cover only 7.78% instances. By contrast, the top-5 predictions of each relation in WIKI-UNI cover more than 52% instances, which is close to the answer distribution and prediction distribution on LAMA. As a result, the performance on WIKI-UNI (mean P@1: 16.47) is significantly worse than that on LAMA (mean P@1: 30.36). In conclusion, the facts of a dataset cannot explain the predictions of MLMs, and therefore previous evaluations of the MLMs' ability on factual knowledge extraction are unreliable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Prompts Dominates Predictions</head><p>Finding 2. The prediction distribution is severely prompt-biased.</p><p>To investigate the underlying factors of the predicting behavior of MLMs, we compare the promptonly prediction distribution using only (prompt, <ref type="bibr">[MASK]</ref>) and the full prediction distribution using (subject, prompt, [MASK]). To obtain the promptonly distribution, we mask the subject and then use ([MASK], prompt, [MASK]) to query MLMs (e.g., <ref type="bibr">[MASK]</ref> was born in <ref type="bibr">[MASK]</ref>). Because there is no subject information in the input, MLMs can only depend on applied prompt's information to make the prediction at the second <ref type="bibr">[MASK]</ref>. Therefore, we regard the probability distribution at the second [MASK] symbol as the prompt-only distribution.</p><p>After that, we analyze the correlations between the prompt-only distribution and the prediction distribution on WIKI-UNI dataset. Figure <ref type="figure" target="#fig_4">4</ref> shows the boxplot. On all three kinds of prompts, correlation coefficients between the prompt-only distribution and the prediction distribution on WIKI-UNI exceed 0.6 in more than half of relations. This demonstrates that in these relations, the promptonly distribution dominates the prediction distribution. Combining with the findings in Section 3.2, we can summarize that the prompt-based retrieval is mainly based on guided guessing, i.e., the predictions are generated by sampling from the promptbiased distribution guided by the moderate impact of subjects.</p><p>Note that among a minor part of relations, the correlations between the prompt-only distribution and the prediction distribution are relatively low. We find that the main reason is the type selectional preference provided by the subject entities, and Section 4 will further discuss the impact of this type-guidance mechanism for MLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Better Prompts are Over-Fitting</head><p>Finding 3. "Better" prompts are the prompts fitting the answer distribution better, rather than the prompts with better retrieval ability.</p><p>Some previous literatures attempt to find better prompts for factual knowledge extraction from MLMs. However, as we mentioned above, the prompt itself will lead to a biased prediction distribution. This raises our concern that whether the found better prompts are really with better knowledge extraction ability, or the better performance just come from the over-fitting between the promptonly distribution and the answer distribution of the test set.</p><p>To answer this question, we evaluate the KL divergence between the prompt-only distribution and the answer distribution of LAMA on different kinds of prompts. The results are shown in Table 2. We find that the KL divergence is a strong indicator of the performance of a prompt, i.e., the smaller the KL divergence between the promptonly distribution and the answer distribution of the test set is, the better performance the prompt achieve. Furthermore, Table <ref type="table">3</ref>: Examples of prompts that can achieve significant improvements on LAMA. We can see that the better performance actually stems from over-fitting: the better prompts are not prompts with a stronger semantic association to the relation.</p><p>their performance on LAMA. We can easily observe that the better-performed prompts are actually over-fitting the dataset, rather than better capturing the underlying semantic of the relation. As a result, previous prompt searching studies are actually optimized on the spurious prompt-dataset compatibility, rather than the universal factual knowledge extraction ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Case-based Analogy</head><p>The case-based analogy enhances the prompt-based paradigm with several illustrative cases. For example, if we want to know the "place-of-birth" of Steve Jobs, we would first sample cases such as (Obama, place-of-birth, Hawaii), and combine them with the original query. In this way, we will use "Obama was born in Hawaii.</p><p>[SEP] Steve Jobs was born in <ref type="bibr">[MASK]</ref>." to query MLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overall Conclusion</head><p>Conclusion 2. Illustrative cases guide MLMs to better recognizing object type, rather than better predicting facts.</p><p>To show this, we first design an effective algorithm to induce the type of an entity set based on Wikidata taxonomy, which can identify the object type of a relation. According to the induced types, we find that the benefits of illustrative cases mainly stem from the promotion of object type recognition. In other words, case-based analogy guides MLMs with better type prediction ability but contributes little to the entity prediction ability. In the following, we first illustrate our type inducing algorithm, and then explain how we reach the conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Entity Set Type Induction</head><p>To induce the object type of a relation, we first collect all its objects in LAMA and form an entity set. Then we induce the type of an entity set by designing a simple but effective algorithm. The main intuition behind our algorithm is that a representative type should be the finest grained type that can cover a sufficient number of the instances in the entity set. Figure <ref type="figure" target="#fig_5">5</ref> shows an example of our algorithm. Given a set of entities in Wikidata, we first construct an entity type graph (ETG) by recursively introducing all ancestor entity types according to the instance-of and subclass-of relations. For the example in Figure <ref type="figure" target="#fig_5">5</ref>, Chicago is in the entity set and is an instance-of Big City. Big City is a subclass-of City. As a result, Chicago, Big City and City will all be introduced into ETG. Then we apply topological sorting <ref type="bibr" target="#b4">(Cook, 1985)</ref> to ETG to obtain a Fine-to-Coarse entity type sequence. Finally, based on the sequence, we select the first type which covers more than 80% of entities in the entity set (e.g., City in Figure <ref type="figure" target="#fig_5">5</ref>). Table <ref type="table">4</ref> illustrates several induced types, and please refer to the Appendix for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Cases Help Type Recognition</head><p>Finding 4. Illustrative cases help MLMs to better recognize the type of objects, and therefore improve factual knowledge extraction.</p><p>For case-based analogy, the first thing we want to know is whether illustrative cases can improve the knowledge extraction performance. To this end, 25% 30% 35% 40% 45%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In-type Rank</head><p>Overall Rank Raised Unchanged Dropped</p><p>Figure <ref type="figure">6</ref>: Percentages on the change of overall rank (among all candidates) and the in-type rank (among candidates with the same type) of golden answer. We can see that the illustrative cases mainly raise the overall rank but cannot raise the in-type rank, which means the performance improvements mainly come from better type recognition.</p><p>for each (subject, relation) query in LAMA, we randomly sample 10 illustrative cases. To avoid answer leakage, we ensure the objects of these cases don't contain the golden answer of the query.</p><p>Then we use (cases, subject, prompt, [MASK]) as the analogous query to MLMs.</p><p>Results show that case-based analogy can significantly improve performance. After introducing illustrative cases, the mean precision increases from 30.36% to 36.23%. Besides, we find that 11.81% instances can benefit from the introduced cases and only 5.94% instances are undermined. This shows that case-based analogy really helps the MLMs to make better predictions.</p><p>By analyzing the predicting behaviors, we observe that the main benefit of introducing illustrative cases comes from the better type recognition. To verify this observation, we investigate how the types of predictions changed after introducing the illustrative cases. Table <ref type="table">4</ref> shows the results on relations whose precision improvement is more than 10% after introducing illustrative cases. From the table, it is very obvious that illustrative cases enhance the factual knowledge extraction by improving type prediction: 1) For queries whose predictions are correctly reversed (from wrong to right), the vast majority of them stems from the revised type prediction; 2) Even for queries whose predictions are mistakenly reversed (from right to wrong), the type of the majority of predictions still remains correct. In conclusion, introducing illustrative cases can significantly improve the knowledge extraction ability by recognizing the object type more accurately. That is, adding illustrative cases will provide more guidance for object type. Table <ref type="table">4</ref>: Detailed analysis on relations where the mean precision increased more than 10%. Precision ∆ and Type Prec. ∆ represents the precision changes on the answer and the type of the answer respectively. "w/ Type Change" and "w/o Type Change" represents the type of prediction changed/unchanged before/after introducing illustrative cases. "-" indicate there is no queries whose predictions are mistakenly reversed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Cases do not Help Entity Prediction</head><p>Finding 5. Illustrative cases are of limited help for selecting the answer from entities of the same type.</p><p>To show this, we introduce a new metric referred as in-type rank, which is the rank of the correct answer within the entities of the same type for a query. By comparing the in-type rank in prompt-based and case-based paradigm, we can evaluate whether the illustrative cases can actually help better entity prediction apart from better type recognition.</p><p>Figure <ref type="figure">6</ref> shows the percentages on the change of overall rank (among all candidates) and the in-type rank (among candidates with the same type) of golden answer. Unfortunately, we find that illustrative cases are of limited help for entity prediction: the change of in-type rank is nearly random. The percentages of queries with Raised/Unchanged/Dropped in-type rank are nearly the same: 33.05% VS 35.47% VS 31.47%. Furthermore, we find that the MRR with the type only changed from 0.491 to 0.494, which shows little improvement after introducing illustrative cases. These results show that the raises of overall rank of golden answer are not because of the better prediction inside the same type. In conclusion, illustrative cases cannot well guide the entity prediction, and they mainly benefit the factual knowledge extraction by providing guidance for object type recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Context-based Inference</head><p>The context-based inference augments the promptbased paradigm with external contexts. For example, if we want to know the "place-of-birth" of Steve Jobs, we can use the external context "Jobs was from California.", and form a context-enriched query "Jobs was from California.</p><p>[SEP] Steve Jobs was born in <ref type="bibr">[MASK]</ref>." to query MLMs. Specifically, we use the same context retrieval method as <ref type="bibr" target="#b29">Petroni et al. (2020)</ref>: for each instance, given the subject and relation as query, we use the first paragraph of DRQA's <ref type="bibr" target="#b3">(Chen et al., 2017)</ref> retrieved document as external contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Overall Conclusion Conclusion 3. Additional context helps MLMs</head><p>to predict the answer because they contain the answer, explicitly or implicitly.</p><p>Several studies <ref type="bibr" target="#b29">(Petroni et al., 2020;</ref><ref type="bibr" target="#b0">Bian et al., 2021)</ref> show that external context can help knowledge extraction from MLMs. To investigate the underlying mechanism, we evaluate which kinds of information in contexts contribute to the fact prediction, and find that the improvement mainly comes from the answer leakage in context. Furthermore, we find the answers can not only be leaked explicitly, but can also be leaked implicitly if the context provides sufficient information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Explicit Answer Leakage Helps</head><p>Finding 6. Explicit answer leakage significantly improves the prediction performance.</p><p>To show this, we split LAMA into two parts ac- cording to whether the additional context contains the answer. Table <ref type="table" target="#tab_3">5</ref> shows the results on these two parts respectively. We can see that the improvements on these two parts diverge significantly. For context containing the answer, context-based inference significantly improves the factual knowledge extraction performance. However, there is even a little performance drop for those instances whose context does not contain the answer. This indicates that the improvement of factual knowledge extraction is mainly due to the explicit existence of the answer in the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Implicit Answer Leakage Helps</head><p>Finding 7. Implicit answer leakage can also significantly improve the prediction performance. As we mentioned above, explicit answer leakage significantly helps the answer prediction. The answer-leaked context may explicitly provide the answer or implicitly guide the prediction by providing answer-specific information. To understanding the underlying mechanism, we mask the answer in the context and verify whether it can still achieve the performance gain.</p><p>Table <ref type="table" target="#tab_4">6</ref> shows the results. We find that the performance gain is still very significant after masking the answer. This indicates that the contexts previously containing the answer are still very effective even the answer doesn't explicitly present. To further investigate the reason behind, we split the masked version of answer-leaked instances into two groups by whether MLMs can or cannot correctly reconstruct the masked answer from the re-maining context. The results are shown in Table <ref type="table" target="#tab_5">7</ref>. We can see that the performance gain significantly diverges in these two groups: the improvements mainly come from the instances whose answer in context can be reconstructed -we refer to this as implicit answer leakage. That is to say, for these instances, the context serves as a sufficient delegator of its answer, and therefore MLMs can obtain sufficient answer evidence even the answer does not explicitly appear. However, for contexts that cannot reconstruct the masked answer, the improvements are relatively minor. In conclusion, the real efficacy of context-based inference comes from the sufficient answer evidence provided by the context, either explicitly or implicitly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Discussions</head><p>In this paper, we thoroughly study the underlying mechanisms of MLMs on three representative factual knowledge extraction paradigms. We find that the prompt-based retrieval is severely promptbiased, illustrative cases enhance MLMs mainly via type guidance, and external contexts help knowledge prediction mostly because they contain the correct answer, explicitly or implicitly. These findings strongly question previous conclusions that current MLMs could serve as reliable factual knowledge bases.</p><p>The findings of this paper can benefit the community in many directions. By explaining the underlying predicting mechanisms of MLMs, we provide reliable explanations for many previous knowledgeintensive techniques. For example, our method can explain why and how incorporating external contexts will help knowledge extraction and Common-senseQA <ref type="bibr" target="#b39">(Talmor et al., 2019)</ref>. Our findings also reveal why PLM probing datasets may not be reliable and how the evaluation can be promoted by designing de-biased evaluation datasets.</p><p>This paper also sheds light on future research directions. For instance, knowing the main benefit of illustrative cases comes from type-guidance, we can enhance many type-centric prediction tasks such as NER <ref type="bibr" target="#b20">(Lample et al., 2016)</ref> and factoid QA <ref type="bibr" target="#b14">(Iyyer et al., 2014)</ref>. Moreover, based on the mechanism of incorporating external contexts, we can better evaluate, seek, and denoise external contexts for different tasks using MLMs. For example, we can assess and select appropriate facts for CommonsenseQA based on whether they can reconstruct the candidate answers.</p><p>This paper focuses on masked language models, which have been shown very effective and are widely used. We also want to investigate another representative category of language models -the generative pre-trained models (e.g., GPT2/3 <ref type="bibr" target="#b32">(Radford et al., 2019;</ref><ref type="bibr" target="#b2">Brown et al., 2020)</ref>), which have been shown to have quite different mechanisms and we leave it for future work due to page limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A WIKI-UNI Construction Details</head><p>To construct WIKI-UNI, we first collect all the triples which belong to the same 41 relations with LAMA from Wikidata <ref type="bibr" target="#b42">(Vrandečić and Krötzsch, 2014)</ref>, then we randomly sample 50K triples with a single-token object for each relation. Similar to LAMA, we filter out the instances whose object is not in MLMs' vocabulary. For each relation, we group the instances based on different objects, and indicate f o as the frequency of each object. We denote the median of f o with f m . For groups where f o &gt; f m , we randomly sample f m instances, and delete the groups where f o &lt; f m . Therefore, we acquire a dataset named WIKI-UNI with a uniform answer distribution. There are 70K facts in WIKI-UNI and 34K facts in LAMA. Since BERT and RoBERTa have a different vocabulary, so the datasets for their evaluation are slightly different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Results on RoBERTa-large</head><p>Our conclusions are similar on BERT-large and RoBERTa-large, therefore, we report the results of BERT-large in the article and results of RoBERTalarge here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Promp-based Retrieval</head><p>Figure <ref type="figure" target="#fig_6">7</ref> shows the very significant correlation between the prediction distributions on LAMA and WIKI-UNI for RoBERTa-large: on all three kinds of prompts, the Pearson correlation coefficient between these two prediction distributions exceeds 0.9 in most relations. Table <ref type="table" target="#tab_6">8</ref> shows the percentage of instances that the topk object entities cover for RoBERTa-large. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Case-based Analogy</head><p>Table <ref type="table">9</ref> shows the performance improvement after introducing illustrative cases for RoBERTa-large model, we can see that the illustrative cases could also significantly increase the knowledge extraction performance for RoBERTa-large. Table <ref type="table" target="#tab_12">14</ref> shows how the entity types of predictions changed after introducing the illustrative cases for RoBERTa-large model, the conclusion is similar with BERT-large. Figure <ref type="figure" target="#fig_7">8</ref> shows the percentage on the change of overall rank and in-type rank for RoBERTa-large model.</p><p>And another finding is that BERT-large has a better type prediction ability than RoBERTa-large, even without illustrative cases. We calculate the overall type precision over prompt-based paradigm (the percentage of predictions that the type is correct). And the type precision for BERT-large is 68% and for RoBERTa-large is only 51%, which partly explains why performance of RoBERTa-large is significantly worse than BERT-large on LAMA dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Enhanced</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Context-based Inference</head><p>Table <ref type="table" target="#tab_8">10</ref> shows the comparison of contexts group by whether the contexts contain the answer for RoBERTa-large. We can see that for contexts containing the answer, context-based inference significantly improves the factual extraction performance. Meanwhile, there is a performance drop for those instances whose context does not contain the answer. Table <ref type="table" target="#tab_9">11</ref> shows the overall performance improvements when introducing different external contexts for RoBERTa-large. Table <ref type="table" target="#tab_10">12</ref> shows the comparison of the masked contexts based on whether they can/cannot reconstruct the masked answer for RoBERTa-large. The improvements mainly comes from the instances whose answer in contexts can be reconstructed.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Full Version of the Type Prediction Results</head><p>Table <ref type="table" target="#tab_11">13</ref> shows the detailed analysis of all relations using case-based analogy paradigm for BERTlarge and Table <ref type="table" target="#tab_12">14</ref> is the results on RoBERTalarge. Because of the page limit, another finding we didn't mention in the article is that, apart from "type guidance", the illustrative cases could also provide a "surface form guidance" in a few relations (e.g., part of, applies to jurisdiction, subclass of). Specifically, the "surface form" indicate that the object entity name (e.g., Apple) is a substring of the subject entity name (e.g., Apple Watch). Such phenomenon is also mentioned in <ref type="bibr" target="#b31">Poerner et al. (2020)</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: This paper explores three different kinds of factual knowledge extraction paradigms from MLMs, and reveal the underlying predicting mechanisms behind them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>However, the prediction distribution made by MLMs on them are still very similar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustration example of the vastly different answer distributions but similar prediction distributions on LAMA and WIKI-UNI on "place-of-birth" relation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Correlations of the prediction distributions on LAMA and WIKI-UNI. Even these two datasets have totally different answer distributions, MLMs still make highly correlated predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Correlations between the prompt-only distribution and prediction distribution on WIKI-UNI. MLMs make correlated predictions w. or w/o. subjects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Illustration of our type induction algorithm.The numbers on the right of each type indicate how many entities does the type cover. The type of an entity set is the finest grained type in the type graph that can cover a sufficient number of the instances in the entity set, which is City in the example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The correlations of the prediction distribution on LAMA and WIKI-UNI for RoBERTa-large.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Percentages on the change of overall rank (among all candidates) and the in-type rank (among candidates with the same type) of golden answer of RoBERTa-large model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Average percentage of instances being cov-</figDesc><table><row><cell cols="4">Distribution Datasets Top1 Top3 Top5 Precision</cell></row><row><cell>Answer</cell><cell>LAMA WIKI-UNI</cell><cell>22.04 39.37 48.03 1.68 5.03 7.78</cell><cell>--</cell></row><row><cell>Prediction</cell><cell cols="2">LAMA WIKI-UNI 27.12 44.19 52.18 31.09 49.21 57.93</cell><cell>30.36 16.47</cell></row></table><note>ered by top-k answers or predictions. For answer distribution, top-5 objects in LAMA cover 6.2 times of instances than that in WIKI-UNI, however, for prediction distribution, they are almost the same. As a result, the precision is significantly dropped in WIKI-UNI.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Table3shows several comparisons between different kinds of prompts and The smaller KL divergence between the prompt-only distribution and golden answer distribution of LAMA, the better performance of the prompt.</figDesc><table><row><cell cols="3">Prompt Precision KL divergence</cell></row><row><cell>T man</cell><cell>30.36</cell><cell>12.27</cell></row><row><cell>T mine</cell><cell>39.49</cell><cell>10.40</cell></row><row><cell>T auto</cell><cell>40.36</cell><cell>10.27</cell></row><row><cell>Relation</cell><cell>Prompt</cell><cell>Source Prec. KL.</cell></row><row><cell>citizenship</cell><cell>x is y citizen x returned to y</cell><cell>T man T mine 43.58 6.32 0.00 24.67</cell></row><row><cell>work location</cell><cell cols="2">x used to work in y T man x was born in y T mine 40.25 2.21 11.01 19.07</cell></row><row><cell>instance of</cell><cell>x is a y x is a small y</cell><cell>T man T mine 52.60 13.98 30.15 22.98</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Comparison between prompt-based and context-based paradigms grouped by whether the answer presents or absents in the context. We can see that only contexts containing the answer can significantly improve the performance.</figDesc><table><row><cell>Answer in context</cell><cell cols="2">Prompt-based Context-based</cell><cell>∆</cell></row><row><cell>Present (45.30%)</cell><cell>34.83</cell><cell>64.13</cell><cell>+29.30</cell></row><row><cell>Absent (54.70 %)</cell><cell>25.37</cell><cell>23.26</cell><cell>-2.11</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Overall performance when introducing different kinds of contexts. "Masked Context-based" indicates that we mask the golden answer in contexts, and there is still a significant performance improvement.</figDesc><table><row><cell cols="4">Prompt-based Context-based Masked Context-based</cell></row><row><cell>30.36</cell><cell>41.44</cell><cell>35.66</cell><cell></cell></row><row><cell>Answer Reconstructable</cell><cell cols="2">Prompt-based Context-based</cell><cell>∆</cell></row><row><cell>Reconstructable (60.23%)</cell><cell>39.58</cell><cell>60.82</cell><cell>+21.24</cell></row><row><cell>Not-reconstructable (39.77 %)</cell><cell>28.84</cell><cell>35.83</cell><cell>+6.99</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Comparison between prompt-based and context-based paradigms grouped by whether the masked answer in the context can be reconstructed from the remaining context. We can see that contexts can reconstruct the masked answer is more likely to improve the performance.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>The percentage of instances that the topk object entities cover for RoBERTa-large. The statistics is different from Table1because we filter LAMA with RoBERTa's vocabulary when evaluate RoBERTa-large.</figDesc><table><row><cell>Distribution</cell><cell>Datasets</cell><cell cols="2">Top1 Top3 Top5 Prec.</cell></row><row><cell>Answer</cell><cell cols="2">LAMA WIKI-UNI 1.84 23.93 42.02 50.08 5.53 8.61</cell><cell>--</cell></row><row><cell>Prediction</cell><cell cols="3">LAMA WIKI-UNI 36.53 55.51 63.58 13.59 37.48 56.85 65.45 23.65</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 :</head><label>10</label><figDesc>Comparison of contexts grouped by whether the answer presents or absents for RoBERTa-large.</figDesc><table><row><cell>Answer in context</cell><cell cols="2">Prompt-based Context-based</cell><cell>∆</cell></row><row><cell>Present (46.04%)</cell><cell>27.95</cell><cell>52.05</cell><cell>+24.10</cell></row><row><cell>Absent (53.96 %)</cell><cell>18.95</cell><cell>14.72</cell><cell>-4.23</cell></row><row><cell cols="4">Without Contexts Full Contexts Masked Contexts</cell></row><row><cell>23.65</cell><cell>31.44</cell><cell cols="2">24.44</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11 :</head><label>11</label><figDesc>The overall performance when introducing different contexts for RoBERTa-large.</figDesc><table><row><cell>Answer Reconstructable</cell><cell cols="2">Prompt-based Context-based</cell><cell>∆</cell></row><row><cell>Reconstructable (61.23%)</cell><cell>30.50</cell><cell>42.37</cell><cell>+11.87</cell></row><row><cell>Not-reconstructable (38.77 %)</cell><cell>22.19</cell><cell>22.15</cell><cell>-0.04</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 12 :</head><label>12</label><figDesc>Comparison of the masked contexts based on whether they can/cannot reconstruct the masked answer for RoBERTa-large.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 13 :</head><label>13</label><figDesc>A detailed analysis of all relations using case-based analogy paradigm for BERT-large, which is corresponding to Table4in the article. "-" indicates the number of queries whose predictions are reversed correctly or mistakenly is less than 3.</figDesc><table><row><cell>Relation</cell><cell>Induced Object Type</cell><cell>Precision ∆</cell><cell>Type Prec. ∆</cell><cell>Wrong → Right w/ Type Change</cell><cell>Right → Wrong w/o Type Change</cell></row><row><cell>named after</cell><cell>physical object</cell><cell>68.06</cell><cell>98.91</cell><cell>99.77</cell><cell>-</cell></row><row><cell>country of citizenship</cell><cell>sovereign state</cell><cell>43.37</cell><cell>84.16</cell><cell>100.00</cell><cell>-</cell></row><row><cell>position held</cell><cell>religious servant</cell><cell>36.88</cell><cell>80.26</cell><cell>91.15</cell><cell>90.00</cell></row><row><cell>religion</cell><cell>religion</cell><cell>33.20</cell><cell>34.88</cell><cell>100.00</cell><cell>-</cell></row><row><cell>work location</cell><cell>city</cell><cell>26.10</cell><cell>70.55</cell><cell>85.04</cell><cell>100.00</cell></row><row><cell>instrument</cell><cell>musical instrument</cell><cell>17.07</cell><cell>55.75</cell><cell>89.08</cell><cell>75.00</cell></row><row><cell>country</cell><cell>sovereign state</cell><cell>14.30</cell><cell>29.04</cell><cell>88.48</cell><cell>87.93</cell></row><row><cell>employer</cell><cell>business</cell><cell>12.01</cell><cell>99.22</cell><cell>100.00</cell><cell>-</cell></row><row><cell>continent</cell><cell>continent</cell><cell>10.87</cell><cell>51.18</cell><cell>96.86</cell><cell>88.24</cell></row><row><cell>languages spoken, written or signed</cell><cell>Indo-European languages</cell><cell>9.91</cell><cell>-0.93</cell><cell>10.56</cell><cell>81.54</cell></row><row><cell>applies to jurisdiction</cell><cell>state</cell><cell>8.71</cell><cell>-6.13</cell><cell>7.23</cell><cell>63.64</cell></row><row><cell>country of origin</cell><cell>sovereign state</cell><cell>8.36</cell><cell>33.22</cell><cell>71.64</cell><cell>98.28</cell></row><row><cell>subclass of</cell><cell>object</cell><cell>7.68</cell><cell>27.28</cell><cell>66.18</cell><cell>87.10</cell></row><row><cell>part of</cell><cell>object</cell><cell>7.51</cell><cell>37.66</cell><cell>54.27</cell><cell>97.87</cell></row><row><cell>language of work or name</cell><cell>Indo-European languages</cell><cell>6.05</cell><cell>10.95</cell><cell>77.23</cell><cell>77.08</cell></row><row><cell>location of formation</cell><cell>city</cell><cell>5.02</cell><cell>66.34</cell><cell>80.77</cell><cell>100.00</cell></row><row><cell>has part</cell><cell>abstract object</cell><cell>5.02</cell><cell>27.26</cell><cell>25.33</cell><cell>100.00</cell></row><row><cell>genre</cell><cell>series</cell><cell>4.62</cell><cell>17.61</cell><cell>95.45</cell><cell>-</cell></row><row><cell>owned by</cell><cell>organization</cell><cell>2.62</cell><cell>11.50</cell><cell>9.57</cell><cell>100.00</cell></row><row><cell>instance of</cell><cell>concrete object</cell><cell>2.06</cell><cell>4.34</cell><cell>35.80</cell><cell>96.77</cell></row><row><cell>occupation</cell><cell>profession</cell><cell>1.35</cell><cell>-0.53</cell><cell>0.00</cell><cell>100.00</cell></row><row><cell>place of death</cell><cell>city</cell><cell>1.26</cell><cell>16.37</cell><cell>68.63</cell><cell>100.00</cell></row><row><cell>twinned administrative body</cell><cell>city</cell><cell>0.91</cell><cell>0.80</cell><cell>15.38</cell><cell>75.00</cell></row><row><cell>diplomatic relation</cell><cell>sovereign state</cell><cell>0.80</cell><cell>1.11</cell><cell>10.00</cell><cell>100.00</cell></row><row><cell>native language</cell><cell>Indo-European languages</cell><cell>0.20</cell><cell>0.62</cell><cell>38.64</cell><cell>92.86</cell></row><row><cell>manufacturer</cell><cell>business</cell><cell>-1.02</cell><cell>0.31</cell><cell>33.33</cell><cell>61.29</cell></row><row><cell>field of work</cell><cell>knowledge</cell><cell>-1.15</cell><cell>0.00</cell><cell>26.09</cell><cell>90.32</cell></row><row><cell>developer</cell><cell>enterprise</cell><cell>-1.52</cell><cell>1.52</cell><cell>4.17</cell><cell>96.97</cell></row><row><cell>location</cell><cell>community</cell><cell>-1.57</cell><cell>4.59</cell><cell>3.03</cell><cell>100.00</cell></row><row><cell>capital</cell><cell>city</cell><cell>-2.00</cell><cell>0.14</cell><cell>4.55</cell><cell>97.22</cell></row><row><cell>position played on team / speciality</cell><cell>position</cell><cell>-4.10</cell><cell>11.03</cell><cell>-</cell><cell>100.00</cell></row><row><cell>headquarters location</cell><cell>city</cell><cell>-4.24</cell><cell>0.62</cell><cell>0.00</cell><cell>100.00</cell></row><row><cell>official language</cell><cell>Nostratic languages</cell><cell>-5.28</cell><cell>-1.14</cell><cell>5.45</cell><cell>90.57</cell></row><row><cell>original language of film or TV show</cell><cell>Nostratic languages</cell><cell>-5.84</cell><cell>-16.71</cell><cell>19.15</cell><cell>43.30</cell></row><row><cell>place of birth</cell><cell>city</cell><cell>-6.25</cell><cell>4.34</cell><cell>14.29</cell><cell>100.00</cell></row><row><cell>capital of</cell><cell>political territorial entity</cell><cell>-6.84</cell><cell>0.42</cell><cell>-</cell><cell>100.00</cell></row><row><cell>shares border with</cell><cell>community</cell><cell>-7.37</cell><cell>2.72</cell><cell>2.22</cell><cell>97.35</cell></row><row><cell>record label</cell><cell>record label</cell><cell>-7.93</cell><cell>-22.38</cell><cell>-</cell><cell>0.00</cell></row><row><cell>original network</cell><cell>television station</cell><cell>-10.56</cell><cell>0.45</cell><cell>11.36</cell><cell>86.13</cell></row><row><cell cols="2">located in the administrative territorial entity community</cell><cell>-12.94</cell><cell>11.69</cell><cell>10.53</cell><cell>99.25</cell></row><row><cell>member of</cell><cell>organization</cell><cell>-14.67</cell><cell>16.45</cell><cell>94.74</cell><cell>98.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 14 :</head><label>14</label><figDesc>A detailed analysis of all relations using case-based analogy paradigm for RoBERTa-large, which is corresponding to Table4in the article. "-" indicates the number of queries whose predictions are reversed correctly or mistakenly is less than 3.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">This paper shows the experimental results on BERT-large because previous work has shown that it can achieve the best performance on factual knowledge extraction among all MLMs. In the Appendix, we also report the experimental results on RoBERTa-large, which also reach the main conclusions reported in the paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"> www.wikidata.org   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">Since we focus on factual knowledge, we use the T-REx<ref type="bibr" target="#b7">(Elsahar et al., 2018)</ref> subset of the LAMA benchmark.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We sincerely thank all anonymous reviewers for their insightful comments and valuable suggestions. This work is supported by the National Key Research and Development Program of China (No. 2020AAA0106400), the National Natural Science Foundation of China under Grants no. U1936207, and in part by the Youth Innovation Promotion Association CAS(2018141).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Benchmarking Knowledge-Enhanced Commonsense Question Answering via Knowledge-to-Text Transformation</title>
		<author>
			<persName><forename type="first">Xianpei</forename><surname>Ning Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00760</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<author>
			<persName><forename type="first">Zied</forename><surname>Bouraoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Schockaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07">2020. February 7-12, 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="7456" to="7463" />
		</imprint>
	</monogr>
	<note>The Thirty-Fourth AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<idno>De- cember 6-12</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mc-Candlish, Alec Radford; NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020. 2020. 2020</date>
		</imprint>
	</monogr>
	<note>Ilya Sutskever, and Dario Amodei</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reading Wikipedia to answer opendomain questions</title>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1171</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A taxonomy of problems with fast parallel algorithms</title>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Cook</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0019-9958(85)80041-3</idno>
	</analytic>
	<monogr>
		<title level="j">Information and Control</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="22" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Commonsense knowledge mining from pretrained models</title>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1109</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1173" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">T-REx: A large scale alignment of natural language with knowledge base triples</title>
		<author>
			<persName><forename type="first">Hady</forename><surname>Elsahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavlos</forename><surname>Vougiouklis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arslen</forename><surname>Remaci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederique</forename><surname>Laforest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Simperl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
				<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models</title>
		<author>
			<persName><forename type="first">Allyson</forename><surname>Ettinger</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00298</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="34" to="48" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02899</idno>
		<title level="m">Do Neural Language Representations Learn Physical Commonsense</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15723</idno>
		<title level="m">Making Pre-trained Language Models Better Few-shot Learners</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Assessing BERT&apos;s Syntactic Abilities</title>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.05287</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A structural probe for finding syntax in word representations</title>
		<author>
			<persName><forename type="first">John</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1419</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4129" to="4138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Do Attention Heads in BERT Track Syntactic Dependencies?</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phu Mon Htut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikha</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bordia</surname></persName>
		</author>
		<author>
			<persName><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12246</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A neural network for factoid question answering over paragraphs</title>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Claudino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
				<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="633" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">X-FACTR: Multilingual factual knowledge retrieval from pretrained language models</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibo</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.479</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020a</date>
			<biblScope unit="page" from="5943" to="5959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">How can we know what language models know? Transactions of the</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00324</idno>
		<imprint>
			<date type="published" when="2020-06">Jun Araki, and Graham Neubig. 2020b</date>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="423" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Are pretrained language models symbolic reasoners over knowledge?</title>
		<author>
			<persName><forename type="first">Nora</forename><surname>Kassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benno</forename><surname>Krojer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.conll-1.45</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Conference on Computational Natural Language Learning</title>
				<meeting>the 24th Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="552" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BERT-kNN: Adding a kNN search component to pretrained language models for better QA</title>
		<author>
			<persName><forename type="first">Nora</forename><surname>Kassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.307</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
				<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020a</date>
			<biblScope unit="page" from="3424" to="3430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly</title>
		<author>
			<persName><forename type="first">Nora</forename><surname>Kassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.698</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020b</date>
			<biblScope unit="page" from="7811" to="7818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n16-1030</idno>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2016-06-12">2016. June 12-17, 2016</date>
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00190</idno>
		<title level="m">Prefix-Tuning: Optimizing Continuous Prompts for Generation</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-Trained Language Models</title>
		<author>
			<persName><forename type="first">Seyeon</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.557</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6862" to="6868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Open sesame: Getting inside BERT&apos;s linguistic knowledge</title>
		<author>
			<persName><forename type="first">Yongjie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chern Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Frank</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-4825</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
				<meeting>the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="241" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Linguistic knowledge and transferability of contextual representations</title>
		<author>
			<persName><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1112</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Language Models as Few-Shot Learner for Task-Oriented Dialogue Systems</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaojiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.06239</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1334</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3428" to="3448" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">How context affects language models&apos; factual predictions</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><surname>Riedel</surname></persName>
		</author>
		<idno type="DOI">10.24432/C5201W</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Automated Knowledge Base Construction, AKBC 2020, Virtual</title>
				<imprint>
			<date type="published" when="2020-06-22">2020. June 22-24, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1250</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">E-BERT: Efficient-yet-effective entity embeddings for BERT</title>
		<author>
			<persName><forename type="first">Nina</forename><surname>Poerner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulli</forename><surname>Waltinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.71</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
				<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">How much knowledge can you pack into the parameters of a language model?</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.437</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5418" to="5426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.11926</idno>
		<title level="m">Few-Shot Text Generation with Pattern-Exploiting Training</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">It&apos;s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07118</idno>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Exploiting cloze-questions for few-shot text classification and natural language inference</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main</title>
				<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-04-19">2021. 2021. April 19 -23, 2021</date>
			<biblScope unit="page" from="255" to="269" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.346</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4222" to="4235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">you are grounded!&quot;: Latent name artifacts in pre-trained language models</title>
		<author>
			<persName><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.556</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6850" to="6861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Commonsenseqa: A question answering challenge targeting commonsense knowledge</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1421</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4149" to="4158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pretraining is (almost) all you need: An application to commonsense reasoning</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Tamborrino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Pellicanò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Pannier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Voitot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louise</forename><surname>Naudin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.357</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3878" to="3887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">What do you learn from context? probing for sentence structure in contextualized word representations</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Poliak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Thomas</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Najoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Wikidata: A free collaborative knowledgebase</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Vrandečić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Krötzsch</surname></persName>
		</author>
		<idno type="DOI">10.1145/2629489</idno>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Do NLP models know numbers? probing numeracy in embeddings</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1534</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5307" to="5315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11967</idno>
		<title level="m">Language Models are Open Knowledge Graphs</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Investigating BERT&apos;s knowledge of language: Five analysis methods with NPIs</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Grosu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hagen</forename><surname>Blix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yining</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Alsop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikha</forename><surname>Bordia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haokun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Parrish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng-Fu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anhad</forename><surname>Mohananey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mon</forename><surname>Phu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paloma</forename><surname>Htut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Jeretic</surname></persName>
		</author>
		<author>
			<persName><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1286</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2877" to="2887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Xuhui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dandan</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11931</idno>
		<title level="m">Evaluating Commonsense in Pretrained Language Models</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
