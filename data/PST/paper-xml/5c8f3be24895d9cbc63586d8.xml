<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">H</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">X</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">D</forename><surname>Ni</surname></persName>
						</author>
						<author>
							<persName><forename type="first">S</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">T</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">B</forename><surname>Lei</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="laboratory" key="lab1">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="laboratory" key="lab2">Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<postCode>518060</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Industrial and Manufacturing</orgName>
								<orgName type="department" key="dep2">Systems Engineering</orgName>
								<orgName type="institution">The University of Michigan</orgName>
								<address>
									<settlement>Dearborn</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5921C3E3AB6BF7BE0261318BC51F2172</idno>
					<idno type="DOI">10.1109/JBHI.2018.2859898</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Skin lesion segmentation</term>
					<term>Dermoscopy image</term>
					<term>Dense deconvolutional layer</term>
					<term>Chained residual pooling</term>
					<term>Hierarchical supervision</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic delineation of skin lesion contours from dermoscopy images is a basic step in the process of diagnosis and treatment of skin lesions. However, it is a challenging task due to the high variation of appearances and sizes of skin lesions. In order to deal with such challenges, we propose a new dense deconvolutional network (DDN) for skin lesion segmentation based on residual learning. Specifically, the proposed network consists of dense deconvolutional layers (DDLs), chained residual pooling (CRP), and hierarchical supervision (HS). First, unlike traditional deconvolutional layers, DDLs are adopted to maintain the dimensions of the input and output images unchanged. The DDNs are trained in an end-to-end manner without the need of prior knowledge or complicated post-processing procedures. Second, the CRP aims to capture rich contextual background information and to fuse multi-level features. By combining the local and global contextual information via multi-level feature fusion, the high-resolution prediction output is obtained. Third, HS is added to serve as an auxiliary loss and to refine the prediction mask. Extensive experiments based on the public ISBI 2016 and 2017 skin lesion challenge datasets demonstrate the superior segmentation results of our proposed method over the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>he number of skin cancer cases has increased substantially in the recent years <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. According to the American Cancer Society, about 76,380 cases of melanomas (i.e., a type of skin cancer) have been reported and it has reached 75% death rate in 2016 <ref type="bibr" target="#b2">[3]</ref>. Hence, early diagnosis is very important for timely treatment to reduce the death rate. To fight skin cancer via melanoma diagnosis, dermoscopic imaging techniques have been widely used in clinical examinations. To determine whether lesions are melanoma or not, non-invasive skin imaging tools are commonly practiced <ref type="bibr" target="#b3">[4]</ref>. However, the clinician's decision mainly depends on his/her experience and the so-called "ugly duckling" sign <ref type="bibr" target="#b4">[5]</ref>, which is quite subjective and unstable. Also, dermatologists with limited experience may not be able to accurately distinguish melanoma from normal skins with the dermoscopic tool <ref type="bibr" target="#b3">[4]</ref>. Accordingly, automatic methods based on the dermoscopic imaging technique <ref type="bibr" target="#b5">[6]</ref> are highly preferred. Motivated by this, many automatic algorithms have been proposed, including support vector machine (SVM) <ref type="bibr" target="#b5">[6]</ref>, AdaBoost <ref type="bibr" target="#b6">[7]</ref>, deep polynomial networks <ref type="bibr" target="#b7">[8]</ref>, and K-nearest neighbor <ref type="bibr" target="#b8">[9]</ref>. However, automatic skin lesion segmentation is not trivial due to the significant variation in appearances (e.g., shape, color, and size) of skin cancers. The blurred boundaries of some lesions due to irregular edges, low contrast, etc., may further render this task challenging. As illustrated in Fig. <ref type="figure">1</ref>, the artifacts, including intrinsic skin features (e.g., natural hair, blood vessels) and artificial artifacts (e.g., air bubbles, ruler markers, and color variants), make automating skin lesion segmentation even more difficult.</p><p>To address these challenges, deep learning models (e.g., convolutional neural networks (CNNs)) have been widely applied due to their impressive performance of the semantic segmentation <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>. However, serial combinations of convolution strides and pooling reduce the resolution of the final output. Hence, these methods may not be able to provide an end-to-end training that maintains the same dimensions of the input and the output. To achieve an end-to-end training for segmentation, several studies developed techniques of up-sampling operation, dilated convolution, and post-processing procedures <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref> to resize the learnt feature maps to the dimensions of the output (i.e., label map). The main purpose of using the up-sampling technique is to enlarge the resolution of the feature maps by expanding the width and the height, which achieves the same dimensions as the original input images. The interpolation and deconvolution methods (a.k.a., transposed convolution) are commonly used for the up-sampling operation. Fully convolutional networks (FCNs) <ref type="bibr" target="#b16">[17]</ref> have extended the traditional CNNs and become one of the most representative models with bilinear interpolation to restore the size of the original input image. However, when bilinear interpolation is used to expand image resolutions, it is unable to automatically learn image features by merging multiple input maps into a single output. By contrast, deconvolution is a mapping from a single input to multiple outputs <ref type="bibr" target="#b19">[20]</ref>. However, direct relationship among adjacent pixels of a feature map is not maintained in the conventional deconvolution layer. Therefore, the 'checkerboard' issue may occur.</p><p>To address this issue, many enhanced methods have been proposed in the past years by incorporating conditional random field (CRF) <ref type="bibr" target="#b20">[21]</ref>, which is a learning based method that considers the spatial relations among pixels. However, it is difficult for traditional CRF to learn high order semantic information in an end-to-end way. As a result, improvement with the CRF post-processing is quite limited. In order to overcome these limitations, RefineNet <ref type="bibr" target="#b21">[22]</ref> is proposed, which adopts a multi-path network to extract information generated in the down-sampling process. RefineNet can obtain high-resolution predictions using long-range residual skip connection with a corresponding residual block. However, the resolution is downgraded by repeated down-sampling operations, which make RefineNet unappealing.</p><p>To further enhance RefineNet, we propose a novel end-to-end framework called dense deconvolutional networks (DDNs) for skin lesion segmentation inspired by <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> in this paper. The DDN follows an encoding-decoding <ref type="bibr" target="#b23">[24]</ref> pipeline without prior knowledge of the input data or complicated post-processing procedures. In the encoding phase, we adopt ResNet to extract semantic information (i.e., either the skin lesion or background). The decoding phase consists of three parts, i.e., a residual block for weight adjustment, a chained residual pooling (CRP) layer, and a dense deconvolutional layer (DDL). In addition, deep hierarchical supervision (HS) is added to further improve the performance. Overall, our main contribution is three-folds:</p><p>1) Devise a CRP to expand the receptive field, capture global information, and to enhance the robustness of the model;</p><p>2) Propose a DDL to solve the 'checkerboard' issue and ambiguous boundary shapes. By establishing a direct relationship among adjacent pixel values of a feature map, DDL can recover the detailed information;</p><p>3) Exploit a deep HS to insert two side-output layers on the last two boundary refinement blocks.</p><p>Our proposed DDN framework has the ability to learn rich hierarchical features by leveraging local and global contextual information. Extensive experiments on the public skin lesion challenge datasets demonstrate the effectiveness of our proposed method for skin lesion segmentation.</p><p>The rest of this paper is organized as follows. The related work is presented in Section II. Section III introduces the proposed network architecture in detail. The experiments and comparison results are illustrated in Section IV. Our discussions are given in Section V. Finally, our conclusions are presented in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Semantic segmentation is commonly used to assign labels to every pixel in the region of interest and to delineate the contour of skin lesions <ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref>. Typically, it is regarded as a dense pixel classification task. However, there are two existing challenges in the task of semantic segmentation: classification and localization. To improve the performance of the classification, the model needs to be insensitive to position information. On the contrary, the segmentation needs to be extremely sensitive to the position information. Since the traditional classification methods mainly use hand-crafted features <ref type="bibr" target="#b29">[30]</ref>, they are unable to stably and fully learn feature representation. To solve the contradiction problem in the image segmentation task, deep learning methods using CNNs have been proposed and achieved remarkable performance <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref>. For instance, the classical deep neural networks (e.g., AlexNet, VggNet, GoogleNet, and ResNet) have been widely applied with impressive performance <ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref>. However, there are mainly two problems in the task of melanoma segmentation using deep CNNs. The first problem lies in the fully-connected layers, which requires a fixed image size. The second problem is the pooling operation and stride in the convolution operation, which downscales the feature size and loses detailed position information. As a result, multiple con-Benign GT Melanoma GT Fig. <ref type="figure">2</ref>. Flowchart of proposed method including an encoding module and a decoding module. We utilize ResNet model to capture multi-level and multi-scale features in the process of encoding. In decoding process, we restore feature response information with four cascaded DDNs. A DDN consists of three parts: residual convolution unit, chained residual pooling and dense deconvolutional layer. HS is added to refine coarse output. Whole pipeline has a long-range connection between residual block and DDN. Size of input is 400√ó400, after "7√ó7 conv" and "3√ó3 max pool", the size of Res1-3, Res4-7, Res8-13 and Res14-16 are 100√ó100, 50√ó50, 25√ó25 and 13√ó13, respectively. volution and pooling operations are unable to improve segmentation accuracy. Many studies have tried to maintain the original size of the input image by using patch-wise images as input for segmenting medical images <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>. However, every patch-wise image contains limited contextual information and ignores global information. If the size of an input patch is enlarged to contain more global information, it results in coarse structures and vague boundaries. Therefore, studies were proposed to generate high-resolution outputs <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b41">42]</ref>. For example, Long et al. <ref type="bibr" target="#b16">[17]</ref> first proposed to transform fully-connected layers into fully-convolutional layers to complete pixel-to-pixel mapping. However, the five max-pooling layers compressed the original input by a factor of 32. To reduce this effect, CRF followed by the convolutional operation was utilized in <ref type="bibr" target="#b20">[21]</ref> to refine segmentation output and obtain clear boundary maps. There are also some other studies used to refine the results of segmentation using CRF. Zheng et al. <ref type="bibr" target="#b42">[43]</ref> proposed an iterative process of CRF, which is actually regarded as recurrent neural networks. Later, Chen et al. <ref type="bibr" target="#b43">[44]</ref> took advantage of the extracted coarse segmentation and then fed these feature maps as the input into the CRF model. However, the post-processing algorithm seems sophisticated and confusing. Moreover, it is possible to optimize segmentation results from the learned model itself.</p><p>To replace the conventional operation to outline a vivid segmentation output map, the DeepLabV2 was proposed to utilize a dilated convolution <ref type="bibr" target="#b17">[18]</ref>. The advantage of the dilated convolution is that, it increases the receptive field without pooling and captures a large amount of contextual information <ref type="bibr" target="#b44">[45]</ref>. Hence, many studies have adopted the dilated convolution <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref>, which can keep the original resolution and increase the receptive field, simultaneously. However, the resulting output (i.e., prediction score map) is still reduced to 1/8 of the original input image. Another possible solution is to perform the convolution operation on intermediate feature maps <ref type="bibr" target="#b17">[18]</ref>. However, due to the high resolution and large spatial dimensions of the intermediate feature maps, it is computationally intensive to perform the convolution operation on these intermediate feature maps with a large number of parameters to be optimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>In our study, the skin lesion segmentation is carried out via RefineNet <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b41">42]</ref>, which is trained in an end-to-end manner. RefineNet uses convolutional layers as a feature extractor to capture hierarchical features and a segmentation network is attached to previous convolutional layers. Fig. <ref type="figure">2</ref> shows the architecture of our proposed DDN framework, which comprises two modules: encoding and decoding. The encoding module is used to down-sample and extract semantic features, while the decoding module is used to up-sample features and restore the detailed information. The original pre-trained ResNet model is divided into four residual blocks according to four different resolutions. Multiple scale outputs are achieved by different resolutions of the feature maps. Four residual blocks are followed by four cascaded DDN structures. Apart from the DDN-4, each DDN has two inputs, the first one is the corresponding resolution from the residual block in the encoding module while the second one is the previous DDN module. For instance, the output of Res 8-13 and DDN-4 are the inputs of DDN-3. Similarly, the output of Res 4-7 and DDN-3 are fed into DDN-2. Finally, the DDN-1 can incorporate multi-level features as inputs via a long-range skip connection between the encoding module and the decoding module. The output of the decoding module is sent to the classifier layer, which ultimately generates the class probability for each pixel independently. Also, two HS layers are inserted as side-output layers in the output of DDN-3 and DDN-2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Residual block</head><p>It is known that multiple layers are piled up to form a deep neural network to extract hierarchical features (i.e., low-level, middle-level and high-level features) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b46">47]</ref>. In fact, the number of layers of the network is a key factor that influences the segmentation performance. The deeper the network, the better the performance is since deep neural networks can learn more hierarchical abstract features from the original input data. However, increasing number of network layers can lead to the gradient vanishing or exploding problem, and the training process becomes more difficult to converge. There is also the likelihood of the model converging at local optimal due to overfitting or local optimal solution of the loss function. If we just simply stack network layers, it will lead to a sharp performance drop due to the increased depth layers and appearance degradation.</p><p>A deep residual learning framework is effective <ref type="bibr" target="#b38">[39]</ref> in resolving the gradient vanishing or exploding problem by identifying the mapping as skip connection to solve the gradient vanishing problem. This framework makes the gradient transfer back to the early layers or another block more easily in the backpropagation process. In addition, the skip connection is useful to alleviate computational complexity and keep the number of parameters unchanged at the same time.</p><p>In general, a residual unit is formulated as follows:</p><formula xml:id="formula_0">ùíΩ !!! = ùëÖùëíùëôùë¢(ùíΩ ! + ‚Ñ±(ùíΩ ! , ùìå ! )),<label>(1)</label></formula><p>where ùíΩ ! , ùíΩ !!! and ùìå ! are the input, output, and a set of weighting parameters of the l-th layer, respectively, ùëÖùëíùëôùë¢ ‚àé is the rectified linear unit function, and ‚Ñ±(‚àé) represents the residual mapping function. The output of the following layers and the gradient derivatives in the process of back propagation are denoted as:</p><formula xml:id="formula_1">ùíΩ ! = ùíΩ ! + ‚Ñ± ! (ùíΩ ! , ùìå ! ) !!! !!! ,<label>(2)</label></formula><formula xml:id="formula_2">!! !ùíΩ ! = !! !ùíΩ ! !ùíΩ ! !ùíΩ ! = !! !ùíΩ ! 1 + ! !ùíΩ ! ‚Ñ± ! ùíΩ ! , ùìå ! !!! !!! ,<label>(3)</label></formula><p>where L in Eq. ( <ref type="formula" target="#formula_2">3</ref>) represents the loss function of the residual block. Eqs. ( <ref type="formula" target="#formula_1">2</ref>) and (3) are adopted in <ref type="bibr" target="#b22">[23]</ref>, which show that it is easy to pass gradients from one residual block to another via a shortcut connection. Inspired by RefineNet <ref type="bibr" target="#b21">[22]</ref>, our proposed method is based on multi-scale feature maps via the deep residual block. To deal with the problems of gradient vanishing and overfitting, we also adopt batch normalization, dropout, and initialization operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dense deconvolutional network</head><p>As illustrated in Fig. <ref type="figure">2</ref>, we can see that DDN consists of a residual convolution unit, a DDL, and a CRP. As illustrated in Fig. <ref type="figure" target="#fig_0">3</ref> The reason to use the multiple inputs is that it can extract multiple resolution information as features. The extracted multiple features can be fused to enhance segmentation performance. In the DDN structure, we choose the bottleneck module in the ResNet as the structure of residual convolution unit to fine-tune the weights of the pre-trained ResNet model. Unlike large convolution kernels, small convolution kernels are utilized to reduce the number of convolution parameters and to avoid the overfitting problem. We apply dense DDL to refine and fuse the feature maps to get a higher resolution of the output images. DDN is able to sharpen object boundaries in an end-to-end fashion. Here, we use DDL instead of the original multi-fusion layers since it restores the original input and the CRP obtains the global contextual information, effectively. The captured global features are effective for the recognition of the whole image and then to help classify pixels in the regions of interest correctly. The checkerboard issue <ref type="bibr" target="#b47">[48]</ref> is addressed by DDN due to the fact that there is direct relationship among the adjacent pixels of the generated output feature map.</p><p>In the DDN framework, we integrate different resolution feature maps extracted in the down-sampling process via ResNet. We pass multi-level feature maps to the corresponding counterparts. The low-level boundary information is generated by pre-training the ResNet model, while the high-level semantic information is produced by segmentation networks. The segmentation network restores the size of feature maps, reconstructs spatial dimensional information, and obtains fine-grained object structures, which are lost during the process of pooling and down-sampling operations. The refined network combines the low-level boundary features with high-level semantic information. All DDN components explore the residual with identity mappings. In this way, the gradient can be propagated directly through the residual connection to ensure effective end-to-end training. Both short-range and long-range connections with their corresponding ResNet blocks are adopted. Accordingly, the gradient can be effectively transmitted to the entire network for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) CRP</head><p>The output feature maps of residual convolution unit are fed into the CRP. The detailed architecture of the CRP is shown in Fig. <ref type="figure" target="#fig_0">3 (b</ref>). There are in total three CRP blocks in total and every CRP block consists of a convolution layer and a max-pooling layer. The input of each CRP block is the output of the previous CRP block. The convolution operation is employed to generate weighting parameters in the training process. Pooling is used to change the size of the feature maps and boost performance via leveraging global contextual information. Here, we choose the kernel size in the pooling operation as five instead of three since large kernel size can expand the receptive field and capture more global information to make the model more robust to translations. Furthermore, the CRP block connects the output feature map generated by the pooling layer with the input feature map by summation via skip connection. In the process of backpropagation, the gradient can be transferred to a shallower layer more effectively with a skip connection. The non-linear ReLU operation is applied to increase pooling effectiveness as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) DDL</head><p>The core idea of dense CNNs is that each layer is directly connected to all other layers in a forward way. As a result, the checkerboard issue occurs. The popular solution is to add a skip connection between the previous layers and the following layers, which use short paths for connection of all layers directly instead of summation. The direct links in the intermediate feature maps reinforce the relation between what and what, make the training converge more quickly, and improve the information flow as well. Hence, the original multi-resolution fusion layer in RefineNet is replaced by DDL. Using this approach, the checkerboard problem is solved. To clearly illustrate the process of generating the DDL, Fig. <ref type="figure" target="#fig_2">4</ref> shows an example of expanding the input size from 4√ó4 to 8√ó8. Let ùëÄ ! , ùëÄ ! , , ùëÄ ! , and ùëÄ ! be the output produced by the input, the first feature map, the second feature map, and the third feature map respectively, and the intermediate feature maps are concatenated as:</p><formula xml:id="formula_3">ùëÄ ! = ùêπ ùëÄ ! , ùëÄ ! , , ùëÄ ! ,<label>(4)</label></formula><p>where F(‚àé) stands for the convolution operation. The first and second feature map interleaves kernel weights with zero to the size of 8√ó8, respectively. The process of interleaving is the same as dilated convolution. Then, the two dilated feature maps are added together. Similarly, the dilated feature maps are generated from the third and the fourth feature maps. The final output feature map is obtained by adding the two dilated feature maps and the resolution of the final output is enlarged a factor of 2.</p><p>The DDL plays an important role in the up-sampling operation to restore the high resolution of the original input. The dense connectivity alleviates the degree of gradient vanishing, strengthens the gradient propagation, and reuses features more effectively. In the decoding process, it is beneficial to restore the detailed low-level feature generated from the encoding module. Hence, we apply DDL to refine and fuse the feature map to obtain higher resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) HS</head><p>We propose a deep HS layer to restore the low-level features. HS is a more refined approach in the shallower layers than those in <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref>. We utilize two-side output layers for deep HS. We also attempt to adopt three-side output layers to refine the detailed edge information. However, it performs quite poorly to insert side output layers for DDN-4 since there is no direct relationship between the prediction output and the ground truth label. The side output layer adopts the lowest resolution feature maps and cannot capture the feature representation of the small lesion. In this regard, we insert three-side output layers on the last three boundary refinement blocks for deep supervision. All side output layers and the resulting layer are calculated as follows:</p><formula xml:id="formula_4">‚Ñí ùêº, ùëä = ùë§ ! ‚Ñí ! ùêº, ùëä + ! ùë§ ! ‚Ñí ! ùêº, ùëä ,<label>(5)</label></formula><formula xml:id="formula_5">‚Ñí ! ùêº, ùëä = ‚Ñí ! ùêº, ùëä = !! ! !,! ! !,! ! ! ! ! ! !,! ! ! ! ! ! ! ! !,! ! ! ! ! ! ,<label>(6)</label></formula><p>where ‚Ñí ùêº, ùëä is the output of the total loss function, ‚Ñí ! ùêº, ùëä is the loss value of the side-output, and ‚Ñí ! ùêº, ùëä is the loss value of the main output. The weighting parameters in loss layers are denoted as ùë§ ! and ùë§ ! .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL SETUP AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>In order to verify the effectiveness of the proposed approach, we conduct experiments on the skin lesion segmentation datasets from International Symposium on Biomedical Imaging (ISBI) of the years 2016 <ref type="bibr" target="#b50">[51]</ref>   <ref type="bibr" target="#b51">[52]</ref>. The datasets are collected from a variety of different treatment centers, archived by the International Skin Imaging Collaboration (ISIC), which hosts a challenge named skin lesion analysis toward melanoma detection to boost the performance of melanoma diagnosis. For the ISBI 2016 dataset, there are a total number of 900 samples for training and a total number of 379 dermoscopy images for testing. For the ISBI 2017 dataset, the training samples are extended to 2000, and the data are expanded to 900. These two datasets are manually annotated by professional dermatologists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation metrics</head><p>In this paper, we evaluate the segmentation performance by following the assessment criteria of the ISBI 2016 and 2017 Lesion Segmentation Challenge <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref>, including Dice coefficient (DC), Jaccard Index (JA), Accuracy (AC), Sensitivity (SE), and Specificity (SP). JA and DC are similarity metrics that measure the overlapping between the predicted results and the ground truth. The metrics for evaluating segmentation results are defined as:</p><formula xml:id="formula_6">DC = !‚Ä¢!" !‚Ä¢!"!!"!!" ,<label>(7)</label></formula><formula xml:id="formula_7">JA = !" !"!!"!!" ,<label>(8)</label></formula><formula xml:id="formula_8">AC = !"!!" !"!!"!!"!!" ,<label>(9)</label></formula><formula xml:id="formula_9">SE = !" !"!!" ,<label>(10)</label></formula><formula xml:id="formula_10">SP = !" !"!!" ,<label>(11)</label></formula><p>where TP, TN, FP, and FN are the numbers of true positive, true negative, false positive, and false negative, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation and data augmentation</head><p>Our experiments are conducted on MATLAB R2014b using the MatConvNet toolbox with a NVDIA TITAN X GPU. The Adam method <ref type="bibr" target="#b52">[53]</ref> (mini-batch size = 20, ùõΩ ! = 0.9, ùõΩ ! = 0.999 and œµ = 1e-8) is used to minimize the loss function. In the training process, the initial learning rate is fixed to 0.00001 for the first 300 epochs and 0.000001 for the second 300 epochs. Besides, dropout layers with 0.5 dropout rate are used to prevent overfitting. To improve the performance of the dense pixel classification, a two-stage strategy is used in our work. The main encoding-decoding pipeline is utilized to get the preliminary segmentation mask and the side-output layers. The hierarchical supervision is leveraged to refine the previous coarse segmentation mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results 1) Experiments on CRP</head><p>We also perform experiments on the CRP module. The results assessed by various metrics are described in Fig. <ref type="figure" target="#fig_3">5</ref> and Table <ref type="table">II</ref>. It can be seen that the results obtained with CRP are significantly higher than those without CRP. However, results listed in Table <ref type="table">II</ref> show that, SE decreases when CRP is used. The main reason is that segmentation is to assign a category per-pixel and obtains the dense pixel prediction. Although segmentation is sensitive to position information, CRP is insensitive to the position information and accuracy may decrease in small lesions. Therefore, the SE will be smaller than the result without CRP. However, from a macro perspective, CRP is beneficial to improve performance. The reason is that each pooling block takes the output of the previous layer as its input, then the receptive field is constantly expanding and capture more global contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Experiments on DDL</head><p>In order to prove the effectiveness of DDL and handle a variety of challenges in the skin lesion segmentation, our results are compared with conventional methods that are connected to the general deconvolutional operation in the decoding module. The main difference between the methods with and without DDL is the connection for deconvolution operation in the decoding module. The method without DDL uses the same encoding architecture with the same multi-level and multi-scale feature maps. DDL adopts dense connection in the up-sampling operation. By using DDL, each layer of the network in our proposed method can combine the feature maps of all the previous layers. The intermediate feature maps are generated by the input feature maps obtained from the pre-trained ResNet model. The remaining ones are generated by the previous feature maps. Finally, all the feature maps are fused for the final feature representation.</p><p>The comparative segmentation results are shown in Fig. <ref type="figure">6</ref>. We can see that the effectiveness of DDL is demonstrated in terms of various metrics. Compared with those without DDL, the results with DDL are improved in DC, JA, AC, SE, and SP, respectively. The main reason is that dense links among deconvolutional layers are advantageous to establish a direct relationship among adjacent pixels of feature maps, which can resolve the issue of blurred boundaries. Moreover, the dense links are useful in reducing the issues of gradient vanishing and to enhance feature propagation and reutilization. Thus, DDL can help the model to obtain low-level detailed features in the process of forward propagation and obtain the high-level gradient information in the phase of backpropagation. In this way, the flow of data between layers is facilitated to the greatest extent. We adopt the feature maps aggregation in the direction of depth via concatenation instead of the addition like ResNet. Therefore, our method performs better than that without DDL. The skin lesion segmentation results show that leveraging DDL rather than conventional deconvolutional layer obtains more promising semantic segmentation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Experiments HS</head><p>We also conduct experiments with and without HS and the segmentation results are shown in Fig. <ref type="figure">7</ref>. The proposed method with HS improves that without HS in DC, JA, AC, SE and SP, respectively, which demonstrates the effectiveness of HS. This is mainly due to the following reasons. For the HS method, we insert three-side output layers on the last three DDN modules. We perform deep supervision via side output layers. As a result, HS can learn rich hierarchical features and refine coarse output prediction. In addition, HS has more discriminative features than the network without HS. By learning multi-level and multi-scale features, feature complementarity in different levels is learned and the details of the edge corner information are integrated. Therefore, better performance is obtained via HS. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Experiments on different numbers of cascaded DDN</head><p>To demonstrate the influence of the number of layers on performance, we conduct experiments to compare different numbers of cascaded DDNs. Several configurations are summarized in Table <ref type="table">I</ref>. The single DDN means that the outputs of ResNet flow to just a single DDN without skip connection between encoding and decoding. The two-cascaded DDNs mean that the first DDN uses the outputs of Res 8-13 and Res14-16 as inputs while the second DDN takes the outputs of Res1-3 and Res4-7. As for three-cascaded DDNs, it adds a DDN based on the two-cascaded DDNs and adopts a skip connection with 7√ó7 convolutional layers. The five-cascaded DDNs are constructed based on the four-cascaded DDNs. As shown in Table <ref type="table">I</ref>, it can be seen that the segmentation results are improved by increasing layers in the network. However, the five-cascaded DDNs perform worse than the four-cascaded DDNs. The reason is that the convolutional layer is not based on the residual structure, which leads to overfitting. We set the number of cascaded DDNs as 4 based on the empirical experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Comparison with other methods</head><p>In order to further show the effectiveness of our DDN method, we compare the proposed method with different segmentation methods on both ISBI 2016 and 2017 datasets. Table <ref type="table">II</ref> and Fig. <ref type="figure" target="#fig_5">8</ref> show the segmentation results of different approaches, where RN is RefineNet. The effectiveness of DDL, CRP, and HS is evaluated by skin lesion segmentation metrics in Section IV-B. It is observed that DDNs achieve the best segmentation results among the selected methods. Since hierarchical supervised neural network accelerates the optimization speed, the gradient is easy to be back-propagated to the previous layers from the layers closer to the end. For this reason, RN-HS outperforms the traditional FCRN method. The main explanation for this is described as below. The DDL can streamline the information flow in the whole network. In the decoding phase, the prediction result is obtained by combining basic, simple features, such as arc and lines, and these basic features determine the restoration capability. Compared with the traditional deconvolutional layers, DDL enhances feature propagation, reuses features, and effectively solves the problem of gradient vanishing. Therefore, RN-DDL outperforms the listed FCRN, RN-HS, and RN-CRF methods.</p><p>For the ISBI dataset, there are several online competition segmentation methods. For instance, fully connected deconvolutional network (FCDN) proposed by Yuan et al. <ref type="bibr" target="#b53">[54]</ref> ranked the first in the melanoma segmentation of the ISIC 2017. They adopted a dual-threshold method as a post-processing procedure. In <ref type="bibr" target="#b54">[55]</ref>, the up-sampled feature maps were obtained from ResNet to produce the output mask. In <ref type="bibr" target="#b55">[56]</ref>, a SMCP approach was proposed based on the discriminative regional feature integration for skin lesion segmentation. In <ref type="bibr" target="#b16">[17]</ref>, the classical FCN model for semantic  segmentation was adopted with fully convolutional layers and skip connection. In <ref type="bibr" target="#b56">[57]</ref>, INSEC TECNALIA used the color constant technique for skin lesion segmentation. Also, we can see that our proposed method consistently achieves the best segmentation results in the two ISBI datasets. Fig. <ref type="figure" target="#fig_6">10</ref> shows the segmentation comparison results. It can be seen that our DDN method achieves the best results among all the listed approaches in terms of various metrics. Accordingly, DDNs have the ability to learn a rich feature representation and detailed low-level local contextual information compared to those networks without HS layers. Hence, the proposed method shows superiority over other related methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. Discussions</head><p>In order to enhance the diagnosis efficiency and accuracy, we propose a novel DDN model followed by an encoding-decoding framework for skin lesion segmentation. To demonstrate the efficiency of the proposed DDN for skin lesion segmentation, we extract the feature maps and visualize them in Fig. <ref type="figure" target="#fig_7">11</ref>. Moreover, we visualize different layers of the whole network, including the encoding module and the decoding module. We can see that our method can restore the original input image effectively. We analyze the effectiveness of our method in the following aspects.</p><p>Although the DDN model is based on RefineNet, it has extra components of DDL, CRP, and HS. The dense connections on the deconvolutional layers are utilized instead of the conventional deconvolutional layers in the decoding phase. Dense links are established between the previous and following feature maps. By dense connection, any layer has access to the feature maps before it and the feature maps in the subsequent layers. Hence, DDNs with dense links are capable of feature reutilization, enhancing the information flow, and addressing the issue of gradient vanishing. CRP is a combination of pooling blocks in which the receptive field increases gradually and obtains more contextual information. Our DDL utilizes a number of skip connections, which address the problem of gradient vanishing and make it easier to converge. Furthermore, HS is added to capture multi-level features. These are all helpful for boosting the performance of skin lesion segmentation.</p><p>Despite the fact that promising segmentation performance is achieved by the proposed DDN method, there are still some limitations. The main limitation of our DDN approach is that the training dataset is insufficient for semantic segmentation in the medical image analysis. Although the use of data augmentation technology is adopted and the original dataset is increased by four folds, we still cannot make full use of deep neural networks for feature learning. The reason is that training images are limited when the network layer goes deeper. In addition, our method fails to segment some skin lesion images with low contrast and irregular edges as shown in Fig. <ref type="figure" target="#fig_8">12</ref>.</p><p>In our future research, we will attempt to fuse hand-crafted   features with the features extracted from deep convolutional neural networks and leverage dense connection in the encoding module. Also, we will try different combinations of DDL to further verify their effectiveness. In addition, we will replace ResNet with DenseNet to capture multi-scale feature maps in the encoding phase. Also, dense connection can be used in both encoding module and decoding module to investigate the possible dense pixel prediction improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we proposed a DDN framework for automatic skin lesion segmentation, which addresses the localization and classification issues, simultaneously. Our proposed method contains a DDL, a CRP block, and a HS layer. The DDNs can learn the discriminative feature representations and integrate multi-level contextual information, effectively. The results showed that our method is able to segment the regions of interest automatically and accurately. The proposed DDNs can reuse learned features from the previous layers via DDL, which establishes dense links among all feature maps. By addressing the issue of gradient vanishing, the proposed method strengthens the propagation of multi-level features in the entire network and boosts the performance of skin lesion segmentation greatly. The extensive experiments are performed on the challenge skin lesion datasets of ISBI 2016 and 2017. Our DDN method obtained the best segmentation performance among the listed methods and showed better results than the state-of-the-art approaches as well. We believe the proposed method is also applicable to other medical image segmentation tasks without data preprocessing and complicated post-processing procedures like CRF.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The architecture of residual block in the residual convolution unit, (b) The architecture of CRP, (c) An illustration of architecture of the DDN-1, the DDN-1 can aggregate multi-level features as inputs; From the top to bottom five sets of input feature maps on the left are generated by four ResNet blocks of different scales and DDN-2, respectively.</figDesc><graphic coords="4,79.85,50.40,462.29,258.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>, in order to explain the structure of DDN clearly, we use the framework of DDN-1 as an example to illustrate the structure of DDN. The number of inputs for each DDN is described as below: DDN-4 receives one scale input, i.e., Res14-16. DDN-3 receives inputs on two scales, i.e., Res 8-13, Res 14-16 and DDN-4, where the scales of Res 14-16 and DDN-4 are the same. DDN-2 receives three different scale inputs, i.e., Res 4-7, Res 8-13 and DDN-3, where the scales of Res 8-13 and DDN-3 are the same. DDN-1 receives four different scale inputs, i.e., Res 1-3, Res 4-7, Res 8-13 and DDN-2, where the scales of Res 4-7 and DDN-2 are the same.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Architecture of proposed dense deconvolutional layer. First feature map is output of CRP, and intermediate feature maps are generated by previous feature maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>5 .</head><label>5</label><figDesc>Segmentation results with and without CRP on (a) ISBI 2016 dataset; (b) ISBI 2017 dataset. (a) ISBI 2016 dataset (b) ISBI 2017 dataset Fig.6. Segmentation results with and without DDL on (a) ISBI2016 dataset; (b) ISBI 2017 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) ISBI 2016 dataset (b) ISBI 2017 dataset Fig.7. Segmentation results with and without HS on (a) ISBI 2016 dataset; (b) ISBI 2017 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>8 .</head><label>8</label><figDesc>Comparison results of different methods on (a) ISBI 2016 dataset; (b) ISBI 2017 dataset. (a) ISBI 2016 dataset (b) ISBI 2017 dataset Fig.9. Comparison results of different online competition algorithms on (a) ISBI 2016 dataset; (b) ISBI 2017 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FCRNFig. 10 .</head><label>10</label><figDesc>Fig. 10. Segmentation results by FCRN, RN-HS, RN-DDL, RN-CRF, RN-CRP, and proposed method. First six columns of images are based on dataset of ISBI 2016 while last six columns are based on dataset of ISBI 2017. Green and red contours are ground truth label, and segmentation result, respectively.</figDesc><graphic coords="9,58.28,65.44,505.12,285.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Visualization of residual block and RefineNet block, (a) input image, (b) -(e) feature maps obtained by four residual blocks of ResNet respectively in encoding module, (f)-(j) feature maps obtained by DDNs, including residual convolution unit, DDL, and CRP layers.</figDesc><graphic coords="9,46.80,596.59,248.55,133.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .</head><label>12</label><figDesc>Fig.12. Images that do not perform well using proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,50.51,51.75,514.43,207.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,55.18,50.40,501.65,142.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Hang Li ‚Ä†, Xinzi He ‚Ä†, Feng Zhou, Zhen Yu, Dong Ni, Member, IEEE, Siping Chen, Tianfu Wang and Baiying Lei*, Senior Member, IEEE</figDesc><table><row><cell>Dense Deconvolutional Network for Skin</cell></row><row><cell>Lesion Segmentation</cell></row></table><note><p><p><p>T Fig.</p>1</p>. Examples of challenges for skin lesion segmentation for dermoscopy images. First four images in each row are from ISBI 2016 dataset and remaining images are from ISBI 2017 dataset. Images in second and fourth rows are their corresponding segmentation masks. From top to bottom: we can see that skin lesion has a wide range of sizes, irregular object boundaries, low contrast between skin lesion with area of surrounding skins, hair occlusion in dermoscopy images, and skin lesion with air bubbles.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>and 2017</figDesc><table><row><cell>Sum</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>feature map output</cell></row><row><cell>Sum</cell><cell></cell><cell>Sum</cell></row><row><cell>Dilation</cell><cell></cell><cell>Dilation</cell></row><row><cell>Conv</cell><cell>Conv</cell><cell>Conv</cell></row><row><cell cols="2">First feature map Second feature map</cell><cell>Third feature map Fourth feature map</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported partly by National Natural Science Foundation of China (Nos. 81571758, 81771922, and 61501305), National Key Research and Develop Program (No. 2016YFC0104703), National Natural Science Foundation of Guangdong Province (Nos. 2017A030313377 and 2016A030313047), Shenzhen Peacock Plan (No. KQTD2016053112051497), Shenzhen Key Basic Research Project (Nos. JCYJ20170818142347251 and JCYJ20170818094109846), NTUT-SZU Joint Research Program (No. 2018006) and Innovation and Entrepreneurship Training Program for College Students (Nos. 803-000027060214 and 803-000027060216) ‚Ä† indicates the two authors contribute equally. Asterisk indicates corresponding author.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Prognostic factors for melanoma</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Wisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Sober</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dermatologic Clinics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="469" to="485" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Final version of 2009 AJCC melanoma staging and classification</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Balch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gershenwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Soong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Atkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Buzaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Cochran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Coit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. clinical oncology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">36</biblScope>
			<biblScope unit="page" from="6199" to="6206" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cancer statistics</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer J. Clinicians</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="30" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>CA</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<author>
			<persName><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Marghoob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Argenziano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Carrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Curiel-Lewandrowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hofmann-Wellenhof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malvehy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Menzies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rabinovitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Standardization of terminology in dermoscopy/dermatoscopy: results of the third consensus conference of the International Society of Dermoscopy</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="1093" to="1106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ugly duckling sign as a major factor of efficiency in melanoma detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gaudy-Marqueste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wazaefi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bruneu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Triller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pellacani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malvehy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Avril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Monestier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Richard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jama Dermatol</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="279" to="284" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A methodological approach to the classification of dermoscopy images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Kingravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Uddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Iyatomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Aslandogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Stoecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Moss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imag. Graph</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="362" to="373" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Toward a combined tool to assist dermatologists in melanoma detection from dermoscopic images of pigmented skin lesions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Capdehourat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Corez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bazzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mus√©</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="2187" to="2196" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stacked deep polynomial network based representation learning for tumor classification with small ultrasound image dataset</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="87" to="94" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automated melanoma recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ganster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rohrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wildling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="239" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">From facial parts responses to face detection: a deep learning approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3676" to="3684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dcan: deep contour-aware networks for accurate gland segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2487" to="2496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic scoring of multiple semantic attributes with multi-task feature leverage: A study on pulmonary nodules in CT images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Z</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="802" to="814" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bridging computational features toward multiple semantic features with multi-task regression: A study of CT pulmonary nodules</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Z</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Med. Imag. Comput. Comput. Assist. Interv</title>
		<imprint>
			<biblScope unit="page" from="53" to="60" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deeplab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A guide to convolution arithmetic for deep learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07285</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kr√§henb√ºhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">RefineNet: multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5168" to="5177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">VoxResNet: deep voxelwise residual networks for brain segmentation from 3D MR images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Boundary regularized convolutional neural network for layer parsing of breast anatomy in automated whole breast ultrasound</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Z</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Med. Imag. Comput. Comput. Assist. Interv</title>
		<imprint>
			<biblScope unit="page" from="259" to="266" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large kernel mattersimprove semantic segmentation by global convolutional network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1743" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">3d deeply supervised network for automatic liver segmentation from ct volumes</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Med. Imag. Comput. Comput. Assist. Interv</title>
		<imprint>
			<biblScope unit="page" from="149" to="157" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Accurate cervical cell segmentation from overlapping clumps in pap smear images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="288" to="300" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Accurate segmentation of cervical cytoplasm and nuclei based on multiscale convolutional network and graph partitioning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2421" to="2433" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep contextual networks for neuronal structure segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artif. Intell</title>
		<meeting>AAAI Conf. Artif. Intell</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1167" to="1173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Auto-context and its application to high-level vision tasks and 3d brain image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1744" to="1757" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artif. Intell</title>
		<meeting>AAAI Conf. Artif. Intell</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Decaf: a deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Histopathological image classification with color pattern random binary hashing-based PCANet and matrix-form classifier</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health. Inf</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1327" to="1337" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automatic detection of cerebral microbleeds from MR images via 3D convolutional neural networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Mok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1182" to="1195" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Standard plane localization in fetal ultrasound via domain transferred deep neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inf</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1627" to="1636" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Representation learning: a unified deep learning framework for automatic prostate MR segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Med. Imag. Comput. Comput. Assist. Interv</title>
		<imprint>
			<biblScope unit="page" from="254" to="261" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Skin Lesion Segmentation via Deep RefineNet</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Med. Imag. Comput. Comput. Assist. Interv. Workshop</title>
		<imprint>
			<biblScope unit="page" from="303" to="311" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Conditional random fields as recurrent neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a DCNN for semantic image segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1742" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning deep cnn denoiser prior for image restoration</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2808" to="2817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Convolutional kernel networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2627" to="2635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Pixel deconvolutional networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06820</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf</title>
		<meeting>Int. Conf</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<ptr target="https://challenge.kitware.com" />
		<title level="m">Part 1: Lesion Segmentation. ISIC 2016: Skin Lesion Analysis Towards Malanoma Detection</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Part 1: Lesion Segmentation. ISIC 2017: Skin Lesion Analysis Towards Melanoma Detection</title>
		<ptr target="https://challenge.kitware.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Automatic skin lesion segmentation with fully convolutional-deconvolutional networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Lo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05165</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Automatic skin lesion analysis using large-scale dermoscopy images and deep residual networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04197</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Segmentation of lesions in dermoscopy images using saliency map and contour propagation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jahanifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Z</forename><surname>Tajeddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Asl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00087</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Galdran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alvarez-Gila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Saratxaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ara√∫jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Aresta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Mendon√ßa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Campilho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03702</idno>
		<title level="m">Data-driven color augmentation techniques for deep skin image analysis</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00373</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
