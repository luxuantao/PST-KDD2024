<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Task Learning with Low Rank Attribute Embedding for Multi-Camera Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chi</forename><surname>Su</surname></persName>
							<email>chisu@pku.edu.cn</email>
						</author>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Fan</forename><surname>Yang</surname></persName>
							<email>fyang@umiacs.umd.edu</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE, Qi</roleName><forename type="first">Tian</forename><surname>Fellow</surname></persName>
						</author>
						<author>
							<persName><roleName>IEEE</roleName><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Wen</forename><surname>Gao</surname></persName>
							<email>wgao@pku.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>qitian@cs.utsa.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Wen Gao are with Peking University</orgName>
								<address>
									<addrLine>‚Ä¢ Chi Su, Shiliang Zhang</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Fan Yang and Larry S. Davis</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20740</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Texas at San Antonio</orgName>
								<address>
									<addrLine>San Antonio</addrLine>
									<postCode>78249</postCode>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Task Learning with Low Rank Attribute Embedding for Multi-Camera Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">10F419EF4E9888AC5BC9DA2B78A1C124</idno>
					<idno type="DOI">10.1109/TPAMI.2017.2679002</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2017.2679002, IEEE Transactions on Pattern Analysis and Machine Intelligence</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multi-task learning</term>
					<term>attribute</term>
					<term>low rank</term>
					<term>person re-identification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose Multi-Task Learning with Low Rank Attribute Embedding (MTL-LORAE) to address the problem of person re-identification on multi-cameras. Re-identifications on different cameras are considered as related tasks, which allows the shared information among different tasks to be explored to improve the re-identification accuracy. The MTL-LORAE framework integrates low-level features with mid-level attributes as the descriptions for persons. To improve the accuracy of such description, we introduce the low-rank attribute embedding, which maps original binary attributes into a continuous space utilizing the correlative relationship between each pair of attributes. In this way, inaccurate attributes are rectified and missing attributes are recovered. The resulting objective function is constructed with an attribute embedding error and a quadratic loss concerning class labels. It is solved by an alternating optimization strategy. The proposed MTL-LORAE is tested on four datasets and is validated to outperform the existing methods with significant margins.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>P ERSON re-identification aims to identify a query person by searching for the most similar instances in a gallery image or video set. Generally, the re-identification precision rate can be improved by acquiring more information from a larger amount of surveillance data. To ensure the recall rate, it is highly necessary to devise effective algorithms to cope with viewpoint variations, illumination conditions, and camera parameter differences across images. This is because that even for the same person appearing in various images, the low-level visual features could be inconsistent and unreliable. Furthermore, in real-world re-identification, images are often collected by a number of non-overlapping cameras with different settings and viewpoints, making person re-identification on multi-cameras a more challenging task.</p><p>Nonetheless, even though a person's appearance can be easily affected by many factors, his/her high-level semantic concepts could remain comparatively consistent and stable under different cameras. These semantic concepts, also known as attributes, have been used in many vision tasks like image classification and object detection, and have demonstrated promising robustness. For a person appearing in different cameras, his/her attributes are more stable and consistent than low-level features. For instance, if a person walking towards the camera has an attribute short hair, there is a high probability that this short hair attribute still could be detected even through this person turns his/her back to the camera. In addition, attributes exhibit substantial correlative relationships, i.e., some attributes tend to co-appear while some never show up at the same time. For example, female is more likely to be related with long hair than with short hair. Also, long pants and short pants are not likely to co-exist in one person.</p><p>By using attributes to describe an image, we can obtain a vector, where each dimension indicates the existence or absence (or the likelihood of existence) of the corresponding attribute. We also find that the above mentioned interattribute correlations could be utilized to map a person's attributes under different cameras into a low rank space. In this space, the original binary attributes can be represented by more accurate and informative continuous values. Additionally, this mapping enables us to eliminate noisy attributes and recover missing attributes, thus resulting in more accurate attributes. In order to take advantages of the inter-attribute correlations, the commonly used strategies model the relationships between camera pairs. However, it is unrealistic to do such modeling for large-scale data because of the quadratic complexity with respect to the number cameras. Therefore, most of conventional methods ignore the relationships in the scenarios containing more than two cameras, and thus show limited flexibility.</p><p>In Multi-Task Learning (MTL), multiple related tasks benefit each other and are jointly optimized. Because of its promising performance in uncovering latent relationships among tasks, MTL has been widely used in machine learning <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> and computer vision <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> tasks. Furthermore, MTL is also suitable for handling scenarios where only a small amount of training data is available for each task.</p><p>In multi-camera person re-identification, persons appear in different cameras. In another word, different cameras share the same set of persons. Person re-identification task also easily suffers from the limited training data under each individual camera. Inspired by this, we leverage the MTL <ref type="bibr" target="#b4">[5]</ref> to explore relationships between features and attributes in cross-camera person re-identification. By considering reidentifications from multiple cameras as related tasks, the MTL framework is well adapted to exploit features and attributes shared across cameras.</p><p>Based on the above considerations, we propose the Multi-Task Learning algorithm with LOw Rank Attribute Embedding (MTL-LORAE) algorithm for person reidentification. In our algorithm, we convert the person reidentification problem into a classification problem. Specifically, we use images from multiple cameras to learn a group of person-specific classifiers. A vector made up by outputs of these classifiers is created to represent each probe and gallery image. For training on each specific person, given his/her images from multiple cameras, we use MTL to learn a discriminative model so that the inter-camera relationships can serve to improve the learned model's quality. Our MTL objective function uses both attributes and low-level features. The low rank attribute embedding is also included in the objective function to discover relationships between attribute pairs. In the embedded space, a person's attributes under different cameras become similar while attributes of different people become more distinct from each other. The embedded space also helps to rectify inaccurate attributes and recover missing attributes. Its low rank structure allows only a small amount of latent attributes to contribute to the classification. An efficient alternating optimization method is proposed to solve the MTL-LORAE objective function. In this sense, our work is different from those algorithms performing distance metric learning <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Similar with representation learning algorithms <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, our goal is to acquire a more robust and informative descriptor, by which we use simple distance matching to do person re-identification.</p><p>We evaluate MTL-LORAE on four person reidentification datasets and demonstrate that MTL-LORAE has achieved satisfactory results. In <ref type="bibr" target="#b19">[20]</ref>, Su et al.has provided a preliminary version of this work. In this paper, we add in-depth theoretical discussions and more extensive experiments and detailed discussions. Besides that, motivated by the promising performance of deep learning in various visual tasks, we test deep learning features with our framework. New comparisons on four public datasets show deep learning features further boost the performance of our approach.</p><p>Our contributions can be summarized in the following three aspects:</p><p>‚Ä¢ By regarding re-identification under multiple cameras as related tasks, we successfully exploit their inter-relationships to learn more discriminative classifiers for accurate person re-identification. To the best of our knowledge, multi-task learning based person re-identification is a rarely studied topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>‚Ä¢</head><p>We introduce low rank embedding into the MTL framework. This integrates complementary features, i.e., mid-level attributes and low-level visual features into the re-identification framework. Moreover, binary attributes are mapped into a continuous space based on the inter-attribute correlations inferred from the training data. This embedding process also rectifies inaccurate attributes and recovers missing attributes, resulting in more accurate attributes and more discriminative classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>‚Ä¢</head><p>We present a novel objective function that jointly learns task-specific classifiers and low rank attribute embedding. Although the objective function is difficult to solve, we successfully propose an efficient alternating optimization strategy with convergence guarantee.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Person Re-Identification</head><p>Person re-identification is attracting more and more attentions nowadays. There are several surveys <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> on person re-identification. Traditional person re-identification works can be classified into three categories: (a) retrieving and encoding robust local features that represent a person's visual appearance; (b) learning a discriminative distance metric to narrow down the distance between features of the same person; (c) learning a new person representation, which should be more robust and informative than the lowlevel descriptor.</p><p>As for feature design, previous works design and use a variety of customized features, including histogram features from various color and texture channels <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, symmetry-driven accumulation of local features <ref type="bibr" target="#b25">[26]</ref>, features from body parts with pictorial structures <ref type="bibr" target="#b26">[27]</ref> to estimate human body configuration, and space-time features from person tracklets <ref type="bibr" target="#b15">[16]</ref>, etc. In order to integrate multiple features, Gray et al. <ref type="bibr" target="#b23">[24]</ref> select a subset of features by boosting for matching pedestrian images. To enhance the descriptive capability of multiple features, Liu et al. <ref type="bibr" target="#b27">[28]</ref> fuse them by learning person-specific weights.</p><p>With respect to distance measurement, some works measure the similarity between images from two cameras by learning an optimized distance metric. Pairwise Constrained Component Analysis <ref type="bibr" target="#b6">[7]</ref> and Relaxed Pairwise Metric Learning <ref type="bibr" target="#b7">[8]</ref> learn a projection from highdimensional input space to a low-dimensional space, where the distance between pairs of data points meets the predefined constraints. The Locally-Adaptive Decision Function in <ref type="bibr" target="#b13">[14]</ref> learns a locally adaptive thresholding rule and a distance metric. The Probabilistic Relative Distance Comparison model <ref type="bibr" target="#b14">[15]</ref> seeks to increase the possibility of finding a true match whose distance is smaller than a false match. In <ref type="bibr" target="#b10">[11]</ref>, K √∂stinger et al.propose a statistical inference perspective to address the problem of metric learning. Kernel-based distance learning <ref type="bibr" target="#b11">[12]</ref> is used to handle linearly non-separable data. In <ref type="bibr" target="#b28">[29]</ref> Li et al.present a deep learning framework to learn filter pairs that are responsible for encoding photometric transforms.</p><p>For representation learning, there are some works <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> which learn new robust descriptors by model training. Matching images of faces from different imaging modalities is an essential step for Heterogeneous Face Recognition (HFR) <ref type="bibr" target="#b16">[17]</ref>. For example, HFR matches a sketch or an infrared image with a photo. In HFR framework, probe and gallery images are both represented with respect to their nonlinear similarities to a group of prototype face images. AN et al. <ref type="bibr" target="#b18">[19]</ref> propose a reference-based cross-camera person re-identification approach. During the training process, a subspace is learned, where Regularized Canonical Correlation Analysis (RCCA) is used to maximize the relationships between reference images captured by multiple cameras. Recently, Zhao et al. <ref type="bibr" target="#b29">[30]</ref> propose to learn mid-level filters, which are designed to address cross-view invariance and use patch matching to infer the geometric configurations of body parts. Xiao et al. <ref type="bibr" target="#b31">[32]</ref> propose a Domain Guided Dropout algorithm trained on multiple domains with Convolutional Neural Networks (CNNs) to improve the deep feature learning procedure.</p><p>Similar to those algorithms, we train multiple person classifiers integrating both low-level features and attribute features to perform representation learning.</p><p>There are also approaches dedicated to person reidentification in large camera networks involving more than two cameras <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Attributes</head><p>Attributes are semantic concepts of objects and can be either learned from low-level features or manually defined. Previous works have studied the inter-attribute correlations in order to improve the performance of zero/one-shot learning for attribute-based classification <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>. In person re-identification, attributes show promising performance in preserving consistent representations of the same person and identifying differences among different persons <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>. However, attributes are often used as supplementary features for low-level features in previous person re-identification works, which also do not consider the correlations between attributes. Although several methods of object classification have managed to model correlations between attributes <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, as far as we know, no work has utilized both low-level features and attribute correlations across cameras for re-identification in a systematic manner. Our algorithm integrates both attributes and low-level features for training and acquire better attribute features through low rank attribute embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-Task Learning</head><p>There are some representative works concerning Multi-Task Learning, including clustered MTL <ref type="bibr" target="#b48">[49]</ref>, Robust MTL <ref type="bibr" target="#b49">[50]</ref>, trace norm regularization <ref type="bibr" target="#b50">[51]</ref>, and <ref type="bibr" target="#b51">[52]</ref>. The modeling of information shared across tasks is often based on the assumption of a shared low rank structure <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>. Kernel method has also been utilized to handle linearly non-separable features <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>. Dictionary learning <ref type="bibr" target="#b56">[57]</ref> and tree sparsity constraint <ref type="bibr" target="#b57">[58]</ref> are also integrated with the standard MTL framework. Chen et al. <ref type="bibr" target="#b58">[59]</ref> apply MTL to concurrently learn inter-attribute correlations and ranking functions for image ranking. By regarding attribute classifiers as auxiliary tasks for object classifiers, Hwang et al. <ref type="bibr" target="#b59">[60]</ref> use MTL to learn a shared structure for improved classification and attribute prediction. Yang and Hospedales <ref type="bibr" target="#b51">[52]</ref> provides a two sided neural network framework, one for original feature and one for associated semantic descriptor, that addresses both multi-domain and multi-task learning.</p><p>Both <ref type="bibr" target="#b58">[59]</ref> and <ref type="bibr" target="#b59">[60]</ref> assume attributes to be related tasks. In <ref type="bibr" target="#b60">[61]</ref>, the multi-task support vector ranks individuals by transferring information of matched or unmatched image pairs from the source domain to the target domain. Ma et al. <ref type="bibr" target="#b61">[62]</ref> use multi-task learning to substitute multiple Mahalanobis distance metrics for the universal distance metric for all cameras. It should be noted that our approach is different from <ref type="bibr" target="#b60">[61]</ref> in that, we directly model low-level features and inter-attribute correlations shared across cameras without using image pairs. Moreover, with respect to both attributes and low-level features, we seek for a shared structure across cameras, rather than learning a metric for each camera pair, which can be computationally expensive for real applications. Although the framework of <ref type="bibr" target="#b62">[63]</ref> has a similar lowrank constraint with our work, it is not MTL based and adopts a different optimization method due to the additional l 1 and l 2 constraints. Robust MTL <ref type="bibr" target="#b49">[50]</ref> can only be used to optimize W for MTL. Compared with the ones in <ref type="bibr" target="#b62">[63]</ref> and <ref type="bibr" target="#b49">[50]</ref>, our formulation is more challenging by involving the optimization of both W for MTL and low rank matrix for attributes correlation. To address this formulation, we have proposed an efficient alternating optimization strategy with convergence guarantee.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>In this paper, learning a good representation for person reidentification is formulated as a problem of classification by learning a set of classifiers using images from multiple cameras, with each classifier corresponds to a specific person. Like <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, if the training set contains C persons, we use the MTL-LORAE to train C classifiers and then formulate each classifier learning as a regression problem. Each probe and gallery image is represented by a vector composed of outputs of these classifiers. Through the computation of the distance between vectors of probe and gallery images, we find and rank gallery images to perform person re-identification. Details of this procedure will be given in Sec. 3.5. For simplicity, no distinction is drawn between cameras and tasks, and we will use two terms interchangeably.</p><p>Given L learning tasks {T 1 , T 2 , ..., T L } sharing the same feature space, we want to use information of all tasks to learn multi-class classifiers on a specific task. Typically, all tasks in a multi-class setting share the same set of C classes (persons). In a supervised one-vs-all manner, for the l-th task T l , we begin with binary classification by considering images belonging to the c-th class as positive samples and regarding those belonging to the rest of the classes as negative samples, where there are totally n l labeled training samples. We follow the standard supervised learning protocol, where labels of all training images are available.</p><p>By learning multiple tasks simultaneously, our method can perform effective task-to-task information transmission, which is a useful function when only a limited amount of training data from a task is available. For better clarity, we omit the class index c from all notations in the following text. For each training sample from the l-th task T l , we have a low level feature vector x l i ‚àà R d and a label y l i ‚àà {-1, 1}, where 1 indicates this sample is from the c-th class and -1 otherwise. Additionally, there is a binary attribute vector a l i ‚àà {0, 1} k for each sample, which may be semantic and manually labeled or correspond to learned binary codes as described in <ref type="bibr" target="#b63">[64]</ref>. For each dimension of a l i , 1 means that the corresponding attribute is present and 0 otherwise. Then, a predictor f l with respect to the task T l will be learned.</p><p>The discriminative and generalization ability of predictors can be enhanced by exploiting the relationship amongst tasks. Hence, information from task T i can be transmitted to another task T j , where there may be only a limited number of training samples available. In this manner, the learning of the predictor f j will benefit from the learning on both T i and T j simultaneously. This motivates the usage of MTL to match images taken by different cameras. Furthermore, the learned predictors can be improved if we integrate attributes and find the correlations among them. The following sections introduce the low rank attribute embedding (LORAE), complete MTL formulation, optimization algorithm, and reidentification process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Low Rank Attribute Embedding</head><p>A simple approach of combining low-level features and attributes is to concatenate feature vectors and original attribute vectors. However, considering the possible inconsistency between human annotators and that it is difficult to obtain exhaustive semantic concepts, attributes tend to be inaccurate or incomplete in most cases. Actually, the absence of an attribute for an instance does not necessarily means that the instance does not have that attribute, which is a fact that could be misinterpreted by the learning algorithm. Similarly, the presence of a wrongly annotated attribute may constitute noise. All this make it difficult for the learned model based on the original attributes to accurately describe the instance. As there are many attributes, they are normally related to each other, meaning that some of them often cooccur across different tasks. Consequently, from the presence of one attribute, we can infer presence of its closely related attributes, which is helpful in recovering missing attributes. Likewise, some attributes are highly mutually exclusive, so that they never occur simultaneously, which serves as a clue to remove noisy attributes.</p><p>Following <ref type="bibr" target="#b62">[63]</ref>, we learn a low rank attribute space to embed the original binary attributes into continuous attributes using attribute dependencies. Specifically, in the low rank space, there exists a transformation matrix Z of each specific person, which is responsible for converting each of the original attribute vector of one person (class) into a new vector with continuous values. The transformation matrix should capture correlations between attribute pairs since an attribute can be affected by other attributes. The refined attributes of one person are able to discover the correlations of related attributes and preserve more accurate information to recognize this person. Moreover, some attributes may show certain local patterns. For example, there usually exists groups of attributes like shorts and barelegs, which are strongly correlated with each other, while being independent with the rest. These local groups essentially imply a low-rank structure in matrix Z. Therefore, Z should be a low-rank matrix to learn the potential correlations among attributes. Formally, given an attribute vector a l i from task T l , the linear embedding is parameterized as</p><formula xml:id="formula_0">Z a 1 1 , a 2 1 , a 3 1 ùúôùúô ùëßùëß (a 1 1 , a 2 1 , a<label>3</label></formula><formula xml:id="formula_1">œÜ Z (a l i ) = Z a l i s.t. rank(Z) ‚â§ r,<label>(1)</label></formula><p>where a l i and œÜ Z (a l i ) ‚àà R k and Z ‚àà R k√ók . Although kernel methods are applicable here, we choose to focus on linear embeddings for easier learning. The rank constraint imposed on Z guarantees that Z is low rank. It means there exists a row Z i,: (or a column Z :,i ) that is a linear combination of other rows (or columns). Therefore, the number of parameters needed for a good embedding is smaller than k √ó k. Hence, the computational complexity is decreased. In this way, we obtain a refined attribute vector with continuous values. It can precisely describes correlations between attributes while recovering missing values and reducing noises. Figure <ref type="figure">1</ref> shows an intuitive illustration of the low rank embedding, where missing values are successfully recovered in the embedded continuous attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-Task Learning with Low Rank Attribute Embedding</head><p>MTL is designed to learn multiple task-specific predictors simultaneously by making use of the correlations among tasks, so that the shared information can be transmitted from one task to another. To obtain an accurate transformation matrix Z for the purpose of attribute embedding, we propose a unified MTL framework that can utilize interattribute correlations across multiple tasks and train taskspecific predictors simultaneously. For the sake of simplicity, we assume a linear classifier for each learning task T l to be represented by a weight vector w l . For notational convenience, we concatenate the embedded attribute vector œÜ Z (a l i ) with x l i to construct a new vector</p><formula xml:id="formula_2">x l i = [x l i ; œÜ Z (a l i )] ‚àà R d+k .</formula><p>Therefore, we have w l ‚àà R d+k . In another word, while learning each person-specific classifier, we also learn a person-specific linear projection of attributes as part of that classifiers feature space. We define the loss function as (y l i , a l i , x l i , Z) which can represent any smooth and convex function measuring the discrepancy between groundtruth and predictions from learning. The MTL-LORAE framework is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. We use a label y l i ‚àà {-1, 1} for classification with the goal of finding better feature descriptors instead of just solving the classification problem. In another word, we want to use the outputs of trained classifiers as a feature vector. Consequently, the task is formulated as a regression problem to learn feature vectors conveying the classification confidence scores. Therefore, we define the loss function as a regression problem, i.e.,</p><formula xml:id="formula_3">(y l i , a l i , x l i , Z) = 1 2 (||y l i -w l x l i || 2 + Œ≥||a l i -Z a l i || 2 ).<label>(2)</label></formula><p>The first term ||y l i -w l x l i || 2 is the quadratic loss caused by applying the learned weight vector w l to the newly constructed sample x l i . The second term ||a l i -Z a l i || 2 is the attribute embedding error, which regularizes the difference between original attributes and refined attributes obtained from the linear embedding through Z. For the results produced by the embedding, their deviation from the original attributes should be small. Œ≥ controls the contributions of the two terms.</p><p>We denote all the task-specific w l as a single weight matrix W = [w 1 , w 2 , ..., w L ] ‚àà R (d+k)√óL . Since information is shared among tasks and each task has a specific structure, similar to <ref type="bibr" target="#b53">[54]</ref>, we assume that W is composed of a low rank matrix shared by all tasks and a task-specific sparse component representing the incoherence caused by individual tasks. Formally, W can be decomposed into a low rank matrix R ‚àà R (d+k)√óL and a sparse component S ‚àà R (d+k)√óL . Therefore, we have W = R + S. Intuitively, non-zeros entries in S indicate the task-specific incoherence between the task and the shared low rank structure. The formulation of MTL-LORAE is then given by min R,S,Z</p><formula xml:id="formula_4">L l=1 n l i=1 (y l i , a l i , x l i , Z) + Œª||S|| 0 s.t. W = R + S, rank(R) ‚â§ r 1 , rank(Z) ‚â§ r 2 , (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>where Œª is a trade-off parameter controlling the importance of the regularization. r 1 and r 2 constrain the matrices R and Z to be low rank. ||S|| 0 is the 0 -norm of S, which counts the number of non-zero entries of S. Solving Eq. ( <ref type="formula" target="#formula_0">3</ref>) is NP-hard since it is non-convex and non-smooth towing to the sparse regularization and low rank constraints. It can be converted into a computationally solvable one through convex relaxation. First, since the 1norm is a convex envelop of 0 -norm, ||S|| 0 is replaced by ||S|| 1 , which is the sum of all non-zero values. Second, the standard convex relaxation for the matrix rank is to use the nuclear norm (trace norm) || ‚Ä¢ || * = i œÉ i , which is the sum of the singular values of a matrix. We then obtain min R,S,Z</p><formula xml:id="formula_6">L l=1 n l i=1 (y l i , a l i , x l i , Z) + Œª||S|| 1 s.t. W = R + S, ||R|| * ‚â§ r 1 , ||Z|| * ‚â§ r 2 ,<label>(4)</label></formula><p>which is our complete MTL-LOREA formulation. For the convenience of notation, the value of the objective function is denoted as F . By minimizing Eq. ( <ref type="formula" target="#formula_6">4</ref>), the desired weight matrix W and transformation matrix Z can be obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Optimization</head><p>The optimization of Eq. ( <ref type="formula" target="#formula_6">4</ref>) is difficult because W (i.e., R and S) and Z are coupled together by x l i . However, the problem becomes solvable when we alternate between the tasks of optimizing the objective function with respect to one variable and fixing the other one. During the process of fixing Z, ||a l i -Z a l i || 2 becomes a constant so it can be omitted. x l i is also constant with respect to w l , so that it can be regarded as an ordinary training sample. By removing the nuclear norm constraint on Z, Eq. ( <ref type="formula" target="#formula_6">4</ref>) reduces to the standard MTL formulation under the assumption of shared low rank structure plus incoherent sparse values</p><formula xml:id="formula_7">min W L l=1 n l i=1 (y l i , x l i ) + Œª||S|| 1 s.t. W = R + S, ||R|| * ‚â§ r 1 ,<label>(5)</label></formula><p>where (y l i ,</p><formula xml:id="formula_8">x l i ) = 1 2 ||y l i -w l x l i || 2 .</formula><p>Eq. ( <ref type="formula" target="#formula_7">5</ref>) can be solved by using the MixedNorm approach as described in detail in <ref type="bibr" target="#b53">[54]</ref>. Details can be found in <ref type="bibr" target="#b53">[54]</ref>.</p><p>In the process of fixing W, both R and S become constant, so we can remove the constraints related to them. Therefore, we obtain the objective function</p><formula xml:id="formula_9">min Z L l=1 n l i=1 (y l i , a l i , x l i , Z) s.t. ||Z|| * ‚â§ r 2 . (<label>6</label></formula><formula xml:id="formula_10">)</formula><p>After relaxing the constraint as a regularization term, we obtain</p><formula xml:id="formula_11">min Z L l=1 n l i=1 (y l i , a l i , x l i , Z) + Œ≤||Z|| * .<label>(7)</label></formula><p>With the nuclear norm regularization, the optimal transformation matrix Z will not degenerate to a trivial solution, i.e., an identity matrix I. However, in the presence of the nonsmooth nuclear constraint on Z, it is difficult to optimize Eq. <ref type="bibr" target="#b6">(7)</ref>. For notational clarity, the loss function with respect to Z is denoted as Z , and the regularization term as h Z = ||Z|| * . Eq. ( <ref type="formula" target="#formula_11">7</ref>) is then rewritten as</p><formula xml:id="formula_12">min Z Z + Œ≤h Z .<label>(8)</label></formula><p>Z is convex, differentiable and Lipschitz continuous. h Z is convex but non-differentiable. Thus, Eq. ( <ref type="formula" target="#formula_12">8</ref>) can be solved by the proximal gradient method iteratively. First, we represent the gradient of Z with respect to Z as ‚àÇ Z . According to the proximal gradient algorithm, at each iteration step j, we then have Z j = prox tj (Z j-1t j ‚àÇ Zj-1 ), where t j &gt; 0 is the step size and j is the iteration index. prox tj is a proximal operator, defined as</p><formula xml:id="formula_13">arg min Z Zj-1 + ‚àÇ Zj-1 , Z -Z j-1 + 1 2tj ||Z -Z j-1 || 2 F + Œ≤h Z ,<label>(9)</label></formula><p>where ‚Ä¢, ‚Ä¢ is the inner product. Eq. ( <ref type="formula" target="#formula_13">9</ref>) finds the Z that minimizes the surrogate of the loss function at point Z j-1 plus a quadratic proximal regularization term and the nonsmooth regularization term. Eq. ( <ref type="formula" target="#formula_13">9</ref>) can be further simplified to</p><formula xml:id="formula_14">arg min Z 1 2tj ||Z -(Z j-1 -t j Zj-1 )|| 2 F + Œ≤h Z .<label>(10)</label></formula><p>It is clear that Eq. ( <ref type="formula" target="#formula_14">10</ref>) can be effectively solved by performing SVD on Z j-1 -t j Zj-1 and then soft-thresholding the singular values.</p><p>In practice, we use the Accelerated Gradient Method (AGM) <ref type="bibr" target="#b50">[51]</ref> to achieve faster optimization. AGM adaptively estimates the step size and introduces the search point Z j that is a linear combination of the latest two approximations Z j-1 and Z j-2 , Z j = Z j-1 + (</p><formula xml:id="formula_15">Œ±j-1-1 Œ±j )(Z j-1 -Z j-2 ).</formula><p>Here, Œ± j-1 and Œ± j control the combination weights of the previous two approximations, which are also updated iteratively by</p><formula xml:id="formula_16">Œ± j = 1+ ‚àö 1+4Œ± 2 j-1 2</formula><p>with Œ± 0 = 1. The gradient in the jth iteration is then performed on Z j instead of Z j , where</p><formula xml:id="formula_17">Z 1 = Z 0 .</formula><p>The gradient ‚àÇ Z is explicitly computed as</p><formula xml:id="formula_18">‚àÇ Z = (y l i -w l x l i ) ‚àÇw l x l i ‚àÇZ + Œ≥ ‚àÇZ a l i ‚àÇZ (a l i -Z a l i ) = (y l i -w l x l i ) ‚àÇw l œÜ Z a l i ‚àÇZ + Œ≥ ‚àÇZ a l i ‚àÇZ (a l i -Z a l i ) = (y l i -w l x l i )a l i w l œÜ + Œ≥a l i (a l i -Z a l i ) = a l i [w l œÜ (y l i -w l x l i ) + Œ≥(a l i -Z a l i ) ],<label>(11)</label></formula><p>where w l œÜ ‚àà R k is part of the weight vector w l corresponding to the embedded attribute œÜ Z (a l i ). When the optimization for Z converges, we update Z, fix it and minimize the objective function for W. The optimization will stop once a pre-defined iteration number P or once the difference ‚àÜF = F j-1 -F j &gt; 0 between consecutive values of the objective function falls below a threshold. The detailed steps of the optimization are shown in Algorithm 1.</p><p>Algorithm 1 Multi-Task Learning with Low Rank Attribute Embedding (MTL-LORAE) Input: training data samples {x l i , a l i , y l i } for all L tasks, initial Z 0 and W 0 , iteration number P and threshold th &gt; 0 to control iteration step. Output: Learned Z and W. Z ‚Üê Z 0 , W ‚Üê W 0 ; Evaluate objective function F 0 using Z and W; for j = 1 to P do Optimize Eq. ( <ref type="formula" target="#formula_7">5</ref>) when fixing Z by MixedNorm; Update W ‚Üê W j ; Optimize Eq. ( <ref type="formula" target="#formula_9">6</ref>) when fixing W by AGM algorithm; Update Z ‚Üê Z j ; Evaluate objective function F j ; Calculate ‚àÜF = F j-1 -F j ; if ‚àÜF &lt; th break; end if end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Re-identification Process</head><p>With C training classes (persons), we obtain C class-specific weight matrices and transformation matrices, each of which is denoted as</p><formula xml:id="formula_19">W (c) = [w 1 (c) , w 2 (c) , ‚Ä¢ ‚Ä¢ ‚Ä¢, w L (c)</formula><p>] and Z (c) , respectively, by performing the optimization with respect to each class. Note that, since different persons may have different sensitivities to attribute correlations, we trained a transformation matrix Z (c) for the c-th specific person to enhance the recognition of this specific person. Therefore, there are C different transformation matrixes Z for reidentification instead of one global transformation matrix. Given an image taken by the l -th camera, l = 1, 2, ‚Ä¢ ‚Ä¢ ‚Ä¢, L, which either comes from the gallery or the probe set, we first extract low level feature x l and attribute vector a l . By utilizing the transformation matrices, we convert our feature and attribute vectors to a new set of vectors, denoted as</p><formula xml:id="formula_20">X l = [ x l (1) , x l (2) , ‚Ä¢ ‚Ä¢ ‚Ä¢, x l (C) ] ‚àà R (d+k)√óC</formula><p>, where the cth column x l (c) = [x l ; Z (c) a l ] is the concatenation of the feature vector and the embedded attribute vector using the c-th transformation matrix Z (c) . Furthermore, we select weight vectors with respect to l -th task from C weight matrices, and multiply them with the new vectors to obtain a score vector s as</p><formula xml:id="formula_21">s = [w l (1) x l (1) , w l (2) x l (2) , ‚Ä¢ ‚Ä¢ ‚Ä¢, w l (C) x l (C) ],<label>(12)</label></formula><p>where w l (c) is the column weight vector extracted from W (c) corresponding to the l -th task T l trained for the cth class. Therefore, each image is finally represented by a C-dimensional score vector s. Then the Euclidean distance between two score vectors is used to measure the similarity between a gallery image and a probe image. It should be noted that the classes in the training set can be either the same as or different from those in the gallery and probe sets.</p><p>In multi-shot cases, a number of images are presented for each probe/gallery identity. Given a probe image set containing m p images, the re-identification process ranks the gallery image sets by aggregating image-level similar-ities Therefore, the voting scheme below is used. First, we calculate the distances between m p probe images and all gallery images. Then, we use a Gaussian kernel to convert the distances into similarities. In order to obtain a single similarity between the probe and a gallery image set of m g images, we sum up all m p √ó m g similarities and divide the sum by the number of gallery images, m g , to discount the effect of a gallery set including a large number of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate our approach on 4 public datasets, iLIDS-VID <ref type="bibr" target="#b15">[16]</ref>, PRID <ref type="bibr" target="#b64">[65]</ref>, VIPeR <ref type="bibr" target="#b65">[66]</ref> and SAIVT-SoftBio <ref type="bibr" target="#b32">[33]</ref>. The iLIDS-VID dataset consists of 600 image sets of 300 persons from two cameras at an airport. This dataset is designed for multi-shot re-identification. Each person has two image sets from the two cameras respectively, where each image set contains 23 to 192 images, sampled from a short video taken within a few seconds. The PRID dataset is used for single-shot scenario. It contains images of different people from two cameras A and B, under different illumination and background conditions. There are 385 and 749 persons appearing in cameras A and B, respectively, of which 200 appear in both cameras. The VIPeR dataset contains 632 persons from two cameras, with only one image per person in each camera. The SAIVT-SoftBio dataset is also designed for multi-shot re-identification, where images are also extracted from a short video containing a person. There are 152 persons from 8 different cameras. Since not every person appears in all cameras, following the evaluation setting in <ref type="bibr" target="#b35">[36]</ref>, we select those appearing in three cameras (i.e., cameras #3, #5 and #8) as our evaluation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We use a 2784-dimensional color and texture descriptor <ref type="bibr" target="#b23">[24]</ref> as our low level feature, which is composed of 8 color channels (RGB, HSV and YCbCr 1 ) and 19 texture channels (Gabor and Schmid). As for attributes, using this descriptor as x, we learn binary SVMs referring to <ref type="bibr" target="#b43">[44]</ref> to predict the same 20-bit attributes in <ref type="bibr" target="#b43">[44]</ref> for PRID and 90-bit attributes in <ref type="bibr" target="#b66">[67]</ref> for VIPeR. For other datasets, we learn attribute functions by <ref type="bibr" target="#b67">[68]</ref> in an unsupervised manner on the training set and generate 32-bit attributes with the descriptor as x. This overall representation is generated by concatenating the feature x and the output of attribute classifiers.</p><p>Following the standard evaluation protocols, we randomly select 150, 100 and 316 persons appearing in all cameras as our training set for iLIDS-VID, PRID and VIPeR, respectively. The remaining 150, 649 and 316 persons serve as the test set (galleries and probes). All the results are averaged over 10 random training/test splits. Parameters for learning are empirically set via cross-validation and fixed for all experiments. r 1 = 2, r 2 = 5 and Œª = 0.3 in Eq. ( <ref type="formula" target="#formula_0">3</ref>). Œ≥ = 0.5 in Eq. ( <ref type="formula" target="#formula_3">2</ref>). Iteration number P = 500 and threshold th = 10 -5 in Algorithm 1. If a classifier for a specific person is to be trained in the experiment, all images of this person are used as positive samples while images of 1. Only one of the luminance channels (V and Y) is used.</p><p>other people are used as negative samples. Consequently, the positive/negative data ratio is highly imbalanced. We thus randomly select negative samples for training according to a 1:4 positive/negative ratio. Note that, this learning procedure is independent for each person. Therefore, all the classes (persons) can be trained in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">iLIDS-VID</head><p>Among 150 persons in the test set, images from one camera are used as the probe set, while those from another camera serve as the gallery set. We first compare our approach with 8 competing learning-based methods for multi-shot re-identification: Salience Matching (Salmatch) <ref type="bibr" target="#b68">[69]</ref>, Learning Mid-level Filters (LM-F) <ref type="bibr" target="#b29">[30]</ref>, Multi-shot Symmetry-driven Accumulation of Local Features (MS-SDALF) <ref type="bibr" target="#b25">[26]</ref>, Multi-shot color with RankSVM (MS-color+RSVM) <ref type="bibr" target="#b15">[16]</ref>, Multi-shot color&amp;LBP with RankSVM (MS-color&amp;LBP+RSVM) <ref type="bibr" target="#b15">[16]</ref>, color&amp;LBP with Dynamic Time Warping (Color&amp;LBP+DTW) <ref type="bibr" target="#b7">[8]</ref>, HoGHoF with DTW (HOGHOF+DTW) <ref type="bibr" target="#b69">[70]</ref>, color&amp;LBP with Discriminative Video fragments selection and Ranking (MS-color&amp;LBP+DVR) <ref type="bibr" target="#b15">[16]</ref>. Note that, all of the above methods are trained by person IDs with supervised learning strategy. We use Cumulative Match Characteristic (CMC) curves to evaluate performance, and show experimental results in Figure <ref type="figure" target="#fig_2">3</ref> and Table <ref type="table" target="#tab_0">1</ref>.</p><p>Table <ref type="table" target="#tab_0">1</ref> clearly shows that our MTL-LOREA approach produces the best results consistently at different ranks. When inspecting the matching rate at rank 1 and rank 5, we find a relatively large improvement compared to the MS-color&amp;LBP+DVR approach that achieves the best performance among all the compared algorithms. Specifically, our method successfully improves the rank 1 accuracy from 34.5% to 43.0%, resulting in an 8.5% increase. In addition, we obtain nearly 100% matching rate at rank 50, while most of the compared methods only achieve 80% matching rate or even less at the same rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">PRID</head><p>Following the protocol in <ref type="bibr" target="#b64">[65]</ref>, we use images of 100 persons from camera A as the probe set, and 649 persons in camera B as the gallery set, excluding all training samples. We compare our algorithm with 11 supervised learning-based methods: Relaxed Pairwise Metric Learning (RPML) <ref type="bibr" target="#b7">[8]</ref>, Probabilistic Relative Distance Comparison (PRDC) <ref type="bibr" target="#b14">[15]</ref>, RankSVM (RSVM) <ref type="bibr" target="#b70">[71]</ref>, Salmatch <ref type="bibr" target="#b68">[69]</ref>, LMF <ref type="bibr" target="#b29">[30]</ref>, Pairwise Constrained Component Analysis (PCCA) <ref type="bibr" target="#b6">[7]</ref>, regularized PCCA (rPCCA) <ref type="bibr" target="#b11">[12]</ref>, Keep It Simple and Straightforward MEtric (KISSME) <ref type="bibr" target="#b10">[11]</ref>, kernel Local Fisher Discriminant Classifier (kLFDA) <ref type="bibr" target="#b11">[12]</ref>, Marginal Fisher Analysis (M-FA) <ref type="bibr" target="#b11">[12]</ref> and Kernel Canonical Correlation Analysis (KC-CA) <ref type="bibr" target="#b71">[72]</ref>. Among these compared methods, PRDC, PCCA, rPCCA, kLFDA and MFA use the same 2784-D low-level feature as our work. Note that, we do not compare with D-VR <ref type="bibr" target="#b15">[16]</ref> because DVR only uses 89 persons for testing, which does not follow the same protocol used by the aforementioned methods. We also use CMC curves to evaluate performance, as shown in Figure <ref type="figure" target="#fig_2">3</ref> and Table <ref type="table" target="#tab_1">2</ref>.  The experimental results show that our MTL-LOREA approach outperforms all existing methods by a large margin. In particular, our approach achieves 50% matching rate at rank 10, while the matching rates of compared approaches are mostly less than 30%. Except for our approach and KCCA, all other methods are only able to obtain a 50% matching rate at rank 55. Our approach also consistently outperforms KCCA, which shows the best performance at various ranks among the compared algorithms. Specifically, our absolute improvement of matching rate over KCCA is about 6% on average. The margin grows larger as we move from lower ranks to higher ranks. In terms of the accuracy at rank 1 and rank 5, our approach achieves a matching rate 18% at rank 1 and 37.4% at rank 5, respectively, leading to a 3.5% and 3.1% performance gain at rank 1 and rank 5 over KCCA. When evaluated with more retrieved samples, our approach still secures the best performance. It thus can be seen that pairwise distance metric learning based on camera pairs is clearly not as powerful as our approach. Although using kernel tricks, without fully investigating the relationships of features and attributes from multiple cameras, KCCA cannot improve the performance substantially. The experiments further verify that MTL-LOREA, which learns attribute correlations in an MTL setting with low rank embedding, successfully exploits relationships among attributes, thus produces a more discriminative model.</p><p>Since all the competing methods only use low level features while MTL-LOREA uses both low level features and attributes, we conduct additional experiments on the PRID dataset, where semantic attributes are provided, to  verify that the performance boost of MTL-LOREA results from our learning framework rather than attributes only. We collect publicly available implementations of 5 existing approaches, which are Salmatch <ref type="bibr" target="#b68">[69]</ref>, LMF <ref type="bibr" target="#b29">[30]</ref>, rPCCA <ref type="bibr" target="#b11">[12]</ref>, kLFDA <ref type="bibr" target="#b11">[12]</ref> and MFA <ref type="bibr" target="#b11">[12]</ref>. We concatenate the original binary attribute vectors and low level features used by each approach to form a set of new feature vectors, while keeping other parts of each implementation unchanged. For fair comparison, we use the default parameter settings provided by original authors for each implementation. The comparisons are shown in Figure <ref type="figure" target="#fig_3">4</ref> and Table <ref type="table" target="#tab_3">3</ref>.  With attribute added, all the 5 compared methods produce better results, justifying the use of attributes. Nevertheless, the performance of the 5 compared methods is still worse than that of our MTL-LOREA approach. This again verifies that our learning framework with MTL and low rank attribute embedding is effective in utilizing shared information amongst tasks, as well as exploiting attribute correlations, to improve the re-identification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">VIPeR</head><p>We apply data augmentation to generate more training samples for MTL-LORAE. Specifically, for each training image, we apply horizontal and vertical translation t ‚àà {-6, -3, 0, 3, 6} pixels and clockwise rotation r ‚àà {-5, 0, 5} degrees, resulting in totally 75 images per original training image.</p><p>We compare MTL-LORAE with some recent supervised learning-based methods, including KISSME <ref type="bibr" target="#b10">[11]</ref>, kLF-DA <ref type="bibr" target="#b11">[12]</ref>, KCCA <ref type="bibr" target="#b71">[72]</ref>, LOMO+XQDA <ref type="bibr" target="#b72">[73]</ref>, TSR <ref type="bibr" target="#b73">[74]</ref>, EP-KFM <ref type="bibr" target="#b74">[75]</ref>and MLAPG <ref type="bibr" target="#b75">[76]</ref>, as shown in Table <ref type="table" target="#tab_5">4</ref>. Our MTL-LORAE achieves the best accuracy at rank 1 and rank 5, outperforming existing methods by a large margin, and comparable results at rank 10 and rank 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">SAIVT-SoftBio</head><p>We use half of the persons as the training set and the remaining as the test set. In the test set, each image set serves as the probe while all the remaining image sets are regarded as the gallery. For fair comparison, we evaluate the performance using precision, recall, and F 1 -score by regarding the identification problem as a classification problem as <ref type="bibr" target="#b35">[36]</ref> does. We do not use the CMC score because it is not applicable to the scenario with more than two cameras. We compare our algorithm to RSVM <ref type="bibr" target="#b70">[71]</ref>, KISSME <ref type="bibr" target="#b10">[11]</ref>, RSVM with Conditional Random Field (R-CRF) <ref type="bibr" target="#b35">[36]</ref>, and KISSME with Conditional Random Field (K-CRF) <ref type="bibr" target="#b35">[36]</ref>. All of these compared methods use the same 2784-D low-level feature as our work. Results are averaged over all possible camera pairs of the three cameras, and are presented in Table <ref type="table" target="#tab_6">5</ref>.</p><p>The table shows that our MTL-LOREA is able to achieve the best F 1 -score, outperforming the best of existing method, K-CRF, by 4.6%. In addition, MTL-LOREA achieves the second best recall rate and comparable precision rate. We also note that our learning framework can learn the models for all cameras simultaneously regardless of the number of cameras, which is more computationally efficient than existing methods that explicitly deal with all pairs of cameras.</p><p>In addition to the above comparisons, we further show comparisons of our approach and other competing methods with respect to each pair of cameras separately in Table <ref type="table" target="#tab_7">6</ref>. Compared with 4 competing methods, our MTL-LOREA approach achieves better or comparable precision and recall, and the best F 1 -score on all the camera pairs, showing its outstanding capability of discovering and identifying a person accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance using Deep Features</head><p>As deep learning has shown promising performance and generalization ability in vision tasks, we also consider to incorporate deep features into our MTL-LOREA algorithm as another type of feature representation. In this experiment, we use the output of VGG-16 network <ref type="bibr" target="#b76">[77]</ref> pre-trained on the ImageNet image classification task [78] as deep features. The VGG-16 network includes 13 convolutional layers, followed by 3 fully-connected layers.</p><p>To improve the performance of deep features, we finetune the VGG-16 network using more than 40,000 samples of 152 persons, which are taken by cameras #1, #2, #4, #6 and #7 on SAIVT-SoftBio dataset. The fine-tuning procedure is conducted in a classification task, where the person IDs are used as class labels. All parameters are the same as those in <ref type="bibr" target="#b76">[77]</ref>. A total of 40,000 iterations are performed. Then, we use the fine-tuned VGG-16 model to extract features of images from cameras #3, #5 and #8 of SAIVT-SoftBio dataset and other datasets as our low-level features x. We select the 4096-dim output of FC7 layer (the second fully-connected layer) as the deep feature, and denote it as VGG-FC7. We run our MTL-LOREA with deep features on the four datasets. The results are summarized in Table <ref type="table" target="#tab_8">7</ref> and Table <ref type="table" target="#tab_9">8</ref>, respectively. In the tables, VGG-FC7 means that we directly use the output of FC7 layer in the VGG-16 network as feature representation and match them using Euclidean distance.</p><p>We do not use the feature of the output layer, i.e., score vector, because the score vector is more related with the training data. As the training data and test data do not share any overlap, the FC7 feature outperforms the score vector. MTL-LOREA-VGG means that the features of the FC7 layer are used as a substitution for low-level features in multi-task learning under MTL-LOREA.</p><p>Table <ref type="table" target="#tab_8">7</ref> shows the results on the iLIDS-VID dataset, PRID dataset and VIPeR dataset, where the CMC scores of VGG-FC7 at rank 1 are 24.1%, 19.8% and 25.1%, respectively. This means that deep features are not discriminative enough to distinguish different persons without being fine-tuned on the target datasets. On the other hand, the CMC scores of MTL-LOREA-VGG at rank 1 are 56.4%, 21.0% and 45.4% on the three datasets, which are 13.4%, 3.0% and 3.1% higher than those of MTL-LOREA, respectively. It demonstrates that our framework further boosts the person reidentification performance by integrating with deep features. The above experiments clearly verify that our MTL-LOREA framework is effective in correctly matching images from the same person, and is not dependent upon specific features.</p><p>Since VGG-16 is fine-tuned on SAIVT-SoftBio, it can be seen from the Table <ref type="table" target="#tab_9">8</ref> that VGG-FC7 has higher Precision, Recall and F 1 -score than MTL-LOREA. This is reasonable because deep features fine-tuned on the same dataset commonly perform better. However, the F 1 -score of MTL-LOREA-VGG shows improvements of 5.8% on C3-C5-C8, 7.9% on C3-C5, 1.8% on C3-C8, and 12.7% C5-C8, respectively, compared to the results of VGG-FC7. Moreover, MTL-LOREA-VGG also produces significantly higher F 1 -score than MTL-LOREA, i.e., about 15% improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Discussion</head><p>In this section, we conduct more experiments to show the characteristics and some interesting aspects of the proposed methods.   other one. When fixing Z, we obtain the Eq. ( <ref type="formula" target="#formula_7">5</ref>), which can be solved by MixedNorm approach in <ref type="bibr" target="#b53">[54]</ref>. The optimization algorithm of MixedNorm approach <ref type="bibr" target="#b53">[54]</ref> guarantees the global convergence with a convergence rate O(1/k 2 ), where k is the iteration number. On the other hand, when fixing W, both the loss function Z and regularization term h Z in Eq. ( <ref type="formula" target="#formula_12">8</ref>) are convex, so that a global optimal solution can be acquired. By adopting the Accelerated Gradient Method (AGM) in <ref type="bibr" target="#b50">[51]</ref>, we can achieve a convergence rate as O(1/k 2 ). Proofs of the convergence rate can be found in <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b53">[54]</ref> and <ref type="bibr">[79]</ref>. Therefore, our approach will find the global optimal via alternating optimization.</p><p>To investigate the convergence rate of MTL-LOREA, we show the values of objective function during the optimization in Figure <ref type="figure" target="#fig_5">5</ref>. The optimization is conducted on the training samples of a person randomly selected from iLIDS-VID and PRID. The figure shows that the objective function value quickly decreases and reaches its minimal after a few iterations, verifying the effectiveness of our optimization strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Analysis on Transformation Matrix</head><p>Based on the assumption that attributes are correlated, we propose to learn the low rank matrix Z to preserve attribution correlations. In Figure <ref type="figure">6</ref> and Figure <ref type="figure">7</ref>, we show some representative examples of learned attribute relations by Z. In these figures, the averaged correlation and mean absolute error over all persons are shown on the PRID and VIPeR datasets.</p><p>Because Z is trained for each specific person, the mean absolute error in Figure <ref type="figure">6</ref> and Figure <ref type="figure">7</ref> essentially represents the variability in attribute-projection that is required to support a good target person-classifier.</p><p>It can be seen from Figure <ref type="figure">6</ref> and Figure <ref type="figure">7</ref> that, the values of averaged correlation are generally larger than the ones of   mean absolute error. It means that the learned correlations between attributes stay relatively stable across different persons. Meanwhile, the figures also demonstrate that different persons do have different sensitivities to attribute correlations. For example, the correlations between attributes like darkhair and barelegs show large mean absolute error across different persons. This means the correlation between darkhair and barelegs has diverse impact in identifying different persons. Therefore, it is necessary to train the low rank matrix Z for each person to encode the character of each person. We use the trained matrix Z on each person rather than a global Z also because such person-specific Z is easier to optimize and could better avoid under-fitting. It can be observed from Figure <ref type="figure">6</ref> and Figure <ref type="figure">7</ref> that, some attributes are closely related and frequently co-occur in the same image. They reasonably have higher averaged correlation scores, e.g., the attributes shorts and barelegs. In contrast, a person cannot wear light bottoms(or light shirt) and dark bottoms (or dark shirt) at the same time. It is reasonable to observe that such attributes have negative correlations. Attributes hairlong and hairshort are also negatively correlated.</p><p>Similarly, the attribute carryingNothing has negative correlations with both the attributes carryingMessengerBag and carryingOther because a person is unlikely to carry different bags simultaneously. Therefore, the learned attribute correlations are reasonable. We thus use the learned correlations to update the initial attributes to improve their accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Evaluation of Individual Components</head><p>To verify the effects of individual components in our framework and show that each of them contributes to the perfor-  <ref type="formula" target="#formula_6">4</ref>), which embeds original attributes to a possible full rank space by making attributes highly uncorrelated. We denote this variant as MTL-FR. The three variants are respectively evaluated on iLIDS-VID, PRID and VIPeR datasets to see how each component affects the performance. We show CMC scores in Table <ref type="table" target="#tab_10">9</ref>. The results by STL are always worse than those by MTL-LOREA and the other two MTL-based variants. This indicates that learning related tasks simultaneously successfully exploits shared information amongst tasks and increases the discriminative ability of the learned model.</p><p>We also find that MTL-FR is inferior to MTL-Att. This suggests that assuming attributes are uncorrelated is unreasonable and degrades the performance of original attributes. However, only using the original attributes without investigating their correlations, MTL-Att cannot produce the best results. The experiments reveal that each of the above mentioned components is important in improving the performance. By integrating all of them, our approach exhibits the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.4">Evaluation of Parameter Sensitivity</head><p>There are two important parameters in our formulation, Œ≥ in Eq. ( <ref type="formula" target="#formula_3">2</ref>) which controls the contribution of attribute embedding error term and Œª in Eq. (3) which controls the importance of sparse regularization. To demonstrate that our MTL-LOREA approach is not sensitive to these parameters, we conduct experiments by changing the parameters and evaluate the performance of MTL-LOREA on three datasets. During our experiments, we vary one parameter while keeping another one and all the other parts fixed. Results are shown in <ref type="bibr">Table 10,</ref><ref type="bibr">11 and 12.</ref> Even though the parameters change within a relatively large range, i.e., the maximal value is 20 times of the minimal value, the performance by MTL-LOREA only slightly changes, i.e., the largest absolute change is no more than 7%. Actually, the absolute change is less than 3% for most  cases, which is negligible given the significant improvement over existing approaches. The experimental results clearly demonstrate that the proposed MTL-LOREA is robust enough and not sensitive to the above mentioned parameters. Therefore, our MTL-LOREA approach does not rely on parameter tuning to obtain outstanding performance, making it suitable for practical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.5">Weakness</head><p>Our approach performs fairly well in dealing with multishot datasets, as shown from the experimental results, but there still are several issues to address. 1) When the model is trained under single-shot scenario, it must use data augmentation to generate more training samples. For example, we need to expand one image into 75 images through rotation and translation on the VIPeR dataset. 2) When performing person re-identification, our method uses person-specific classifiers. Therefore, we need to train again for each additional person added to the dataset, which takes more time.</p><p>3) It is difficult to encode shared information across cameras for every person because there are few cameras but many images of persons. In the future work, we will consider two alternatives: (a) cameras are treated independently, while all persons from a camera share common characteristics; (b) information is shared across both persons and cameras, which could be a more effective solution. 4) Attributes provide auxiliary information apart from low-level features. However, it is usually expensive and impractical to obtain attributes from manual annotations. Even though data driven attribute can be learned, it still requires additional training and annotation efforts. Therefore, it is hard to perform person re-identification tasks with limited training data. Our future work will work on one-shot attribute learning algorithm to address this problem. We will also combine better features to further improve the performance of our method. 5) Due to its powerful feature learning ability, deep model has shown promising performance on person Re-ID.</p><p>Previous work <ref type="bibr" target="#b51">[52]</ref> has implement a two-side neural network for multi-task and multi-domain learning. It is possible to propose a deep neural network structure that implements multi-task learning and attribute embedding. This will be explored in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We have proposed a multi-task learning (MTL) formulation with low rank attribute embedding for person reidentification. Multiple cameras are treated as related tasks, whose relationships are decomposed as a low rank structure shared by all tasks and task-specific sparse components for individual tasks by MTL. Both low level features and semantic/data-driven attributes are used. We have further proposed a low rank attribute embedding that learns attributes correlations to convert original binary attributes to continuous attributes, where incorrect and incomplete attributes are rectified and recovered. Our objective function can be effectively solved by an alternating optimization under proper relaxation. Experiments on four datasets have demonstrated the outstanding performance and robustness of the proposed approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 )Fig. 1 .</head><label>11</label><figDesc>Fig. 1. Illustration of low rank attribute embedding with three attribute vectors from task T 1 as examples.With the learned transformation matrix, the original binary attributes are converted to continuous attributes. Semantically related attributes are recovered even though they are absent in the original attribute vectors, i.e., the attribute female is non-zero in the embedded attribute vector due to the presence of both skirt and handbag, even though its value is 0 in the original attribute vector a1  3 .</figDesc><graphic coords="4,308.46,44.07,252.00,93.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of our MTL-LORAE framework.</figDesc><graphic coords="5,91.64,253.27,162.93,73.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. CMC curves of our approach and state-of-the-art approaches on the iLIDS-VID dataset (left) and PRID dataset (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.<ref type="bibr" target="#b3">4</ref>. CMC curves of our approach and 5 state-of-the-art approaches with attributes added on the PRID dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The values of objective function during optimization on the iLIDS-VID dataset (top) and PRID dataset (bottom).</figDesc><graphic coords="11,370.79,392.17,185.88,111.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. Examples of attribute correlations learned on the PRID dataset. The averaged correlation and mean absolute error across different persons are shown in each example.</figDesc><graphic coords="11,370.79,539.29,185.88,116.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>TABLE 10   CMC scores at rank 1, 5 and 10 by MTL-LOREA with varying Œ≥ (importance of attribute embedding error term) and Œª (sparse regularization) on iLIDS-VID dataset. Numbers indicate the percentage (%) of correct matches within a specific rank. we evaluate three variants of our approach. Instead of using multi-task learning, we assume tasks are independent and learn classifiers for each task separately. The resulting classifiers are acquired based on Single Tasks Learning (STL). In this way, STL trains its classifiers under different cameras separately, without sharing any data. We also use the original attributes without embedding. We thus have another variant denoted as MTL-Att by discarding the embedding error term in the objective function in Eq. (2). It means during training and testing of MTL-Att, x is set as [x; a]. In addition, we remove the low rank constraint on Z in Eq. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>CMC scores of ranks from 1 to 50 on the iLIDS-VID dataset. Numbers indicate the percentage (%) of correct matches within a specific rank.</figDesc><table><row><cell>Rank</cell><cell>1</cell><cell>5</cell><cell>10 20 30 50</cell></row><row><cell>Salmatch [69]</cell><cell cols="3">8.0 24.8 35.4 52.9 61.3 74.8</cell></row><row><cell>LMF [30]</cell><cell cols="3">11.7 29.0 40.3 53.4 64.3 78.8</cell></row><row><cell>MS-SDALF [26]</cell><cell cols="3">5.1 19.0 27.1 37.9 47.5 62.4</cell></row><row><cell>MS-color+RSVM [16]</cell><cell cols="3">16.4 37.3 48.5 62.6 70.7 80.6</cell></row><row><cell cols="4">MS-color&amp;LBP+RSVM [16] 20.0 44.0 52.7 68.0 78.7 84.7</cell></row><row><cell>Color&amp;LBP+DTW [16]</cell><cell cols="3">9.3 21.6 29.5 43.0 49.1 61.0</cell></row><row><cell>HoGHoF+DTW [16]</cell><cell cols="3">5.3 16.0 29.7 44.7 53.1 66.7</cell></row><row><cell cols="4">MS-color&amp;LBP+DVR [16] 34.5 56.4 67.0 77.4 84.0 91.7</cell></row><row><cell>MTL-LOREA</cell><cell cols="3">43.0 60.0 70.2 85.3 90.2 96.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2</head><label>2</label><figDesc>CMC scores of ranks from 1 to 50 on the PRID dataset. Numbers indicate the percentage (%) of correct matches within a specific rank.</figDesc><table><row><cell>Rank</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>50</cell></row><row><cell>RPML [8]</cell><cell cols="6">4.8 14.3 21.6 30.2 37.2 48.1</cell></row><row><cell>PRDC [15]</cell><cell cols="6">4.5 12.6 19.7 29.5 35.8 46.0</cell></row><row><cell>RSVM [71]</cell><cell cols="6">6.8 16.5 22.7 31.5 38.4 49.3</cell></row><row><cell cols="7">Salmatch [69] 4.9 17.5 26.1 33.9 40.5 47.8</cell></row><row><cell>LMF [30]</cell><cell cols="6">12.5 23.9 30.7 36.5 42.6 51.6</cell></row><row><cell>PCCA [7]</cell><cell cols="6">3.5 10.9 17.9 27.1 34.2 45.0</cell></row><row><cell>rPCCA [12]</cell><cell cols="6">3.8 12.3 18.3 27.5 35.2 45.4</cell></row><row><cell cols="7">KISSME [11] 4.1 12.8 21.1 31.8 40.7 52.5</cell></row><row><cell>kLFDA [12]</cell><cell cols="6">7.6 18.9 25.6 37.4 46.7 58.5</cell></row><row><cell>MFA [12]</cell><cell cols="6">7.2 18.7 27.6 39.1 47.4 58.7</cell></row><row><cell cols="7">KCCA [72] 14.5 34.3 46.7 59.1 67.2 75.4</cell></row><row><cell cols="2">MTL-LOREA 18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>.0 37.4 50.1 66.6 73.1 82.3</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Matching Rate (%)</cell><cell>40 60 80 20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Salmatch+Att LMF+Att rPCCA+Att kLFDA+Att</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MFA+Att</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MTL-LORAE</cell></row><row><cell></cell><cell>0 0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>60</cell><cell>70</cell><cell>80</cell><cell>90</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Rank</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3</head><label>3</label><figDesc>CMC scores of our approach and 5 state-of-the-art approaches with attributes added at ranks from 1 to 50 on the PRID dataset. Numbers indicate the percentage (%) of correct matches within a specific rank."Att" indicates attributes are added to the original features.</figDesc><table><row><cell>Rank</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>50</cell></row><row><cell cols="7">Salmatch [69] 4.9 17.5 26.1 33.9 40.5 47.8</cell></row><row><cell cols="7">Salmatch+Att 9.6 22.6 30.2 38.8 44.8 53.1</cell></row><row><cell>LMF [30]</cell><cell cols="6">12.5 23.9 30.7 36.5 42.6 51.6</cell></row><row><cell>LMF+Att</cell><cell cols="6">15.0 26.2 33.6 39.3 44.1 54.7</cell></row><row><cell>rPCCA [12]</cell><cell cols="6">3.8 12.3 18.3 27.5 35.2 45.4</cell></row><row><cell>rPCCA+Att</cell><cell cols="6">8.7 14.4 20.8 31.5 36.0 46.7</cell></row><row><cell>kLFDA [12]</cell><cell cols="6">7.6 18.9 25.6 37.4 46.7 58.5</cell></row><row><cell>kLFDA+Att</cell><cell cols="6">9.4 22.0 30.2 44.1 53.9 66.8</cell></row><row><cell>MFA [12]</cell><cell cols="6">7.2 18.7 27.6 39.1 47.4 58.7</cell></row><row><cell>MFA+Att</cell><cell cols="6">10.7 22.1 32.0 47.3 53.8 63.7</cell></row><row><cell cols="2">MTL-LOREA 18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>.0 37.4 50.1 66.6 73.1 82.3</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4</head><label>4</label><figDesc>CMC scores of ranks from 1 to 20 on the VIPeR dataset. Numbers indicate the percentage (%) of correct matches within a specific rank.</figDesc><table><row><cell>Rank</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>KISSME [11]</cell><cell>19.6</cell><cell>47.5</cell><cell>62.2</cell><cell>77.0</cell></row><row><cell>kLFDA [12]</cell><cell>32.2</cell><cell>65.8</cell><cell>79.7</cell><cell>90.9</cell></row><row><cell>KCCA [72]</cell><cell>37.3</cell><cell>71.4</cell><cell>84.6</cell><cell>92.3</cell></row><row><cell>LOMO + XQDA [73]</cell><cell>40.0</cell><cell>68.9</cell><cell>81.5</cell><cell>91.1</cell></row><row><cell>TSR [74]</cell><cell>31.6</cell><cell>68.6</cell><cell>82.8</cell><cell>94.6</cell></row><row><cell>EPKFM [75]</cell><cell>36.8</cell><cell>70.4</cell><cell>83.7</cell><cell>91.7</cell></row><row><cell>MLAPG [76]</cell><cell>40.7</cell><cell>69.9</cell><cell>82.3</cell><cell>92.4</cell></row><row><cell>MTL-LORAE</cell><cell>42.3</cell><cell>72.2</cell><cell>81.6</cell><cell>89.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5</head><label>5</label><figDesc>Comparison of precision, recall and F 1 -score (in %) by existing methods and our approach on SAIVT-SoftBio dataset.</figDesc><table><row><cell></cell><cell cols="5">RSVM [71] KISSME [11] R-CRF [36] K-CRF [36] MTL-LOREA</cell></row><row><cell>Precision</cell><cell>22.0</cell><cell>19.7</cell><cell>53.7</cell><cell>50.3</cell><cell>45.2</cell></row><row><cell>Recall</cell><cell>42.1</cell><cell>66.1</cell><cell>39.4</cell><cell>49.8</cell><cell>63.7</cell></row><row><cell>F1-score</cell><cell>26.2</cell><cell>29.5</cell><cell>42.0</cell><cell>48.3</cell><cell>52.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6</head><label>6</label><figDesc>Comparison of precision, recall and F 1 -score (in %) regarding all camera pairs by existing methods and our approach on SAIVT-SoftBio dataset. C3, C5 and C8 represent cameras #3, #5 and #8.</figDesc><table><row><cell></cell><cell cols="5">RSVM [71] KISSME [11] R-CRF [36] K-CRF [36] MTL-LOREA</cell></row><row><cell>C3-C5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Precision</cell><cell>14.9</cell><cell>15.9</cell><cell>37.2</cell><cell>38.0</cell><cell>38.1</cell></row><row><cell>Recall</cell><cell>24.7</cell><cell>50.3</cell><cell>15.5</cell><cell>28.5</cell><cell>75.1</cell></row><row><cell>F1-score</cell><cell>15.9</cell><cell>23.4</cell><cell>18.2</cell><cell>30.3</cell><cell>50.5</cell></row><row><cell>C3-C8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Precision</cell><cell>27.7</cell><cell>20.7</cell><cell>55.4</cell><cell>48.4</cell><cell>41.0</cell></row><row><cell>Recall</cell><cell>29.4</cell><cell>70.1</cell><cell>43.1</cell><cell>51.1</cell><cell>65.6</cell></row><row><cell>F1-score</cell><cell>20.1</cell><cell>31.0</cell><cell>43.4</cell><cell>47.6</cell><cell>50.4</cell></row><row><cell>C5-C8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Precision</cell><cell>25.7</cell><cell>19.9</cell><cell>45.2</cell><cell>47.1</cell><cell>36.8</cell></row><row><cell>Recall</cell><cell>43.4</cell><cell>65.4</cell><cell>30.8</cell><cell>44.7</cell><cell>53.8</cell></row><row><cell>F1-score</cell><cell>24.6</cell><cell>29.6</cell><cell>32.4</cell><cell>43.7</cell><cell>43.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 7</head><label>7</label><figDesc>CMC scores of MTL-LOREA with deep features, i.e., percentage (%) of correct matches, of ranks 1, rank 5, rank 10 and rank 20 on the iLIDS-VIDdataset, PRID dataset and VIPeR dataset.</figDesc><table><row><cell></cell><cell>Methods</cell><cell cols="4">Rank 1 Rank 5 Rank 10 Rank 20</cell></row><row><cell>iLIDS-VID</cell><cell cols="2">VGG-FC7 MTL-LOREA MTL-LOREA-FC7 56.4 24.1 43.0</cell><cell>43.6 60.0 69.0</cell><cell>52.8 70.2 78.4</cell><cell>65.6 85.3 87.4</cell></row><row><cell></cell><cell>VGG-FC7</cell><cell>19.8</cell><cell>28.5</cell><cell>42.4</cell><cell>53.9</cell></row><row><cell>PRID</cell><cell>MTL-LOREA</cell><cell>18.0</cell><cell>37.4</cell><cell>50.1</cell><cell>66.6</cell></row><row><cell></cell><cell cols="2">MTL-LOREA-FC7 21.0</cell><cell>44.0</cell><cell>55.9</cell><cell>68.7</cell></row><row><cell></cell><cell>VGG-FC7</cell><cell>25.1</cell><cell>39.8</cell><cell>48.5</cell><cell>60.6</cell></row><row><cell>VIPeR</cell><cell>MTL-LOREA</cell><cell>42.3</cell><cell>72.2</cell><cell>81.6</cell><cell>89.6</cell></row><row><cell></cell><cell cols="2">MTL-LOREA-FC7 45.4</cell><cell>76.6</cell><cell>85.3</cell><cell>91.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 8</head><label>8</label><figDesc>Comparison of precision, recall andF 1 -score (in %) regarding all camera pairs by our approach with deep features on SAIVT-SoftBio dataset. C3, C5 and C8 represent cameras #3, #5 and #8.</figDesc><table><row><cell></cell><cell cols="3">VGG-FC7 MTL-LOREA MTL-LOREA-FC7</cell></row><row><cell>C3-C5-C8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Precision</cell><cell>54.4</cell><cell>45.2</cell><cell>57.5</cell></row><row><cell>Recall</cell><cell>69.0</cell><cell>63.7</cell><cell>79.5</cell></row><row><cell>F1-score</cell><cell>60.9</cell><cell>52.9</cell><cell>66.7</cell></row><row><cell>C3-C5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Precision</cell><cell>45.9</cell><cell>38.1</cell><cell>50.8</cell></row><row><cell>Recall</cell><cell>69.1</cell><cell>75.1</cell><cell>79.5</cell></row><row><cell>F1-score</cell><cell>54.8</cell><cell>50.5</cell><cell>62.7</cell></row><row><cell>C3-C8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Precision</cell><cell>57.9</cell><cell>41.0</cell><cell>55.9</cell></row><row><cell>Recall</cell><cell>73.0</cell><cell>65.6</cell><cell>81.9</cell></row><row><cell>F1-score</cell><cell>64.5</cell><cell>50.4</cell><cell>66.3</cell></row><row><cell>C5-C8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Precision</cell><cell>32.2</cell><cell>36.8</cell><cell>44.7</cell></row><row><cell>Recall</cell><cell>66.0</cell><cell>53.8</cell><cell>74.6</cell></row><row><cell>F1-score</cell><cell>43.2</cell><cell>43.7</cell><cell>55.9</cell></row><row><cell cols="2">4.5.1 Convergence Analysis</cell><cell></cell><cell></cell></row><row><cell cols="4">Our original formulation in Eq. (4) is difficult to optimize.</cell></row></table><note><p>We solve this problem by alternatively optimizing the objective function with respect to one variable and fixing the</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 9</head><label>9</label><figDesc>CMC scores of ranks from 1 to 50 on the iLIDS-VID, PRID and VIPeR datasets by STL, MTL-Att, MTL-FR and the complete MTL-LOREA. Numbers indicate the percentage (%) of correct matches within a specific rank.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">iLIDS-VID</cell><cell></cell><cell></cell></row><row><cell>Rank</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>50</cell></row><row><cell>STL</cell><cell cols="6">14.7 42.7 41.8 58.5 83.5 91.7</cell></row><row><cell>MTL-FR</cell><cell cols="6">37.7 54.0 47.4 64.9 85.3 92.5</cell></row><row><cell>MTL-Att</cell><cell cols="6">40.5 54.9 47.5 64.2 84.2 91.2</cell></row><row><cell cols="7">MTL-LOREA 43.0 60.0 70.2 85.3 90.2 96.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">PRID</cell><cell></cell><cell></cell></row><row><cell>Rank</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>50</cell></row><row><cell>STL</cell><cell cols="6">11.3 27.9 41.8 53.0 68.5 74.6</cell></row><row><cell>MTL-FR</cell><cell cols="6">11.3 34.1 47.4 61.1 69.8 79.0</cell></row><row><cell>MTL-Att</cell><cell cols="6">12.2 34.7 47.5 61.7 70.9 79.8</cell></row><row><cell cols="2">MTL-LOREA 18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>.0 37.4 50.1 66.6 73.1 82.3</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">VIPeR</cell><cell></cell><cell></cell></row><row><cell>Rank</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>50</cell></row><row><cell>STL</cell><cell cols="6">13.3 27.4 32.8 42.7 56.2 68.3</cell></row><row><cell>MTL-FR</cell><cell cols="6">35.3 63.3 75.6 83.8 89.9 94.4</cell></row><row><cell>MTL-Att</cell><cell cols="6">37.2 64.2 76.3 84.9 91.4 95.3</cell></row><row><cell>MTL-LOREA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>42.3 72.2 81.6 89.6 93.1 97.4</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 11 CMC</head><label>11</label><figDesc>scores at rank 1, 5 and 10 by MTL-LOREA with varying Œ≥ (importance of attribute embedding error term) and Œª (sparse regularization) on PRID dataset. Numbers indicate the percentage (%) of correct matches within a specific rank.</figDesc><table><row><cell>Œ≥</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>Œª</cell><cell>1</cell><cell>5</cell><cell>10</cell></row><row><cell cols="4">0.1 12.0 38.2 49.3</cell><cell cols="4">0.05 16.6 36.9 49.1</cell></row><row><cell cols="4">0.3 17.5 36.1 50.6</cell><cell>0.1</cell><cell cols="3">15.4 37.0 48.6</cell></row><row><cell cols="4">0.5 18.0 37.4 50.1</cell><cell>0.3</cell><cell cols="3">18.0 37.4 50.1</cell></row><row><cell>1</cell><cell cols="3">16.3 36.7 47.2</cell><cell>0.5</cell><cell cols="3">17.2 37.4 50.0</cell></row><row><cell>2</cell><cell cols="3">18.2 40.1 53.7</cell><cell>1</cell><cell cols="3">19.1 37.3 51.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 12 Precision</head><label>12</label><figDesc>(P), recall (R) and F 1 -score (in %) by MTL-LOREA with varying Œ≥ (importance of attribute embedding error term) and Œª (sparse regularization) on SAIVT-SoftBio dataset.</figDesc><table><row><cell>Œ≥</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>Œª</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell cols="4">0.1 43.2 64.1 51.7</cell><cell cols="4">0.05 43.9 64.4 52.2</cell></row><row><cell cols="4">0.3 46.0 63.7 53.4</cell><cell>0.1</cell><cell cols="3">44.0 63.4 51.9</cell></row><row><cell cols="4">0.5 45.2 63.7 52.9</cell><cell>0.3</cell><cell cols="3">45.2 63.7 52.9</cell></row><row><cell>1</cell><cell cols="3">44.3 63.2 52.1</cell><cell>0.5</cell><cell cols="3">44.1 63.8 52.4</cell></row><row><cell>2</cell><cell cols="3">43.6 64.0 51.9</cell><cell>1</cell><cell cols="3">43.4 64.3 51.8</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>JOURNAL OF L A T E X CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Wen Gao is a professor of computer science at the Institute of Digital Media, EECS, Peking University. He is working in the areas of multimedia and computer vision, including video coding, video analysis, multimedia retrieval, face recognition, and multimodal interface. He is a fellow of IEEE, a fellow of ACM, and a member of Chinese Academy of Engineering.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A framework for learning predictive structures from multiple tasks and unlabeled data</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1817" to="1853" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-task learning for classification with dirichlet process priors</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krishnapuram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="35" to="63" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visual classification with multitask joint sparse representation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4349" to="4360" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scalable multitask representation learning for scene classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lapin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multitask learning: A knowledge-based source of inductive bias</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain transfer support vector ranking for person re-identification without target camera label information</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pedestrian recognition with a learned metric</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dikmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Relaxed pairwise learned metric for person re-identification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Local fisher discriminant analysis for pedestrian re-identification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pedagadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Orwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Velastin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boghossian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph embedding and extensions: A general framework for dimensionality reduction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="40" to="51" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName><forename type="first">M</forename><surname>√ñstinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Person reidentification using kernel-based metric learning methods</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sznaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pop: Person reidentification post-rank optimisation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning locally-adaptive decision functions for person verification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Re-identification by relative distance comparison</title>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Person re-identification by video ranking</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Heterogeneous face recognition using kernel prototype similarities</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1410" to="1422" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attribute and simile classifiers for face verification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reference-based person re-identification</title>
		<author>
			<persName><forename type="first">L</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kafai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bhanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AVSS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multitask learning with low rank attribute embedding for person reidentification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A survey of approaches and trends in person re-identification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bedagkar-Gala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="270" to="286" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">People reidentification in surveillance and forensics: A survey</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vezzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baltieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Appearance-based person reidentification in camera networks: problem overview and current approaches</title>
		<author>
			<persName><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rittscher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Ambient Intelligence and Humanized Computing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="151" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Person re-identification by symmetry-driven accumulation of local features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Custom pictorial structures for re-identification</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stoppa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Person re-identification: what features are important?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning midlevel filters for person reidentification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Person reidentification by attributes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Layne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning deep feature representations with domain guided dropout for person reidentification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A database for person re-identification in multi-camera surveillance networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bialkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Fookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DICTA</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">3dpes: 3d people dataset for surveillance and forensics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Baltieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vezzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM workshop</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Consistent re-identification in a camera network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Open-world person re-identification by multi-label assignment inference</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cancela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Joint learning of visual attributes, object classes and visual saliency</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A discriminative latent model of object classes and attributes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attribute-based transfer learning for object categorization with zero/one training example</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A joint learning framework for attribute models and object descriptions</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sellamanickam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Tree-structured CRF models for interactive image labeling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="476" to="489" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Labelembedding for attribute-based classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Towards person identification and re-identification with attributes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Layne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attributes-based reidentification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Layne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Person Re-Identification</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="93" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Re-id: Hunting attributes in the wild</title>
		<author>
			<persName><forename type="first">R</forename><surname>Layne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multi-label learning by exploiting label correlations locally</title>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Submodular multi-label learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Petterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Caetano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multi-label learning by exploiting label dependency</title>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Clustered multi-task learning via alternating structure optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Robust multi-task feature learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">An accelerated gradient method for trace norm minimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A unified perspective on multidomain and multi-task learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A convex formulation for learning shared structures from multiple tasks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning incoherent sparse and low-rank patterns from multiple tasks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning multiple tasks with kernel methods</title>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Micchelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="615" to="637" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning a kernel for multi-task clustering</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Online multi-task learning via sparse dictionary optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ruvolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eaton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Encoding tree sparsity in multi-task learning: A probabilistic framework</title>
		<author>
			<persName><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Predicting multiple attributes via relative multi-task learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1027" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Sharing features between objects and their attributes</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1761" to="1768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Domain transfer support vector ranking for person re-identification without target camera label information</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Person re-identification over camera networks using multi-task distance metric learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3656" to="3670" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning lowrank label correlations for multi-label classification with missing labels</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning to hash with binary reconstructive embeddings</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Person reidentification by descriptive and discriminative classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Beleznai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="91" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Evaluating appearance models for recognition, reacquisition, and tracking</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Performance Evaluation for Tracking and Surveillance</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Pedestrian attribute recognition at far distance</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="789" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Attribute discovery via predictable discriminative binary codes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Person re-identification by salience matching</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Person re-identification by support vector ranking</title>
		<author>
			<persName><forename type="first">B</forename><surname>Prosser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mary</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Matching people across camera views using kernel canonical correlation analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lisanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Del Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICDSC</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2197" to="2206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Transferring a semantic representation for person re-identification and search</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4184" to="4193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Similarity learning on an explicit polynomial kernel feature map for person re-identification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Efficient psd constrained asymmetric metric learning for person re-identification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>ab- s/1409.1556</idno>
		<editor>CoRR</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
