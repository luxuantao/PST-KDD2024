<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">First-order Stochastic Algorithms for Escaping From Saddle Points in Almost Linear Time</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yi</forename><surname>Xu</surname></persName>
							<email>yi-xu@uiowa.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Iowa</orgName>
								<address>
									<postCode>52246</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rong</forename><surname>Jin</surname></persName>
							<email>jinrong.jr@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="department">Machine Intelligence Technology</orgName>
								<orgName type="institution">Alibaba Group</orgName>
								<address>
									<postCode>98004</postCode>
									<settlement>Bellevue</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianbao</forename><surname>Yang</surname></persName>
							<email>tianbao-yang@uiowa.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Iowa</orgName>
								<address>
									<postCode>52246</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">First-order Stochastic Algorithms for Escaping From Saddle Points in Almost Linear Time</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9068B0A107A5E57940891E03AFBF40E1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we consider first-order methods for solving stochastic non-convex optimization problems. The key building block of the proposed algorithms is firstorder procedures to extract negative curvature from the Hessian matrix through a principled sequence starting from noise, which are referred to NEgative-curvature-Originated-from-Noise or NEON and are of independent interest. Based on this building block, we design purely first-order stochastic algorithms for escaping from non-degenerate saddle points with a much better time complexity (almost linear time in the problem's dimensionality) under a bounded variance condition of stochastic gradients than previous first-order stochastic algorithms. In particular, we develop a general framework of first-order stochastic algorithms with a secondorder convergence guarantee based on our new technique and existing algorithms that may only converge to a first-order stationary point. For finding a nearly second-order stationary point x such that ∇F (x) ≤ and ∇ 2 F (x) ≥ -√ I (in high probability), the best time complexity of the presented algorithms is O(d/ 3.5 ), where F (•) denotes the objective function and d is the dimensionality of the problem. To the best of our knowledge, this is the first theoretical result of first-order stochastic algorithms with an almost linear time in terms of problem's dimensionality for finding second-order stationary points, which is even competitive with existing stochastic algorithms hinging on the second-order information.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The problem of interest in this paper is Stochastic Non-Convex Optimization given by min</p><formula xml:id="formula_0">x∈R d F (x) E ξ [f (x; ξ)],<label>(1)</label></formula><p>where ξ is a random variable and f (x; ξ) is a random smooth non-convex function of x. The only information available of F (x) to us is sampled stochastic functions f (x; ξ) and their gradients.</p><p>A popular choice of algorithms for solving <ref type="bibr" target="#b0">(1)</ref> is (mini-batch) stochastic gradient descent (SGD) method and its variants <ref type="bibr" target="#b5">[6]</ref>. However, these algorithms do not necessarily guarantee to escape from a saddle point (more precisely a non-degenerate saddle point) x satisfying that: ∇F (x) = 0 and the minimum eigen-value of ∇ 2 F (x)) is less than 0. Recently, new variants of SGD by adding isotropic noise into the stochastic gradient were proposed (noisy SGD <ref type="bibr" target="#b4">[5]</ref>, stochastic gradient Langevin dynamics (SGLD) <ref type="bibr" target="#b22">[23]</ref>). These two works provide rigorous analyses of the noise-injected update for escaping from a saddle point. Unfortunately, both variants suffer from a polynomial time complexity with a super-linear dependence on the dimensionality d (at least a power of 4), which renders them not practical for optimizing problems of high dimension.</p><p>On the other hand, second-order information carried by the Hessian has been utilized to escape from a saddle point, which usually yields an almost linear time complexity in terms of the dimensionality d under the assumption that the Hessian-vector product (HVP) can be performed in a linear time. In Table <ref type="table">1</ref>: Comparison with existing Stochastic Algorithms for achieving an ( , γ)-SSP to <ref type="bibr" target="#b0">(1)</ref>, where p is a number at least 4, IFO (incremental first-order oracle) and ISO (incremental second-order oracle) are terminologies borrowed from <ref type="bibr" target="#b19">[20]</ref>, representing ∇f (x; ξ) and ∇ 2 f (x; ξ)v respectively, T h denotes the runtime of ISO and T g denotes the runtime of IFO. O(•) hides a poly-logarithmic factor. SM refers to stochastic momentum methods. For γ, we only consider as lower as 1/2 . Algorithm Oracle Target Time Complexity</p><p>Noisy SGD <ref type="bibr" target="#b4">[5]</ref> IFO ( , 1/2 )-SSP, high probability O (T g d p -p ) SGLD <ref type="bibr" target="#b22">[23]</ref> IFO ( , 1/2 )-SSP, high probability O T g d p -4</p><p>Natasha2 <ref type="bibr" target="#b0">[1]</ref> IFO + ISO ( , 1/2 )-SSP, expectation O T g -3.5 + T h -2.5</p><p>Natasha2 <ref type="bibr" target="#b0">[1]</ref> IFO + ISO ( , 1/4 )-SSP, expectation O T g -3.25 + T h -1.75</p><p>SNCG <ref type="bibr" target="#b16">[17]</ref> IFO + ISO ( , 1/2 )-SSP, high probability O T g -4 + T h -2.5</p><p>SVRG-Hessian <ref type="bibr" target="#b19">[20]</ref> (finite-sum objectives) IFO + ISO ( , 1/2 )-SSP, high probability O T g (n 2/3 -2 + n -1.5 ) (n is number of components) +T h (n -1.5 + n </p><formula xml:id="formula_1">NEON-SVRG (this work) (finite sum) IFO ( , 1/2 )-SSP, high probability O T g n 2/3 -2 + n -1.5 + -2.75</formula><p>practice, HVP can be estimated by a finite difference approximation using two gradient evaluations. However, the rigorous analysis of algorithms using such noisy approximation for solving non-convex optimization remains unsolved, and heuristic approaches may suffer from numerical issues. Although for some problems with special structures (e.g., neural networks), HVP can be efficiently computed using gradients, a HVP-free method that can escape saddle points for a broader family of non-convex problems is still desirable.</p><p>This paper aims to design HVP-free stochastic algorithms for solving <ref type="bibr" target="#b0">(1)</ref>, which can converge to second order stationary points with a time complexity that is almost linear in the problem's dimensionality. Our main contributions are:</p><p>• As a key building block of proposed algorithms, first-order procedures (NEON) are proposed to extract negative curvature from the Hessian using a principled sequence starting from noise. Interestingly, our perspective of NEON connects the existing two classes of methods (noisebased and HVP-based) for escaping from saddle points. We provide a formal analysis of simple procedures based on gradient descent and accelerated gradient method for exacting a negative curvature direction from the Hessian.</p><p>• We develop a general framework of first-order algorithms for stochastic non-convex optimization by combining the proposed first-order NEON procedures to extract negative curvature with existing first-order stochastic algorithms that aim at a first-order critical point. We also establish the time complexities of several interesting instances of our general framework for finding a nearly ( , γ)second-order stationary point (SSP), i.e., ∇F (x) ≤ , and λ min (∇ 2 F (x)) ≥ -γ, where • represents Euclidean norm of a vector and λ min (•) denotes the minimum eigen-value. A summary of our results and existing results for Stochastic Non-Convex Optimization is presented in Table <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Other Related Work</head><p>SGD and its many variants (e.g., mini-batch SGD and stochastic momentum (SM) methods) have been analyzed for stochastic non-convex optimization <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">22]</ref>. The iteration complexities of all these algorithms is O(1/ 4 ) for finding a first-order stationary point (FSP) (in expectation E[ ∇F (x) 2  2 ] ≤ 2 or in high probability). Recently, there are some improvements for stochastic non-convex optimization. <ref type="bibr" target="#b13">[14]</ref> proposed a first-order stochastic algorithm (named SCSG) using the variance-reduction technique, which enjoys an iteration complexity of O(1/ -10/3 ) for finding an FSP (in expectation), i.e., E[ ∇F (x) 2  2 ] ≤ 2 . <ref type="bibr" target="#b0">[1]</ref> proposed a variant of SCSG (named Natasha1.5) with the same convergence and complexity. An important application of NEON is that previous stochastic algorithms that have a first-order convergence guarantee can be strengthened to enjoy a second-order convergence guarantee by leveraging the proposed first-order NEON procedures to escape from saddle points. We will analyze several algorithms by combining the updates of SGD, SM, and SCSG with the proposed NEON.</p><p>Several recent works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b19">20]</ref> propose to strengthen existing first-order stochastic algorithms to have second-order convergence guarantee by leveraging the second-order information. <ref type="bibr" target="#b16">[17]</ref> used mini-batch SGD, <ref type="bibr" target="#b19">[20]</ref> used SVRG for a finite-sum problem, and <ref type="bibr" target="#b0">[1]</ref> used a similar algorithm to SCSG for their first-order algorithms. The second-order methods used in these studies for computing negative curvature can be replaced by the proposed NEON procedures. It is notable although a generic approach for stochastic non-convex optimization was proposed in <ref type="bibr" target="#b19">[20]</ref>, its requirement on the first-order stochastic algorithms precludes many interesting algorithms such as SGD, SM, and SCSG. Stronger convergence guarantee (e.g., converging to a global minimum) of stochastic algorithms has been studied in <ref type="bibr" target="#b8">[9]</ref> for a certain family of problems, which is beyond the setting of the present work.</p><p>It is also worth mentioning that the field of non-convex optimization is moving so fast that similar results have appeared online after the preliminary version of this work <ref type="bibr" target="#b1">[2]</ref>. Allen-Zhu and Li <ref type="bibr" target="#b1">[2]</ref> proposed NEON2 for finding a negative curvature, which includes a stochastic version and a deterministic version. We notice several differences between the two works: (i) they used Gaussian random noise with a variance proportional to d -C , where C is a large unknown constant, in contrast our NEON and NEON+ procedures use random noise sampled from the sphere of an Euclidean ball with radius proportional to log -2 (d); (ii) the update of their deterministic NEON2 det is constructed based on the Chebyshev polynomial, in contrast our NEON+ with a similar iteration complexity is based on the well-known Nesterov's accelerated gradient method; (iii) we provide a general framework/analysis for promoting first-order algorithms to enjoy second-order convergence, which could be useful for promoting new first-order stochastic algorithms; (iv) the reported iteration complexity of their NEON2 online is better than our stochastic variants of NEON. However, in most cases the total complexity for finding an ( , √ )-SSP is dominated by the complexity for finding a stationary point not by the complexity of stochastic NEON for finding a negative curvature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>Let • denote the Euclidean norm of a vector and • 2 denote the spectral norm of a matrix. Let S d r denote the sphere of an Euclidean ball centered at zero with radius r, and [t] denote a set {0, . . . , t}. A function f (x) has a L 1 -Lipschitz continuous gradient if it is differentiable and there exists L 1 &gt; 0 such that ∇f (x) -∇f (y) ≤ L 1 xy holds for any x, y ∈ R d . A function f (x) has a L 2 -Lipschitz continuous Hessian if it is twice differentiable and there exists</p><formula xml:id="formula_2">L 2 &gt; 0 such that ∇ 2 f (x) -∇ 2 f (y) 2 ≤ L 2 x -y holds for any x, y ∈ R d . It implies that |f (x) -f (y) -∇f (y) (x -y) -1 2 (x -y) ∇ 2 f (y)(x -y)| ≤ L2 6 x -y 3 , and ∇f (x + u) -∇f (x) -∇ 2 f (x)u ≤ L 2 u 2 /2.</formula><p>(2) We first make the following assumptions regarding the problem (1). Assumption 1. For the problem (1), we assume that (i). every random function f (x; ξ) is twice differentiable, and it has L 1 -Lipschitz continuous gradient and L 2 -Lipschitz continuous Hessian. (ii). given an initial point x 0 , there exists ∆ &lt; ∞ such that F (x 0 ) -F (x * ) ≤ ∆, where x * denotes the global minimum of (1). (iii). there exists</p><formula xml:id="formula_3">G &gt; 0 such that E[exp( ∇f (x; ξ) -∇F (x) 2 /G 2 )] ≤ exp(1) holds.</formula><p>Remark. (1) the analysis of NEON or NEON + or their stochastic versions for extracting the negative curvature only requires Assumption 1 (i). Indeed, the Lipschitz continuous Hessian can be relaxed to locally Lipchitz continuous Hessian condition according to our analysis. (2) Assumptions 1 (ii) (iii) are used in the analysis of Section 5, which are standard assumptions made in the literature of stochastic non-convex optimization <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref></p><formula xml:id="formula_4">. Assumption 1 (iii) implies that E[ ∇f (x; ξ) - ∇F (x) 2 ] ≤ V G 2 holds.</formula><p>For stating our time complexities, we assume G is independent of d for finding an approximate local minimum in Section 5. Nevertheless, our comparison of the proposed algorithms with previous algorithms (e.g., SGLD <ref type="bibr" target="#b22">[23]</ref>, SNCG <ref type="bibr" target="#b16">[17]</ref>, Natasha2 <ref type="bibr" target="#b0">[1]</ref>) in the stochastic setting are fair because similar assumptions are also made. We also note that <ref type="bibr" target="#b4">[5]</ref> makes a stronger assumption about the stochastic gradients, i.e., ∇f (x; ξ) -∇F (x) ≤ O(d), which leads to a worse dependence of time complexity on d, i.e., O(d p ) with p ≥ 4.</p><p>Next, we discuss a second-order method based on HVPs to escape from a non-degenerate saddle point x of a function f (x) that satisfies λ min (∇ 2 f (x)) ≤ -γ, which can be found in many previous studies <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b3">4]</ref>. The method is based on a negative curvature (NC for short is used in the sequel)</p><formula xml:id="formula_5">direction v ∈ R d that satisfies v = 1 and v ∇ 2 f (x)v ≤ -cγ,<label>(3)</label></formula><p>where c &gt; 0 is a constant. Given such a vector v, we can update the solution according to</p><formula xml:id="formula_6">x + = x - cγ L 2 sign(v ∇f (x))v, or x + = x - cγ L 2 ξv,<label>(4)</label></formula><p>where ξ ∈ {1, -1} is a Rademacher random variable used when ∇f (x) is not available. The following lemma establishes that the objective value of x + or x + is less than that of x by a sufficient amount, which makes it possible to escape from the saddle point x.</p><p>Lemma 1. For x satisfying λ min (∇ 2 f (x)) ≤ -γ and v satisfying (3), let x + , x + be given in ( <ref type="formula" target="#formula_6">4</ref>),</p><formula xml:id="formula_7">then we have f (x) -f (x + ) ≥ c 3 γ 3 3L 2 2 and E[f (x) -f (x + )] ≥ c 3 γ 3 3L 2 2 .</formula><p>To compute a NC direction v that satisfies (3), we can employ the Lanczos method or the Power method for computing the maximum eigen-vector of the matrix (I -η∇ 2 f (x)), where ηL 1 ≤ 1 such that I -η∇ 2 f (x) 0. The Power method starts with a random vector v 1 ∈ R d (e.g., drawn from a uniform distribution over the unit sphere) and iteratively compute v τ +1 = (I -η∇ 2 f (x))v τ , τ = 1, . . . , t. Following the results in <ref type="bibr" target="#b12">[13]</ref>, it can be shown that if λ min (∇ 2 f (x)) ≤ -γ, then with at most</p><formula xml:id="formula_8">log(d/δ 2 )L1 γ</formula><p>HVPs, the Power method finds a vector vt</p><formula xml:id="formula_9">= v t / v t such that v t ∇ 2 f (x)v t ≤ -γ 2</formula><p>holds with high probability 1 -δ. Similarly, the Lanczos method (e.g., Lemma 11 in <ref type="bibr" target="#b20">[21]</ref>) can find such a vector vt with a lower number of HVPs, i.e., min(d, log(d/δ 2 )</p><formula xml:id="formula_10">√ L1 2 √ 2ε</formula><p>).</p><p>4 Key Building Block: Extracting NC From Noise</p><p>Our HVP-free stochastic algorithms with provable guarantees for solving (1) presented in next section are based on a key building block, i.e., extracting NC from noise using only first-order information.</p><p>To tackle the stochastic objective in (1), our method is to compute a NC based on a mini-batch of functions m i=1 f (x; ξ i )/m for a sufficiently large number of samples. Thus, a key building block of the proposed method is a first-order procedure to extract NC for a non-convex function f (x) <ref type="foot" target="#foot_1">1</ref> .</p><p>Below, we first propose a gradient descent based method for extracting NC, which achieves a similar iteration complexity to the Power method. Second, we present an accelerated gradient method to extract the NC to match the iteration complexity of the Lanczos method. Finally, we discuss the application of these procedures for stochastic non-convex optimization using mini-batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Extracting NC by NEON</head><p>The NEON is inspired by the perturbed gradient descent (PGD) method (a method for solving deterministic non-convex problems) proposed in the seminal work <ref type="bibr" target="#b10">[11]</ref> and its connection with the Power method as discussed shortly. Around a saddle point x, the PGD method first generates a random noise vector ê from the sphere of an Euclidean ball with a proper radius, then starts with a noise perturbed solution x 0 = x + ê, the PGD generates the following sequence of solutions:</p><formula xml:id="formula_11">x τ = x τ -1 -η∇f (x τ -1 ).</formula><p>(5) To establish a connection with the Power method and motivate the proposed NEON, let us define another sequence of x τ = x τ -x. Then we have the recurrence for</p><formula xml:id="formula_12">x τ = x τ -1 -η∇f ( x τ -1 + x), τ = 1, . . . , t. It is clear that for τ = 1, . . . , t, x τ = x τ -1 -η∇f (x) -η(∇f ( x τ -1 + x) -∇f (x)</formula><p>). To understand the above update, we adopt the following approximation: ∇f (x) ≈ 0 for an approximate saddle point, and from the Lipschitz continuous Hessian condition (2), we can see that ∇f ( x τ -1 + x) -∇f (x) ≈ ∇ 2 f (x) x τ -1 as long as x τ -1 is small. Then for τ = 1, . . . , t,</p><formula xml:id="formula_13">x τ ≈ x τ -1 -η∇ 2 f (x) x τ -1 = (I -η∇ 2 f (x)) x τ -1 .</formula><p>It is obvious that the above approximated recurrence is close to the the sequence generated by the Power method with the same starting random vector ê = v 1 . This intuitively explains that why the updated solution x t = x + x t can decrease the objective value due to that x t is close to a NC of the Algorithm 1 NEON(f, x, t, F, r) 1: Input: f, x, t, F, r 2: Generate u 0 randomly from S d r 3: for τ = 0, . . . , t do 4:</p><formula xml:id="formula_14">u τ +1 = u τ -η(∇f (x + u τ ) -∇f (x)) 5: end for 6: if min i∈[t+1], ui ≤U fx (u i ) ≤ -2.5F 7: return u τ , τ = arg min i∈[t+1], ui ≤U fx (u i ) 8: else return 0 Algorithm 3 NCFind (y 0:τ , u 0:τ ) 1: if min j=0,...,τ y j -u j ≥ ζ √ 6ηF 2: return y j , j = min{j : y j -u j ≥ ζ √ 6ηF} 3: else return y τ -u τ Algorithm 2 NEON + (f, x, t, F, U, ζ, r) 1: Input: f, x, t, F, U, ζ, r 2:</formula><p>Generate y 0 = u 0 randomly from S r 3: for τ = 0, . . . , t do</p><formula xml:id="formula_15">4: if ∆ x (y τ , u τ ) &lt; -γ 2 y τ -u τ 2 then 5:</formula><p>return v =NCFind(y 0:τ , u 0:τ ) return 0 14: end if Hessian ∇ 2 f (x). To provide a formal analysis, we will first analyze the following recurrence:</p><formula xml:id="formula_16">u τ = u τ -1 -η(∇f (x + u τ -1 ) -∇f (x)), τ = 1, . . .<label>(6)</label></formula><p>starting with a random noise vector u 0 , which is drawn from the sphere of an Euclidean ball with a proper radius r denoted by S d r . It is notable that the recurrence in ( <ref type="formula" target="#formula_16">6</ref>) is slightly different from that in <ref type="bibr" target="#b4">(5)</ref>. We emphasize that this simple change is useful for extracting the NC at any points whose Hessian has a negative eigen-value not just at non-degenerate saddle points, which can be used in some stochastic or deterministic algorithms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b15">16]</ref>. The proposed procedure NEON based on the above sequence for finding a NC direction of ∇ 2 f (x) is presented in Algorithm 1, where fx (u) is defined in <ref type="bibr" target="#b6">(7)</ref>. The following theorem states our result of NEON for extracting the NC.</p><p>Theorem 1. Under Assumption 1 (i), let γ ∈ (0, 1) and δ ∈ (0, 1) be a sufficiently small. For any constant ĉ ≥ 18, there exists a constant c max that depends on ĉ, such that if NEON is called with t = ĉ log(dL1/(γδ))</p><formula xml:id="formula_17">ηγ , F = ηγ 3 L 1 L -2 2 log -3 (dL 1 /(γδ)), r = √ ηγ 2 L -1/2 1 L -1 2 log -2 (dL 1 /(γδ)), U = 4ĉ( √ ηL 1 F/L 2 ) 1/3 and a constant η ≤ c max /L 1 , then at a point x satisfying λ min (∇ 2 f (x)) ≤ -γ with high probability 1 -δ it returns u such that u ∇ 2 f (x)u u 2 ≤ - γ 8ĉ 2 log(dL1/(γδ)) ≤ -Ω(γ).</formula><p>If NEON returns u = 0, then the above inequality must hold; if NEON returns 0, we can conclude that λ min (∇ 2 f (x)) ≥ -γ with high probability 1 -O(δ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark:</head><p>The above theorem shows that at any point x whose Hessian has a negative eigen-value (including non-degenerate saddle points), NEON can find a NC of ∇ 2 f (x) with high probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Finding NC by Accelerated Gradient Method</head><p>Although NEON provides a similar guarantee for extracting a NC as that provided by the Power method, but its iteration complexity O(1/γ) is worse than that of the Lanczos method, i.e., O(1/ √ γ).</p><p>In this subsection, we present a first-order method that matches O(1/ √ γ) of the Lanczos method.</p><p>Let us recall the sequence <ref type="bibr" target="#b5">(6)</ref>, which is essentially an application of gradient descent (GD) method to the following objective function:</p><formula xml:id="formula_18">fx (u) = f (x + u) -f (x) -∇f (x) u.<label>(7)</label></formula><p>In the sequel, we write fx (u) = f (u), where the dependent x should be clear from the context. By the Lipschitz continuous Hessian condition, we have that</p><formula xml:id="formula_19">1 2 u ∇ 2 f (x)u - L 2 6 u 3 ≤ f (u).</formula><p>It implies that if f (u) is sufficiently less than zero and u is not too large, then u ∇ 2 f (x)u u 2 will be sufficiently less than zero. Hence, NEON can be explained as using GD updates to decrease f (u).</p><p>A natural question to ask is whether the convergence of GD updates of NEON can be accelerated by accelerated gradient (AG) methods. It is well-known from convex optimization literature that AG methods can accelerate the convergence of GD method for smooth problems. Recently, several studies have explored AG methods for non-convex optimization <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b11">12]</ref>. Notably, <ref type="bibr" target="#b18">[19]</ref> analyzed the behavior of AG methods near strict saddle points and investigated the rate of divergence from a strict saddle point for toy quadratic problems. <ref type="bibr" target="#b11">[12]</ref> analyzed a single-loop algorithm based on Nesterov's AG method for deterministic non-convex optimization. However, none of these studies provide an explicit complexity guarantee on extracting NC from the Hessian matrix for a general non-convex problem. Inspired by these studies, we will show that Nesterov's AG (NAG) method <ref type="bibr" target="#b17">[18]</ref> when applied the function f (u) can find a NC with a complexity of O(1/ √ γ).</p><p>The updates of NAG method applied to the function f (u) at a given point x is given by</p><formula xml:id="formula_20">y τ +1 = u τ -η∇ f (u τ ), u τ +1 = y τ +1 + ζ(y τ +1 -y τ ),<label>(8)</label></formula><p>where ζ(y τ +1 -y τ ) is the momentum term, and ζ ∈ (0, 1) is the momentum parameter. The proposed algorithm based on the NAG method (referred to as NEON + ) for extracting NC of a Hessian matrix ∇ 2 f (x) is presented in Algorithm 2, where</p><formula xml:id="formula_21">∆ x (y τ , u τ ) = fx (y τ ) -fx (u τ ) -∇ fx (u τ ) (y τ -u τ )</formula><p>, and NCFind is a procedure that returns a NC by searching over the history y 0:τ , u 0:τ shown in Algorithm 3. The condition check in Step 4 is to detect easy cases such that NCFind can easily find a NC in historical solutions without continuing the update, which is designed following a similar procedure called Negative Curvature Exploitation (NCE) proposed in <ref type="bibr" target="#b11">[12]</ref>. However, the difference is that NCFind is tailored to finding a negative curvature satisfying (3), while NCE in <ref type="bibr" target="#b11">[12]</ref> is for ensuring a decrease on a modified objective. The theoretical result of NEON + is presented below. Theorem 2. Under Assumption 1 (i), let γ ∈ (0, 1) and δ ∈ (0, 1) be a sufficiently small. For any constant ĉ ≥ 43, there exists a constant c max that depends on ĉ, such that if NEON + is called with</p><formula xml:id="formula_22">t = ĉ log(dL1/(γδ)) ηγ , F = ηγ 3 L 1 L -2 2 log -3 (dL 1 /(γδ)), r = √ ηγ 2 L -1/2 1 L -1 2 log -2 (dL 1 /(γδ)), U = 12ĉ( √ ηL 1 F/L 2 ) 1/3</formula><p>, a small constant η ≤ c max /L 1 , and a momentum parameter ζ = 1-√ ηγ, then at any point x satisfying λ min (∇ 2 f (x)) ≤ -γ with high probability 1 -δ it returns u such that</p><formula xml:id="formula_23">u ∇ 2 f (x)u u 2</formula><p>≤ -γ 72ĉ 2 log(dL1/(γδ)) ≤ -Ω(γ). If NEON + returns u = 0, then the above inequality must hold; if NEON + returns 0, we can conclude that λ min (∇ 2 f (x)) ≥ -γ with high probability 1 -O(δ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Stochastic Approach for Extracting NC</head><p>In this subsection, we present a stochastic approach for extracting NC for F (x) in <ref type="bibr" target="#b0">(1)</ref>. For simplicity, we refer to both NEON and NEON + as NEON. The challenge in employing NEON for finding a NC for the original function F (x) in ( <ref type="formula" target="#formula_0">1</ref>) is that we cannot evaluate the gradient of F (x) exactly. To address this issue, we resort to the mini-batching technique.</p><p>Let S = {ξ 1 , . . . , ξ m } denote a set of random samples and define a sub-sampled function F S (x) = 1 |S| ξ∈S f (x; ξ). Then we apply NEON to F S (x) for finding an approximate NC u S of ∇ 2 F S (x). Below, we show that as long as m is sufficiently large, u S is also an approximate NC of ∇ 2 F (x). Theorem 3. Under Assumption 1 (i), for a sufficiently small δ ∈ (0, 1) and ĉ ≥ 43, let m ≥ 16L 2  1 log(6d/δ)</p><formula xml:id="formula_24">s 2 γ 2</formula><p>, where</p><formula xml:id="formula_25">s = log -1 (3dL1/(2γδ)) (12ĉ) 2</formula><p>is a proper small constant. If λ min (∇ 2 F (x)) ≤ -γ, there exists c &gt; 0 such that with probability 1 -δ, NEON(F S , x, t, F, r) returns a vector u S such that</p><formula xml:id="formula_26">u S ∇ 2 F (x)u S u S 2</formula><p>≤ -cγ, where c = (12ĉ) -2 log -1 (3dL 1 /(2γδ)). If NEON(F S , x, t, F, r) returns 0, then with high probability 1 -O(δ) we have λ min (∇ 2 F (x)) ≥ -2γ. In either case, NEON terminates with an IFO complexity of O(1/γ 3 ) or O(1/γ 2.5 ) corresponding to Algorithm 1 and Algorithm 2, respectively. if first-order condition of y j not met then </p><formula xml:id="formula_27">≤ |S 1 | 2: Set m 1 = |S 1 |, η = c (m 1 /b) -2/3 , c ≤ 1/6 3: Compute ∇F S (x j-1 ) and let x 0 = x 4: Generate N ∼ Geom(m 1 /(m 1 + b)) 5: for k = 1, 2, . . . , N do 6:</formula><p>Sample samples S k of size b 7:</p><formula xml:id="formula_28">v k = ∇F S k (x k-1 ) -∇F S k (x 0 ) + ∇F S (x 0 ) 8:</formula><p>x k = x k-1 -ηv k 9: end for 10: return x N 5 First-order Algorithms for Stochastic Non-Convex Optimization</p><p>In this section, we will first describe a general framework for promoting existing first-order stochastic algorithms denoted by A to enjoy a second-order convergence, which is shown in Algorithm 4. Here, we require A(x j ) to return two points (y j , z j ) that satisfy ( <ref type="formula" target="#formula_30">9</ref>) and the mini-batch sample size m = |S 2 | satisfies the condition in Lemma 3. The proposed NEON is used for escaping from a saddle point. It should be noted that Algorithm 4 is abstract depending on how to implement Step 3, how to check the first-order condition, and how to set the step size parameter ξ in Step 9.</p><p>For theoretical interest, we will analyze Algorithm 4 with a Rademacher random variable ξ ∈ {1, -1} and its three main components satisfying the following properties.</p><formula xml:id="formula_29">Property 1. (1) Step 7 -Step 9 guarantees that if λ min (∇ 2 F (y j )) ≤ -γ, there exists C &gt; 0 such that E[F (x j+1 ) -F (y j )] ≤ -Cγ 3 .</formula><p>Let the total IFO complexity of Step 7 -Step 9 be T n . ( <ref type="formula">2</ref>) There exists a first-order stochastic algorithm (y j , z j ) = A(x j ) that satisfies:</p><p>if</p><formula xml:id="formula_30">∇F (y j ) ≥ , then E[F (z j ) -F (x j )] ≤ -ε( , α) if ∇F (y j ) ≤ , then E[F (y j ) -F (x j )] ≤ Cγ 3 /2<label>(9)</label></formula><p>where ε( , α) is a function of and a parameter α &gt; 0. Let the total IFO complexity of A(x) be T a . (3) the check of first-order condition can be implemented by using a mini-batch of samples S, i.e., ∇F S (y j ) ≤ , where S is independent of y j such that ∇F (y j ) -∇F S (y j ) ≤ /2. Let the IFO complexity of checking the first-order condition be T c .</p><p>Property (1) can be guaranteed by Theorem 3 and Lemma 1. When using NEON, T n = O(1/γ 3 ) and when using NEON + , T n = O(1/γ 2.5 ). For Property (2), we will analyze several interesting algorithms. Property (3) can be guaranteed by Lemma 2 in the supplement under Assumption (1) (iii) with T c = O( <ref type="formula" target="#formula_0">1</ref>2 ). Based on the above properties, we have the following convergence of Algorithm 4. Theorem 4. Assume Properties 1 hold. Then with high probability 1 -δ, NEON-A terminates with a total IFO complexity of O(max</p><formula xml:id="formula_31">( 1 ε( ,α) , 1 γ 3 )(T n + T a + T c )).</formula><p>Upon termination, with high probability ∇F (y j ) ≤ O( ) and λ min (∇ 2 F (y j )) ≥ -2γ, where O(•) hides logarithmic factors of d and 1/δ, and problem's other constant parameters.</p><p>Next, we present corollaries of Theorem 4 for several instances of A, including stochastic gradient descent (SGD) method, stochastic momentum (SM) methods, mini-batch SGD (MSGD), and SCSG. SGD and its momentum variants (including stochastic heavy-ball (SHB) method and stochastic Nesterov's accelerated gradient (SNAG) method) are popular stochastic algorithms for solving a stochastic non-convex optimization problem. We will consider them in a unified framework as established in <ref type="bibr" target="#b21">[22]</ref>. The updates of SM starting from x 0 are x τ +1 = x τ -η∇f (x τ ; ξ τ ),</p><formula xml:id="formula_32">x s τ +1 = x τ -sη∇f (x τ ; ξ τ ), x τ +1 = x τ +1 + β( x s τ +1 -x s τ ),<label>(10)</label></formula><p>Algorithm 5 SM: (x 0 , η, β, s, t)</p><p>1: for τ = 0, 1, 2, . . . , t do 2:</p><p>Compute x τ +1 according to <ref type="bibr" target="#b9">(10)</ref> 3:</p><p>Compute x + τ +1 according to (11) 4: end for 5: return (x + τ , x + t+1 ), where τ ∈ {0, . . . , t} is a randomly generated.</p><p>#IFO (or #ISO) for τ = 0, . . . , t and x s 0 = x 0 , where β ∈ (0, 1) is a momentum constant, η is a step size, s = 0, 1, 1/(1 -β) corresponds to SHB, SNAG and SGD. Let sequence x + τ with x + 0 = x 0 be defined as</p><formula xml:id="formula_33">x + τ = x τ + p τ , τ ≥ 1, p τ = β 1 -β (x τ -x τ -1 -sη∇f (x τ -1 ; ξ τ -1 )).<label>(11)</label></formula><p>We can implement A by Algorithm 5 and have the following result. Corollary 5. Let A(x j ) be implemented by Algorithm 5 with t = Θ(1/ 2 ) iterations, η = Θ( 2 ), β ∈ (0, 1), s ∈ (0, 1/(1 -β)). Then T a = O(1/ 2 ) and ε( , α) = Θ( 2 ). Suppose that γ ≥ 2/3 and E[ ∇f (x; ξ) 2 ] is bounded for s = 1/(1 -β). Then with high probability, NEON-SM finds an ( , γ)-SPP with a total IFO complexity of O(max</p><formula xml:id="formula_34">( 1 2 , 1 γ 3 )(T n + 1 2 )), where T n = O(1/γ 3 ) (NEON) or T n = O(1/γ 2.5 ) (NEON + ).</formula><p>Remark: When γ = 1/2 , NEON-SM has an IFO complexity of O( <ref type="formula" target="#formula_0">1</ref>4 ). MSGD computes (y j , z j ) by</p><formula xml:id="formula_35">z j = x j -L -1 1 ∇F S1 (x j ), y j = x j<label>(12)</label></formula><p>where S 1 is a set of samples independent of x j . Corollary 6. Let A(x j ) be implemented by <ref type="bibr" target="#b11">(12)</ref> with</p><formula xml:id="formula_36">|S 1 | = O(1/ 2 ). Then T a = O(1/ 2 ) and ε( , α) = 2 4L1</formula><p>. With high probability, NEON-MSGD finds an ( , γ)-SPP with a total IFO complexity of O(max( <ref type="formula" target="#formula_0">1</ref>2 , 1 γ 3 )(T n + 1/ 2 )). Remark: Compared to Corollary 5, there is no requirement on γ ≥ 2/3 , which is due to that MSGD can guarantee that E[F (y j ) -F (x j )] ≤ 0. SCSG was proposed in <ref type="bibr" target="#b13">[14]</ref>, which only provides a first-order convergence guarantee. SCSG runs with multiple epochs, and each epoch uses similar updates as SVRG with three distinct features: (i) it was applied to a sub-sampled function F S1 ; (ii) it allows for using a mini-batch samples of size b independent of S 1 to compute stochastic gradients; (ii) the number of updates of each epoch is a random number following a geometric distribution dependent on b and |S 1 |. These features make each SGCG epoch denoted by SCSG-epoch(x, S 1 , b) have an expected IFO complexity of T a = O(|S 1 |). We present SCSG-epoch(x, S 1 , b) in Algorithm 6. For using SCSG, y j and z j are</p><formula xml:id="formula_37">y j = SCSG-epoch(x j , S 1 , b), z j = y j<label>(13)</label></formula><p>Corollary 7. Let A(x j ) be implemented by <ref type="bibr" target="#b12">(13)</ref> with</p><formula xml:id="formula_38">|S 1 | = O max(1/ 2 , 1/(γ 9/2 b 1/2 )) . Then ε( , α) = Ω( 4/3 /b 1/3 ) and E[T a ] = O max(1/ 2 , 1/(γ 9/2 b 1/2 )) . With high probability, NEON- SCSG finds an ( , γ)-SSP with an expected total IFO complexity of O(max( b 1/3 4/3 , 1 γ 3 )(T n + 1/ 2 + 1/(γ 9/2 b 1/2 ))), where T n = O(1/γ 3 ) (NEON) or T n = O(1/γ 2.5 ) (NEON + ). Remark: When γ = 1/2 , b = 1/ √ , NEON-SCSG has an expected IFO complexity of O( 1 3.5 ). When γ ≥ 4/9 , b = 1, NEON-SCSG has an expected IFO complexity of O(1/ 3.33 ).</formula><p>Finally, we mention that the proposed NEON or NEON + can be used in existing second-order stochastic algorithms that require a NC direction as a substitute of second-order methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20]</ref>. <ref type="bibr" target="#b0">[1]</ref> developed Natasha2, which uses second-order online Oja's algorithm for finding the NC. <ref type="bibr" target="#b19">[20]</ref> developed a stochastic algorithm for solving a finite-sum problem by using SVRG and a second-order stochastic algorithm for computing the NC. We can replace the second-order methods for computing a NC in these algorithms by the proposed NEON or NEON + , with the resulting algorithms referred to as NEON-Natasha and NEON-SVRG. It is a simple exercise to derive the convergence results in Table <ref type="table">1</ref>, which is left to interested readers. 6 Experiments</p><p>Extracting NC. First, we present some simulations to verify the proposed NEON procedures for extracting NC. To this end, we consider minimizing non-linear least square loss with a non-convex regularizer for classification, i.e., F (x) = d i=1</p><formula xml:id="formula_39">x 2 i 1+x 2 i + λ n n i=1 (b i -σ(x a i )) 2</formula><p>, where b i ∈ {0, 1} denotes the label and a i ∈ R d denotes the feature vector of the i-th data, λ &gt; 0 is a trade-off parameter, and σ(•) is a sigmoid function. We generate a random vector x ∼ N (0, I) as the target point to construct Fx (u) and compute a NC of ∇ 2 F (x). We use a binary classification data named gisette from the libsvm data website that has n = 6000 examples and d = 5000 features, and set λ = 3 in our simulation to ensure there is significant NC from the non-linear least-square loss. The step size η and initial radius in NEON procedures are set to be 0.01 and the momentum parameter in NEON + is set to be 0.9. These values are tuned in a certain range.</p><p>We compare the two NEON procedures and their stochastic variants (denoted by NEON-st and NEON + -st in the figure) with second-order methods that use HVPs, namely the Power method and the Lanczos method, where the HVPs are calculated exactly. The result is shown in Figure <ref type="figure" target="#fig_0">1</ref> whose y-axis denotes the value of u H u, where u represents the found normalized NC vector and H = ∇ 2 F (x) is the Hessian matrix. For NEON-st and NEON + -st, we use a sample size of 100. Please note that the solid red curve corresponding to NEON + -st terminates earlier due to that NCFind is executed. Several observations follow: (i) NEON performs similarly to the Power method (the two curves overlap in the figure); (ii) NEON + has a faster convergence than NEON; (iv) the stochastic versions of NEON and NEON + can quickly find a good NC directions than their full versions in terms of IFO complexity and are even competitive with the Lanczos method. We include several more results in the supplement.</p><p>Escaping Saddles. Second, we present some simulations to verify the proposed NEON and NEON + based algorithms for minimizing a stochastic objective. We consider a non-convex optimization problem with f (x; ξ) = d i=1 ξ i (x 4 i -4x 2 i ) where ξ i are a normal random variables with mean of 1 so that the saddle points of the expected function are known <ref type="bibr" target="#b9">[10]</ref>. Assuming the noise ξ is only accessed through a sampler, then we compare NEON-SGD with a state-of-the-art algorithm Noisy SGD <ref type="bibr" target="#b4">[5]</ref> for different values of d ∈ {10 3 , 10 4 , 10 5 }. The step size of Noisy SGD is tuned in a wide range and the best one is used. The step size in NEON procedures are set to be the same value as Noisy SGD. The radius in NEON procedures is set to be 0.01 and the momentum paramenter in NEON + is set to be 0.9. The mini-batch size is tuned from {50, 100, 200, 500}. All algorithms are started with a same saddle point as the initial solution. The results are presented in Figure <ref type="figure" target="#fig_3">2</ref>, showing that two variants of NEON-SGD methods can escape saddles faster than Noisy SGD. NEON + -SGD escapes saddle points the fastest among all algorithms for different values of d. In addition, the increasing of dimensionality d has much larger effect on the IFO complexity of Noisy-SGD than that of NEON-SGD methods, which is consistent with theoretical results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We have proposed novel first-order procedures to extract negative curvature from a Hessian matrix by using a noise-initiated sequence, which are of independent interest. A general framework for promoting a first-order stochastic algorithm to enjoy a second-order convergence is also proposed. Based on the proposed general framework, we designed several first-order stochastic algorithms with state-of-the-art second-order convergence guarantee.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 4 NEON-A 1 :</head><label>1</label><figDesc>Input: x 1 , other parameters of algorithm A 2: for j = 1, 2, . . . , do 3:Compute (y j , z j ) = A(x j ) 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>7 : 8 : 9 : 6 1 :</head><label>78961</label><figDesc>u j = NEON(F S2 , y j , t, F, r) if u j = 0 return y j else let x j+1 = y j -SCSG-epoch: (x, S 1 , b) Input: x, an independent set of samples S 1 and b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 1: NEON vs Second-order Methods for Extracting NC</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: NEON-SGD vs Noisy SGD. (All algorithms converge to local minimum)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>+1 , u τ +1 ) by (8) 8: end for 9: if min i, yi ≤U fx (y i ) ≤ -2F then 10: let τ = arg min i, yi ≤U fx (y i )</figDesc><table><row><cell>6:</cell><cell>end if</cell></row><row><cell cols="2">7: compute (y τ 11: return y τ</cell></row><row><cell cols="2">12: else</cell></row><row><cell>13:</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>We abuse the same notation f here.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The authors thank the anonymous reviewers for their helpful comments. Y. Xu and T. Yang are partially supported by National Science Foundation (IIS-1545995).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Natasha 2: Faster non-convex optimization than sgd</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<idno>CoRR, /abs/1708.08694</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neon2: Finding local minima via first-order oracles</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR, abs/1711.06673</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">convex until proven guilty&quot;: Dimension-free acceleration of gradient descent on non-convex functions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sidford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="654" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Accelerated methods for nonconvex optimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sidford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1751" to="1772" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Escaping from saddle points -online stochastic gradient for tensor decomposition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="797" to="842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stochastic first-and zeroth-order methods for nonconvex stochastic programming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghadimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2341" to="2368" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accelerated gradient methods for nonconvex nonlinear and stochastic programming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghadimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="59" to="99" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghadimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="267" to="305" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On graduated optimization for stochastic non-convex problems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1833" to="1841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Non-convex optimization for machine learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="142" to="336" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">How to escape saddle points efficiently</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Netrapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1724" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Accelerated gradient descent escapes saddle points faster than gradient descent</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Netrapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1042" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Estimating the largest eigenvalue by the power and lanczos algorithms with a random start</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kuczynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wozniakowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1094" to="1122" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Non-convex finite-sum optimization via SCSG methods</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2345" to="2355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Accelerated proximal gradient methods for nonconvex programming</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On noisy negative curvature descent: Competing with gradient descent for faster non-convex optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<idno>CoRR, abs/1709.08571</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Stochastic non-convex optimization with strong high probability secondorder convergence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<idno>CoRR, abs/1710.09447</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Introductory lectures on convex optimization : a basic course. Applied optimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Kluwer Academic Publ</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Behavior of accelerated gradient methods near critical points of nonconvex problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>O'neill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<idno>CoRR, abs/1706.07993</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A generic approach for escaping saddle points</title>
		<author>
			<persName><forename type="first">S</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1233" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Complexity analysis of second-order line-search algorithms for smooth nonconvex optimization</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Royer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1448" to="1477" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A unified analysis of stochastic momentum methods for deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2955" to="2961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A hitting time analysis of stochastic gradient langevin dynamics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="1980">1980-2022, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
