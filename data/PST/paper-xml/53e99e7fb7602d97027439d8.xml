<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Markerless Augmented Reality with a Real-time Affine Region Tracker</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
							<email>ferrari@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Group (BIWI)</orgName>
								<orgName type="institution">ETH Zuerich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
							<email>tinne.tuytelaars@esat.kuleuven.ac.be</email>
							<affiliation key="aff1">
								<orgName type="department">ESAT-PSI</orgName>
								<orgName type="institution">University of Leuven</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
							<email>vangool@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Group (BIWI)</orgName>
								<orgName type="institution">ETH Zuerich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">ESAT-PSI</orgName>
								<orgName type="institution">University of Leuven</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Markerless Augmented Reality with a Real-time Affine Region Tracker</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">78F90888601870CB56C3AB98C8A588F6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a system for planar augmented reality based on a new real-time affine region tracker. Instead of tracking fiducial points, we track planar local image patches, and bring these into complete correspondence, so a virtual texture can directly be added to them. Moreover, the local image patches can be extracted in an invariant way, even without any a priori information from previous frames. Hence it is possible to use them as natural beacons, that can be used to recognize the scene and to identify the individual patches. This results in a powerful system, that can work without artificial markers or fiducial points and with a minimal amount of user interference.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Augmented Reality superimposes information -textual or figural -onto parts of the real world. Issues to be resolved are what should be shown where, and how. The latter problem is especially important when the visual appeal of the result is crucial. Then much effort has to go into seamlessly fitting the information into the scene, both geometrically and photometrically (occlusions, shadowing, mutual reflections, chromatic adaptation to scene illumination, ...). In this paper we only deal with the two other issues: what to place where. We even simplify the problem further by assuming that planar textures are to be superimposed onto planar parts in the scene (but these planar parts can be small, so, in contradistinction to e.g. Simon et al. <ref type="bibr" target="#b12">[13]</ref>, we do not assume that dominant planes are present.)</p><p>Yet, even under such simplified conditions these problems do not become trivial. Suppose a field worker has to perform a job somewhere in a building. The engineer has gone through the building the previous day, and has prepared virtual annotations that indicate what is to be done at different locations. When the worker enters a room, his AR system should superimpose this information through a head-mounted display, e.g. a todo list, or a video demon-strating the expected manipulation. The system should therefore recognize where it is, and which information is to be superimposed where. As mentioned, we assume that planar textures are to be superimposed onto planar surface patches. In fact, the information need not be projected right on top of these patches, but it will be rigidly connected to them. The following tasks have to be solved:</p><p>1. recognize the overall position of the user (context) 2. recognize the individual patches 3. track the patches and superimpose their virtual textures</p><p>The first two steps can be considered to be a kind of initialization. The first step, finding the user's overall position within a possibly large scene, need not be solved entirely based on visual information. Vision could be combined with GPS and we plan such integration. In our current implementation this step is carried out pure visually. In fact, the first two steps are combined. The joint recognition of a set of patches as a configuration yields the location. This step is not carried out in real-time. Once the patches have been recognized, the system displays their annotations. The user can select a region, and then the system allows to track and augment the selected region in real-time. Hence, the correct, superimposed information rigidly moves with the patch.</p><p>Due to the real-time demands of AR, researchers often have to take refuge to putting 'markers' or 'landmarks' into the scene <ref type="bibr" target="#b0">[1]</ref>. They are designed in a way that they are easy to detect. Detection may be eased by giving them distinctive photometric characteristics (e.g. colored markers <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, LEDs <ref type="bibr" target="#b5">[6]</ref>, and retroreflective markers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12]</ref>) or special shapes (like squares <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15]</ref>, circles <ref type="bibr" target="#b13">[14]</ref>, dot codes <ref type="bibr" target="#b7">[8]</ref> or even 2D barcodes <ref type="bibr" target="#b10">[11]</ref>). The novelty of the proposed work is that we attempt to use patches of the natural scene as landmarks. The extraction of these landmarks is automatic and robust against changes in viewpoint and illumination. A single patch can be tracked in real-time, where out-of-plane rotations as well as major shifts between frames are allowed. During tracking accurate correspondences between points of the landmark patches are established, because the complete affine transformations between frames are recovered (in contradistinction with other real-time region trackers, often resorting to translation and scale only <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b18">19]</ref>). This allows to annotate the landmark patch with a texture, that is locked to it and deforms as if it were part of the physical scene, mimicing the same 3D motion as the patch undergoes.</p><p>This differs with earlier contributions with natural landmarks in that there is no initialization stage based on fiducial markers <ref type="bibr" target="#b6">[7]</ref> or interactively indicated points needed for calibration <ref type="bibr" target="#b12">[13]</ref>, in that the planarity assumption is limited to the small landmark patch instead of an extended part of the image <ref type="bibr" target="#b12">[13]</ref>, in that no 3D model of the object is required to predict the deformations of the natural landmark regions <ref type="bibr" target="#b15">[16]</ref>, in that the 3D camera/object motion is not constrained <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, and in that the computations run realtime on a single, mainstream workstation processor (Sun UltraSparc-IIi, 440 MHz) <ref type="bibr" target="#b15">[16]</ref>. On the other hand, the virtual texture is purely planar and is simply mapped onto the landmark plane. Nevertheless, for quite a few AR applications this is sufficient. 3D shapes could be added based on the joint tracking of several natural landmarks. However, this is not yet considered.</p><p>The landmarks correspond to regions that are 'good for tracking' according to the criteria of Shi and Tomasi <ref type="bibr" target="#b3">[4]</ref> (similar work in <ref type="bibr" target="#b25">[26]</ref>). A difference with their regions is that the affine deformations are not determined through a reference to the initial frame. Frames play a more symmetric role. This seems better as the first frame could be a frame that was taken from a particularly large distance or under a grazing angle. In that case the texture within the region does not support cross-correlations well. Also, in our case there is no double process where tracking assumes the flow field to amount to a translation whereupon a more precise affine deformation is determined, but, instead, affine deformations are determined directly by the tracking process.</p><p>The paper is organized as follows. Section 2 explains the specific characteristics of the image patches used and how to extract them from an image without a priori information. Section 3 explains the scene recognition and patch identification based on affine moment invariants. Section 4 looks at the real-time tracking. Section 5 deals with the virtual pattern overlay. Section 6 discusses some experimental results, and section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Affinely invariant regions</head><p>In order to use patches of the natural scene as beacons instead of artificial markers, a method is required to extract exactly the same scene patch, in spite of changing viewpoint and/or illumination conditions. This can be achieved through the use of affinely invariant regions <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. With 'invariant' we mean that they automatically deform their shape with changing viewpoint as to keep on covering iden- tical physical parts of a scene (under the assumption of local planarity). In addition to the affine geometric invariance, they are also invariant to linear photometric changes, with a different offset and scale-factor for each colorband.</p><p>These local invariant regions can be distinguished based on a feature vector of moment invariants; hence they can be recognized on an individual basis. Moreover, they also turn out to be good features to track. While tracking, we can exploit specific knowledge about the regions, which allows to obtain real-time performance, even under out-of-plane rotation and large discontinuous motion. Finally, since the affine transformation is completely retrieved, tracking a single invariant region suffices for augmenting the scene with a virtual texture (in the same plane as the tracked region, and close to it such that perspective effects can be neglected).</p><p>In this section, we summarize two algorithms for extracting invariant regions -although for a more in depth discussion we refer to earlier work <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. We deal with two different region "types" (figure <ref type="figure" target="#fig_0">1</ref>). The first one is based on a geometry-based approach, exploiting corners and edges in the image, and yields parallelogram-shaped regions, while the second one is purely intensity-based and yields elliptical regions (anchored on local intensity extrema). Because of the nature of their respective anchor points and extraction methods, these two types complement each other well and experiments show that hundreds of uniformly distributed regions can be extracted from images of typical indoor and outdoor scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Geometry-based region extraction</head><p>The first method starts from a Harris corner point h and its nearby edges (extracted using the Canny edge detector). Although a general method for curved edges has been described in <ref type="bibr" target="#b22">[23]</ref>, we focus in this work on the special case of straight edges only. The two straight edges intersecting in the proximity of h fix one corner of the parallelogramshaped region (call it c) and the orientation of its sides. The remaining degrees of freedom (DOF) are lifted by examining some photometric quantities, evaluated over the parallelogram-shaped regions spanned by c, a point moving along the first edge p 1 and a point moving along the second edge p 2 (figure <ref type="figure" target="#fig_1">2</ref>).</p><p>The photometric quantities we use are:</p><formula xml:id="formula_0">f 1 (Ω) = abs( |p 1 -p g p 2 -p g | |p -p 1 p -p 2 | ) M 1 00 M 2 00 M 0 00 -(M 1 00 ) 2 f 2 (Ω) = abs( |p -p g q -p g | |p -p 1 p -p 2 | ) M 1 00 M 2 00 M 0 00 -(M 1 00 ) 2 with M n</formula><p>pq the nth order, (p+q)th degree moment computed over the neighborhood Ω, p g the center of gravity of the region, weighted with image intensity I(x, y) (one of the three color bands R, G or B), and q the corner of the parallelogram opposite to c.</p><formula xml:id="formula_1">M n pq = Ω I n (x, y)x p y q dxdy, p g = ( M 1 10 M 1 00 , M 1 01 M 1 00 )</formula><p>Since these functions are invariant under the considered geometric and photometric transformations, a possible strategy could be to select the parallelogram-shaped region for which one of these functions reaches a (local) extremum. However, f 1 (Ω) and f 2 (Ω) do not show clear, well-defined extrema. Rather, we have some shallow valleys of low values (corresponding to cases where the center of gravity lies on or close to one of the diagonals). Instead of taking the inaccurate local extrema of one function, we combine the two functions and take the intersections of the two valleys, as shown in figure <ref type="figure">3</ref>. The special case where the two valleys (almost) coincide must be detected and rejected, since the intersection will not be accurate in that case Although this method seems rather ad-hoc and not a very principled one, the regions we obtain prove to be much more stable than those based on a 2D local extremum. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Intensity-based region extraction</head><p>The second region extraction method starts from local extrema in the image intensity, extracted with a nonmaximum suppression algorithm. Given such a local extremum i, the intensity function along rays emanating from i is studied, as shown in figure <ref type="figure">4</ref>. The following invariant function is evaluated along each ray:</p><formula xml:id="formula_2">f (t) = |I(t) -I 0 | max( t 0 |I(t)-I0|dt t , d)</formula><p>with t the Euclidean arclength along the ray, I(t) the intensity at position t, I 0 the intensity of the extremum i and d a small number which has been added to prevent a division by zero. The point for which f (t) reaches an extremum is invariant under the aforementioned affine geometric and photometric transformations (given the ray). Typically, such extrema occur at positions where the intensity suddenly increases or decreases dramatically compared to the intensity changes encountered on the line up to that point.</p><p>Next, all points corresponding to maxima of f (t) along rays originating from i are linked to enclose an (affinely invariant) region (figure <ref type="figure">4</ref>). This often irregularly-shaped region is then replaced by an ellipse having the same shape moments up to the second order. This ellipse-fitting is affinely invariant as well. Finally, the area of the ellipse. is doubled. This leads to a higher distinctive power of the regions, due to a more diversified texture pattern within the regions and hence facilitates the recognition process, at the cost of a higher risk of non-planarity due to the less local character of the regions. Note that the ellipse is not centered on i. From now on, we will use c to refer to the center of elliptical regions, and to the edge intersection for parallelogram-shaped regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Scene recognition</head><p>Once local, invariant regions have been extracted, a scene can be recognized by recognizing the invariant regions it contains. By recognizing we mean determining whether the current scene contents match a pre-processed model view of the scene. This is achieved by means of a nearest neighbor classification scheme, based on feature vectors containing moment invariants computed over the affinely invariant regions. As in the region extraction step, we consider invariance both under affine geometric changes and linear photometric changes, with different offsets and different scale factors for each of the three color bands.</p><p>Each region is characterized by a feature vector of moment invariants. The moments we use are Generalized Color Moments <ref type="bibr" target="#b21">[22]</ref>. They contain powers of the image coordinates and of the intensities of the different color channels.</p><formula xml:id="formula_3">M abc pq = Ω x p y q [R(x, y)] a [G(x, y)] b [B(x, y)] c dxdy</formula><p>with order p+q and degree a+b+c. In fact, they implicitly characterize the shape, the intensity and the color distribution of the region pattern in a uniform manner. We use a feature vector containing 18 moment invariants, composed of moments up to the first order and second degree. As an additional invariant, we use the region type (the method that has been used for the region extraction). Only if the type of two regions corresponds, they can be matched. Similarity is quantified using the Mahalanobis distance:</p><formula xml:id="formula_4">d M (b, a) = (b -a) C -1 (b -a)</formula><p>where a and b represent two feature vectors, and C denotes the covariance matrix (estimated based on a set of tracking experiments). After the invariant-based matching, a cross-correlation is performed as a final check. To this end, the region in the image R i and the model region R m from the model view are first aligned with one another by applying an affine transformation A. For the parallelogram-shaped regions, A is completely defined by putting into correspondence the corners of R m with the corners of R i . However, an ellipse having only five DOF, it is not possible to directly compute A from R i and R m . The missing DOF corresponds to a free rotation in the ellipse plane around its center. We propose here a new solution to this problem which is both more elegant and more time-efficient than our earlier approach <ref type="bibr" target="#b23">[24]</ref> (consisting of an exhaustive search for the rotation maximizing the cross-correlation). The new solution is based on a photometric invariant version of the axis of inertia. First R i is affinely mapped to a reference circular region O. The major and minor axes of inertia are then extracted (figure <ref type="figure" target="#fig_3">5</ref>) as the lines passing through the center of O with orientations θ max , θ min defined by the solutions of: with m pq the p + qth order, first degree moment centered on the region's geometric center. Equation ( <ref type="formula">1</ref>) differs from the usual definition of the axis of inertia by the use of these moments instead of moments centered on the center of gravity weighted with image intensity. This makes them invariant to linear intensity changes. These axes are invariant under rotation, in the sense that they will cover the same part of the region after a rotation. Mapping the axes back to the original elliptical region provides two affinely invariant lines and their intersection points with the ellipse. The mapping of the center of the ellipse and these intersections allow us to compute A; the cross-correlation test can follow and, if failed, the region match will be rejected. If a sufficient number of regions are recognized, the system considers the scene as a whole as recognized.</p><formula xml:id="formula_5">tan 2 (θ) + m 20 -m 02 m 11 tan θ -1 = 0 (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Region tracking</head><p>This section describes the new affine region tracker, which is the heart of the proposed AR system. Both the geometry-based and intensity-based regions are tracked using the same scheme. In the following we consider tracking a region R from a frame F i-1 to its successor frame F i in the image sequence. First we compute a prediction Ri = A i-1 R i-1 of R i using the affine transformation A i-1 between the two previous frames. An estimate âi = A i-1 a i-1 of the region's anchor point 1 , is computed, around which a circular search space S i is defined. The radius of S i is proportional to the current translational velocity of the region. This allows to continuously adapt the range of S i so as to maximize the chances of finding R i , while keeping it as small as possible (efficiency). The anchor points in S i are extracted. These provide potentially better estimates for the region's location. We investigate the point closest to âi looking for the target region R i . The anchor point investigation algorithm differs for geometrybased and intensity-based regions and will be explained in the two following subsections. Since the anchor points are sparse in the image, the one closest to the predicted location is, in most cases, the correct one. If not, the anchor points are iteratively investigated, from the closest (to âi ) to the farthest, until R i is found (figure <ref type="figure" target="#fig_4">6</ref>). Hence, the investigation process runs on a sparse subset of points in S i . This way of pruning the search space is made possible by the region model we consider and allows to keep the radius of S i wide enough to ensure tolerance to large image displacements, while staying within the tight time bounds imposed by real-time requirements.</p><p>In some cases it is possible that no correct R i is found around any anchor point in S i . This can be due to several reasons, including occlusion of the region, sudden acceleration (the anchor point of R i is outside S i ) and failure of the anchor point extractor. When this happens the region's location is set to the prediction (a i = âi ), and the tracking process proceeds to the next frame, with a larger S. In most cases this simple mechanism allows to recover the region in one of the next few frames, while avoiding the computationally expensive process of searching F i further.</p><p>Our tracking scheme can be summarized as follows:</p><p>1. Predict the target region Ri = A i-1 R i-1 and its anchor point âi = A i-1 a i-1 . 2. Define the search space S and extract anchor points. 3. Investigate the closest anchor point searching for R i . 4. Loop to 3 for the next closest anchor point if needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Geometry-based regions</head><p>Given a corner point h, the region prediction Ri , and the region in the previous frame R i-1 , we want to test if R i is anchored to h and, in that case, extract it. The idea is to construct at h the region most similar to R i-1 . The process follows two steps; the first tracks two of the straight region sides exploiting the geometric information (edges) of the image, and already yields partial information about R i . The second step starts from the output of the first, and completes R i by exploiting intensity information (texture).</p><p>In the first step a polyline snake with three-vertices recovers two of the sides, but not yet their lengths. We exploit the fact that translating Ri so that ĉ = h automatically provides an estimation of the sides. We initialize the center vertex v c of the snake at h and the other two vertices v 1 , v 2 so that the line segments v c v 1 and v c v 2 have the orientation of the predicted region sides (figure <ref type="figure" target="#fig_5">7</ref>). The three points are iteratively moved in order to maximize the total sum of gradient magnitudes along the two line segments:</p><formula xml:id="formula_6">E S (v c , v 1 , v 2 ) = p∈vcv1 | I(p)| + p∈vcv2 | I(p)|</formula><p>where | I(p)| is the image gradient magnitude at pixel p. The snake can deform only by hinging around v c and the length of the line segments is kept fixed (we are interested in their orientation only). These constraints reduce the number of DOF to four, thereby reducing the search space and improving efficiency. The optimization process is efficiently implemented by a Dynamic Programming algorithm inspired by <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25]</ref>. The algorithm has a higher probability of being attracted toward contours than the traditional snake implementation <ref type="bibr" target="#b20">[21]</ref>, and it is ensured to converge <ref type="bibr" target="#b16">[17]</ref>. In practice h is often very close (a few pixels) to the intersection point c of the target region sides. Hence our initialization is often very good and this reduces the number of iterations and the risk of being attracted by nearby distractor edges.</p><p>The tracked region sides lift four DOF: the two coordinates of c = v c and the orientations of the two sides. These correspond to the translation, rotation and skew components of the affine transformation A i mapping R i-1 to R i . This is all the information we can extract from the geometric features of the image. The two remaining DOF correspond to the position of the point q (they arise from the scale components of A i ) and are derived from the texture content of the region by the second step of the algorithm.</p><p>An initial estimate q is obtained by aligning Ri on the structure formed by v 1 , v c , v 2 , so that ĉ = v c and the sides are oriented like v c v 1 , v c v 2 (figure <ref type="figure" target="#fig_5">7</ref>). This estimation is refined by moving q so as to maximize the similarity between the resulting region R i (q) and the region in the previous frame R i-1 . As a similarity measure we use the normalized cross-correlation between R i-1 and R i (q) after aligning them via A(q), the affine transformation mapping R i-1 onto R i (q). Therefore, the objective function to be maximized is:</p><formula xml:id="formula_7">E C (q) = CrossCorr(A(q)R i-1 , R i (q))<label>(2)</label></formula><p>Notice that this similarity measure is invariant not only under geometric affine transformations, but also under linear transformations of the pixels intensities. This makes the tracking process relatively robust to changes in illumination conditions. The maximization process is implemented by Gradient Descent, initialized on q, where at each iteration q is moved 1 pixel in the direction of maximal increase. Typically q is initialized close to the absolute maximum, because most of the variability of the affine transformation is lifted by the sides tracking step. This strongly reduces the risk of converging toward a local maximum and keeps the number of iterations low. Extensive experiments confirm this consideration and indicate that, in most cases, 3 iterations are enough.</p><p>At the end of the second step, the most similar region to R i-1 anchored to h is constructed. This does not mean that it is the correct region though, as h could just be the wrong corner. Hence, as final verification we check if the maximum cross-correlation value is above a predefined threshold (typically 0.9), otherwise the algorithm proceeds to the next corner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Intensity-based regions</head><p>Let us now focus on the tracking of intensity-based regions. Given an intensity extremum i, the region prediction Ri and the region in the previous frame R i-1 , is R i anchored to i ? Since the elliptical regions exploit only the raw intensity pattern of the image and do not rely on the presence of nearby edges, we can no longer devise a two-step search strategy like for the parallelogram-shaped case. A natural alternative would be to look for the complete set of 6 parameters of A i simultaneously, by minimizing a cross-correlation based criteria similar to equation (2) (as proposed in <ref type="bibr" target="#b17">[18]</ref>). The search process could be initialized from the affine transformation A i-1 between the two previous frames. The problem is that searching for an optimum in this six-dimensional space, starting from an imprecise initialization, would probably take too much computation power to be achieved in real-time.</p><p>We exploit instead the property that R i can be extracted from F i independently, provided we are considering the correct intensity extremum. In a first step, R i is extracted around i using an optimized implementation of the algorithm described in section 2.2. In a second step, the extracted region R i is submitted to the two following geometric continuity tests:</p><formula xml:id="formula_8">1 e &lt; e(R i ) e(R i-1 ) &lt; e , 1 a &lt; a(R i ) a(R i-1 ) &lt; a (3)</formula><p>where e(R), a(R) are, respectively, the eccentricity and area of an elliptical region R and e , a are thresholds (typical values e = 1.6, a = 1.6). The two tests in ( 3) quickly detect the case where the extracted region can not be the correct one, and in practice reveal very effective in keeping 000000 000000 000000 000000 000000 000000 000000 000000 000000 000000 000000 000000 000000  the tracker away from the attraction of spurious elliptical regions located in the proximity of the correct one. The geometric continuity tests represents a necessary, but not sufficient condition: a last step consisting of verifying that R i cross-correlates with R i-1 above a predefined threshold (after aligning them using A i ) is still required. A i is computed by putting in correspondence the center and the intersection points of the axes of inertia with the ellipse between regions R i-1 and R i (section 2.2).</p><formula xml:id="formula_9">111111</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Augmenting a region</head><p>The previous section explained how to track a region through the image sequence. In this section, we move to the next goal: attaching a texture to the region, in every frame of the sequence. The texture will be projected in every frame on the same plane as R.</p><p>The user provides a parallelogram-shaped texture T and a set of three points Ψ = {l 1 , l 2 , l 3 } indicating where it should be placed. These points have to be selected in a userdefined reference frame F r only (e.g.: first frame) and define a parallelogram-shaped area Ω(l 1 , l 2 , l 3 ) where the texture should be mapped. This gives the user valuable creative freedom. Nevertheless Ψ should be chosen in the proximity of an affinely invariant region R, so as to maximize the quality of the results (the affine transformation of R may only hold in a limited neighborhood). Besides, it must not be forgotten that T will appear in the images as if it was on the same plane as R.</p><p>We restrict the explanation to a single frame F i . The algorithm consists of two main steps: the first constructs the affine transformation M mapping the texture to F i , the second does the actual texture-mapping. The algorithm is iteratively applied to every frame in the sequence. In order to construct M , we first compute A: the affine transformation mapping the region from the reference frame (R r ) to the current frame (R i ). The placement of the texture is obtained by warping {l j } j=1..3 by A . By enforcing the correspondence between three texture corners and the obtained points {Al j } j=1..3 , we find the desired affine transformation M . It is now possible to continue with the second step: augment the region by mapping T in the image via M (figure <ref type="figure">8</ref>):</p><formula xml:id="formula_10">W (p) = T (M -1 p), ∀p ∈ Ω(M l 1 , Ml 2 , Ml 3 )</formula><p>where W (p), T (p) is the color of the augmented region, respectively of the texture, at pixel p. Since the region's points are in correspondence along the frames, the texture will be projected always on the same physical surface.</p><p>Partially transparent textures are easily integrated in this approach: all pixels of T of a certain predefined color will not be projected on F i . This simple process allows artificial textures of any shape, and with holes, which often blend much better in the real image. Animated textures are also supported by projecting on each frame a different texture from a user-given texture sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental results</head><p>We present four image sequences demonstrating the qualities of the proposed system. The images of the tracked planar patches are put into complete correspondence along the frames, allowing to define in every frame an affine reference coordinate system where to project the artificial texture (section 5).</p><p>The Cinema sequence (figure <ref type="figure" target="#fig_7">9</ref>) shows the tracker robustness to discontinuous motion. A parallelogram-shaped region on a window of the background building is tracked and two advertising neon-light characters AR are added on the wall. Notice the texture is not projected on top of the region, but to the right of it, because this better fits in the scene. This is allowed by the mechanism explained in section 5. The AR characters change color, from red to blue every 20 frames, to simulate a flashing advertising neonlight. The sequence was taken by a fast moving hand-held camera and therefore contains considerable amounts of irregular motion: the region often bruskly changes direction and velocity, making it hard to predict the next location accurately. Moreover, the region moves fast: there are often large displacement between subsequent frames. As a result, the predicted location is often far from the correct one. The region was successfully tracked in every frame despite displacements of up to 22 pixels between subsequent frames and predictions that were off by up to 22 pixels from the target. The physical planar patch covered by the region is accurately tracked along the sequence, as proven by the constant very high cross-correlation score (average 0.97). This is very important for the quality of the final visual impact of the augmented scene, as it directly influences the accuracy by which the artificial texture is mapped into the real environment. As a result, the texture is perceived as strongly sticking on the building's wall and relative displacements are hardly noticeable. The computational performance 2 meets the real-time expectations: the sequence was processed in 22 ms per frame on average, and in no frames the tracker needed more than 34 ms (which corresponds to 30 2 All experiments performed on a Sun ULTRA10 workstation, with UltraSparc-IIi 440Mhz processor. Hz, the real-time bound). Since the time to map the texture was 9 ms per frame, the total average is still in the real-time limits.</p><p>The Graffiti sequence shows the combined application of scene recognition, region tracking and augmentation. The topmost image of figure <ref type="figure" target="#fig_8">10</ref> shows the model view. The second image is the first frame of the sequence to be augmented; the system successfully extracts and recognizes in this frame 8 regions of the model view and therefore knows this is the sequence it has to augment. The particular region to be augmented is found in the set of recognized regions (sunflower's eye). We see here the importance of elliptical regions: the Graffiti is drawn with very round lines and the wall contains several blob-like structures, but no strong corners or straight lines. The augmentation consists in the pictogram of a fish sprayed over the eye of the sunflower.</p><p>The scene was tracked at average 24 ms per frame, and the average cross-correlation between frames is 0.85.</p><p>Control of out-of-plane rotation is exemplified in the Desk sequence (figure <ref type="figure" target="#fig_9">11</ref>). The goal is to attach the BIWI logo to the top-right corner of a poster. The challenge is posed by the region's motion, which contains significant rotation around the vertical axis, causing skew and anisotropic scaling effects in the image. The tracker was able to handle the situation by correctly transforming the 2D region's shape: despite the very different viewpoints of frames 1 and 168, the region is covering the same physical surface. This feature of the tracker allows to deform the virtual texture so as to reproduce the effects of viewpoint changes, and is therefore crucial for the quality of the resulting augmented scene. A frame of the sequence was tracked in on average 19 ms, with 0.97 average cross-correlation score.</p><p>The last sequence illustrates a potential application of the proposed AR system (figure <ref type="figure" target="#fig_10">12</ref>). The goal is to visually help a worker, who is supposed to wear an head-mounted display, in the task of sticking a lightning metal plate on an electricity control panel. As the panel comes into the field of view, a glowing signal attracts the worker's attention to the area where the plate has to be stuck. As the worker approaches, the lightning plate appears and starts blinking exactly where it should be attached, looking like it was already stuck on the panel. As the worker moves, the superimposed plate moves and deforms with the viewpoint, in order to align with the image of the panel. The sign blinks faster as the worker gets closer, until it stops and it is continuously shown. The quality of the augmentation alignment is once again ensured by the accurate correspondences of the tracked region between frames (cross-correlation average 0.94). Each frame was tracked in average 24 ms, which, once added 8 ms for the texture mapping, fulfills the realtime promises.</p><p>In the current implementation we use a simple texture mapping algorithm carried out by the main processor. This allows to augment image areas of the size of the above examples (about 80x60 pixels) in about 8 ms. Since the tracker runs in average faster than real-time, this is still an acceptable overhead, and allows to perform the complete track-augment cycle in real-time. However, for much larger areas to be augmented, the texture-mapping overhead can become unacceptable. Fortunately cheaply available 3D accelerator chips have proven extreme texture mapping performances (e.g.: 3D games), providing an easy solution to this limitation. Notice how the system needs only the tracked region to be on a planar surface and does not need a large, dominant physical plane to work.</p><p>Although the tracker succeeded in finding the target region in all frames of the presented sequences, failure is possible in some frames. Most common reasons include strong  In many such cases the mechanism described in section 4 allows to recover the region after a few frames.</p><p>The experiments confirmed that the system can accurately superimpose virtual textures to a user-selected planar part of a natural scene in real-time, under general motion conditions, without the need of markers or other artificial beacons. The sequences are available at www.vision.ee.ethz.ch/˜ferrari/ISAR01. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this contribution a novel method for augmenting planar patches in image sequences with (animated) textures was introduced. The most important feature of the system is the ability of working with natural patches of the scene instead of artificial landmarks. Moreover, it is able to automatically recognize the scene and to identify the patch to be augmented, in spite of changing viewpoint and/or illumination conditions. Once the patch has been identified, it is tracked and augmented in real-time. As shown in our experiments, the tracker is robust to large discontinuous motion, achieves real-time performance on a standard workstation, and it brings the image patches into complete correspondence, even in case of large out-of-plane rotations, as it recovers the affine deformations of the image patch as it evolves in the sequence. As a result, tracking a single patch suffices for rigidly connecting a planar virtual texture to the scene. Future developments include photometric changes in the virtual textures and extension to 3D virtual objects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. A parallelogram-shaped and an elliptical region.</figDesc><graphic coords="2,341.67,72.21,173.20,109.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Parallelogram-shaped region construction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>Figure 3. Parallelogram-shaped regions: the intersection of the "valleys" of two different functions is used to determine the q point.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Original elliptical region; mapped to circular region; axes of inertia in circular region; axes mapped back to elliptical region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1Figure 6 .</head><label>6</label><figDesc>Figure 6. Anchor points (thick dots) are extracted in the search space S i , defined around the predicted anchor point â.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Left: Polyline snake initialization. Right: q initialization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>2 TFigure 8 .</head><label>28</label><figDesc>Figure 8. Mapping the texture T to F i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Cinema Scene. Top: tracked region and AR texture in frame 1; middle: texture in Frame 72; bottom left: cross-correlation score (y axe) in each frame (x axe); bottom right: velocity (dotted) and prediction errors (in pixels).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Graffiti Scene. Top: Model view; second: first frame of sequence; third: Fish in first frame; bottom: Fish in frame 66.</figDesc><graphic coords="8,329.91,509.53,173.20,150.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Desk Scene. Top: region and BIWI logo in frame 1; other images: BIWI logo in frames 86 and 168. motion blur (disturbing the parallelogram-shaped regions sides tracker), failure of the anchor point extractor (especially corner extraction) and (partial) occlusion of the patch.In many such cases the mechanism described in section 4 allows to recover the region after a few frames.The experiments confirmed that the system can accurately superimpose virtual textures to a user-selected planar part of a natural scene in real-time, under general motion conditions, without the need of markers or other artificial beacons. The sequences are available at www.vision.ee.ethz.ch/˜ferrari/ISAR01.</figDesc><graphic coords="9,62.91,373.28,198.00,151.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Panel Scene. Top: region and signal in frame 1; second: signal in frame 24; third: lighting sign in frame 101; bottom: lightning in frame 202.</figDesc><graphic coords="9,311.79,513.63,227.70,139.19" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors gratefully acknowledge financial support from EC projects CIMWOS and VIBES.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Azuma</surname></persName>
		</author>
		<title level="m">A survey of augmented reality, Presence: Teleoperations and Virtual Environments</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="355" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Virtual Round Table -A Collaborative Augmented Multi-User Environment</title>
		<author>
			<persName><forename type="first">W</forename><surname>Broll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd Int. Conf. on Collaborative Virtual Environments</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dealing with Speed and Robustness Issues for Video-Based Registration on a Wearable Computing Platform</title>
		<author>
			<persName><forename type="first">L-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J A</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Int. Symp. on Wearable Computers</title>
		<meeting>2nd Int. Symp. on Wearable Computers</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="84" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Good features to track</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Comp. Vis. and Pat</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="593" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust tracking for augmented reality using retroreflective markers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dorfmueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="795" to="800" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fusion of data from head-mounted and fixed sensors</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">1st Int. Workshop on Augmented Reality</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Stereo Vision-based Mixed Reality System with Natural Feature Point Tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kanbara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fuji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Takemura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yokoya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd Int. Symp. on Mixed Reality</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="56" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Experimental evaluation of Augmented Reality for assembly training</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kustaborder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Workshop on Augmented Reality</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">AR 2 Hockey: A case study of collaborative augmented reality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ohshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Satoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Virtual Reality Annual Int. Symp</title>
		<meeting>IEEE Virtual Reality Annual Int. Symp</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="268" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Realtime camera parameter estimation from images for a Mixed Reality system</title>
		<author>
			<persName><forename type="first">T</forename><surname>Okuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sakaue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Takemura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yokoya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Pat. Rec</title>
		<meeting>Int. Conf. Pat. Rec</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">CyberCode: Designing Augmented Reality Environments with Visual Tags</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Rekimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Ayatsuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Designing Augmented Reality Environments</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A new optical tracking system for virtual and augmented reality applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ribo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fuhrmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Markerless tracking using planar structures in the scene</title>
		<author>
			<persName><forename type="first">G</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Procs. of the Int. Symp. on Augmented Reality</title>
		<meeting>s. of the Int. Symp. on Augmented Reality</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visual Servoing-based Augmented Reality</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sundareswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Behringer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procs. Int. Workshop on Augmented Reality</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Best of Two Worlds: Merging Virtual and Real for Face-to-Face Collaboration</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Poupyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Billinghurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Regenbrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tetsuani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Shared Environments to Support Face-to-Face Collaboration</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vision-Based Object Registration For Real-Time Image Overlay</title>
		<author>
			<persName><forename type="first">M</forename><surname>Uenohara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Comp. in Biology and Medicine</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="260" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Using Dynamic Programming for Solving Variational Problems in Vision</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Weymouth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans on Patt Analysis and Machine Intell</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="855" to="866" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Bascle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Deriche</surname></persName>
		</author>
		<title level="m">Region tracking through image sequences Int. Conf. on Comp. Vis</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="302" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
		<title level="m">Elliptical Head Tracking Using Intensity Gradients and Color Histograms IEEE Conf. on Comp. Vis. and Pat</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="232" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
		<title level="m">Real-Time Tracking of Non-Rigid Objects using Mean Shift IEEE Conf. on Comp. Vis. and Pat</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="142" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Snakes: active contour models Int</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Comp. Vis</title>
		<imprint>
			<biblScope unit="page" from="321" to="331" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recognizing color patterns irrespective of viewpoint and illumination</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mindru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Comp. Vis. and Pat</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="368" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Content-Based Image Retrieval based on Local</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Affinely invariant Regions&quot; Third Int. Conf. on Visual Information Systems</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="493" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Wide Baseline Stereo based on Local, Affinely invariant Regions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="412" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mase</surname></persName>
		</author>
		<title level="m">Tracking Moving Contours Using Energy-Minimizing Elastic Contour Models&quot; 2nd European Conf. On Comp. Vis</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="453" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Natural Feature Tracking for Augmented Reality</title>
		<author>
			<persName><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="64" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
