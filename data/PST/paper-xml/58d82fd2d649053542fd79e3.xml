<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE Transactions on Image Processing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">IEEE Transactions on Image Processing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0BEB01EBD3E68CC57FF2D0BED01CB9FD</idno>
					<idno type="DOI">10.1109/TIP.2017.2655449</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Effective Multi-Query Expansions: Collaborative Deep Networks for Robust Landmark Retrieval Yang Wang, Xuemin Lin, Fellow, IEEE, Lin Wu, Wenjie Zhang Abstract-Given a query photo issued by a user (q-user), the landmark retrieval is to return a set of photos with their landmarks similar to those of the query, while the existing studies on the landmark retrieval focus on exploiting geometries of landmarks for similarity matches between candidate photos and a query photo. We observe that the same landmarks provided by different users over social media community may convey different geometry information depending on the viewpoints and/or angles, and may subsequently yield very different results. In fact, dealing with the landmarks with low quality shapes caused by the photography of q-users is often nontrivial and has seldom been studied. In this paper we propose a novel framework, namely multi-query expansions, to retrieve semantically robust landmarks by two steps. Firstly, we identify the top-k photos regarding the latent topics of a query landmark to construct multi-query set so as to remedy its possible low quality shape . For this purpose, we significantly extend the techniques of Latent Dirichlet Allocation. Then, motivated by the typical collaborative filtering methods, we propose to learn a collaborative deep networks based semantically, nonlinear and high-level features over the latent factor for landmark photo as the training set, which is formed by matrix factorization over collaborative userphoto matrix regarding the multi-query set. The learned deep network is further applied to generate the features for all the other photos, meanwhile resulting into a compact multi-query set within such space. Then, the final ranking scores are calculated over the high-level feature space between the multi-query set and all other photos, which are ranked to serve as the final ranking list of landmark retrieval. Extensive experiments are conducted on real-world social media data with both landmark photos together with their user information to show the superior performance over the existing methods, especially our recently proposed multiquery based mid-level pattern representation method <ref type="bibr" target="#b0">[1]</ref>.</p><p>Index Terms-Landmark Photo Retrieval, Multi-Query Expansions, Collaborative Deep Networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE popularity of personal digital photography has led to an exponential growth of photos with landmarks. The current social media data set characterize both the landmark photos and associated uploaded user information. e.g., panoramio.com and Picasa Web Album 1 . This highly demands for the research in the area of efficient and effective retrieval of photos based on landmarks (e.g., tower and churches), namely landmark retrieval. Given a query photo, the landmark retrieval returns the set of photos with their landmarks highly similar to that of the query photo. Y. Wang, X. Lin and W. Zhang are with the University of New South Wales, Kensington, Sydney, Australia (e-mail:{wangy, lxue, zhangw}@cse.unsw.edu.au).</p><p>L. Wu is with Institute of social science research and ITEE, The University of Queensland, Brisbane, Australia. Email: lin.wu@uq.edu.au 1 picasa.google.com Fig. <ref type="figure">1</ref>: A q-user has issued a biased landmark photo of Eiffel Tower in Paris in the left most photo. Multiple users with the same latent topic as the q-user are selected to recommend three more landmark photos taken at the same place, that complement the given query landmark to construct a multi-query set.</p><p>Unlike the conventional photo retrieval that performs within the low-level feature spaces (e.g., color and texture), the landmark retrieval is conducted based on geometry information of landmarks. A number of paradigms <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref> have been proposed to perform the landmark retrieval under the heterogeneous feature spaces, including the paradigms based on patch level region features <ref type="bibr" target="#b1">[2]</ref>, mid-level attributes <ref type="bibr" target="#b2">[3]</ref>, and the combination of low-level features <ref type="bibr" target="#b3">[4]</ref>.</p><p>Among the existing techniques, there is one critical assumption: a high quality of query photo is always provided; that is, the landmark captured from a query photo always provides a shape with high quality. Nevertheless, such an assumption is not always true in practice. Indeed, the landmark of a query photo may provide shape with low quality due to various reasons, such as personal preference, photography, etc. For example, as illustrated in Fig. <ref type="figure">1</ref>, the left most photo taken by a q-user gives the landmark Eiffel Tower with a very low quality shape . Consequently, the existing methods may not be able to return the set of photos that contain the Eiffel Tower since the geometry quality of the landmark in the query photo is too low.</p><p>Motivated by this, in this paper we propose a novel method, depicted in Fig. <ref type="figure" target="#fig_2">2</ref>, for a robust landmark retrieval through a novel paradigm based on multi-query expansions over the social media data set with the information for both users and their uploaded landmark photos. Firstly, we propose to identify a set of photos that share the similar latent topics with the query landmark by adopting the Latent Dirichlet Allocation (LDA) techniques <ref type="bibr" target="#b4">[5]</ref>. Then, we propose a localized matrix factorization technique over the user-photo collaborative ma-Fig. <ref type="figure" target="#fig_2">2:</ref> The flow chart of our framework. Our framework comprises the following main phases: 1) query landmark based topic and user community discovery; 2) select multiple landmark photos from user-query photo matrix to form a multi-query set; and 3) Learning a collaborative deep network based feature representation to characterize the multi-query set and other landmark photos in the database (see Fig. <ref type="figure" target="#fig_1">4</ref>), and calculate the ranking score between multi-query set and each photo in the database, leading to the final ranking based retrieval result.</p><p>trix that encodes the information from the selected photos, where the latent factor regarding landmark photos can be obtained. The k-mean clustering algorithm is subsequently applied to the landmark photo latent factor to generate the clustering result, with each cluster as one pseudo class. Remark. we remark that clustering is performed over latent factors for landmark photo rather than original data objects, as indicated by collaborative filtering methods, such as <ref type="bibr" target="#b5">[6]</ref>, the latent factors can encode rich underlying similarity so that the ideal clustering result can be achieved.</p><p>The deep convolutional Neural Network is then trained under all pseudo classes so that the high-level semantic feature can be extracted via deep network over both query photo and all other landmark photos.</p><p>Note that we can select the top-s photos from the multiquery set by calculating the similarity score between the query landmark photo and all other landmark photos within the highlevel deep feature space, resulting into a more compact yet encoding similar latent topics as the query landmark photo. We simply combine all the high-level feature representations for all the photos in the multi-query set to be the final representation, which is utilized to calculate the final ranking similarity score over all landmark photos in the database. Remark. We remark that our work is a non-trivial extension of our previous published work <ref type="bibr" target="#b0">[1]</ref>,</p><p>• where a mid-level pattern representation over the expanded multi-query set is proposed for robust landmark retrieval. Following <ref type="bibr" target="#b0">[1]</ref>, our method exploits not only the information regarding the landmark photos but also the associated user information for multi-query expansion over robust landmark retrieval. Different from <ref type="bibr" target="#b0">[1]</ref> by learning a mid-level pattern representation for landmark retrieval, we propose to learn high-level semantic feature over the pseudo classes by clustering the latent factors regarding landmark photos yielded from the matrix factorization over the collaborative user-photo matrix.</p><p>Based on <ref type="bibr" target="#b0">[1]</ref>, we features the following novel contributions in this paper.</p><p>• Unlike learning the mid-level pattern representations in <ref type="bibr" target="#b0">[1]</ref>, we learn the high-level deep features to tackle the problem of landmarks in query photos with possibly low quality shapes , over both multi-query expansions and landmark photos. To the best of our knowledge, we are the first to learn the deep features for this problem.</p><p>• Unlike <ref type="bibr" target="#b0">[1]</ref> by exploiting user information to perform multi-query expansion only, We train the deep network over the pseudo classes corresponding to the clusters over the latent factors from the user-photo matrix via matrix factorization. • As the matrix factorization over user-photo matrix is categorized as collaborative filtering field, hence we propose a collaborative deep network feature learning method for both multi-query set and all other landmark photos, which are further utilized to conduct landmark retrieval within such high-level feature space. We conducted more experiments than <ref type="bibr" target="#b0">[1]</ref> over real-world landmark photo datasets, validating the effectiveness of our approach.</p><p>The rest of this paper is structured as follows. We review the related work in Section II. Then, we describe our proposed technique in Section III. We experimentally validate the performance of our approach in Section IV, and conclude this paper in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we mainly review the related work on landmark retrieval and related query argumentation technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Feature Learning for Landmark Retrieval</head><p>The increasing amount of landmark photos has resulted in numerous methods for landmark retrieval <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Hays et al. <ref type="bibr" target="#b3">[4]</ref> presented a feature matching approach to return the K nearest neighbors with respect to the query landmark photo where a query photo and photos in database are represented by aggregating a set of low-level features to perform landmark retrieval. Zhu et al. <ref type="bibr" target="#b6">[7]</ref> proposed to learn the landmark feature by combining low-level features while assisted with Support Vector Machine (SVM). In <ref type="bibr" target="#b1">[2]</ref>, a region based recognition method is proposed to detect discriminative landmark regions at patch level, which is seen as the feature for landmark retrieval. To augment semantic interpretation on landmark representation, Fang et al. <ref type="bibr" target="#b2">[3]</ref> presented an effective approach, namely GIANT, to discover both discriminative and representative mid-level attributes for landmark retrieval. Zhu et al. <ref type="bibr" target="#b7">[8]</ref> propose a hypergraph model for landmark retrieval.</p><p>However, these approaches are still using a single query photo for landmark retrieval whilst our approach is focusing on mining robust patterns of landmark photos from an expanded multi-query set. Wang et al. <ref type="bibr" target="#b0">[1]</ref> proposed to learn a mid-level pattern representations for landmark retrieval over multi-query set, which applied the KRIMP <ref type="bibr" target="#b8">[9]</ref> algorithm and Minimumdescription-length (MDL) principle <ref type="bibr" target="#b9">[10]</ref> to mine the compact pattern representation. Unlike that, we propose a deep network based high-level features for landmark retrieval. As a related problem, several scene categorization methods e.g., <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> via deep feature learning problem is also proposed. Besides the different problem from ours, we also exploited the user information for deep feature learning for landmark retrieval.</p><p>There are abundant related work towards feature learning via dictionary learning <ref type="bibr" target="#b12">[13]</ref>. For instance, Zhu et al. <ref type="bibr" target="#b13">[14]</ref> proposed a weakly-supervised cross-domain dictionary learning method is proposed to learn a reconstructive, discriminative and domain adaptive dictionary. To this end, the weakly labeled data is utilized from the source data set to span the coverage of intra-class diversity over the training data to gain discriminative power. However, they have not studied the problem of landmark retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Query Argumentation Technique</head><p>In <ref type="bibr" target="#b14">[15]</ref>, a query expansion technique is brought into the visual domain in which a strong spatial constraint between a query image and each result allows for an accurate verification of each return, improving image retrieval performance. This simple method of well-performing query expansion is referred to as Average Query Expansion <ref type="bibr" target="#b14">[15]</ref> (AQE) where given a query image, images are ranked using tf-idf scores and corresponding visual words in these images are averaged together with the query visual words, and this resulting query expanded visual word vector is recast to be a new query to re-query the database. Observing that AQE is lack of discrimination, Arandjelovic et al. <ref type="bibr" target="#b15">[16]</ref> enhanced it using a linear SVM to discriminatively learn a weight vector for re-querying yields a significant improvement over the standard average query expansion method, called DQE. The most related work to ours is <ref type="bibr" target="#b16">[17]</ref> (PQE), where a query expansion approach for a particular object retrieval is presented. Our method is different from them in terms of multi-query construction. Zhu et al. <ref type="bibr" target="#b17">[18]</ref> propose to perform landmark classification with a hierarchical multi-modal exemplar features. There are also research <ref type="bibr" target="#b18">[19]</ref> aiming at developing the feature representations for diverse landmark search.</p><p>These methods are commonly based on the idea that those particular multiple queries are manually selected or simply retrieved from top-k similar items whilst we automatically determine helpful queries by exploring the latent topics of query landmark as well as the informative user communities. Besides, previous query expansion pipelines are not applicable in the context of social media networks, which cannot be addressed by simple variations of methods in literature. To address the limitation, Wang et al. <ref type="bibr" target="#b0">[1]</ref> proposed a multiquery expansion technique to learn a robust mid-level pattern representation. Based on that, we propose to learn high-level deep feature over multi-query set, which is superior to the mid-level representation in <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Geo-tagging by Exploring User Community</head><p>Geo-tagging refers to adding geographical identification metadata into various multimedia data such as images <ref type="bibr" target="#b19">[20]</ref> in websites, blogs, and photo-sharing web-services <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b22">[23]</ref>. Associating time and location information (latitude/longitude) with pictures can facilitate geotagging-enabled information services in terms of finding location-based news, photos, or other resources <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b25">[26]</ref>. There are also a number of approaches <ref type="bibr" target="#b26">[27]</ref> trying to learn the location based visual codebook for landmark search. Similar to our method, such research area also explore the user communities. However, this kind of research is apparently different from landmark retrieval studied in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Our Method</head><p>To the best of our knowledge, we are the first to learn the deep network based high-level features over latent factors regarding landmark photos for landmark retrieval. To achieve that, the user information are also explored, which can be easily obtained from the social media data.</p><p>Unlike the previous work, inspired by <ref type="bibr" target="#b27">[28]</ref>, our proposed method aims at learning a deep convolutional neural network based high-level features over landmark latent factor obtained by performing matrix factorization over collaborative userphoto matrix corresponding to the multi-query set. Then the deep features for both the multi-query set and landmark photos can be generated for further landmark retrieval within highlevel deep feature space.</p><p>It is well known that learning deep network features plays a crucial role for a wide range of applications. Shao et al. <ref type="bibr" target="#b28">[29]</ref> proposed a multi-spectral neural networks (MSNN) is by projecting multiple penultimate layer of the deep network, namely multi-columns deep network, into multi-spectral smooth embedding to achieve the complementary multi-view spectral relationship. To this end, an effective multi-view alignment strategy is proposed. As a by-product, the smooth low-dimensional manifold embedding enables it to be robust to possible noise corruptions in raw feature representations. A multi-objective genetic program with four-layer structure is proposed in <ref type="bibr" target="#b29">[30]</ref> to automatically generate domain adaptive global feature descriptors for image classification. A typical yet simple deep network <ref type="bibr" target="#b30">[31]</ref> is learned for image classification, by employing PCA to learn multi-stage filter banks, followed by simple binary hashing and block histograms for indexing and pooling. Some multi-view feature fusion methods <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> Our method features the following non-trivial differences from the above deep network research. by exploiting the user information associated with the landmark photos to form the landmark photo clusters i.e., pseudo-classes, upon which we further learn the deep features for landmark retrieval over such latent factors. We seek the latent cluster representations for user-landmark matrix via matrix factorization, which characterize more useful information than original user-landmark matrix. Based on the above, given a query landmark photo to be issued, we expand the multi-landmark photo to augment the single query candidate to address the possibly biased query via the Latent Dirichlet Allocation (LDA) for final retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED TECHNIQUE</head><p>In this section, we formally present our technique consists of the following components: The first stage is about latent topic discovery on landmark photos via LDA model, and multi-query expansion based on topic related user groups. The second stage is to train a deep network suitable for landmark photos with user priors such that given input of a multiquery set offered by the first stage, deep features can be extracted regarding both the landmark database as well as the multiple complementary query photos, upon which the deep feature of each photo in the multi-query set are aggregated into overall discriminative representations for retrieval over landmark database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Latent Topics and User Group Discovery</head><p>As stated above, our first stage is to detect the latent topics over landmark photos including query landmark. We first introduce some preliminaries on notations and definitions in LDA model.</p><p>1) Preliminaries about LDA: Formally, given a set of users U , each of which is associated with their landmark photos. We assume these observed photos can be grouped into clusters as per their latent topics, where each latent topic is related to a set of landmarks; e.g., an photo containing the landmark "Eiffel tower" may belong to the topic "architecture"; the landmark "Himalayas" may belong to the topic "mountain". Suppose a set of topics for a query are detected, each photo can then be modeled as a probabilistic distribution over these topics.</p><p>Based on that, it can be seen as a generative model where each user album, a.k.a document i, is composed of a number of topics, and the generation for each photo is probabilistically determined by the topics of the album.</p><p>It remains a challenge to recommend landmark photos that can complement a query landmark because it is unable to model their similarities by referring to visual appearance which are displaying dramatic changes. To this end, we propose to discover latent topics over the dataset of landmark photos, where the photos sharing the same topics can be clustered together to constitute an augmented multi-query set. We utilized color SIFT descriptors <ref type="bibr" target="#b33">[34]</ref> to encode landmark photos, where LDA is further latent topic detection. In this step, we use Latent Dirichlet Allocation (LDA) <ref type="bibr" target="#b4">[5]</ref> which allows sets of observations (landmark photos) to be explained by unobserved groups that explain how much of the landmark photos are similar. In our case, photos are regarded as observations that are collected into user albums (a.k.a documents in LDA), and it posits that each document is a mixture of a small number of topics such that each photo's creation is attributable to one of the document's topics. Thus, the topic model of LDA can be used to explain the relatedness of landmark photos which cannot be described reliably by feature descriptors. Once latent topics are discovered w.r.t user album, we can construct a user group that consists of users who upload landmark photos sharing similar topics to the query landmark, from which multi-query expansion can be performed (Section III-B). Now, we are ready to reformulate the problem of query landmark topic discovery into a LDA model. Let α and β be the parameter of the Dirichlet prior on the per-document (album) topic distributions and the per-topic word (photo) distribution. θ i is the topic distribution for album i, and φ z denotes the photo distribution for topic z. z ij is the topic for the j-th photo in album i, and ω ij is the specific photo. The ω ij are the only observable variables, and the other variables are latent variables. In fact, φ is a Z × V Markov matrix (Z is number of topics considered in LDA, and V is the dimension of the vocabulary, i.e., number of photos), and each row of which denotes the photo distribution of a topic. Thus, LDA assumes a generative process for a corpus D consisting of |D| user albums each of length N i : 1) Choose the number of topics: Z 2) Choose θ i ∼ Dir(α), where i ∈ {1, . . . , |D|}, and Dir(α) is the Dirichlet distribution for parameter α 3) Choose φ z ∼ Dir(β), where z ∈ {1, . . . , Z} 4) For each of the photo position i, j where j ∈ {1, . . . , N i }, and i ∈ {1, . . . , |D|} As a result, the output of LDA over D, denoted as LDA(D), can be seen as a partition of D into multiple groups, each of which contains photos characterizing the same latent topics. Remark. To detect the latent topic of the query landmark, we need to perform LDA over the entire photo database, and such process is performed via an off-line fashion in our implementation.</p><formula xml:id="formula_0">• Choose a topic z ij ∼ M ultinomial(θ i ) • Choose an photo ω ij ∼ M ultinomial(φ zij ).</formula><p>Fig. <ref type="figure" target="#fig_0">3</ref>: The LDA model of graphical representation on landmark photo distributions. A topic distribution θ i (e.g., "architecture" and "mountain") on album i is chosen by Dirichlet prior Dir(α), and photo distribution φ z on topic z is chosen by Dir(β). Then each photo ω from i can be modeled as a distribution on its topic z ij (ω ∼ M ultinomial(φ zij )) with topic z ij characterized by z ij ∼ M ultinomial(θ i ).</p><p>2) Detecting Latent Topics for A Landmark Query: We propose to detect potential latent topics for a landmark query such that highly relevant photos as per each topic w.r..t the query can be clustered, from which a multi-query set can be constructed. Recall that each topic z has the probability of generating photos, and for the query q, its probability distribution over z is denoted as P (q|z). Commonly, we set a probability threshold λ to decide the candidate topics for the query q. That is, P (q|z) ≥ λ indicates that z could be one candidate topic for q. Then, we have two cases regarding the probability threshold λ:</p><p>1) if λ is small, we may generate a lot of candidate topics, while many of them might be non-relevant to q. 2) if λ is large, the size of topic set would be quite small, which may not be inclusive to involve true topics for q. Thus, we learn a trade-off value for λ from empirical studies, which is critical to decide the topic set regarding the query q. Once the query topic set is determined, it is plausible to recommend a number of photos sharing the same topics to q. To this end, we propose to utilize user communities to assist multi-query set construction. Specifically, an user is seen to be related to the query user, if he/she uploads photos sharing at least one of the topics associated with an photo uploaded by the query user. Hence, an user-photo matrix can be constructed as M ∈ R |U |×|J| where U and J denote the user and photo set, respectively. |U | and |J| denote the number of users and photos, and u q ∈ U represents the query user. In M , we have M (u, j) = 1 if a user u ∈ U uploaded a photo j into J, and M (u, j) = 0, otherwise. In what follows, we will elaborate the process of selecting top-K photos to complement the query q to be a robust multi-query set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-Query Expansion</head><p>To discover an appropriate set of photos for query landmark expansion, we perform non-negative matrix factorization <ref type="bibr" target="#b34">[35]</ref> on the user-photo matrix M , where the users (include u q ) and relevant photos are detected by LDA described in Section III-A. By factorizing M into a lower-dimensional latent space, each photo's possibility of being recommended to u q is computed based on the resulting factorization. Then, photos with K highest relevant scores (high possibility to share same topics w.r.t the query) are selected, together with the original query, to form a robust multi-query set. Specifically, we perform the non-negative matrix factorization on M by minimizing the objective function below.</p><formula xml:id="formula_1">min P,V ||M -P V || F , P ≥ 0, V ≥ 0,<label>(1)</label></formula><p>where ||•|| F denotes the Frobenius norm. Two low-rank factor matrices P ∈ R |U |×L and V ∈ R L×|J| are obtained by solving Eq.( <ref type="formula" target="#formula_1">1</ref>), where P models the mapping of users in the lowdimensional latent space with L dimensions, and V defines the mappings of photos to the same latent space. That is, each user u is represented as the u-th row vector of P denoted by P (u q , •), while each photo j is encoded by the j-th column vector of V denoted by V (•, j). We reformulate Eq. ( <ref type="formula" target="#formula_1">1</ref>) into the following optimization problem</p><formula xml:id="formula_2">min P,V ||M -P V || 2 F + β(||P || 2 F + ||V || 2 F ),<label>(2)</label></formula><p>where β is the balance parameter, and the stochastic gradient descent <ref type="bibr" target="#b35">[36]</ref> is applied to solve Eq. ( <ref type="formula" target="#formula_2">2</ref>).</p><p>As presented in our earlier work <ref type="bibr" target="#b0">[1]</ref>, relevant photos can be selected by computing their inner product with the query user vector in the factorized space. Specifically, for u q ∈ U , a confidence score can be computed to determine if photo j will be recommended to u q through the inner product of P (u q , .) and V (., j):</p><formula xml:id="formula_3">S(u q , j) =&lt; P (u q , •), V (•, j) &gt; .<label>(3)</label></formula><p>Through the computation of Eq. ( <ref type="formula" target="#formula_3">3</ref>), we can select K photos with high confidence scores to construct a robust multiquery set Q that describe a landmark of interest from different aspects. To improve the landmark recognition, faithfully representative descriptions for categorical landmark photos are needed. To leverage multiple photos in Q and generate descriptive representation for a landmark, an effective midlevel pattern mining strategy is proposed in our earlier work <ref type="bibr" target="#b0">[1]</ref> where frequent patches are discovered by principled minimum description length. However, this pattern representation is obtained via an unsupervised fashion, which carries no semantically categorical information in landmark photos.</p><p>To learn high-level discriminative representations for landmark photos, we propose to learn deep features for Q through the powerful convolutional neural networks (CNNs) <ref type="bibr" target="#b27">[28]</ref> which have shown impressive performance in a variety of recognition tasks. In our case, a trainable deep network can be fine-tuned on the landmark data to generate semantically high-level features regarding landmark samples in categories. We describe the design and training of deep network in Section III-C. Once multiple deep features for landmark photos in Q are learned, we aggregate them into overall feature representation to describe a landmark of interest, and perform similarity search between the query and each landmark photo in database with their high-level features. In our paper, we use Euclidean distance to calculate similarity scores. Remark. One may think only LDA can make the satisfied retrieval results. However 1) the LDA can only detect the landmarks sharing the similar latent topics above a probability threshold. 2) Even the candidates within the similar topics, the final retrieved landmark candidates still need to be ranked according to their landmark relevance to achieve a satisfied accuracy based on evaluation metric. To this end, we need to learn a strong deep feature representations over both multiquery set and landmark photos for the final retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Deep Landmark Feature Learning on Collaborative Convolutional Neural Networks (C-CNNs)</head><p>The query expansion step produces a set of query photos that are describing the landmark of interest in different aspects. To combine these complementary priors, we focus on constructing photo representations, i.e., encoding functions φ mapping each query photo I to a vector φ(I) ∈ R d , and an effective combination method to integrate multiple vectors into an overall feature vector. Since these query photos are displaying landmarks with visual variance, we employ the Convolutional Neural Networks (CNN) <ref type="bibr" target="#b27">[28]</ref> with several layers of non-linear feature extractors to generate high-level deep representations.</p><p>1) Deep Representations with Pre-training: Our deep representations are inspired by the success of CNN in photo classification <ref type="bibr" target="#b27">[28]</ref>. As shown in <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, the vector of φ(I) of a deep CNN, learned on large dataset such as photoNet <ref type="bibr" target="#b38">[39]</ref>, can be used as a powerful photo descriptor applicable to other datasets. Here we adopt a fast architecture (CNN-F <ref type="bibr" target="#b39">[40]</ref>) which is similar to the one used by Krizhevsky et al. <ref type="bibr" target="#b27">[28]</ref>. It comprises 8 learnable layers, 5 of which are convolutional, and the last 3 are fully-connected. The input photo size is 224×224. Fast processing is ensured by the 4 pixel stride in the first convolutional layer. The main difference between CNN-F and that of <ref type="bibr" target="#b27">[28]</ref> are the reduced number of convolutional layers and the dense connectivity between convolutional layers. The structure of CNN-F is shown in Table <ref type="table">I</ref>.</p><p>2) CNN Fine-tuning on the Target Dataset: It was demonstrated <ref type="bibr" target="#b40">[41]</ref> that fine-tuning a pre-trained CNN on the target data can significantly improve the performance, and we consider this scenario to obtain photo features which are datasetspecific to landmark dataset. In our framework, we fine-tuned CNN-F using the landmark dataset where fine-tuning was carried out using the CNN pre-trained parameters on ILSVRC <ref type="bibr" target="#b41">[42]</ref>. We employ data augmentation in the form of cropping and flipping. CNN requires photos to be transformed to a fixed size (224 × 224), and hence the photo is downsized so that the smallest dimension is equal to 224 pixels. Then 224 × 224 crops are extracted from the four corners and the centre of the photo. These crops are then flipped about the y-axis, producing 10 perturbed samples per input photo.</p><p>Note that the last fully-connected layer (FC3) has output dimensionality equal to the number of classes, which differs between datasets. To this end, we cluster the landmark photos within the latent factor with L dimensional space into C clusters via K-means, which naturally corresponds to the C pseudo-classes regarding supervision information (see Fig. <ref type="figure" target="#fig_2">2</ref>). Clustering landmark photos in their factorized latent space instead of using their original category priors suggests user related supervision signals. As the latent factors regarding the landmark photos yielded by matrix factorization follows the principle of collaborative filtering <ref type="bibr" target="#b5">[6]</ref> in recommendation system, we name the feature learning procedure as deep landmark feature learning on Collaborative Convolutional Neural Networks (denoted as C-CNNs). In this way, latent factors characterize rich similarity information among landmark photos, which can facilitate the clustering process to produce accurate clusters as supervision signals. The essence of the proposed C-CNNs is to map latent factors to the landmark photo pixels, such that the user-photo correspondence can be encoded. However, latent factors regarding a landmark query could be noisy. To this end, softmax function is adopted <ref type="bibr" target="#b42">[43]</ref> on the top layer of the deep network to classify the membership of input photo probabilistically.</p><p>3) Fisher Vector Encoding on Multi-Query Set: Once we have obtained trainable parameters of C-CNNs, the multiquery set Q can be transformed through the network to produce multiple high-level feature vectors Q = {x 1 , . . . , x |Q| }. To aggregate these vectors into an overall representation that best describes the landmark of interest, we employ Fisher Vector to encode them with high-order statistics <ref type="bibr" target="#b43">[44]</ref> (See Fig. <ref type="figure" target="#fig_1">4</ref>). An alternative aggregation strategy is average-pooling, which is formulated as: x = 1 |Q| |Q| i=1 x i . However, averagepooling is unable to capture the correlation of multiple feature vectors in regards to a particular landmark. In fact, landmarks within scene photos could exhibit dramatically different appearances, shapes, and aspect ratios. To distinguish one landmark category from another, it is much desired to harvest discriminative and representative landmark-specific object parts, which are collected in the multi-query set Q.</p><p>To this end, we use Fisher Vector (FV) <ref type="bibr" target="#b43">[44]</ref> to aggregates multiple deep feature vectors into a high dimensional nonlinear representation that is suitable for classifier. FV can effectively address geometric variance in scene photos, and can be combined with CNNs <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref> to improve classification performance. We quantitatively evaluate the combination of C-CNNs with FV or average-pooling, as shown in IV-E.</p><p>The Fisher vector encoding Φ of a set of features is based on fitting a parametric generative model such as the Gaussian Mixture Model (GMM) to the features, and then encoding the derivatives of the log-likelihood of the model with respect to its parameters <ref type="bibr" target="#b46">[47]</ref>. It has been shown to be a state-of-the-art local patch encoding technique <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b47">[48]</ref>. Our base representation of a query photo is a set of deep feature vectors corresponding to multi-query set Q. The descriptors are first PCA-projected to reduce their dimensionality and decorrelate their coefficients to be amenable to the FV description based on diagonalcovariance GMM. Given a GMM with G Gaussians, parameterized by {π g , µ g , σ g , g = 1 . . . , G}, the Fisher vector encoding leads to the representation which captures the average first and second order differences between the features and each of the GMM centres <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>. For a descriptor x ∈ R D , we define a vector Φ(x) = [ϕ 1 (x), . . . , ϕ G (x), ψ 1 (x), . . . , ψ G (x)] ∈ R 2GD . The subvectors are defined as</p><formula xml:id="formula_4">ϕ g (x) = 1 √ π g γ g (x) x -µ g σ g ∈ R D , ψ g (x) = 1 2π g γ g (x) (x -µ g ) 2 σ 2 g -1 ∈ R D ,<label>(4)</label></formula><p>where {π g , µ g , σ g } g are the mixture weights, means, and diagonal covariance of the GMM, which are computed on the training set; γ g (x) is the soft assignment weight of the feature  x to the g-th Gaussian. In this paper, we use a GMM with G = 256 Gaussians.</p><p>To represent a query set Q, one averages/sum-pool the vector representations of all descriptors, that is,</p><formula xml:id="formula_5">Φ(Q) = 1 |Q| |Q| i=1 Φ(x i ).</formula><p>The Fisher vector is further processed by performing signed square root and 2 normalization on each dimension d = 1, . . . , 2GD:</p><formula xml:id="formula_6">Φd (Q) = (sign Φ d (Q)) |Φ d (Q)|/ ||Φ(Q)|| 1 (5)</formula><p>For simplicity, we refer to the resulting vectors from Eq.( <ref type="formula">5</ref>) as Fisher vectors and to their inner product as Fisher kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we present extensive experimental results by comparison to a variety of baselines and state-of-arts to evaluate the effectiveness of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>Two datasets are constructed by collecting landmark photos from Flickr and Picasa Web Album. They are suitable for landmarks retrieval because they contain both user information and corresponding landmark photos.</p><p>• Flickr. We use the Flickr API to retrieve landmark photos taken at a city posted by a large number of users. We sort out 11 cities: London, Paris, Barcelona, Sydney, Singapore, Beijing, Tokyo, Taipei, Cairo, New York city, and Istanbul. In each city, such as Paris, we obtained photos by querying the associated text tags for famous landmarks such as "Paris Eiffel Tower" or "Paris Triomphe"   amount of GPS-tagged photos uploaded by users who have visited the landmarks, along with their text tags. We manually download a fraction of photos and their user information on 6 cities: London, Paris, Beijing, Sydney, Chicago, and Barcelona. The statistics for user information and their uploaded photos over the two datasets are summarized in Table <ref type="table" target="#tab_1">II</ref>.</p><p>Remark. It is noteworthy to remark the followings • For our data set, we don't follow the assumption of selecting the same landmark photo from the different users, as also mentioned by our previous conference paper <ref type="bibr" target="#b0">[1]</ref>, such multi-query expansion comprising the same redundant landmark photos is not informative. Actually, for our practical implementations, we seldom meet such scenario. • It is more commonly a scenario that the same landmark with different capturing conditions such as varied capturing angles. Given a possible biased query landmark photo, we just attempt to choose the other more photos that characterize the same landmark yet with a more ideal conditions to reflect its ground-truth. • However, we have to admit that each photo set as we adopt characterizes the same ground-truth landmark with a relative smaller portions of biased landmark photo, which ensure to validate the effectiveness of our multiquery expansion technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training and validation</head><p>For all datasets, we split them with the training, validation and testing 70%, 10% and 20% respectively. The training set is utilized for C-CNN feature learning and the testing evaluates the learned feature representation. We also leave a small portion of training set for validation. As aforementioned, we perform matrix factorization over the entire training data by solving Eq. ( <ref type="formula" target="#formula_2">2</ref>) to estimate the parameter β. Note that we adopt the root-mean-square error <ref type="bibr" target="#b50">[51]</ref> as the validation metric to estimate the optimal parameter β for Eq. ( <ref type="formula" target="#formula_2">2</ref>). The validation process shows that β=0.05 and L=200 can reach satisfied performance in terms of root-mean-square error. Besides, we set the cluster number over latent factors C=1000 for C-CNN feature learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Settings a) Evaluation Metric:</head><p>We choose precision-recall as the metrics to evaluate both our model and state-of-the-art methods over top-100 retrieval ranked list w.r.t each query landmark. In each landmark, we issue 20 queries and an average precision score is computed for all queries, which are averaged to obtain a mean Average Precision (mAP) for the each landmark category.</p><p>b) Baselines: We consider several variants of our model to understand each component of the proposed C-CNN. Also, some recent deep scene classification methods are considered as strong baselines:</p><p>• CNN-F+Average: The CNN-F model is fine tuned on original class labels, and then average pooling is used to generate the overall feature vector. • CNN-F+FV: The CNN-F model is fine tuned on original class labels, and then Fisher Vector encoding is used to generate the overall feature vector. • C-CNN+Average: The proposed collaborative CNN is fine tuned on pseudo labels from user supervision, and then average pooling is used to generate the overall feature vector regarding the multi-query set. • Ours (C-CNN+FV): The architecture is the same as C-CNN except that Fisher Vector encoding is employed to produce the overall feature representation. • Deep Places Features: This is a strong deep baseline which uses CNN to learn deep features from the largest scene-centric database, namely Places Database <ref type="bibr" target="#b10">[11]</ref>. The Places database contains over 7 millions labeled pictures of scenes from which the CNN features can achieve stateof-the-art results. • MetaObject-CNN <ref type="bibr" target="#b11">[12]</ref>: This method is to learn discriminative features for scene classification by fine-tuning CNN on pre-generated patches containing objects and parts that frequently occur in photos for scene category. c) Competitors: In our experiment, we consider nine competitors listed below. In their implementation of training, SIFT descriptor <ref type="bibr" target="#b51">[52]</ref> is used as a local descriptor due to its excellent performance in object recognition <ref type="bibr" target="#b52">[53]</ref>. Specifically, we adopt a dense sampling strategy to select the interest regions from which SIFT descriptors are extracted.</p><p>• K-NN <ref type="bibr" target="#b3">[4]</ref>: A feature matching approach to return the K nearest neighbors with respect to the query landmark photo where a query photo and photos in database are represented by aggregating a set of low-level features to perform landmark retrieval. • LF+SVM: Low-level features <ref type="bibr" target="#b6">[7]</ref> combined with SVM.</p><p>• DRLR <ref type="bibr" target="#b1">[2]</ref>: A region based location recognition method that detects discriminative regions at the patch-level.</p><p>• GIANT <ref type="bibr" target="#b2">[3]</ref>: A method to discover geo-informative attributes that are discriminative for location recognition.</p><p>• AQE <ref type="bibr" target="#b14">[15]</ref>: Average Query Expansion method that proceeds as follows: given a query region, it ranks a list of photos using tf-idf scores. Bag-of-Word vectors corresponding to these regions are averaged with BoW vectors of the query, resulting in an expanded vector used to requery the database.</p><p>• DQE <ref type="bibr" target="#b15">[16]</ref>: Discriminative Query Expansion that enriches a query in the exactly same way as AQE. It considers photos with lower tf-idf scores as negative data to train a linear SVM for further rankings and retrievals.</p><p>• PQE <ref type="bibr" target="#b16">[17]</ref>: A Pattern based Query Expansion algorithm that combines top-K retrieved photos with a query to find a set of patterns.</p><p>• PAMQE <ref type="bibr" target="#b0">[1]</ref>: A query expansion method by yielding a mid-level pattern representation over expanded multiquery set via LDA model against the user-photo matrix.</p><p>To ensure its generalization where test data differs from the dataset used to generate the quantization, we use a different dataset, the Oxford dataset<ref type="foot" target="#foot_0">3</ref> , to construct the vocabulary book from which local bag-of-words are encoded as representations <ref type="foot" target="#foot_1">4</ref> .</p><p>• MMHG <ref type="bibr" target="#b7">[8]</ref>: This is an effective modeling scheme to characterize the associations between landmark photos by using multiple hypergraphs to describe high-order relationship from different aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Tuning Parameters</head><p>In experiments, we tune three parameters as follows,</p><p>• K: the number of queries after matrix factorization in the multi-query set.  Following <ref type="bibr" target="#b0">[1]</ref>, we set K=40, λ=0.4 and L= 64 for a fair comparison. We set s=20 to select top 20 photos, which characterize the 20 most similar items to the query landmark described by the C-CNN high-level features, to construct a compact multi-query set for landmark retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparison with Baselines</head><p>This experiment is conducted to evaluate the effectiveness of our method in capturing the high-level discriminative representations for landmark retrieval by comparison with strong deep baselines. We use CNN pre-trained on ImageNet with the hope to reduce the chance of over-fitting to certain scenes. For the efficiency of the feature extraction process, we use Caffe library <ref type="bibr" target="#b53">[54]</ref>. For the fine-tuning process, we take all images in the training set for each category as the input, and use the CNN to perform a prediction for each image. Please note that for the proposed C-CNN, training images are clustered into pseudo labels. The fully-connected layer generates a feature vector of 4096-dim which can be used to train one-vs-all SVMs for each scene category. In testing, an input image goes through the network model corresponding to each baseline, and its deep features are used to predict its scene classification for each linear SVM model, then we assign the one with highest confidence score.</p><p>In Table <ref type="table" target="#tab_3">IV</ref>, we report classification accuracies of baselines on the testing set of Flickr and Picasa datasets. We can see that C-CNN model outperforms CNN-F and its variants, i.e., CNN-F+Average and CNN-F+FV. This verifies that fine-tuning CNNs on pseudo labels with user information is helpful in constructing multi-query set from which more discriminative features can be generated. This is mainly because the latent factors revealed by user-landmark matrix factorization provide semantics to describe landmark photos from different perspectives. On the other hand, the proposed C-CNN with Fisher Vector pooling is superior to state-of-the-art deep baselines, Deep Places Features and MetaObject-CNN. The main reason is deep places features are trained from a very large scenecentric dataset which contains photos from search engines with wide-ranging diversity and density. However, these trainable parameters cannot be easily applied into social media data without fine-tuning. MetaObject-CNN uses a region proposal technique to generate a set of patches potentially containing objects, and apply a pre-trained CNN to extract generic deep features from these patches. Consequently, the MetaObject-CNN is inferior to our model not only because its lack of fine-tuning but also because the region proposal step has no guarantee to generate highly discriminative patches in describing landmarks. By contrast, the proposed C-CNN with Fisher Vector with a multi-query set as input can produce discriminative representations by encoding multiple deep features in their higher order.</p><p>An illustration of feature responses from C-CNN is shown in Fig. <ref type="figure" target="#fig_5">6</ref>. We can see that C-CNN is able to use the most informative region as the highest response during prediction. For example, for London clock tower, highest responses come from tower pin and circular clock plate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Cross Dataset Generalization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. CNN Feature Training</head><p>Considering the smaller size of our training dataset when compared with ILSVRC-2012, we controlled the over-fitting by using lower initial learning rates for the fine-tuned hidden   layers (FC1 and FC2). The learning rate schedule for the last layer and hidden layers was: (10 -1 , 10 -4 ) → (10 -3 , 10 -4 ) → (10 -4 , 10 -4 ) →(10 -5 , 10 -5 ). Our network has the same dimensionality of the last hidden layer (FC2): 4096. This design choice is in accordance with state-of-the-art architecture <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b39">[40]</ref>. We further train three modifications of the CNN-F network, with lower dimensional FC2 layers of 2048, 1024, and 128 dimensions, respectively. To speed up training, all layers aside from FC2/FC3 were set to the those of the CNN-F net and a lower initial learning rate of 10 -3 was used. The initial learning rate of FC2/FC3 was set to 10 -2 . Experimental results are given in Table <ref type="table" target="#tab_4">V</ref>. It can be seen that mAP values of two datasets decrease as the dimensionality of the last hidden layer becomes lower. One possible reason is 4096 is already rather compact, and further reduction on dimensionality would lead to under-performed results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Comparison with State-of-the-art Approaches</head><p>We evaluate mAP for our approach and competitors. For each landmark in Flickr and Picasa Web Album, we randomly sample 20 query photos for each landmark and perform landmark retrieval to return the ranking list, where we obtain the precision-recall value and plot a precision-recall curve, plotting precision p(r) versus the recall r in Fig. <ref type="figure">8</ref>, where our method outperforms others especially over our recent PAMQE <ref type="bibr" target="#b0">[1]</ref> to demonstrate the strength of high-level deep features learned over user-landmark latent subspace. For each dataset, we also use mAP as the evaluation metric to examine the performance of each approach. In the case of duplicate landmark photos, the evaluation protocol of mean average precision (mAP) considers that a duplicate photo to the query is viewed as ''junk" <ref type="bibr" target="#b54">[55]</ref>. This protocol is a common setting Fig. <ref type="figure">8</ref>: Precision-recall results over Flickr and Picasa. in object retrieval <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>, and won't affect the retrieval accuracy values.</p><p>For a query q, the average precision (AP) is defined as AP (q) = 1 Lq n r=1 P q (r)θ q (r), where L q is the ground-truth neighbors of query q in database, n is the number of photos in ranked list, we set n = 100, P q (r) denotes the precision of the top r retrieved photos, and θ q (r) = 1 if the r-th retrieved photo is a ground-truth within this landmark category and θ q (r) = 0, otherwise. In our case, given 20 query photos for each landmark, the mAP is defined over all 20 queries for each landmark as: mAP = 1 20 20 i=1 AP (q i ). In the step of multi-query expansions, by default, the multi-query set Q are composed of 40 recommended photos.</p><p>The compared landmark retrieval results with mAP values are shown in Table <ref type="table" target="#tab_2">III</ref> and Table <ref type="table" target="#tab_5">VI</ref>. We observe that: (1) Our method and PAMQE outperform all competitors, due to the effectiveness of exploiting the complementary information of multiple queries, while our method is better than PAMQE due to the superiority of high-level features over mid-level pattern representation; (2) The large intra-class variance limits the performance of LF+SVM and K-NN, especially for the Flickr dataset; (3) DRLR detects discriminative regions from a single query photo, which degrades its performance when a query photo was shot from a bad viewpoint; and (4) MMHG works by developing multiple hypergraphs to identify the associations between landmark photos. However, they are still using low-level features such as color moments and local binary patterns, making them underperformed. Although GIANT also performs better, it only utilizes user-generated content to obtain visual attributes. This also demonstrates the need of exploiting user information to assist query understanding. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Comparison with Query Expansion Approaches</head><p>Unlike AQE and DQE where plausible queries are selected via a low-level feature matching procedure, the proposed multi-query selection can complement a q-user by exploiting latent topics and information from users. While PQE uses similar scheme to expand a single query into a query set, its multiquery set is determined by manual selection. To demonstrate the superiority of our method over existing multiple query methods that can be adopted for landmark retrieval, we perform comparison against several query expansion approaches: AQE, DQE, PQE and PAMQE. Results are shown in Fig. <ref type="figure" target="#fig_7">9</ref>, where we conclude that PAMQE outperforms AQE, DQE, and PQE by a large margin in terms of mAP values in two databases. However, it performs inferior to the proposed C-CNN with Fisher vector encoding, which can produce highly discriminative features for landmark retrieval. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper, we propose a novel collaborative deep networks for robust landmark retrieval, which works over landmark latent factors to further generate the high-level semantic feature for both multi-query set and other landmark photos. Compared with both low-level feature and mid-level pattern representation based methods, our proposed method achieved state-of-the-art performance, validated by experimental results on real-world social landmark photo datasets associated with the user information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3</head><label>3</label><figDesc>depicts a graphical model for this representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Fig.4: Given a multi-query set Q in testing, fine-tuned parameter set of CNN-F are applied to feed-forwardly extract high-level discriminative features for each photo in Q. The resulting multiple feature vectors are encoded using Fisher vector encoding to generate an overall feature vector which can best describe the landmark of interest in a robust and discriminative way.TABLE I:The CNN-F architecture. It contains 5 convolutional layers (conv 1-5) and three fully-connected layers (FC 1-3). The details of each convolutional layer are given in three sub-rows: the first specifies the number of convolution filters and their receptive field size as "num × size × size"; the second indicates the convolution stride ("st.") and spatial padding ("pad"); the third indicates if Local Response Normalisation (LRN) is applied, and the max-pooling downsampling factor.</figDesc><graphic coords="7,121.77,56.07,368.49,56.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 .</head><label>2</label><figDesc>Example photos containing a variety of landmarks from this dataset are shown in Fig. 5. • Picasa Web Album. Providing rich information about interesting tourist attractions, this source contains a vast 2 In Paris, 12 queries were used to collect photos from Flickr: La Defense Paris, Eiffel Tower Paris, Hotel des Invalides Paris, Louvre Paris, Moulin Rouge Paris, Musee d'Orsay Paris, Notre Dame Paris, Pantheon Paris, Pompidou Paris, Sacre Coeur Paris, Arc de Triomphe Paris, Paris</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Examples of landmark photos from our collected Flickr dataset.</figDesc><graphic coords="7,341.20,365.05,192.62,99.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>• λ: the threshold to determine the latent topic set w.r.t the query landmark.• L: the reduced dimension or number of latent factor in matrix factorization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: An example consists of the original test photo (left) and the corresponding heatmap represented the highest response regions during the prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Cross dataset generalization of training and testing on different datasets. See text for details.</figDesc><graphic coords="9,318.14,280.01,113.38,114.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Comparison of query expansion methods over mAPs.</figDesc><graphic coords="11,75.28,382.31,198.42,85.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,135.92,56.07,340.16,216.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Statistics of datasets.</figDesc><table><row><cell>Dataset</cell><cell>Flickr</cell><cell>Picasa Web Album</cell></row><row><cell># Landmark</cell><cell>55</cell><cell>16</cell></row><row><cell cols="2"># photo per landmark nearly 1,000</cell><cell>100 ∼ 300</cell></row><row><cell># Total photo</cell><cell>49,840</cell><cell>4,100</cell></row><row><cell># User</cell><cell>7,332</cell><cell>577</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>The mAP (%) values of selected landmarks by different approaches against Flickr dataset.</figDesc><table><row><cell>Method</cell><cell cols="7">London Triumphal Sagrada Sydney Singapore Temple bridge arch familia bridge city heaven skytree Tokyo</cell><cell cols="4">Taipei Cairo Brooklyn Maiden's Average mAP 101 tower bridge tower</cell></row><row><cell>K-NN [4]</cell><cell>46.75</cell><cell>32.03</cell><cell>33.55</cell><cell>38.93</cell><cell>29.83</cell><cell>54.33</cell><cell>17.21</cell><cell>32.39 44.37</cell><cell>22.56</cell><cell>64.58</cell><cell>38.78</cell></row><row><cell>LF+SVM [7]</cell><cell>40.52</cell><cell>52.35</cell><cell>45.78</cell><cell>36.09</cell><cell>29.17</cell><cell>61.23</cell><cell>38.96</cell><cell>15.99 34.55</cell><cell>33.58</cell><cell>62.33</cell><cell>40.96</cell></row><row><cell>DRLR [2]</cell><cell>52.45</cell><cell>35.65</cell><cell>51.48</cell><cell>34.58</cell><cell>40.33</cell><cell>62.45</cell><cell>42.33</cell><cell>40.24 44.58</cell><cell>38.34</cell><cell>57.35</cell><cell>45.52</cell></row><row><cell>GIANT [3]</cell><cell>53.15</cell><cell>42.44</cell><cell>50.89</cell><cell>39.15</cell><cell>42.36</cell><cell>64.37</cell><cell>46.73</cell><cell>43.59 47.21</cell><cell>37.82</cell><cell>64.58</cell><cell>49.29</cell></row><row><cell>PAMQE [1]</cell><cell>57.83</cell><cell>64.76</cell><cell>57.33</cell><cell>45.59</cell><cell>63.48</cell><cell>68.38</cell><cell>60.48</cell><cell>59.32 70.04</cell><cell>54.58</cell><cell>72.38</cell><cell>61.29</cell></row><row><cell>MMHG [8]</cell><cell>49.44</cell><cell>41.91</cell><cell>47.15</cell><cell>40.91</cell><cell>41.02</cell><cell>60.94</cell><cell>49.27</cell><cell>41.37 44.29</cell><cell>39.84</cell><cell>60.67</cell><cell>46.31</cell></row><row><cell>Ours</cell><cell>60.25</cell><cell>67.42</cell><cell>59.64</cell><cell>49.22</cell><cell>65.77</cell><cell>70.14</cell><cell>62.25</cell><cell>61.25 71.25</cell><cell>58.77</cell><cell>74.05</cell><cell>63.64</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>The classification accuracies/precision by different baselines against Flickr and Picasa Web Album datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ours</cell></row><row><cell>Flickr</cell><cell>54.32±0.14</cell><cell>58.42±0.76</cell><cell>58.61±0.78</cell><cell>60.68±0.92</cell><cell>59.29±0.33</cell><cell>64.18±0.88</cell></row><row><cell>Picasa</cell><cell>56.79±0.37</cell><cell>57.22±0.92</cell><cell>56.59±0.85</cell><cell>57.79±0.99</cell><cell>57.18±0.31</cell><cell>61.23±0.76</cell></row></table><note><p><p><p><p>Landmark CNN-F+Average CNN-F+FV C-CNN+Average Deep Places Features</p><ref type="bibr" target="#b10">[11]</ref> </p>MetaObject-CNN</p><ref type="bibr" target="#b11">[12]</ref> </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V :</head><label>V</label><figDesc>mAP of two datasets with respect to modified architecture of CNN-F feature learning.</figDesc><table><row><cell></cell><cell>Flickr (mAP)</cell><cell>Picasa (mAP)</cell></row><row><cell>CNN-F (4096)</cell><cell>63.64</cell><cell>63.62</cell></row><row><cell>CNN-F (2048)</cell><cell>63.07</cell><cell>63.14</cell></row><row><cell>CNN-F (1024)</cell><cell>62.45</cell><cell>61.95</cell></row><row><cell>CNN-F(128)</cell><cell>61.21</cell><cell>60.82</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI :</head><label>VI</label><figDesc>The mAP values (%) of selected landmarks by different approaches against Picasa Web Album dataset.Landmark K-NN [4] LF+SVM [7] DRLR [2] GIANT [3] PAMQE [1] Ours</figDesc><table><row><cell>London clock</cell><cell>58.37</cell><cell>50.03</cell><cell>60.65</cell><cell>62.20</cell><cell>71.89</cell><cell>74.05</cell></row><row><cell>Eiffel tower</cell><cell>51.24</cell><cell>51.05</cell><cell>46.77</cell><cell>52.13</cell><cell>61.71</cell><cell>65.43</cell></row><row><cell>Forbidden city</cell><cell>44.04</cell><cell>55.28</cell><cell>58.57</cell><cell>58.94</cell><cell>70.68</cell><cell>72.74</cell></row><row><cell>Opera house</cell><cell>49.26</cell><cell>33.48</cell><cell>46.88</cell><cell>48.48</cell><cell>61.74</cell><cell>65.87</cell></row><row><cell>Catalunya plaza</cell><cell>24.87</cell><cell>12.37</cell><cell>21.67</cell><cell>23.23</cell><cell>42.83</cell><cell>48.24</cell></row><row><cell>Chicago plaza</cell><cell>35.48</cell><cell>38.38</cell><cell>33.76</cell><cell>38.74</cell><cell>50.84</cell><cell>55.39</cell></row><row><cell>Average mAP</cell><cell>43.87</cell><cell>40.09</cell><cell>44.73</cell><cell>47.28</cell><cell>59.87</cell><cell>63.62</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>http://www.robots.ox.ac.uk/vgg/data/oxbuildings/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>The Oxford Building dataset contains</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>5,062 photos, which is a standard set of particular objects for retrieval. The reason for choosing Oxford landmarks is that the photos exhibit scenes similar to, rather than identical landmarks, those in the two test databases (e.g., buildings have much similarities in their architectural styles).</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Effective multi-query expansions: Robust landmark retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="79" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What makes paris look like paris?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Giant: Geo-informative attributes for location recognition and exploration</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="13" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">im2gps: estimating geographic information from a single image</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Collaborative filtering beyond the user-item matrix: A survey of the state of the art and future challenges</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Near-duplicate keyframe retrieval by nonrigid image matching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="41" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Content-based visual landmark search via multimodal hypergraph learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2756" to="2769" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Krimp: mining itemsets that compress</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vreeken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Siebes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Discov</title>
		<imprint>
			<biblScope unit="page" from="169" to="214" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The minimum description length principle</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Grunwald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>The MIT press</publisher>
			<pubPlace>Cambridge, Massachusetts, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning deep features for scence recognition using places database</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Harvesting discriminative meta objects with deep cnn features for scence classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploiting attribute correlations: A novel trace lasso-based weakly supervised dictionary learning method</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Weakly-supervised cross-domain dictionary learning for visual recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="42" to="59" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Total recall: automatic query expansion with a generative feature model for object retrieval</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Three things everyone should know to improve object retrieval</title>
		<author>
			<persName><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2911" to="2918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mining multiple queries for image retrieval: on-the-fly learning of an object-specific mid-level representation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2544" to="2551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Landmark classification with hierarchical multi-modal exemplar feature</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="981" to="993" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generating diverse and representative image search results for landmarks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW 08</title>
		<meeting>WWW 08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="297" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Geo-location estimation of flickr images: Social web based enrichment</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hauff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Houben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECIR 12</title>
		<meeting>ECIR 12</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="85" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Web-a-where: geotagging web content</title>
		<author>
			<persName><forename type="first">E</forename><surname>Amitay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Soffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="273" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Geotagging in multimedia and computer vision-a survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gallagher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Multimedia Tools and Applications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mining interesting locations and travel sequences from gps trajectories</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of World Wide Web</title>
		<meeting>World Wide Web</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="791" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Relevance ranking in georeferenced video search</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="105" to="125" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Geo-location estimation from two shadow trajectories</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="585" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image sequence geolocation with human travel priors</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vesselova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV 09</title>
		<meeting>ICCV 09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="253" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Location discriminative vocabulary coding for mobile landmark search</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="290" to="314" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning deep and wide: A spectral method for learning deep networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2303" to="2308" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Feature learning for image classification via multiobjective genetic programming</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1359" to="1371" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pcanet: A simple deep learning baseline for image classification</title>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5017" to="5032" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust subspace clustering for multi-view data by exploiting correlation consensus</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3939" to="3949" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised metric fusion over multiview data by graph random walk-based crossview diffusion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="70" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scene classification via plsa</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Muoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descen</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Statistics</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="177" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.1531</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0575</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>in arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep learning using linear support vector machines</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Representational Learning, ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Image classification with the fisher vector: theory and practice</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="222" to="245" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep fisher kernels -end to end learning of the fisher kernel gmm parameters</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sydorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sakurada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fisher vectors meet neural networks: A hybrid classification architecture</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Exploiting generative models in discriminative classifiers</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">The devil is in the details: an evaluation of recent feature encoding methods</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep fisher networks for large-scale image classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The yahoo! music dataset and kdd-cup</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Koenigstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM KDD CUP</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning mid-level features for recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2559" to="2566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Caffe: convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Objective retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Xuemin has published more than 260 research papers, with most of which published at major venues. Prof Lin has been the steering committee member and conference chairs for a lot of data science conferences</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><surname>Tcyb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Acm</forename><surname>Multimedia</surname></persName>
		</author>
		<author>
			<persName><surname>Sigir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ieee</forename><surname>Ijcai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Acm</forename><surname>Icdm</surname></persName>
		</author>
		<author>
			<persName><surname>Cikm Etc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Xuemin is selected to be an IEEE Fellow in 2015, and be appointed as the Editor-in-Chief of IEEE Transactions on Knowledge and Data Engineering</title>
		<meeting><address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<publisher>UNSW</publisher>
			<date type="published" when="2014">2015. Jan 2017. 2014</date>
		</imprint>
		<respStmt>
			<orgName>University of New South Wales ; East China Normal University ; Australian Research Council (ARC) and National Science Foundation of China (NSFC ; Lin Wu received the PhD degree from University of New South Wales ; The University of Adelaide, and currently a research fellow at Institute of Social Science Research and ITEE at the University of Queensland, Australia</orgName>
		</respStmt>
	</monogr>
	<note>Australian Centre for Robotic vision. Lin has published 35 research papers in the major venues, including CVPR, ACM MM, ACM SIGIR, IJCAI, IEEE TIP</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Wenjie Zhang is a currently a senior lecturer in UNSW, Australia. She has won the Australian Research Council Discovery Early Career Researcher Award (DECRA). So far she has published more than 90 papers in the data science era</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
