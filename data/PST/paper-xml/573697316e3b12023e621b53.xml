<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Optimal Cache Partition-Sharing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jacob</forename><surname>Brock</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chencheng</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chen</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yechen</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaolin</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yingwei</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Rochester Rochester</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Optimal Cache Partition-Sharing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ICPP.2015.84</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>When a cache is shared by multiple cores, its space may be allocated either by sharing, partitioning, or both. We call the last case partition-sharing. This paper studies partition-sharing as a general solution, and presents a theory an technique for optimizing partition-sharing. We present a theory and a technique to optimize partition sharing. The theory shows that the problem of partition-sharing is reducible to the problem of partitioning. The technique uses dynamic programming to optimize partitioning for overall miss ratio, and for two different kinds of fairness.</p><p>Finally, the paper evaluates the effect of optimal cache sharing and compares it with conventional solutions for thousands of 4-program co-run groups, with nearly 180 million different ways to share the cache by each co-run group. Optimal partition-sharing is on average 26% better than freefor-all sharing, and 98% better than equal partitioning. We also demonstrate the trade-off between optimal partitioning and fair partitioning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>There are two popular options for sharing an individual resource such as cache: partitioning and sharing. In partitioning, each core gets a protected portion of the total cache. In sharing, each core has access to the whole cache, and may evict data belonging to another core. Partitioning provides each program protection against aggressive partners, while sharing prevents resources from going unused when they may be needed 2 . If there are only two programs, they may either share or partition any given resource. However, for three or more programs, partitioning and sharing may both be used. For example, two programs may share a partition, while the third program has its own partition. We propose to call this type of scheme partition-sharing. Strict partitioning * The research is supported in part by the National Science Foundation (Contract No. CNS-1319617, CCF-1116104, CCF-0963759); IBM CAS Faculty Fellow program; the National Science Foundation of China (Contract No. 61232008, 61272158, 61328201, 61472008 and 61170055); the 863 Program of China under Grant No.2012AA010905, 2015AA015305; and a grant from Huawei. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding organizations. 1 Chencheng Ye is a visiting student, funded by the Chinese Scholarship Council. His home institution is Huazhong University of Science and Technology, Wuhan, China. 2 Parihar et al. presented a counter-based hardware mechanism to provide programs the protection of partitioning, without the risk of unused space <ref type="bibr" target="#b3">[4]</ref>. and free-for-all sharing can be seen as opposite edge cases of partition-sharing. In this paper, we investigate partitionsharing as a general problem and give an optimal solution for both throughput and fairness.</p><p>The new solution has two components. First, we reduce the problem of partition-sharing to just partitioning, i.e., the optimal solution for partitioning is at least as good as the optimal solution for partition-sharing. The essence of the reduction is the concept of a Natural Cache Partition. Given a set of programs sharing a cache, the natural partition is a cache partition such that each program has the same miss ratio in its natural partition as it has in the shared cache. The performance of any shared cache is then equivalent to the performance of naturally partitioned cache. The general problem of finding the best partition-sharing is thus reduced to finding the best partitioning. We give the formal derivation including the precise conditions for optimality.</p><p>The second component of the solution is an optimization algorithm to find the best cache partitioning. The previous solution by <ref type="bibr">Stone et al.</ref> was designed to maximize performance (minimal group miss ratio) and required the assumption that the individual miss ratio curve be convex <ref type="bibr" target="#b8">[9]</ref>. Our algorithm uses dynamic programming to examine the entire solution space. It generalizes the optimization in two ways. First, the miss ratio curve of individual programs does not have to be convex. In fact, it can be any function. Second, it can optimize for any objective function, for example, fairness and quality of service (QoS) in addition to throughput.</p><p>For fairness, we define two types of baseline optimization which increase the group performance if no program would perform worse than it is with a baseline partition. We consider two example baselines: the equal partition and the natural partition (i.e., free-for-all sharing).</p><p>The main contributions of the paper are:</p><p>• Cache partition-sharing, for which the existing problems of cache partitioning and sharing are special cases. • Natural cache partition, which gives a theoretical reduction to make the optimization problem solvable. • Optimal partitioning algorithm, which handles all types of programs and objective functions. A set of program traces that will benefit from partition sharing. Core 1 and core 2 run streaming programs, so partitioning them off prevents them from polluting the cache. Cores 3 and 4 alternate between having large and small working sets. Sharing a partition allows them each to use more cache when it is needed. Capacity misses are shaded with gray.</p><p>• Evaluation, which shows the effect of optimization for a set of benchmark programs. The new solution is useful for a programmer and a computer architect to understand and solve the optimization problem. It fully evaluates the potential of general cache sharing, for both performance and fairness and over existing, narrower solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PARTITION-SHARING OPTIONS FOR MULTICORE CACHE</head><p>The general need for partition sharing can be demonstrated with a simple example trace, with 4 cores sharing a cache size of 6. If cores 1 and 2 are running streaming applications, and cores 3 and 4 alternate between large and small working sets (i.e., core 3 needs cache, then core 4 needs cache), cores 1 and 2 should be partitioned to prevent cache pollution, while cores 3 and 4 should share, so that each may use the shared partition while the other does not. The example is illustrated in Figure <ref type="figure">1</ref>.</p><p>In order to understand the complexity of decision making for the partition-sharing, we can divide the problem into 3 sub-problems, illustrated in Figure <ref type="figure" target="#fig_1">2</ref> and described below. In the following descriptions, n pr is the number of programs, n pa is the number of partitions, n c is the number  of caches, C is the size of each cache, n g is the number of groups, and G is the size of each group. The number of possible arrangements (within the constraints) is called the search space size (S). The search space sizes shown below count every possible unique arrangement of programs in caches/partitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Sharing, Multiple Caches:</head><p>There are multiple caches, but the number of users for each cache may vary. Grouping is still the only variable, but now group sizes are not required to be the same.</p><p>The problem size here is the number of ways we can separate n pr programs into n c non-empty groups. This happens to be the well-known Stirling number of the second kind:</p><formula xml:id="formula_0">S 1 = n pr n c . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>2. Partition-Sharing, Single Cache: There is only one cache, which is partitioned. Groups of programs are assigned to partitions. The problem size is the sum of the Stirling numbers multiplied by the number of ways to assign "walls" that partition the cache. That is, for each number of partitions n pa , there are npr npa ways to group the programs, and for each of those, there are C+npa−1 npa−1 ways of assigning cache (balls) to the partitions (bins).</p><formula xml:id="formula_2">S 2 = npr npa=1 n pr n pa C + n pa − 1 n pa − 1 (2)</formula><p>3. Partitioning Only: When partitioning alone is used, the number of partitions equals the number of programs. This problem is simply to assign units of cache (balls) to each program (bins). The problem size is</p><formula xml:id="formula_3">S 3 = C + n pr − 1 n pr − 1 . (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>As an example, for 4 programs run on an 8MB cache that is divisible into 64B units, we have n pr = 4 and C = 8MB/64B = 131072. In this case, S 2 = 375, 368, 690, 761, 743 and S 3 = 375, 317, 149, 057, 025. In other words, the solution set of partitioning-only covers 99.99% of the solution set of partition-sharing. This is due to the fact that there are so many more ways to assign cache to 4 partitions than there are ways to assign it to 1, 2, or 3 partitions. Fortunately, we have an algorithm (presented in Section V-B) to find the optimal solution among those 99.99%. We expect the solution in this space to be approach the performance of the optimal partition-sharing solution, particularly for programs without strong phase behavior, and for higher partitioning granularity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE HIGHER ORDER THEORY OF LOCALITY</head><p>Relationships between several data locality metrics were developed by Xiang et al. <ref type="bibr" target="#b15">[16]</ref> in a theory called the higher order theory of locality (HOTL). The metrics gives preliminary definitions, as well as the metrics and their relationships.</p><p>Window: A window beginning at the i th element (inclusive) of the trace and continuing for k elements is denoted window (i, k). The windows <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b5">6)</ref> and (4, 2) are boxed in Figure <ref type="figure" target="#fig_3">3</ref>.</p><p>Reuse Pair: A pair of accesses to the same datum, with no other accesses to that datum in-between. Two reuse pairs are highlighted in Figure <ref type="figure" target="#fig_3">3</ref>.</p><p>Reuse Window: The smallest window containing both members of a reuse pair. Two reuse windows are boxed in Figure <ref type="figure" target="#fig_3">3</ref>.</p><p>Reuse Time: The reuse time for a reuse pair is the length of their reuse window. E.g., for the i th and j th elements of a trace d</p><formula xml:id="formula_5">1 ...d n , rt(d i , d j ) = j − i + 1.<label>(4)</label></formula><p>It is calculated at the second access of the pair, as shown in Figure <ref type="figure" target="#fig_3">3</ref>. The reuse time histogram for a trace can be expressed as a function freq(rt), denoting the number of reuse pairs with reuse time rt.</p><p>Footprint: There are two formulations of the footprint function: WSS(W ) is the number of distinct data accessed in a trace window W , and fp(w) is the average of WSS(W ) for all windows W of length w in a given memory trace. The average footprint function can be approximated from a histogram of reuse times.</p><formula xml:id="formula_6">fp(w) ≡ 1 n − w + 1 n−w+1 i=1 WSS(i, w) (5)</formula><p>For the rest of the paper, we will only refer to the average footprint function.  Fill Time: The average footprint fp(c) is the average number of distinct data in a window of length c (we use c now because, as will soon be clear, it represents a cache size). The fill time is defined as the expected number of accesses it takes to touch a given amount of distinct data, so an equivalent statement is that ft(fp(c)) = c. So we have the relationship:</p><formula xml:id="formula_7">ft(c) = fp −1 (c). (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>Inter-Miss Time: The fill time for a cache of size c + 1 is the fill time for a cache of size c plus the average time until the next miss on the size c cache.</p><formula xml:id="formula_9">im(c) = ft(c + 1) − ft(c) (7)</formula><p>Miss Ratio: The miss ratio is the reciprocal of the intermiss time.</p><formula xml:id="formula_10">mr(c) = 1 im(c)<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. MISS RATIO COMPOSITION THEORY</head><p>It is necessary to have a theory that can predict the miss ratios of co-run programs using metrics measured only on each program. For a scheduling problem with 20 programs that need to be scheduled on 2 processors sharing a cache, we would like to be able to predict cache performance based on 20 metrics, not 20-choose-2 (and for 4 processors, this becomes 20-choose-4, and so on). In addition to scheduling, another possible application of this would be to monitor performance on-line, and stall individual programs based on the predicted benefit of doing so. For example, if two programs are traversing different 60MB arrays while sharing a 64MB cache, stalling one of them will prevent thrashing, and they may both finish sooner this way.</p><p>As with the prior footprint composition method, we begin with the following equation for fp(w), the average footprint of a window of length w, given the access rates ar 1 and ar 2 of each program<ref type="foot" target="#foot_1">3</ref> :</p><p>Stretched Footprint: When the memory accesses of two programs interleave, the result is a single trace of memory accesses. Taken as a part of this whole, each program's footprint function is horizontally stretched based on the ratio of its accesses to the other programs' accesses in any given amount of time (i.e., ai a1+a1 ). The overall footprint is then the sum of the individual stretched footprints. For two programs, this is</p><formula xml:id="formula_11">fp(w, ar 1 , ar 2 ) =fp 1 w * ar 1 ar 1 + ar 2 + fp 2 w * ar 2 ar 1 + ar 2 . (<label>9</label></formula><formula xml:id="formula_12">)</formula><p>To reiterate, this equation states that in an interleaved memory access trace from two non-data-sharing programs, the expected number of distinct memory addresses accessed in w total accesses is the sum of the expected number of distinct memory addresses accessed by each program. The latter is calculated given each of their independently measured average footprint functions and the number of total accesses belonging to each, based on their access rates (w * ar1 ar1+ar2 and w * ar2 ar1+ar2 ). What we are most interested in, the miss ratio, can be derived from Equations the miss ratio can be calculated using Equations 6, 7 and 8 <ref type="bibr" target="#b15">[16]</ref>:</p><formula xml:id="formula_13">mr(c) = fp(w + 1) − c, (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>where w is chosen so that fp(w) = c. In the scope of co-run programs (using Equation <ref type="formula" target="#formula_11">9</ref>), we rewrite this as mr(c, ar 1 , ar 2 ) =fp 1 (w + 1) * ar 1 ar 1 + ar 2</p><formula xml:id="formula_15">+ fp 2 (w + 1) * ar 2 ar 1 + ar 2 − c, (<label>11</label></formula><formula xml:id="formula_16">)</formula><p>where w is still chosen so that fp(w) = fp 1 (w * ar1 ar1+ar2 ) + fp 2 (w * ar2 ar1+ar2 ) = c. Since the footprint function is monotonic, the appropriate w can be found in O(log(c)) time using binary search. If both programs always had the same miss ratio, then the above equation would be a sufficient prediction of the miss ratio. However, since both programs' access rates vary with time, and we cannot predict what they will be at any given moment 4 , we must treat the access rates as independent random variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CACHE PARTITIONING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Natural Cache Partition</head><p>At a given time, each program sharing a cache will have some quantity of data in the cache. This is called the program's cache occupancy, or c i for program i. In a warm cache, the sum of the cache occupancies equals the size of the cache (i.e., the cache is full). The footprint metric gives us a way to predict the cache occupancies of each program for a cache in a steady state; We call the ordered set of 4 There may be some feedback between the two access rates via misses and cache stalls, but we leave this to future work. The overall footprint and the stretched footprints of a set of co-run programs can be used to determine the NCP. Of course, the individual stretched footprints always add up to the overall footprint. As illustrated in Figure <ref type="figure" target="#fig_4">4</ref>, when the overall footprint equals the cache size, each individual footprint indicates that program's cache occupancy.</p><p>For programs with no phase behavior, one might expect that a shared cache would provide the same performance as a "naturally" partitioned cache. We call this notion the Natural Partition Assumption (NPA). Since the optimal partition is at least as good as the natural partition (by the definition of "optimal"), insofar as it is correct, the NPA indicates that the optimal cache partition is at least as good as sharing.</p><p>If the NPA holds, then every partition-sharing scheme corresponds to some partitioning scheme. And that partitioning scheme may not be the optimal one. Therefore, the optimal partition scheme offers an upper-bound on the partitionsharing options. This implies that if NPA is true, then we should always partition when possible. If not, then partitionsharing may be justifiable, as in the example in Figure <ref type="figure">1</ref>.</p><p>The natural partition is derived using the HOTL theory described in Section III. NPA holds if and only if the HOTL prediction of the miss ratio is accurate. Previous studies have shown that the prediction was accurate as compared to the measurement from simulation and hardware performance counters. We further discuss the validation of the NPA in Section VII-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Optimal Partitioning</head><p>The idea of memory or cache partitioning has been around for a while. In 1992, Stone, et al. proposed cache partitioning as an improvement over sharing with LRU both for single-threaded programs by partitioning the cache between instructions and data, and for multiprogramming by giving each process a partition <ref type="bibr" target="#b8">[9]</ref>.</p><p>In the way of optimizing their algorithm, they proved that the average miss ratio for two or more uniformly interleaved processes is minimized when the derivative (with respect to partition size) of the miss ratio function for each process is equal. The proof, for only two programs, is straightforward. The total number of misses in some interval T is</p><formula xml:id="formula_17">1 2 mr 1 (c 1 ) + 1 2 mr 2 (C − c 1 ) * T. (<label>12</label></formula><formula xml:id="formula_18">)</formula><p>Assuming that each miss ratio function is convex and decreasing 5 , setting the derivative of this function, with respect to program 1's cache size, equal to zero gives</p><formula xml:id="formula_19">dmr 1 (c 1 ) dc 1 = − dmr 1 (C − c 1 ) dc 1 = dmr 2 (c 2 ) dc 2 c2=C−c1 . (<label>13</label></formula><formula xml:id="formula_20">)</formula><p>And it is easy to show for the more general case where program i represents some fraction f i of the trace, that the optimal partitioning exists when</p><formula xml:id="formula_21">f 1 * dmr 1 (c 1 ) dc 1 = f 2 * dmr 2 (c 2 ) dc 2 c2=C−c1 . (<label>14</label></formula><formula xml:id="formula_22">)</formula><p>We call this the Stone, Thiebault, Turek, Wolf (STTW) Cache Partitioning. The miss ratio derivatives are also equal when there are more than two caches.</p><p>Optional Cache Partition Dynamic Programming Algorithm: The optimal partitioning can be determined using a dynamic programming algorithm. Given a set of programs P and a cache of size C, and letting mc i (c i ) be the miss count (that is, miss ratio times number of memory accesses) of program P i at a cache size of c i , the object is to determine the set S of partition sizes so that i mc i (c i ) is minimized, and i c i = C: S argmin</p><formula xml:id="formula_23">(c1,c2,••• ) i mc i (c i ) i c i = C (<label>15</label></formula><formula xml:id="formula_24">)</formula><p>The dynamic programming algorithm constructs S program-by-program. When each program P i is added, the new program is assigned the c i that minimizes the sum of its miss count and the miss count of the optimal partitioning of the first i − 1 programs with cache size of C − c i . Letting S k,i represent the sub-solution for the first i programs with 5 The convexity assumption is something like the law of diminishing returns; the larger the cache, the less a program benefits by adding another byte to the cache. However, miss ratio curves often have drop-offs where a particular working set fits into the cache. The imperfection of this assumption is addressed by <ref type="bibr" target="#b10">Thiébaut et al. (1992)</ref>  <ref type="bibr" target="#b10">[11]</ref>. The assumption that the miss ratio curve is non-increasing is because of the inclusion property of LRU caches. cache size k and mc(S k,i ) represent the total miss count for that sub-solution, each step of the algorithm is</p><formula xml:id="formula_25">c i = argmin ci {mc(S C−ci,i−1 ) + mc i (c i )} S C,i = S C−ci,i−1 + (c i ).<label>(16)</label></formula><p>The time-complexity of this algorithm is O(P C 2 ). The space complexity is O(P C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. BASELINE OPTIMIZATION</head><p>In this section, we consider the optimization of fair cache sharing. In order to allow some group miss rate reduction without compromising fairness, we define two different types of baseline optimization, in which the above dynamic programming algorithm can be used, but with an added constraint at each step:</p><p>Equal Baseline: The group's miss ratio is minimized under the constraint that no single program has a higher miss ratio than it would with an equal partition of the cache.</p><p>Natural Baseline: The group's miss ratio is minimized under the constraint that no single program has a higher miss ratio than it would with the natural cache partitioning.</p><p>Fairness in cache has been studied extensively. There are many definitions. One may define it by sharing incentive, i.e., how the actual performance compares to equal partition. Alternatively, one may define it by interference, i.e., how the actual performance compares to having the full cache. The difference may seem artificial as in the proverbial question whether a glass is half empty or half full. However, the interference is more complex to define, i.e., how much increase in the miss ratio is too much, and whether we count the increase or the relative increase. In this work, we define fairness by the sharing incentive and will use the optimization technique described in Section V to optimize the performance of fair cache sharing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EXPERIMENTAL DESIGN</head><p>In this section, we first describe the methods of evaluation, then evaluate the effect of optimal cache sharing, and finally discuss the issues of validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Methodology</head><p>Following the methodology of Wang et al. <ref type="bibr" target="#b11">[12]</ref>, we randomly choose 16 programs from SPEC 2006: perlbench, bzip2, mcf, zeusmp, namd, dealII, soplex, povray, hmmer, sjeng, h264ref, tonto, lbm,omnetpp, wrf, sphinx3 and use the first reference input. The selection is representative, and the number of co-run groups is not too many to visualize. We enumerate all 4-program subsets, which gives us 1820 groups. We model their co-run miss ratio in the 8MB shared cache. The shared cache is partitioned by the unit of 128 cache blocks (8KB). There are a total of 1024 units.</p><p>For each set of 4 programs, the total number of partitions is 1026   3   or ≈ 180 million (from Equation <ref type="formula" target="#formula_3">3</ref>). The complexity of dynamic programming is O(P C 2 ), where P = 4, C 2 = 1024 2 . P C 2 calculates to about 4 million. The granularity of 8KB is chosen to reduce the cost of dynamic programming, which is 128 2 = 16384 times smaller when partitioning in 8KB units than in 64-byte cache blocks.</p><p>Among the solutions of cache sharing in each group, we model the performance of six solutions:</p><p>• Equal: each program has 2MB cache.</p><p>• Natural: the performance when the four programs share the 8MB cache. • Equal Baseline: Group optimization with individual baselines set at the equal partition's performance. • Natural Baseline: Group optimization with individual baselines set at the natural partition's performance. • Optimal: the solution with the lowest group miss ratio.</p><p>• STTW: the classic solution to minimize the group miss ratio. Many studies in the past have examined some but not all solutions of cache sharing. Using the terminology of Xie and Loh, Natural is the same as the capitalist cache sharing, and Equal the same as the socialist sharing <ref type="bibr" target="#b16">[17]</ref>. The capitalist miss ratio depends on peers (market condition), but the socialist miss ratio is guaranteed at all times. In this study, we are the first to examine the entire solution space of partition sharing.</p><p>We show results for all 4-program groups. The exhaustive evaluation is important, since a random subset from these 1,840 groups can mislead since the subset result may differ significantly from other subsets and from the whole set. There is no sure way to choosing a representative subset unless we have evaluated the whole set. The exhaustive evaluation is possible because of the techniques developed in this paper.</p><p>The Implementation and the Cost of Analysis: The implementation of dynamic programming is in C++. The scripting is by Ruby. To test the speed, we use a machine with 1.7GHz Intel Core i5-3317U (11 inch MacBook Air, power cord unplugged). For all 1820 groups, it takes 4 minutes 20 seconds real time for optimal partitioning, 5 minutes 8 seconds for natural baseline optimization, and 4 minutes 3 seconds for equal baseline. For each group, the optimizer reads 4 footprints from 4 files. There are 16 footprint files for the 16 programs. The size ranges between 242KB and 375KB. The file size can be made smaller by storing in binary rather than ASCII format. The implementation has not been optimized for speed, but it is fast enough -on average it takes less than 0.21 second to optimize a program group. In comparison, it takes STTW 3 minutes 38 seconds for 1820 groups or 0.11s per group.</p><p>The footprint measurement we use is the same as the implementation by Xiang et al. <ref type="bibr" target="#b15">[16]</ref> and Wang et al. <ref type="bibr" target="#b11">[12]</ref> Xiang et al. reported on average 23 times slowdown from the full-trace footprint analysis. Wang et al. developed a sampling method called adaptive bursty footprint (ABF) profiling, which takes on average 0.09 second per program. To have reproducible results, our implementation uses the full-trace footprint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Optimization Results</head><p>The Effect of Cache Sharing -Natural vs Equal: In a co-run group, we call a program a gainer if its miss ratio in the shared cache is lower than in the equal partition, and a loser if the opposite is true. Because of the natural-partition assumption, the comparison can be made between Natural and Equal, as shown in Figure <ref type="figure">5</ref>.</p><p>In lbm and sphinx3, Natural is nearly always lower than Equal. Hence, the two programs mostly gain from sharing the cache. In perlbench and sjeng, Natural is almost never lower than Equal. The latter two programs almost never gain from sharing. All programs can have a large relative change in the miss ratio due to cache sharing, although the absolute change may be small if the miss ratio is small to start with.</p><p>Sharing has a harmonizing effect to narrow the difference between program miss ratios. The plots in Figure <ref type="figure">5</ref> are ordered by the decreasing miss ratio of Equal. A tendency for programs with a high miss ratio (whose plots are at the front of the page) to gain from sharing and programs with a low miss ratio to lose by sharing. The division line is roughly 1.35%. However, the tendency is not strict. The program perlbench always loses by sharing, but its miss ratio is higher than two programs, hmmer and tonto, where sharing is mostly beneficial.</p><p>The large variation within a plot and between plots means that it is difficult to classify programs by its behavior in the shared cache. For example, 12 of the 16 programs have mixed gains and losses due to sharing. Classification by the miss ratio is also not effective, since the relative portions of gaining and losing cases do not strictly correlate with the miss ratio. In addition, classification does not solved the problem of deciding which cases gain or lose and by how much.</p><p>Effect of Optimization: Table <ref type="table" target="#tab_1">I</ref> shows the improvement by Optimal over all the other methods. Optimal improves Natural by 26% on average. It is at least 10% better for 58% of the groups and 20% better for 45% of the groups. The improvement is greater for Equal. The average improvement in all groups is 125%, and 77% and 58% of the groups are improved by at least 10% and 20% respectively.</p><p>Unfairness of Optimization: While Optimal makes a group much better overall, it may be unfair and makes a member program worse. Consider the miss ratio of Natural or Equal as the baseline. Optimal is unfair if it increases the miss ratio of a program compared to its baseline. Individually, we see clear evidence of unfairness in Figure <ref type="figure">5</ref> -Optimal makes a program worse as often as it makes it better. This is true regardless which baseline we examine.</p><p>We may classify a program by how likely it gains or loses ground in Optimal. For example, sphinx3 is almost The miss ratio of individual programs running with different peer groups, under natural, equal, natural baseline, equal baseline, and optimal cache partition. Programs are sorted by the (constant) miss ratio in the equal partition. Under the other four schemes, the miss ratios vary with the peer group. Baseline optimization is at least as good as the baseline.</p><p>Optimal may improve or degrade the individual miss ratio. The group miss ratios are shown in Figure <ref type="figure" target="#fig_6">6</ref> and summarized in Table <ref type="table" target="#tab_1">I</ref>. Due to space limitations, we only show 8 representative programs out of the 16 tested. The others have similar behaviors.</p><p>always made better in Optimal, and namd is almost always made worse. Overall, Figure <ref type="figure">5</ref> shows that the optimization attempts to reduce the high miss ratios more than it increases the low miss ratios. However, there are plenty of exceptions. For example, two programs, hmmer and tonto, have very low miss ratios but gain rather lose from the optimization in most of their co-run groups. In almost all programs, whether to gain or lose depends on its co-run group, so is the degree of gain or loss.</p><p>Baseline Optimization: Baseline optimization is more effective for Equal than for Natural. As shown in the individual results in Figure <ref type="figure">5</ref> and the group results in Figure <ref type="figure" target="#fig_6">6</ref>, Natural Baseline improves upon Natural in only a few cases. Table <ref type="table" target="#tab_1">I</ref> shows similar improvements and a similar distribution of improvements for Optimal over Natural and over Natural Baseline. On the other hand, Baseline Equal improves upon Equal in far more cases for both the individual and the group miss ratio. On average, Baseline Equal is better than Equal by nearly 30%. The different improvements from the two baselines make a clear contrast. It shows that there is much more under-utilized cache space in equal partitions than in natural partitions.</p><p>To use the terminology of Xie and Loh <ref type="bibr" target="#b16">[17]</ref>, there is more resource under utilization in the socialist allocation than in the capitalist allocation. Note that this conclusion cannot be made without baseline optimization, which is only possible because of the techniques developed in this paper.</p><p>Sampling is Unscientific: The shape and the range of Natural miss ratios in all groups cannot be predicted by taking a few random points. It is almost guaranteed that differently sampled groups have few results in common. If a program always or never gains by cache sharing, sampling can recognize these cases. However, the amount of gains and losses is consistently inconsistent and cannot be fully analyzed by sampling.</p><p>Stone-Thiebaut-Turek-Wolf (STTW): The classic solution of Stone et al. minimizes the group miss ratio through convex optimization. We show the group-by-group comparison between STTW and optimal in Figure <ref type="figure" target="#fig_7">7</ref> and the statistical comparison in the bottom row in Table I labeled STTW.</p><p>When the convexity assumption holds, STTW is as good as optimal. However, as shown in Table <ref type="table" target="#tab_1">I</ref>, in 34% of co-run groups, STTW is at least 10% worse than optimal, showing that the convexity assumption does not hold at least for   these cases. In most of these groups, STTW is at least 20% worse. For these groups, STTW is actually worse than the natural partition, shown by the higher average improvement of Optimal over STTW (34%) than over Natural (26%). These results show that the convexity assumption is flawed to the extent that STTW optimization has the worse average performance than the simple free-for-all cache sharing. The problem is exacerbated when more programs share the cache, since a larger group increases the chance of the violation of the assumption by one or more members. In addition, STTW cannot optimize for fairness. The problems of STTW have been solved by our new algorithm, which improves the maximal performance by 34% on average over STTW and enables baseline optimization of fairness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Validation</head><p>Previous work has measured the accuracy of the HOTL theory. Xiang et al. compared the prediction for two-program pairs <ref type="bibr" target="#b15">[16]</ref>. They tested all 190 pair combinations. For each pair, they repeatedly ran each program so the co-run effect was stable from one test to another, <ref type="foot" target="#foot_2">6</ref> and measured the miss ratio for each program using three hardware counters:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OFFCORE_RESPONSE_0.DATA_IN.LOCAL_DRAM MEM_INST_RETIRED.LOADS MEM_INST_RETIRED.STORES</head><p>The first is the miss count, and the sum of the last two is the access count. Their ratio is the miss ratio. Xiang et al. compared the 380 measured miss ratios with the prediction and showed the results in Figure <ref type="figure">9</ref> in <ref type="bibr" target="#b15">[16]</ref>. The prediction was accurate or nearly accurate for all but two miss ratios. From their results, we draw the conclusion that the natural partition assumption is largely true.</p><p>We do not simulate system performance for several reasons. First, our purpose and expertise both lie in pro-gram analysis and optimization, not hardware cache design. Second, our goal is not to maximize performance, which depends on more factors than we can rigorously study in one paper. By focusing on cache and the slowdown-free miss rate, the achieved optimality is actually CPU independent, i.e., optimal regardless of the CPU design.</p><p>A simulator, especially for a multicore system, has many parameters. We are not confident that we can produce the same accuracy that we can with locality modeling. Finally, simulation is slow. Most computer architecture studies simulate a small fraction of a program. For example, Hsu et al. used a cache simulator called CASPER to measure the miss ratio curves from cache sizes 16KB to 1024KB in increments of 16KB <ref type="bibr" target="#b0">[1]</ref>. They noted that "miss rate errors would have been unacceptably high with larger cache sizes" because they "were limited by the lengths of some of the traces." Our analysis considers all data accesses in an execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. DISCUSSION ON OPTIMALITY</head><p>The nature cache partition as we have derived in Section V implies that optimal cache partition is the optimal solution for partition sharing. This optimality requires a number of assumptions, which we enumerate and discuss here.</p><p>HOTL Theory Correctness: The HOTL theory assumes the reuse window hypothesis, which means that the footprint distribution in reuse windows is the same as the footprint distribution in all windows <ref type="bibr" target="#b15">[16]</ref>. When the hypothesis holds, the HOTL prediction is accurate for fully associative LRU cache. The correctness has been evaluated and validated for solo-use cache, including the two initial studies of the footprint theory <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Independent validation can be found in the use of the footprint theory in optimal program symbiosis in shared cache <ref type="bibr" target="#b11">[12]</ref>, optimal memory allocation in Memcached <ref type="bibr" target="#b1">[2]</ref>, and a study from the OS community on server cache performance for disk access traces <ref type="bibr" target="#b12">[13]</ref>.</p><p>Random Phase Interaction: We assume that programs interact in their average behavior. Programs may have phases, but their phases align randomly rather than deterministically. An example of synchronized phase interaction is shown at the beginning of the paper in Figure <ref type="figure">1</ref>. The natural partition does not exist since no cache partition can give the performance of cache sharing. However, synchronized phase interaction is unlikely for independent applications. It is unlikely that they have the same-length phases, and one program always finishes a phase just when a peer program starts another phase. We assume that the phase interaction is random. This assumption is implicit and implicitly validated in Xiang et al., who tested 20 SPEC programs and found that HOTL is acceptable in 99.5% of cases of 20 2 possible paired running <ref type="bibr" target="#b15">[16]</ref>.</p><p>Fully Associative LRU Cache: The locality theory is partly mathematical, and the mathematics is critical in establishing the formal connection between program-and machine-level concepts, e.g., from the reuse time to the miss ratio. In comparison, the real cache is set-associative, and not all cache sizes are possible. In addition, the replacement policy may be an approximation or improvement of LRU. Past work has validated the theory result by measuring the actual miss ratio (using the hardware counters) on a real system. Xiang et al. showed accurate prediction by the HOTL theory for all three levels of cache on a test machine (Figure <ref type="figure" target="#fig_6">6</ref> in <ref type="bibr" target="#b15">[16]</ref>). In addition, the HOTL theory can derive the reuse distance, which can be used to statistically estimate the effect of associativity <ref type="bibr" target="#b7">[8]</ref>, and as Sen and Wood recently showed, the performance of non-LRU policies <ref type="bibr" target="#b6">[7]</ref>.</p><p>Locality-performance Correlation: A recent study by Wang et al. shows that the HOTL-based miss ratio prediction has a linear relationship between execution time, with a coefficient of 0.938 <ref type="bibr" target="#b11">[12]</ref>. They measure execution times and miss ratios of all 1820 4-programs co-run groups from a set of 16 SPEC programs. Thus, reducing execution time can be achieved though reducing same portion of miss ratio.</p><p>Practicality: The profiled metrics are average footprint, total number of memory accesses, and solo-run time for each program. Xiang et al. reported on average 23 times slowdown from the full-trace footprint analysis <ref type="bibr" target="#b15">[16]</ref>. Wang et al. developed a sampling method called adaptive bursty footprint (ABF) profiling, which takes on average 0.09 second per program <ref type="bibr" target="#b11">[12]</ref>. To have reproducible results, our implementation uses the full-trace footprint. We assume that in practice, the data can be collected in real time.</p><p>While these assumptions do not always hold, they have been carefully studied and validated through experiments on actual systems. Sections VII-C and IX give more details on some of these studies. In this paper, we use these assumptions to develop optimal partition sharing for general programs on any size cache. For hardware cache design, these assumptions may not be adequate, and careful simulation may be necessary. However, our objective here is narrow and specific, which is a machine-independent strategy for program co-run optimization for cache performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. RELATED WORK</head><p>The Footprint Theory: Optimization must be built on theory, which in this case is the higher-order theory of locality (HOTL) <ref type="bibr" target="#b15">[16]</ref>. As a metric, footprint quantifies the active data usage in all time scales. Because the footprint is composable, it can be used to predict the performance of cache sharing. In this paper, we show a consequence of the HOTL theory: the performance of shared cache equals to a particular solution of cache partitioning called the natural partition, which we then use to optimize not just cache sharing, but also cache partitioning and partition sharing.</p><p>Three recent studies provide fresh evidence on the accuracy of the footprint analysis in modeling fully-associative LRU cache. Wang et al. tested the analysis on program execution traces for CPU cache <ref type="bibr" target="#b11">[12]</ref>, Hu et al. on keyvalue access traces for Memcached <ref type="bibr" target="#b1">[2]</ref>, and Wires et al.</p><p>on disk access traces for server cache <ref type="bibr" target="#b12">[13]</ref>. The three studies re-implemented the footprint analysis independently and reported high accuracy through extensive testing. Hu et al. tested the speed of convergence, i.e., how quickly the memory allocation stablizes under a steadystate workload, and found that optimal partition converges 4 times faster than free-for-all sharing <ref type="bibr" target="#b1">[2]</ref>. Finally, Wang et al. showed strong correlation (coefficient 0.938) between the predicted miss ratio and measured co-run speed <ref type="bibr" target="#b11">[12]</ref>. The correlation means that if we minimize the miss ratio in shared cache, we minimize the execution time of co-run programs.</p><p>Cache Partitioning: Cache partitioning is an effective solution to improve performance, which has been shown by many studies including the following two on optimal cache partitioning. Stone et al. gave a greedy solution to allocate cache among N processes, which allocates the next cache block to the process with the highest miss-rate derivative. The allocation is optimal if the miss-rate derivatives are as equal as possible <ref type="bibr" target="#b8">[9]</ref>. The optimality depends on several assumptions. One is that the miss-rate derivative must be monotonic. In other words, the MRC must be convex. Suh et al. gave a solution which divides MRC between nonconvex points but concluded that the solution "may be too expensive" <ref type="bibr" target="#b9">[10]</ref>.</p><p>The previous optimization is limited to cache partitioning. Here we optimize for both partitioning and sharing. Furthermore, our solution does not depend on any assumption of the MRC curve. It can use any cost function. The generality is useful when optimizing with constraints or for multiple objectives, e.g., both throughput and fairness in elastic cache utility optimization <ref type="bibr" target="#b17">[18]</ref>.</p><p>Concurrent Reuse Distance: Simulation of shared cache is costly, and the result is specific to the cache being simulated. More general solutions are based on the concept of concurrent reuse distance (CRD), which shows the performance of both partitioned and shared cache for all cache sizes <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b13">[14]</ref>. However, CRD is for a given set of programs and must be measured again when the set changes. It cannot derive the optimal grouping of programs, which is needed for partition sharing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X. SUMMARY</head><p>In this paper we presented partition-sharing as a general problem for assigning cache space to programs running on multi-core architectures. We enumerated the search spaces for 5 different sub-problems of partition-sharing, used the Higher Order Theory of Locality to show that the best partitioning-only solution must be the optimal partitionsharing solution, and presented a new algorithm for attaining optimal partitioning solution. In addition, we defined two resource-efficient fairness conditions, equal baseline and natural baseline, and used a modified algorithm to optimize performance under each one. Experiments show that the baseline optimization can significantly improve equal partitioning but not free-for-all sharing. Each optimization result is obtained from a very large solution space for different ways to share the cache.</p><p>This paper is subtitled with a quote from Robert Frost: "Don't ever take a fence down until you know why it was put up". We argue that despite there being a vast number of options for allocating shared cache among programs, the best option is usually to assign a partition to each program. If we are to trust the great American poet, as well as the nowestablished Higher Order Theory of Locality, we'd better default to leaving up fences between our co-run programs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Free 6 { 2 { 4 Figure 1 :</head><label>6241</label><figDesc>Figure1: A set of program traces that will benefit from partition sharing. Core 1 and core 2 run streaming programs, so partitioning them off prevents them from polluting the cache. Cores 3 and 4 alternate between having large and small working sets. Sharing a partition allows them each to use more cache when it is needed. Capacity misses are shaded with gray.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples of possible partition sharing scenarios with 4 programs and one or two caches. 1: Two caches shared by any number of programs. 2: One cache partitioned for any number of groups of any number of programs (general partition-sharing case). 3: One cache with dedicated partitions for each program (partitioning-only).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Datum | a a x b b y a a x b b</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example trace with two reuse windows boxed. Each of the reuse windows decreases every containing window's footprint due to the data redundancy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The natural cache partition is defined by the quantity of data in a cache expected to belong to each program. For two programs, the figure shows that the cache occupancy of each program can be predicted to be the individual (stretched) footprint of that program where the total footprint equals the cache size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure5: The miss ratio of individual programs running with different peer groups, under natural, equal, natural baseline, equal baseline, and optimal cache partition. Programs are sorted by the (constant) miss ratio in the equal partition. Under the other four schemes, the miss ratios vary with the peer group. Baseline optimization is at least as good as the baseline. Optimal may improve or degrade the individual miss ratio. The group miss ratios are shown in Figure6and summarized in TableI. Due to space limitations, we only show 8 representative programs out of the 16 tested. The others have similar behaviors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The group miss ratio of the five partitioning methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The group miss ratio of Optimal and STTW.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table I :</head><label>I</label><figDesc>The improvement of group performance by Optimal partition over five other partitioning methods. The last two columns show the percent groups that are improved by at least 10% and 20% respectively.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Methods of</cell><cell cols="3">Improvement by Optimal</cell><cell>Improved by at least</cell></row><row><cell></cell><cell></cell><cell></cell><cell>partitioning</cell><cell>Max</cell><cell>Avg</cell><cell>Median</cell><cell>10%</cell><cell>20%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Equal</cell><cell cols="3">4746.43% 125.25% 26.48% 77.08%</cell><cell>57.80%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Equal baseline</cell><cell cols="3">2954.52% 97.75% 22.50% 70.27%</cell><cell>52.69%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Natural</cell><cell>266.78%</cell><cell cols="2">26.35% 14.51% 57.80%</cell><cell>45.16%</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Natural baseline 266.78%</cell><cell cols="2">26.21% 14.29% 56.81%</cell><cell>45.10%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>STTW</cell><cell>306.55%</cell><cell>33.68%</cell><cell>2.50% 34.39%</cell><cell>33.02%</cell></row><row><cell></cell><cell></cell><cell>Natural</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Equal</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.08</cell><cell>Natural baseline Equal baseline Optimal</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.06</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>miss ratio</cell><cell>0.04</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.02</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.00</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>500</cell><cell>1000</cell><cell>1500</cell><cell></cell></row><row><cell></cell><cell cols="4">4−program co−run groups (sorted by Optimal miss ratio)</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 08:50:59 UTC from IEEE Xplore. Restrictions apply.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">The access rate is measured for each program as the length of its memory access trace divided by the number of seconds the program ran for.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2">The stable result avoids the problem of performance variation, which can be modeled using Sandberg et al.<ref type="bibr" target="#b4">[5]</ref> </note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Communist, utilitarian, and capitalist cache policies on CMPs: caches as a shared resource</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Makineni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACT</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="13" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">LAMA: Optimized locality-aware memory allocation for key-value cache</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of USENIX ATC</title>
				<meeting>USENIX ATC</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Is reuse distance applicable to data locality analysis on chip multiprocessors?</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CC</title>
				<meeting>CC</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="264" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Protection and utilization in shared cache through rationing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Parihar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of PACT</title>
				<meeting>PACT</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="487" to="488" />
		</imprint>
	</monogr>
	<note>short paper</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modeling performance variation due to cache sharing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sandberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sembrant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hagersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Black-Schaffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HPCA</title>
				<meeting>HPCA</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="155" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Accelerating multicore reuse distance analysis with sampling and parallelization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Schuff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Pai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of PACT</title>
				<meeting>PACT</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="53" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reuse-based online models for caches</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGMETRICS</title>
				<meeting>SIGMETRICS</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="279" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the effectiveness of set associative page mapping and its applications in main memory management</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICSE</title>
				<meeting>ICSE</meeting>
		<imprint>
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimal partitioning of cache memory</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Turek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1054" to="1068" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamic partitioning of shared cache memory</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Devadas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Supercomputing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="26" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving disk cache hit-ratios through cache partitioning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Thiébaut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="665" to="676" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimal program symbiosis in shared cache</title>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CCGrid</title>
				<meeting>CCGrid</meeting>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Characterizing storage workloads with counter stacks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ingram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Drudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Warfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Data</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of OSDI</title>
				<meeting>OSDI</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="335" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient reuse distance analysis of multicore scaling for loop-based parallel programs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Linear-time modeling of program working set in shared cache</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of PACT</title>
				<meeting>PACT</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="350" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">HOTL: a higher order theory of locality</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ASPLOS</title>
				<meeting>ASPLOS</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="343" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic classication of program memory behaviors in CMPs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CMP-MSI Workshop</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">RECU: Rochester elastic cache utility</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NPC</title>
				<meeting>NPC</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
