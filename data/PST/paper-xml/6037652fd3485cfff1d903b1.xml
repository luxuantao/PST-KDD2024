<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SceneNet: Remote sensing scene classification deep learning network using multi-objective neural evolution architecture search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-01-04">4 January 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ailong</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuting</forename><surname>Wan</surname></persName>
							<email>wanyuting@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanfei</forename><surname>Zhong</surname></persName>
							<email>zhongyanfei@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junjue</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SceneNet: Remote sensing scene classification deep learning network using multi-objective neural evolution architecture search</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-01-04">4 January 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1016/j.isprsjprs.2020.11.025</idno>
					<note type="submission">Received 10 May 2020; Received in revised form 6 August 2020; Accepted 18 November 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Scene classification deep neural network remote sensing multi-objective optimization evolutionary algorithm neural architecture search</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The scene classification approaches using deep learning have been the subject of much attention for remote sensing imagery. However, most deep learning networks have been constructed with a fixed architecture for natural image processing, and they are difficult to apply directly to remote sensing images, due to the more complex geometric structural features. Thus, there is an urgent need for automatic search for the most suitable neural network architecture from the image data in scene classification, in which a powerful search mechanism is required, and the computational complexity and performance error of the searched network should be balanced for a practical choice. In this article, a framework for scene classification network architecture search based on multi-objective neural evolution (SceneNet) is proposed. In SceneNet, the network architecture coding and searching are achieved using an evolutionary algorithm, which can implement a more flexible hierarchical extraction of the remote sensing image scene information. Moreover, the computational complexity and the performance error of the searched network are balanced by employing the multi-objective optimization method, and the competitive neural architectures are obtained in a Pareto solution set. The effectiveness of SceneNet is demonstrated by experimental comparisons with several deep neural networks designed by human experts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scene classification refers to distinguishing the different semantic features of remote sensing images, which means that the different features and spatial distributions are reflected in the remote sensing images <ref type="bibr" target="#b6">(Cheng et al., 2017a)</ref>. Compared with image interpretation based on the pixel and object levels, scene level based image classification considers the different spatial distribution modes for the different objects in the high spatial resolution (HSR) remote sensing images <ref type="bibr" target="#b47">(Xia et al., 2017;</ref><ref type="bibr" target="#b59">Zhu et al., 2018)</ref>. There are various applications of scene classification, especially for land-use identification and urban planning <ref type="bibr" target="#b5">(Cheng et al., 2015;</ref><ref type="bibr" target="#b57">Zhao et al., 2019)</ref>. However, scene classification is still an arduous task due to the complex spatial and structural patterns of remote sensing images.</p><p>Scene classification has developed a lot in recent years. Compared with pixel-level classification and object detection, the semantic label information is emphasized in remote sensing scene classification, and can give more social semantic attributes <ref type="bibr" target="#b56">(Zhao et al., 2016)</ref>, such as airport, industrial area, commercial area, golf course, etc. Moreover, the scene classification methods can be classified into the three categories of low-level, middle-level, and high-level methods. The low-level features in scene classification include the color histogram <ref type="bibr" target="#b19">(Hafner et al., 1995)</ref>, local binary patterns (LBPs) <ref type="bibr" target="#b36">(Ojala et al., 2002)</ref>, and the gray-level cooccurrence matrix (GLCM) <ref type="bibr" target="#b22">(Haralick et al., 1973)</ref>. The middle-level methods, such as the bag-of-visual-words (BoVW) model, are an important way to extract the visual descriptors of the scenes <ref type="bibr" target="#b59">(Zhu et al., 2016;</ref><ref type="bibr" target="#b13">Csurka et al., 2004)</ref>. However, the high-level features are usually ignored in these handcrafted feature based traditional approaches.</p><p>Recently, deep learning based methods have played a vital role in extracting the high-level features, and have emerged as a dominant paradigm in pattern recognition and computer vision <ref type="bibr" target="#b4">(Chen and Liu, 2018)</ref>. In deep learning technology, convolutional neural networks (CNNs) can be regarded as the typical data-driven methods, and are powerful tools that can be used to discover the intricate structures and extract the essential information of HSR remote sensing imagery <ref type="bibr" target="#b63">(Zou et al., 2015)</ref>, in addition to the hierarchical convolutional features of hyperspectral imagery <ref type="bibr" target="#b7">(Cheng et al., 2018a)</ref>. For image classification in ImageNet <ref type="bibr" target="#b27">(Krizhevsky et al., 2012)</ref>, GoogLeNet <ref type="bibr" target="#b42">(Szegedy et al., 2015)</ref> and CaffeNet <ref type="bibr" target="#b26">(Jia et al., 2014)</ref> have been employed for scene classification <ref type="bibr" target="#b3">(Castelluccio et al., 2015)</ref>. Furthermore, a series of deep learning based scene classification approaches have been proposed <ref type="bibr" target="#b9">(Cheng et al., 2020)</ref>. For example, to replace the traditional handcrafted feature descriptor, <ref type="bibr" target="#b8">(Cheng et al., 2017b)</ref> proposed a scene classification method based on the bag of convolutional features; <ref type="bibr" target="#b10">(Cheng et al., 2018b)</ref> proposed the learning of a discriminative CNN to solve the problems of inter-class similarity and intra-class diversity in scene image classification; <ref type="bibr" target="#b31">(Lu et al., 2019b</ref>) designed a scene image classification CNN through aggregating the end-to-end features; and <ref type="bibr" target="#b18">Gong et al. (2018)</ref> put forward a diversity-enhanced metric learning approach for deep structures in HSR image scene classification. Other deep learning based approaches for scene image classification have also been developed by <ref type="bibr" target="#b2">Anwer et al. (2018)</ref>, <ref type="bibr">Lu et al. (2019a)</ref>, <ref type="bibr" target="#b20">Han et al. (2018)</ref>, and <ref type="bibr" target="#b60">Zhu et al. (2019)</ref>.</p><p>However, in order to design a satisfactory deep CNN that can extract different levels of image information for scene semantic classification, comprehensive domain knowledge of the recognition and interpretation of remote sensing image features and deep learning in computer vision is required <ref type="bibr" target="#b45">(Wang et al., 2020)</ref>, and strenuous effort must be devoted to the design of the relevant network structure by human experts. Thus, it is natural to consider whether a computer could be used to automatically search for and obtain a suitable data-driven network. Fortunately, thanks to the rapid expansion of graphics processing units (GPUs) and other hardware, computing power has been greatly improved. The Google automatic machine learning (AutoML) platform is a suite of machine learning products, in which the implementation mechanisms are collectively referred to as neural architecture search (NAS) <ref type="bibr" target="#b24">(He et al., 2019;</ref><ref type="bibr" target="#b25">He et al., 2018)</ref>. The principal idea of NAS can be summarized as three steps: 1) definition of the search space; 2) the search strategy, in which candidate network structures can be found through the search strategy and then evaluated; and 3) the performance estimation strategy, in which the next iteration is carried out according to the feedback <ref type="bibr" target="#b16">(Elsken et al., 2018)</ref>. In the field of natural image interpretation, NAS methods have outperformed manually designed architectures in natural image classification <ref type="bibr" target="#b61">(Zoph and Le, 2017;</ref><ref type="bibr" target="#b62">Zoph et al., 2018;</ref><ref type="bibr" target="#b29">Liu et al., 2018)</ref> and semantic segmentation <ref type="bibr" target="#b28">(Liu et al., 2019)</ref>. Thus, it can be found that the structure of the network can be established through the characteristics of the dataset itself. Moreover, the search strategy plays an important role in NAS, and can be categorized into three types <ref type="bibr">(Lu et al., 2019c;</ref><ref type="bibr" target="#b49">Xie et al., 2018)</ref>: 1) gradient-based (GB) search; 2) reinforcement learning (RL) search; and 3) evolutionary algorithms (EAs). <ref type="bibr" target="#b38">Real et al. (2019)</ref> conducted a case study of the different search strategies, and they found that the RL-and EA-based methods can obtain similar performances, and these two approaches both perform better than the GB-based methods. In addition, more computing resources are needed in the RL-based methods, and smaller models can be obtained by the EA-based methods. In fact, EA-based NAS methods have been a topic of interest for a period of time, and are also referred to as neuroevolution methods. Back in the 1990s, Yao and colleagues <ref type="bibr" target="#b52">(Yao, 1993;</ref><ref type="bibr" target="#b54">Yao and Liu, 1997;</ref><ref type="bibr" target="#b53">Yao, 1999)</ref> suggested that neuroevolution is a different kind of deep learning. Since then, studies combining evolutionary methods with artificial neural networks (ANNs) have aroused the attention of scholars. In addition, neuroevolution has also crossed from shallow architecture search to deep network architecture search. Thus, EA-based NAS is further discussed in this article.</p><p>Learning from biological evolution and natural selection, the EAbased NAS methods attempt to explain the connection between the CNN structure and natural evolution <ref type="bibr" target="#b48">(Xie and Yuille, 2017)</ref>. Currently, with the rapid development of deep CNNs, EA-based architecture search using deep CNNs is attracting more and more attention in the artificial intelligence community <ref type="bibr" target="#b38">(Real et al., 2019;</ref><ref type="bibr" target="#b44">Wang et al. 2019)</ref>. EA-based architecture search involves attempting to evolve, design, and build the neural network through the EA, instead of stochastic gradient descent and artificial design. Moreover, EAs have also played a significant role in traditional machine learning for remote sensing image interpretation <ref type="bibr" target="#b58">(Zhong et al., 2018)</ref>, in applications such as remote sensing image clustering <ref type="bibr" target="#b46">(Wan et al., 2019;</ref><ref type="bibr" target="#b1">Alok et al., 2016)</ref>, subpixel mapping <ref type="bibr" target="#b41">(Song et al., 2019)</ref>, sparse unmixing <ref type="bibr" target="#b17">(Gong et al., 2017)</ref>, and change detection <ref type="bibr" target="#b40">(Song et al., 2018)</ref>. Thus, EAs represent a potential solution for the automatic search for a satisfactory deep CNN for scene classification. In addition, there is also a lack of networks automatically evolved and searched from remote sensing data in image interpretation, which could avoid the need for arduous manual tuning.</p><p>The objective functions optimized in the EA are also important. In addition to the use of the test accuracy to evaluate a neural network for image interpretation, the computational complexity of the network should also be considered for a comprehensive evaluation <ref type="bibr">(Lu et al., 2019c;</ref><ref type="bibr" target="#b43">Tan et al., 2019)</ref>. Thus, the computational complexity and the test error/accuracy are required to be balanced, so that a competitive solution set is obtained. Fortunately, the population-based EAs can provide an ideal solution to the multi-objective optimization problems <ref type="bibr" target="#b11">(Coello, 2006)</ref>, and multi-objective evolutionary optimization methods have been successfully employed in traditional machine learning methods for remote sensing image interpretation, in applications such as image clustering <ref type="bibr" target="#b34">(Ma et al., 2015)</ref>, subpixel mapping <ref type="bibr" target="#b41">(Song et al., 2019;</ref><ref type="bibr" target="#b33">Ma et al., 2018)</ref>, hyperspectral feature selection <ref type="bibr" target="#b55">(Zhang et al., 2018)</ref>, and hyperspectral image sparse unmixing <ref type="bibr" target="#b50">(Xu and Shi, 2017)</ref>. Thus, in a NAS-based scene classification network, multi-objective optimization of the test error and the computational complexity should be considered in the evolutionary NAS to provide a non-dominated choice in the competitive solution set.</p><p>In this article, in order to automatically search for a satisfactory network for scene classification from the image dataset itself, a framework of scene classification network architecture search based on multiobjective neural evolution is proposed. The main contributions of this article are summarized below:</p><p>1) A framework of scene classification network architecture search based on multi-objective neural evolution. The proposed SceneNet is an EA-based NAS approach for the remote sensing image scene classification task, which has not been achieved in the existing studies. In SceneNet, the most suitable network can be automatically searched from the dataset itself, without requiring handcrafted design, and the computational complexity and the performance error of the searched network can be balanced adaptively.</p><p>2) Evolutionary algorithm based flexible extraction of the scene information and the powerful search capability. In SceneNet, the connection modes of the network architecture are encoded through the form of binary coding in the EA, and a search space definition is established, which can provide a flexible test of the different connection modes between the convolutional layers, so that more flexible hierarchical extraction of the remote sensing image scene information can be achieved for a better classification result. Moreover, the powerful exploitation and exploration search capabilities of the network architecture can be attributed to the global search capability of the EA and the local search capability of the Bayesian optimization algorithm (BOA) <ref type="bibr" target="#b37">(Pelikan et al., 1999)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nomenclature</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>It max</head><p>Maximum number of iterations. NP Population size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NI j i</head><p>The j-th phase in the i-th neural network individual in a population.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NI j*</head><p>The j-th phase in the neural network individual sampled after the Bayesian network. HSR High spatial resolution. NAS Neural architecture search. CNN Convolutional neural network. PF Pareto front. GFLOPs Giga floating-point operations.</p><p>3) The multi-objective trade-off for network design. For realworld deployment, the computational complexity should be simultaneously optimized with the accuracy, so the floating-point operations (FLOPs) are counted as the computational complexity. Thus, the accuracy and computational complexity are balanced in the proposed Sce-neNet method by utilizing multi-objective optimization. The nondominated network individuals and a competitive Pareto optimal solution set are then obtained, providing the user with practical choices.</p><p>The rest of this paper is organized as follows. The related research background is presented in Section 2. The proposed SceneNet method is introduced in Section 3. Section 4 describes the experiments undertaken in this study. A discussion and our conclusions are provided in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related research background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The classic CNN classification networks</head><p>With regard to CNN classification networks, a lot of successful examples have been used in scene classification, with the aim being to capture the global information of the remote sensing imagery. As shown in Fig. <ref type="figure">1</ref>(a), AlexNet, which was designed by <ref type="bibr" target="#b27">Krizhevsky et al. (2012)</ref>, obtained first place in the 2012 ImageNet Large Scale Visual Recognition Challenge (ILSVRC 2012). There are eight layers in AlexNet, which can be divided into five convolutional layers and three fully connected layers. AlexNet is one of the earliest deep CNNs, and has been employed in many image scene classification tasks <ref type="bibr" target="#b35">(Nogueira et al., 2017;</ref><ref type="bibr" target="#b21">Han et al., 2017)</ref>. However, its performance in extracting deep and complex features is limited, due to its shallow layers. As shown in Fig. <ref type="figure">1(b)</ref>, the Visual Geometry Group network with 16 layers (VGG16) <ref type="bibr" target="#b39">(Simonyan and Zisserman, 2014)</ref> increases the depth of the network structure, with 13 convolutional layers, five pooling layers, and three fully connected layers. Furthermore, as the structure depths continued to increase, the residual network (ResNet) was designed and proposed by <ref type="bibr" target="#b23">He et al. (2016)</ref>   presented, it can be found that the residual connections are the biggest difference with the other networks, except for the depth, which can ensure the integrity of the information in the process of transmission.</p><p>Although the above typical deep CNNs have made a lot of breakthroughs in natural image classification, it is mainly fixed image information that can be extracted under the fixed network structure and connection modes, and they are not driven by the characteristics of the image data. Thus, in this article, an evolutionary NAS approach for the scene classification of HSR remote sensing imagery is investigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Evolutionary algorithm based multi-objective optimization</head><p>To address the drawbacks of the existing deep CNNs for scene classification, the evolutionary multi-objective optimization method is used to automatically search for a trade-off solution set of the network individuals. The EA and the multi-objective optimization used in the proposed approach are introduced in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Evolutionary algorithm</head><p>Learning from natural biological evolution, the behavior is imitated by computer, and a series of algorithms have been derived, which are called EAs. Moreover, EAs are a kind of population-oriented random search technology and method that simulate the evolutionary process of creatures in nature <ref type="bibr" target="#b14">(De Jong, 2016)</ref>. They can also be considered as a kind of heuristic random search method, as they are not highly dependent on the mathematics of the problem, they can solve non-convex optimization problems, and they have strong global search capabilities. Therefore, the essence of an EA is a swarm search algorithm, which searches the strategy space by the ergodic method and maintains more diversity while optimizing, which can overcome the local minimum problem in nonlinear problems better than gradient descent. Moreover, due to the complexity and multiple peaks of the NAS search space, EAs can be used as one of the effective global search strategies in NAS. The main idea of EAs is presented in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, a population is first defined in a living environment, and the solution individuals in the problem are represented and encoded by the genomes. Moreover, the living environment is determined by the objective functions, and it is decided whether the individuals survive or not by comparing the values of the objective functions. The evolutionary operations are then conducted, including  crossover, mutation, etc., so that the population is expanded. In order to keep a constant population size, evaluation and selection are then conducted, and the fitness refers to the relevant objective function value. Thus, the population and the individuals are updated, and the superior solutions are selected for the next iteration, until the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Multi-objective optimization</head><p>A trade-off and competitive solution set for the network is obtained by simultaneously optimizing the test accuracy and FLOPs, and multiobjective optimization is then employed. As the name implies, the multi-objective optimization problems require the simultaneous optimization of multiple goals, which is different to the single-objective optimization problem in the usual sense. To allow a better understanding, some basic explanations are given below.</p><p>D.1. The multi-objective optimization problem: The m objectives are assumed to be minimized, as shown in Eq. ( <ref type="formula">1</ref>), where X denotes the decision variable, and the multiple conflicting objective functions satisfy the constraints in Eqs. ( <ref type="formula">2</ref>) and (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>minimize F(</head><formula xml:id="formula_0">X) = {f 1 (X), f 2 (X), ... , f j (X) , ... , f p (X)} (1) h τ (X) = 0, (τ = 1, 2, ..., m) (2) g τ (X)⩾0, (τ = 1, 2, ..., n) (3) D.</formula><p>2. Pareto domination: Compared to single-value comparison, multiple objective function values in the multi-objective problem are required to perform the comparison. For the solution individuals X 1 and X 2 , X 1 ≻ (Pareto dominates) X 2 when the condition in Eq. ( <ref type="formula">4</ref>) is satisfied. Furthermore, as shown in Fig. <ref type="figure" target="#fig_4">3</ref>, it can be inferred that</p><formula xml:id="formula_1">X 1 ≻ X 2 ≻ X 3 . ∀i = 1, 2, ... , p, f i (X 1 )⩽f i (X 2 )∧ ∃k = 1, 2, ... , p, f k (X 1 ) &lt; f k (X 2 ).</formula><p>(4) D.3. Pareto front: Compared to the single solution obtained in a single-objective optimization problem, a solution set is finally obtained in multi-objective optimization. From Fig. <ref type="figure" target="#fig_4">3</ref>, it can be found that the individuals in the resulting solution set are not better than each other. In addition, this solution set is the Pareto optimal solution set, and is often called the Pareto front.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Scene classification network architecture search based on multi-objective neural evolution (SceneNet)</head><p>In order to achieve automatic design of the network architecture for HSR remote sensing scene classification, a framework of nature-inspired multi-objective neural evolution is proposed. In Fig. <ref type="figure" target="#fig_3">4</ref>, the overall flowchart of the designed SceneNet approach is presented, and a more detailed description is provided in the later sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-objective modeling for SceneNet</head><p>In the real-world design of deep CNNs, the available memory, FLOPs, etc. are commonly considered, due to the limited computing devices and abilities. Thus, in the network design, we need to strike a balance between the multiple objectives, which can be achieved using the test performance and the computational complexity. In the proposed Sce-neNet method, the test error and the FLOPs are employed, as shown in Eqs. ( <ref type="formula">5</ref>) and ( <ref type="formula">6</ref>). The test error is the error percentage for the test samples in the search stage, num incorrected samples is the number of incorrectly classified samples, and num all test samples denotes the number of test </p><formula xml:id="formula_2">f (2) conv. = FLOPs = 2H in W in ( C in K 2 + 1 ) C out f (2) FC. = FLOPs = (2I − 1)O (6)</formula><p>where H in and W in are the input feature height and width, respectively; C in represents the number of input channels; K represents the kernel size; and C out is the number of output channels when calculating the FLOPs in the convolutional layer. I and O are the numbers of input and output channels, respectively, when calculating the FLOPs of the fully connected layer. Furthermore, GFLOPs = 10 9 × FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-objective neural evolution for scene classification</head><p>The test error and computational complexity, as defined above, are used in the evolutionary multi-objective optimization based NAS for scene classification. From Fig. <ref type="figure" target="#fig_3">4</ref>, it can be seen that there are several main procedures, including individual encoding for the search space and population initialization, evolutionary reproduction (exploration [crossover and mutation] and ranking), and exploitation (the Bayesian network).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Individual encoding for the search space and population initialization</head><p>In the proposed SceneNet method, the nature-inspired concept and the population-based global search are the typical features. Population initialization should be first conducted, and each individual network in the population should be encoded. Moreover, the individual in the EA represents the solution for the scene network to be searched for, and the architecture of the scene classification network is encoded into the individual. As shown in Fig. <ref type="figure" target="#fig_3">4</ref> and Fig. <ref type="figure" target="#fig_5">5</ref>, for example, it should be noted that there are two phases with several nodes in a network individual, the connection modes are different in the different phases, and the different connection modes means that different image information is extracted. Thus, the connection modes of the scene classification network are encoded in the individual and searched for in the proposed SceneNet method. In addition, the phase denotes a computational block, where the nodes in the phase denote batch normalization and a rectified linear unit (ReLU) after 3 × 3 convolution. As shown in Fig. <ref type="figure" target="#fig_6">6</ref>(a), the double circle is the 1 × 1 convolutional node, the gold circle represents the sum node, "Max Pooling" represents the maximum pooling with stride 2, "Avg Pooling" denotes the global average pooling, and "Linear" represents the fully connected layer.</p><p>In addition, for the individual encoding, as shown in Fig. <ref type="figure" target="#fig_5">5</ref>, the connection mode of the nodes in each phase is encoded, while the other nodes and operations are fixed. Compared to most of the artificial and NAS-based method search architectures, the phase of the computational block is not repeated in the proposed SceneNet method. Furthermore, the operations of an HSR remote sensing image scene classification network individual are encoded by</p><formula xml:id="formula_3">NI i = ( NI 1 i , NI 2 i , ..., NI j i , ..., NI np i ) ,</formula><p>where i denotes the i-th network individual and np is the number of phases in the architecture. Furthermore, the specific coding form of the phase is, for example, NI j i = 1 −10 −101 −0101 −00111 −1, as given in Fig. <ref type="figure" target="#fig_6">6(b</ref>). The values of "0" and "1" are the variable elements of a solution individual, where "1" indicates that the nodes are connected, while "0" indicates that the nodes are not connected. Moreover, the last gene position denotes whether the "1 × 1 convolutional node" and "sum node" are directly connected. Each phase is further encoded by utilizing this scheme, and the NP network individuals for the HSR image scene classification task are randomly initialized simultaneously to form a solution set, which is called the population in the EA.</p><p>Furthermore, as shown in Fig. <ref type="figure" target="#fig_5">5</ref> and Fig. <ref type="figure" target="#fig_6">6</ref>, it can be inferred that this encoding mechanism is flexible, and it can offer a terse representation of the scene image classification network architectures in the encoding space. In addition, from Fig. <ref type="figure" target="#fig_5">5</ref>, for example, the connection mode of the second phase NI 2 2 = 0 −00 −000 −1011 −10011 −0 of the second individual in the initialized population is similar to that of DenseNet. Furthermore, it can be inferred that many handcrafted computational blocks can also be searched in the proposed SceneNet method, such as ResNet, VGG, and DenseNet, which shows that the flexibility and expansibility of this encoding scheme are superior.</p><p>Based on the encoding scheme and initialization method in the SceneNet algorithm, the search space of the HSR image scene classification network individuals can be obtained, as shown in Eq. ( <ref type="formula">7</ref>):</p><formula xml:id="formula_4">Π = np × 2 n(n−1)/2+1 (7)</formula><p>where Π represents the search space and n denotes the number of computing nodes in a phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Evolutionary reproduction: exploration and ranking</head><p>Learning from natural evolution, the evolution of organisms benefits from the crossover and mutation of chromosomes and the choice of natural environment. In the proposed SceneNet method, the scene classification network individuals are encoded as the chromosomes, and the two used objectives are regarded as the living environment for the selection of the network individuals. Thus, the evolutionary reproduction of the network individuals and population can be implemented by crossover, mutation, and ranking for selection. In addition, from the The matrix representation of the binary coding for each phase in the network architecture: "1" denotes that the corresponding nodes are connected, while "0" indicates that the nodes are not connected. aspect of optimization, the crossover and mutation operations can be regarded as the exploration <ref type="bibr" target="#b12">(Crepinšek et al., 2013)</ref>, as they perform the long-distance search in the search space.</p><p>Step 1 (exploration: crossover and mutation): The evolutionary operations, i.e., crossover and mutation, are conducted for generating the new scene classification network individuals. As shown in Fig. <ref type="figure" target="#fig_8">7</ref>, for example, there are two parent network individuals randomly selected from the population: NI_P 1 = 1-00-010-0001-00001-0 and NI_P 2 = 1-00-010-0101-01011-1, and an offspring individual of NI_O 1 = 1-00-010-0101-01001-1 is then obtained after the crossover operation. The newly generated network has three more connections than NI_P 1 , including the connections between the nodes of "2" and "5", the connections between the nodes of "2" and "6", and the connections between the "1 × 1 convolutional node" and "sum node". In addition, it should be noted that the architectures of NI_P 1 and NI_P 2 are similar to those of the VGG network and DenseNet, respectively, and the newly generated network of NI_O 1 is similar to that of ResNet. Thus, the crossover operation in the EA is used to generate a new scene classification network architecture through the two original network structures, which changes the original connection mode of the calculation module and generates a network computing module connection mode that is suitable for image scene classification through the multi-objective function evaluation. This once again proves the flexibility and diversity of this search strategy, which is beneficial for the knowledge learning of the network structure.</p><p>The mutation operation is undertaken after the crossover operation, and it directly changes the connection mode of the calculation modules of the scene classification network architecture in the population to produce a more favorable network structure. For example, the multipoint mutation operation is conducted on the parent network individual of NI_P 2 . The offspring network individual of NI_O 2 = 1-00-010-1101-00011-0 is then generated, and two connections are removed while one connection between node "1" and node "5" is added. Overall, through the crossover and mutation operations, the scene classification network individuals in a population for HSR remote sensing imagery can be globally explored in the search space.</p><p>Step 2 (new population and evaluation): After the evolutionary operations, a new population with more individuals is then obtained. In order to perform the ranking, the evaluation of each network individual should be conducted. For each network, the second objective function of the FLOPs is first counted through Eq. ( <ref type="formula">6</ref>), the searched network is then trained using the training instances, and the first objective function of the test error is then obtained through the prediction of the test samples by Eq. ( <ref type="formula">5</ref>).</p><p>Step 3 (Pareto and density ranking): Learning from the theory of biological evolution, the population size should be kept constant for ecological equilibrium in a constant living environment, and thus natural selection plays a significant role in keeping the better individuals. Thus, a ranking strategy should be applied so that the superior network individuals can be selected for the next generation. Pareto ranking and density ranking are utilized in the proposed SceneNet method for updating the individuals and population. As shown in Fig. <ref type="figure" target="#fig_7">8</ref>(a), Pareto ranking is first employed, according to Eq. ( <ref type="formula">4</ref>), and Pareto comparison of all the individuals is performed. The first front is then obtained by the individuals that are not dominated by the others. Analogously, the second, third, fourth, and fifth fronts can be obtained separately with the rest of the network individuals. In addition, since a constant number of solution individuals is required to enter the next iteration, the network individuals on the first front will of course be the first to be selected. However, the number of network individuals in the first front is often more or less than NP. For example, as given in Fig. <ref type="figure" target="#fig_7">8</ref>, the number of  network solutions is 7 while NP = 8. In order to solve this problem, a density ranking strategy is utilized, as shown in Fig. <ref type="figure" target="#fig_7">8(b)</ref>, where the calculation of the density is used to assess the tightness of the network individuals. The equation for the density is presented in Eq. ( <ref type="formula">8</ref>), where it can be seen that the density between the i-th individual and the surrounding two solutions is counted, and the network solution individuals with larger values are chosen in the constant population for the next generation, which can maintain the diversity of individuals in the population, prevent premature convergence, and obtain more global search results for the scene classification network architecture.</p><formula xml:id="formula_5">Density i = f 1 (NI i−1 ) − f 1 (NI i+1 ) f max 1 − f min 1 + f 2 (NI i−1 ) − f 2 (NI i+1 ) f max 2 − f min 2 (8)</formula><p>where f 1 (⋅) and f 2 (⋅) denote the function values of the test_error and FLOPs in Eqs. ( <ref type="formula">5</ref>) and ( <ref type="formula">6</ref> Step 4 (termination condition): Repeat steps 1-3 until the number of generations reaches It max , and a temporary Pareto optimal solution set is then obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Exploitation: Bayesian network</head><p>From the perspective of the optimization, the global exploration and the local exploitation are two significant parts of the optimization process <ref type="bibr" target="#b15">(Deb et al., 2002)</ref>. The global exploration can help to obtain solutions in promising regions, which means a long-distance search capability. The local exploitation with short-distance search can help the algorithm to obtain a superior network structure for scene classification. Thus, after the evolutionary reproduction through the global exploration, exploitation is also conducted in the proposed SceneNet method. Moreover, a Bayesian network is proposed, which is inspired by the BOA <ref type="bibr" target="#b37">(Pelikan et al., 1999)</ref>.</p><p>The BOA is designed to optimize the variables with internal correlation, and is employed to relate to the paths and blocks through the different phases in a globally searched scene classification network architecture. Moreover, this step aims to use the information of all the networks evaluated in the past to guide the last step of the search. For example, as shown in Fig. <ref type="figure" target="#fig_5">5</ref>, there are two phases in an image scene classification network, which are NI 1 and NI 2 . Thus, as shown in Fig <ref type="figure" target="#fig_10">9</ref>, in the Bayesian network, the distributions P(NI 1 ) and P(NI 2 |NI 2 ) are then estimated through the search history of the network architectures, and the new offspring solutions NI 1 * and NI 2 * are obtained by sampling from this Bayesian network. After this exploitation step, the competitive Pareto optimal solution set of the scene classification network individuals is again obtained, as given in Fig. <ref type="figure" target="#fig_3">4</ref>.</p><p>However, considering the evaluation efficiency in the search progress, the network training is not sufficient in the search stage. Thus, in order to perform network retraining, the HSR remote sensing image dataset is divided into five folds, as in the example shown in Fig. <ref type="figure" target="#fig_3">4</ref>. Then, in the last step of the proposed SceneNet framework, the corresponding mean test accuracy ± standard deviation and the GFLOPs are recorded. Moreover, the retrained parameters are also preserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and analyses</head><p>To prove the effectiveness of the designed approach-scene classification network architecture search based on multi-objective neural evolution (SceneNet)-several state-of-the-art networks designed by human experts were compared with the proposed SceneNet method, i.e., AlexNet <ref type="bibr" target="#b27">(Krizhevsky et al., 2012)</ref>, VGG16 <ref type="bibr" target="#b39">(Simonyan and Zisserman, 2014)</ref>, ResNet34 <ref type="bibr" target="#b23">(He et al., 2016)</ref>, and GoogLeNet <ref type="bibr" target="#b42">(Szegedy et al., 2015)</ref>. The UC Merced (UCM) land-use dataset <ref type="bibr" target="#b51">(Yang and Newsam, 2010)</ref>, the NWPU RESISC45 <ref type="bibr" target="#b6">(Cheng et al., 2017a</ref>) (NWPU45) dataset, and the Aerial Image Dataset (AID) <ref type="bibr" target="#b47">(Xia et al., 2017)</ref> were utilized in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Parameter settings and evaluation metrics</head><p>The UCM, NWPU45, and AID scene classification datasets were utilized to test the superiority of the designed SceneNet approach. The corresponding parameter settings for the test datasets are presented in Table <ref type="table" target="#tab_0">1</ref>. Moreover, the experiments were divided into search and retraining stages.</p><p>The test accuracy and efficiency are important measures for the scene image classification task. The overall accuracy (OA) and the corresponding kappa coefficient obtained through the test instances were utilized to access the image scene prediction accuracy. The formulas for the OA and kappa are presented in Eqs. ( <ref type="formula">9</ref>) and ( <ref type="formula" target="#formula_6">10</ref>), respectively. The GFLOPs were used to access the computational complexity of the scene  Note: Cosine A.: A cosine annealing based learning rate that decayed from 0.03 to 0.001.</p><p>image classification network architecture through Eq. ( <ref type="formula">6</ref>), and the inference speed (samples per second) was also counted.</p><formula xml:id="formula_6">OA = ∑ K i=1 Nu ii Nu (9) Kappa = Nu ∑ K i=1 Nu ii − ∑ K i=1 Nu i+ × Nu +j Nu 2 − ∑ K i=1 Nu i+ × Nu +j (<label>10</label></formula><formula xml:id="formula_7">)</formula><p>where Nu ii denotes the number of image instances with correct predictions; Nu +j is the number of image instances of the j-th class in the prediction results; and Nu i+ is the number of image instances of the i-th class in the test image instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">UCM dataset experiments</head><p>As the first public remote sensing scene image classification dataset, the UCM dataset has been used in many studies. The UCM dataset has 2,100 scene images, which contain 21 land-use scene categories, including agricultural, beach, harbor, intersection, overpass, parking lot, river, etc. In each class, there are 100 scene images. The size of each image is 256 × 256 pixels, and the spatial resolution of each scene image is 0.3 m. Some scene image examples from the UCM dataset are presented in Fig. <ref type="figure" target="#fig_11">10</ref>. It should be noted that there are a few overlapping scene categories in the UCM dataset, which may produce a measure of influence for the scene classification approaches.</p><p>In the search stage, for the experimental settings, the images were split into two groups of 80% and 20%, and the 80% group were used as the training image instances, while the 20% group were utilized as the test image instances. The corresponding function values of test_error and FLOPs were counted through the searched network and the test samples. After the search stage, a non-dominated Pareto optimal solution set containing the competitive networks was then obtained for the UCM  Examples of the evolutionary-searched competitive scene classification networks in the solution set for the UCM classification dataset are presented in Fig. <ref type="figure" target="#fig_12">11</ref>. The specific structures of the first two searched neural evolutionary networks for the UCM land-use dataset are given in Fig. <ref type="figure" target="#fig_13">12</ref>. The connection modes of the first two phases in the first network are more complex than in the second network, while the last phase is simpler, which causes the first network to have higher GFLOPs and a lower test error. Thus, the multi-objective optimization scheme can provide diverse solutions for the image scene classification networks.</p><p>Furthermore, in order to prove the outstanding effect of the searched  Table <ref type="table">2</ref> Comparison of the mean OA ± standard deviation and mean kappa coefficients ± standard deviation for the human expert and evolutionary deep learning methods for the UCM dataset networks, a comparison with the networks designed by human experts was conducted, and the network with the lowest error in the searched solution set of the scene classification networks was selected for the next experiment. Moreover, five-fold cross-validation was utilized, where four folds of the scene images in the UCM dataset were randomly selected to be used as the training images, while the fifth fold was utilized as the test images. Thus, five separate network trainings and tests were conducted. Furthermore, the mean OA ± standard deviation and mean kappa coefficient ± standard deviation for the human expert and evolutionary deep learning approach for the UCM land-use dataset are given in Table <ref type="table">2</ref>. It should be noted that the searched network obtains a better performance than the other benchmark classification CNNs, which infers that the deep neural networks designed by human experts are mainly used for natural image classification, and the fixed network architecture can only obtain fixed image information. As a result, these networks are not driven by the characteristics of the remote sensing image data, and it can be difficult to apply these networks directly to remote sensing image data. Moreover, in the proposed method, a more suitable learning network for the UCM land-use dataset HSR scene image classification task can be automatically searched through the neural evolution based NAS. Finally, the classification confusion matrix is presented in Fig. <ref type="figure" target="#fig_14">13</ref>, where it should be noted that the SceneNet approach achieves a superior performance in all the scene categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">NWPU45 dataset experiments</head><p>To further prove the effectiveness of the SceneNet approach, the NWPU45 dataset was utilized, which is currently the largest scene classification dataset. There are 31500 scene images in the NWPU45 dataset, which contains 45 scene categories, such as airplane, cloud, golf course, snow berg, sparse residential, terrace, and wetland. Furthermore, there are 700 scene images in each category, the image size is 256 × 256 pixels, and the spatial resolution varies from approximately 0.2 to 30 m. Some scene image examples from the NWPU45 dataset are presented in Fig. <ref type="figure" target="#fig_16">14</ref>. There are many challenges for the classification of this dataset, such as the distinction between the medium residential and dense residential classes, the lake and wetland classes, the palace and church classes, etc.</p><p>In the search stage, as with the UCM dataset, 80% of the scene images in the NWPU45 dataset were randomly selected as the training  From the results shown in Fig. <ref type="figure" target="#fig_15">15</ref>, it can be observed that the searched networks are not better than each other, and they have different structures. In addition, with the same parameter settings, the scene classification network architectures for the NWPU45 dataset show competitive advantages in the test error and computational complexity. Moreover, from the specific structures of the first and the sixth networks, as presented in Fig. <ref type="figure" target="#fig_17">16</ref>, it can be found that the first network has two more connections between the "1 × 1 convolutional node" and "sum node" than the sixth network, which indicates that the original information of the images shows a certain promotion effect for the NWPU45 dataset.</p><p>In the retraining stage, as with the UCM dataset, the network with  Table <ref type="table">3</ref> Comparison of the mean OA ± standard deviation and mean kappa coefficient ± standard deviation for the human expert and evolutionary deep learning methods for the NWPU45 dataset the lowest error in the Pareto optimal solution set was selected for the next experiment. In addition, five-fold cross-validation was again utilized. The mean OA ± standard deviation and mean kappa coefficient ± standard deviation for the human expert and evolutionary deep learning approaches for the NWPU45 dataset are given in Table <ref type="table">3</ref>. It can be observed that the designed SceneNet approach obtains the best result, and VGG16 achieves second place. Furthermore, to show the prediction results of the SceneNet algorithm among the different classes, the classification confusion matrix is presented in Fig. <ref type="figure" target="#fig_18">17</ref>. In Fig. <ref type="figure" target="#fig_18">17</ref>, it can be noted that the phenomenon of mixing mainly occurs between the palace and church classes, where 10% of the church images are misclassified as the palace class, while 16% of the palace images are misclassified as the church class. However, the proposed method obtains a superior performance on the other classes. Thus, from the experiments with the NWPU45 dataset, it can be seen that the proposed SceneNet method can also obtain a satisfactory network architecture which is more suitable for remote sensing images than the deep neural networks designed by human experts, which can only extract fixed information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">AID dataset experiments</head><p>After the former two scene image classification experiments, to further test the effectiveness of the designed approach, the AID dataset with larger-scale images was utilized. The AID dataset contains 10000 images, with 30 categories. In addition, there are 220-420 images in each class, the image size is 600 × 600 pixels, and the spatial resolution varies from 0.5 to 8 m. Some scene image examples from the AID dataset are shown in Fig. <ref type="figure" target="#fig_19">18</ref>. There are many challenges in the classification of this dataset, such as classification of the commercial and school classes, and classification of the church, commercial, and dense residential classes.</p><p>In the search stage, differing from the experiments with the UCM and  Examples of the evolutionary-searched competitive networks in the non-dominated optimal network set for the AID image dataset classification are presented in Fig. <ref type="figure" target="#fig_20">19</ref>. It should also be noted that the searched networks obtain a low test error with high GFLOPs, which denotes a more complex connection mode. In addition, the specific structures of the first and the sixth searched neural evolutionary networks for the AID dataset are also given, as shown in Fig. <ref type="figure" target="#fig_21">20</ref>, where it is clear that the connection modes of the nodes in the three phases for the first network architecture are all more complex than those in the sixth network architecture for the AID dataset image scene classification. This again proves the effectiveness of the multi-objective strategy, which can provide the user with different network structures, to adapt to different HSR image scene classification tasks.</p><p>In the retraining stage, as with the UCM and NWPU45 datasets, the network with the lowest error in the Pareto optimal solution set was selected for the next experiment. However, the experimental settings of the five-fold cross-validation differed from the experiments with the previous datasets, in that a fold was randomly selected as the training samples from the five folds, while the other four folds were used as the test samples. Thus, there were five separate experiments for the AID  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>Comparison of the mean OA ± standard deviation and mean kappa coefficient ± standard deviation for the human expert and evolutionary deep learning methods for the AID dataset dataset in the retraining stage. The mean OA ± standard deviation and mean kappa coefficient ± standard deviation for the deep CNNs designed by human expert and the proposed SceneNet approach for the AID dataset are presented in Table <ref type="table">4</ref>.</p><p>From Table <ref type="table">4</ref>, it can be seen that the SceneNet approach obtains the best results, ResNet34 achieves second place, and AlexNet obtains the worst classification results, due to its shallow layers for extracting the features of the larger-scale images in the AID dataset. Furthermore, to show the prediction results of the SceneNet algorithm among the different classes, the classification confusion matrix is also presented, as shown in Fig. <ref type="figure" target="#fig_22">21</ref>, where it can be noted that the phenomenon of mixing mainly occurs between the commercial, church, and dense residential classes, where 8% and 7% of the commercial images are misclassified as the church class and dense residential class, respectively In addition, there is also a misclassification phenomenon in the center class, where the center class images are misclassified as the church, square, stadium, and storage tanks classes, due to the similar circular features, However, the proposed SceneNet approach obtains a superior performance on the other classes. Overall, the results are consistent with those of the previous two groups of experiments, in that the proposed SceneNet method can obtain a more suitable scene classification network architecture for the AID dataset than the deep neural networks designed by human experts, which can only extract fixed information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Test of the efficiency of the proposed SceneNet approach</head><p>In order to further test the efficiency of the SceneNet algorithm, a comparison of the efficiency of the human expert and the evolutionarysearched networks for the UCM dataset (SceneNet_UCM) and the NWPU45 dataset (SceneNet_NWPU) is given in Table <ref type="table" target="#tab_1">5</ref>. The corresponding metrics were all obtained using an NVIDIA Tesla P100 GPU, and the inputs were 224 × 224 images with a batch size of 16. Although the performances of the searched networks in both GFLOPs and speed are not the best, they are superior to VGG16. The proposed method focuses on automatically searching for a network architecture that is the most suitable for remote sensing image scene classification, which may not be considered in these aspects. However, the searched network results in the least amount of theoretical parameter size and GPU memory occupation, which could be used for on-board processing <ref type="bibr" target="#b0">(Alcolea et al., 2020)</ref>. Thus, the proposed method is still competitive in efficiency.</p><p>In addition, for the AID dataset (SceneNet_AID), the corresponding metrics for the state-of-art networks designed by human expert and the evolutionary-searched networks are presented in Table <ref type="table" target="#tab_2">6</ref>, where the inputs were 600 × 600 images with a batch size of 8. From the table, it can be found that the comparison results are similar to those obtained with the UCM and NWPU datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Parameter analysis of the population size NP in the evolutionary algorithm</head><p>The population size NP is the main parameter in the EA, so it was further analyzed to test the robustness of the proposed SceneNet approach. In order to achieve this, the population size NP was set to 5, 10, 15, 20, and 25 in the search stage for the UCM dataset, and 5, 10, 15, and 20 in the search stage for the NWPU45 and AID datasets, due to the larger amount of data. The final optimal solution individual for the scene classification network architecture was then selected from the Pareto optimal solution set. The OA and kappa coefficient were then calculated and output after the retraining stage, and five-fold cross-validation experiments were also conducted.</p><p>The test results for the three groups of datasets are presented in Fig. <ref type="figure" target="#fig_23">22</ref>, where the display form is similar to a box diagram. In addition to the presentation of the original test data, the median has been replaced by the mean value, and the quartiles and whiskers have been replaced by the standard error of the mean (SEM) and standard deviation (SD). From Fig. <ref type="figure" target="#fig_23">22</ref>, as the population size affects the search power to a certain extent, it can be found that the prediction accuracy increases with the increase of the population size, and then tends to be stable later. In general, the range of the precision variance is smaller and better than that of the deep neural networks designed by human experts. Therefore, the proposed SceneNet method has a certain stability for the population size NP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and discussion</head><p>In this article, in order to provide an evolutionary scene image classification network for remote sensing datasets, a nature-inspired multi-objective neural evolution method has been proposed (Scene-Net). Furthermore, the search space is encoded as natural chromosomes, with a more flexible and diverse presentation. The results obtained in this study demonstrated that the proposed SceneNet algorithm can provide a competitive Pareto optimal solution set of scene classification networks for different choices. Moreover, it was found that SceneNet can obtain a better scene image classification performance than several of the state-of-the-art deep CNNs designed by human experts, including AlexNet, VGG16, ResNet34, and GoogLeNet. In addition, the complexity of the network architecture can be controlled by the numbers of nodes and phases. Moreover, compared to the repetition of the computational blocks in the deep CNNs designed by human experts, SceneNet can provide more fine-grained control over the two objective functions through changing the connection modes in the phases.</p><p>In terms of the EA-based NAS for HSR remote sensing scene image classification tasks, many challenges still remain in the future research work, such as: 1) How to consider the transferability of the searched network. Can a large enough dataset be established for evolutionary search of the network so that it can be used for the direct prediction and classification of new image data? However, at present, the transferability of the searched network is still a problem that we need to overcome. 2) How to further improve the search space of the network itself. If more freedom for the neural evolution can be given when the computing equipment is advanced enough, more advanced networks could be obtained for HSR remote sensing scene image classification or other remote sensing dataset recognition tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of Competing Interest</head><p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.  A. Ma et al.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig. 1. Three classic CNN classification networks. (a) AlexNet. (b) VGG16. (c) ResNet34.</figDesc><graphic url="image-4.png" coords="3,92.07,55.42,411.12,280.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. The main procedure of an EA.</figDesc><graphic url="image-5.png" coords="3,120.47,576.76,354.38,149.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The overall flowchart of the proposed SceneNet method, which includes the three stages of input, search, and retraining. The dataset is divided into a training dataset and test dataset in the input stage, the multi-objective modeling and multi-objective neural evolution for scene classification in the proposed SceneNet method are undertaken in the search stage, and the retrained parameters for the selected network are obtained and the test accuracy and GFLOPs are output in the retraining stage.</figDesc><graphic url="image-6.png" coords="4,56.75,460.47,481.82,237.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The Pareto comparison between the individuals in the decision and objective spaces, and the display of the Pareto front.</figDesc><graphic url="image-7.png" coords="4,124.67,55.42,345.89,142.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Individual encoding and population initialization, with NP individuals being randomly initialized in a population.</figDesc><graphic url="image-8.png" coords="5,99.21,55.41,396.86,344.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Node definition in the phase and phase encoding. (a) The sequence of operations for each node: batch normalization and ReLU after 3 × 3 convolution. (b)The matrix representation of the binary coding for each phase in the network architecture: "1" denotes that the corresponding nodes are connected, while "0" indicates that the nodes are not connected.</figDesc><graphic url="image-9.png" coords="6,99.21,55.43,396.86,163.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Multi-objective ranking for individuals in a population. (a) Pareto ranking: the concept of Pareto domination is utilized for ranking the individuals in the population, until all the individuals are numbered. (b) Density ranking: calculation of the density between the i-th individual and the surrounding two solution individuals.</figDesc><graphic url="image-10.png" coords="7,124.78,566.39,345.74,140.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Exploration for evolutionary reproduction, including the crossover and mutation of the individuals in the population.</figDesc><graphic url="image-11.png" coords="7,92.13,55.44,410.98,144.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>function values of the test_error and FLOPs in the present population.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Exploitation by the Bayesian network. The histograms denote the conditional distributions between the phases in the scene classification network architecture in the long-distance search step and after updating in the short-distance search stage. Furthermore, in the exploitation, the scene classification networks are obtained from the Bayesian network.</figDesc><graphic url="image-12.png" coords="8,124.78,55.44,345.74,126.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Some scene image examples from the UCM dataset.</figDesc><graphic url="image-13.png" coords="9,92.13,55.42,410.98,228.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Examples of the evolutionary-searched competitive scene classification networks in the solution set for the UCM</figDesc><graphic url="image-14.png" coords="9,113.39,322.45,368.50,208.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Specific structures of the two searched scene classification neural networks for the UCM dataset.</figDesc><graphic url="image-15.png" coords="10,113.39,55.40,368.50,269.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. UCM dataset: The classification confusion matrix for the SceneNet algorithm.</figDesc><graphic url="image-16.png" coords="10,108.57,507.36,378.14,218.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Examples of the evolutionary-searched competitive scene classification networks in the solution set for the NWPU45 dataset.</figDesc><graphic url="image-17.png" coords="11,113.39,279.56,368.50,217.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Some scene image examples from the NWPU45 dataset.</figDesc><graphic url="image-18.png" coords="11,92.07,55.44,411.12,185.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Specific structures of the two searched scene classification neural networks for the NWPU45 dataset.</figDesc><graphic url="image-19.png" coords="12,113.39,55.41,368.50,232.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. NWPU45 dataset: The classification confusion matrix for the SceneNet algorithm.</figDesc><graphic url="image-20.png" coords="12,63.78,515.88,467.71,197.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. Some scene image examples from the AID dataset.</figDesc><graphic url="image-21.png" coords="13,92.13,55.39,410.98,212.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. Examples of the evolutionary-searched competitive scene classification networks in the solution set for the AID dataset.</figDesc><graphic url="image-22.png" coords="13,113.39,312.17,368.50,211.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Fig. 20 .</head><label>20</label><figDesc>Fig. 20. Specific structures of the two searched scene classification neural networks for the AID dataset.</figDesc><graphic url="image-23.png" coords="14,113.39,55.40,368.50,226.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Fig. 21 .</head><label>21</label><figDesc>Fig. 21. AID dataset: The classification confusion matrix for the SceneNet algorithm.</figDesc><graphic url="image-24.png" coords="14,63.78,535.74,467.71,187.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Fig. 22 .</head><label>22</label><figDesc>Fig. 22. Parameter analysis of the population size NP for the three groups of datasets.</figDesc><graphic url="image-25.png" coords="16,83.23,123.58,428.82,515.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>The parameter settings for the three tested datasets</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>UCM</cell><cell>NWPU45</cell><cell>AID</cell></row><row><cell>Search stage Retraining stage</cell><cell>Number of phases Number of nodes Population size Batch size Learning rate Momentum Weight decay Epoch_1 Batch size Learning rate Weight decay Epoch_2</cell><cell>3 8 20 32 0.03 0.9 3e−4 200 32 Cosine A. 3e−4 1000</cell><cell>3 8 10 32 0.03 0.9 3e−4 25 32 Cosine A. 3e−4 1000</cell><cell>3 8 10 8 0.03 0.9 3e−4 150 8 Cosine A. 3e−4 1000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 5</head><label>5</label><figDesc>Comparison of the efficiency of the human expert and evolutionary deep learning methods for the UCM and NWPU45 datasets</figDesc><table><row><cell>Model</cell><cell>Params (M)</cell><cell>Memory (M)</cell><cell>GFLOPs</cell><cell>Speed</cell></row><row><cell>AlexNet VGG16 ResNet34 GoogLeNet SceneNet_UCM SceneNet_NWPU45</cell><cell>61.106 138.377 21.259 6.646 1.076 1.021</cell><cell>1027 1857 945 875 526 516</cell><cell>0.724 15.45 4.15 1.51 10.03 9.47</cell><cell>1215 151 447 635 298 369</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 6</head><label>6</label><figDesc>Comparison of the efficiency of the human expert and evolutionary deep learning methods for the AID dataset</figDesc><table><row><cell>A. Ma et al.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Params (M)</cell><cell>Memory (M)</cell><cell>GFLOPs</cell><cell>Speed</cell></row><row><cell>AlexNet VGG16 ResNet34 GoogLeNet SceneNet_AID</cell><cell>61.106 138.377 21.259 6.646 1.060</cell><cell>5.597 10.121 5.150 4.769 2.870</cell><cell>5.04 110.21 26.75 10.73 54.08</cell><cell>292 37 107 148 98</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement This work was supported in part by the National Key Research and Development Program of China under Grant 2017YFB0504202, in part by the National Natural Science Foundation of China under Grant 41801267, 42071350, and 41771385, in part by the Fundamental Research Funds for the Central Universities grand NO. 2042020kf0014.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Inference in supervised spectral classifiers for on-board hyperspectral imaging: An overview</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alcolea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Paoletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Haut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Resano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Remote Sens. 12, 534</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-objective semi-supervised clustering for automatic pixel classification from remote sensing imagery</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Alok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ekbal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="4733" to="4751" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Binary patterns encoded convolutional neural networks for texture recognition and remote sensing scene classification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Anwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Molinier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laaksonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="74" to="85" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Land use classification in remote sensing images by convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Castelluccio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sansone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verdoliva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.00092</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lifelong machine learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="207" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Effective and efficient midlevel visual elements-oriented land-use classification using VHR remote sensing images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="4238" to="4249" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Remote sensing image scene classification: Benchmark and state of the art</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="1865" to="1883" />
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploring hierarchical convolutional features for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="6712" to="6722" />
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Remote sensing image scene classification using bag of convolutional features</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1735" to="1739" />
			<date type="published" when="2017">2017b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Remote sensing image scene classification meets deep learning: challenges, methods, benchmarks, and opportunities</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.01094</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">When deep learning meets metric learning: Remote sensing image scene classification via learning discriminative CNNs</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="2811" to="2821" />
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Evolutionary multi-objective optimization: a historical view of the field</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Coello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Intell. Mag</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="28" to="36" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploration and exploitation in evolutionary algorithms: A survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Crepinšek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mernik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv. (CSUR)</title>
		<imprint>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visual categorization with bags of keypoints</title>
		<author>
			<persName><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on statistical learning in computer vision, ECCV Vis</title>
				<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Evolutionary computation: a unified approach</title>
		<author>
			<persName><forename type="first">De</forename><surname>Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 on Genetic and Evolutionary Computation Conference Companion</title>
				<meeting>the 2016 on Genetic and Evolutionary Computation Conference Companion</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="185" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A fast and elitist multi-objective genetic algorithm: NSGA-II</title>
		<author>
			<persName><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Meyarivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="182" to="197" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Neural architecture search: A survey</title>
		<author>
			<persName><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A multiobjective cooperative coevolutionary algorithm for hyperspectral sparse unmixing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="234" to="248" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Diversity-promoting deep structural metric learning for remote sensing scene classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="371" to="390" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient color histogram indexing for quadratic form distance functions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Equitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Flickner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Niblack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="729" to="736" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A semi-supervised generative framework with deep learning features for high-resolution remote sensing image scene classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="23" to="43" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Pre-trained AlexNet architecture with pyramid pooling and supervision for high spatial resolution remote sensing image scene classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">848</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sens. 9</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Textural features for image classification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Dinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cyber. SMC</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="610" to="621" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">AutoML: A survey of the state-of-the-art</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.00709</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">AMC: AutoML for model compression and acceleration on mobile devices</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="784" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
				<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Auto-DeepLab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="82" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multisource compensation network for remote sensing cross-domain scene classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2019.2951779</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens. To be published</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A feature aggregation convolutional neural network for remote sensing scene classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="7894" to="7906" />
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">NSGA-Net: neural architecture search using multi-objective genetic algorithm</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Whalen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Boddeti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dhebar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Banzhaf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Genetic and Evolutionary Computation Conference</title>
				<meeting>the Genetic and Evolutionary Computation Conference</meeting>
		<imprint>
			<publisher>GECCO</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="419" to="427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multiobjective subpixel landcover mapping</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="422" to="435" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adaptive multiobjective memetic fuzzy clustering algorithm for remote sensing imagery</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="4202" to="4217" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards better exploiting convolutional neural networks for remote sensing scene classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Penatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patt. Recogn</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="539" to="556" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maenpaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">BOA: The Bayesian optimization algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pelikan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cantú-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Genetic and Evolutionary Computation Conference</title>
				<meeting>the Genetic and Evolutionary Computation Conference</meeting>
		<imprint>
			<publisher>GECCO</publisher>
			<date type="published" when="1999">1999. 1999</date>
			<biblScope unit="page" from="525" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
				<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Change detection based on multi-feature clustering using differential evolution for Landsat imagery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Remote Sens. 10, 1664</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multiobjective sparse subpixel mapping for remote sensing imagery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="4490" to="4508" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">MnasNet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Evolving deep neural networks by multiobjective particle swarm optimization for image classification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Genetic and Evolutionary Computation Conference</title>
				<meeting>the Genetic and Evolutionary Computation Conference</meeting>
		<imprint>
			<publisher>GECCO</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="490" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">RSNet: The search for remote sensing deep neural networks in recognition tasks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2020.3001401</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fully automatic spectral-spatial fuzzy clustering using an adaptive multiobjective memetic algorithm for multispectral imagery</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="2324" to="2340" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">AID: A benchmark data set for performance evaluation of aerial scene classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="3965" to="3981" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Genetic CNN</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision, ICCV</title>
				<meeting>the IEEE International Conference on Computer Vision, ICCV</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1379" to="1388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09926</idno>
		<title level="m">SNAS: stochastic neural architecture search</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi-objective based spectral unmixing for hyperspectral images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="54" to="69" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Bag-of-visual-words and spatial extensions for land-use classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems</title>
				<meeting>the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A review of evolutionary artificial neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="539" to="567" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Evolving artificial neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
				<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="1423" to="1447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A new evolutionary system for evolving artificial neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. neural net</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="694" to="713" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Hyperspectral band selection based on multiobjective optimization with high information and low redundancy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="604" to="621" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A spectral-structural bag-of-features scene classifier for very high spatial resolution remote sensing imagery</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="73" to="85" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Exploring semantic elements for urban scene recognition: Deep integration of high-resolution imagery and OpenStreetMap (OSM)</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tiede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Emery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="237" to="250" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Computational intelligence in optical remote sensing image processing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="75" to="93" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Bag-of-visual-words scene classifier with local and global features for high spatial resolution remote sensing imagery</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="747" to="751" />
			<date type="published" when="2016">2018. 2016</date>
		</imprint>
	</monogr>
	<note>A deep-local-global feature fusion framework for high spatial resolution imagery scene classification</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Semi-supervised center-based discriminative adversarial learning for cross-domain scene-level land-cover classification of aerial images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="72" to="89" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
				<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="middle">Q V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep learning based feature selection for remote sensing scene classification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. and Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2321" to="2325" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
