<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Convolutional Neural Network-Based Robot Navigation Using Uncalibrated Spherical Images †</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-06-12">12 June 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lingyan</forename><surname>Ran</surname></persName>
							<email>lingyanran@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanning</forename><surname>Zhang</surname></persName>
							<email>ynzhang@nwpu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qilin</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Highly Automated Driving Team</orgName>
								<orgName type="institution">HERE Technologies Automotive Division</orgName>
								<address>
									<postCode>60606</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Yang</surname></persName>
							<email>tyang@nwpu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vittorio</forename><forename type="middle">M N</forename><surname>Passaro</surname></persName>
						</author>
						<title level="a" type="main">Convolutional Neural Network-Based Robot Navigation Using Uncalibrated Spherical Images †</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-06-12">12 June 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">3F6397A604DE744AD688930F295A4589</idno>
					<idno type="DOI">10.3390/s17061341</idno>
					<note type="submission">Received: 23 February 2017; Accepted: 6 June 2017;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>convolutional neural networks</term>
					<term>vision-based robot navigation</term>
					<term>spherical camera</term>
					<term>navigation via learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision-based mobile robot navigation is a vibrant area of research with numerous algorithms having been developed, the vast majority of which either belong to the scene-oriented simultaneous localization and mapping (SLAM) or fall into the category of robot-oriented lane-detection/trajectory tracking. These methods suffer from high computational cost and require stringent labelling and calibration efforts. To address these challenges, this paper proposes a lightweight robot navigation framework based purely on uncalibrated spherical images. To simplify the orientation estimation, path prediction and improve computational efficiency, the navigation problem is decomposed into a series of classification tasks. To mitigate the adverse effects of insufficient negative samples in the "navigation via classification" task, we introduce the spherical camera for scene capturing, which enables 360 • fisheye panorama as training samples and generation of sufficient positive and negative heading directions. The classification is implemented as an end-to-end Convolutional Neural Network (CNN), trained on our proposed Spherical-Navi image dataset, whose category labels can be efficiently collected. This CNN is capable of predicting potential path directions with high confidence levels based on a single, uncalibrated spherical image. Experimental results demonstrate that the proposed framework outperforms competing ones in realistic applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Vision-based methods have been attracting a huge amount of research interest for decades in autonomous navigation on various platforms, such as quadrotors, self-driving cars, and ground robotics. Various camera sensors and algorithms have been incorporated in these platforms to improve the machine's sensing ability in challenging indoor and outdoor environments. For most applications, it is imperative to precisely localize the navigation path and detect potential obstacles. Among them, accurate position and orientation estimation is arguably the core task for mobile robot navigation.</p><p>One major category of navigation methods, the simultaneous localization and mapping (SLAM), build virtual 3D maps of the surroundings while tracking the location and orientation of the platform. During last two decades, SLAM and its derivative methods have been dominating the navigation research field. Various systems have been proposed, such as MonoSLAM <ref type="bibr" target="#b0">[1]</ref>, PTAM <ref type="bibr" target="#b1">[2]</ref>, FAB-MAP <ref type="bibr" target="#b2">[3]</ref>, DTAM <ref type="bibr" target="#b3">[4]</ref>, KinectFusion <ref type="bibr" target="#b4">[5]</ref>, etc. Besides the use of monocular cameras, Caruso et al. <ref type="bibr" target="#b5">[6]</ref> recently developed a SLAM system directly based on omnidirectional cameras and greatly expanded their applications. All the aforementioned SLAM systems share a common impediment to mobile platforms with limited computational capabilities, such as tablet PCs, quadrotors, and moving robotics as in our case (Figure <ref type="figure" target="#fig_0">1</ref>).</p><p>Another category of human vision inspired approaches, i.e., the robot-oriented heading-field road detection and trajectory planning methods, address the navigation problem directly with visual paths detection via local road segmentation <ref type="bibr" target="#b6">[7]</ref> and trajectory prediction <ref type="bibr" target="#b7">[8]</ref>, etc. For example, Lu et al. <ref type="bibr" target="#b8">[9]</ref> built a robust road detection system based on hierarchical vision sensor. Chang et al. <ref type="bibr" target="#b9">[10]</ref> presented a system using two biologically-inspired scene understanding models. In spite of their simplicity, the navigation performances of human vision inspired methods are heavily dependent on the quality of low level local features for their segmentation steps. In addition, for panorama images with heavy fisheye effect, traditional human vision inspired methods require prior calibration and warping preprocessing, further complicating these solutions.</p><p>In this paper, we focus on mobile robot navigation in open fields, with potential applications such as wheeled ground robots and low-flying quadrotors. This is a challenging problem: for one thing, such platforms lack computational capabilities required by SLAM-type algorithms and their typical tasks do not require sophisticated virtual 3D reconstruction. For another, those robots could be deployed outdoors with unpaved trails, where traditional path planning algorithms are likely to fail due to the increased difficulty in detecting unpaved surfaces.</p><p>As early as the 1990s, Pomerleau <ref type="bibr" target="#b10">[11]</ref> formulated the road following task as a classification problem. Decades later, Hadsell et al. <ref type="bibr" target="#b11">[12]</ref> developed a similar system for ground robot navigation in unknown environments. Recently, Giusti et al. <ref type="bibr" target="#b12">[13]</ref> framed the camera orientation estimation as a three-class classification (Left, Front and Right) and captured a set of forest trail images with 3 head-mounted cameras, each pointing in one direction. Given one frame input, their model can decide the next optimal move (left/right turn or keep forward). One major drawback of this system is the coarse steering decisions, three fixed cameras and three choices are not precise enough for applications with higher orientation accuracy requirements.</p><p>We propose to replace multiple monocular cameras in <ref type="bibr" target="#b12">[13]</ref> with a single spherical camera <ref type="bibr" target="#b13">[14]</ref>. Thanks to the imaging characteristic of spherical cameras, each image captures the 360 • panorama of the scene, eliminating the limitation on available steering choices.</p><p>One of the major challenges with spherical images is the heavy barrel distortion due to the ultra wide-angle fisheye lens, which complicates the implementation of conventional human vision inspired methods such as lane detection and trajectory tracking. Additional preprocessing steps such as prior calibration and dewarping are often required. However, in this paper, we circumvent these preprocessing steps by formulating navigation as a classification problem on finding the optimal potential path orientation directly based on the raw, uncalibrated spherical images.</p><p>Another challenge in the "navigation via classification" task is the shortage of negative training examples. Negative training samples typically represent wrong heading commands, which could lead to disasters such as collision. Inspired by <ref type="bibr" target="#b10">[11]</ref>, we reformulate navigation as classifying spherical images into a series of rotation categories. Unlike <ref type="bibr" target="#b10">[11]</ref>, negative training samples (wrong heading direction) could be conveniently generated by simple rotations of positive training samples (optimal heading direction), thanks to the 360 • fisheye panorama.</p><p>The contributions of this paper are as follows:</p><p>• A native "navigation via classification" framework based purely on 360 • fisheye panoramas is proposed in this paper, without the need of any additional calibration or unwarping preprocessing steps. Uncalibrated spherical images could be directly fed into the proposed framework for training and navigation, eliminating strenuous efforts such as pixel-level labeling of training images, or high resolution 3D point cloud generation for training.</p><p>• An end-to-end convolutional neural network (CNN) based framework is proposed, achieving extraordinary classification accuracy on our realistic dataset. The proposed CNN framework is significantly more computational efficient (in the testing phase) than SLAM-type algorithms and readily deployable on more mobile platforms, especially battery powered ones with limited computational capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>A novel 360 • fisheye panoramas dataset, i.e., the Spherical-Navi image dataset is collected, with a unique labeling strategy enabling automatic generation of an arbitrary number of negative samples (wrong heading direction). The rest of this paper is organized as follows: Section 2 reviews related literature on deep learning based navigation and spherical images based navigation. Section 3 presents our proposed "navigation via classification" framework based directly on 360 • fisheye panoramas. A novel fisheye panoramas dataset (Spherical-Navi image dataset), is introduced in Section 4 together with the evaluation of the proposed "navigation via classification" framework in Section 5. Finally, Section 6 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and Related Work</head><p>Numerous research efforts have been devoted to robot navigation since decades ago, and SLAM-type algorithms had been the preferable method until the recent trends in applying deep learning techniques in all low-level/mid-level computer vision tasks. Various classification methods (even with advanced and multisensory data, <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>) and radar based localization methods <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref> had not been competitive enough against SLAM-type algorithms, due to increased sensor complexity and mediocre recognition accuracy. The "navigation via classification" framework is made both feasible and attractive to researchers only after deep learning based methods dramatically improved the classification accuracy.</p><p>The current advent of General-Purpose computing on Graphics Processing Units (GPGPU) reduces the typical CNN training time to feasible levels (the total training time of the proposed network is approximately 20 h). The low computational cost of deployed CNN makes real-time processing easily attainable (the proposed network prototype achieves 100 fps without any sophisticated optimization).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Deep Learning in Navigation</head><p>Deep learning has shown its overwhelmingly advantages over conventional methods in many research areas, including object detection <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> and tracking <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, image segmentation <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, and hyper-spectral image classification <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> etc. With the success of AlexNet <ref type="bibr" target="#b28">[29]</ref> on ImageNet classification challenge <ref type="bibr" target="#b29">[30]</ref>, convolutional neural networks (CNNs) have become off-the-shelf solution for classification problems.</p><p>A number of improvements have been proposed over the years to further improve the classification performance of CNNs, such as the pioneering work <ref type="bibr" target="#b30">[31]</ref>, which shows the regularization efficiency of "Dropout", especially for exploring extremely large amount of parameters. Another example is Lin et al. <ref type="bibr" target="#b31">[32]</ref>, which enhances model discriminability for local patches within the receptive field by incorporating micro neural networks within complex structures.</p><p>Navigation based on classifying the surrounding scene images with neural networks has been explored as early as 1990s. The Autonomous Land Vehicle In a Neural Network (ALVINN) <ref type="bibr" target="#b10">[11]</ref> project is arguably one of the most influential ones, with realistic visual perception tasks and performance target of real-time processing. However, the tiny scale, oversimplified structure of early day neural networks, the primitive imaging sensors as well as abysmal computing power limited the usability of <ref type="bibr" target="#b10">[11]</ref> in reality.</p><p>Subsequently, many improvements to ALVINN have been proposed. Hadsell et al. <ref type="bibr" target="#b11">[12]</ref> developed a more stable system for navigation in unknown environments by incorporating a self-supervised learning framework capable of long-range sensing. This system is capable of accurately classifying complex terrains at distances up to the horizon (from 5 to over 100 m away from the platform, far beyond the maximum stereo range of 12 m), thus significantly improving path-planning.</p><p>Recently, Giusti et al. demonstrated a quadrotor platform autonomously following forest trails in <ref type="bibr" target="#b12">[13]</ref>. They formulated the optimization of heading orientation as a three-class classification problem (Left, Front and Right) and captured a series of forest trail images with 3 inboard cameras, each facing Left, Front and Right, respectively. Given one image frame, the deployed CNN model determines the optimal heading orientation among the three available choices: left turn, straight forward or right turn. The major drawback of this design is the limited number of choices of three (due to three cameras), which is a compromise between steering accuracy and quadrotor load capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Spherical Cameras in Navigation</head><p>There are a few published prior attempts on navigation based on spherical cameras, however, their performances are adversely affected by either rectification errors in pre-processing or lack of accurate reference frame.</p><p>First, considering the heavy barrel distortion due to the ultra wide-angle lens (e.g., omnidirectional cameras, fish-eye cameras, and spherical cameras), conventional navigation applications usually require pre-processing efforts such as calibration and rectification (i.e., removing fisheye effects). For example, Li <ref type="bibr" target="#b32">[33]</ref> proposed a calibration method for full-view spherical camera images. We argue that this pre-processing steps incur unnecessary computational complexity and accumulate errors thus we favor the alternative approach, i.e., navigation based directly on spherical images.</p><p>A related but subtly different research field, spherical rotation estimation, has been investigated as early as a decade ago. For example, Makadia et al. <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> estimated 3D spherical rotations via the transformations induced in the spectral domain, and directly via the sphere images without correspondence, respectively. A recent paper by Bazin et al. <ref type="bibr" target="#b35">[36]</ref> estimated spherical rotations based on vanishing points in omnidirectional images. Caruso et al. <ref type="bibr" target="#b5">[6]</ref> proposed an image alignment method based on a unified omnidirectional model, achieving fast and accurate incremental stereo matching based directly on curvilinear, wide-angled images.</p><p>For the former "calibration and rectification" based methods, the error-accumulating pre-processing step would be eliminated if raw spherical images are directly used for navigation. For the latter group of methods, a major difference of these "spherical rotations estimation" attempts from the navigation tasks is the requirement of reference image frame: in rotation estimation problems, an estimated rotation angle is evaluated with respect to the reference image frame; however, reference image frames are almost never readily available in robot navigation applications. To overcome these limitations, a highly accurate, raw spherical image based "navigation via classification" framework is proposed in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CNN Based Robot Navigation Framework Using Spherical Images</head><p>Deep convolutional neural networks have been widely used in many computer vision and image sensing tasks, e.g., object detection <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, semantic segmentation <ref type="bibr" target="#b24">[25]</ref>, and classification <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>. In this section, a convolutional neural network based robot navigation framework is formulated to accurately estimate robot heading direction using raw spherical images. Figure <ref type="figure" target="#fig_0">1</ref> illustrates our capturing hardware platform, with a spherical camera mounted on a wheeled robot capable of capturing 360 • fisheye panoramas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Formulation: Navigation via Classification</head><p>Given a series of N spherical images x 1 , • • • , x N sampled at time instances 1, • • • , N, respectively, the target of robot navigation can be formulated as the estimation of the optimal discrete heading direction {y n } N n=1 ∈ Y, with Y being some predefined turning options determined by robot tasks and complexity settings. Without loss of generality, let</p><formula xml:id="formula_0">Y = {Y 0 , Y ±1 , • • • , Y ±K }<label>(1)</label></formula><p>where positive and negative k values represent anticlockwise and clockwise turn, respectively. Larger k values denote larger turning angles. The cardinality of Y (i.e., 2K + 1, the number of navigation choices) could be conveniently set to satisfy the turning precision requirements of any given robot navigation application. Specifically, define Y 0 = 0 • as the option to keep the current heading direction (straight forward).</p><p>With this model, the navigation task is the minimization of the global penalty L over the entire</p><formula xml:id="formula_1">time instance range n = 1, • • • , N, L = N ∑ n=1 {1 -δ( ŷn , y n )}<label>(2)</label></formula><p>in which ŷn = F(x n ; w, b) denotes network prediction at time instance n based on spherical image data x n , where F(x; w, b) is a non-linear warping function learned with a convolutional neural network, w and b being the weights and bias terms, respectively. y n is the ground truth denoting the manually marked, optimal heading direction; and δ( ŷn , y n ) is the Kronecker delta,</p><formula xml:id="formula_2">δ( ŷn , y n ) = 0 if ŷn = y n , 1 if ŷn = y n .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Configuration and Training</head><p>Inspired by Alexnet <ref type="bibr" target="#b28">[29]</ref> and Giusti et al. <ref type="bibr" target="#b12">[13]</ref>, a new convolutional neural network based robot navigation framework for the spherical images is proposed as shown in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>Following the naming convention of current popular convolutional neural networks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29]</ref>, convolutional layers (Conv), pooling layers (Pool) and fully connected layers (FC) are illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. The proposed network consists of four convolutional layers, three pooling layers, and two fully connected layers. Each convolutional layer is coupled with a max-pooling layer to enhance the local contrast.</p><p>Table <ref type="table" target="#tab_0">1</ref> summarizes the network parameter settings of each layer of three networks, i.e., the baseline Giusti <ref type="bibr" target="#b12">[13]</ref> network, the proposed "Rectified Linear Units" (ReLU) <ref type="bibr" target="#b38">[39]</ref> based Model 1 network and another proposed "Parametric Rectified Linear Units" (PReLU) <ref type="bibr" target="#b39">[40]</ref> based Model 2 network. Giusti et al. incorporated the scaled hyperbolic tangent activation function (Tanh) in <ref type="bibr" target="#b12">[13]</ref> but did not provide the rationale behind this specific choice of non-linear warping function. In our experimental evaluation, we observe that both the ReLU based Model 1 network and the PReLU based Model 2 network outperform the Tanh units based one.  Optimizing a deep neural network is not trivial due to the gradient vanishing/exploding problem. In addition, it is also possible that optimization got stuck in a saddle point, resulting premature termination and inferior low level features. This becomes especially challenging for spherical images, due to their similar visual appearance.</p><p>To combat the aforementioned challenges, the Batch Normalization (BN) <ref type="bibr" target="#b40">[41]</ref> is incorporated in the Model 2 network as shown in Table <ref type="table" target="#tab_0">1</ref>, which forces the network's activations to generate larger variances across different training samples, accelerating the optimization in the training phase and also achieving a better classification performance.</p><p>During the training phase, both the Models 1 and 2 networks are optimized with the adaptive subgradient online learning (Adagrad) optimizer <ref type="bibr" target="#b41">[42]</ref>, allowing the derivation of strong regret guarantees. In addition, online regret bounds can be converted into a rate of convergence and generalization bounds. The usage of Adagrad optimization method eliminates the need of tuning the learning rates and momentum hyper-parameters as in the stochastic gradient decent (SGD) methods <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Spherical-Navi Dataset</head><p>A dataset with balanced class distributions is crucial for the effective training of a classification model. A common pitfall of navigation training dataset is the shortage of negative training samples. The negative samples typically represent wrong heading directions and could lead to accidents such as collision, hence they are not sufficiently collected in practice (to avoid damage to robot platform).</p><p>Inspired by <ref type="bibr" target="#b12">[13]</ref>, we propose to use spherical images for address this challenge. For one thing, every single spherical camera is capable of capturing a 360 • fisheye panorama, covering all possible heading directions, including wrong ones. For another thing, negative training samples could be conveniently generated by directly annotate the same 360 • fisheye panorama with an arbitrary number of wrong heading directions.</p><p>The following part of this section provides details on the proposed spherical image dataset, which is flexible with arbitrary number (2K + 1 as in Equation ( <ref type="formula" target="#formula_0">1</ref>)) of navigation choices (At each time instance n, heading directions</p><formula xml:id="formula_3">y n = -Y K , • • • , 0, • • • , Y K</formula><p>are all potential navigation choices).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Formulation</head><p>As shown in Figure <ref type="figure" target="#fig_0">1B</ref>, an upward-facing spherical camera captures its 360 • surroundings and maps the scene into a non-rectilinear image. These spherical images share one distinctive characteristic, i.e., azimuth rotations of these cameras only lead to a simple two dimensional rotation of their captured images, as shown in Figure <ref type="figure" target="#fig_4">3</ref>. The robot platform rotates from +70 • to -70 • , the captured spherical images (Figure <ref type="figure" target="#fig_4">3a-g</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Data Capturing</head><p>A robot platform shown in Figure <ref type="figure" target="#fig_0">1A</ref> is used to collect images for training. The upward-facing spherical camera is mounted on top of the platform with a clearance of approximately 1.9 m above the ground, where the typical occlusions such as those caused by pedestrians and parked vehicles are rare. In total, we have captured ten video sequences with the robot platform traversing the school campus of Northwestern Polytechnical University, Xi'an, Shaanxi, China. The videos are captured at 60 frames per second with a resolution of 4608 × 3456. To increase the variety of the scenes in this dataset, navigation paths have been manually designed to cover as many feasible locations as possible.</p><p>In addition, we also designed some overlapping path segments in these video sequences to discourage machine learning algorithms from simply "memorizing the routes". Figure <ref type="figure" target="#fig_5">4</ref> shows typical example images in this dataset (Videos are publicly avaliable online at: https://hijeffery.github.io/PanoNavi). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Data Preparation</head><p>Due to the movement of the robot platform and lack of image stabilization, vibration could deteriorate a small fraction of video frames significantly. Therefore, the local temporal relative sharpness measurement V i,p <ref type="bibr" target="#b43">[44]</ref> is incorporated to reject low quality image frames,</p><formula xml:id="formula_4">V i,p = ∑ q∈N (p) ∇J i,q 1 ∑ M j=1 ∑ q∈N (p) ∇J j,q 1 +<label>(4)</label></formula><p>where V i,p is a normalized local sum of gradient magnitudes, with J i denoting i-th frame in a sequence of M frames, J i,p as its p-th pixel, N (p) as the set of spatially neighboring pixels of p, and the temporal relative sharpness of frame J i with P pixels is measured as the mean of local relative sharpness:</p><formula xml:id="formula_5">V i = ∑ P p=1 V i,p P<label>(5)</label></formula><p>Additionally, temporal subsampling is carried out to reduce the very high temporal correlation between consecutive frames, since the video capturing frame rate is relatively high given the limited maximum speed of the robot platform. Without loss of generality, two video frames are randomly sampled (without replacement) per second (from the original 60 frames, less blurry frames that fail the Equation ( <ref type="formula" target="#formula_5">5</ref>) criterion, if any). Six video sequences (with a total of 2000 frames) are randomly selected as training data; while the remaining 1500 frames from the other four video sequences are kept as testing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Label Synthesis</head><p>In the proposed "navigation via classification" framework, the optimal heading direction at time instance n is</p><formula xml:id="formula_6">y n ∈ {-Y K , • • • , 0 • , • • • , Y K } ,<label>(6)</label></formula><p>with Y K denoting the maximum steering angle (in degrees) permitted by the robot platform. In our experimental settings, Y K = 90 • (anticlockwise turn) and Y -K = -90 • (clockwise turn). Collection of the positive labels (i.e., spherical images with correct heading direction) is trivial: manual inputs from the remote control are directly paired with the corresponding video frame. As is shown in Figure <ref type="figure" target="#fig_4">3</ref>, azimuth rotations of an upward-facing spherical camera only lead to a planer rotation about the ground normal (z axis, which is perpendicular to the horizon). Therefore, negative label could be easily synthesized.</p><p>After the robot platform has finished one capturing drive (without crashing) under manual remote control, the manual navigation inputs {y n } N n=1 inputted by human are used directly as positive training labels (Positive training samples are image-label pairs denoting optimal heading direction, the label y n itself does not have a 'positive' degree value. By definition in Equation ( <ref type="formula" target="#formula_6">6</ref>), y n = Y 0 = 0 • ). More importantly, arbitrary number of negative samples could be conveniently synthesized at virtually no risk or cost at each time instance n by assigning alternative values to {y n } N n=1 . Figure <ref type="figure" target="#fig_6">5</ref> illustrates the synthesis of negative labels with various k values (k = ±1, • • • , ±K, larger k denotes larger offset from the optimal heading direction).</p><p>To minimize the dataset bias <ref type="bibr" target="#b44">[45]</ref>, most of the synthesized image-label pairs are sampled adjacent to the optimal heading direction (i.e., with small Y k values). In this way, the training set is statistically better matched with real navigation scenarios and empirically leads to a lower probability of consecutive contradictory steering decisions. More image-label pairs are synthesized with small rotation angles (small k values) with respect to the optimal heading direction, in order to enhance the navigation "inertia", i.e., to avoid frequent, unnecessary drastic steering adjustments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Sky Pixels Elimination</head><p>The proposed robot platform collects data under various illumination conditions, due to different time-of-day and weather. Before feeding the spherical images into training networks, the central sky pixels (within a predefined radius) are masked out. Empirically, we found that these sky pixel values are heavily susceptible to illumination changes and our network gains 1%+ overall classification accuracy if these sky pixels are masked out. Subsequently, spherical images are normalized in the YUV color space to achieve zero mean and unit variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Network Setup and Training</head><p>Three algorithms are compared on the proposed Spherical-Navi dataset in Table <ref type="table" target="#tab_0">1</ref>. All of them share identical convolutional layers with the filter size 4. Their following pooling layers are of "Max-pooling" type, which select the local maximum values from the winning neurons.</p><p>We follow the training configurations in Giusti et al. <ref type="bibr" target="#b12">[13]</ref>, with weights initialized as in <ref type="bibr" target="#b39">[40]</ref> and biases initialized by zeros. During the training procedure, a higher initial learning rate (10 -4 ) is selected for the proposed "Model 1" than that (10 -5 ) in the proposed "Model 2". When the training loss stops decreasing, the learning rate is adjusted to one-tenth of the previous one. For better generalization to the testing phase, a mini-batch of size 10 is incorporated and all training samples are shuffled before each epoch. The training losses are illustrated against epoch in Figure <ref type="figure" target="#fig_7">6</ref>, where our "Model 2" with batch normalization achieves significantly faster convergence to "better" local minima with smaller training loss value.</p><p>The proposed "Model 1" and "Model 2" algorithms are developed with the Torch7 deep learning package <ref type="bibr" target="#b45">[46]</ref> and the respective network parameters are summarized in Table <ref type="table" target="#tab_0">1</ref>. With the proposed models and Spherical-Navi Dataset, all training procedures finish within 3 days using a PC with one Intel Core-i7 3.4 GHz CPU, or less than 20 h with a PC equipped with one Nvidia Titan X GPU. During the testing procedure, it takes the Nvidia Jetson TK1 installed onboard the robot platform only 10 milliseconds to process each spherical image. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Quantitative Results and Discussion</head><p>Table <ref type="table">2</ref> summarizes the overall classification accuracies among competing algorithms with different number of navigation choices (i.e., 2K + 1 as in Equation ( <ref type="formula" target="#formula_0">1</ref>)). The LIBSVM <ref type="bibr" target="#b46">[47]</ref> software with default settings (RBF kernel, C = 1, γ = 0.04) is chosen to implement the popular Support Vector Machine (SVM) classifier as a competing baseline. All deep learning based algorithms have achieved evident performance gains against the SVM baseline in various K settings. Generally, with more navigation choices (larger K), the classification accuracies drop for all competing algorithm, due to the increased complexity in the multiclass problem. Another factor that might contribute to imperfect classification is the camera mounting calibration, there could be some small rotating movements in the spherical camera during the capture process due to vibration.</p><p>Additionally, Figure <ref type="figure">7</ref> provides the multi-class classification confusion matrix <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref> with 7 navigation choices (2K + 1 = 7, last row in Table <ref type="table">2</ref>). With more navigation choices, spherical images from adjacent heading directions appear even more visually similar. The misclassification of adjacent choices leads to relatively larger sub-diagonal and super-diagonal values than other off-diagonal elements. We also note that while the robot platform is moving along a long stretch of straight path with non-distinctive scenes, Left3 view (leftmost view with y n perpendicular to drive path) appears to be a horizontal/vertical flip of Right3 view (rightmost view with y n perpendicular to drive path). This visual similarity could contribute to the slightly higher value in the upper-right element in the confusion matrix in Figure <ref type="figure">7</ref>. Deep learning based methods can be generally regarded as a superbly discriminative feature extractor, and Figure <ref type="figure" target="#fig_9">8</ref> illustrates the progressive discriminability enhancement procedure layer after layer. Class-wise aggregated 8000 sample training images are fed into the proposed "Model 2" network with 7 navigation choices (K = 3). The input of FC1 layer (i.e., the output from the last CONV layer), the output of FC1 layer (i.e., the input of FC2 layer) and the output of FC2 layer are visualized in Figure <ref type="figure" target="#fig_9">8a-c</ref>, respectively. As the dimension of features decreases from 288 in "FC1 input" to 7 in "FC2 output" (Please refer to the last column in Table <ref type="table" target="#tab_0">1</ref> for the layer structure of the proposed "Model 2" network), features are condensed into more compact and discriminative format, which is visually verifiable by inspecting the distinctive patterns in Figure <ref type="figure" target="#fig_9">8c</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Robot Navigation Evaluation</head><p>The robot navigation performance of the proposed "Model 2" network (with 7 navigation choices) is evaluated in this section via multiple navigation tasks.</p><p>In the first simulated evaluation, a special test drive path (covering all possible heading directions) is manually selected, visualized in Figure <ref type="figure" target="#fig_10">9a</ref> and discretized in Figure <ref type="figure" target="#fig_10">9b</ref> to match the number of available navigation choices. The robot platform (as shown in Figure <ref type="figure" target="#fig_0">1A</ref>) is subsequently deployed with the exact navigation manual input in Figure <ref type="figure" target="#fig_10">9b</ref> and a series of evaluation spherical images are collected correspondingly. These spherical images are then fed into the trained network as testing images and the predicted heading directions are visualized in Figure <ref type="figure" target="#fig_10">9c</ref>. The overall average prediction accuracy in Figure <ref type="figure" target="#fig_10">9c</ref> is 87.3%, as compared to the ground truth in Figure <ref type="figure" target="#fig_10">9b</ref>. Most of the misclassification errors happen during confusing the adjacent heading direction classes, which is understandable given the spatial similarity of typical scenes.</p><p>In the following real-world navigation evaluation, the robot platform (as shown in Figure <ref type="figure" target="#fig_0">1A</ref>) is deployed in the Jing-Wu Garden (as shown in Figure <ref type="figure" target="#fig_11">10</ref>) inside the campus of Northwestern Polytechnical University, Xi'an, Shaanxi, China. The tested paths cover both paved walking trails and unpaved surfaces (mostly lawn). Training data collection (Training data collection is illustrated in Figure <ref type="figure" target="#fig_12">11a</ref>,d,g and Figure <ref type="figure" target="#fig_13">12a</ref>,d,g): the robot platform is manually controlled to drive along 3 pre-defined paths multiple times, and the collected spherical images with synthesized optimal heading directions (detailed in Section 4.4) are used for "Model 2" network training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Navigation with raw network predictions (Navigation with raw network predictions is illustrated in Figure <ref type="figure" target="#fig_12">11b</ref>,e,h) and Figure <ref type="figure" target="#fig_13">12b</ref>,e,h): the robot platform is deployed at the starting point of each trail and it autonomously navigate along the path with raw network predictions as inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>Navigation with smoothed network predictions (Navigation with smoothed network predictions is illustrated in Figure <ref type="figure" target="#fig_12">11c</ref>,f,i) and Figure <ref type="figure" target="#fig_13">12c</ref>,f,i): the robot platform is deployed at the starting point of each trail and it autonomously navigate along the path with smoothed network predictions as inputs. The smoothing is carried out with a temporal median filter of size 3.</p><p>The ideal heading directions and ideal two-dimensional trails are shown in Figure <ref type="figure" target="#fig_12">11a</ref>,d,g and Figure <ref type="figure" target="#fig_13">12a</ref>,d,g, respectively. A total of 7 heading direction choices are available at each frame, with 0 for straight forward, positive values for right turns and negative values for left turns. While navigating through corners, a series of consecutive small turning maneuvers (multiple +1 and -1 heading directions in Figures <ref type="figure" target="#fig_12">11</ref> and<ref type="figure" target="#fig_13">12</ref>) are preferred over sharp turns, allowing more training samples to be collected during these maneuvering frames.</p><p>Figure <ref type="figure" target="#fig_12">11b</ref>,e,h demonstrate the raw predictions out of the "Model 2" network. Overall, vast majority of predictions are accurate for the straight forward (heading direction 0) sequences; while small portions of turning maneuvers are overestimated (with predicted heading directions ±2 and ±3). This could arise from the ambiguity of consecutive small turns and a single sharp turn achieving identical drive trail. In addition, there are only very subtle appearance differences during the limited number of frames while making turning maneuvers, which could result in confusions. In conjunction with the sporadic appearances of pedestrians, these confusions could lead to the spurious heading directions with excessive values (±2 and ±3). To remedy the situation, temporal coherence of heading directions need to be addressed. Empirically, a naive temporal median filter with window size 3 is effective enough to remove most spurious results, as shown in Figure <ref type="figure" target="#fig_12">11c</ref>,f,i. Figure <ref type="figure" target="#fig_13">12</ref> demonstrates the corresponding 2D trails of Figure <ref type="figure" target="#fig_12">11</ref>. A few overestimated turning maneuvers in Figure <ref type="figure" target="#fig_12">11b</ref>,e,h lead to wildly different trails (Figure <ref type="figure" target="#fig_13">12b</ref>,e,h) from the ideal ones (Figure <ref type="figure" target="#fig_13">12a,</ref><ref type="figure">d,</ref><ref type="figure">g</ref>). However, the smoothing-with-median-filtering remedy is highly successful in Figure <ref type="figure" target="#fig_13">12c</ref>,i, only with Figure <ref type="figure" target="#fig_13">12f</ref> showing an obvious difference from the Figure <ref type="figure" target="#fig_13">12d</ref> towards the end of Test 2. A demonstration video is available online (Video demo: https://www.youtube.com/watch? v=4ZjnVOa8cKA.) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, a Convolutional Neural Network-based robot navigation framework is proposed to address the drawbacks in conventional algorithms, such as intense computational complexity in the testing phase and difficulty in collecting high quality labels in the training phase. The robot navigation task is formulated as a series of classification problems based on uncalibrated spherical images. The unique design of training data preparation eliminates time-consuming calibration and rectilinear correction processes, and enables automatic generation of an arbitrary number of negative training samples for better performance.</p><p>One potential improvement direction is the incorporation of temporal information via Recurrent Neural Networks (RNNs)/Long Short Term Memory networks (LSTMs). In addition, there are also multiple related problems for future research, such as indoor navigation and off-road collision avoidance. Source codes of the proposed methods and the Spherical-Navi dataset are available for download on our project web page (Project page: https://hijeffery.github.io/PanoNavi/).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. (A) A spherical camera mounted on a ground robot platform. (B) The spherical camera coordinate systems and its imaging model. Natural scenes are warped into the circular fisheye image. (C) Samples of captured spherical images. Red arrows denote the detected optimal path. Our objective is to generate navigation signals (denoted by blue arrows, i.e., steering direction and angles) based directly on these 360 • fisheye panoramas.</figDesc><graphic coords="3,127.56,214.95,340.16,318.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Proposed convolutional neural network based navigation framework. It consists of four convolutional layers, three pooling layers, and two fully connected layers. The inputs are raw, uncalibrated spherical images and the outputs are navigation signals (steering direction and angles).</figDesc><graphic coords="6,208.94,608.85,75.84,75.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>) are corresponding 2-dimensional rotations of each other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Sample images when an upward-facing spherical camera rotates. The central bottom pixels are the current heading direction of the robot platform and the red arrows represent one specific potential direction. The azimuth rotations of the robot platform merely lead to corresponding 2-dimensional rotations of the spherical images.</figDesc><graphic coords="7,141.04,587.45,62.64,62.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Sample images from the proposed Spherical-Navi dataset. Data collection is carried out throughout the school campus of Northwestern Polytechnical University, Xi'an, Shaanxi, China, and under different illumination conditions (i.e., different weather, time-of-day).</figDesc><graphic coords="8,98.65,254.69,397.96,195.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Negative Label Synthesis. Red arrow denotes the optimal heading direction, i.e., manual input from a remote control. Negative labels are rotations of this optimal heading direction, k = ±1, • • • , ±K.More image-label pairs are synthesized with small rotation angles (small k values) with respect to the optimal heading direction, in order to enhance the navigation "inertia", i.e., to avoid frequent, unnecessary drastic steering adjustments.</figDesc><graphic coords="9,141.73,473.78,311.78,137.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Training losses of competing models with the Adagrad optimizer. The learning rate is decreased by a factor of ten every 500 epochs. Our proposed "Model 2" with "Batch Normalization" achieves the fastest convergence and the lowest training loss.</figDesc><graphic coords="10,177.17,412.99,240.95,180.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Table 2 .Figure 7 .</head><label>27</label><figDesc>Figure 7. Classification confusion matrix filled with normalized accuracies in the case of 7 navigation choices. Diagonal elements denote correct classification; while off-diagonals denote mistakes.</figDesc><graphic coords="11,201.10,356.55,206.87,165.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Extracted features from different layers in the proposed "Model 2" network with ordered 8000 sample training images and 7 navigation choices (X-axis: feature dimension; Y-axis: index of samples, best viewed in color). The network nonlinear mappings of fully connected layer 1 (FC1) ((a)→(b)) and FC2 ((b)→(c)) enhance the discriminability one after another.</figDesc><graphic coords="12,81.65,97.85,143.99,144.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Robot Navigation Simulation. X-axis: frame index; Y-axis: sine of heading direction angle. The ideal, manually selected navigation path in (a) covers the entire possible heading direction angle y n , from 0 • to 90 • , then -90 • and back to 0 • . Therefore, sin(y n ) covers [-1, 1]. (b) Limited by the 7 navigation choices, the robot navigation inputs are discretized. (c) visualizes the network predictions based purely on the collected spherical images.</figDesc><graphic coords="12,92.44,481.52,136.80,103.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Jing-Wu Garden, inside the campus of Northwestern Polytechnical University, Xi'an, Shaanxi, China, where the navigation of the robot platform was tested.</figDesc><graphic coords="13,155.91,127.12,283.46,159.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Robot navigation heading directions in 3 separate tests. X-axis: frame index; Y-axis: heading direction choices (0, ±1, ±2, ±3, positive values for right turns and negative values for left turns). (a,d,b) are manually labeled ideal heading directions; (b,e,h) are corresponding raw predictions from the proposed "Model 2" network; (c,f,i) are smoothed predictions using a temporal median filter of size 3.</figDesc><graphic coords="14,76.54,397.37,151.20,113.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Robot navigation 2D trails in 3 separate tests. (a,d,g) are manually selected ideal drive trails; (b,e,h) are trails with raw network predictions as navigation inputs; (c,f,i) are trails with smoothed network predictions as navigation inputs.</figDesc><graphic coords="15,76.54,431.38,151.20,113.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparsions of network structure and layer parameters of convolutional neural networks (CNNs).</figDesc><table><row><cell></cell><cell>Features</cell><cell cols="5">Giusti [13] Proposed Model 1 Proposed Model 2</cell></row><row><cell>0</cell><cell>3 × 101 × 101</cell><cell>Inputs</cell><cell>Inputs</cell><cell></cell><cell cols="2">Inputs</cell></row><row><cell>1</cell><cell>32 × 98 × 98</cell><cell>Conv</cell><cell>Conv</cell><cell></cell><cell></cell><cell>Conv</cell></row><row><cell>2</cell><cell>32 × 49 × 49</cell><cell>Pool</cell><cell>Pool</cell><cell></cell><cell></cell><cell>Pool</cell></row><row><cell>3</cell><cell>-</cell><cell>Tanh</cell><cell>ReLU</cell><cell></cell><cell cols="2">BN + PReLU</cell></row><row><cell>4</cell><cell>32 × 46 × 46</cell><cell>Conv</cell><cell>Conv</cell><cell></cell><cell></cell><cell>Conv</cell></row><row><cell>5</cell><cell>32 × 23 × 23</cell><cell>Pool</cell><cell>Pool</cell><cell></cell><cell></cell><cell>Pool</cell></row><row><cell>6</cell><cell>-</cell><cell>Tanh</cell><cell>ReLU</cell><cell></cell><cell cols="2">BN + PReLU</cell></row><row><cell>7</cell><cell>32 × 20 × 20</cell><cell>Conv</cell><cell>Conv</cell><cell></cell><cell></cell><cell>Conv</cell></row><row><cell>8</cell><cell>32 × 10 × 10</cell><cell>Pool</cell><cell>Pool</cell><cell></cell><cell></cell><cell>Pool</cell></row><row><cell>9</cell><cell>-</cell><cell>Tanh</cell><cell>ReLU</cell><cell></cell><cell cols="2">BN + PReLU</cell></row><row><cell cols="2">10 32 × 7 × 7</cell><cell>Conv</cell><cell>Conv</cell><cell></cell><cell></cell><cell>Conv</cell></row><row><cell cols="2">11 32 × 3 × 3</cell><cell>Pool</cell><cell>Pool</cell><cell></cell><cell></cell><cell>Pool</cell></row><row><cell cols="2">12 -</cell><cell>Tanh</cell><cell>ReLU</cell><cell></cell><cell cols="2">BN + PReLU</cell></row><row><cell cols="2">13 288→200</cell><cell>FC1</cell><cell>FC1</cell><cell></cell><cell></cell><cell>FC1</cell></row><row><cell cols="2">14 -</cell><cell>Tanh</cell><cell>ReLU</cell><cell></cell><cell cols="2">PReLU</cell></row><row><cell cols="2">15 200→K</cell><cell>FC2</cell><cell>FC2</cell><cell></cell><cell></cell><cell>FC2</cell></row><row><cell>Input</cell><cell>Conv1</cell><cell></cell><cell>Conv2</cell><cell cols="2">Conv3</cell><cell>Conv4</cell><cell>FC1 FC2</cell></row><row><cell></cell><cell></cell><cell>Pool1</cell><cell cols="2">Pool2</cell><cell cols="2">Pool3</cell></row><row><cell>3*101*101</cell><cell cols="2">32*98*98</cell><cell>32*23*23</cell><cell cols="3">32*10*10 32*7*7 200</cell><cell>K</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Sensors 2017, 17, 1341; doi:10.3390/s17061341 www.mdpi.com/journal/sensors</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This work is supported by the National Natural Science Foundation of China (No. 61672429, No. 61272288, No. 61231016), The National High Technology Research and Development Program of China (863 Program) (No. 2015AA016402), ShenZhen Science and Technology Foundation (JCYJ20160229172932237), Northwestern Polytechnical University (NPU) New AoXiang Star (No. G2015KY0301), Fundamental Research Funds for the Central Universities (No. 3102015AX007), NPU New People and Direction (No. 13GH014604).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Author Contributions: L.R. and T.Y. conceived and designed the experiments and analysed the data; L.R. wrote the manuscript; Y.Z., and Q.Z. analysed the data and updated the manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of Interest:</head><p>The authors declare no conflict of interest. The founding sponsors had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript, and in the decision to publish the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abbreviations</head><p>The following abbreviations are used in this manuscript:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN</head><p>Convolutional Neural Networks SLAM Simultaneous Localization And Mapping</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MonoSLAM: Real-time single camera SLAM</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Molton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Stasse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1052" to="1067" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Parallel tracking and mapping for small AR workspaces</title>
		<author>
			<persName><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality</title>
		<meeting>the 2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality<address><addrLine>Nara, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-11">November 2007</date>
			<biblScope unit="page" from="225" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Probabilistic localization and mapping in the space of appearance</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><surname>Fab-Map</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Robot. Res</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="647" to="665" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">DTAM: Dense tracking and mapping in real-time</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-11">November 2011</date>
			<biblScope unit="page" from="2320" to="2327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">KinectFusion: Real-time dense surface mapping and tracking</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th IEEE International Symposium on Mixed and Augmented Reality (ISMAR)</title>
		<meeting>the 10th IEEE International Symposium on Mixed and Augmented Reality (ISMAR)<address><addrLine>Basel, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-10">October 2011</date>
			<biblScope unit="page" from="127" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large-Scale Direct SLAM for Omnidirectional Cameras</title>
		<author>
			<persName><forename type="first">D</forename><surname>Caruso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting>the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)<address><addrLine>Hamburg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-10-02">28 September-2 October 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recent progress in road and lane detection: A survey</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Hillel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Raz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vis. Appl</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="727" to="745" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive Image-Based Trajectory Tracking Control of Wheeled Mobile Robots With an Uncalibrated Fixed Camera</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Control Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2266" to="2282" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vision sensor-based road detection for field robot navigation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="29594" to="29617" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mobile robot vision navigation &amp; localization using gist and saliency</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Siagian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting>the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-10">October 2010</date>
			<biblScope unit="page" from="4147" to="4154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient training of artificial neural networks for autonomous navigation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Pomerleau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning long-range vision for autonomous off-road driving</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Scoffier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Field Robot</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="120" to="144" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots</title>
		<author>
			<persName><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fontana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Faessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Di Caro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robot. Autom. Lett</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="661" to="667" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Autonomous Wheeled Robot Navigation with Uncalibrated Spherical Images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Conference on Intelligent Visual Surveillance</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="47" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-View Visual Recognition of Imperfect Testing Data</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual ACM Conference on Multimedia Conference</title>
		<meeting>the 23rd Annual ACM Conference on Multimedia Conference<address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-10-30">26-30 October 2015</date>
			<biblScope unit="page" from="561" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Auxiliary Training Information Assisted Visual Recognition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IPSJ Trans. Comput. Vis. Appl</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="138" to="150" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Can Visual Recognition Benefit from Auxiliary Information in Training?</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ACCV</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014-01-05">2014. 1-5 November 2014</date>
			<biblScope unit="volume">9003</biblScope>
			<biblScope unit="page" from="65" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Iterative sparse asymptotic minimum variance based approaches for array processing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Abeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Merabtine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Proc. IEEE Trans</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="933" to="944" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast implementation of sparse iterative covariance-based estimation for source localization</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Abeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Rowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page" from="1249" to="1259" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast implementation of sparse iterative covariance-based estimation for array processing</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Abeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Rowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference Record of the Forty Fifth Asilomar Conference on Signals, Systems and Computers (ASILOMAR)</title>
		<meeting>the 2011 Conference Record of the Forty Fifth Asilomar Conference on Signals, Systems and Computers (ASILOMAR)<address><addrLine>Pacific Grove, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-11">November 2011</date>
			<biblScope unit="page" from="2031" to="2035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Region-based convolutional networks for accurate object detection and segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="142" to="158" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">STCT: Sequentially Training Convolutional Networks for Visual Tracking</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07-01">26 June-1 July 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning Multi-Domain Convolutional Neural Networks for Visual Tracking</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07-01">26 June-1 July 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">CANNET: Context aware nonlocal convolutional networks for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing (ICIP)</title>
		<meeting>the IEEE International Conference on Image Processing (ICIP)<address><addrLine>Quebec City, QC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-30">27-30 September 2015</date>
			<biblScope unit="page" from="4669" to="4673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07-01">26 June-1 July 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep Convolutional Neural Networks for Hyperspectral Image Classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1155/2015/258619</idno>
	</analytic>
	<monogr>
		<title level="j">J. Sens</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bands Sensitive Convolutional Network for Hyperspectral Image Classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Internet Multimedia Computing and Service</title>
		<meeting>the International Conference on Internet Multimedia Computing and Service<address><addrLine>Xi&apos;an, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08">August 2016</date>
			<biblScope unit="page" from="268" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>the Advances in Neural Information Processing Systems (NIPS)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Network In Network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International conference on Learning Representations (ICLR)</title>
		<meeting>the International conference on Learning Representations (ICLR)<address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04">April 2014</date>
			<biblScope unit="page" from="14" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Full-view spherical image camera</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Pattern Recognition (ICPR&apos;06)</title>
		<meeting>the 18th International Conference on Pattern Recognition (ICPR&apos;06)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-08">August 2006</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="386" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rotation estimation from spherical images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sorgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 17th International Conference on Pattern Recognition</title>
		<meeting>the 2004 17th International Conference on Pattern Recognition<address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-08-26">26 August 2004</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="590" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rotation recovery from spherical images without correspondences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1170" to="1175" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rotation estimation and vanishing point extraction by omnidirectional vision in urban environment</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Bazin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Demonceaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vasseur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Robot. Res</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="63" to="81" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Visualizing Deep Neural Network Decisions: Prediction Difference Analysis</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Zintgraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04595</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)<address><addrLine>Haifa, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07">July 2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT</title>
		<meeting>COMPSTAT<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Handling motion blur in multi-frame super-resolution</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="page" from="5224" to="5232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Colorado Springs, CO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06">June 2011</date>
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the BigLearn, NIPS Workshop</title>
		<meeting>the BigLearn, NIPS Workshop<address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-12">December 2011</date>
			<biblScope unit="page" from="12" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<date type="published" when="2011">2011, 2, 27</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Selecting and interpreting measures of thematic classification accuracy</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Stehman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Environ</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="77" to="89" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An introduction to ROC analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fawcett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="861" to="874" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
