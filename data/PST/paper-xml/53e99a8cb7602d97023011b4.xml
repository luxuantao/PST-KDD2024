<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Online Learning in Episodic Markovian Decision Processes by Relative Entropy Policy Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Zimin</surname></persName>
							<email>alexander.zimin@ist.ac.at</email>
						</author>
						<author>
							<persName><forename type="first">Gergely</forename><surname>Neu</surname></persName>
							<email>gergely.neu@gmail.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Science and Technology</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">INRIA Lille -Nord Europe</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Budapest University of Technology and Economics</orgName>
								<orgName type="institution" key="instit2">MTA SZTAKI Institute for Computer Science and Control</orgName>
								<address>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Online Learning in Episodic Markovian Decision Processes by Relative Entropy Policy Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">90D0ED42CCA144798849759612D587A8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the problem of online learning in finite episodic Markov decision processes (MDPs) where the loss function is allowed to change between episodes. The natural performance measure in this learning problem is the regret defined as the difference between the total loss of the best stationary policy and the total loss suffered by the learner. We assume that the learner is given access to a finite action space A and the state space X has a layered structure with L layers, so that state transitions are only possible between consecutive layers. We describe a variant of the recently proposed Relative Entropy Policy Search algorithm and show that its regret after T episodes is 2 L|X ||A|T log(|X ||A|/L) in the bandit setting and 2L T log(|X ||A|/L) in the full information setting, given that the learner has perfect knowledge of the transition probabilities of the underlying MDP. These guarantees largely improve previously known results under much milder assumptions and cannot be significantly improved under general assumptions.</p><p>Parts of this work were done while Alexander Zimin was enrolled in the MSc. programme of the Central European University, Budapest, and Gergely Neu was working on his PhD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, we study the problem of online learning in a class of finite non-stationary episodic Markov decision processes. The learning problem that we consider can be formalized as a sequential interaction between a learner (often called agent) and an environment, where the interaction between the two entities proceeds in episodes. Every episode consists of multiple time steps: In every time step of an episode, a learner has to choose one of its available actions after observing some part of the current state of the environment. The chosen action influences the observable state of the environment in a stochastic fashion and imposes some loss on the learner. However, the entire state (be it observed or not) also influences the loss. The goal of the learner is to minimize its total (non-discounted) loss that it suffers. In this work, we assume that the unobserved part of the state evolves autonomously from the observed part of the state or the actions chosen by the learner, thus corresponding to a state sequence generated by an oblivious adversary such as nature. Otherwise, absolutely no statistical assumption is made about the mechanism generating the unobserved state variables. As usual for such learning problems, we set our goal as minimizing the regret defined as the difference between the total loss suffered by the learner and the total loss of the best stationary state-feedback policy. This setting fuses two important paradigms of learning theory: online learning <ref type="bibr" target="#b4">[5]</ref> and reinforcement learning <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>The learning problem outlined above can be formalized as an online learning problem where the actions of the learner correspond to choosing policies in a known Markovian decision process where the loss function changes arbitrarily between episodes. This setting is a simplified version of the learning problem first addressed by Even-Dar et al. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, who consider online learning unichain MDPs. In their variant of the problem, the learner faces a continuing MDP task where all policies are assumed to generate a unique stationary distribution over the state space and losses can change arbitrarily between consecutive time steps. Assuming that the learner observes the complete loss function after each time step (that is, assuming full information feedback), they propose an algorithm called MDP-E and show that its regret is O(τ 2 T log |A|), where τ &gt; 0 is an upper bound on the mixing time of any policy. The core idea of MDP-E is the observation that the regret of the global decision problem can be decomposed into regrets of simpler decision problems defined in each state. Yu et al. <ref type="bibr" target="#b22">[23]</ref> consider the same setting and propose an algorithm that guarantees o(T ) regret under bandit feedback where the learner only observes the losses that it actually suffers, but not the whole loss function. Based on the results of Even-Dar et al. <ref type="bibr" target="#b8">[9]</ref>, Neu et al. <ref type="bibr" target="#b15">[16]</ref> propose an algorithm that is shown to enjoy an O(T 2/3 ) bound on the regret in the bandit setting, given some further assumptions concerning the transition structure of the underlying MDP. For the case of continuing deterministic MDP tasks, Dekel and Hazan <ref type="bibr" target="#b6">[7]</ref> describe an algorithm guaranteeing O(T 2/3 ) regret. The immediate precursor of the current paper is the work of Neu et al. <ref type="bibr" target="#b13">[14]</ref>, who consider online learning in episodic MDPs where the state space has a layered (or loop-free) structure and every policy visits every state with a positive probability of at least α &gt; 0. Their analysis is based on a decomposition similar to the one proposed by Even-Dar et al. <ref type="bibr" target="#b8">[9]</ref>, and is sufficient to prove a regret bound of O(L 2 T |A| log |A|/α) in the bandit case and O(L 2 T log |A|) in the full information case.</p><p>In this paper, we present a learning algorithm that directly aims to minimize the global regret of the algorithm instead of trying to minimize the local regrets in a decomposed problem. Our approach is motivated by the insightful paper of Peters et al. <ref type="bibr" target="#b16">[17]</ref>, who propose an algorithm called Relative Entropy Policy Search (REPS) for reinforcement learning problems. As Peters et al. <ref type="bibr" target="#b16">[17]</ref> and Kakade <ref type="bibr" target="#b10">[11]</ref> point out, good performance of policy search algorithms requires that the information loss between the consecutive policies selected by the algorithm is bounded, so that policies are only modified in small steps. Accordingly, REPS aims to select policies that minimize the expected loss while guaranteeing that the state-action distributions generated by the policies stay close in terms of Kullback-Leibler divergence. Further, Daniel et al. <ref type="bibr" target="#b5">[6]</ref> point out that REPS is closely related to a number of previously known probabilistic policy search methods. Our paper is based on the observation that REPS is closely related to the Proximal Point Algorithm (PPA) first proposed by Martinet <ref type="bibr" target="#b12">[13]</ref> (see also <ref type="bibr" target="#b19">[20]</ref>).</p><p>We propose a variant of REPS called online REPS or O-REPS and analyze it using fundamental results concerning the PPA family. Our analysis improves all previous results concerning online learning in episodic MDPs: we show that the expected regret of O-REPS is bounded by 2 L|X ||A|T log(|X ||A|/L) in the bandit setting and 2L T log(|X ||A|/L) in the full information setting. Unlike previous works in the literature, we do not have to make any assumptions about the transition dynamics apart from the loop-free assumption. The full discussion of our results is deferred to Section 5.</p><p>Before we move to the technical content of the paper, we first fix some conventions. Random variables will be typeset in boldface (e.g., x, a) and indefinite sums over states and actions are to be understood as sums over the entire state and action spaces. For clarity, we assume that all actions are available in all states, however, this assumption is not essential. The indicator of any event A will be denoted by I {A}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem definition</head><p>An episodic loop-free Markov decision process is formally defined by the tuple M = {X , A, P }, where X is the finite state space, A is the finite action space, and P : X × X × A is the transition function, where P (x |x, a) is the probability that the next state of the Markovian environment will be x , given that action a is selected in state x. We will assume that M satisfies the following assumptions:</p><p>• The state space X can be decomposed into non-intersecting layers, i.e. X = L k=0 X k where X l ∩ X k = ∅ for l = k. • X 0 and X L are singletons, i.e. X 0 = {x 0 } and X L = {x L }.</p><p>• Transitions are possible only between consecutive layers. Formally, if P (x |x, a) &gt; 0, then</p><p>x ∈ X k+1 and x ∈ X k for some 0 ≤ k ≤ L -1.</p><p>The interaction between the learner and the environment is described on Figure <ref type="figure" target="#fig_0">1</ref>. The interaction of an agent and the Markovian environment proceeds in episodes, where in each episode the agent starts in state x 0 and moves forward across the consecutive layers until it reaches state x L . <ref type="foot" target="#foot_0">1</ref> We assume that the environment selects a sequence of loss functions { t } T t=1 and the losses only change between episodes. Furthermore, we assume that the learner only observes the losses that it suffers in each individual state-action pair that it visits, in other words, we consider bandit feedback. <ref type="foot" target="#foot_1">2</ref>Parameters: Markovian environment M = {X , A, P }; For all episodes t = 1, 2, . . . , T , repeat</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">The environment chooses the loss function</head><formula xml:id="formula_0">t : X × A → [0, 1].</formula><p>2. The learner starts in state x 0 (t) = x 0 .</p><p>3. For all time steps l = 0, 1, 2, . . . , L -1, repeat (a) The learner observes x l (t) ∈ X l . (b) Based on its previous observations (and randomness), the learner selects a l (t). (c) The learner suffers and observes loss t (x l (t), a l (t)). (d) The environment draws the new state x l+1 (t) ∼ P (•|x l (t), a l (t)). For defining our performance measure, we need to specify a set of reference controllers that is made available to the learner. To this end, we define the concept of (stochastic stationary) policies: A policy is defined as a mapping π : A × X → [0, 1], where π(a|x) gives the probability of selecting action a in state x. The expected total loss of a policy π is defined as</p><formula xml:id="formula_1">L T (π) = E T t=1 L-1 k=0 t (x k , a k ) P, π ,</formula><p>where the notation E [ •| P, π] is used to emphasize that the random variables x k and a k are generated by executing π in the MDP specified by the transition function P . Denote the total expected loss suffered by the learner as</p><formula xml:id="formula_2">L T = T t=1 L-1 k=0 E [ t (x k (t), a k (t))| P ],</formula><p>where the expectation is taken over the internal randomization of the learner and the random transitions of the Markovian environment. Using these notations, we define the learner's goal as minimizing the (total expected) regret defined as</p><formula xml:id="formula_3">R T = L T -min π L T (π),</formula><p>where the minimum is taken over the complete set of stochastic stationary policies. <ref type="foot" target="#foot_2">3</ref>It is beneficial to introduce the concept of occupancy measures on the state-action space X × A: the occupancy measure q π of policy π is defined as the collection of distributions generated by executing policy π on the episodic MDP described by P :</p><formula xml:id="formula_4">q π (x, a) = P x k(x) = x, a k(x) = a P, π ,</formula><p>where k(x) denotes the index of the layer that x belongs to. It is easy to see that the occupancy measure of any policy π satisfies</p><formula xml:id="formula_5">a q π (x, a) = x ∈X k(x)-1 a P (x|x , a )q π (x , a ),<label>(1)</label></formula><p>for all x ∈ X \ {x 0 , x l }, with q π (x 0 , a) = π(a|x 0 ) for all a ∈ A. The set of all occupancy measures satisfying the above equality in the MDP M will be denoted as ∆(M ). The policy π is said to generate the occupancy measure q ∈ ∆(M ) if π(a|x) = q(x, a)</p><p>b q(x, b) holds for all (x, a) ∈ X × A. It is clear that there exists a unique generating policy for all measures in ∆(M ) and vice versa. The policy generating q will be denoted as π q . In what follows, we will redefine the task of the learner from having to select individual actions a k (t) to having to select occupancy measures q t ∈ ∆(M ) in each episode t. To see why this notion simplifies the treatment of the problem, observe that</p><formula xml:id="formula_6">E L-1 k=0 t (x k , a k ) P, π q = L-1 k=0 x∈X k a q(x, a) t (x, a) = x,a q(x, a) t (x, a) def = q, t ,<label>(2)</label></formula><p>where we defined the inner product •, • on X × A in the last line. Using this notation, we can reformulate our original problem as an instance of online linear optimization with decision space ∆(M ). Assuming that the learner selects occupancy measure q t in episode t, the regret can be rewritten as</p><formula xml:id="formula_7">R T = max q∈∆(M ) E T t=1 q t -q, t .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The algorithm: O-REPS</head><p>Using the formalism introduced in the previous section, we now describe our algorithm called Online Relative Entropy Policy Search (O-REPS). O-REPS is an instance of online linear optimization methods usually referred to as Follow-the-Regularized-Leader (FTRL), Online Stochastic Mirror Descent (OSMD) or the Proximal Point Algorithm (PPA)-see, e.g., <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b1">[2]</ref> for a discussion of these methods and their relations. To allow comparisons with the original derivation of REPS by Peters et al. <ref type="bibr" target="#b16">[17]</ref>, we formalize our algorithm as an instance of PPA. Before describing the algorithm, some more definitions are in order. First, define D (q q ) as the unnormalized Kullback-Leibler divergence between two occupancy measures q and q : D (q q ) = x,a q(x, a) log q(x, a) q (x, a) -</p><p>x,a (q(x, a) -q (x, a)) .</p><p>Furthermore, let R(q) define the unnormalized negative entropy of the occupancy measure q: R(q) =</p><p>x,a q(x, a) log q(x, a) -</p><p>x,a q(x, a).</p><p>We are now ready to define O-REPS formally. In the first episode, O-REPS chooses the uniform policy with π 1 (a|x) = 1/|A| for all x and a, and we let q 1 = q π1 . <ref type="foot" target="#foot_3">4</ref> Then, the algorithm proceeds recursively: After observing</p><formula xml:id="formula_8">u t = (x 0 (t), a 0 (t), t (x 0 (t), a 0 (t)), . . . , x L-1 (t), a L-1 (t), t (x L-1 (t), a L-1 (t)), x L (t))</formula><p>in episode t, we define the loss estimates ˆ t as</p><formula xml:id="formula_9">ˆ t = t (x, a) q t (x, a) I {(x, a) ∈ u t } ,</formula><p>where we used the notation (x, a) ∈ u t to indicate that the state-action pair (x, a) was observed during episode t. After episode t, O-REPS selects the occupancy measure that solves the optimization problem q t+1 = arg min q∈∆(M ) η q, ˆ t + D(q||q t ) .</p><p>In episode t, our algorithm follows the policy π t = π qt . Defining U t = (u 1 , u 2 , . . . , u t ), we clearly have that q t (x, a) = P [ (x, a) ∈ u t | U t-1 ], so ˆ t (x, a) is an unbiased estimate of t (x, a) for all (x, a) such that q t (x, a) &gt; 0:</p><formula xml:id="formula_11">E ˆ t (x, a) U t-1 = t (x, a) q t (x, a) P [ (x, a) ∈ u t | U t-1 ] = t (x, a).<label>(4)</label></formula><p>We now proceed to explain how the policy update step (3) can be implemented efficiently. It is known (see, e.g., Bartók et al. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr">Lemma 8.6]</ref>) that performing this optimization can be reformulated as first solving the unconstrained optimization problem qt+1 = arg min q η q, ˆ t + D(q||q t )</p><p>and then projecting the result to ∆(M ) as</p><formula xml:id="formula_12">q t+1 = arg min q∈∆(M ) D (q qt+1 ) .</formula><p>The first step can be simply carried out by setting qt+1 (x, a) = q t (x, a)e -η ˆ t(x,a) . The projection step, however, requires more care. To describe the projection procedure, we need to introduce some more notation. For any function v : X → R and loss function :</p><formula xml:id="formula_13">X × A → [0, 1] we define a function δ(x, a|v, ) = -η (x, a) - x ∈X v(x )P (x |x, a) + v(x).<label>(5)</label></formula><p>As noted by Peters et al. <ref type="bibr" target="#b16">[17]</ref>, the above function can be regarded as the Bellman error corresponding to the value function v. The next proposition provides a succinct formalization of the optimization problem (3). Proposition 1. Let t &gt; 1 and define the function</p><formula xml:id="formula_14">Z t (v, k) = x∈X k ,a∈A</formula><p>q t (x, a)e δ(x,a|v, ˆ t) .</p><p>The update step (3) can be performed as</p><formula xml:id="formula_15">q t+1 (x, a) = q t (x, a)e δ(x,a|vt, ˆ t) Z t (v t , k(x)) ,</formula><p>where</p><formula xml:id="formula_16">vt = arg min v L k=0 ln Z t (v, k).<label>(6)</label></formula><p>Minimizing the expression on the right-hand side of Equation ( <ref type="formula" target="#formula_16">6</ref>) is an unconstrained convex optimization problem (see Boyd and Vandenberghe <ref type="bibr" target="#b3">[4]</ref> and the comments of Peters et al. <ref type="bibr" target="#b16">[17]</ref>) and can be solved efficiently. It is important to note that since q 1 (x, a) &gt; 0 holds for all (x, a) pairs, q t (x, a) is also positive for all t &gt; 0 by the multiplicative update rule, so Equation <ref type="formula" target="#formula_11">4</ref>holds for all state-action pairs (x, a) in all time steps.</p><p>The proof follows the steps of Peters et al. <ref type="bibr" target="#b16">[17]</ref>, however, their original formalization of REPS is slightly different, which results in small changes in the analysis as well. For further comments regarding the differences between O-REPS and REPS, see Section 5.</p><p>Proof of Proposition 1. We start with formulating the projection step as a constrained optimization problem:</p><formula xml:id="formula_17">min q D (q qt+1 )</formula><p>subject to a q(x, a) =</p><p>x ,a P (x|x , a )q(x , a ) for all x ∈ X \ {x 0 , x l }, x∈X k a q(x, a) = 1 for all k = 0, 1, . . . , L -1.</p><p>To solve the problem, consider the Lagrangian:</p><formula xml:id="formula_18">L t (q) =D (q qt+1 ) + L-1 k=0 λ k   x∈X k ,a∈A q(x, a) -1   + L-1 k=1 x∈X k v(x)   x ∈X k-1 a q(x , a )P (x|x , a ) - a q(x, a)   =D (q qt+1 ) + a q(x 0 , a) λ 0 + x v(x )P (x |x 0 , a) - L-1 k=0 λ k + x =x0 a q(x, a) λ k(x) + x v(x )P (x |x, a) -v(x) ,</formula><p>where {λ k } L-1 k=0 and {v(x)} x∈X \{x0,x l } are Lagrange multipliers. In what follows, we set v(x 0 ) = v(x L ) = 0 for convenience. Differentiating the Lagrangian with respect to any q(x, a), we get</p><formula xml:id="formula_19">∂L t (q) ∂q(x, a) = ln q(x, a) -ln qt+1 (x, a) + λ k(x) + x v(x )P (x |x, a) -v(x).</formula><p>Hence, setting the gradient to zero, we obtain the formula for q t+1 (x, a):</p><formula xml:id="formula_20">q t+1 (x, a) = qt+1 (x, a)e -λ k(x) -x v(x )P (x |x,a)+v(x) .</formula><p>Substituting the formula for qt+1 (x, a), we get</p><formula xml:id="formula_21">q t+1 (x, a) = q t (x, a)e -λ k(x) +δ(x,a|v, ˆ t) .</formula><p>Using the second constraint, we have for every k = 0, 1, . . . , L -1 that x∈X k a q t (x, a)e -λ k +δ(x,a|v, ˆ t) = 1, yielding e -λ k = 1/Z t (v, k), which leaves us with computing the value of v at the optimum. This can be done by solving the dual problem of maximizing</p><formula xml:id="formula_22">x,a qt+1 (x, a) -L - L-1 k=0 λ k over {λ k } L-1</formula><p>k=0 . If we drop the constants and express each λ k in terms of Z t (v, k), then the problem is equivalent to maximizing -L-1 k=0 ln Z t (v, k), that is, solving the optimization problem (6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis</head><p>The next theorem states our main result concerning the regret of O-REPS under bandit feedback. The proof of the theorem is based on rather common ideas used in the analysis of FTRL/OSMD/PPAstyle algorithms (see, e.g., <ref type="bibr" target="#b23">[24]</ref>, Chapter 11 of <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b1">[2]</ref>). After proving the theorem, we also present the regret bound for O-REPS when used in a full information setting where the learner gets to observe t after each episode t. Theorem 1. Assuming bandit feedback, the total expected regret of O-REPS satisfies</p><formula xml:id="formula_23">R T ≤ η|X ||A|T + L log |X ||A| L η .</formula><p>In particular, setting η = L</p><formula xml:id="formula_24">log |X ||A| L T |X ||A| yields R T ≤ 2 L|X ||A|T log |X ||A| L .</formula><p>Proof. By standard arguments (see, e.g., <ref type="bibr" target="#b18">[19,</ref><ref type="bibr">Lemma 12]</ref>, <ref type="bibr" target="#b2">[3,</ref><ref type="bibr">Lemma 9.2]</ref> or <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">Theorem 11</ref>.1]), we have</p><formula xml:id="formula_25">T t=1 q t -q, ˆ t ≤ T t=1 q t -qt+1 , ˆ t + D (q q 1 ) η .<label>(7)</label></formula><p>Using the exact form of qt+1 and the fact that e x ≥ 1 + x, we get that qt+1 (x, a) ≥ q t (x, a) -ηq t (x, a) ˆ t (x, a)</p><p>and thus</p><formula xml:id="formula_26">T t=1 q t -qt+1 , ˆ t ≤ η T t=1 x,a q t (x, a) ˆ 2 t (x, a) ≤ η T t=1 x,a q t (x, a) t (x, a) q t (x, a) ˆ t (x, a) ≤ η T t=1 x,a ˆ t (x, a).</formula><p>Combining this with <ref type="bibr" target="#b6">(7)</ref>, we get</p><formula xml:id="formula_27">T t=1 q t -q, ˆ t ≤ η T t=1 x,a ˆ t (x, a) + D (q q 1 ) η .<label>(8)</label></formula><p>Next, we take an expectation on both sides. By Equation ( <ref type="formula" target="#formula_11">4</ref>), we have</p><formula xml:id="formula_28">E T t=1 x,a ˆ t (x, a) ≤ |X ||A|T.</formula><p>It also follows from Equation ( <ref type="formula" target="#formula_11">4</ref>) that E q, ˆ t = q, t and E q t , ˆ t = E [ q t , t ]. Finally, notice that D (q q 1 ) ≤R(q) -R(q 1 ) ≤ L-1 k=0 x∈X k a q 1 (x, a) log 1 q 1 (x, a)</p><formula xml:id="formula_29">(since R(q) ≤ 0) ≤ L-1 k=0 log |X k ||A| ≤ L log |X ||A| L ,</formula><p>where we used the trivial upper bound on the entropy of distributions and Jensen's inequality in the last step. Plugging the above upper bound into Equation ( <ref type="formula" target="#formula_27">8</ref>), we obtain the statement of the theorem.</p><p>Theorem 2. Assuming full feedback, the total expected regret of O-REPS satisfies</p><formula xml:id="formula_30">R T ≤ ηLT + L log |X ||A| L η .</formula><p>In particular, setting η =</p><formula xml:id="formula_31">log |X ||A| L T yields R T ≤ 2L T log |X ||A| L .</formula><p>The proof of the statement follows directly from the proof of Theorem 1, with the only difference that we set ˆ t = t and we can use the tighter upper bound</p><formula xml:id="formula_32">T t=1 q t -qt+1 , t ≤ η T t=1 x,a q t (x, a) 2 t (x, a) ≤ η T t=1 x,a q t (x, a) = ηLT,</formula><p>where we used that x∈X k a q t (x, a) = 1 for all layers k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and future work</head><p>Comparison with previous results We first compare our regret bounds with previous results from the literature. First, our guarantees for the full information case trade off a factor of L present in the bounds of Neu et al. <ref type="bibr" target="#b13">[14]</ref> to a (usually much smaller) factor of log |X |. More importantly, our bounds trade off a factor of L 3/2 /α in the bandit case to a factor of |X |. This improvement is particularly remarkable considering that we do not need to assume that α &gt; 0, that is, we drop the rather unnatural assumption that every stationary policy has to visit every state with positive probability. In particular, dropping this assumption enables our algorithm to work in deterministic loop-free MDPs, that is, to solve the online shortest path problem (see, e.g., <ref type="bibr" target="#b9">[10]</ref>). In the shortest path setting, O-REPS provides an alternative implementation to the Component Hedge algorithm analyzed by Koolen et al. <ref type="bibr" target="#b11">[12]</ref>, who prove identical bounds in the full information case. As shown by Audibert et al. <ref type="bibr" target="#b1">[2]</ref>, Component Hedge achieves the analog of our bounds in the bandit case as well.</p><p>O-REPS also bears close resemblance to the algorithms of Even-Dar et al. <ref type="bibr" target="#b8">[9]</ref> and Neu et al. <ref type="bibr" target="#b15">[16]</ref> who also use policy updates of the form π t+1 (a|x) ∝ π t (a|x) exp(-η t (x, a)x P (x |x, a)v t (x )).</p><p>The most important difference between their algorithm and O-REPS is that their value functions v t are computed as the solution of the Bellman-equations instead of the solution of the optimization problem <ref type="bibr" target="#b5">(6)</ref>. By a simple combination of our analysis and that of Even-Dar et al. <ref type="bibr" target="#b8">[9]</ref>, it is possible to show that O-REPS attains a regret of O( √ τ T ) in the unichain setting with full information feedback, improving their bound by a factor of τ 3/2 under the same assumptions. It is an interesting open problem to find out if using the O-REPS value functions is a strictly better idea than solving the Bellman equations in general. Another important direction of future work is to extend our results to the case of unichain MDPs with bandit feedback and the setting where the transition probabilities of the underlying MDP is unknown (see Neu et al. <ref type="bibr" target="#b14">[15]</ref>).</p><p>Lower bounds Following the proof of Theorem 10 in Audibert et al. <ref type="bibr" target="#b1">[2]</ref>, it is straightforward to construct an MDP consisting of |X |/L chains of L consecutive bandit problems each with |A| actions such that no algorithm can achieve smaller regret than 0.03L T log(|X ||A|) in the full information case and 0.04 L|X ||A|T in the bandit case. These results suggest that our bounds cannot be significantly improved in general, however, finding an appropriate problem-dependent lower bound remains an interesting open problem in the much broader field of online linear optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REPS vs. O-REPS</head><p>As noted several times above, our algorithm is directly inspired by the work of Peters et al. <ref type="bibr" target="#b16">[17]</ref>. However, there is a slight difference between the original version of REPS and O-REPS, namely, Peters et al. aim to solve the optimization problem q t+1 = arg min q∈∆(M ) q, ˆ t subject to the constraint D (q q t ) ≤ ε for some ε &gt; 0. This is to be contrasted with the following property of the occupancy measures generated by O-REPS (proved in the supplementary material): Lemma 1. For any t &gt; 0, D (q t q t+1 ) ≤ η 2 2 q t , ˆ 2 t .</p><p>In particular, if the losses are estimated by bounded sample averages as done by Peters et al. <ref type="bibr" target="#b16">[17]</ref>, this gives D (q t q t+1 ) ≤ η 2 /2. While this is not the exact same property as desired by REPS, both inequalities imply that the occupancy measures stay close to each other in the 1-norm sense by Pinsker's inequality. Thus we conjecture that our formulation of O-REPS has similar properties to the one studied by Peters et al. <ref type="bibr" target="#b16">[17]</ref>, while it might be somewhat simpler to implement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The protocol of online learning in episodic MDPs.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Such MDPs naturally arise in episodic decision tasks where some notion of time is present in the state description.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>In the literature of online combinatorial optimization, this feedback scheme is often called semi-bandit feedback, see Audibert et al.<ref type="bibr" target="#b1">[2]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The existence of this minimum is a standard result of MDP theory, see Puterman<ref type="bibr" target="#b17">[18]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Note that q π can be simply computed by using (1) recursively.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Alexander Zimin is an OMV scholar. Gergely Neu's work was carried out during the tenure of an ERCIM "Alain Bensoussan" Fellowship Programme. The research leading to these results has received funding from INRIA, the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreements 246016 and 231495 (project CompLACS), the Ministry of Higher Education and Research, Nord-Pas-de-Calais Regional Council and FEDER through the "Contrat de Projets Etat Region (CPER) 2007-2013".</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Competing in the dark: An efficient algorithm for bandit linear optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Abernethy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rakhlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual Conference on Learning Theory (COLT)</title>
		<meeting>the 21st Annual Conference on Learning Theory (COLT)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="263" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Regret in online combinatorial optimization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Audibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lugosi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Mathematics of Operations Research</publisher>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Online learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bartók</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pál</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Szita</surname></persName>
		</author>
		<ptr target="https://moodle.cs.ualberta.ca/file.php/354/notes.pdf" />
	</analytic>
	<monogr>
		<title level="m">Lecture notes, University of Alberta</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Prediction, Learning, and Games</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lugosi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical relative entropy policy search</title>
		<author>
			<persName><forename type="first">C</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fifteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="273" to="281" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Better rates for any adversarial deterministic mdp</title>
		<author>
			<persName><forename type="first">O</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML-13)</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</editor>
		<meeting>the 30th International Conference on Machine Learning (ICML-13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="675" to="683" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Experts in a Markov decision process</title>
		<author>
			<persName><forename type="first">E</forename><surname>Even-Dar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS-17</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="401" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Online Markov decision processes</title>
		<author>
			<persName><forename type="first">E</forename><surname>Even-Dar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="726" to="736" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The on-line shortest path problem under partial monitoring</title>
		<author>
			<persName><forename type="first">A</forename><surname>György</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lugosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gy</forename><surname>Ottucsák</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="2369" to="2403" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A natural policy gradient</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 14 (NIPS)</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="1531" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hedging structured concepts</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Koolen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Warmuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kivinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual Conference on Learning Theory (COLT)</title>
		<meeting>the 23rd Annual Conference on Learning Theory (COLT)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="93" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Régularisation d&apos;inéquations variationnelles par approximations successives</title>
		<author>
			<persName><forename type="first">B</forename><surname>Martinet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESAIM: Mathematical Modelling and Numerical Analysis -Modélisation Mathématique et Analyse Numérique</title>
		<imprint>
			<date type="published" when="1970">1970</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="154" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The online loop-free stochastic shortestpath problem</title>
		<author>
			<persName><forename type="first">G</forename><surname>Neu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>György</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cs</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual Conference on Learning Theory (COLT)</title>
		<meeting>the 23rd Annual Conference on Learning Theory (COLT)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="231" to="243" />
		</imprint>
	</monogr>
	<note>a)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The adversarial stochastic shortest path problem with unknown transition probabilities</title>
		<author>
			<persName><forename type="first">G</forename><surname>Neu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>György</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cs</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS 2012</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="805" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Online Markov decision processes under bandit feedback</title>
		<author>
			<persName><forename type="first">G</forename><surname>Neu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>György</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cs</forename><surname>Szepesvári</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Antos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS-23</title>
		<imprint>
			<publisher>CURRAN</publisher>
			<date type="published" when="2010">2010b</date>
			<biblScope unit="page" from="1804" to="1812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Relative entropy policy search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mülling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 2010</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1607" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Markov Decision Processes: Discrete Stochastic Dynamic Programming</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Puterman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Wiley-Interscience</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Rakhlin</surname></persName>
		</author>
		<title level="m">Lecture notes on online learning</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Monotone Operators and the Proximal Point Algorithm</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Rockafellar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control and Optimization</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="877" to="898" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Algorithms for Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Cs</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Markov decision processes with arbitrary reward processes</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shimkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="737" to="757" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Online convex programming and generalized infinitesimal gradient ascent</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zinkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on Machine Learning</title>
		<meeting>the Twentieth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="928" to="936" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
