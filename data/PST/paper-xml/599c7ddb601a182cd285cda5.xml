<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Classification of Non-Functional Requirements from Augmented App User Reviews</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mengmeng</forename><surname>Lu</surname></persName>
							<email>lumengmeng@whu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Liang</surname></persName>
							<email>liangp@whu.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">State Key Lab of Software Engineering</orgName>
								<orgName type="institution">Wuhan University Luojiasha</orgName>
								<address>
									<postCode>430072</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">State Key Lab of Software Engineering</orgName>
								<orgName type="institution">Wuhan University Luojiasha</orgName>
								<address>
									<postCode>430072</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Classification of Non-Functional Requirements from Augmented App User Reviews</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C86C90A3A0D644B43162ED13DCD35B81</idno>
					<idno type="DOI">10.1145/3084226.3084241</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Non-Functional Requirements</term>
					<term>User Reviews</term>
					<term>Automatic Classification</term>
					<term>Textual Semantics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context:</head><p>The leading App distribution platforms, Apple App Store, Google Play, and Windows Phone Store, have over 4 million Apps. Research shows that user reviews contain abundant useful information which may help developers to improve their Apps. Extracting and considering Non-Functional Requirements (NFRs), which describe a set of quality attributes wanted for an App and are hidden in user reviews, can help developers to deliver a product which meets users' expectations. Objective: Developers need to be aware of the NFRs from massive user reviews during software maintenance and evolution. Automatic user reviews classification based on an NFR standard provides a feasible way to achieve this goal. Method: In this paper, user reviews were automatically classified into four types of NFRs (reliability, usability, portability, and performance), Functional Requirements (FRs), and Others. We combined four classification techniques BoW, TF-IDF, CHI 2 , and AUR-BoW (proposed in this work) with three machine learning algorithms Naive Bayes, J48, and Bagging to classify user reviews. We conducted experiments to compare the F-measures of the classification results through all the combinations of the techniques and algorithms. Results: We found that the combination of AUR-BoW with Bagging achieves the best result (a precision of 71.4%, a recall of 72.3%, and an Fmeasure of 71.8%) among all the combinations. Conclusion: Our finding shows that augmented user reviews can lead to better classification results, and the machine learning algorithm Bagging is more suitable for NFRs classification from user reviews than Naï ve Bayes and J48.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Mobile Apps are developed specially for mobile devices such as tablets and smartphones. The leading App distribution platforms, Apple App Store, Google Play, and Windows Phone Store had over 4 million Apps as of June 2016 <ref type="bibr" target="#b30">[31]</ref>. The number of downloads is around 1 billion per month in Apple App Store <ref type="bibr" target="#b1">[2]</ref>. Users can evaluate an App by giving a rating with a text feedback after they download and use the App. As a type of collective knowledge <ref type="bibr" target="#b13">[14]</ref>, user reviews contain valuable information which may help developers to better understand user needs and complaints during software maintenance and evolution <ref type="bibr" target="#b1">[2]</ref> <ref type="bibr" target="#b2">[3]</ref>. Making use of user reviews to mine valuable information for improving Apps is critical for retaining the existing users and attracting new users. Studies show that over one third of users changed their ratings following a developer response, and the median rating change is a one-star increase out of five <ref type="bibr" target="#b26">[27]</ref>. However, analyzing user reviews face challenges. A recent study found that mobile Apps received approximately 23 user reviews per day and popular Apps, such as Facebook, received on average 4275 user reviews per day <ref type="bibr" target="#b1">[2]</ref>. Due to the large number of user reviews, it is time-consuming, tedious, and infeasible for manual inspection to get useful opinions from the user reviews, and the unstructured and informal nature of user reviews complicates the identification of valuable information.</p><p>To this end, methods and tools are proposed and developed to automatically analyze user reviews with the aim of reducing the human effort and acquiring valuable information from the user reviews <ref type="bibr">[3][9]</ref>[20] <ref type="bibr" target="#b23">[24]</ref>. Chen et al. used a classification technique and LDA (Latent Dirichlet Allocation) <ref type="bibr" target="#b6">[7]</ref> to mine the informative user reviews by filtering noisy and irrelevant ones from a large and rapidly increasing pool of user reviews in App markets <ref type="bibr" target="#b5">[6]</ref>. Maalej and Nabil employed review metadata, text classification, natural language processing, and sentiment analysis techniques to automatically classify user reviews into four types: bug reports, feature requests, user experiences, and ratings <ref type="bibr" target="#b0">[1]</ref>. Panichella et al. used natural language processing and sentiment analysis techniques to automatically classify user reviews into four types: information seeking, information giving, feature request, and problem discovery, which are relevant to software maintenance and evolution <ref type="bibr" target="#b7">[8]</ref>. Gu and Kim not only automatically classified user reviews into five types: aspect evaluation, bug reports, feature requests, praise, and others, but also summarized users' sentiments and opinions toward corresponding aspects, answering the important question "what parts are loved by users" for App developers <ref type="bibr" target="#b8">[9]</ref>. <ref type="bibr">Vu et al.</ref> proposed a keyword-based framework for semi-automated user review analysis. Their method lists the user reviews most relevant to those keywords which describe developers' interests <ref type="bibr" target="#b9">[10]</ref>.</p><p>Non-Functional Requirements (NFRs) describe a set of quality attributes that a software system should exhibit. NFRs specify a broad range of qualities such as reliability, performance. These qualities play a critical role in user experience, and should be identified and considered in App development. Moreover, NFRs are often overlooked during requirements elicitation which leads to the situation that the implemented system fails to meet users' expectation. The goal of this work is to help App developers to identify and consider NFRs in App development through automatically classifying user reviews. In this work, we combined four classification techniques BoW (Bag-of-Words), TF-IDF (Term Frequency -Inverse Document Frequency), CHI 2  (Chi Squared), and AUR-BoW (Augmented User Reviews -Bagof-Words) with three machine learning algorithms Naive Bayes, J48, and Bagging to automatically classify user reviews into four types of NFRs (reliability, usability, portability, and performance), Functional Requirement (FR), and Others. The definition of each type is detailed in Section 3.2. We conducted experiments on the user reviews of two popular Apps (iBooks in Apple App Store and WhatsApp in Google Play Store) to compare the classification results through all the combinations of the techniques and algorithms.</p><p>The rest of the paper is organized as follows: Section 2 introduces the classification techniques used in this work in detail. Section 3 describes the research questions, the user reviews classification process with several important phases, and the dataset for the experiments. Section 4 presents the experiment results and discusses their implications. Limitation and threats to validity are discussed in Section 5. Related work is presented in Section 6. We conclude this work with further work directions in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">User Reviews Classification Techniques</head><p>We introduce four classification techniques BoW, TF-IDF, CHI 2 , and AUR-BoW in this section, which are used for user reviews classification. BoW employs all unique terms in the user reviews of a dataset as textual features, and uses term frequency as the weight of textual features. Different from BoW, TF-IDF combines term frequency with inverse document frequency to get the weight of textual features, which is influenced by the frequency of the term in the user reviews of the dataset. Also compared with BoW, CHI 2 decreases the number of textual features, and AUR-BoW augments user reviews by most similar words. In these four classification techniques, AUR-BoW is our proposed approach, which classifies user reviews augmented by most similar words to the user reviews based on words similarity. The processing phases of user reviews classification are introduced in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Bag-of-Words</head><p>The Bag-of-Words (BoW) model <ref type="bibr" target="#b12">[13]</ref> is widely used to represent textual documents in information retrieval systems and is one of the most popular representation techniques for object classification. Maalej and Nabil extracted textual features and calculated the weight of textual features by BoW for user reviews classification <ref type="bibr" target="#b0">[1]</ref>. BoW considers a dictionary which is composed of all unique terms of user reviews in the corpus as textual features, and uses term frequency (TF), the times a term appears in a user review as the weight of textual features for training classifiers. In summary, using BoW a user review j is expressed by a vector X j = (x 1,j … x i,j … x n,j ), in which x i,j denotes the weight of feature i calculated by the frequency of term i in user review j, and n denotes the number of terms in the dictionary. After that, the manually classified user reviews which are expressed by vectors act as input of supervised machine learning algorithms, and consequently are used to train classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Term Frequency -Inverse Document Frequency</head><p>Term Frequency -Inverse Document Frequency (TF-IDF) uses the same textual features (i.e., words) as BoW does, but different from BoW, TF-IDF uses not only TF but also inverse document frequency (IDF) as the weight of textual features (words). TF-IDF of each word is defined in Formula (1):</p><formula xml:id="formula_0">( )<label>(1)</label></formula><p>f i,j denotes the frequency of a word i in user review j. We use total user reviews that contain word i as denominator since some words might appear frequently in many user reviews, which means that these words contain less type information. On the other hand, IDF alone does not consider type information, and it cannot handle the situation that a word appears in many user reviews of the same type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Chi Squared</head><p>Motivated by IDF, we use a feature selection algorithm Chi Squared <ref type="bibr" target="#b14">[15]</ref>, which is a common statistical test and considers type information of user reviews, to select textual features important to user reviews classification from the textual features acquired by BoW. Words that exist in more types contain less type information, and consequently are less important for user reviews classification. Furthermore, many research results show that feature selection can improve the performance of text classification <ref type="bibr" target="#b4">[5]</ref>. The frequently used feature selection algorithms are Mutual Information, Information Gain, Chi Squared (CHI 2 ), etc. <ref type="bibr" target="#b14">[15]</ref>. Among them, CHI 2 is reported by many studies as one of the most effective algorithms. CHI 2 is defined in Formula (2):</p><p>(</p><p>The variables a, b, c, and d of Formula (2) are described in Table <ref type="table" target="#tab_0">1</ref>. N in Formula (2) denotes the total number of user reviews in the training set. The greater the value of CHI 2 (t i , C k ) is, the more type information term t i contains. We acquire the CHI 2 value of each word in each type. After that for each type, words are ranked by the CHI 2 value in a descending order and we choose top n percent of words as textual features. After the value of n is determined, term frequency of textual features is used as the weight of the selected textual features. The value of n is a threshold which depends on specific classification experiments on the dataset. The potential value of n ranges from 1% to 100% and we use binary search method, which excludes half of the remaining possible values each time, to efficiently choose the value of n which achieves the best classification result in Fmeasure. For binary search, there are three variables: begin, end, and best classification result which denote begin of range, end of range, and best classification result in F-measure respectively. Algorithm 1 shows the execution process of binary search which returns the value of n achieving the best classification result. The time complexity of binary search is O(logN), which is acceptable in practice. For Naï ve Bayes, J48, and Bagging (the three machine learning algorithms used for training classifiers, see Section 3.6), the values of n of CHI 2 are set to 9%, 11%, and 17% respectively since these values achieve the best classification results in Fmeasure. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Augmented User Reviews (the proposed approach)</head><p>Most user reviews are very short and contain less than 100 words, and user reviews are further split into sentences, which makes the text to be classified even shorter. To handle this problem, the sentences are augmented by several most similar words to the user reviews in the training set. After that, the augmented user reviews act as input of BoW, and we name this classification technique as AUR-BoW.</p><p>We use the word2vec <ref type="foot" target="#foot_0">1</ref> tool, which provides a vector-based representation of words to get words similarity by multiplying the vector of words. A recent study shows that word2vec provides the state-of-the-art performance for measuring words similarity <ref type="bibr" target="#b10">[11]</ref>. The similarity between user review r k and term t j is calculated based on the definition in <ref type="bibr" target="#b15">[16]</ref>, which is shown in Formula (3):</p><formula xml:id="formula_2">∑( )<label>(3) (4)</label></formula><p>in which r k denotes the user review k expressed by a vector</p><formula xml:id="formula_3">r k = (t k,1 … t k,i … t k,n ), t k,i denotes term i in r k</formula><p>, n denotes the number of terms in r k , w k,i denotes the weight of term t k,i , and sim(t k,i , t j ) denotes the similarity between terms t k,i and t j , which is calculated by word2vec. Specially the similarity between t k,i and t j is excluded when t k,i is equal to t j , which increases the possibility of selecting terms not belonging to r k when augmenting user reviews. For each user review, we traverse all unique terms in the training set to get the similarity values between terms and the user review. The top N terms in the training set ranked according to their similarity values are added to the end of the user review, which leads to an augmented user review. The value of N depends on the length of the user review and is calculated by Formula (4). θ is a threshold increasing from 0 with an increment interval of 0.1. With the increase of θ, the classification results in F-measure have no obvious tendency, making it challenging to choose the value of θ which achieves the best classification result in Fmeasure. To enhance the practicability of AUR-BoW, the maximum value of θ is set to 2. For Naï ve Bayes, J48, and Bagging (the three machine learning algorithms used for training classifiers, see Section 3.6), the values of θ of AUR-BoW are set to 1.9, 1.1, and 1.5 respectively, which achieve the best classification results in F-measure within the set range of (0, 2). For example, a preprocessed user review is "crash every time" (see Section 3.5 for details about preprocessing user reviews).</p><p>According to AUR-BoW when combined with J48, three words "freezing", "log", and "everytime" are chosen for augmenting the user review, which makes the user review contain more related information than its original form. The augmented user review for classification is "crash every time freezing log everytime". Different from BoW, TF-IDF, and CHI 2 , AUR-BoW exploits textual semantics of user reviews, while BoW, TF-IDF, and CHI 2 only consider whether a word is contained in a user review without considering the relationship between words. AUR-BoW makes use of words similarity to augment user reviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESEARCH DESIGN</head><p>The research design of this work is described in this section in detail. We first define two research questions (RQs) in Section 3.1. We explain why we classify user reviews into four types of NFRs (reliability, usability, portability, and performance), Functional Requirements (FRs), and Others in Section 3.2. We then introduce the dataset used for experiments in Section 3.3.</p><p>After that, we describe the user reviews classification process in Section 3.4. At last, we describe the important phases (Phase 2, Phase 5, and Phase 6) of the user reviews classification process in Section 3.5, 3.6, and 3.7 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Research Questions</head><p>The goal of this work is to help App developers to identify and consider NFRs in App development by automatically classifying user reviews. More specifically, we plan to study how accurately the classification techniques and machine learning algorithms can classify the user reviews into NFRs, FRs, and Others based on an NFR standard (ISO 25010) <ref type="bibr" target="#b34">[35]</ref>. Since classification techniques and machine learning algorithms are two main components of automatic user reviews classification as detailed in Section 3.4, two research questions are formulated:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RQ1. Which classification technique works best (BoW vs. TF-IDF vs. CHI 2 vs. AUR-BoW) for classifying user reviews into NFRs, FRs, and Others?</head><p>Rationale: We use four classification techniques to represent user reviews. Different from the three classification techniques BoW, TF-TDF, and CHI 2 , AUR-BoW proposed in this work exploits textual semantics to augment user reviews for classification. We therefore seek to understand whether AUR-BoW works best or not in all the four classification techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RQ2. Which machine learning algorithm works best (Naï ve Bayes vs. J48 vs. Bagging) for classifying user reviews into NFRs, FRs, and Others?</head><p>Rationale: A machine learning algorithm may lead to different performance when employed in various applications. For instance, Naï ve Bayes performs best when classifying user reviews into four types (Bug reports, Feature requests, User experiences, and Ratings) in <ref type="bibr" target="#b0">[1]</ref>, while J48 gets the best results when classifying user reviews into four types (Feature Request, Problem Discovery, Information Seeking, and Information Giving) in <ref type="bibr" target="#b7">[8]</ref>. By answering this question, we seek to understand which machine learning algorithm is most suitable for classifying user reviews into NFRs, FRs, and Others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Types of User Reviews</head><p>User reviews are automatically classified into six types according to ISO 25010 <ref type="bibr" target="#b34">[35]</ref>, which defines quality characteristics of software systems. In ISO 25010, quality requirements are classified into eight types: functional suitability, performance efficiency, compatibility, usability, reliability, security, maintainability, and portability. In these quality requirement types, functional suitability considers the meta level of requirements, while user reviews normally elaborate concrete requirements; maintainability is about the internal quality, while user reviews concern more about the external quality of a system; and security and compatibility are not found in our dataset (see Section 3.3). For these reasons, we consider four types of NFRs in ISO 25010: usability, reliability, portability, and performance, with FR and Others as the types for the classification of user reviews. The descriptions and examples of these types are provided in Table <ref type="table" target="#tab_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experiment Material</head><p>We evaluated the combination of the classification techniques and machine learning algorithms on a set of user reviews. We selected two popular Apps (iBooks in the books category from Apple App Store and WhatsApp in the communication category from Google Play) as the cases for experimentation. We collected 6696 raw user reviews from iBooks and 4400 raw user reviews from WhatsApp as the dataset. 21969 user review sentences obtained from the dataset are used for training word2vec. For each App, 2000 user review sentences were randomly sampled and manually classified, which are 4000 user review sentences in total and act as the ground truth for the evaluation (the processing process from raw user reviews to user review sentences is detailed in Section 3.4). All sampled user review sentences were manually classified by three researchers, the two authors and a master student in software engineering. We first conducted a pilot classification of 100 user review sentences from each App (200 user review sentences in total) by the three researchers independently through following an NFR standard (ISO 25010 <ref type="bibr" target="#b34">[35]</ref>), and any disagreements on the classification results were discussed and resolved by the three researchers. After reaching a consensus on the classification of user review types, the first author and the master student classified the remaining user review sentences independently and the agreement between them is 88% (3523/4000). After that any disagreements on labeled sentences were discussed and confirmed with the second author. At last, the agreement between the first author and the master student achieved 100%. Table <ref type="table" target="#tab_3">3</ref> shows the numbers and percentages of manually labeled user review sentences in the dataset that were classified as a certain type. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">User Reviews Classification Process</head><p>Fig. <ref type="figure">1</ref> shows the execution process we followed and the techniques and algorithms we employed to automatically classify user reviews. Specially, the process is composed of six phases (Phase 6 is about the evaluation of the classifiers): </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Preprocess User Reviews</head><p>User reviews preprocessing is composed of four steps: removing stop words, lemmatization and stemming, splitting user reviews into sentences, and transforming slang words or abbreviations into their basic forms. (1) Stop words usually refer to the most common words such as "is", "a", and "at", which do not influence the semantics of a user review. As there is no single universal list of stop words used by all natural language processing tools, we remove the words with a length of less than three letters. (2) In English, words appear in several inflected forms for grammatical reasons but have the same meaning, such as the verb "walk" may appear as "walks", "walked", and "walking". The goal of both stemming and lemmatization is to reduce inflectional forms, but they differ in their flavor. Lemmatization takes the morphological analysis and linguistic context of the term into consideration, while stemming usually refers to a crude heuristic process that cuts out the end of words. For example, if confronting with the token "better", stemming might just return "better", whereas lemmatization would attempt to return either "good" or "better" depending on the context. If confronting with the token "crashes", lemmatization might return "crashes" according to the context, whereas stemming would return "crash". In summary, we combine both lemmatization and stemming to get the basic form of a word. (3) A recent study found that up to 30% of user reviews raise various types of issues in a user review <ref type="bibr" target="#b11">[12]</ref>. NFRs and FRs are specific issues raised in user reviews, and to classify the content of user reviews in better granularity into NFRs, FRs, and Others, we split user reviews into sentences as the unit of classification. ( <ref type="formula">4</ref>) Occasionally users express their opinions with slang words or abbreviations, such as "idk" means "I would like" and "fav" means "favorite". To address this issue, after splitting user reviews into sentences, we transform these words into their basic forms using a continuously updated table of slang words and abbreviations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Train Classifiers</head><p>We used four classification techniques BoW, TF-IDF, CHI 2 , and AUR-BoW introduced in Section 2 to extract textual features, which act as the input of machine learning algorithms for training classifiers. Specially, we conducted experiments using Weka 3 with the three machine learning algorithms, Naive Bayes, J48, and Bagging. The reason that we chose these algorithms for user reviews classification is that they had been successfully employed for object classification in many previous works, e.g., <ref type="bibr" target="#b0">[1]</ref>[17] <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Evaluation Methodology</head><p>We used precision, recall, and F-measure (see Formula ( <ref type="formula">5</ref>), <ref type="bibr" target="#b5">(6)</ref>, and ( <ref type="formula">7</ref>)) which are commonly used in performance evaluation of information retrieval, and weighted average (see Formula ( <ref type="formula">8</ref>)) which is used in calculating user reviews classification results (e.g., <ref type="bibr" target="#b7">[8]</ref>) to measure the performance of classification results. Number i in Formula ( <ref type="formula">8</ref>) denotes the number of user review sentences in type i in the testing set.</p><p>In Formula ( <ref type="formula">5</ref>) and ( <ref type="formula">6</ref>), TP i denotes the number of instances classified as type i and actually are of type i; FP i denotes the number of instances classified as type i but actually are of type j where i ≠ j; FN i denotes the number of instances classified as type j and actually are of type i where i ≠ j. We performed a 10fold cross-validation on the dataset, and each fold contained 400 user review sentences. 3 http://www.cs.waikato.ac.nz/ml/weka/ (5) (6) ( <ref type="formula">7</ref>)</p><formula xml:id="formula_4">∑ (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS ANALYSIS</head><p>In this section we present the experiment results and discuss their implications. Table <ref type="table" target="#tab_4">4</ref> provides an overview of the experiment results obtained through the four classification techniques BoW, TF-IDF, CHI 2 , and AUR-BoW combined with three machine learning algorithms Naï ve Bayes, J48, and Bagging, which show the weighted average precision, recall, and F-measure of the classification results. In general, the precision, recall, and Fmeasure of all the combinations are higher than 0.644. To concentrate on a single variable for answering RQ1 and RQ2, we analyzed the classification results achieved by combining Naï ve Bayes with the four classification techniques (we did the same for J48 and Bagging) and combining AUR-BoW with the three machine learning algorithms. We present and analyze the results of RQ1 and RQ2, and discuss the classification results of different types in this section.</p><p>(1) RQ1. Classification techniques For answering RQ1, we found that the highest F-measure (0.718) is achieved by AUR-BoW which makes use of word2vec to exploit textual semantics to augment user reviews. From Table <ref type="table" target="#tab_4">4</ref>, it can be found that there is no much difference between TF-IDF and CHI 2 . When using Naï ve Bayes, compared with BoW, the F-measures achieved with TF-IDF, CHI 2 , and AUR-BoW are increased by 0.61%, 1.07%, and 4.74% respectively. When using J48, compared with BoW, the F-measures achieved with TF-IDF, CHI 2 , and AUR-BoW are increased by 0.75%, 0.75%, and 2.86% respectively. When using Bagging, compared with BoW, the Fmeasures achieved with TF-IDF, CHI 2 , and AUR-BoW are increased by 0.57%, 0.29%, and 4.06% respectively.</p><p>In the four classification techniques, AUR-BoW works best for classifying user reviews into NFRs, FRs, and Others. This finding can be attributed to the word2vec tool, which exploits surrounding words in a user review sentence, and maps words with similar meanings to similar vectors. When using all the three machine learning algorithms, compared with BoW, the F-measure achieved with AUR-BoW is increased much higher than that achieved with TF-IDF and CHI 2 . This finding indicates that for user review sentences, it is effective to improve user reviews classification results by adding textual semantics to the sentences. One explanation is that for short text like user review sentences, extending them by several most similar words can augment the (2) RQ2. Machine learning algorithms For AUR-BoW, the results in Table <ref type="table" target="#tab_4">4</ref> show that the highest precision and recall are achieved by Naï ve Bayes (0.720) and Bagging (0.723) respectively. Overall, for the balance between precision and recall, i.e., F-measure, the Bagging algorithm works best (0.718), which shows that Bagging is a more suitable machine learning algorithm for NFRs classification from user reviews than Naï ve Bayes and J48.</p><p>The best F-measure (0.718) got in our experiment is similar to the best F-measure (0.720) of the experimental results produced by Panichella et al. in <ref type="bibr" target="#b7">[8]</ref>, which classified user reviews into four types (Feature Request, Problem Discovery, Information Seeking, and Information Giving). The machine learning algorithm J48 gets the best F-measure in <ref type="bibr" target="#b7">[8]</ref>, while in our experiments Bagging performs best (and J48 gets the worst F-measure), which indicates that for various classification applications (i.e., classified into different types), the best performance can be produced by different machine learning algorithms. The possible reason is that user reviews are classified into different types, which have different textual features. For example, feature requests mainly focus on missing functionality of an App, while usability NFRs mainly focus on whether the App can satisfy users.</p><p>(</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Classification results of different types</head><p>We further analyze the classification results of different types achieved by the combination of AUR-BoW with Bagging, which gets the best F-measure in the experiments. Table <ref type="table" target="#tab_5">5</ref> shows the precision, recall, and F-measure for each type obtained by the combination of AUR-BoW with Bagging. We notice that there is an obvious difference between the F-measures of the classification results of different types ranging from 0.335 to 0.822. This finding can be attributed to that there are very few portability (119/4000, 2.9%) and performance (121/4000, 3.0%) NFRs in the user review sentences (i.e., the dataset of the experiments) compared with other types, which makes the textual features of these two NFR types for training classifiers are less than that of other types.</p><p>The F-measure of usability is higher than that of reliability and FR. This finding can be attributed to the reason that reliability and FR have lots of common words which makes it difficult to distinguish between reliability and FR. For example, the word "download", in a reliability NFR: "I bought a book on august 12 and it hasn't downloaded yet." which means that the normal function of downloading books was broken down; in a FR: "I downloaded some books individually on each device and the bookmarks won't sync either." which denotes that iBooks App provided no synchronization function of the bookmarks. Fig. <ref type="figure" target="#fig_1">2</ref> shows the comparison between the F-measure and proportion of each type in the dataset. The comparison indicates that the trend of the classification results of different types largely follows the trend of the proportions of the types in an unbalanced dataset (i.e., the dataset of the experiments). Automatic classification performs worse when the size of certain types is smaller (e.g., portability and performance), while there is no clear trend when the size of the types are similar (e.g., usability and reliability). This finding is interesting because it suggests that to some extent, increasing the size of certain types in the experiment dataset can improve the classification results of the types. This hypothesis should be further investigated with new experiments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">LIMITATION AND THREATS TO VALIDITY</head><p>In this section, we discuss the limitation and threats to validity in this work according to the guidelines in <ref type="bibr" target="#b32">[33]</ref>, and how these threats were partially mitigated in this work. Limitation refers to the issues which currently cannot be addressed by our approach. We labeled each user review sentence with one type, but in reality one user review sentence may belongs to more than one type of NFRs or FR. During the process of labeling user review sentences, we found that the proportion of the sentences with multiple types is 1.1% (44/4000) in our dataset. We manually separated these sentences into more fine-grained sentences which can be labeled in one type before conducting the classification experiments. For instance, the sentence "it uses more battery power, i know i can have a custom black background on my conversations but not on contacts and groups list, can't you guys create an option for that on a future update" should be classified as performance NFR and FR, we separated this user review into two sentences, one is "it uses more battery power" which was classified as performance NFR and the other is "i know i can have a custom black background on my conversations but not on contacts and groups list, can't you guys create an option for that on a future update" which was classified as FR. But we admit that this is a limitation of our approach which cannot attach multiple labels to one user review sentence.</p><p>Construct validity focuses on whether the theoretical constructs are interpreted and measured correctly. A threat to construct validity in this study involves whether the user review sentences used for the experiments were classified correctly by the researchers. To achieve a common understanding of various NFR types, we used the definitions of NFR types in the ISO 25010 standard <ref type="bibr" target="#b34">[35]</ref>. But using a standard cannot guarantee that the researchers understand the definitions of various NFR types in the same way, and another risk is that whether the researchers classify the user review sentences from iBooks and WhatsApp into the types which they actually belong to. A pilot classification of 100 user review sentences from each App (200 sentences in total) was conducted by three researchers independently, and any disagreements on the classification results were further discussed and resolved by the three researchers, in order to get a consensus among researchers on the classification of user review types. The remaining of the user review sentences were manually classified by the first author and a master student in software engineering independently, and any disagreements on labeled sentences were discussed with the second author. These measures were used to partially mitigate personal bias in user reviews classification. Another threat to the construct validity in this study is whether the user reviews used in the experiment are sufficient draw reasonable conclusions. To mitigate this threat, we used a random sample of collected user reviews.</p><p>Internal validity focuses on the design of a study, especially whether the results follow from the data. One threat to the internal validity in this study is the tests overfitting of the machine learning <ref type="bibr" target="#b36">[37]</ref>. To mitigate the influence of this threat, we applied a 10-fold cross-validation in our experiments.</p><p>External validity refers to the degree to which our findings from this study can be generalized in other settings. To mitigate this threat, we conducted the experiments on the user reviews of two popular Apps: iBooks (iOS) in the books category and WhatsApp (Android) in the communication category. The diversity of the chosen Apps and their platforms increases the generalizability of the experiment results and decreases the potential sampling bias. But it is still unclear whether this experiment can attain similar results when being applied to other categories of Apps (e.g., Facebook). Another threat to external validity is that two types of NFRs, security and compatibility, were not available in the experiment dataset, and there are very few portability and performance NFRs in the dataset. We plan to conduct a large-scale empirical study through collecting more user reviews from diverse Apps, in order to improve the external validity in the next step.</p><p>Reliability refers to whether the study yields the same results if other researchers replicate this study, which in this work is related to the processes of automatic user reviews classification (the process of the experiments) and manual user reviews classification (the process for producing the experiment dataset). By making explicit these processes (as detailed in Section 3.3 to Section 3.6), we believe that this threat is partially mitigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>We summarize and discuss the related work on user reviews analysis in this section, including user reviews filtering, classification, and summarization.</p><p>In recent years, App Store analysis has become a popular topic in software engineering <ref type="bibr" target="#b27">[28]</ref>, and users can easily submit their feedback about the Apps they used in App platforms. Al-Subaihin et al. extracted features from textual description of Apps in order to cluster the Apps using agglomerative hierarchical clustering and produced an effective categorization of Apps <ref type="bibr" target="#b29">[30]</ref>. Tian et al. analyzed the characteristics of user reviews and identified the characteristics of high-rated Apps <ref type="bibr" target="#b28">[29]</ref>. Study shows that user analytics tools will help developers to deal with the large numbers of user feedback (e.g., user reviews) by filtering, classifying, and summarizing them, to decide what requirements and features they should add, change, or eliminate <ref type="bibr" target="#b17">[18]</ref>.</p><p>App reviews filtering has drawn increasing attention in the software engineering community since only 1/3 of user reviews are useful for developers <ref type="bibr" target="#b7">[8]</ref>. Oh et al. proposed an algorithm that automatically identifies informative reviews reflecting user involvement to reduce the information overload of developers <ref type="bibr" target="#b20">[21]</ref>. Chandy and Gu proposed an approach to automatically identify spam user reviews in App Store using a latent class model with interpretable structure and low complexity <ref type="bibr" target="#b3">[4]</ref>. Our work is different from these works in that we try to employ supervised machine learning algorithms to not only filter non-informative user reviews but also classify user reviews into six types (four types of NFRs, FRs, and Others).</p><p>Recently, a number of approaches have been proposed for automatically classifying user reviews. Yang and Liang combined TF-IDF and regular expression (an NLP technique) with human intervention to classify user reviews into FRs and NFRs <ref type="bibr" target="#b37">[38]</ref>. Panichella et al. merged three techniques: natural language processing, text analysis, and sentiment analysis, to automatically classify user reviews into four types: feature request, problem discovery, information seeking, and information giving <ref type="bibr" target="#b7">[8]</ref>. Villarroel et al. not only classified user reviews into suggestion for new features, bug reports, and other types, but also clustered together related user reviews and recommended the user review cluster developers should satisfy in the next release <ref type="bibr" target="#b38">[39]</ref>. Hoon et al. developed three ontologies: emotion ontology, functional ontology, and quality ontology to classify user reviews into three categories which follow the three ontologies <ref type="bibr" target="#b35">[36]</ref>. <ref type="bibr">Maalej et al. and Blei et al.</ref> analyzed user reviews at the sentence level <ref type="bibr" target="#b0">[1]</ref>[8], while McIlroy et al. presented an approach at the user review level that can automatically assign multi-labels to user reviews <ref type="bibr" target="#b11">[12]</ref>. Similar with Chen and Kao's work which combined the word co-occurrence information with Bi-term topic model (BTM) to augment Topic Model and made Topic Model more appropriate for shorter text like tweets <ref type="bibr" target="#b39">[40]</ref>, in this work we not only split user reviews into sentences, but also augmented user review sentences by calculating the similarity between terms and user review sentences.</p><p>A number of researchers have focused on mining and analyzing textual data with the goal of deriving important information (e.g., NFRs) to help developers maintain and evolve their software systems. Sorbo et al. summarized thousands of user reviews to recommend future software changes according to the summary of user reviews <ref type="bibr" target="#b21">[22]</ref>. Rastkar et al. produced bug report summaries which help developers to save time in detecting duplicate bug reports <ref type="bibr" target="#b22">[23]</ref>. Gao et al. developed a tool AR-Tracker, which automatically collects user reviews of Apps and ranks them in order to optimize the representation of the reviews set <ref type="bibr" target="#b18">[19]</ref>. Guzman et al. used a feature and sentiment centric approach to extract different opinions and experiences about using Apps from user reviews <ref type="bibr" target="#b40">[41]</ref>. Other researchers proposed various approaches to automatically classify non-functional requirements from requirements specifications through information retrieval <ref type="bibr" target="#b24">[25]</ref>, clustering <ref type="bibr" target="#b25">[26]</ref>, and text mining techniques <ref type="bibr" target="#b33">[34]</ref> respectively. In this work, we focus on classifying NFRs from user reviews, which are short and unstructured compared with requirements specifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS AND FUTURE WORK</head><p>In this work, we combined four classification techniques with three machine learning algorithms to automatically classify user reviews into four types of NFRs (reliability, usability, portability, and performance), FRs, and Others. Specially, we exploited textual semantics to augment user reviews by word2vec for automatically classifying user reviews. We evaluated the combinations of the classification techniques and machine learning algorithms with user reviews collected from two popular Apps: iBooks and WhatsApp which belong to different categories (domains) from different App stores (platforms). We conducted experiments to compare the F-measure of the classification results through all the combinations. We found that the combination of AUR-BoW with Bagging achieves the highest F-measure of 71.8%, with a precision of 71.4% and a recall of 72.3% respectively. This finding shows that augmented user reviews can improve the results of user reviews classification. Moreover, in an unbalanced dataset, automatic classification performs worse when the size of certain types is obviously smaller. The automatic classification of NFRs from user reviews can help App developers to better understand user reviews and meet user needs from an NFR perspective, which is meaningful for developers to retain the existing users and attract new users.</p><p>In the next step, the approach AUR-BoW, which achieves the best classification results on NFRs, can be improved in two promising aspects:</p><p>(1) To validate AUR-BoW with the user reviews of other categories of Apps (e.g., Facebook) in order to improve the external validity of the results.</p><p>(2) To combine AUR-BoW with other classification techniques (CHI 2 , TF-IDF), which may achieve better results for user reviews classification.</p><p>(3) Two types of NFRs, security and compatibility, do not appear in the experiment dataset, and there are very few portability and performance NFRs in the dataset. We plan to collect user reviews from more Apps, which contain various types of NFRs for replicating the experiment. This is another aspect to mitigate the threats to the external validity: whether the results hold for other types of NFRs.</p><p>(4) The types is imbalanced in the experiment dataset. There are very few portability (119/4000, 2.9%) and performance (121/4000, 3.0%) NFRs in the user review sentences. We plan to use techniques (e.g., <ref type="bibr" target="#b41">[42]</ref>) to mitigate this imbalance in the dataset, which might improve the accuracy of types with fewer training data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Phase 1 :Phase 1 :Phase 2 :Phase 3 :Phase 6 :Figure 1 :</head><label>112361</label><figDesc>Fig.1shows the execution process we followed and the techniques and algorithms we employed to automatically classify user reviews. Specially, the process is composed of six phases (Phase 6 is about the evaluation of the classifiers):Phase 1: Input User Reviews: Collect user reviews from App Store by an open source tool 2 and Google Play Store.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison between the F-measure and proportion of each type in the dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Variables of Formula (2) Belong to category C k Do not belong to category C k The number of user reviews with term t i a b The number of user reviews without term t i c d</head><label>1</label><figDesc></figDesc><table><row><cell cols="2">1 begin = 1%</cell></row><row><cell cols="2">2 end = 100%</cell></row><row><cell cols="2">3 n = 100%</cell></row><row><cell cols="2">4 initialize best_classification_result according to n</cell></row><row><cell cols="2">5 while ( begin &lt; end)</cell></row><row><cell>6</cell><cell>n = ( begin + end ) / 2</cell></row><row><cell>7</cell><cell>get classification_result_in_F-measure according to n</cell></row><row><cell>8</cell><cell>if classification_result_in_F-measure &lt; best_classification_result</cell></row><row><cell>9</cell><cell>begin = n</cell></row><row><cell cols="2">10 else</cell></row><row><cell>11</cell><cell>end = n</cell></row><row><cell>12</cell><cell>best_classification_result = classification_result_in_F-measure</cell></row><row><cell cols="2">13 n = end</cell></row><row><cell cols="2">14 return n</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Algorithm 1: Binary search method for getting the value of n.</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 : Descriptions and Examples of Each Type (NFRs [35], FR, and Others) Type Description Example from User Reviews</head><label>2</label><figDesc>Usability Degree to which a product or system can be used by specified users to achieve specified goals with effectiveness, efficiency, and satisfaction in a specified context of use.</figDesc><table><row><cell>"I have to flip through chapters to start the</cell></row><row><cell>book."</cell></row></table><note><p><p>"can you plz make a screen recorder so we can make cool videos for youtube." Others Any types which are not associated with NFR and FR, e.g., emotional expressions.</p>"I loved this App."</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 : Numbers and Percentages of Manually Labeled User Review Sentences in the Dataset</head><label>3</label><figDesc></figDesc><table><row><cell>Type</cell><cell>#Sentence</cell><cell>Proportion</cell></row><row><cell>Usability</cell><cell>432</cell><cell>0.108</cell></row><row><cell>Reliability</cell><cell>587</cell><cell>0.147</cell></row><row><cell>Portability</cell><cell>119</cell><cell>0.029</cell></row><row><cell>Performance</cell><cell>121</cell><cell>0.030</cell></row><row><cell>FR</cell><cell>558</cell><cell>0.140</cell></row><row><cell>Others</cell><cell>2183</cell><cell>0.546</cell></row><row><cell>Total</cell><cell>4000</cell><cell>1.000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 : Results of the Combinations of One Classification Technique from BoW, TF-IDF, CHI 2 , and AUR-BoW with One Machine Learning Algorithm from Naï ve Bayes, J48, and Bagging</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>BoW</cell><cell></cell><cell></cell><cell>TF-IDF</cell><cell></cell><cell></cell><cell>CHI 2</cell><cell></cell><cell></cell><cell>AUR-BoW</cell></row><row><cell></cell><cell cols="2">Precision Recall</cell><cell>F1</cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>Naï ve Bayes</cell><cell>0.665</cell><cell>0.644</cell><cell cols="2">0.654 0.668</cell><cell>0.649</cell><cell cols="2">0.658 0.669</cell><cell>0.654</cell><cell>0.661</cell><cell>0.720</cell><cell>0.653</cell><cell>0.685</cell></row><row><cell>J48</cell><cell>0.656</cell><cell>0.674</cell><cell cols="2">0.665 0.661</cell><cell>0.678</cell><cell cols="2">0.670 0.661</cell><cell>0.680</cell><cell>0.670</cell><cell>0.677</cell><cell>0.690</cell><cell>0.684</cell></row><row><cell>Bagging</cell><cell>0.685</cell><cell>0.694</cell><cell cols="2">0.690 0.689</cell><cell>0.699</cell><cell cols="2">0.694 0.688</cell><cell>0.697</cell><cell>0.692</cell><cell>0.714</cell><cell>0.723</cell><cell>0.718</cell></row><row><cell cols="6">information of short text, and consequently provide more</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">information to improve classification results.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 : Percentages of Classified Sentences in the Dataset and Classification Results of Various Types Obtained by the Combination of AUR-BoW and Bagging</head><label>5</label><figDesc></figDesc><table><row><cell>Type</cell><cell>Proportion</cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell></row><row><cell>Usability</cell><cell>0.108</cell><cell>0.757</cell><cell>0.565</cell><cell>0.647</cell></row><row><cell>Reliability</cell><cell>0.147</cell><cell>0.595</cell><cell>0.552</cell><cell>0.572</cell></row><row><cell>Portability</cell><cell>0.029</cell><cell>0.632</cell><cell>0.327</cell><cell>0.431</cell></row><row><cell>Performance</cell><cell>0.030</cell><cell>0.596</cell><cell>0.233</cell><cell>0.335</cell></row><row><cell>FR</cell><cell>0.140</cell><cell>0.630</cell><cell>0.587</cell><cell>0.608</cell></row><row><cell>Others</cell><cell>0.546</cell><cell>0.770</cell><cell>0.881</cell><cell>0.822</cell></row><row><cell>Weighted Average</cell><cell>1.000</cell><cell>0.714</cell><cell>0.723</cell><cell>0.718</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://code.google.com/p/word2vec/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work has been partially sponsored by the Natural Science Foundation of China (NSFC) under Grant No. 61472286. We would like to thank Tianlu Wang, who participated in the manual classification of the user review sentences for the experiments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bug report feature request or simply praise? On automatically classifying app reviews</title>
		<author>
			<persName><forename type="first">W</forename><surname>Maalej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nabil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd IEEE International Requirements Engineering Conference (RE&apos;15)</title>
		<meeting>the 23rd IEEE International Requirements Engineering Conference (RE&apos;15)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="116" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">User feedback in the appstore: an empirical study</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pagano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Maalej</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st IEEE International Requirements Engineering Conference (RE&apos;13)</title>
		<meeting>the 21st IEEE International Requirements Engineering Conference (RE&apos;13)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="125" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Retrieving and analyzing mobile apps feature requests from online reviews</title>
		<author>
			<persName><forename type="first">C</forename><surname>Iacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Harrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 10th IEEE Working Conference on Mining Software Repositories (MSR&apos;13)</title>
		<meeting>eeding of the 10th IEEE Working Conference on Mining Software Repositories (MSR&apos;13)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="41" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Identifying spam in the IOS app store</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chandy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Joint WICOW/AIRWeb Workshop on Web Quality (WebQuality&apos;12)</title>
		<meeting>the 2nd Joint WICOW/AIRWeb Workshop on Web Quality (WebQuality&apos;12)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="56" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A comparative study on feature selection in text categorization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Machine Learning (ICML&apos;97)</title>
		<meeting>the 14th International Conference on Machine Learning (ICML&apos;97)</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="412" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">AR-miner: mining informative reviews for developers from mobile app marketplace</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Software Engineering (ICSE&apos;14)</title>
		<meeting>the 36th International Conference on Software Engineering (ICSE&apos;14)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="767" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How can I improve my app? Classifying user reviews for software maintenance and evolution</title>
		<author>
			<persName><forename type="first">S</forename><surname>Di Panichella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sorbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Guzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Visaggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Canfora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st IEEE International Conference on Software Maintenance and Evolution (ICSME&apos;15</title>
		<meeting>the 31st IEEE International Conference on Software Maintenance and Evolution (ICSME&apos;15</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="281" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">What parts of your apps are loved by users?</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering (ASE&apos;15)</title>
		<meeting>the 30th IEEE/ACM International Conference on Automated Software Engineering (ASE&apos;15)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="760" to="770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mining user opinions in mobile app reviews: a keyword-based approach</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Pham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering (ASE&apos;15)</title>
		<meeting>the 30th IEEE/ACM International Conference on Automated Software Engineering (ASE&apos;15)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="749" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop of 1st International Conference on Learning Representations (ICLR&apos;13)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Analyzing and automatically labelling the types of user issues that are raised in mobile app reviews</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mcilroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khalid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Hassan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Empirical Software Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1067" to="1106" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Understanding bag-of-words model: a statistical framework</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Machine Learning and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="43" to="52" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">From collective knowledge to intelligence: pre-requirements analysis of large and complex systems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Avgeriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Web 2.0 for Software Engineering (Web2SE&apos;10)</title>
		<meeting>the 1st Workshop on Web 2.0 for Software Engineering (Web2SE&apos;10)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="26" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An extensive empirical study of feature selection metrics for text classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Forman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1289" to="1305" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Text categorization algorithms using semantic approaches corpus-based thesaurus and WordNet</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="765" to="772" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Combining text mining and data mining for bug report classification?</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th IEEE International Conference on Software Maintenance and Evolution (ICSME&apos;14</title>
		<meeting>the 30th IEEE International Conference on Software Maintenance and Evolution (ICSME&apos;14</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="311" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Toward data-driven requirements engineering</title>
		<author>
			<persName><forename type="first">W</forename><surname>Maalej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nayebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Johann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ruhe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Software</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="48" to="54" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ar-tracker: track the dynamics of mobile apps via user review mining</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th IEEE Symposium on Service-Oriented System Engineering (SOSE&apos;15)</title>
		<meeting>the 10th IEEE Symposium on Service-Oriented System Engineering (SOSE&apos;15)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="284" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Review spam detection via temporal pattern discovery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD&apos;12)</title>
		<meeting>the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD&apos;12)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="823" to="831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Facilitating developer-user interactions with mobile app review digests</title>
		<author>
			<persName><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI&apos;13 Extended Abstracts on Human Factors in Computing Systems (CHI&apos;13)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1809" to="1814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">What would users change in my app? summarizing app reviews for recommending software changes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Di Sorbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Panichella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Alexandru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shimagaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Visaggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Canfora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering (FSE&apos;16)</title>
		<meeting>the 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering (FSE&apos;16)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="499" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic summarization of bug reports</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rastkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="366" to="380" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Analysis of user comments: an approach for software requirements evolution</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Galvis Carreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Winbladh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Software Engineering (ICSE&apos;13)</title>
		<meeting>the 35th International Conference on Software Engineering (ICSE&apos;13)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="582" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automated classification of non-functional requirements</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cleland-Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Settimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Solc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Requirements Engineering</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="103" to="120" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Detecting classifying and tracing nonfunctional software requirements</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Grant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Requirements Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Is it worth responding to reviews? A case study of the top free apps in the Google Play store</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mcilroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hassan</surname></persName>
		</author>
		<idno type="DOI">10.1109/MS.2015.149</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Software</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Survey of app store analysis for software engineering</title>
		<author>
			<persName><forename type="first">W</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harman</surname></persName>
		</author>
		<idno type="DOI">=10.1109/TSE.2016.2630689</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">What are the characteristics of high-rated apps? A case study on free Android applications</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nagappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Hassan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th IEEE International Conference on Software Maintenance and Evolution (ICSME&apos;15</title>
		<meeting>the 31th IEEE International Conference on Software Maintenance and Evolution (ICSME&apos;15</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="301" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Clustering mobile apps based on mined textual features</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Al-Subaihin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Capra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhangtavecchia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM&apos;16)</title>
		<meeting>the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM&apos;16)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<ptr target="http://www.statista.com/statistics/276623/number-of-apps-available-inleading-app-stores/" />
		<title level="m">Number of apps available in leading app stores as of</title>
		<imprint>
			<date type="published" when="2016-06">June 2016. 2016-07-01</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bagging boosting and C4.5</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th AAAI Conference on Artificial Intelligence (AAAI&apos;96)</title>
		<meeting>the 13th AAAI Conference on Artificial Intelligence (AAAI&apos;96)</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="725" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Guide to advanced empirical software engineering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Sjøberg</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-84800-044-5</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An empirical study on classification of non-functional requirements</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Software Engineering and Knowledge Engineering (SEKE&apos;15)</title>
		<meeting>the 23rd International Conference on Software Engineering and Knowledge Engineering (SEKE&apos;15)</meeting>
		<imprint>
			<publisher>Knowledge Systems Institute</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="190" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">ISO/IEC 25010, 2011. Systems and software engineering -Systems and software Quality Requirements and Evaluation (SQuaRE) -System and software quality models</title>
		<author>
			<persName><surname>Iso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISO/IEC FDIS 25010</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">App reviews: breaking the user and developer language barrier</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Rodriguez-Garcí A</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Valencia-Garcí A</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Trends and Applications in Software Engineering</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="223" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Overfitting and undercomputing in machine learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="326" to="327" />
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Identification and classification of requirements from app user reviews</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Software Engineering and Knowledge Engineering (SEKE&apos;15)</title>
		<meeting>the 27th International Conference on Software Engineering and Knowledge Engineering (SEKE&apos;15)</meeting>
		<imprint>
			<publisher>Knowledge Systems Institute</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Release planning of mobile apps based on user reviews</title>
		<author>
			<persName><forename type="first">L</forename><surname>Villarroel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bavota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Oliveto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Di Penta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Software Engineering (ICSE&apos;16)</title>
		<meeting>the 38th International Conference on Software Engineering (ICSE&apos;16)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="14" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Word co-occurrence augmented topic model in short text</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Kao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computational Linguistics and Chinese Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="45" to="64" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Retrieving diverse opinions from app reviews</title>
		<author>
			<persName><forename type="first">Emitza</forename><surname>Guzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Bruegge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM&apos;15)</title>
		<meeting>the 9th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM&apos;15)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Class imbalance, redux</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Small</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brodley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Trikalinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th IEEE International Conference on Data Mining (ICDM&apos;11)</title>
		<meeting>the 11th IEEE International Conference on Data Mining (ICDM&apos;11)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="754" to="763" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
