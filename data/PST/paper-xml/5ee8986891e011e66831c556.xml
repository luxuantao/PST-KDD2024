<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GNNGUARD: Defending Graph Neural Networks against Adversarial Attacks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-16">16 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
							<email>xiang_zhang@hms.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
							<email>marinka@hms.harvard.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GNNGUARD: Defending Graph Neural Networks against Adversarial Attacks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-16">16 Jun 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2006.08149v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning methods for graphs achieve remarkable performance on many tasks. However, despite the proliferation of such methods and their success, recent findings indicate that small, unnoticeable perturbations of graph structure can catastrophically reduce performance of even the strongest and most popular Graph Neural Networks (GNNs). Here, we develop GNNGUARD, a general defense approach against a variety of training-time attacks that perturb the discrete graph structure. GNNGUARD can be straightforwardly incorporated into any GNN. Its core principle is to detect and quantify the relationship between the graph structure and node features, if one exists, and then exploit that relationship to mitigate negative effects of the attack. GNNGUARD uses network theory of homophily to learn how best assign higher weights to edges connecting similar nodes while pruning edges between unrelated nodes. The revised edges then allow the underlying GNN to robustly propagate neural messages in the graph. GNNGUARD introduces two novel components, the neighbor importance estimation, and the layer-wise graph memory, and we show empirically that both components are necessary for a successful defense. Across five GNNs, three defense methods, and four datasets, including a challenging human disease graph, experiments show that GNNGUARD outperforms existing defense approaches by 15.3% on average. Remarkably, GN-NGUARD can effectively restore the state-of-the-art performance of GNNs in the face of various adversarial attacks, including targeted and non-targeted attacks.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning on graphs and Graph Neural Networks (GNNs), in particular, have achieved remarkable success in a variety of application areas <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. The key to the success of GNNs is the neural message passing scheme <ref type="bibr" target="#b4">[5]</ref> in which neural messages are propagated along edges of the graph and typically optimized for performance on a downstream task. In doing so, the GNN is trained to aggregate information from neighbors for every node in each layer, which allows the model to eventually generate representations that capture useful node feature as well as topological structure information <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. While the aggregation of neighbor nodes' information is a powerful principle of representation learning, the way that GNNs exchange that information between nodes makes them vulnerable to adversarial attacks <ref type="bibr" target="#b7">[8]</ref>. Adversarial attacks on graphs, which carefully rewire the graph topology by selecting a small number of edges or inject carefully designed perturbations to node features, can contaminate local node neighborhoods, degrade learned representations, confuse the GNN to misclassify nodes in the graph, and can catastrophically reduce the performance of even the strongest and most popular GNNs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. This lack of GNN robustness is a critical issue in many application areas where adversarial perturbations can not only undermine public trust and slow down science and development of technology but can also substantially interfere with human decision making <ref type="bibr" target="#b10">[11]</ref>. For this reason, it is vital to develop GNNs that are robust against adversarial attacks. While the vulnerability of machine learning methods to adversarial attacks has raised many concerns and has led to theoretical insights into robustness <ref type="bibr" target="#b11">[12]</ref> and the development of effective defense techniques <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13]</ref>, adversarial attacks and defense on graphs remain poorly understood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Poisoned node</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adversarial attack</head><p>Ground-truth label 1 Ground-truth label 2 Predict</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classifier with GNNGuard GNNGuard[GNN]</head><p>Poisoned node is classified correctly Present work. Here, we introduce GNNGUARD <ref type="foot" target="#foot_0">1</ref> , an approach that can defend any GNN model against a variety of training-time attacks that perturb graph structure (Figure <ref type="figure" target="#fig_0">1</ref>). GNNGUARD takes as input an existing GNN model. It mitigates adverse effects by modifying the GNN's neural message passing operators. In particular, it revises the message passing architecture such that the revised model is robust to adversarial perturbations while at the same time the model keeps it representation learning capacity. To this end, GNNGUARD develops two key components that estimate neighbor importance for every node and coarsen the graph through an efficient memory layer. The former component dynamically adjusts the relevance of nodes' local network neighborhoods, prunes likely fake edges, and assigns less weight to suspicious edges based on network theory of homophily <ref type="bibr" target="#b13">[14]</ref>. The latter components stabilizes the evolution of graph structure by preserving, in part the memory from a previous layer in the GNN.</p><p>We compare GNNGUARD to three state-of-the-art GNN defenders across four datasets and under a variety of attack types, including direct targeted, influence targeted, and non-targeted attacks.</p><p>Experiments show that GNNGUARD improves state-of-the-art methods by up to 15.3% in defense performance. Importantly, unlike existing GNN defenders <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>, GNNGUARD is a general approach and can be effortlessly combined with any GNN architecture. To that end, we integrate GNNGUARD into five GNN models. Remarkably, results show that GNNGUARD can effectively restore state-of-the-art performance of even the strongest and most popular GNNs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>, thereby demonstrating broad applicability and relevance of GNNGUARD for graph machine-learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Adversarial attacks in continuous and discrete space. Adversarial attacks on machine learning have received increasing attention in recent years <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22]</ref>. The attackers add small perturbations on the samples to completely alter the output of the machine learning model. The deliberately manipulated perturbations are often designed to be unnoticeable. Modern studies have shown that machine leaning models, especially deep neural networks, are highly fragile to adversarial attacks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. The majority of existing works focus on grid data or independent samples <ref type="bibr" target="#b25">[26]</ref> whilst a few work investigate adversarial attack on graphs.</p><p>Adversarial attacks on graphs. Based on the goal of the attacker, adversarial attacks on graphs <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> can be divided into poisoning attacks poisoning attacks poisoning attacks poisoning attacks poisoning attacks poisoning attacks poisoning attacks poisoning attacks poisoning attacks poisoning attacks poisoning attacks poisoning attacks poisoning attacks poisoning attacks poisoning attacks poisoning attacks poisoning attacks (e.g., Nettack <ref type="bibr" target="#b7">[8]</ref>) that perturb the graph in training-time and evasion attacks evasion attacks evasion attacks evasion attacks evasion attacks evasion attacks evasion attacks evasion attacks evasion attacks evasion attacks evasion attacks evasion attacks evasion attacks evasion attacks evasion attacks evasion attacks evasion attacks (e.g., RL-S2V <ref type="bibr" target="#b27">[28]</ref>) that perturb the graph in testing-time. GNNGUARD is designed to improve robustness of GNNs against poisoning attacks. There are two types of poisoning attacks: a targeted attack and a non-targeted attack <ref type="bibr" target="#b28">[29]</ref>. The former deceives the model to misclassify a specific node (i.e., target node) <ref type="bibr" target="#b7">[8]</ref> while the latter degrades the overall performance of the trained model <ref type="bibr" target="#b25">[26]</ref>. The targeted attack can be categorized into direct targeted attack where the attacker perturbs edges touching the target node and the influence targeted attack where the attacker only manipulates edges of the target node's neighbors. Nettack <ref type="bibr" target="#b7">[8]</ref> generates perturbations by modifying graph structure (i.e., structure attack) and node attributes (i.e., feature attack) such that perturbations maximally destroy downstream GNN's predictions. Bojcheshki et al. <ref type="bibr" target="#b29">[30]</ref> derive adversarial perturbations that poison the graph structure. Similarly, Zügner et al. <ref type="bibr" target="#b25">[26]</ref> propose a non-targeted poisoning attacker by using meta-gradient to solve bi-level problem. In contrast, our GNNGUARD is a defense approach that inspects the graph and recovers adversarial perturbations.</p><p>Defense on graphs. While deep learning on graphs has shown exciting results in a variety of applications <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>, little attention has been paid to the robustness of such models, in contrast to an abundance of research for image (e.g., <ref type="bibr" target="#b33">[34]</ref>) and text (e.g., <ref type="bibr" target="#b34">[35]</ref>) adversarial defense. We briefly overview the state-of-the-art defense methods on graphs. GNN-Jaccard <ref type="bibr" target="#b14">[15]</ref> is a defense approach that pre-processes the adjacency matrix of the graph to identify the manipulated edges. While GNN-Jaccard can defend targeted adversarial attacks on known and already existing GNNs, there has also been work on novel, robust GNN models. For example, RobustGCN <ref type="bibr" target="#b16">[17]</ref> is a novel GNN that adopts Gaussian distributions as the hidden representations of nodes in each convolutional layer to absorb the effect of an attack. Similarly, GNN-SVD <ref type="bibr" target="#b15">[16]</ref> uses a low-rank approximation of adjacency matrix that drops noisy information through an SVD decomposition. Tang et al. <ref type="bibr" target="#b17">[18]</ref> improve the robustness of GNNs against poisoning attack through transfer learning but has a limitation that requires several unperturbed graphs from the similar domain during training. However, all these approaches have drawbacks (see Section 4.3) that prevent them from realizing their potential for defense to the fullest extent. GNNGUARD eliminates these drawbacks, successfully defending targeted and non-targeted poisoning attacks on any GNN without decreasing its accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background and Problem Formulation</head><p>Let G = (V, E, X) denotes a graph where V is the set of nodes, E is the set of edges and X = {x 1 , ..., x n }, x u ∈ R M is the M -dimensional node feature for node u ∈ V. Let N = |V| and E = |E| denote the number of nodes and edges, respectively. Let A ∈ R N ×N denote an adjacency matrix whose element A uv ∈ {0, 1} indicates existence of edge e uv that connects node u and v. We use N u to denote immediate neighbors of node u, including the node itself (u ∈ N u ). We use N * u to indicate u's neighborhood, excluding the node itself (u / ∈ N * u ). Without loss of generality, we consider node classification task, wherein a GNN f classifies nodes into C labels. Let ŷu = f u (G) denote prediction for node u, and let y u ∈ {1, . . . , C} denote the associated ground-truth label for node u. To degrade the performance of f , an adversarial attacker perturbs edges in G, resulting in the perturbed version of G, which we call G = (V, E , X) (A is adjacency matrix of G ).</p><p>Background on graph neural networks. Graph neural networks learns compact, low-dimensional representations, i.e., embeddings, for nodes such that representation capture nodes' local network neighborhoods as well as nodes' features <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b35">36]</ref>. The learned embeddings can be used for a variety of downstream tasks <ref type="bibr" target="#b2">[3]</ref>. Let h k u ∈ R D k denote the embedding of node u in the k-th layer of GNN, k = {1, . . . , K}. The D k stands for the dimension of h k u . Note that h 0 u = x u . The computations in the k-th layer consist of a message-passing function MSG, an aggregation function AGG, and an update function UPD. This means that a GNN f can be specified as f = (MSG, AGG, UPD) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">32]</ref>. Given a node u and its neighbor v ∈ N u , the messaging-passing function MSG specifies what neural message m k uv needs to be propagated from v to u. The message is calculated by</p><formula xml:id="formula_0">m k uv = MSG(h k u , h k v , A uv )</formula><p>, where MSG receives node embeddings of u and v along with their connectivity information e uv . This is followed by the aggregation function AGG that aggregates all messages received by u. The aggregated message mk u is computed by mk</p><formula xml:id="formula_1">u = AGG({m k uv ; v ∈ N * u }).</formula><p>Lastly, the update function UPD combines u's embedding h k u and the aggregated message mk u to generate the embedding for next layer as</p><formula xml:id="formula_2">h k+1 u = UPD(h k u , mk u ).</formula><p>The final node representation for u is h K u , i.e., the output of the K-th layer.</p><p>Background on poisoning attacks. Attackers try to fool a GNN by corrupting the graph topology during training <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>. The attacker carefully selects a small number of edges and manipulates them through perturbation and rewiring. In doing so, the attacker aims to fool the GNN into making incorrect predictions <ref type="bibr" target="#b17">[18]</ref>. The attacker finds optimal perturbation A through optimization <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b7">8]</ref>:</p><formula xml:id="formula_3">argmin A ∈P G ∆ L attack (f(A , X; Θ * ), y) s.t. Θ * = argmin Θ L predict (f(A , X; Θ), y)<label>(1)</label></formula><p>where y denotes ground-truth labels, L attack denotes the attacker's loss function, and L predict denotes GNN's loss. The Θ * refers to optimal parameters and f(A , X; Θ * ) is prediction of f with parameters Θ * on the perturbed graph A and node features X. To ensure that attacker perturbs only a small number of edges, a budget ∆ is defined to constrain the number of perturbed edges: ||A − A|| 0 ∆ and P G ∆ are perturbations that fit into budget ∆. Let T be target nodes that are intended to be mis-classified, and let A be attacker nodes that are allowed to be perturbed. We consider three types of attacks. ( <ref type="formula" target="#formula_3">1</ref> Non-targeted attacks. The attacker aims to degrade overall GNN classification performance <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">39]</ref>. Here, T = A = V test where V test denotes the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">GNNGUARD: Problem Formulation</head><p>GNNGUARD is a defense mechanism that is easy to integrate into any GNN f , resulting in a new GNN f that is robust to poisoning attacks. This means that f can make correct predictions even when trained on poisoned graph G . Given a GNN f = (MSG, AGG, UPD), GNNGUARD will return a new GNN f = (MSG , AGG , UPD ), where MSG is the message-passing function, AGG is the aggregation function, and UPD is the update function. The f solves the following defense problem. Problem (Defense Against Poisoning Attacks on Graphs). In a poisoning attack, the attacker injects adversarial edges in G, meaning that the attack changes training data, which can decrease the performance of GNN considerably. Let G denote the perturbed version of G that is poisoned by the attack. We seek GNN f such that for any node u ∈ G :</p><formula xml:id="formula_4">min f u (G ) − f u (G),<label>(2)</label></formula><p>where f u (G ) = ŷ u is the prediction when GNN f is trained on G . Here, f u (G) = ŷu denotes a hypothetical prediction that the GNN would made if it had access to clean graph G.</p><p>It is worth noting that, in this paper, we learn a defense mechanism for semi-supervised node classification. GNNGUARD is a general framework for defending any GNN on various graph mining tasks such as link prediction. Since there exists a variety of GNNs that achieve competitive performance on G, an intuitive idea is to force f u (G ) to approximate f u (G) and, in doing so, ensure that f will make correct predictions on G . For this reason, we design f to learn neural messages on G that, in turn, are similar to the messages that a hypothetical f would learn on G. However, since it is impossible to access clean graph G, Eq. ( <ref type="formula" target="#formula_4">2</ref>) can not be directly optimized. The key to restore the structure of G is to design a message-passing scheme that can detect fake edges, block them and then attend to true, unperturbed edges. To this end, the impact of perturbed edges in G can be mitigated by manipulating the flow of neural messages and thus, the structure of G can be restored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GNNGUARD</head><p>Next, we describe GNNGUARD, our GNN defender against poisoning attacks. Recent studies found <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b14">15]</ref> that most damaging attacks add fake edges between nodes that have different, i.e., dissimilar, features and labels. Because of that, the core defense principle of GNNGUARD is to detect such fake edges and alleviate their negative impact on prediction by them or assigning them lower weights in neural message passing. GNNGUARD has two key components: (1) neighbor importance estimation, and (2) layer-wise graph memory, the first component being an essential part of a robust GNN architecture while the latter is designed to smooth the defense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Neighbor Importance Estimation</head><p>GNNGUARD estimates an importance weight for every edge e uv to quantify how relevant node u is to another node v in the sense that it allows for successful routing of GNN's messages. In contrast to attention mechanisms (e.g., GAT <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b39">40]</ref>), GNNGUARD determines importance weights using theory of network homophily <ref type="bibr" target="#b13">[14]</ref>, positing that similar nodes (i.e., nodes with similar features) are more likely to interact than dissimilar nodes. To this end, we quantify similarity s k uv between u and its neighbor v in the k-th layer of GNN as follows:</p><formula xml:id="formula_5">s k uv = d(h k u , h k v ) = (h k u h k v )/(||h k u || 2 ||h k v || 2 )</formula><p>, where d is a similarity function and denotes dot product. In this work, we use cosine similarity for d <ref type="bibr" target="#b40">[41]</ref>. Larger similarity s k uv indicates that edge e uv is strongly supported by node features of the edge's endpoints.</p><p>We normalize s k uv at the node-level within u's neighborhood N u . The problem here is to specify what is the similarity of the node to itself. We normalize node similarities as:  where</p><formula xml:id="formula_6">α k uv = s k uv / v∈N * u s k uv × N k u /( N k u + 1) if u = v 1/( N k u + 1) if u = v,<label>(3)</label></formula><formula xml:id="formula_7">N k u = v∈N * u ||s k uv || 0 .</formula><p>We refer to α k uv as an importance weight representing the contribution of node v towards node u in the GNN's passing of neural messages in poisoned graph G . In doing so, GNNGUARD assigns small importance weights to suspicious neighbors, which reduce the interference of suspicious nodes in GNN's operation. Further, to alleviate the impact of fake edges, we prune edges that are likely forged. Building on network homophily and findings <ref type="bibr" target="#b14">[15]</ref> that fake edges tend to connect dissimilar nodes, we prune edges using importance weights. For that, we define a characteristic vector c k uv = [α k uv , α k vu ] describing edge e uv . Although s k uv = s k vu , it is key to note that α k uv = α k vu because of self-normalization in Eq. ( <ref type="formula" target="#formula_6">3</ref>). GNNGUARD calculates edge pruning probability for e uv through a non-linear transformation as σ(c k uv W ). Then, it maps the pruning probability to a binary indicator 1 P0 : σ(c k uv W ) where P 0 is a user-defined threshold:</p><formula xml:id="formula_8">1 P0 (σ(c k uv W )) = 0 if σ(c k uv W ) &lt; P 0 1 otherwise.<label>(4)</label></formula><p>Finally, we prune edges by updating importance weight α k uv to αk uv as follows:</p><formula xml:id="formula_9">αk uv = α k uv 1 P0 (σ(c k uv W )),<label>(5)</label></formula><p>meaning that the perturbed edges connecting dissimilar nodes will likely be ignored by the GNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Layer-Wise Graph Memory</head><p>Neighbor importance estimation and edge pruning change the graph structure between adjacent GNN layers. This can destabilize GNN training, especially if a considerable number of edges gets pruned in a single layer (e.g., due to the weight initialization). To allow for robust estimation of importance weights and smooth evolution of edge pruning, we use layer-wise graph memory. This unit, applied at each GNN layer, keeps partial memory of the pruned graph structure from the previous layer (Figure <ref type="figure" target="#fig_2">2</ref>). We define layer-wise graph memory as follows:</p><formula xml:id="formula_10">ω k uv = βω k−1 uv + (1 − β) αk uv ,<label>(6)</label></formula><p>where ω k uv represents defense coefficient for edge e uv in the k-th layer and β is a memory coefficient specifying memory, i.e., the amount of information from the previous layer that should be kept in the current layer. Memory coefficient β ∈ [0, 1] is a learnable parameter and is set to β = 0 in the first GNN layer, meaning that ω 0 uv = α0 uv . Using defense coefficients, GNNGUARD controls information flow across all neural message passing layers. It strengthens messages from u's neighbors with higher defense coefficients and weakens messages from u's neighbors with lower defense coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Overview of GNNGUARD</head><p>GNNGUARD is shown in Algorithm 1. The method is easy to plug into an existing GNN to defend the GNN against poisoning attacks. Given a GNN f = (MSG, AGG, UPD), GNNGUARD formulates a revised version of it, called f = (MSG , AGG , UPD ). In each layer, f takes current node representations and (possibly attacked) graph G . It estimates importance weights αuv and generates defense coefficients ω uv by combining importance weights from the current layer and defense coefficients from the previous layer. In summary, aggregation function AGG in layer k</p><formula xml:id="formula_11">is: AGG = AGG({ω k uv m k uv ; v ∈ N * u }). The update function UPD is: UPD = UPD(ω k uu h k u , AGG({ω k uv m k uv ; v ∈ N * u })).</formula><p>The message function MSG remains unchanged MSG = MSG as neural messages are specified by the original GNN f . Taken together, the guarded f attends differently to different node neighborhoods and propagates neural information only along most relevant edges. Our derivations here are for undirected graphs with node features but can be extended to directed graphs and edge features (e.g., include them into calculation of characteristic vectors). Algorithm 1: GNNGUARD.</p><p>Input: GNN model of interest f = (MSG, AGG, UPD); Poisoned graph G = (V, E , X), (A is adjacency matrix of E ); Trainable parameters Θ, W , and β Initialize parameters Θ, W , and β; initialize node representations h 0 u = xu ∀u ∈ V for layer k ← 1 to K do for u ∈ V do Calculate α k uv using Eq. ( <ref type="formula" target="#formula_6">3</ref>) for all v ∈ Nu // Neighbor Importance Estimation</p><formula xml:id="formula_12">c k uv = [α k uv , α k vu ] αk uv = α k uv 1P 0 (σ(c k uv W</formula><p>)) using Eq. ( <ref type="formula" target="#formula_9">5</ref>)</p><formula xml:id="formula_13">ω k uv = βω k−1 uv + (1 − β) αk uv using Eq. (6) // Layer-Wise Graph Memory m k uv = MSG (h k u , h k u , A uv ) using Section 4.3 // Neural Message Passing mk u = AGG ({ω k uv m k uv ; v ∈ N * u }) using Section 4.3 h k+1 u = UPD (ω k uu h k u , mk u ) using Section 4.3 end end</formula><p>Any GNN model. State-of-the-art GNNs use neural message passing comprising of MSG, AGG, and UPD functions. As we demonstrate in experiments, GNNGUARD can defend such GNN architectures against adversarial attacks. GNNGUARD works with many GNNs, including Graph Convolutional Network (GCN) <ref type="bibr" target="#b2">[3]</ref>, Graph Attention Network (GAT) <ref type="bibr" target="#b18">[19]</ref>, Graph Isomorphism Network (GIN) <ref type="bibr" target="#b6">[7]</ref>, Jumping Knowledge (JK-Net) <ref type="bibr" target="#b19">[20]</ref>, GraphSAINT <ref type="bibr" target="#b20">[21]</ref>, GraphSAGE <ref type="bibr" target="#b35">[36]</ref>, and SignedGCN <ref type="bibr" target="#b41">[42]</ref>.</p><p>Computational complexity. GNNGUARD is practically efficient because it exploits the sparse structure of real-world graphs. The time complexity of neighbor importance estimation is O(D k E) in layer k, where D k is the embedding dimensionality and E is the graph size, and the complexity of layer-wise graph memory is O(E). This means that time complexity of GNNGUARD grows linearly with the size of the graph as node embeddings are low-dimensional, D k E. Finally, the time complexity of a GNN endowed with GNNGUARD is on the same order as that of the GNN itself.</p><p>Further related work on adversarial defense for graphs. We briefly contrast GNNGUARD with existing GNN defenders. Compared to GNN-Jaccard <ref type="bibr" target="#b14">[15]</ref>, which examines fake edges as a GNN preprocessing step, GNNGUARD dynamically updates defense coefficients at every GNN layer for defense. In contrast to RobustGCN <ref type="bibr" target="#b16">[17]</ref>, which is limited to GCN, a particular GNN variant, and is challenging to use with other GNNs, GNNGUARD provides a generic mechanism that is easy to use with many GNN architectures. Further, in contrast to GNN-SVD <ref type="bibr" target="#b15">[16]</ref>, which uses only graph structure for defense, GNNGUARD takes advantage of information encoded in both node features and graph structure. Also, <ref type="bibr" target="#b15">[16]</ref> is designed specifically for the Nettack attacker <ref type="bibr" target="#b7">[8]</ref> and so is less versatile. Another technique <ref type="bibr" target="#b17">[18]</ref> uses transfer learning to detect fake edges. While that is an interesting idea, it requires a large number of clean graphs from the same domain to successfully train the transfer model. On the contrary, GNNGUARD takes advantage of correlation between node features and graph structure and does not need any external data. Further, recent studies (e.g., <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>) focus on theoretical certificates for GNN robustness instead of defense mechanisms. That is an important but orthogonal direction to this paper, where the focus is on a practical adversarial defense framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Datasets. We test GNNGUARD on four graphs. We use two citation networks with undirected edges and binary features: Cora <ref type="bibr" target="#b44">[45]</ref> and Citeseer <ref type="bibr" target="#b45">[46]</ref>. We also consider a directed graph with numeric node features, ogbn-arxiv <ref type="bibr" target="#b46">[47]</ref>, representing a citation network of CS papers published between 1971 and 2014. We use a Disease Pathway (DP) <ref type="bibr" target="#b47">[48]</ref> graph with continuous features describing a system Generating adversarial attacks. We compare our model to baselines under three kinds of adversarial attacks: direct targeted attack (Nettack-Di <ref type="bibr" target="#b7">[8]</ref>), influence targeted attack (Nettack-In <ref type="bibr" target="#b7">[8]</ref>), and non-targeted attack (Mettack <ref type="bibr" target="#b25">[26]</ref>). In Mettack, we set the perturbation rate as 20% (i.e., ∆ = 0.2E) with 'Meta-Self' training strategy. In Nettack-Di, ∆ = N 0 u . In Nettack-In, we perturb 5 neighbors of the target node and set ∆ = N 0 v for all neighbors. In the targeted attack, we select 40 correctly classified target nodes (following <ref type="bibr" target="#b7">[8]</ref>): 10 nodes with the largest classification margin, 20 random nodes, and 10 nodes with the smallest margin. We run the whole attack and defense procedure for each target node and report average classification accuracy. (2) GNNs. GNNs. GNNs. GNNs. GNNs.</p><p>GNNs. GNNs. GNNs. GNNs. GNNs. GNNs. GNNs. GNNs. GNNs. GNNs. GNNs. GNNs. We integrate GNNGUARD with five GNNs (GCN <ref type="bibr" target="#b2">[3]</ref>, GAT <ref type="bibr" target="#b18">[19]</ref>, GIN <ref type="bibr" target="#b6">[7]</ref>, JK-Net <ref type="bibr" target="#b19">[20]</ref>, and GraphSAINT <ref type="bibr" target="#b20">[21]</ref>) and present the defense performance against adversarial attacks. ( <ref type="formula" target="#formula_6">3</ref> Baseline defense algorithms. We compare GNNGUARD to three state-of-the-art graph defenders: GNN-Jaccard <ref type="bibr" target="#b14">[15]</ref>, RobustGCN <ref type="bibr" target="#b16">[17]</ref>, and GNN-SVD <ref type="bibr" target="#b15">[16]</ref>. Hyperparameters and model architectures are in Appendix E. Results for direct targeted attacks. We observe in Table <ref type="table" target="#tab_1">1</ref> that Nettack-Di is a strong attacker and dramatically cuts down the performance of all GNNs (cf. "Attack" vs. "No Attack" columns). However, the proposed GNNGUARD outperforms state-of-art defense methods by 15.3% in the accuracy on average. Further, it successfully restores the performance of GNNs to the level comparable to when there is no attack. We also observe that RobustGCN fails to defend against Nettack-Di, possibly because the Gaussian layer in RobustGCN cannot absorb big effects when all fake edges are in the vicinity of a target node. In contrast, GNN-SVD works well here because it is sensitive to high-rank noise caused by the perturbation of many edges that are incident to a single node. ( <ref type="formula" target="#formula_4">2</ref> Results for non-targeted attacks. Table <ref type="table" target="#tab_4">2</ref> shows that Mettack has a considerable negative impact on GNN performance, decreasing the accuracy of even the strongest GNN by 18.7% on average. Moreover, we see that GNNGUARD achieves a competitive performance and outperforms baselines in 19 out of 20 settings. In summary, experiments show the GNNGUARD consistently outperforms all baseline defense techniques. Further, GNNGUARD can defend a variety of GNNs against different types of attacks, indicating that GNNGUARD is a powerful GNN defender against adversarial poisoning. Ablation study. We conduct an ablation study to evaluate the necessity of every component of GNNGUARD. For that, we took the largest dataset (ogbn-arxiv) and the most threatening attack (Nettack-Di) as an example. Results are in Table <ref type="table">3</ref> (Left). We observe that full GNNGUARD behaves A case study of attack and defense. We report an example of attack and defense illustrating how GNNGUARD works. Let's examine the paper "TreeP: A Tree Based P2P Network Architecture" by Hudzia et al. <ref type="bibr" target="#b48">[49]</ref> that received four citations. The topic/label of this paper (i.e., node u in ogbn-arxiv graph G) and its cited works (i.e., neighbors) is Internet Technology (IT). A GIN <ref type="bibr" target="#b6">[7]</ref> trained on clean ogbn-arxiv graph makes a correct prediction for the paper with high confidence, f u (G) = 0.536. Then, we poison the paper using Nettack-Di attacker, which adds four fake citations between the paper and some very dissimilar papers from the Artificial Intelligence (AI) field. We re-trained GIN on perturbed graph G and found the resulting classifier misclassifies the paper <ref type="bibr" target="#b48">[49]</ref> into topic AI with confidence of f u (G ) = 0.201, which is high on this prediction task with 40 distinct topics/labels. This fragility is especially worrisome as the attacker has only injected four fake citations and was already able to easily fool a state-of-the-art GNN. We then re-trained GIN with GNNGUARD defense on the same perturbed graph and, remarkably, the paper <ref type="bibr" target="#b48">[49]</ref> was correctly classified to IT with high confidence (f u (G ) = 0.489) even after the attack. This example illustrates how easily an adversary can fool a powerful graph ML algorithm on citation networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results: Ablation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduce GNNGUARD, an algorithm for defending any graph neural network (GNN) against poisoning attacks, including direct targeted, influence targeted, and non-targeted attacks. GNN-GUARD mitigates adverse effects by modifying neural message passing of the underlying GNN. This is achieved through the estimation of neighbor relevance and the use of graph memory, which are two critical components that are vital for a successful defense. In doing so, GNNGUARD can prune likely fake edges and assign less weight to suspicious edges, a principle grounded in network theory of homophily. Experiments on four datasets and across five GNNs show that GNNGUARD outperforms state-of-the-art defense algorithms by a large margin. Lastly, it would be interesting to extend GNNGUARD to fight adversaries that exploit structural equivalence (a principle orthogonal to homophily). While such adversarial attackers do not exist yet, this is a fruitful future direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A. Small, adversarial perturbations of the graph structure and node features lead to GNN to misclassifying target u. B. The GNN, when integrated with GNNGUARD, correctly classifies the target.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A. Illustration of neural message passing in u's local network neighborhood in the k-th layer of GNN f . B. The message flow in f , which is the GNN f endowed by GNNGUARD defense. We first calculate defense coefficients ω k uv based on node representations h k u and h k v .The defense coefficients are then used to control the message stream such as blocking the message from v but strengthening messages from v and v . Thick blue arrow indicates the higher weights during message aggregation. To stabilize the evolution of graph structure, current defense coefficients (e.g., ω k uv ) keep a partial memory of the previous layer (e.g., ω k−1 uv ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>) Direct targeted attacks. Direct targeted attacks. The attacker aims to destroy prediction for target node u by manipulating the incident edges of u<ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15]</ref>. Here, T = A = {u}. (2) Influence targeted attacks. The attacker aims to destroy prediction for target node u by perturbing the edges of u's neighbors. Here, T = {u} and A = N * u . (3) Non-targeted attacks.</figDesc><table><row><cell>Direct targeted attacks. Direct targeted attacks. Direct targeted attacks. Direct targeted attacks. Direct targeted attacks. Direct targeted attacks. Direct targeted attacks. Direct targeted attacks. Direct targeted attacks. Direct targeted attacks. Direct targeted attacks. Direct targeted attacks. Direct targeted attacks. Direct targeted attacks. Direct targeted attacks.</cell></row><row><cell>Influence targeted attacks. Influence targeted attacks. Influence targeted attacks. Influence targeted attacks. Influence targeted attacks. Influence targeted attacks. Influence targeted attacks. Influence targeted attacks. Influence targeted attacks. Influence targeted attacks. Influence targeted attacks. Influence targeted attacks. Influence targeted attacks. Influence targeted attacks. Influence targeted attacks. Influence targeted attacks.</cell></row><row><cell>Non-targeted attacks. Non-targeted attacks. Non-targeted attacks. Non-targeted attacks. Non-targeted attacks. Non-targeted attacks. Non-targeted attacks. Non-targeted attacks. Non-targeted attacks. Non-targeted attacks. Non-targeted attacks. Non-targeted attacks. Non-targeted attacks. Non-targeted attacks. Non-targeted attacks.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Defense performance (multi-class classification accuracy) against direct targeted attacks. whose malfunction collectively leads to diseases. The task is to predict for every protein node what diseases the protein might cause. Details are in Appendix D.</figDesc><table><row><cell>Model</cell><cell>Dataset</cell><cell>No Attack</cell><cell>Attack</cell><cell>GNN-Jaccard</cell><cell>RobustGCN</cell><cell>GNN-SVD</cell><cell>GNNGUARD</cell></row><row><cell></cell><cell>Cora</cell><cell>0.826</cell><cell>0.250</cell><cell>0.525</cell><cell>0.215</cell><cell>0.475</cell><cell>0.705</cell></row><row><cell>GCN</cell><cell>Citeseer ogbn-arxiv</cell><cell>0.721 0.667</cell><cell>0.175 0.235</cell><cell>0.435 0.305</cell><cell>0.230 0.245</cell><cell>0.615 0.370</cell><cell>0.720 0.425</cell></row><row><cell></cell><cell>DP</cell><cell>0.682</cell><cell>0.215</cell><cell>0.340</cell><cell>0.315</cell><cell>0.395</cell><cell>0.430</cell></row><row><cell></cell><cell>Cora</cell><cell>0.827</cell><cell>0.245</cell><cell>0.295</cell><cell>0.215</cell><cell>0.365</cell><cell>0.625</cell></row><row><cell>GAT</cell><cell>Citeseer ogbn-arxiv</cell><cell>0.718 0.669</cell><cell>0.265 0.210</cell><cell>0.575 0.355</cell><cell>0.230 0.245</cell><cell>0.575 0.445</cell><cell>0.765 0.520</cell></row><row><cell></cell><cell>DP</cell><cell>0.714</cell><cell>0.205</cell><cell>0.320</cell><cell>0.315</cell><cell>0.335</cell><cell>0.445</cell></row><row><cell></cell><cell>Cora</cell><cell>0.831</cell><cell>0.270</cell><cell>0.375</cell><cell>0.215</cell><cell>0.375</cell><cell>0.645</cell></row><row><cell>GIN</cell><cell>Citeseer ogbn-arxiv</cell><cell>0.725 0.661</cell><cell>0.285 0.315</cell><cell>0.570 0.425</cell><cell>0.230 0.245</cell><cell>0.570 0.475</cell><cell>0.755 0.640</cell></row><row><cell></cell><cell>DP</cell><cell>0.719</cell><cell>0.245</cell><cell>0.410</cell><cell>0.315</cell><cell>0.405</cell><cell>0.460</cell></row><row><cell></cell><cell>Cora</cell><cell>0.834</cell><cell>0.305</cell><cell>0.445</cell><cell>0.215</cell><cell>0.425</cell><cell>0.690</cell></row><row><cell>JK-Net</cell><cell>Citeseer ogbn-arxiv</cell><cell>0.724 0.678</cell><cell>0.275 0.335</cell><cell>0.615 0.375</cell><cell>0.230 0.245</cell><cell>0.610 0.325</cell><cell>0.775 0.635</cell></row><row><cell></cell><cell>DP</cell><cell>0.726</cell><cell>0.220</cell><cell>0.335</cell><cell>0.315</cell><cell>0.360</cell><cell>0.450</cell></row><row><cell></cell><cell>Cora</cell><cell>0.821</cell><cell>0.225</cell><cell>0.535</cell><cell>0.235</cell><cell>0.460</cell><cell>0.695</cell></row><row><cell>Graph SAINT</cell><cell>Citeseer ogbn-arxiv DP</cell><cell>0.716 0.683 0.739</cell><cell>0.195 0.245 0.205</cell><cell>0.470 0.365 0.315</cell><cell>0.350 0.245 0.295</cell><cell>0.395 0.315 0.330</cell><cell>0.770 0.375 0.485</cell></row><row><cell cols="4">of interacting proteins Setup. (1) Generating adversarial attacks. Generating adversarial attacks. Generating adversarial attacks. Generating adversarial attacks. Generating adversarial attacks. Generating adversarial attacks. Generating adversarial attacks. Generating adversarial attacks. Generating adversarial attacks. Generating adversarial attacks. Generating adversarial attacks. Generating adversarial attacks. Generating adversarial attacks. Generating adversarial attacks. Generating adversarial attacks. Generating adversarial attacks.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>) Baseline defense algorithms.</figDesc><table><row><cell>Baseline defense algorithms. Baseline defense algorithms. Baseline defense algorithms. Baseline defense algorithms. Baseline defense algorithms. Baseline defense algorithms. Baseline defense algorithms. Baseline defense algorithms. Baseline defense algorithms. Baseline defense algorithms. Baseline defense algorithms. Baseline defense algorithms. Baseline defense algorithms. Baseline defense algorithms. Baseline defense algorithms.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 ,</head><label>2</label><figDesc>GNNGUARD achieves the best classification accuracy comparing to other baseline defense algorithms. Taking a closer look at the results, we we can find that Nettack-In is relatively less threaten than Nettack-Di indicating part of the perturbed information was scattered during neural message passing. (3) Results for non-targeted attacks.</figDesc><table><row><cell>)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Study and Inspection of Defense Mechanism</figDesc><table><row><cell>(1) Ablation study. Ablation study. Ablation study. Ablation study. Ablation study. Ablation study. Ablation study. Ablation study. Ablation study. Ablation study. Ablation study. Ablation study. Ablation study. Ablation study. Ablation study. Ablation study.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We will share the GNNGUARD implementation, baseline defense algorithms, and all datasets upon publication.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices to "GNNGUARD: Defending Graph Neural Networks against Adversarial Attacks" Appendix A Defense Performance Against Influence Targeted Attacks</head><p>Results are shown in Table <ref type="table">4</ref>. We find that the proposed GNNGUARD achieves the best defensive performance against influence targeted attack across five GNN models and four datasets. In particular, GNNGUARD outperforms state-of-the-art defense models by 8.77% on average. Furthermore, compared to the case where the GNN is attacked without any defense, GNNGUARD brings a significant accuracy improvement of 22.6% on average. Remarkably, results show that even most recently published GNNs (e.g., GraphSAINT <ref type="bibr" target="#b20">[21]</ref>) are sensitive to adversarial perturbations of the graph structure (cf. "Attack" vs. "No Attack" columns in Table <ref type="table">4</ref>), yet GNNGUARD can successfully defend GNNs against influence targeted attacks and can restore their performance to levels comparable to learning on clean, non-attacked graphs.  <ref type="table">5</ref>. We also show defense performance of GNNGUARD relative to state-of-the-art GNN defense techniques. First, we find that non-targeted attacks can have a considerable negative impact on the performance of the GNNs. The accuracy of even the strongest GNN is reduced by 18.7% on average. In addition, results show that our GNNGUARD outperforms baselines in most experiments and improves upon baselines considerably. Experiments indicate the proposed GNN defender can successfully mitigate negative impacts brought forward by non-targeted attacks on graphs. Next, we want to investigate whether the GNN defender can harm the performance of the underlying GNN if the defender is used on clean, non-attacked graphs. Note that this is a practically important question, as in practice, users might not know a priori whether malicious agents have altered their graph datasets. Because of that, it is essential that a successful GNN defender does not decrease the predictive performance of the GNN in cases when GNNGUARD is turned on, but there is no attack.</p><p>From results in the main paper and Appendix A-B, we already know that GNNGUARD can defend GNNs when they are attacked. Here, we show that GNNGUARD does not hinder GNNs even when they are not attacked.</p><p>Results are shown in Table <ref type="table">6</ref>. We observe that GNNs, trained on clean datasets, yield approximately the same performance irrespective of whether a GNN integrates GNNGUARD defense or not. These results suggest that the use of GNNGUARD does not reduce GNN's expressive power or its representation capacity when there are no adversarial attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D Further Details on Datasets</head><p>Upon publication, we will share GNNGUARD implementation as well as all datasets and baseline algorithms with the community through a public project website, which will be accompanied by a Github repository with relevant data loaders. We provide further dataset statistics in Table <ref type="table">7</ref>.</p><p>The new, Disease Pathway (DP) <ref type="bibr" target="#b47">[48]</ref> dataset describes a system of interacting human proteins whose malfunction collectively leads to a variety of diseases. Nodes in the network represent human proteins and edges indicate protein-protein interactions. The task is to predict for every protein node what diseases (i.e., labels/classes) that protein might cause. The dataset has 73-dimensional continuous node features representing graphlet-orbit counts (i.e., the number of occurrences of higher-order network motifs), which we normalize via z-scores. This is a multi label node classification dataset. We select 10 most-common labels (diseases), reformulate the task as 10 independent balanced binary classification problems and report average performance across multiple independent runs. We randomly split the dataset into training (10%), validation (10%), and test set (80%) following the experimental setup in <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E Further Details on Hyperparameter Setting</head><p>To select hyperparameters and GNN model architectures, we closely follow original authors' guidelines and relevant papers on GNNs (GCN <ref type="bibr" target="#b2">[3]</ref>, GAT <ref type="bibr" target="#b18">[19]</ref>, GIN <ref type="bibr" target="#b6">[7]</ref>, JK-Net <ref type="bibr" target="#b19">[20]</ref>, and Graph-SAINT <ref type="bibr" target="#b20">[21]</ref>), baseline defense algorithms (GNN-Jaccard <ref type="bibr" target="#b14">[15]</ref>, RobustGCN <ref type="bibr" target="#b16">[17]</ref>, and GNN-SVD <ref type="bibr" target="#b15">[16]</ref>), and models for generating adversarial attacks (Nettack-Di <ref type="bibr" target="#b7">[8]</ref>, Nettack-In <ref type="bibr" target="#b7">[8]</ref>, and Mettack <ref type="bibr" target="#b25">[26]</ref>).</p><p>We use PyTorch DeepRobust package (https://github.com/DSE-MSU/DeepRobust) to implement adversarial attack models and baseline defense algorithms, and PyTorch Geometric package (https://github.com/rusty1s/pytorch_geometric) to implement and train GNN models. In all experiments, we set the number of epochs to 200 and use early stopping (we stop training if validation accuracy does not increase for 10 consecutive epochs). We repeat every experiment 5 times and report average performance across independent runs. We set P 0 = 0.5, K = 2, D 2 = 16, and dropout rate as 0.5, optimize cross-entropy loss using Adam optimizer and learning rate of 0.01. For other parameters, we follow the setup in <ref type="bibr" target="#b7">[8]</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivasan</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soheil</forename><surname>Moosavinasab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yungui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graph embedding on biomedical networks: methods, applications and evaluations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1241" to="1251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph embedding and unsupervised learning predict genomic sub-compartments from hic chromatin interaction data</title>
		<author>
			<persName><forename type="first">Haitham</forename><surname>Ashoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Rosikiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep learning on graphs: A survey</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>TKDE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Graph structure learning for robust graph neural networks</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianfeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>KDD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks? In ICLR</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adversarial attacks on neural networks for graph data</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Akbarnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2847" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bringing robustness against adversarial attacks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André Cplf De</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><surname>Carvalho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="499" to="500" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning models for electrocardiograms are susceptible to adversarial attack</title>
		<author>
			<persName><forename type="first">Xintian</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Foschini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Chinitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lior</forename><surname>Jankelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Medicine</title>
		<imprint>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adversarial explanations for understanding image classification decisions and improved neural network robustness</title>
		<author>
			<persName><forename type="first">Walt</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Teuscher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="508" to="516" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adversarial attacks and defenses in deep learning. Engineering</title>
		<author>
			<persName><forename type="first">Tianhang</forename><surname>Kui Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ai can now defend itself against malicious messages hidden in speech</title>
		<author>
			<persName><surname>Hutson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Birds of a feather: Homophily in social networks</title>
		<author>
			<persName><forename type="first">Lynn</forename><surname>Miller Mcpherson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Smith-Lovin</surname></persName>
		</author>
		<author>
			<persName><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Sociology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="415" to="444" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adversarial examples for graph data: Deep insights into attack and defense</title>
		<author>
			<persName><forename type="first">Huijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuriy</forename><surname>Tyshetskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Docherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liming</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4816" to="4823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">All you need is low (rank) defending against adversarial attacks on graphs</title>
		<author>
			<persName><forename type="first">Negin</forename><surname>Entezari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saba</forename><forename type="middle">A</forename><surname>Al-Sayouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirali</forename><surname>Darvishzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><forename type="middle">E</forename><surname>Papalexakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust graph convolutional networks against adversarial attacks</title>
		<author>
			<persName><forename type="first">Dingyuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1399" to="1407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transferring robustness for graph neural network against poisoning attacks</title>
		<author>
			<persName><forename type="first">Xianfeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaxiu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasenjit</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Graphsaint: Graph sampling based inductive learning method. ICLR</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Policy teaching via environment poisoning: Training-time adversarial attacks against reinforcement learning. ICML</title>
		<author>
			<persName><forename type="first">Amin</forename><surname>Rakhsha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Radanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rati</forename><surname>Devidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adish</forename><surname>Singla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adversarial attacks on medical machine learning</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Samuel G Finlayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joichi</forename><surname>Bowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">L</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Zittrain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><forename type="middle">S</forename><surname>Beam</surname></persName>
		</author>
		<author>
			<persName><surname>Kohane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">363</biblScope>
			<biblScope unit="issue">6433</biblScope>
			<biblScope unit="page" from="1287" to="1289" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Boosting adversarial attacks with momentum</title>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhou</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A simple unified framework for detecting out-of-distribution samples and adversarial attacks</title>
		<author>
			<persName><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adversarial attacks on graph neural networks via meta learning</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00653</idno>
		<title level="m">Adversarial attacks and defenses on graphs: A review and empirical study</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adversarial attack on graph structured data</title>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A unified framework for data poisoning attack to graph-based semi-supervised learning</title>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adversarial attacks on node embeddings via graph poisoning</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="695" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Modeling polypharmacy side effects with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="457" to="466" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Gnnexplainer: Generating explanations for graph neural networks</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning</title>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Gainza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Freyr</forename><surname>Sverrisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><surname>Correia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adversarial examples for evaluating reading comprehension systems</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Graph universal adversarial attacks: A few bad actors ruin graph learning models</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04784</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Shen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengzhang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingchao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03679</idno>
		<title level="m">Adversarial defense framework for graph neural network</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Topology attack and defense for graph neural networks: An optimization perspective</title>
		<author>
			<persName><forename type="first">Kaidi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongge</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin</forename><surname>Yu Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsui</forename><surname>Wei Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3961" to="3967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention models in graphs: A survey</title>
		<author>
			<persName><forename type="first">John</forename><surname>Boaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungchul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nesreen</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunyee</forename><surname>Koh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TKDD</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Octavian-Eugen</forename><surname>Ganea</surname></persName>
		</author>
		<title level="m">Non-Euclidean Neural Representation Learning of Words, Entities and Hierarchies</title>
				<imprint>
			<publisher>ETH Zurich</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Signed graph convolutional networks</title>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="929" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Certifiable robustness and robust training for graph convolutional networks</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Certifiable robustness to graph perturbations</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8317" to="8328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Automating the construction of internet portals with machine learning</title>
		<author>
			<persName><forename type="first">Andrew Kachites</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristie</forename><surname>Seymore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="163" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Large-scale analysis of disease pathways in the human interactome</title>
		<author>
			<persName><forename type="first">Monica</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific Symposium on Biocomputing</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="111" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Treep: A tree based p2p network architecture</title>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Hudzia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M-Tahar</forename><surname>Kechadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Ottewill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCC</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
