<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Convergence and Stability of Graph Convolutional Networks on Large Random Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-02">2 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Keriven</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alberto</forename><surname>Bietti</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Samuel</forename><surname>Vaiter</surname></persName>
						</author>
						<title level="a" type="main">Convergence and Stability of Graph Convolutional Networks on Large Random Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-02">2 Jun 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2006.01868v1[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study properties of Graph Convolutional Networks (GCNs) by analyzing their behavior on standard models of random graphs, where nodes are represented by random latent variables and edges are drawn according to a similarity kernel. This allows us to overcome the difficulties of dealing with discrete notions such as isomorphisms on very large graphs, by considering instead more natural geometric aspects. We first study the convergence of GCNs to their continuous counterpart as the number of nodes grows. Our results are fully non-asymptotic and are valid for relatively sparse graphs with an average degree that grows logarithmically with the number of nodes. We then analyze the stability of GCNs to small deformations of the random graph model. In contrast to previous studies of stability in discrete settings, our continuous setup allows us to provide more intuitive deformationbased metrics for understanding stability, which have proven useful for explaining the success of convolutional representations on Euclidean domains.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph Convolutional Networks (GCNs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25]</ref>) are deep architectures defined on graphs inspired by classical Convolutional Neural Networks (CNNs <ref type="bibr" target="#b26">[27]</ref>). In the past few years, they have been successfully applied to, for instance, node clustering <ref type="bibr" target="#b9">[10]</ref>, semi-supervised learning <ref type="bibr" target="#b24">[25]</ref>, or graph regression <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b18">19]</ref>, and remain one of the most popular variant of Graph Neural Networks (GNN). We refer the reader to the review papers <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b44">45]</ref> for more details.</p><p>Many recent results have improved the theoretical understanding of GNNs. While some architectures have been shown to be universal <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b22">23]</ref> but not implementable in practice, several studies have characterized GNNs according to their power to distinguish (or not) graph isomorphisms <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b32">33]</ref> or compute combinatorial graph parameters <ref type="bibr" target="#b11">[12]</ref>. However, such notions usually become moot for large graphs, which are almost never isomorphic to each other, but for which GCNs have proved to be successful in identifying large-scale structures nonetheless, e.g., for segmentation or spectral clustering <ref type="bibr" target="#b9">[10]</ref>. Under this light, a relevant notion is that of stability: since GCNs are trained then tested on different (large) graphs, how much does a change in the graph structure affect its predictions? In the context of signals defined on Euclidean domains, including images or audio, convolutional representations such as scattering transforms or certain CNN architectures have been shown to be stable to spatial deformations <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b37">38]</ref>. However the notion of deformations is not well-defined on discrete graphs, and most stability studies for GCNs use purely discrete metrics that are less intuitive for capturing natural changes in structure <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b46">47]</ref>.</p><p>In statistics and machine learning, there is a long history of modelling large graphs with random models, see for instance <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35]</ref> and references therein for reviews. Latent space models represent each node as a vector of latent variables and independently connect the nodes according to a similarity kernel * Equal contribution.</p><p>† CNRS &amp; GIPSA-lab. 11 rue des Mathématiques, 38400 St-Martin-d'Herès, France. firstname.name@gipsa-lab.grenoble-inp.fr ‡ INRIA. 2 rue Simone Iff, 75012, Paris, France. firstname.name@inria.fr § CNRS &amp; IMB, Université de Bourgogne. 9 avenue Alain Savary, 21000 Dijon, France. firstname.name@u-bourgogne.fr applied to their latent representations. This large family of random graph models includes for instance Stochastic Block Models (SBM) <ref type="bibr" target="#b20">[21]</ref>, graphons <ref type="bibr" target="#b29">[30]</ref>, random geometric graphs <ref type="bibr" target="#b35">[36]</ref>, or ε-graphs <ref type="bibr" target="#b8">[9]</ref>, among many others <ref type="bibr" target="#b34">[35]</ref>. A key parameter in such models is the so-called sparsity factor α n that controls the number of edges in O(n 2 α n ) with respect to the number of nodes n. The dense case α n ∼ 1 is the easiest to analyze, but often not realistic for real-world graphs. On the contrary, many questions are still open in the sparse case α n ∼ 1/n <ref type="bibr" target="#b0">[1]</ref>. A middle ground, which will be the setting for our analysis, is the so-called relatively sparse case α n ∼ log n/n, for which several non-trivial results are known <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b23">24]</ref>, while being more realistic than the dense case.</p><p>Outline and contributions. In this paper, we analyze the convergence and stability properties of GCNs on large random graphs. We define a "continuous" counterpart to discrete GCNs acting on graph models in Section 2, study notions of invariance and equivariance to isomorphism of random graph models, and give convergence results when the number of nodes grows in Section 3. In particular, our results are fully non-asymptotic, valid for relatively sparse random graphs, and unlike many studies <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b38">39]</ref> we do not assume that the similarity kernel is smooth or bounded away from zero. In Section 4, we analyze the stability of GCNs to small deformation of the underlying random graph model. Similar to CNNs <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b3">4]</ref>, studying GCNs in the continuous world allows us to define intuitive notions of model deformations and characterize their stability. Interestingly, for GCNs equivariant to permutation, we relate existing discrete notions of distance between graph signals to a Wasserstein-type metric between the corresponding continuous representations, which to our knowledge did not appear in the literature before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related work on large-scale random graphs.</head><p>There is an long history of studying the convergence of graph-related objects on large random graphs. A large body of works examine the convergence of the eigenstructures of the graph adjacency matrix or Laplacian in the context of spectral clustering <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b40">41]</ref> or learning with operators <ref type="bibr" target="#b38">[39]</ref>. The theory of graphons <ref type="bibr" target="#b29">[30]</ref> defines (dense) graph limits for more general metrics, which is also shown to lead to spectral convergence <ref type="bibr" target="#b14">[15]</ref>. Closer to our work, notions of Graph Signal Processing (GSP) such as the graph Fourier Transform have been extended to graphons <ref type="bibr" target="#b39">[40]</ref> or sampling of general Laplacian operators <ref type="bibr" target="#b28">[29]</ref>. Partial results on the capacity of GCNs to distinguish dense graphons are derived in <ref type="bibr" target="#b30">[31]</ref>, however their analysis based on random walks differs greatly from ours. In general, many of these studies are asymptotic <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b39">40]</ref>, valid only in the dense case <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31]</ref>, or assume kernels that are smooth or bounded away from zero <ref type="bibr" target="#b38">[39]</ref>, and thus exclude several important cases such as SBMs, ε-graphs, and non-dense graphs altogether. By specifying models of (relatively sparse) random graphs, we derive non-asymptotic, fully explicit bounds with relaxed hypotheses.</p><p>Related work on stability. The study of stability to deformations has been pioneered by Mallat <ref type="bibr" target="#b31">[32]</ref> in the context of the scattering transform for signals on Euclidean domains such as images or audio signals <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b1">2]</ref>, and was later extended to more generic CNN architectures <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b37">38]</ref>. A more recent line of work has studied stability properties of GCNs or scattering representations on discrete graphs, by considering certain well-chosen discrete perturbations and metrics <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b46">47]</ref>, which may however have limited interpretability without an underlying model. In contrast, our continuous setup allows us to define more intuitive geometric perturbations based on deformations of random graph models and to obtain deformation stability bounds that are similar to those on Euclidean domains <ref type="bibr" target="#b31">[32]</ref>. We note that <ref type="bibr" target="#b28">[29]</ref> also considers GCN representations with continuous graph models, but the authors focus on the different notion of "transferability" of graph filters on different discretizations of the same underlying continuous graph structure, while we consider explicit deformations of this underlying structure and obtain non-asymptotic bounds for the resulting random graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Notations. The norm • is the Euclidean norm for vector and spectral norm for matrices, and • F is the Frobenius norm. We denote by B(X ) the space of bounded real-valued functions on X equipped with the norm f ∞ = sup x |f (x)|. Given a probability distribution P on X , we denote by L 2 (P ) the Hilbert space of P -square-integrable functions endowed with its canonical inner product. For multivariate functions f = [f 1 , . . . , f d ] and any norm • , we define f = (</p><formula xml:id="formula_0">d i=1 f i 2 ) 1 2 . For two probability distributions P, Q on R d , we define the Wasserstein-2 distance W 2 2 (P, Q) = inf{E X − Y 2 | X ∼ P, Y ∼ Q},</formula><p>where the infimum is over all joint distributions of (X, Y ). We denote by f ♯ P the push-forward of P by f , that is, the distribution of f (X) when X ∼ P .</p><p>A graph G = (A, Z) with n nodes is represented by a symmetric adjacency matrix A ∈ {0, 1} n×n such that a ij = 1 if there is an edge between nodes i and j, and a matrix of signals over the nodes Z ∈ R n×dz , where z i ∈ R dz is the vector signal at node i. We define the normalized Laplacian matrix as</p><formula xml:id="formula_1">L = L(A) = D(A) − 1 2 AD(A) − 1 2</formula><p>, where D(A) = diag(A1 n ) is the degree matrix, and (D(A) − 1 2 ) i = 0 if D(A) i = 0. The normalized Laplacian is often defined by Id − L in the literature, however this does not change the considered networks since the filters include a term of order 0.</p><p>Graph Convolutional Networks (GCN). GCNs are defined by alternating filters on graph signals and non-linearities. We use analytic filters (said of order-k if β ℓ = 0 for ℓ k + 1):</p><formula xml:id="formula_2">h : R → R, h(λ) = k 0 β k λ k .<label>(1)</label></formula><p>We write h(L) = k β k L k , i.e., we apply h to the eigenvalues of L when it is diagonalizable.</p><p>A GCN with M layers is defined as follows. The signal at the input layer is</p><formula xml:id="formula_3">Z (0) = Z with dimension d 0 = d z and columns z (0) j ∈ R n . Then, at layer ℓ, the signal Z (ℓ) ∈ R n×d ℓ with columns z (ℓ) j ∈ R n is propagated as follows: ∀j = 1, . . . d ℓ+1 , z (ℓ+1) j = ρ d ℓ i=1 h (ℓ) ij (L)z (ℓ) i + b (ℓ) j 1 n ∈ R n ,<label>(2)</label></formula><p>where h</p><formula xml:id="formula_4">(ℓ) ij (λ) = k β (ℓ) ijk λ k are learnable analytic filters, b<label>(ℓ)</label></formula><p>j ∈ R are learnable biases, and the activation function ρ : R → R is applied pointwise. Once the signal at the final layer Z (M) is obtained, the output of the entire GCN is either a signal over the nodes denoted by Φ A (Z) ∈ R n×dout or a single vector denoted by ΦA (Z) ∈ R dout obtained with an additional pooling over the nodes:</p><formula xml:id="formula_5">Φ A (Z) def. = Z (M) θ + 1 n b ⊤ , ΦA (Z) def. = 1 n n i=1 Φ A (Z) i ,<label>(3)</label></formula><p>where θ ∈ R dM ×dout , b ∈ R dout are the final layer weights and bias, and Φ A (Z) i ∈ R dout is the output signal at node i. This general model of GCN encompasses several models of the literature, including all spectral-based GCNs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref>, or GCNs with order-1 filters <ref type="bibr" target="#b24">[25]</ref> which are assimilable to message-passing networks <ref type="bibr" target="#b18">[19]</ref>, see <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b5">6]</ref> for reviews. For message-passing networks, note that almost all our results would also be valid by replacing the sum over neighbors by another aggregation function such as max. We assume (true for ReLU, modulus, or sigmoid) that the function ρ satisfies:</p><formula xml:id="formula_6">|ρ(x)| |x| , |ρ(x) − ρ(y)| |x − y| .<label>(4)</label></formula><p>Two graphs G = (A, Z), G ′ = (A ′ , Z ′ ) are said to be isomorphic if one can be obtained from the other by relabelling the nodes. In other words, there exists a permutation matrix σ ∈ Σ n , where Σ n is the set of all permutation matrices, such that</p><formula xml:id="formula_7">A = σ • A ′ def. = σA ′ σ ⊤ and Z = σ • Z ′ def.</formula><p>= σZ ′ , where "σ • " is a common notation for permuted matrices or signal over nodes. In graph theory, functions that are invariant or equivariant to permutations are of primary importance (respectively, permuting the input graph does not change the output, or permutes the output). These properties are hard-coded in the structure of GCNs, as shown by the following proposition (proof in Appendix B).</p><formula xml:id="formula_8">Proposition 1. We have Φ σ•A (σ • Z) = σ • Φ A (Z) and Φσ•A (σ • Z) = ΦA (Z).</formula><p>Random graphs. Let (X , d) be a compact metric space. In this paper, we consider latent space graph models where each node i is represented by an unobserved latent variable x i ∈ X , and nodes are connected randomly according to some similarity kernel. While the traditional graphon model <ref type="bibr" target="#b29">[30]</ref> considers (without lost of generality) X = [0, 1], it is often more intuitive to allow general spaces to represent meaningful variables <ref type="bibr" target="#b12">[13]</ref>. We consider that the observed signal z i ∈ R dz is a function of the latent variable x i , without noise for now. In details, a random graph model Γ = (P, W, f ) is represented by a probability distribution P over X , a symmetric kernel W : X × X → [0, 1] and a bounded function f : X → R dz . A random graph G with n nodes is then generated as follows:</p><formula xml:id="formula_9">∀j &lt; i n : x i iid ∼ P, z i = f (x i ), a ij ∼ Ber(α n W (x i , x j )). (<label>5</label></formula><formula xml:id="formula_10">)</formula><p>where Ber is the Bernoulli distribution. We define d W,P def.</p><p>= W (•, x)dP (x) the degree function of Γ. As outlined in the introduction, the sparsity factor α n ∈ [0, 1] plays a key role. The so-called relatively sparse case α n ∼ log n n will be the setting for our analysis. Let us immediately make some assumptions that will hold throughout the paper. We denote by N (X , ε, d) the ε-covering numbers (that is, the minimal number of balls of radius ε required to cover X ) of X , and assume that they can be written under the form N (X , ε, d) ε −dx for some constant d x &gt; 0 (called the Minkowski dimension of X ), and diam(X )</p><p>1. Both conditions can be obtained by a rescaling of the metric d. Let c min , c max &gt; 0 be constants. A function f : X → R is said to be (c Lip. , n X )piecewise Lipschitz if there is a partition X 1 , . . . , X nX of X such that, for all x, x ′ in the same X i , we have |f (x) − f (x ′ )| c Lip. d(x, x ′ ). All considered random graph models Γ = (P, W, f ) satisfy that for all x ∈ X ,</p><formula xml:id="formula_11">W (•, x) ∞ c max , d W,P (x) c min , W (•, x) is (c Lip. , n X )-piecewise Lipschitz.<label>(6)</label></formula><p>Unlike other studies <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b38">39]</ref>, we do not assume that W itself is bounded away from 0 or smooth, and thus include important cases such as SBMs (piecewise constant W ) and ε-graphs (threshold kernels).</p><p>Continuous GCNs. Since d W,P &gt; 0, we define the normalized Laplacian operator L W,P by</p><formula xml:id="formula_12">L W,P f def. = W (•, x) d W,P (•)d W,P (x) f (x)dP (x).<label>(7)</label></formula><p>Analytic filters on operators are h(L)</p><formula xml:id="formula_13">= k β k L k , with L k = L • . . . • L.</formula><p>We do not assume that the filters are of finite order (even if they usually are in practice <ref type="bibr" target="#b13">[14]</ref>), however we will always assume that</p><formula xml:id="formula_14">k k |β k | (2c max /c min ) k converges.</formula><p>Similar to the discrete case, we define continuous GCNs (c-GCN) that act on random graph models, by replacing the input signal Z with f , the Laplacian L by L W,P , and propagating functions instead of node signals i.e., we take f (0) = f the input function with coordinates f</p><formula xml:id="formula_15">(0) 1 , . . . , f<label>(0)</label></formula><p>dz and:</p><formula xml:id="formula_16">∀j = 1, . . . , d ℓ+1 , f (ℓ+1) j = ρ • d ℓ i=1 h (ℓ) ij (L W,P )f (ℓ) i + b (ℓ) j 1(•) ,<label>(8)</label></formula><p>where 1(•) represent the constant function 1 on X . Once the final layer function f (M) : X → R dM is obtained, the output of the c-GCN is defined as in the discrete case, either as a multivariate function Φ W,P (f ) : X → R dout or a single vector ΦW,P (f ) ∈ R dout obtained by pooling:</p><formula xml:id="formula_17">Φ W,P (f ) def. = θ ⊤ f (M) + b1(•), ΦW,P (f ) = Φ W,P (f )(x)dP (x). (<label>9</label></formula><formula xml:id="formula_18">)</formula><p>Hence the same parameters {(β</p><formula xml:id="formula_19">(ℓ) ijk , b<label>(ℓ)</label></formula><p>j ) ijkℓ , θ, b} define both a discrete and a continuous GCN, the latter being (generally) not implementable in practice but useful to analyze their discrete counterpart.</p><p>For a random graph model Γ = (P, W, f ), and any map φ : X → X , we define φ = {φ : φ ♯ P = P } (e.g., a translation when P is the Lebesgue measure, or a rotation when P is the surface measure on the sphere), then φ • Γ = (P, φ • W, φ • f ) defines the same probability distribution as Γ = (P, W, f ) over discrete graphs. Therefore, we say that Γ and φ • Γ are isomorphic, which is a generalization of isomorphic graphons <ref type="bibr" target="#b29">[30]</ref> when P is the uniform measure on [0, 1]. Here the set Σ P to which φ belongs depends on P contained in Γ, while in the discrete case we always considered all permutations. As with discrete graphs, functions on random graph models can be invariant or equivariant, and c-GCNs satisfy these properties (proof in Appendix B).</p><formula xml:id="formula_20">• W def. = W (φ(•), φ(•)) and φ • f = f • φ. If φ</formula><formula xml:id="formula_21">Proposition 2. For all φ ∈ Σ P , Φ φ•W,P (φ • f ) = φ • Φ W,P (f ) and Φφ•W,P (φ • f ) = ΦW,P (f ).</formula><p>In the rest of the paper, most notation-heavy multiplicative constants are given in the appendix. They depend on c min , c max , c Lip. and the operator norms of the matrices</p><formula xml:id="formula_22">B (ℓ) k = (β (ℓ) ijk ) ij .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Convergence of Graph Convolutional Networks</head><p>In this section, we show that a GCN applied to a random graph G ∼ Γ will be close to the corresponding c-GCN applied to Γ. In the invariant case, ΦA (Z) and ΦW,P (f ) are both vectors in R dout . In the equivariant case, we will show that the output signal Φ A (Z) i ∈ R dout at each node is close to the function Φ W,P (f ) evaluated at x i . To measure this, we consider the (square root of the) Mean Square Error at the node level: for a signal</p><formula xml:id="formula_23">Z ∈ R n×dout , a function f : X → R dout and latent variables X, we define MSE X (Z, f ) def. = (n −1 n i=1 Z i − f (x i ) 2 ) 1/2 .</formula><p>In the following theorem we use the shorthand</p><formula xml:id="formula_24">D X (ρ) def. = cLip. cmin √ d x + cmax+cLip. cmin log nX ρ .</formula><p>Theorem 1 (Convergence to continuous GCN). Let Φ be a GCN and G be a graph with n nodes generated from a model Γ, denote by X its latent variables. There are two universal constants c 1 , c 2 such that the following holds. Take any ρ &gt; 0, assume n is large enough such that n c 1 D X (ρ) 2 + 1 ρ , and the sparsity level is such that α n c 2 c max c −2 min • n −1 log n. Then, with probability at least 1 − ρ,</p><formula xml:id="formula_25">MSE X (Φ A (Z), Φ W,P (f )) R n def. = C 1 D X ρ ℓ d ℓ n − 1 2 + C 2 (nα n ) − 1 2 , ΦA (Z) − ΦW,P (f ) R n + C 3 log(1/ρ)n − 1 2 .</formula><p>Discussion. The constants C i are of the form C ′ i f ∞ + C ′′ i and detailed in the appendix. When the filters are normalized and there is no bias, they are proportional to M f ∞ . In particular, they do not depend on the dimension d x .</p><p>The proof use standard algebraic manipulations, along with two concentration inequalities. The first one exploits Dudley's inequality <ref type="bibr" target="#b41">[42]</ref> to show that, for a fixed function f and in the absence of random edges, L W,P f is well approximated by its discrete counterpart. Note here that we do not seek a uniform proof with respect to a functional space, since the c-GCN is fixed. This allows us to obtain non-asymptotic rate while relaxing usual smoothness hypotheses <ref type="bibr" target="#b38">[39]</ref>. This first concentration bound leads to the standard rate in O(1/ √ n).</p><p>The second bound uses a fairly involved recent concentration inequality for normalized Laplacians of relatively sparse graphs with random edges derived in <ref type="bibr" target="#b23">[24]</ref>, which gives the term in O(1/ √ α n n). Although this second term has a strictly worse convergence rate except in the dense case α n ∼ 1, its multiplicative constant is strictly better, in particular it does not depend on the Minkowski dimension d x . The condition n 1/ρ, which suggests a polynomial concentration instead of the more traditional exponential one, comes from this part of the proof.</p><p>It is known in the literature that using the normalized Laplacian is often more appropriate than the adjacency matrix. If we where to use the latter, a normalization by (α n n) −1 would be necessary <ref type="bibr" target="#b27">[28]</ref>. However, α n is rarely known, and can change from one case to the other. The normalized Laplacian is adaptative to α n and does not require any normalization.</p><p>Example of applications. Invariant GCNs are typically used for regression or classification at the graph level. Theorem 1 shows that the output of a discrete GCN directly approaches that of the corresponding c-GCN. Equivariant GCNs are typically used for regression at the node level. Consider an ideal function f * : X → R dout that is well approximated by an equivariant c-GCN Φ W,P (f ) in terms of L 2 (P )-norm. Then, the error between the output of the discrete GCN Φ A (Z) and the sampling of f * satisfies with high probability MSE 4 ) using a triangle inequality, Theorem 1 and Hoeffding's inequality.</p><formula xml:id="formula_26">X (Φ A (Z), f * ) Φ W,P (f ) − f * L 2 (P ) + R n + O(n −<label>1</label></formula><p>Noisy or absent signal. Until now, we have considered that the function f was observed without noise. Noise can be handled by considering the Lipschitz properties of the GCN. For instance, in the invariant case, by Lemma 7 in Appendix F, we have ΦA (Z 1 ) − ΦA (Z 2 )</p><formula xml:id="formula_27">1 √ n Z 1 − Z 2 F . Hence, if the input signal is the noisy z i = f (x i ) + ν i ,</formula><p>where ν is centered iid noise, a GCN deviates from the corresponding c-GCN by an additional n −1/2 (ν i ) i F , which converges to the standard deviation of the noise. Interestingly, the noise can be filtered out: for instance, if one inputs Z = LZ into the GCN, then by a concentration inequality it is not difficult to see that the smoothed noise term converges to 0, and the GCN converges to the c-GCN with smoothed input function f = L W,P f .</p><p>In some cases such as spectral clustering <ref type="bibr" target="#b9">[10]</ref>, one does not have an input signal over the nodes, but has only access to the structure of the graph. In this case, several heuristics have been used in the literature, but a definitive answer is yet to emerge. For instance, a classical strategy is to use the (normalized) degrees of the graph Z = A1 n /n as input signal <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>. In this case, using our proofs (Lemma 4 in the appendix) and the spectral concentration in <ref type="bibr" target="#b27">[28]</ref>, it is not difficult to show that a discrete GCN will converge to its countinuous version with the degree function f = d W,P as input. We will see in Prop. 3 in the next section that this leads to desirable stability properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Stability of GCNs to model deformations</head><p>Stability to deformations is an essential feature for the generalization properties of deep architectures. Mallat <ref type="bibr" target="#b31">[32]</ref> studied the stability to small deformations of the wavelet-based scattering transform, which was extended to more generic learned convolutional network, e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b37">38]</ref>, and tries to establish bounds of the following form for a signal representation Φ(•):</p><formula xml:id="formula_28">Φ(f τ ) − Φ(f ) N (τ ) f ,<label>(10)</label></formula><p>where</p><formula xml:id="formula_29">f τ (x) = f (x−τ (x)</formula><p>) is the deformed signal and N (τ ) quantifies the size of the deformation, typically through norms of its jacobian ∇τ , such as ∇τ ∞ = sup x ∇τ (x) . As we have seen in the introduction, it is not clear how to extend the notion of deformation on discrete graphs <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref>. We show here that it can be done in the continuous world. We first derive generic stability bounds for discrete random graphs involving a Wasserstein-type metric between the corresponding c-GCNs, then derive bounds of the form (10) for c-GCNs by studying various notions of deformations of random graph models. We note that "spatial" deformations x → x − τ (x) are or course not the only possible choice for these models, and leave other types of perturbations to future work.</p><p>From discrete to continuous stability. We first exploit the previous convergence result to deport the stability analysis from discrete to continuous GCNs. Let G 1 and G 2 be two random graphs with n nodes drawn from models Γ 1 and Γ 2 , and a GCN Φ. In the invariant case, we can directly apply Theorem 1 and the triangle inequality to obtain that ΦA1 (Z 1 ) − ΦA2 (Z 2 ) ΦW1,P1 (f 1 ) − ΦW2,P2 (f 2 ) + 2R n , and study the robustness of ΦW,P (f ) to deformations of the model. The equivariant case is more complex. A major difficulty, compared for instance to <ref type="bibr" target="#b28">[29]</ref>, is that, since we consider two different samplings X 1 and X 2 , there are no implicit ordering over the nodes of G 1 and G 2 , and one cannot directly compare the output signals of the equivariant GCN e.g., in Frobenius norm. To compare two graph representations, a standard approach in the study of stability (and graph theory in general) has been to define a metric that minimizes over permutations σ of the nodes (e.g., <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref>), thus we define MSE Σ (Z, Z ′ ) def.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>= min</head><formula xml:id="formula_30">σ∈Σn (n −1 i Z i − Z ′ σ(i)</formula><p>2 ) 1/2 . Theorem 2 relates this to a Wasserstein metric between the c-GCNs (proof in Appendix D).</p><p>Theorem 2 (Finite-sample stability in the equivariant case). Adopt the notations of Theorem 1. For r = 1, 2, define the distribution Q r = Φ Wr ,Pr (f r ) ♯ P r . With probability 1 − ρ, we have</p><formula xml:id="formula_31">MSE Σ (Φ A1 (Z 1 ), Φ A2 (Z 2 )) W 2 (Q 1 , Q 2 ) + R n + C 1 n − 1 dz + C 2 + 4 log 1 ρ n − 1 4 (<label>11</label></formula><formula xml:id="formula_32">)</formula><p>where C 1 and C 2 are defined in the appendix. When f 1 and f 2 are piecewise Lipschitz, the last terms can be replaced by</p><formula xml:id="formula_33">C ′ 1 (n −1/dx + (C ′ 2 + 4 log(1/ρ))n −1/4 ) for some C ′ 1 , C ′ 2 .</formula><p>In other words, we express stability in terms of a Wasserstein metric between the push-forwards of the measures P r by their respective c-GCNs. By definition, the l.h.s. of ( <ref type="formula" target="#formula_31">11</ref>) is invariant to permutation of the graphs G r . Moreover, for φ ∈ Σ P by Prop. 2 we have Φ φ•W,P (φ • f ) ♯ P = Φ W,P (f ) ♯ (φ ♯ P ) = Φ W,P (f ) ♯ P , and therefore the r.h.s. of ( <ref type="formula" target="#formula_31">11</ref>) is also invariant to continuous permutation φ.</p><p>We recover the rate R n from Theorem 1, as well as a term in 1/n 1/4 and a term that depends on the dimension. In the relatively sparse case, the term in 1/ √ α n n in R n still has the slowest convergence rate. The proof uses classic manipulations in Optimal Transport <ref type="bibr" target="#b36">[37]</ref>, as well as concentration results of empirical distributions in Wasserstein norm <ref type="bibr" target="#b43">[44]</ref>. In particular, it is known that the latter yields slow convergence rates with the dimension n −1/d . While the Q r 's live in R dz , when the c-GCNs are Lipschitz we can replace d z by the Minkowski dimension of X , which may be advantageous when X is a low-dimensional manifold.</p><p>In the rest of this section, we analyze the stability of c-GCNs to deformation of random graph models, directly through the Wasserstein bound above (or simple Euclidean norm in the invariant case). Finitesample bounds are then obtained with Theorem 1 and 2.</p><p>Stability of continuous GCNs: assumptions. Assume from now on that X ⊂ R d equipped with the Euclidean norm. Given a diffeomorphism τ : X → X , we consider spatial deformations of random graph models of the form (Id − τ ), and aim at obtaining bounds of the form (10) for c-GCNs. Given a reference random graph model Γ = (P, W, f ), we may consider perturbations to P , W , or f , and thus define W τ def.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>= (Id</head><formula xml:id="formula_34">− τ ) • W , P τ def. = (Id − τ ) ♯ P and f τ def.</formula><p>= (Id − τ ) • f . Of course, after deformation, we still consider that the assumptions on our random graph models (6) are verified. As can be expected, translation-invariant kernels W such as Gaussian kernels or ε-graph kernels are particularly adapted to such deformations, therefore we will often make the following assumption:</p><formula xml:id="formula_35">W (x, x ′ ) = w(x − x ′ ), C ∇w def. = sup x∈X ∇w x−x ′ 2 • x ′ − x dP (x ′ ) &lt; ∞. (A1)</formula><p>We also define C W def.</p><p>= sup x |W (x, x ′ )|dP (x ′ ) c max . While C W and C ∇w are easily bounded when W, ∇w are bounded, they are typically much smaller than such naive bounds when W and ∇w are well localized in space with fast decays, e.g., for the Gaussian kernel or a smooth ε-graph kernel with compact support (for instance, in the latter case, C W is proportional to εc max instead of c max ).</p><p>In the case where P is replaced by P τ , some of our results will be valid beyond translation-invariant kernels. We will instead assume that P τ has a density with respect to P , close to one: for all x, q τ (x)</p><formula xml:id="formula_36">def. = dPτ dP (x), q τ (x), q τ (x) −1 C Pτ &lt; ∞, N P (τ ) def. = q τ − 1 ∞ . (A2)</formula><p>When (Id − τ ) ∈ Σ P , then we have N P (τ ) = 0, so that N P (τ ) measures how much it deviates from such neutral elements and quantifies the size of deformations. In particular, when P is proportional to the Lebesgue measure and ∇τ ∞ &lt; 1, we have q τ (x) = det(I − ∇τ (x)) −1 ; then, for small enough ∇τ ∞ , we obtain N P (τ ) d ∇τ ∞ , recovering the more standard quantity of Mallat <ref type="bibr" target="#b31">[32]</ref>. In this case, we also have the bound</p><formula xml:id="formula_37">C Pτ 2 d if we assume ∇τ ∞ 1/2.</formula><p>In the rest of the section, we will assume for simplicity that the considered GCNs Φ have zero bias at each layer. Unless otherwise written, f refers to L 2 (P )-norm. All the proofs are in Appendix E.</p><p>Deformation of translation-invariant kernels. We first consider applying deformations to the kernel W , which amounts to a perturbation to the edge structure of the graph. For GCNs, this affects the Laplacian operator used for the filters, and could be seen as a perturbation of the "graph shift operator" in the framework of Gama et al. <ref type="bibr" target="#b17">[18]</ref>. The following result shows that in this case the stability of GCN representations, both invariant and equivariant, is controlled by ∇τ ∞ .</p><p>Theorem 3 (Kernel deformation). Consider a GCN representation Φ with no bias and a random graph Γ = (P, W, f ). Define Q = Φ W,P (f ) ♯ P and Q τ = Φ Wτ ,P (f ) ♯ P . Assume (A1) and ∇τ ∞ 1/2.</p><p>We have</p><formula xml:id="formula_38">ΦWτ ,P (f ) − ΦW,P (f ) W 2 (Q, Q τ ) C(C W + C ∇w ) f ∇τ ∞ , (<label>12</label></formula><formula xml:id="formula_39">)</formula><p>where C is given in the appendix.</p><p>Deformation of the distribution. Let us now consider perturbations of P to P τ , which corresponds to a change in the node distribution. In practice, this may correspond to several, fairly different, "practical" situations. We describe two different frameworks below.</p><p>In shape analysis, P may be supported on a manifold, and P τ can then represent a deformation of this manifold, e.g., a character that rigidly moves a body part. In this case in particular, we can expect τ ∞ to be large, but ∇τ ∞ to be small (i.e., large translation but small deformation). Moreover, if the kernel is translation-invariant, there will be little change in the structure of the generated graph. If additionally the input signal of the c-GCN is approximately deformed along with P , then one can expect the outputs to be stable, which we prove in the following theorem.</p><p>Theorem 4 (Distribution deformation, translation-invariant case). Consider a GCN representation Φ with no bias and a random graph Γ = (P, W, f ), along with a function f ′ . Define Q = Φ W,P (f ) ♯ P and Q τ = Φ W,Pτ (f ′ ) ♯ P τ . Assume (A1) and ∇τ ∞ 1/2. We have</p><formula xml:id="formula_40">ΦW,P (f ) − ΦW,Pτ (f ′ ) W 2 (Q, Q τ ) C(C W + C ∇w ) f ∇τ ∞ + C ′ f ′ τ − f ,<label>(13)</label></formula><p>where C, C ′ are given in the appendix.</p><formula xml:id="formula_41">When f = f ′ are both constant, or when f ′ = (Id − τ ) −1 • f , that is, f ′ is</formula><p>the mapping of the original signal f on the deformed structure, then we have f ′ τ − f = 0. As mentioned before, in the absence of input signal, a standard choice is to take the degree functions as inputs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>. The next result shows that this choice also leads to the desired stability.</p><formula xml:id="formula_42">Proposition 3. Assume (A1) and ∇τ ∞ 1/2. If f = d W,P and f ′ = d W,Pτ , then we have f ′ τ − f C ∇w ∇τ ∞ .</formula><p>Let us now take a look at the case where W is not translation-invariant. We will then assume that P τ has a density with respect to P , and in particular that it has the same support: one may for instance imagine a social network with a slightly changing distribution of user preferences, SBMs with changing community sizes, geometric random graphs <ref type="bibr" target="#b35">[36]</ref>, or graphons <ref type="bibr" target="#b29">[30]</ref>. The analysis here being slightly more complex, we focus on invariant c-GCNs.</p><p>Theorem 5 (Distribution deformation, non-translation-invariant case). Consider a GCN representation Φ with no bias and a random graph Γ = (P, W, f ). Assume (A2). We have</p><formula xml:id="formula_43">ΦW,P (f ) − ΦW,Pτ (f ) CC 3 Pτ C W + C ′ f N P (τ ),<label>(14)</label></formula><p>where C, C ′ are given in the appendix.</p><p>As mentioned above, in the case where P is the Lebesgue measure, e.g., for graphons <ref type="bibr" target="#b29">[30]</ref>, then we recover the quantity N P (τ ) d ∇τ ∞ .</p><p>Deformations of the signal. Finally, we consider deformations of the signal on the graph and show a bound similar to the ones in the Euclidean case <ref type="bibr" target="#b9">(10)</ref>. As can be seen in the proofs, this case is in fact a combination of the previous results ( <ref type="formula" target="#formula_38">12</ref>) and ( <ref type="formula" target="#formula_43">14</ref>); hence we must assume both (A1) and (A2) and obtain a dependence on both ∇τ ∞ and N P (τ ). Once again we focus on invariant c-GCNs with pooling, similar to classical scattering transform <ref type="bibr" target="#b31">[32]</ref>.</p><p>Proposition 4 (Signal deformation). Consider a GCN representation Φ with no bias and a random graph Γ = (P, W, f ). Assume (A1), (A2), and ∇τ ∞ 1/2. We have</p><formula xml:id="formula_44">ΦW,P (f ) − ΦW,P (f τ ) (CC 1/2 Pτ (C W + C ∇w ) ∇τ ∞ + CC 3 Pτ C W + C ′ N P (τ )) f ,<label>(15)</label></formula><p>where C, C ′ are given in the appendix.</p><p>When P is proportional to the Lebesgue measure, since N P (τ ) is controlled by ∇τ ∞ , the GCN is invariant to translations and stable to deformations, similar to Euclidean domains <ref type="bibr" target="#b31">[32]</ref>. We note that studies of stability are often balanced by discussions on how the representation preserves signal (e.g., <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17]</ref>). In our context, the empirical success of GCNs suggests that these representations maintain good discrimination and approximation properties, though a theoretical analysis of such properties for GCNs is missing and provides an important direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and outlooks</head><p>GCNs have proved to be efficient in identifying large-scale structures in graphs and generalizing across graphs of different sizes, which can only partially be explained with discrete graph notions like isomorphisms <ref type="bibr" target="#b45">[46]</ref> or stability with permutation-minimizing metrics <ref type="bibr" target="#b16">[17]</ref>. In contrast, we have shown that combining them with random models of large graphs allows us to define intuitive notions of deformations and stability in the continuous world like the Euclidean case <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b37">38]</ref>, with direct applications in community-based social networks or shape analysis on point clouds. For this we derived non-asymptotic convergence bounds, valid on relatively sparse random graphs with non-smooth kernels, and new tools like a Wasserstein-type stability bounds on equivariant c-GCNs.</p><p>We believe our work to be a first step toward a better understanding of GNNs on large graphs, with many potential outlooks. First, it would be useful to improve the dependence of our bounds on regularity properties of the filters, as done in <ref type="bibr" target="#b17">[18]</ref> for the discrete setting, while preserving the mild dependence on the number of filters. In the same vein, finer results may be obtained in particular cases: e.g., the case where X is a sub-manifold can be studied under the light of Riemannian geometry, stability bounds on SBMs may be expressed with a direct dependence on their parameters, or more explicit stability bounds may be obtained when the (c-)GCN is a structured architecture like the scattering transform on graphs <ref type="bibr" target="#b15">[16]</ref>. Convergence results can also be obtained for many other models of random graphs like k-Nearest Neighbor graphs <ref type="bibr" target="#b8">[9]</ref>. Finally, while we focus on stability in this paper, as mentioned above the approximation power of GCNs (beyond untractable universality <ref type="bibr" target="#b22">[23]</ref>) can also be expressed through that of their continuous counterpart, and characterizing which functions are computable by a c-GCN (e.g., with growing width or number of layers) is of foremost importance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material</head><p>In Appendix A, we introduce additional notations and objects that will be used in the proofs. In Appendix B, we study the equivariance of GCNs and prove Props 1 and 2. In Appendix C, we prove Theorem 1 on the convergence of GCNs. In Appendix D, we prove the Wasserstein bound in Theorem 2. In Appendix E, we derive the stability bounds of Section 4. Finally, in Appendix F, we give technical concentration bounds and in Appendix G we provide some third-party results for completeness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Notations</head><p>Given a GCN, we define some bounds on its parameters that will be used in the multiplicative constants of the theorem. Recall that the filters are written h</p><formula xml:id="formula_45">(ℓ) ij (λ) = ∞ k=0 β (ℓ) ijk λ k . We define B (ℓ) k = β (ℓ) ijk ji</formula><p>∈ R d ℓ+1 ×d ℓ the matrix containing the order-k coefficients, and by</p><formula xml:id="formula_46">B (ℓ) k,|•| = β (ℓ) ijk ji</formula><p>the same matrix with absolute value on all coefficients. Then, we define the following bounds:</p><formula xml:id="formula_47">H (ℓ) 2 = k B (ℓ) k H (ℓ) ∂,2 = k B (ℓ) k k H (ℓ) ∞ = k B (ℓ) k,|•| 2cmax cmin k H (ℓ) ∂,∞ = k B (ℓ) k k 2cmax cmin k−1</formula><p>which all converge by our assumptions on the β k . We may also denote H 2 by H L 2 (P ) for convenience but this quantity does not depend on P . Note that, only for H ∞ , we use the spectral norm of the matrix B k,|•| with non-negative coefficients, which is suboptimal compared to using B k . This is due to a part of our analysis where we do not operate in a Hilbert space but only the Banach space B(X ), see Lemma 6.</p><p>We also define b (ℓ) = j (b (ℓ) j ) 2 . Given X, we define the empirical degree function</p><formula xml:id="formula_48">d X = d W,X def. = 1 n i W (•, x i ) (<label>16</label></formula><formula xml:id="formula_49">)</formula><p>Which will be denoted by d X when the kernel is clear. Although d W,P is bounded away from 0 by the assumption (6), this is not necessarily the case for d W,X . This is however true with high probability, as shown by the following Lemma.</p><p>Lemma 1. Let Γ be a model of random graphs. There is a universal constant C such that, if</p><formula xml:id="formula_50">n CD X (ρ) 2<label>(17)</label></formula><p>where</p><formula xml:id="formula_51">D X (ρ) = cLip. cmin √ d x + cmax+cLip. cmin log nX ρ , then with probability 1 − ρ, d W,X c min /2 &gt; 0.</formula><p>Proof. Apply Lemma 4 with f = 1 to obtain the result.</p><p>For W and X such that d W,X &gt; 0, we define the following empirical Laplacian operator:</p><formula xml:id="formula_52">L X f = L W,X f def. = 1 n i W (•, x i ) d X (•)d X (x i ) f (x i ) (<label>18</label></formula><formula xml:id="formula_53">)</formula><p>which we will also denote by L X when W is clear. Assuming that d X c min /2, L W,X is a bounded operator and L W,X ∞ 2cmax cmin . Given X = {x 1 , . . . , x n } and any dimension d, we denote by S X the normalized sampling operator acting on functions f : X → R d defined by S X f def.</p><formula xml:id="formula_54">= 1 √ n [f (x 1 ), . . . , f (x n )] ∈ R n×d . The normalizing factor 1 √ n is natural: we have S X f F</formula><p>f ∞ and by the Law of Large Numbers S X f F → f L 2 (P ) a.s. Finally, given X and W , we define W (X)</p><p>def.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>= (W (x</head><formula xml:id="formula_55">i , x j )) ij ∈ R n×n , and remark that L(W (X)) • S X = S X • L W,X .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Invariance and equivariance</head><p>Proof of Prop. 1. The proof is immediate, by observing that L(σ</p><formula xml:id="formula_56">• A) = σ • L(A), therefore h(L(σ • A))(σ • Z) = σ •(h(L(A))Z)</formula><p>, and permutations commute with the pointwise activation function. For the invariant case, we just observe the final pooling on the equivariant case.</p><p>Prop. 2 will actually be a particular case of the following proposition when φ ∈ Σ P and thus φ ♯ P = P .</p><p>Proposition 5. For all φ : X → X such that d W,φ ♯ P &gt; 0, we have</p><formula xml:id="formula_57">Φ φ•W,P (φ • f ) = φ • Φ W,φ ♯ P (f ) and Φφ•W,P (φ • f ) = ΦW,φ ♯ P (f ).</formula><p>Proof. Let us first observe that the degree function is such that</p><formula xml:id="formula_58">d W,φ ♯ P (φ(x)) = W (φ(x), x ′ )dφ ♯ P (x ′ ) = (φ • W )(x, x ′ )dP (x ′ ) = d φ•W,P (x)</formula><p>Then, we have</p><formula xml:id="formula_59">φ • (L W,φ ♯ P f )(x) = W (φ(x), x ′ ) d W,φ ♯ P (φ(x))d W,φ ♯ P (x ′ ) f (x ′ )dφ ♯ P (x ′ ) = (φ • W )(x, x ′ ) d φ•W,P (x)d φ•W,P (x ′ ) (φ • f )(x ′ )dP (x ′ ) = L φ•W,P (φ • f )</formula><p>Then, by recursion, we have</p><formula xml:id="formula_60">L k φ•W,P (φ • f ) = L k−1 φ•W,P (φ • (L W,φ ♯ P f )) = . . . = φ • L k W,φ ♯ P f</formula><p>, and the same is true for filters h(L). We conclude by observing that permutation commutes with pointwise non-linearity:</p><formula xml:id="formula_61">ρ • (φ • f ) = φ • (ρ • f ) = ρ • f • φ.</formula><p>The invariant case follows with a final integration against P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Convergence of GCNs: proof of Theorem 1</head><p>We are going to prove Theorem 1 with the following constants:</p><formula xml:id="formula_62">C 1 ∝ c max + c Lip. c min M−1 ℓ=0 C (ℓ) H (ℓ) ∂,∞ M−1 s=ℓ+1 H (s) 2 , C 2 ∝ c max c 2 min M−1 ℓ=0 C (ℓ) H (ℓ) ∂,2 M−1 s=ℓ+1 H (s) 2 , C 3 ∝ C (M) with C (ℓ) def. = θ f ∞ ℓ−1 s=0 H (s) ∞ + ℓ−1 s=0 b (s) ℓ−1 p=s+1 H (p) ∞ (<label>19</label></formula><formula xml:id="formula_63">)</formula><p>The proof will mainly rely on an application of Dudley's inequality <ref type="bibr" target="#b41">[42,</ref><ref type="bibr">Thm 8.1.6</ref>] (Lemma 5 in Appendix F) and a recent spectral concentration inequality for normalized Laplacian in relatively sparse graphs (Theorem 6 in Appendix G).</p><p>Proof. We begin the proof by the equivariant case, the invariant case will simply use an additional concentration inequality. Denoting by Z (ℓ) (resp. f (ℓ) ) the signal at each layer of the GCN (resp. the function at each layer of the c-GCN), we have</p><formula xml:id="formula_64">MSE X (Φ A (Z), Φ W,P (f )) = Φ A (Z) √ n − S X Φ W,P (f ) F θ Z (M) √ n − S X f (M) F</formula><p>where we recall that S X is the normalized sampling operator (see App. A). We therefore seek to bound that last term.</p><p>Assume that the following holds with probability 1 − ρ: for all 0 ℓ M − 1</p><formula xml:id="formula_65">j i h (ℓ) ij (L)S X f (ℓ) i − S X h (ℓ) ij (L W,P )f (ℓ) i 2 ∆ (ℓ)<label>(20)</label></formula><p>Then, using (4) and Lemma 6, we can show by recursion that</p><formula xml:id="formula_66">Z (ℓ) √ n − S X f (ℓ) F ε ℓ implies Z (ℓ+1) √ n − S X f (ℓ+1) F =   j ρ d ℓ i=1 h (ℓ) ij (L) z (ℓ) i √ n + b (ℓ) j 1 n √ n − S X ρ d ℓ i=1 h (ℓ) ij (L W,P )f (ℓ) i + b (ℓ) j 2   1 2   j d ℓ i=1 h (ℓ) ij (L) z (ℓ) i √ n − S X h (ℓ) ij (L W,P )f (ℓ) i 2   1 2   j d ℓ i=1 h (ℓ) ij (L) z (ℓ) i √ n − S X f (ℓ) i 2   1 2 +   j d ℓ i=1 h (ℓ) ij (L)S X f (ℓ) i − S X h (ℓ) ij (L W,P )f (ℓ) i 2   1 2 ε ℓ+1 def. = H (ℓ) 2 ε ℓ + ∆ (ℓ)</formula><p>Since Z (0) √ n = S X f (0) we have ε 0 = 0 and an easy recursion shows that</p><formula xml:id="formula_67">Z (M) √ n − S X f (M) F M−1 ℓ=0 ∆ (ℓ) M−1 s=ℓ+1 H (s) 2<label>(21)</label></formula><p>We now need to prove that (20) holds with probability 1 − ρ for all ℓ with the appropriate ∆ (ℓ) . Recall that L(W (X)) • S X = S X • L W,X , and that by <ref type="bibr" target="#b16">(17)</ref>, with probability 1</p><formula xml:id="formula_68">− ρ/2 we have L W,X ∞ 2cmax cmin .</formula><p>Assuming this is satisfied, by Lemma 6 we have</p><formula xml:id="formula_69">j i h (ℓ) ij (L)S X f (ℓ) i − S X h (ℓ) ij (L W,P )f (ℓ) i 2 j i h (ℓ) ij (L) − h (ℓ) ij (L(W (X)) S X f (ℓ) i 2 + j i S X h (ℓ) ij (L W,X ) − h (ℓ) ij (L W,P ) f (ℓ) i 2 H (ℓ) ∂,2 L − L(W (X)) f (ℓ) ∞ + k B k i k−1 ℓ=0 2c max c min ℓ (L W,X − L W,P )L k−1−ℓ W,P f (ℓ) i ∞ 2<label>(22)</label></formula><p>The first term in ( <ref type="formula" target="#formula_69">22</ref>) is handled with a recent concentration inequality for normalized Laplacian in the relatively sparse graphs with random edges <ref type="bibr" target="#b23">[24]</ref>, recalled as Theorem 6 in Appendix G. We use the following version.</p><p>Corollary 1 (of Theorem 6). Assume ( <ref type="formula" target="#formula_50">17</ref>) is satisfied and n 1/ρ for simplicity. There is a universal constant C such that, if</p><formula xml:id="formula_70">α n Cc max c 2 min • log n n<label>(23)</label></formula><p>Then, with probability at least 1 − ρ, we have</p><formula xml:id="formula_71">L − L(W (X)) c max c 2 min • 1 √ α n n</formula><p>Proof. By <ref type="bibr" target="#b16">(17)</ref> with the appropriate constant, with probability 1 − ρ/2 we have d X c min /2, we can therefore apply Theorem 6 to bound L − L(W (X)) conditionally on X using c ∼ 1 + log(1/ρ) log(n) ∼ 1, then use a union bound to conclude.</p><p>We now bound the second term in <ref type="bibr" target="#b21">(22)</ref>. Define ρ k = Cρ (k+1) 2 ℓ d ℓ with C such that kℓ d ℓ ρ k = ρ/4 (even when the filters are not of finite order). Using an application of Dudley's inequality detailed in Lemma 5 in Appendix F and a union bound, we obtain with probability 1 − ρ/4 that: for all i, ℓ, k, we have</p><formula xml:id="formula_72">(L W,X − L W,P )L k W,P f (ℓ) i ∞ c max L k W,P f ℓ i ∞ D X (ρ k ) c min √ n c max c min k c max f ℓ i ∞ D X Cρ (k+1) 2 ℓ d ℓ c min √ n 2c max c min k (c max + c Lip. ) f ℓ i ∞ D X ρ ℓ d ℓ c min √ n</formula><p>Coming back to the second term of ( <ref type="formula" target="#formula_69">22</ref>), with probability 1 − ρ/4:</p><formula xml:id="formula_73">k B k i k−1 ℓ=0 2c max c min ℓ (L W,X − L W,P )L k−1−ℓ W,P f (ℓ) i ∞ 2 (c max + c Lip. )D X ρ ℓ d ℓ c min √ n k B k k 2c max c min k i f (ℓ) i 2 ∞ (c max + c Lip. )D X ρ ℓ d ℓ c min √ n H (ℓ) ∂,∞ f (ℓ)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∞</head><p>At the end of the day we obtain that with probability 1 − ρ, (20) is satisfied with</p><formula xml:id="formula_74">∆ (ℓ) ∝ f (ℓ) ∞   H (ℓ) ∂,2 c max c 2 min √ α n n + H (ℓ) ∂,∞ (c max + c Lip. )D X ρ ℓ d ℓ c min √ n  </formula><p>We then use Lemma 8 to bound f (ℓ) ∞ and conclude. Finally, in the invariant case we have</p><formula xml:id="formula_75">ΦA (Z) − ΦW,P (f ) MSE X (Φ A (Z), Φ W,P (f )) + θ 1 n i f (M) (x i ) − Ef (M) (X)</formula><p>Using a vector Hoeffding's inequality (Lemma 11) and a bound on f (M) ∞ by Lemma 8 we bound the second term and conclude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Wasserstein convergence: proof of Theorem 2</head><p>We are going to prove Theorem 2 with the following constants:</p><formula xml:id="formula_76">C 1 ∝ θ max r=1,2 f r ∞ ℓ−1 s=0 H (s) ∞ + ℓ−1 s=0 b (s) ℓ−1 p=s+1 H (p) ∞ C 2 = 27 dz/4<label>(24)</label></formula><formula xml:id="formula_77">C ′ 1 ∝ (n X n f ) 1 dx max r=1,2 D r C ′ 2 = 27 dx/4</formula><p>where D r are defined as <ref type="bibr" target="#b35">(36)</ref> with the function f r as input. The proof will mainly rely on results of the concentration rate of the empirical distribution of iid data to its true value in Wasserstein norm <ref type="bibr" target="#b43">[44]</ref> (Theorem 7 in Appendix G).</p><p>Proof. For r = 1, 2, define y r,i = Φ Wr ,Pr (f r )(x r,i ) which are drawn iid from Q r def.</p><p>= Φ Wr ,Pr (f r ) ♯ P r , and we denote by Qr = n −1 i δ yr,i the empirical distributions. By Theorem 1 and triangle inequality, we have</p><formula xml:id="formula_78">min σ n −1 i Φ A1 (Z 1 ) i − Φ A2 (Z 2 ) σ(i) 2 min σ 1 n i y 1,i − y 2,σ(i) 2 + 2R n</formula><p>The first term is known, among other appellations, as the so-called "Monge" formulation of optimal transport (OT) <ref type="bibr" target="#b36">[37,</ref><ref type="bibr">Chap. 2]</ref>. For uniform weights, as is the case here, it is known that Monge formulation of OT is equivalent to its Kantorovich relaxation, which in turns gives the traditional Wasserstein metric: by [37, Prop. 2.1], we have</p><formula xml:id="formula_79">1 n i y 1,i − y 2,σ(i) 2 = W 2 ( Q1 , Q2 ) W 2 (Q 1 , Q 2 ) + r=1,2 W 2 ( Qr , Q r )</formula><p>We must therefore bound the distance between an empirical distribution Q and its true value Q = g ♯ P for some function g = Φ W,P (f ).</p><p>The distribution Q is supported on gX ⊂ R dz , which is bounded by g ∞ which can be bounded by Lemma 8. Hence its covering numbers are as N (gX , ε, • ) ( g ∞ /ε) dz . We can then conclude by Theorem 7.</p><p>When f is (c f , n f )-piecewise Lipschitz however, by Lemma 9 g is (C, n X n f )-Lipschitz where C is defined as <ref type="bibr" target="#b35">(36)</ref>, and in this case it is easy to see that the covering numbers of gX also satisfy N (gX , ε, • ) n X n f (C/ε) dx . Applying again Theorem 7, we conclude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Stability</head><p>In this section, norms • always refer to L 2 (P ) norms (for functions and operators), and we drop the subscript for simplicity. We denote the pooling operator by U P f = f dP . We denote U = U P and L = L W,P for short. For τ : X → X , we denote W τ = (Id − τ ) • W , P τ = (Id − τ ) ♯ P , and f τ = (Id − τ ) • f . Then we define the shorthands L Wτ = L Wτ ,P , L Pτ = L W,Pτ , the composition operator T τ f = (Id − τ ) • f and U τ = U Pτ . Finally, we define A and A τ the integral operators of kernels W and W τ w.r.t. the measure P , the corresponding diagonal degree operators by D and D Wτ , so that we have L = D −1/2 AD −1/2 and similarly for L Wτ . Similarly, we define D Pτ the diagonal degree operator of W but with respect to P τ .</p><p>We observe that for two functions f, f ′ and any P , we have</p><formula xml:id="formula_80">f dP − f ′ dP W 2 (f ♯ P, f ′ ♯ P ) f − f ′ L 2 (P )<label>(25)</label></formula><p>The second inequality is immediate by considering the definition</p><formula xml:id="formula_81">W 2 2 (f ♯ P, f ′ ♯ P ) = inf E f (X) − f ′ (Y ) 2 | X ∼ P, Y ∼ P</formula><p>and taking X = Y as a coupling. We will therefore manipulate mainly L 2 (P ) norms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Proof of Theorem 3 (deformation change to W )</head><p>We are going to prove Theorem 3 with constant</p><formula xml:id="formula_82">C ∝ θ c −2 min M−1 ℓ=0 H (ℓ) ∂,2 M−1 s=0, s =ℓ H (s) 2<label>(26)</label></formula><p>In this setting, we have two random graph models whose only difference is in the choice of kernel, from W to W τ , while fixing P and f . This in turn leads to a change in the Laplacian, which is the main quantity that we will need to control.</p><p>Since we have assumed the bias to be zero, we have the following Lemma, which we apply with f = f ′ in this section.</p><p>Lemma 2. We have</p><formula xml:id="formula_83">Φ W,P (f ) − Φ Wτ ,P (f ′ ) C f L − L Wτ + C ′ f − f ′ with C = θ M−1 ℓ=0 H (ℓ) ∂,2 M−1 s=0, s =ℓ H (s) 2<label>(27)</label></formula><formula xml:id="formula_84">C ′ = θ M−1 ℓ=0 H (ℓ) 2<label>(28)</label></formula><p>Proof. Denoting f (ℓ) and (f (ℓ) ) ′ the functions at each layer, we have using Lemma 6 and (4):</p><formula xml:id="formula_85">f (ℓ) − (f (ℓ) ) ′ j d ℓ−1 i=1 h (ℓ−1) ij (L)f (ℓ−1) i − h (ℓ−1) ij (L Wτ )(f (ℓ−1) i ) ′ 2 j d ℓ−1 i=1 (h (ℓ−1) ij (L) − h (ℓ−1) ij (L Wτ ))f (ℓ−1) i 2 * + j d ℓ−1 i=1 h (ℓ−1) ij (L Wτ )(f (ℓ−1) i − (f (ℓ−1) i ) ′ ) 2 H (ℓ−1) ∂,2 L − L Wτ f (ℓ−1) + H (ℓ−1) 2 f (ℓ−1) − (f (ℓ−1) ) ′</formula><p>An easy recursion and Lemma 8 give the result.</p><p>The rest of the proof then consists in obtaining a bound on the quantity L Wτ − L in L 2 (P ). We have</p><formula xml:id="formula_86">L Wτ − L = D − 1 2 Wτ A τ D − 1 2 Wτ − D − 1 2 AD − 1 2 = D − 1 2 Wτ A τ (D − 1 2 Wτ − D − 1 2 ) + D − 1 2 Wτ (A τ − A)D − 1 2 + (D − 1 2 Wτ − D − 1 2 )AD − 1 2 . (<label>29</label></formula><formula xml:id="formula_87">)</formula><p>We now bound different operators in this decomposition separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bound on A</head><formula xml:id="formula_88">τ − A . Define k(x, x ′ ) = w(x − τ (x) − x ′ + τ (x ′ )) − w(x − x ′ ),<label>(30)</label></formula><p>so that A τ − A is an integral operator with kernel k. We have, by the fundamental theorem of calculus,</p><formula xml:id="formula_89">k(x, x ′ ) = 1 0 ∇w(x − x ′ + t(τ (x ′ ) − τ (x))), τ (x ′ ) − τ (x) dt.</formula><p>Now, note that we have</p><formula xml:id="formula_90">|τ (x ′ ) − τ (x)| ∇τ ∞ • x − x ′ |x − x ′ + t(τ (x ′ ) − τ (x))| 1 2 x − x ′ ,</formula><p>where the last inequality follows from the reverse triangle inequality and the assumption ∇τ ∞ 1/2. By Cauchy-Schwarz, and since ∇w(x) decreases with x , we have</p><formula xml:id="formula_91">|k(x, x ′ )|dP (x ′ ) ∇τ ∞ ∇w((x − x ′ )/2) • x ′ − x dP (x ′ ) C ∇w ∇τ ∞ .</formula><p>Similarly, we obtain |k(x, x ′ )|dP (x) C ∇w ∇τ ∞ . Then, Schur's test (Lemma 12) yields</p><formula xml:id="formula_92">A τ − A C ∇w ∇τ ∞ .<label>(31)</label></formula><p>Bound on D </p><formula xml:id="formula_93">− D −1/2 is diagonal with elements d τ (x) −1/2 − d(x) −1/2 , such that D −1/2 Wτ − D −1/2 d −1/2 τ − d −1/2 ∞</formula><p>. Note that we have</p><formula xml:id="formula_94">|d τ (x) − d(x)| = k(x, x ′ )dP (x ′ ) C ∇w ∇τ ∞ ,</formula><p>Then as in the proof of Lemma 5 we have</p><formula xml:id="formula_95">d −1/2 τ − d −1/2 ∞ c − 3 2 min d τ − d ∞ C ∇w c − 3 2 min ∇τ ∞ .</formula><p>Final bound. Note that by Schur's test, we have</p><formula xml:id="formula_96">A C w , A τ C w + C ∇w ∇τ ∞ . Further, we have D −1/2 , D −1/2 Wτ c −1/2</formula><p>min . Plugging back into (29), we have the desired bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Proof of Theorem 4 and 5 (deformation change to P )</head><p>In this case, we have two random graph models with distributions P and P τ , while W remains fixed. Depending on the case, the input f will change or not.</p><p>Translation-invariant case. We are going to prove Theorem 4 with C defined as <ref type="bibr" target="#b25">(26)</ref> and C ′ as <ref type="bibr" target="#b27">(28)</ref>. Using <ref type="bibr" target="#b24">(25)</ref> and Prop 5, we obtain that</p><formula xml:id="formula_97">W 2 (Φ W,Pτ (f ′ ) ♯ P τ , Φ W,P (f ) ♯ P ) T τ Φ W,Pτ (f ′ ) − Φ W,P (f ) = Φ Wτ ,P (f ′ τ ) − Φ W,P<label>(f )</label></formula><p>In the invariant case, we have similarly that ΦW,Pτ (f ′ ) − ΦW,P (f ) = ΦW,Pτ (f ′ ) − ΦW,P (f ) Φ Wτ ,P (f ′ τ ) − Φ W,P (f ) . Using Lemma 2 and the computation of the previous section, we obtain</p><formula xml:id="formula_98">Φ Wτ ,P (f ′ τ ) − Φ W,P (f ) C(C W + C ∇w ) f ∇τ ∞ + C ′ f ′ τ − f ,</formula><p>where C is defined by <ref type="bibr" target="#b25">(26)</ref> and C ′ is defined by <ref type="bibr" target="#b27">(28)</ref>.</p><p>Proof of Prop. 3. When using degree functions as input, using the proof of Prop 5 and the computations of the previous sections we have</p><formula xml:id="formula_99">T τ d W,Pτ − d W,P = d Wτ ,P − d W,P C ∇w ∇τ ∞</formula><p>Non Translation-invariant case. We are going to prove Theorem 5 with constants C defined as <ref type="bibr" target="#b25">(26)</ref> and C ′ defined as <ref type="bibr" target="#b27">(28)</ref>. By Assumption (A2) we easily have the following:</p><formula xml:id="formula_100">C −1/2 Pτ f L 2 (Pτ ) f C 1/2 Pτ f L 2 (Pτ )<label>(32)</label></formula><p>such that for any k we have L k Pτ C Pτ by observing that</p><formula xml:id="formula_101">L k Pτ f C 1/2 Pτ L k Pτ f L 2 (Pτ ) C 1/2 Pτ f L 2 (Pτ ) C Pτ f</formula><p>We can then prove the following Lemma.</p><p>Lemma 3 (Stability in terms of measure change). We have</p><formula xml:id="formula_102">ΦW,Pτ (f ) − ΦW,P (f ) C 2 Pτ C 1 f L Pτ − L + C 2 f U τ − U<label>(33)</label></formula><p>with C 1 is the same as <ref type="bibr" target="#b26">(27)</ref> and</p><formula xml:id="formula_103">C 2 = θ M−1 ℓ=0 H (ℓ) 2 .</formula><p>Proof. From a simple triangle inequality and the estimate</p><formula xml:id="formula_104">U τ C Pτ since |U τ f | = f dP τ = f q τ dP C Pτ f , we have ΦW,Pτ (f ) − ΦW,P (f ) = U τ Φ W,Pτ (f ) − U Φ W,P (f ) C Pτ Φ W,Pτ (f ) − Φ W,P (f ) + U τ − U Φ W,P<label>(f )</label></formula><p>We must now bound the first term. Denoting f (ℓ) and (f (ℓ) ) ′ the functions at each layer, we have using Lemma 6, (4) and the fact that L k Pτ C Pτ :</p><formula xml:id="formula_105">f (ℓ) − (f (ℓ) ) ′ j d ℓ−1 i=1 h (ℓ−1) ij (L)f (ℓ−1) i − h (ℓ−1) ij (L Pτ )(f (ℓ−1) i ) ′ 2 j d ℓ−1 i=1 (h (ℓ−1) ij (L) − h (ℓ−1) ij (L Pτ ))(f (ℓ−1) i ) ′ 2 + j d ℓ−1 i=1 h (ℓ−1) ij (L)(f (ℓ−1) i − (f (ℓ−1) i ) ′ ) 2 C Pτ H (ℓ−1) ∂,2 L − L Pτ (f (ℓ−1) ) ′ + H (ℓ−1) 2 f (ℓ−1) − (f (ℓ−1) ) ′</formula><p>Then, we use Lemma 8 and the fact that there is no bias to obtain</p><formula xml:id="formula_106">(f (ℓ−1) ) ′ C 1/2 Pτ (f (ℓ−1) ) ′ L 2 (Pτ ) C 1/2 Pτ f L 2 (Pτ ) ℓ−1 s=0 H (s) 2 C Pτ f ℓ−1 s=0 H (s)<label>2</label></formula><p>an easy recursion gives the result.</p><p>We first bound U τ − U easily, by</p><formula xml:id="formula_107">|U τ f − U f | = f (x)dP τ (x) − f (x)dP (x) = f (x) (q τ (x) − 1) dP (x) N P (τ ) f</formula><p>which is also true for multivariate functions.</p><p>We now bound L Pτ − L . If we denote J τ the diagonal change of variables operator with elements q τ (x), we may write</p><formula xml:id="formula_108">L Pτ − L = D −1/2 Pτ AD −1/2 Pτ J τ − D −1/2 AD −1/2 = (D −1/2 Pτ − D −1/2 )AD −1/2 Pτ J τ + D −1/2 A(D −1/2 Pτ − D −1/2 )J τ + D −1/2 AD −1/2 (J τ − Id)</formula><p>The following estimates are easily obtained using Schur's test or by a pointwise supremum:</p><formula xml:id="formula_109">A C w , J τ C Pτ , J τ − Id N P (τ ) and D −1/2 , D −1/2 Pτ c −1/2 min . It is left to bound and D −1/2 Pτ − D −1/2 . As before, D −1/2 Pτ − D −1/2 c −3/2 min d τ − d ∞ . and |d τ (x) − d(x)| = W (x, x ′ )dP τ (x ′ ) − W (x, x ′ )dP (x ′ ) = W (x, x ′ ) (q τ (x ′ ) − 1) dP (x ′ ) C w N P (τ ) E.3 Proof of Proposition 4 (signal deformations to f )</formula><p>This is just a triangle inequality, combined with the previous theorems and Prop 5:</p><formula xml:id="formula_110">ΦW,P (T τ f ) − ΦW,P (f ) ΦW,P (T τ f ) − ΦWτ ,P (T τ f ) + ΦWτ ,P (T τ f ) − ΦW,P (f ) C(C W + C ∇w ) ∇τ ∞ T τ f + ΦW,Pτ (f ) − ΦW,P (f ) C(C W + C ∇w )C 1/2 Pτ f ∇τ ∞ + (CC 3 Pτ C W + C ′ ) f N P (τ )</formula><p>where C is given by ( <ref type="formula" target="#formula_82">26</ref>) and C ′ is given by <ref type="bibr" target="#b27">(28)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Technical Lemma F.1 Concentration inequalities</head><p>Lemma 4 (Chaining on non-normalized kernels). Consider a kernel W and a probability distribution P satisfying (6), any function f ∈ B(X ), and x 1 , . . . , x n drawn iid from P . Then, with probability at least 1 − ρ,</p><formula xml:id="formula_111">1 n i W (•, x i )f (x i ) − W (•, x)f (x)dP (x) ∞ f ∞ c Lip. √ d x + (c max + c Lip. ) log nX ρ √ n</formula><p>Proof. Without lost of generality, we do the proof for f ∞ 1. For any x ∈ X , define</p><formula xml:id="formula_112">Y x = 1 n i W (x, x i )f (x i ) − W (x, x ′ )f (x ′ )dP (x ′ )</formula><p>Since W (•, x)f ∞ c max , for any fixed x 0 ∈ X , by Hoeffding's inequality we have: with probability at least 1 − ρ,</p><formula xml:id="formula_113">|Y x0 | c max log(1/ρ) √ n</formula><p>and for all x, y,</p><formula xml:id="formula_114">1 d X (y)d X (x) − 1 d P (y)d P (x) 1 d X (y) 1 d X (x) − 1 d P (x) + 1 d P (x) 1 d X (y) − 1 d P (y) ε d c 2 min Now, define W (x, y) def. = W (x,y) √ dP (x)dP (y)</formula><p>. For all j n X and x, x ′ ∈ X j and y ∈ X , we have W (</p><formula xml:id="formula_115">•, y) ∞ cmax cmin and W (x, y) − W (x ′ , y) |W (x, y)| d P (y) 1 d P (x) − 1 d P (x ′ ) + 1 d P (x ′ )d P (y) |W (x, y) − W (x ′ , y)| c Lip. c max c 2 min d(x, x ′ )</formula><p>Hence by applying Lemma 4 we obtain that with probability 1 − ρ/2,</p><formula xml:id="formula_116">1 n i W (•, x i )f (x i ) − W (•, x)f (x)dP (x) ∞ ε W def. = cmax cmin cLip. cmin √ d x log C X + 1 + cLip. cmin log nX ρ √ n</formula><p>We can now conclude, observing that </p><formula xml:id="formula_117">(L X − L P )f ∞ = 1 n i W (•, x i ) d X (•)d X (x i ) f (x i ) − W (•, x) d P (•)d P (x) f (x)dP (x) ∞ sup x 1 n i |W (x, x i )f (x i )| 1 d X (x)d X (x i ) − 1 d P (x)d P (x i ) + 1 n i W (•, x i )f (x i ) − W (•, x)f (x)dP (x) ∞ c max c 2 min ε d + ε W c max c 2 min ε d F.2</formula><formula xml:id="formula_118">S i h ij (L)x i 2 H k B k L k i x i 2 E and j S i (h ij (L) − h ij (L ′ ))x i 2 H k B k i k−1 ℓ=0 L ℓ (L − L ′ )(L ′ ) k−1−ℓ x i E<label>2</label></formula><p>When H is only a Banach space, the same results hold with</p><formula xml:id="formula_119">B k,|•| = (|β ijk |) ji instead of B k .</formula><p>Proof. Let {e ℓ } l 1 be an orthonormal basis for H. For all i, k, decompose SL k x i = ℓ b ikℓ e ℓ . We have</p><formula xml:id="formula_120">j S i h ij (L)x i 2 H j ik β ijk SL k x i 2 H k j iℓ β ijk b ikℓ e ℓ 2 H k ℓ j i β ijk b ikℓ 2 k B k 2 iℓ b 2 ikℓ = k B k i SL k x i 2 H k B k C k i x i 2 E</formula><p>The proof of the second claim is obtained in the same way by decomposing S(L k − (L ′ ) k )x i in H and using at the last step:</p><formula xml:id="formula_121">(L k − (L ′ ) k )x E = k−1 ℓ=0 L ℓ (L − L ′ )(L ′ ) k−1−ℓ x E k−1 ℓ=0 C ℓ (L − L ′ )(L ′ ) k−1−ℓ x E</formula><p>Finally, when H is not a Hilbert space, we directly use</p><formula xml:id="formula_122">j S i h ij (L)x i 2 H j ik |β ijk | C k x i E 2 k C k j i |β ijk | x i E 2 k B k,|•| C k i x i 2 E</formula><p>Lemma 7 (Lipschitz property of discrete GCNs). Let G 1 = (A, Z 1 ) and G 2 = (A, Z 2 ) be two graphs with the same structure, and a GCN Φ. Denote by Z (M) r the signal at the last layer when applying Φ to G r . We have</p><formula xml:id="formula_123">Z (M) 1 − Z (M) 2 F M−1 ℓ=0 H (ℓ) 2 Z 1 − Z 2 F</formula><p>Proof. For j d ℓ , using Lemma 6 and (4) we write  An easy recursion gives the result. </p><p>where f (ℓ) L 2 (P ) can be bounded by Lemma 8.</p><p>Proof. Define the partition X ′ 1 , . . . , X ′ n f on which f is Lipschitz, and take x, x ′ ∈ X i ∩ X ′ j for some i, j. Using the same strategy as in the proof of Lemma 6, we have  </p><formula xml:id="formula_125">f (M) (x) − f (M) (x ′</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Third-party results</head><p>Lemma 10 (Hoeffding's inequality). Let X 1 , . . . , X n ∈ R be independent random variables such that a X i b almost surely. Then we have</p><formula xml:id="formula_126">P 1 n i (X i − EX i ) ε 2 exp − 2ε 2 n (b − a) 2<label>(37)</label></formula><p>Lemma 11 (Generalized Hoeffding's inequality <ref type="bibr" target="#b38">[39]</ref>). Let H be a separable Hilbert space and ξ 1 , . . . , ξ n ∈ H be independent zero-mean random variables such that ξ i C almost surely. Then with probability at least 1 − ρ we have</p><formula xml:id="formula_127">1 n i ξ i C 2 log(2/ρ) √ n<label>(38)</label></formula><p>Theorem 6 (Spectral concentration of normalized Laplacian <ref type="bibr" target="#b23">[24,</ref><ref type="bibr">Theorem 4]</ref>). Let A be an adjacency matrix of a graph drawn with independent edges a ij ∼ Ber(α n p ij ), where p ij c max and for all i, </p><p>Theorem 7 (Wasserstein convergence <ref type="bibr" target="#b43">[44]</ref>). Let (X , d) be a compact metric space with diam(X ) B and N ε (X ) (B/ε) dx . Let P be a probability distribution on X , and x 1 , . . . , x n drawn iid from P . With probability 1 − ρ, W 2 ( P , P ) B n − 1 dx + 27 where P = n −1 i δ xi .</p><p>Proof. The result is obtained by combining Prop. 5 and Prop. 20 in <ref type="bibr" target="#b43">[44]</ref> with ε ′ = 1, with the assumed simplified expression for the covering numbers of X and a rescaling of the metric such that B disappears from the covering numbers expression.</p><p>Lemma 12 (Schur's test). Let T be the integral operator defined by T f (x) = k(x, x ′ )f (x ′ )dµ(x ′ ).</p><p>If the kernel k satisfies </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>−1/ 2 − D − 1 / 2 .</head><label>212</label><figDesc>Wτ Define d = d W,P and d τ = d Wτ ,P . The operator D −1/2 Wτ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Z</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 FLemma 8 (</head><label>28</label><figDesc>An easy recursion gives the result. Bound on c-GCNs). Apply a c-GCN to a random graph model Γ = (P, W, f ). Denote by f (ℓ) the function at each layer. Then we havef (ℓ) *where * indicates L 2 (P ) or ∞.Proof. For j d ℓ , using Lemma 6 and (4) we write f(ℓ)    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Lemma 9 (</head><label>9</label><figDesc>Piecewise Lipschitz property of c-GCNs). Let Γ be a random graph model. Assume that f is piecewise (c f , n f )-Lipschitz. Then, Φ W,P (f ) is piecewise (C, n f n X )-Lipschitz with C = θ c f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>.</head><label></label><figDesc>As we have seen in the proof of Lemma 5, W is piecewise the X i . So, for k 1, for x, x ′ ∈ X i by Schwartz inequality we have) (x) − f (M) (x ′ ) B (M−1) 0 f (M) (x) − f (M) (x ′ ) + H (M−1) 2 f (M−1) L 2 (P ) c Lip. c max c 2 min d(x, x ′ )A recursion gives the result, with Lemma 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>1 nj 2 min √ nα n e − 3c 2 12+4c</head><label>122</label><figDesc>p ij c min &gt; 0. Denote by P the n × n matrix containing the p ij . There is a universal constant C such that:P L(A) − L(P ) C(1 + c)c max c −log(14) n + e − 3c 2 12+4c nαn+log(n) + e − 3c 2 min nαn 25cmax +log(n) + n − c 4 +6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>, x ′ )|dµ(x ′ ) C and sup x ′ |k(x, x ′ )|dµ(x) C, then T is bounded in L 2 (µ), with T L 2 (µ) C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>leaves the distribution P unchanged, or in other words, if φ ∈ Σ P</figDesc><table><row><cell>def.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Misc. boundsLemma 6 (Operator norms of filters). Let (E, • E ) be a Banach space and (H, • H ) be a separable Hilbert space. Let L, L ′ be two bounded operators on E, and S : E → H be a linear operator such that S H→E 1. For 1 i d and 1 j d ′ , let h ij = k β ijk λ k be a collection of analytic filters, with B k = (β ijk ) ji ∈ R d ′ ×d the matrix of order-k coefficients, with operator norm B k . Let x 1 , . . . , x d ∈ E be a collection of points. Then:</figDesc><table><row><cell>j</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Consider any j n X . For any x 0 ∈ X j , we have</p><p>The second term is bounded by the inequality above. For the first term, we are going to use Dudley's inequality "tail bound" version <ref type="bibr" target="#b41">[42,</ref><ref type="bibr">Thm 8.1.6]</ref>. We first need to check the sub-gaussian increments of the process Y x . For any x, x ′ ∈ X j , we have</p><p>where we have used, from <ref type="bibr" target="#b41">[42]</ref>, Prop. 2.6.1 for the first line, Lemma 2.6.8 for the second, Example 2.5.8 for the third, and the Lipschitz property of W for the last. Now, we apply Dudley's inequality <ref type="bibr" target="#b41">[42,</ref><ref type="bibr">Thm 8.1.6</ref>] to obtain that with probability 1 − ρ,</p><p>Combining with the decomposition above and applying a union bound over the X j yields the desired result.</p><p>Lemma 5 (Chaining on normalized Laplacians). Consider a kernel W and a probability distribution P satisfying (6), any function f ∈ B(X ), and x 1 , . . . , x n drawn iid from P . Assume n satisfies <ref type="bibr" target="#b16">(17)</ref>. Then with probability at least 1 − ρ,</p><p>where</p><p>Proof. Again we assume f ∞ 1 without lost of generality.</p><p>By Lemma 4 with f = 1 and (17), with probability 1 − ρ/2 we have</p><p>and in particular d X c min /2. In this case, for all x, we have </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Community detection and stochastic block models: recent developments</title>
		<author>
			<persName><forename type="first">E</forename><surname>Abbe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="page" from="1" to="86" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep scattering spectrum</title>
		<author>
			<persName><forename type="first">J</forename><surname>Andén</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="4114" to="4128" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<title level="m">Convergence of Laplacian eigenmaps. NIPS</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Group invariance, stability to deformations, and complexity of deep convolutional representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bietti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1" to="49" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Random Graphs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bollobas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Geometric Deep Learning: Going beyond Euclidean data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1872" to="1886" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spectral Networks and Locally Connected Networks on Graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improved spectral convergence rates for graph Laplacians on epsilon-graphs and k-NN graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Trillos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.1347</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Supervised community detection with line graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the equivalence between graph isomorphism testing and function approximation with GNNs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Can graph neural networks count substructures?</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04025</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adaptive Estimation of Nonparametric Geometric Graphs</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">De</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lacour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M P</forename><surname>Ngoc</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02107</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Model-free consistency of graph partitioning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guillot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rajaratnam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03860</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Diffusion scattering transforms on graphs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stability of Graph Scattering Transforms</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Stability Properties of Graph Neural Networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04497</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A survey of statistical network models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Goldenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Fienberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Airoldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="233" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stochastic blockmodels: First steps</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Holland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soc. Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="137" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Aided Mol. Des</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Universal Invariant and Equivariant Graph Neural Networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Keriven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Sparse and Smooth: improved guarantees for Spectral Clustering in the Dynamic Stochastic Block Model</title>
		<author>
			<persName><forename type="first">N</forename><surname>Keriven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vaiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.02892</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="30" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-Supervised Learning with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Kolaczyk</surname></persName>
		</author>
		<title level="m">Statistical Analysis of Network Data: Methods and Models</title>
				<imprint>
			<publisher>Springer Series in Statistics</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Consistency of spectral clustering in stochastic block models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rinaldo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="215" to="237" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kutyniok</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12972</idno>
		<title level="m">Transferability of Spectral Graph Convolutional Neural Networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Large networks and graph limits</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lovász</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Colloquium Publications</publisher>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">487</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">The Power of Graph Convolutional Networks to Distinguish Random Graph Models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Magner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baranwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Hero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.12954</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Group Invariant Scattering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Pure Appl. Math</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1331" to="1398" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Provably Powerful Graph Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On the Universality of Invariant Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Segol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Modeling heterogeneity in random graphs through latent space models: a selective review</title>
		<author>
			<persName><forename type="first">C</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Robin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESAIM: Proc</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="55" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Random Geometric Graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Penrose</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Optimal Transport. Found. Trends Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="355" to="206" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">DCFNet: Deep Neural Network with Decomposed Convolutional Filters</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Calderbank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="6687" to="6696" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On learning with integral operators</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">De</forename><surname>Vito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="905" to="934" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F O</forename><surname>Chamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05030</idno>
		<title level="m">Graphon Signal Processing</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Limit theorems for eigenvectors of the normalized Laplacian for random graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Priebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2360" to="2415" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">High-dimensional probability: An introduction with applications in data science</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vershynin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Consistency of spectral clustering</title>
		<author>
			<persName><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="555" to="586" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sharp asymptotic and finite-sample rates of convergence of empirical measures in Wasserstein distance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4 A</biblScope>
			<biblScope unit="page" from="2620" to="2648" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A Comprehensive Survey on Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks via scattering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Comput. Harmon. Anal</title>
		<imprint>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
