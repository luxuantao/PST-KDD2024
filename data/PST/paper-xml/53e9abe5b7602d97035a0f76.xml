<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CLOSET+: Searching for the Best Strategies for Mining Frequent Closed Itemsets *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jianyong</forename><surname>Wang</surname></persName>
							<email>wangj@cs.uiuc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
							<email>hanj@cs.uiuc.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
							<email>jianpei@cse.buffalo.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering State</orgName>
								<orgName type="institution">University of New York at Buffalo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CLOSET+: Searching for the Best Strategies for Mining Frequent Closed Itemsets *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">64E2F65C354E7DF080D272C06B5A12CA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H</term>
					<term>2</term>
					<term>8 [Database Management]: Database applications-Data Mining Frequent closed itemsets, association rules</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mining frequent closed itemsets provides complete and nonredundant results for frequent pattern analysis. Extensive studies have proposed various strategies for efficient frequent closed itemset mining, such as depth-first search vs. breadthfirst search, vertical formats vs. horizontal formats, treestructure vs. other data structures, top-down vs. bottomup traversal, pseudo projection vs. physical projection of conditional database, etc. It is the right time to ask "what are the pros and cons of the strategies?" and "what and how can we pick and integrate the best strategies to achieve higher performance in general cases?"</p><p>In this study, we answer the above questions by a systematic study of the search strategies and develop a winning algorithm CLOSET+. CLOSET+ integrates the advantages of the previously proposed effective strategies as well as some ones newly developed here. A thorough performance study on synthetic and real data sets has shown the advantages of the strategies and the improvement of CLOSET+ over existing mining algorithms, including CLOSET, CHARM and OP, in terms of runtime, memory usage and scalability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Since the introduction of association rule mining <ref type="bibr" target="#b1">[1]</ref>, there have been extensive studies on efficient frequent itemset mining methods, such as <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr">8,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b10">10]</ref>. Most of the well studied frequent pattern mining algorithms, including Apriori <ref type="bibr" target="#b2">[2]</ref>, FP-growth <ref type="bibr">[8]</ref>, H-mine <ref type="bibr" target="#b13">[13]</ref>, and OP <ref type="bibr" target="#b10">[10]</ref>, mine the complete set of frequent itemsets. These algorithms may have good performance when the support threshold is high and the pattern space is sparse. However, when the support threshold drops low, the number of frequent itemsets goes up dramatically, and the performance of these algorithms deteriorates quickly because of the generation of a huge number of patterns. Moreover, the effectiveness of the mining of the complete set degrades because it generates numerous redundant patterns. A simple example is that, in a database having only one transaction of length 100, it will generate 2 100 -1 frequent itemsets if the absolute minimum support threshold is set to 1.</p><p>The closed itemset mining, initially proposed in <ref type="bibr" target="#b12">[12]</ref>, mines only those frequent itemsets having no proper superset with the same support. Mining closed itemsets, as shown in <ref type="bibr" target="#b17">[17]</ref>, can lead to orders of magnitude smaller result set (than mining frequent itemsets) while retaining the completeness, i.e., from this concise result set, it is straightforward to generate all the frequent itemsets with accurate support counts.</p><p>In the last several years, extensive studies have proposed fast algorithms for mining frequent closed itemsets, such as A-close <ref type="bibr" target="#b12">[12]</ref>, CLOSET <ref type="bibr" target="#b14">[14]</ref>, MAFIA (it has an option to generate closed itemsets) <ref type="bibr" target="#b5">[5]</ref>, and CHARM <ref type="bibr" target="#b18">[18]</ref>. Various search strategies have been developed, such as depth-first search vs. breadth-first search, vertical formats vs. horizontal formats, tree-structure vs. other data structures, topdown vs. bottom-up traversal, pseudo projection vs. physical projection of conditional database, etc. However, two critical things are missing: <ref type="bibr" target="#b1">(1)</ref> there is no systematic study on comparing the strategies and evaluate their pros and cons objectively; and (2) there is no thorough discussion on how to integrate the winning strategies and achieve an even better algorithm. With the research proceeded so far, it is the right time to ask "what are the pros and cons of the strategies?" and "what and how can we pick and integrate the best strategies to achieve higher performance in general cases?"</p><p>In this study, we answer the above questions by a systematic study on the search strategies and develop a winning algorithm CLOSET+. CLOSET+ integrates the advantages of the previously proposed effective strategies as well as some ones newly developed here. A thorough performance study on synthetic and real data sets has shown the advantages of the strategies and the improvement of CLOSET+ over existing mining algorithms, including CLOSET, CHARM and OP, in terms of runtime, memory usage and scalability.</p><p>The remaining of the paper is organized as follows. In Section 2, we briefly revisit the problem definition of frequent closed itemset mining and the related work. In Section 3, we present an overview of the principal search strategies developed before and analyze their pros and cons. In Section 4, we devise algorithm CLOSET+ by integrating some winning strategies as well as some novel ones developed here. A thorough performance study of CLOSET+ in comparison with several recently developed efficient algorithms is reported in Section 5. We conclude this study in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROBLEM DEFINITION AND RELATED WORK</head><p>A transaction database T DB is a set of transactions, where each transaction, denoted as a tuple tid, X , contains a set of items (i.e., X) and is associated with a unique transaction identity tid. Let I = {i1, i2, . . . , in} be the complete set of distinct items appearing in T DB. An itemset Y is a non-empty subset of I and is called an l-itemset if it contains l items. An itemset {x1, . . . , x l } is also denoted as</p><formula xml:id="formula_0">x1 • • • x l . A transaction tid, X is said to contain itemset Y if Y ⊂ X.</formula><p>The number of transactions in TDB containing itemset Y is called the support of itemset Y , denoted as sup(Y ). Given a minimum support threshold, min sup, an itemset Y is frequent if sup(Y ) ≥ min sup.</p><formula xml:id="formula_1">Definition 1 (Frequent closed itemset). An item- set Y is a frequent closed itemset if it is frequent and there exists no proper superset Y ⊃ Y such that sup(Y ) = sup(Y ).</formula><p>Example 1. The first two columns in Table <ref type="table" target="#tab_0">1</ref> show the transaction database TDB in our running example. Suppose min sup = 2, we can find and sort the list of frequent items in support descending order. The sorted item list is called f list. In this example f list = f:4, c:4, a:3, b:3, m:3, p:3 . The frequent items in each transaction are sorted according to f list and shown in the third column of Table <ref type="table" target="#tab_0">1</ref>. Itemset f c is a frequent 2-itemset with support 3, but it is not closed, because it has a superset f cam whose support is also 3. f acm is a frequent closed itemset.  <ref type="bibr" target="#b12">[12]</ref>, CLOSET <ref type="bibr" target="#b14">[14]</ref>, MAFIA <ref type="bibr" target="#b5">[5]</ref> and CHARM <ref type="bibr" target="#b18">[18]</ref>. A-close uses a breadth-first search to find the frequent closed patterns. In dense datasets or datasets with long patterns, breadth-first searches may encounter difficulties since there could be many candidates and the searches need to scan the database many times. This is shown in several performance studies (e.g., <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b18">18]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related work Popular algorithms for mining frequent closed itemsets include A-close</head><p>CLOSET <ref type="bibr" target="#b14">[14]</ref>, is an extension of the FP-growth algorithm <ref type="bibr">[8]</ref>, which constructs a frequent pattern tree FP-tree and recursively builds conditional FP-trees in a bottom-up treesearch manner. Although CLOSET uses several optimization techniques to enhance the mining performance, its performance still suffers in sparse datasets or when the support threshold is low.</p><p>Both MAFIA <ref type="bibr" target="#b5">[5]</ref> and CHARM <ref type="bibr" target="#b18">[18]</ref> use a vertical representation of the datasets. MAFIA is mainly designed for mining maximal itemsets, but it has an option to mine closed itemsets. One of its main features is the compressed vertical bitmap structure. CHARM enumerates closed itemsets using a dual itemset-tidset search tree and adopts the Diffset technique to reduce the size of the intermediate tidsets. The most costly operation for the algorithms using vertical format is the intersection on tidsets. CHARM shows better performance than A-close, Pascal, MAFIA, and CLOSET in many dense datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">STRATEGIES FOR FREQUENT CLOSED ITEMSET MINING</head><p>Various strategies for mining frequent closed itemsets are proposed in the previous studies. In this section, we present a systematic overview of these strategies, and analyze their pros and cons.</p><p>One essential principle for frequent itemset mining is the Apriori property <ref type="bibr" target="#b2">[2]</ref>: "every subset of a frequent itemset must be frequent". Accordingly, every frequent itemset consists of only frequent items.</p><p>Given a set of frequent items, F , the complete set of itemsets is a lattice over 2 F . It can be shown that the complete set of closed itemsets forms a sub-lattice of 2 F . Fig. <ref type="figure" target="#fig_0">1</ref>(a) draws the portion of frequent closed itemsets in our running example. The problem of searching for the complete set of frequent closed itemsets is to find the complete set of frequent itemsets in the lattice of closed itemsets. Most methods start from the top of the lattice (i.e., the frequent items). The strategies can be divided into several orthogonal categories.</p><p>Breadth-first vs. depth-first search. The breadth-first search approaches search the lattice level-by-level: it uses the frequent itemsets at the current level with length k to generate the candidates at the next level with length (k +1), and a new database scan is needed to count the supports of length-(k + 1) candidates. Due to its too many database scans, it is not suitable for mining long patterns. In contrast, a depth-first search method traverses the lattice in depth-first order, and the subtree of an itemset is searched only if the itemset is frequent. Moreover, when the itemsets becomes longer, depth-first search shrinks search space quickly. As a result, the depth-first search method is usually a winner for mining long patterns. Some previous studies (e.g., <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b18">18]</ref>) clearly elaborate that the depth-first search methods are usually more efficient than the breadth-first search methods like A-close.</p><p>Horizontal vs. vertical data formats. The transaction information can be recorded in two alternative formats. The horizontal format is an intuitive bookkeeping of the transactions. Each transaction is recorded as a list of items. In the vertical format, instead of recording the transactions explicitly, a tid-list is kept for every item, where the identities of the transactions containing the item are listed. A-close and CLOSET use the horizontal data format, while CHARM and MAFIA use the vertical one.</p><p>A vertical format-based method needs to maintain a tidset for each frequent itemset. When the database is big, each tidset is on average big, and many such intermediate results will consume a lot of memory. In contrast, if we properly choose a compressed structure like FP-tree, a horizontal format-based method will not cause too much space usage, because the itemsets can share common path if they share common prefix, and each of their tidlists is represented by a count. Moreover, for a vertical format-based algorithm, one intersection operation can only find one frequent itemset. For a horizontal format-based method like CLOSET, one scan of a projected database can find many local frequent items which can be used to grow the prefix itemset to generate frequent itemsets. In this paper, we will compare CLOSET+, a horizontal format-based algorithm, with CHARM, a vertical format-based one, in terms of scalability and efficiency in both runtime and space usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data compression techniques.</head><p>A transaction database is usually huge. If a database can be compressed and only the information related to the mining is kept, the mining can be efficient. Recently, some data compression methods have been devised. FP-tree and Diffset are two typical examples.</p><p>An FP-tree <ref type="bibr">[8]</ref> of a transaction database is a prefix tree of the lists of frequent items in the transactions. The idea can be illustrated in the following example.</p><p>Example 2. The FP-tree of our running example is constructed as follows: Scan the database once to find the set of frequent items and sort them in the support descending order to get the f list (see Example 1). To insert a transaction into the FP-tree, infrequent items are removed and the remaining items in the transaction are sorted according to the item ordering in f list, i.e., the least frequent item at the leaf, and the items with higher global support at a higher level in the FP-tree. Fig. <ref type="figure" target="#fig_0">1(b)</ref> shows the FP-tree.</p><p>The FP-tree structure has several advantages in mining frequent itemsets. First, FP-tree often has a high compression ratio in representing the dataset because (1) infrequent items identified in the first database scan will not be used in the tree construction, and (2) a set of transactions sharing the same subset of items may share common prefix paths from the root in an FP-tree. According to our experience, for some dense datasets, its compression ratio can reach several thousand. Even for sparse datasets, it is still quite effective in compressing original datasets, especially when database is large (e.g., many real retail databases contain billions of transactions), since many transactions may share some common subsets of items. Second, the high compression ratio leads to efficient frequency counting at iterative scanning of FP-tree. Third, efficient depth-first search becomes straightforward using FP-tree. More importantly, FP-tree contains all the necessary information for mining frequent itemsets, its completeness can assure the correctness of an FP-tree based algorithm.</p><p>Diffset is an efficient compression of the tid-set for methods adopting the vertical data format. For a vertical formatbased algorithm like CHARM, computing the supports requires intersections on tidsets, when the tidset cardinality becomes large, not only will the tidsets consume much memory, but also the tidset intersection gets costly. To overcome this, CHARM develops a Diffset technique to keep track of only the differences in the tids of a candidate pattern from its parent pattern. Experiments in <ref type="bibr" target="#b18">[18]</ref> showed Diffset can reduce the space usage by orders of magnitude.</p><p>Pruning techniques for closed itemset mining. In the previous studies of depth-first search approaches for mining frequent closed (or maximal) itemsets, mainly two search space pruning techniques have been proposed as the following two lemmas. These techniques have been used in Max-Miner <ref type="bibr" target="#b3">[3]</ref>, CLOSET <ref type="bibr" target="#b14">[14]</ref>, MAFIA <ref type="bibr" target="#b5">[5]</ref> and CHARM <ref type="bibr" target="#b18">[18]</ref>. Lemma 3.1. (item merging) Let X be a frequent itemset. If every transaction containing itemset X also contains itemset Y but not any proper superset of Y , then X ∪ Y forms a frequent closed itemset and there is no need to search any itemset containing X but no Y .</p><p>Example 3. In our running example shown in Table <ref type="table" target="#tab_0">1</ref>, the projected conditional database for prefix itemset f c:3 is {(a,m,p), (a,m,p), (a,b,m)} (items d and g are infrequent and removed), from which we can see each of its transaction contains itemset am but no proper superset of am. Itemset am can be merged with f c to form a closed itemset f cam:3, and we do not need to mine closed itemsets containing f c but no am. Lemma 3.2. (sub-itemset pruning) Let X be the frequent itemset currently under consideration. If X is a proper subset of an already found frequent closed itemset Y and sup(X) = sup(Y ), then X and all of X's descendants in the set enumeration tree <ref type="bibr" target="#b15">[15]</ref> cannot be frequent closed itemsets and thus can be pruned.</p><p>Example 4. Many frequent pattern mining algorithms follow the divide-and-conquer paradigm. In our running example shown in Fig. <ref type="figure" target="#fig_0">1</ref>(b), a top-down divide-and-conquer paradigm follows the f list order shown in Example 1 (in contrast, a bottom-up divide-and-conquer paradigm will follow the inverse f list order): (1) first mine the patterns containing item f , (2) mine the patterns containing item c but no f , (3) mine the patterns containing item a but no f nor c, ..., and finally mine the patterns containing only p. At some point when we want to mine the patterns with prefix itemset ca:3, we will find that ca:3 is a proper subset of an already found closed itemset f cam:3 with the same support, we can safely stop mining the closed patterns with prefix ca:3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CLOSET+: AN EFFICIENT METHOD FOR CLOSED ITEMSET MINING</head><p>In this section, we devise a new frequent closed itemset mining algorithm, CLOSET+, by integrating some winning search strategies and developing some novel ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview of CLOSET+</head><p>CLOSET+ follows the popular divide-and-conquer paradigm which has been shown one possible instance in Example 4 and the depth-first search strategy which has been verified a winner for mining long patterns by several efficient frequent pattern mining algorithms. It uses FP-tree as the compression technique. A depth-first search and horizontal format-based method like CLOSET+ will compute the local frequent items of a certain prefix by building and scanning its projected database. Therefor, a hybrid tree-projection method will be introduced to improve the space efficiency.</p><p>Unlike frequent itemset mining, during the closed itemset mining process there may exist some prefix itemsets that are unpromising to be used to grow closed itemsets. We should detect and remove such unpromising prefix itemsets as quickly as possible. Besides adopting the above mentioned item merging and sub-itemset pruning methods, we also proposed the item skipping technique to further prune search space and speed up mining.</p><p>Previous algorithms need to maintain all the frequent closed itemsets mined so far in memory in order to check if a newly found closed itemset candidate is really closed. If there exist a lot of frequent closed patterns, such kind of closure checking will be costly in both memory usage and runtime. We have also designed an efficient subset-checking scheme: the combination of the 2-level hash-indexed result tree based method and the pseudo-projection based upward checking method, which can be used to save memory usage and accelerate the closure-checking significantly. In the following, the above mentioned mining techniques and the CLOSET+ algorithm are developed step by step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Hybrid Tree Projection Method</head><p>Most previously developed FP-tree-based methods, such as FP-growth and CLOSET, grow patterns by projection of conditional databases in a bottom-up manner <ref type="bibr">[8,</ref><ref type="bibr" target="#b14">14]</ref>. However, such a search order may not always lead to the best performance for different kinds of data sets. In CLOSET+, a hybrid tree-projection method is developed, which builds conditional projected databases in two ways: bottom-up physical tree-projection for dense datasets and top-down pseudo tree-projection for sparse datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Bottom-up physical tree-projection</head><p>For dense datasets, their FP-trees can be hundreds (or even thousands) times smaller than their corresponding original datasets due to compression. Its conditional projected FP-trees are usually very compact as well. Each projected FP-tree is much smaller than the original FP-tree, and mining on such a compact structure will also be efficient. As a result, for dense datasets CLOSET+ still physically builds projected FP-trees and it is done recursively in a bottom-up manner (i.e., in support ascending order).</p><p>To assist the physical FP-tree projection, there is a header table for each FP-tree, which records each item's ID, count, and a side-link pointer that links all the nodes with the same itemID as the labels. The global FP-tree in our exam- ple is shown in Fig. <ref type="figure" target="#fig_1">2</ref>(a). To build conditional FP-tree for prefix item p:3, we will first find the conditional database containing p (denoted as T DB|p:3) by following item p's side-link pointers. A prefix path from a node, Np, which has an itemID p and a count Cp, upward to the root node represents an amalgamative transaction with support Cp for prefix item p. Here T DB|p:3 consists of two amalgamative transactions: f cam : 2 and cb : 1 , from which the projected FP-tree for prefix p:3 is built as Fig. <ref type="figure" target="#fig_1">2(b</ref>).</p><p>After the projected FP-tree for prefix p:3 has been constructed, we will mine the frequent closed itemsets with prefix p:3 from it. (1) First, we mine the closed itemsets with prefix pm:2. By following item m's side-link pointer in Fig. <ref type="figure" target="#fig_1">2</ref>(b), we build prefix pm:2's conditional database, as T DB|pm:2 = cf a : 2 . According to the item merging technique, prefix pm:2 can be merged with itemset cf a:2 to form a frequent closed itemset, pmcf a:2, and we do not need to mine closed itemsets with prefix pmc:2, pmf :2, or pma:2. (2) Second, we'll mine closed itemsets with prefix pa:2. Because pa:2 is a proper subset of an already mined itemset pmaf c:2 with the same support, according to the sub-itemset pruning method, there is no need to mine closed itemsets with prefix pa:2. (3) Similarly, prefix pf :2 cannot be used as a start point to grow any closed itemsets. (4) Finally, we will mine closed itemsets with prefix pc:3, by following item c's side-link pointer in Fig. <ref type="figure" target="#fig_1">2</ref>(b), we find its conditional database is empty, we only need to output prefix pc:3 as a frequent closed itemset candidate. Until now all the frequent closed itemsets for prefix p:3 have been mined.</p><p>Similarly, we can build physically projected FP-trees from the global FP-tree and mine frequent closed itemsets from them in a recursive way for prefixes m:3, b:3, a:3, c:4, and f :4 respectively, these FP-trees are shown in Fig. <ref type="figure" target="#fig_1">2(c</ref>)-(f) (The FP-tree for prefix f :4 is empty and is not shown).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Top-down pseudo tree-projection</head><p>Physically projecting FP-trees will introduce some overhead both in space usage and runtime (due to allocating and freeing memory for projected FP-trees), especially if the dataset is sparse, the projected FP-tree will not be very compact and does not shrink quickly. Instead of physically building projected FP-trees, a new method is developed for sparse datasets: top-down pseudo projection of FP-tree. Unlike bottom-up physical projection of FP-trees, the pseudo projection is done in the f list order (i.e., in support descending order). Similar to bottom-up physical projection, a header table is also used to record enough information such as local frequent items, their counts and side-link pointers to FP-tree nodes in order to locate the subtrees for a certain prefix itemset. Based on our running example, the top-down pseudoprojection method is illustrated as follows. Fig. <ref type="figure" target="#fig_3">3(a)</ref> shows the global FP-tree and the status of global header table. Initially only the child nodes (e.g., nodes f :4 and c:1) directly under the root node are linked from the global header table. Because we build FP-tree according to the f list order, all the projected transactions containing item f can be found from the subtree under the node with label f :4 (i.e., the dashed polygon in Fig. <ref type="figure" target="#fig_3">3(a)</ref>).</p><p>By following the side-link pointer of item f in the global header table of Fig. <ref type="figure" target="#fig_2">3</ref>(a), we can locate the subtree under node f :4. The local frequent items can be found by scanning this subtree and used to build the header table for prefix itemset f :4, as shown in Fig. <ref type="figure" target="#fig_4">4(a</ref>). Here only the child nodes directly under node f :4 are linked from the header table H f :4 . Based on the header table H f :4 , we can mine the frequent itemsets with prefix f . (1) First, we'll mine closed itemsets containing f c:3. By scanning the subtree under node c:3 in Fig. <ref type="figure" target="#fig_4">4</ref>(a), we can find the local frequent items and build the header table for prefix itemset f c:3, as shown in Fig. <ref type="figure" target="#fig_4">4(b)</ref>. Items a and m have the same support as that of prefix f c:3, according to the item merging technique, they can be merged with f c:3 to form a new prefix f cam:3 (and it is also a closed itemset) and we will not need to mine closed itemsets with prefix f ca:3 or f cm:3. Although f ca:3 cannot be used as a prefix to grow closed itemsets, we still need to follow item a's side-link pointer to find all the child nodes directly under node a:3 and make them linked from header table H f c:3 . Because item b is infrequent in H f c:3 , the node b:1 under node a:3 does not need to be linked from H f c:3 , instead, its child node m:1 will be linked from H f c:3 . The new header table H f c:3 with adjusted side-link pointers is shown in Fig. <ref type="figure" target="#fig_4">4(c</ref>). Similarly, we do not need to mine closed itemsets with prefix f cm:3, but the child nodes under nodes m:2 and m:1 should be linked from H f c:3 , as shown in Fig. <ref type="figure" target="#fig_4">4(d)</ref>. When we mine the closed itemsets with prefix f camp:2, we find the subtree under node p:2 is empty, we'll output f camp:2 as a frequent closed itemset candidate and stop mining closed itemsets with prefix f c:3. (2) Second, after the child node a:3 of node c:3 (see Fig. <ref type="figure" target="#fig_4">4(a)</ref>) has been linked from header table H f :4 , we can mine closed itemsets with prefix f a:3 but no c. (3) In a similar way, we can mine closed itemsets with prefix f b but no c nor a, with f m but no c nor a and nor b, and those only with f p, respectively.</p><p>As illustrated in the above example, we need to do two kinds of things in the process of mining closed itemsets for a certain prefix: (1) find its subtrees by following its sidelink pointers and recursively mine these subtrees to find all its frequent closed itemsets; and (2) after that, do side-link pointer adjustment, i.e., all its child nodes should be linked from the head table.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The Item Skipping Technique</head><p>Since CLOSET+ adopts depth-first search, at each level, there will be a prefix itemset X associated with a header table and a projected FP-tree. According to the Apriori property, a local frequent item in X's header table must also appear in the higher-level header tables. If a local frequent item has the same support in more than one header table at different levels, we can use Lemma 4.1 to prune search space.</p><p>Lemma 4.1. (Item skipping) If a local frequent item has the same support in several header tables at different levels, one can safely prune it from the header tables at higher levels.</p><p>Proof. Assume item x at level l is a local frequent item of prefix itemset X l and has the same support in level k's header table of prefix itemset X k , where (0</p><formula xml:id="formula_2">≤ k &lt; l) ∧ (X k ⊂ X l ). Let X k = X k ∪ x and X l = X l ∪ x, it is obvious that (X k ⊂ X l )∧(sup(X k ) = sup(X l ))</formula><p>, which means any frequent itemset grown from X k can be subsumed by a corresponding frequent itemset grown from X l . As a result it is non-closed and item x can be pruned from header table at level k.</p><p>This pruning method can be used in both bottom-up physical and top-down pseudo tree-projection paradigms. For example, in Fig. <ref type="figure" target="#fig_1">2(c</ref>), there is a local frequent item a with support 3 for prefix itemset m:3. We also find that item a appears in the global header table (see Fig. <ref type="figure" target="#fig_1">2(a)</ref>) with the same support, item a can be safely pruned from the global header table. Similarly, from Fig. <ref type="figure" target="#fig_4">4</ref>, we know that items a, m, and p in H f c:3 have the same supports as those in H f :4 , these items can be safely removed from H f :4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Efficient Subset Checking</head><p>The search space pruning methods can only be used to remove some prefix itemsets that are unpromising to be used as a start point to grow closed itemsets, but they cannot assure that a frequent prefix itemset is closed. When we get a new frequent prefix itemset, we need to do two kinds of closure checking: the superset-checking checks if this new frequent itemset is a superset of some already found closed itemset candidates with the same support, while the subset-checking checks if the newly found itemset is a subset of an already found closed itemset candidate with the same support. Because both bottom-up physical projection and top-down pseudo projection work under the divide-andconquer and depth-first-search framework, the following theorem states that CLOSET+ only needs to do subset-checking in order to assure a newly found itemset is closed. Theorem 4.1. (Subset checking) Under the framework of divide-and-conquer and using the item merging pruning method introduced in Lemma 3.1, a frequent itemset found by CLOSET+ must be closed if it cannot be subsumed by any other already found frequent closed itemset.</p><p>Proof. Let the list of items in which order CLOSET+ mines frequent closed itemset be m list = I1, I2, . . . , In and the current frequent itemset CLOSET+ has just found is Sc = Ic1Ic2 . . . Icx. Also, we define the relationship between two items Im and In as Im &lt; In if item Im is located before item In in m list. For any two itemsets S1 = I11I12 . . . I1i and S2 = I21I22 . . . I2j where i &gt; 0 and j &gt; 0, we define S1 &lt; S2 if there exists an integer k (k ≥ 1), I 1k &lt; I 2k and I 1l = I 2l (for all l &lt; k) hold. Following we will prove itemset Ic1Ic2 . . . Icx cannot be subsumed by a later found frequent closed itemset, which also means Ic1Ic2 . . . Icx cannot subsume any other already found closed itemsets.</p><p>First, for any frequent itemset S l = I l1 I l2 . . . I ly which is mined later than Sc, we can classify it into one of the two categories: (1) S l is generated by growing Sc; (2) S l is not generated by growing prefix Sc. If S l belongs to the first category, S l is a superset of Sc, but because we have applied the item merging technique, which means all the local items with the same support as Sc's must have been included in Sc, sup(S l ) must be smaller than sup(Sc). As a consequence, Sc cannot be subsumed by S l . If S l belongs to the second category, based on the nature of our divide-and-conquer framework, we know Sc &lt; S l , and there must exist an integer k, I ck &lt; I lk holds, which means S l does not contain item I ck and as a result S l cannot be a superset of Sc.</p><p>From Theorem 4.1, we know that a newly found frequent itemset cannot be subsumed by any later found frequent itemset. That is, if it cannot be subsumed by any already found closed itemset, it must be closed. To assist the subsetchecking, we have designed two efficient techniques.</p><p>Two-level hash-indexed result tree. The first method maintains in a compressed way the set of closed itemsets mined so far in memory. Inspired by the FP-tree <ref type="bibr">[8]</ref> implementation, we can store all the frequent closed itemsets in a compressed result tree which has been used in a slightly different way to do both superset and subset checking <ref type="bibr" target="#b9">[9]</ref>.</p><p>Here we will use it to perform subset-checking in order to assure a newly found itemset is closed and only if it is closed can we insert it into the result tree.</p><p>Now we need to consider how to design efficient subsetchecking based on the result-tree. In CLOSET+, we try to exploit some features of closed itemsets to reduce the search space. If the current itemset Sc can be subsumed by another already found closed itemset Sa, they must have the following relationships: (1) Sc and Sa have the same support; (2) length of Sc is smaller than that of Sa; and (3) all the items in Sc should be contained in Sa. Using these heuristics, we can improve the structure of result tree. First, we introduce 2-level hash indices into result tree: one level uses ID of the last item in Sc as hash key, another uses support of Sc as hash key, and the result tree nodes falling into the same bucket will be linked together. Second, we insert each closed itemset into the result tree according to the f list order, and at each node we also record its length of the path from this node to the root node. Following we use our running example to illustrate the maintenance of result tree and how to do subset checking.</p><p>Example 5. Under depth-first search paradigm, the set of closed itemsets shown in Fig. <ref type="figure" target="#fig_0">1</ref>(a) will be mined and inserted into the result tree in the following order: f :4, f cam:3, f camp:2, f b:2, c:4, cb:2, cp:3, and b:3. Fig. <ref type="figure" target="#fig_5">5</ref> depicts the status of the result tree after inserting closed itemset c:4 (Here we only show part of the index structure due to limited space). In each node, we record the itemID, support, and the length (relative to root node) respectively. Differently from the FP-tree structure, when several closed itemsets share some common prefixes, the support of a node in the common prefix will be the maximum one among the supports of itemsets sharing the common prefix instead of the sum of supports of itemsets sharing the common prefix.</p><p>At the status shown in Fig. <ref type="figure" target="#fig_5">5</ref>, we may later get a frequent itemset ca:3. By using itemID a and support 3 as hash keys and following the corresponding hash link, we will find the node labeled as a:3,3 with a length greater than 2, we then check if ca:3 can be absorbed by the path from node a:3,3 to the root. Unfortunately it cannot pass the checking and will not be inserted into the result tree.</p><p>Pseudo-projection based upward checking. Although the result tree can compress the set of closed itemsets a lot, it still consumes much memory and is not very spaceefficient for sparse datasets. Can we totally remove the requirement of maintaining the set of closed itemsets in memory for subset-checking? As we know, the global FP-tree contains the complete information about the whole set of frequent closed itemsets, thus we can use the global FP-tree to check if a newly found frequent itemset is closed. In such a way, we do not need any additional memory for storing the set of already mined closed itemsets and once a newly found itemset has passed the checking, it will be directly stored in an output file, F.</p><p>The problem becomes how to do subset-checking based on the global FP-tree. As we know, in the top-down pseudo projection method, all the tree nodes and their corresponding prefix paths w.r.t. a prefix itemset, X, can be traced by following its side-link pointer recorded in its header table. We can use the following lemma 4.2 to judge whether a newly found frequent itemset is closed. Lemma 4.2. For a certain prefix itemset, X, as long as we can find any item which (1) appears in each prefix path w.r.t. prefix itemset, X, and (2)does not belong to X, any itemset with prefix X will be non-closed, otherwise, if there's no such item, the union of X and the complete set of its local frequent items which have the same support as sup(X) will form a closed itemset.</p><p>Proof. The first part is easy to prove: If we can find an item, ix, which (1)appears in each prefix path w.r.t. prefix itemset, X, and (2) does not belong to X, this will mean (X ⊂ (X ∪ {ix})) and (sup(X) = sup(X ∪ {ix})) hold, as a result, X or any itemset with prefix X will be non-closed.</p><p>For the second part, because we cannot find any item which (1)appears in each prefix path w.r.t. prefix itemset, X, and (2) does not belong to X, and any other possible items that always appear together with X can only belong to the set of X's local frequent items, as a result, the union of X and the complete set of its local frequent items which have the same support as sup(X) must form a closed itemset.</p><p>Here we'll use some examples to illustrate the upward subset-checking. Assume the prefix X=c:4, we can locate nodes c:1 and c:3 by following the side-link pointer of item c in Fig <ref type="figure" target="#fig_3">3(b</ref>) and find there is only one item, f , which appears in prefix itemset c:4's prefix paths to the root, and it only co-occurs 3 times with prefix c:4. In addition, there's no local frequent item of prefix c:4 with support 4, thus c:4 is closed and will be stored in output file F. Using this method, we can easily figure out that prefix am:3 is not closed, because in the prefix paths of nodes m:2 and m:1, there are two other items, f and c, which appear with am 3 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">The Algorithm</head><p>By integration of the techniques discussed above, we derive the CLOSET+ algorithm as follows.</p><p>Algorithm 1: Closed itemset mining with CLOSET+ Input: (1)A transaction database T DB, and (2) support threshold min sup. Output: The complete set of frequent closed itemsets. Method:</p><p>1. Scan T DB once to find the global frequent items and sort them in support descending order. The sorted frequent item list forms the f list.</p><p>2. Scan T DB and build FP-tree using the f list.</p><p>Note: In the tree building process, compute the average count of an FP-tree node. After the tree has been built, we will judge whether the dataset is dense or sparse according to the average count of an FP-tree node: for dense dataset, choose bottom-up physical tree-projection method; whereas for sparse dataset, use top-down pseudo tree-projection method. According to the chosen tree projection method, initialize the global header table.</p><p>3. With the divide-and-conquer and depth-first searching paradigm, mine FP-tree for frequent closed itemsets in a top-down manner for sparse datasets or bottom-up manner for dense datasets. During the mining process, use the item merging, item skipping, and subitemset pruning methods to prune search space. For each candidate frequent closed itemset, use the twolevel hash indexed result tree method for dense datasets or pseudo-projection based upward checking method for sparse datasets to do closure checking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Stop when all the items in the global header table have been mined. The complete set of frequent closed itemsets can be found either from the result tree or the output file F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">PERFORMANCE EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Test environment and datasets</head><p>In this section we will evaluate CLOSET+ in comparison with three algorithms, OP, CHARM and CLOSET. All the experiments were performed on an IBM ThinkPad R31 with 384 MB memory and Windows XP installed. We used an improved implementation of CLOSET with better performance than that claimed in <ref type="bibr" target="#b14">[14]</ref>. The source code of CHARM and the executable of OP were provided by their authors. We ran the four algorithms on the same Cygwin environment and with -e 1 -d options turned on for CHARM. Because our performance study showed that both OP and CLOSET cannot compete with CLOSET+, we only compared the peak memory usage between CLOSET+ and CHARM.</p><p>We used six real datasets to evaluate the performance and memory usage, and some synthetic datasets to test the scalability by varying database size and the number of distinct items. The characteristics of the datasets are shown in Table <ref type="table" target="#tab_1">2</ref> (the last column shows the average and maximal transaction length).</p><p>Real datasets: Among the six real datasets three are dense and three are sparse (see the distribution of the number of frequent closed itemsets by support threshold in Table <ref type="table" target="#tab_2">3</ref>). The connect dataset contains game state information, mushroom dataset contains characteristics of various species of mushrooms, pumsb* contains census data. The gazelle dataset contains click-stream data from Gazelle.com.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental results</head><p>Comparison with OP. Our experiments show that due to generating a huge number of frequent itemsets, even the best frequent itemset mining algorithm like OP cannot compete with CLOSET+. Fig. <ref type="figure" target="#fig_6">6</ref> and Fig. <ref type="figure" target="#fig_7">7</ref> show the experimental results for mushroom and gazelle datasets.</p><p>As we can see in Fig. <ref type="figure" target="#fig_6">6</ref>, for dense datasets like mushroom CLOSET+ always outperforms OP and when the support threshold is low, CLOSET+ is more than one order of magnitude faster than OP. For sparse datasets like gazelle (see Fig. <ref type="figure" target="#fig_7">7</ref>), when the support is high there will not be too many frequent itemsets, and due to overhead incurred by closure checking in CLOSET+, OP is a little faster than CLOSET+. But once the support threshold is lowered to a certain point, there will be explosive increase in the number of frequent itemsets (e.g., with support 0.05%, a not too low support threshold for a sparse dataset like gazelle, the longest frequent closed itemset has a length 45, from which 2 45 -1 frequent itemsets can be generated), the pruning methods adopted by CLOSET+ will make CLOSET+ orders of magnitude faster than OP.      Comparison with CHARM and CLOSET. We used all the 6 real datasets to test CLOSET+'s performance and memory usage in comparison with two other closed itemset mining algorithms: CHARM and CLOSET. For dense dataset connect, Fig. <ref type="figure">8</ref> and Fig. <ref type="figure">9</ref> show the results. Fig. <ref type="figure">8</ref> shows CLOSET+ can be orders of magnitude faster than CLOSET. When the support is not too low (i.e., higher than 20%), CLOSET+ is several times faster than CHARM. When support threshold is further lowered, they will have similar performance, but at support 10%, CHARM cannot run by reporting an error REALLOC: Not enough core . From Fig. <ref type="figure">9</ref> we know overall CLOSET+ uses less memory than CHARM. For example, at support 85%, CLOSET+ consumes about 1MB while CHARM consumes about 15MB. Pumsb* is another dense dataset. Fig. <ref type="figure" target="#fig_10">10</ref> and Fig. <ref type="figure" target="#fig_11">11</ref> depict the results. Both CLOSET+ and CHARM have significantly better performance than CLOSET and once the support is lower than 20%, CLOSET just cannot finish running. Overall CLOSET+ and CHARM have very similar performance when the support threshold is not too low. At low support threshold like 15%, CHARM will outperform CLOSET+. Fig. <ref type="figure" target="#fig_11">11</ref> shows that CLOSET+ uses much less memory than CHARM.</p><p>Fig. <ref type="figure" target="#fig_1">12</ref> and Fig. <ref type="figure" target="#fig_3">13</ref> demonstrate the results for mushroom dataset. We can see CLOSET can be orders of magnitude slower than CLOSET+ and CHARM, and CLOSET even cannot finish running once the support is less than 0.1%. But there is no clear winner between CLOSET+ and CHARM: At high support threshold, CLOSET+ is several times faster than CHARM; and at very low support threshold, CHARM is a little better than CLOSET+. But CLOSET+ always beats CHARM in terms of memory usage.  Fig. <ref type="figure" target="#fig_14">14</ref> and Fig. <ref type="figure" target="#fig_15">15</ref> present the evaluation results for sparse dataset gazelle. Fig. <ref type="figure" target="#fig_14">14</ref> shows that CLOSET+ and CHARM are faster than CLOSET. At high support CLOSET+ and CHARM have similar performance, and at a little lower support, CHARM is several times faster than CLOSET+, but once we continued lowering the support threshold to 0.005%, CHARM could not run by reporting an error REALLOC: Not enough core . From Fig. <ref type="figure" target="#fig_15">15</ref>, we see that CHARM consumes about two orders of magnitude more memory than CLOSET+ at low support.</p><p>Fig. <ref type="figure" target="#fig_16">16</ref> and Fig. <ref type="figure" target="#fig_17">17</ref> demonstrate the results for retail-chain dataset. CLOSET+ runs the fastest among the three algorithms and uses less memory than CHARM: When support threshold is set to 0.005%, CLOSET+ runs almost 5 times faster than CHARM, but uses only 1/9 of the memory that  CHARM consumes. Fig. <ref type="figure" target="#fig_18">18</ref> and Fig. <ref type="figure" target="#fig_0">19</ref> show the results for the big-market real dataset. We can see that CLOSET+ is also the fastest among the three and uses less memory than CHARM. It runs several times faster than CHARM but uses less memory.   Scalability test. We used the IBM synthetic datasets to test the scalability of CLOSET+ and compared it with both CHARM and CLOSET. We first tested the scalability in terms of database size using the dataset series T10I4DxP1k with base size from 200k tuples to 1400k tuples and support threshold set at 0.005%. From Fig. <ref type="figure" target="#fig_1">20</ref> we can see that, CLOSET has the poorest scalability, and it even cannot run when the dataset contains more than 600K tuples. In comparison with CHARM, CLOSET+ not only runs much faster, it also has much better scalability in terms of base size:</p><p>the slope ratio for CHARM is much higher than that for CLOSET+.</p><p>We also tested the scalability of CLOSET+ in terms of number of distinct items using T10I4D100KPx series with number of distinct items set at 4333, 13845, 24550 and 29169, respectively, and minimum support set at 0.005%. From Fig. <ref type="figure" target="#fig_1">21</ref>, we can see that initially these three algorithms have very similar performance when the number of distinct items is small, but once the number of distinct items increases, the runtime of CHARM and CLOSET will have a much bigger jump than CLOSET+, which means CLOSET+ also has better scalability than both CHARM and CLOSET in terms of the number of distinct items.</p><p>The above experimental results show that: (1) Although CHARM adopts the Diffset technique which can reduce space usage significantly <ref type="bibr" target="#b18">[18]</ref>, it still consumes more memory than CLOSET+, and in some cases it can use over an order of magnitude more memory than CLOSET+. (2) Due to the new techniques developed here, such as the hybrid tree-projection mining strategy, the item-skipping pruning method, and the subset-checking techniques(i.e., the two-level hash-indexed result-tree and pseudo-projection based upward checking), CLOSET+ can be orders of magnitude faster than CLOSET, and is very efficient with low support even in the case CLOSET and CHARM cannot run. (3) CLOSET+ has linear scalability and is more scalable than CHARM and CLOSET in terms of both base size and the number of distinct items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>Frequent pattern mining has been studied extensively in data mining research. In this study, we have re-examined some previously proposed methodologies, and mainly focused on the new techniques developed for CLOSET+, a highly scalable and both runtime and space efficient algorithm for dense and sparse datasets, on different data distributions and support thresholds.</p><p>The thorough performance evaluation in this study reveals that: (1) For mining frequent patterns, one should work on mining closed patterns instead of all patterns because the former has the same expressive power as the latter but leads to more compact and meaningful results and likely better efficiency. (2) There is a popular myth: Algorithms based on the vertical-format are better than those based on the horizontal-format. Our study shows that an algorithm based on the vertical format, due to its necessity to identify tids (even using the Diffset compression technique) will likely take more memory than an FP-tree-based algorithm and is less scalable if the latter is implemented nicely. (3) Multiple, integrated optimization techniques for database projection, search space pruning, and pattern closure-checking are needed for high performance pattern mining. Often, different data characteristics may require different mining methodologies, e.g., in CLOSET+, we use the top-down pseudo tree-projection and upward subset-checking for sparse datasets, whereas for dense datasets, the bottom-up physical tree-projection and a compressed result-tree have been adopted.</p><p>Currently CLOSET+ has been successfully employed to mine non-redundant association rules. In the future, we will explore more applications, including association-based classification, clustering, and dependency/linkage analysis in large databases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The set of frequent closed itemsets and the FP-tree in the running example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Bottom-up physical tree-projection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Top-down pseudo tree-projection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 (</head><label>3</label><figDesc>b), Fig.3(c), and Fig.3(d)show the side-link adjustment of the global header table H after closed itemsets with prefix f :4, c:4, and a:3 have been mined respectively, and the dashed polygons in those figures represent the projected FP-trees for prefix c:4, a:3 and b:3, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Top-down pseudo tree-projection for f :4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Two-level hash indexed result tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Runtime (mushroom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Runtime (gazelle).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Fig. 8. Runtime performance (connect).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Fig. 9. Memory usage (connect).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Runtime performance (pumsb* ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Memory usage (pumsb* ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Fig. 12. Runtime performance (mushroom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Fig. 13. Memory usage (mushroom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Runtime performance (gazelle).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Memory usage (gazelle).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Runtime performance (retail-chain).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. Memory usage (retail-chain).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. Runtime performance (big-market).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>Fig. 19. Memory usage (big-market).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>Fig. 21. Scalability test(T10I4D100kPx).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>A transaction database T DB.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Dataset Characteristics.</figDesc><table><row><cell>These</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Number of frequent closed itemsets vs. relative support threshold.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGEMENTS</head><p>We are grateful to Dr. Mohammed Zaki for providing us the source code of CHARM and Junqiang Liu for providing us the executable code of the OP algorithm.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* The work was supported in part by U.S. National Science Foundation NSF IIS-02-09199, University of Illinois, Microsoft Research, and IBM Faculty Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mining association rules between sets of items in large databases</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Imielinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD&apos;93</title>
		<imprint>
			<date type="published" when="1993-05">May 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast algorithms for mining association rules</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB&apos;94</title>
		<imprint>
			<date type="published" when="1994-09">Sept. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Efficiently Mining long patterns from databases. SIGMOD&apos;98</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Bayardo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998-06">June 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamic Itemset Counting and Implication Rules for Market Basket Data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD&apos;97</title>
		<imprint>
			<date type="published" when="1997-05">May 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MAFIA: A maximal frequent itemset algorithm for transactional databases</title>
		<author>
			<persName><forename type="first">D</forename><surname>Burdick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Calimlim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE&apos;01</title>
		<imprint>
			<date type="published" when="2001-04">April 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discovering All Most Specific Sentences by Randomized Algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gunopulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saluja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDT&apos;97</title>
		<imprint>
			<date type="published" when="1997-01">Jan. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scalable Parallel Data Mining for Association Rules</title>
		<author>
			<persName><forename type="first">E</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In TKDE</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mining frequent patterns without candidate generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD&apos;00</title>
		<imprint>
			<date type="published" when="2000-05">May 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mining Top-k frequent closed patterns without minimum support</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tzvetkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM&apos;02</title>
		<imprint>
			<date type="published" when="2002-12">Dec. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mining frequent item sets by opportunistic projection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD&apos;02</title>
		<imprint>
			<date type="published" when="2002-07">July 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An Effective Hash Based Algorithm for Mining Association Rules</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD&apos;95</title>
		<imprint>
			<date type="published" when="1995-05">May 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discovering frequent closed itemsets for association rules</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pasquier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bastide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Taouil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lakhal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDT&apos;99</title>
		<imprint>
			<date type="published" when="1999-01">Jan. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nishio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>H-Mine</surname></persName>
		</author>
		<title level="m">Hyper-structure mining of frequent patterns in large databases. In ICDM&apos;01</title>
		<imprint>
			<date type="published" when="2001-11">Nov. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">CLOSET: An efficient algorithm for mining frequent closed itemsets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DMKD&apos;00</title>
		<imprint>
			<date type="published" when="2000-05">May 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Search through Systematic Set Enumeration</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rymon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 3rd Int. Conf. on Principles of Knowledge Representation and Reasoning</title>
		<meeting>of 3rd Int. Conf. on Principles of Knowledge Representation and Reasoning</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sampling Large Databases for Association Rules</title>
		<author>
			<persName><forename type="first">H</forename><surname>Toivonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB&apos;96</title>
		<imprint>
			<date type="published" when="1996-09">Sept. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generating non-redundant association rules</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD&apos;00</title>
		<imprint>
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">CHARM: An efficient algorithm for closed itemset mining</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hsiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM&apos;02</title>
		<imprint>
			<date type="published" when="2002-04">April 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Real world performance of association rule algorithms</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mason</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD&apos;01</title>
		<imprint>
			<date type="published" when="2001-08">Aug. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
