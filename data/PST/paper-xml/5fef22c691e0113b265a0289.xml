<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MINILMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-12-31">31 Dec 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
							<email>shaohanh@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
							<email>lidong1@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
							<email>fuwei@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MINILMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-12-31">31 Dec 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2012.15828v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We generalize deep self-attention distillation in MINILM <ref type="bibr" target="#b40">(Wang et al., 2020)</ref> by only using self-attention relation distillation for taskagnostic compression of pretrained Transformers. In particular, we define multi-head selfattention relations as scaled dot-product between the pairs of query, key, and value vectors within each self-attention module. Then we employ the above relational knowledge to train the student model. Besides its simplicity and unified principle, more favorably, there is no restriction in terms of the number of student's attention heads, while most previous work has to guarantee the same head number between teacher and student. Moreover, the fine-grained self-attention relations tend to fully exploit the interaction knowledge learned by Transformer. In addition, we thoroughly examine the layer selection strategy for teacher models, rather than just relying on the last layer as in MINILM. Experimental results demonstrate that our models 1 distilled from base-size and large-size teachers (BERT, and RoBERTa) outperform the state of the art. * Contact person. 1 Distilled models and code will be publicly available at https://aka.ms/minilm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pretrained Transformers <ref type="bibr" target="#b23">(Radford et al., 2018;</ref><ref type="bibr" target="#b7">Devlin et al., 2018;</ref><ref type="bibr" target="#b9">Dong et al., 2019;</ref><ref type="bibr" target="#b9">Yang et al., 2019;</ref><ref type="bibr" target="#b15">Joshi et al., 2019;</ref><ref type="bibr" target="#b21">Liu et al., 2019;</ref><ref type="bibr" target="#b1">Bao et al., 2020;</ref><ref type="bibr" target="#b24">Radford et al., 2019;</ref><ref type="bibr" target="#b25">Raffel et al., 2019;</ref><ref type="bibr" target="#b19">Lewis et al., 2019)</ref> have been highly successful for a wide range of natural language processing tasks. However, these models usually consist of hundreds of millions of parameters and are getting bigger. It brings challenges for fine-tuning and online serving in real-life applications due to the restrictions of computation resources and latency. Knowledge distillation (KD; <ref type="bibr" target="#b11">Hinton et al. 2015</ref><ref type="bibr" target="#b28">, Romero et al. 2015)</ref> has been widely employed to compress pretrained Transformers, which transfers knowledge of the large model (teacher) to the small model (student) by minimizing the differences between teacher and student features. Soft target probabilities (soft labels) and intermediate representations are usually utilized to perform KD training. In this work, we focus on task-agnostic compression of pretrained Transformers <ref type="bibr" target="#b30">(Sanh et al., 2019;</ref><ref type="bibr" target="#b36">Tsai et al., 2019;</ref><ref type="bibr" target="#b14">Jiao et al., 2019;</ref><ref type="bibr" target="#b34">Sun et al., 2019b;</ref><ref type="bibr" target="#b40">Wang et al., 2020)</ref>. The student models are distilled from large pretrained Transformers using large-scale text corpora. The distilled task-agnostic model can be directly finetuned on downstream tasks, and can be utilized to initialize task-specific distillation.</p><p>DistilBERT <ref type="bibr" target="#b30">(Sanh et al., 2019)</ref> uses soft target probabilities for masked language modeling predictions and embedding outputs to train the student. The student model is initialized from the teacher by taking one layer out of two. Tiny-BERT <ref type="bibr" target="#b14">(Jiao et al., 2019)</ref> utilizes hidden states and self-attention distributions (i.e., attention maps and weights), and adopts a uniform function to map student and teacher layers for layer-wise distillation. MobileBERT <ref type="bibr" target="#b34">(Sun et al., 2019b)</ref> introduces specially designed teacher and student models using inverted-bottleneck and bottleneck structures to keep their layer number and hidden size the same, layer-wisely transferring hidden states and self-attention distributions. MINILM <ref type="bibr" target="#b40">(Wang et al., 2020)</ref> proposes deep self-attention distillation, which uses self-attention distributions and value relations to help the student to deeply mimic teacher's self-attention modules. MINILM shows that transferring knowledge of teacher's last layer achieves better performance than layer-wise distillation. In summary, most previous work relies on self-attention distributions to perform KD training, which leads to a restriction that the number of attention heads of student model has to be the same as its teacher.</p><p>In this work, we generalize and simplify deep self-attention distillation of MINILM <ref type="bibr" target="#b40">(Wang et al., 2020)</ref> by using self-attention relation distillation. We introduce multi-head self-attention relations computed by scaled dot-product of pairs of queries, keys and values, which guides the student training. Taking query vectors as an example, in order to obtain queries of multiple relation heads, we first concatenate query vectors of different attention heads, and then split the concatenated vector according to the desired number of relation heads. Afterwards, for teacher and student models with different head numbers, we can align their queries with the same number of relation heads for distillation. Moreover, using a larger number of relation heads brings more fine-grained self-attention knowledge, which helps the student to achieves a deeper mimicry of teacher's self-attention module. In addition, for large-size (24 layers, 1024 hidden size) teachers, extensive experiments indicate that transferring an upper middle layer tends to perform better than using the last layer as in MINILM.</p><p>Experimental results show that our student models distilled from BERT and RoBERTa both outperform state-of-the-art models in different parameter sizes. The 6×768 (6 layers, 768 hidden size) model distilled from BERT LARGE is 2.0× faster, meanwhile, achieving better performance than BERT BASE . The base-size model distilled from RoBERTa LARGE outperforms RoBERTa BASE even using much fewer training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Backbone Network: Transformer</head><p>Multi-layer Transformer <ref type="bibr" target="#b38">(Vaswani et al., 2017)</ref> has been widely adopted in pretrained models. Each Transformer layer consists of a self-attention sublayer and a position-wise fully connected feedforward sub-layer.</p><p>Self-Attention Transformer relies on multi-head self-attention to capture dependencies between words. Given previous Transformer layer's output</p><formula xml:id="formula_0">H l−1 ∈ R |x|×d h , the output of a self-attention head O l,a , a ∈ [1, A h ] is computed via: Q l,a = H l−1 W Q l,a (1) K l,a = H l−1 W K l,a (2) V l,a = H l−1 W V l,a (3) O l,a = softmax( Q l,a K l,a √ d k )V l,a<label>(4)</label></formula><p>Previous layer's output H l−1 is linearly projected to queries, keys and values using parameter matrices </p><formula xml:id="formula_1">W Q l,a , W K l,a , W V l,a ∈ R d h ×d k ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pretrained Language Models</head><p>Pre-training has led to strong improvements across a variety of natural language processing tasks. Pretrained language models are learned on large amounts of text data, and then fine-tuned to adapt to specific tasks. BERT <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref> proposes to pretrain a deep bidirectional Transformer using masked language modeling (MLM) objective. UNILM <ref type="bibr" target="#b9">(Dong et al., 2019)</ref> is jointly pretrained on three types language modeling objectives to adapt to both understanding and generation tasks. <ref type="bibr">XL-Net (Yang et al., 2019)</ref> introduces permutation language modeling objective to predict masked tokens auto-regressively. SpanBERT <ref type="bibr" target="#b15">(Joshi et al., 2019)</ref> improves BERT by incorporating span information. RoBERTa <ref type="bibr" target="#b21">(Liu et al., 2019)</ref> achieves strong performance by training longer steps using large batch size and more text data. MASS <ref type="bibr" target="#b32">(Song et al., 2019)</ref>, T5 <ref type="bibr" target="#b25">(Raffel et al., 2019)</ref> and BART <ref type="bibr" target="#b19">(Lewis et al., 2019)</ref> employ a standard encoder-decoder structure and pretrain the decoder auto-regressively. <ref type="bibr" target="#b1">Bao et al. (2020)</ref> propose a pseud-masked language model by jointly pretrained on MLM and partially auto-regressive MLM objectives. Besides monolingual pretrained models, multilingual pretrained models <ref type="bibr" target="#b7">(Devlin et al., 2018;</ref><ref type="bibr" target="#b17">Lample and Conneau, 2019;</ref><ref type="bibr" target="#b3">Chi et al., 2019;</ref><ref type="bibr" target="#b5">Conneau et al., 2019;</ref><ref type="bibr" target="#b4">Chi et al., 2020</ref>) also advance the state-of-the-art on cross-lingual understanding and generation benchmarks. </p><formula xml:id="formula_2">… Q-Q Relation (ℝ 𝐴𝑟× 𝑥 ×|𝑥| ) K-K Relation (ℝ 𝐴𝑟× 𝑥 ×|𝑥| ) Q-V Relation (ℝ 𝐴𝑟× 𝑥 ×|𝑥| ) … Queries (Q) (ℝ 𝑥 ×𝑑 h ′ ) … Keys (K) (ℝ 𝑥 ×𝑑 ℎ ′ ) … Values (V) (ℝ 𝑥 ×𝑑 ℎ ′ ) Q-Q Relation (ℝ 𝐴𝑟× 𝑥 ×|𝑥| ) … K-K Relation (ℝ 𝐴𝑟× 𝑥 ×|𝑥| ) Q-V Relation (ℝ 𝐴𝑟× 𝑥 ×|𝑥| )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attention Relations</head><p>(KL-Divergence) For large-size teacher, we transfer the self-attention knowledge of an upper middle layer of the teacher. For base-size teacher, using the last layer achieves better performance. Our student models are named as MINILMv2.</p><formula xml:id="formula_3">(KL-Divergence) (KL-Divergence) Q-Q Transfer K-K Transfer Q-V Transfer Multi-Head Dot-Product Multi-Head Dot-Product</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Knowledge Distillation</head><p>Knowledge distillation has been proven to be a promising way to compress large models while maintaining accuracy. Knowledge of a single or an ensemble of large models is used to guide the training of small models. <ref type="bibr" target="#b11">Hinton et al. (2015)</ref> propose to use soft target probabilities to train student models. More fine-grained knowledge such as hidden states <ref type="bibr" target="#b28">(Romero et al., 2015)</ref> and attention distributions <ref type="bibr">(Zagoruyko and Komodakis, 2017;</ref><ref type="bibr" target="#b13">Hu et al., 2018)</ref> are introduced to improve the student model.</p><p>In this work, we focus on task-agnostic knowledge distillation of pretrained Transformers. The distilled task-agnostic model can be fine-tuned to adapt to downstream tasks. It can also be utilized to initialize task-specific distillation <ref type="bibr" target="#b33">(Sun et al., 2019a;</ref><ref type="bibr" target="#b37">Turc et al., 2019;</ref><ref type="bibr" target="#b0">Aguilar et al., 2019;</ref><ref type="bibr" target="#b22">Mukherjee and Awadallah, 2020;</ref><ref type="bibr" target="#b20">Xu et al., 2020;</ref><ref type="bibr" target="#b12">Hou et al., 2020;</ref><ref type="bibr" target="#b20">Li et al., 2020)</ref>, which uses a finetuned teacher model to guide the training of the student on specific tasks. Knowledge used for distillation and layer mapping function are two key points for task-agnostic distillation of pretrained Transformers. Most previous work uses soft target probabilities, hidden states, self-attention distributions and value-relation to train the student model. For the layer mapping function, TinyBERT <ref type="bibr" target="#b14">(Jiao et al., 2019)</ref> uses a uniform strategy to map teacher and student layers. MobileBERT <ref type="bibr" target="#b34">(Sun et al., 2019b)</ref> assumes the student has the same number of layers as its teacher to perform layer-wise distillation. MINILM <ref type="bibr" target="#b40">(Wang et al., 2020)</ref> transfers selfattention knowledge of teacher's last layer to the student last Transformer layer. Different from previous work, our method uses multi-head selfattention relations to eliminate the restriction on the number of student's attention heads. Moreover, we show that transferring the self-attention knowledge of an upper middle layer of the large-size teacher model is more effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multi-Head Self-Attention Relation Distillation</head><p>Following MINILM, the key idea of our approach is to deeply mimic teacher's self-attention module, which draws dependencies between words and is the vital component of Transformer. MINILM uses teacher's self-attention distributions to train the student model. It brings the restriction on the number of attention heads of students, which is required to be the same as its teacher. To achieve a deeper mimicry and avoid using teacher's selfattention distributions, we introduce multi-head self-attention relations of pairs of queries, keys and values to train the student. Besides, we conduct extensive experiments and find that layer selection of  <ref type="bibr" target="#b37">(Turc et al., 2019)</ref> is trained using the MLM objective, without using knowledge distillation. We also report the results of truncated BERT BASE and truncated RoBERTa BASE , which drops the top 6 layers of the base model. Top-layer dropping has been proven to be a strong baseline <ref type="bibr" target="#b29">(Sajjad et al., 2020)</ref>. The fine-tuning results are an average of 4 runs.</p><p>the teacher model is critical for distilling large-size models. Figure <ref type="figure" target="#fig_0">1</ref> gives an overview of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-Head Self-Attention Relations</head><p>Multi-head self-attention relations are obtained by scaled dot-product of pairs 3 of queries, keys and values of multiple relation heads. Taking query vectors as an example, in order to obtain queries of multiple relation heads, we first concatenate queries of different attention heads and then split the concatenated vector based on the desired number of relation heads. The same operation is also performed on keys and values. For teacher and student models which uses different number of attention heads, we convert their queries, keys and values of different number of attention heads into vectors of the same number of relation heads to perform KD training. Our method eliminates the restriction on the number of attention heads of student models. Moreover, using more relation heads in computing self-attention relations brings more fine-grained self-attention knowledge and improves the performance of the student model. We use A 1 , A 2 , A 3 to denote the queries, keys and values of multiple relation heads. The KLdivergence between multi-head self-attention re-2 In addition to task-agnostic distillation, TinyBERT uses task-specific distillation and data augmentation to further improve the model. We report the fine-tuning results of their public task-agnostic model.</p><p>3 There are nine types of self-attention relations, such as query-query, key-key, key-value and query-value relations. lations of the teacher and student is used as the training objective:</p><formula xml:id="formula_4">L = 3 i=1 3 j=1 α ij L ij<label>(5)</label></formula><formula xml:id="formula_5">L ij = 1 A r |x| Ar a=1 |x| t=1 D KL (R T ij,l,a,t R S ij,m,a,t ) (6) R T ij,l,a = softmax( A T i,l,a A T j,l,a √ d r ) (7) R S ij,m,a = softmax( A S i,m,a A S j,m,a d r )<label>(8)</label></formula><p>where</p><formula xml:id="formula_6">A T i,l,a ∈ R |x|×dr and A S i,m,a ∈ R |x|×d r (i ∈ [1, 3]</formula><p>) are the queries, keys and values of a relation head of l-th teacher layer and m-th student layer. d r and d r are the relation head size of teacher and student models. R T ij,l ∈ R Ar×|x|×|x| is the selfattention relation of A T i,l and A T j,l of teacher model. R S ij,m ∈ R Ar×|x|×|x| is the self-attention relation of student model. For example, R T 11,l represents teacher's Q-Q relation in Figure <ref type="figure" target="#fig_0">1</ref>. A r is the number of relation heads. α ij ∈ {0, 1} is the weight assigned to each self-attention relation loss. We find that only using the query-query, key-key and value-value relations achieves competitive performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Layer Selection of Teacher Model</head><p>Besides the knowledge used for distillation, mapping function between teacher and student layers is also important. As in MINILM, we only transfer the self-attention knowledge of one of the teacher layers to the student last layer. Different from previous work which usually conducts experiments on base-size models, we experiment with different large-size teachers and find that transferring self-attention knowledge of an upper middle layer performs better than using other layers.</p><p>For BERT LARGE and BERT LARGE-WWM , transferring the 21-th (start at one) layer achieves the best performance. For RoBERTa LARGE , using the selfattention knowledge of 19-th layer achieves better performance. For the base-size model, experiments indicate that using teacher's last layer performs better than other layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct distillation experiments on different teacher models including BERT BASE , BERT LARGE , BERT LARGE-WWM and RoBERTa LARGE . We use multi-head query-query, key-key and value-value relations to perform KD training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>We  <ref type="bibr">and BookCorpus (Zhu et al., 2015)</ref>, and follow the preprocess and the WordPiece tokenization of <ref type="bibr" target="#b7">Devlin et al. (2018)</ref>. We train student models using 256 as the batch size and 6e-4 as the peak learning rate for 400, 000 steps. We use linear warmup over the first 4, 000 steps and linear decay. We use Adam <ref type="bibr" target="#b16">(Kingma and Ba, 2015)</ref> with β 1 = 0.9, β 2 = 0.999.  <ref type="formula">2019</ref>), which includes 160GB text corpora from English Wikipedia, Book-Corpus <ref type="bibr">(Zhu et al., 2015)</ref>, OpenWebText, CC-News <ref type="bibr" target="#b21">(Liu et al., 2019), and</ref><ref type="bibr">Stories (Trinh and</ref><ref type="bibr" target="#b35">Le, 2018)</ref>. We use the self-attention knowledge of teacher's 19-th layer for training the small models. For the 12×768 (base-size) student model, we use Adam with β 1 = 0.9, β 2 = 0.98. The rest hyperparameters are the same as models distilled from BERT. We conduct distillation experiments using 8 V100 GPUs with mixed precision training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Downstream Tasks</head><p>Following previous pre-training <ref type="bibr" target="#b7">(Devlin et al., 2018;</ref><ref type="bibr" target="#b21">Liu et al., 2019)</ref> and task-agnostic distillation <ref type="bibr" target="#b34">(Sun et al., 2019b;</ref><ref type="bibr" target="#b14">Jiao et al., 2019)</ref> work, we evaluate the models on GLUE benchmark and extractive question answering.</p><p>GLUE General Language Understanding Evaluation (GLUE) benchmark <ref type="bibr" target="#b39">(Wang et al., 2019)</ref> consists of two single-sentence classification tasks (SST-2 <ref type="bibr" target="#b31">(Socher et al., 2013)</ref> and CoLA <ref type="bibr" target="#b41">(Warstadt et al., 2018)</ref>), three similarity and paraphrase tasks <ref type="bibr">(MRPC (Dolan and Brockett, 2005)</ref>, STS-B <ref type="bibr">(Cer et al., 2017)</ref> and QQP), and four inference tasks (MNLI <ref type="bibr" target="#b42">(Williams et al., 2018)</ref>, QNLI <ref type="bibr" target="#b27">(Rajpurkar et al., 2016)</ref>, RTE <ref type="bibr" target="#b6">(Dagan et al., 2006;</ref><ref type="bibr" target="#b2">Bar-Haim et al., 2006;</ref><ref type="bibr" target="#b10">Giampiccolo et al., 2007;</ref><ref type="bibr">Bentivogli et al., 2009)</ref> and WNLI <ref type="bibr" target="#b18">(Levesque et al., 2012)</ref>). Following BERT <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref> Following MobileBERT <ref type="bibr" target="#b34">(Sun et al., 2019b)</ref>, the reported results are directly fine-tuned on downstream tasks. We compute the speedup of MobileBERT according to their reported latency. Table <ref type="table">4</ref>: Results of 12×768 MINILMv2 on the dev sets of the GLUE benchmark and SQuAD 2.0. The finetuning results are an average of 4 runs for each task. We report F1 for SQuAD 2.0, Pearson correlation for STS-B, Matthews correlation coefficient for CoLA and accuracy for the rest.</p><p>representation.</p><p>Extractive Question Answering The task aims to predict a continuous sub-span of the passage to answer the question. We evaluate on SQuAD 2.0 <ref type="bibr" target="#b26">(Rajpurkar et al., 2018)</ref>, which has been served as a major question answering benchmark. We pack the question and passage tokens together with special tokens to form the input: "[CLS] Q [SEP] P [SEP]". Two linear output layers are introduced to predict the probability of each token being the start and end positions of the answer span. The questions that do not have an answer are treated as having an answer span with start and end at the [CLS] token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results</head><p>Table <ref type="table" target="#tab_3">1</ref> presents the dev results of 6×384 and 6×768 models distilled from BERT BASE , BERT LARGE and RoBERTa LARGE on GLUE and SQuAD 2.0. (1) Previous methods <ref type="bibr" target="#b30">(Sanh et al., 2019;</ref><ref type="bibr" target="#b14">Jiao et al., 2019;</ref><ref type="bibr" target="#b33">Sun et al., 2019a;</ref><ref type="bibr" target="#b40">Wang et al., 2020)</ref>  MobileBERT compresses a specially designed teacher model (in the BERT LARGE size) with inverted bottleneck modules into a 24-layer student using the bottleneck modules. To compare with MobileBERT, we use a public large-size model (BERT LARGE-WWM ) as the teacher, which achieves similar performance as the teacher of Mobile-BERT. We distill BERT LARGE-WWM into a student model, which contains the same number of parameters (25M parameters, 12×384 with 128 embedding size), using the same training data (English Wikipedia and BookCorpus). The test results of GLUE benchmark and dev result of SQuAD 2.0 are illustrated in Table <ref type="table" target="#tab_7">3</ref>. MINILMv2 outperforms Mo-bileBERT across most tasks with a faster inference speed. Moreover, our method can be applied for different teachers and has much fewer restrictions of student models.</p><p>We compress RoBERTa LARGE and BERT LARGE into a base-size student model. Dev results of  For BERT BASE , using the last layer achieves better performance than other layers. For BERT LARGE , we find that using one of the upper middle layers achieves the best performance. The same trend is also observed for BERT LARGE-WWM and RoBERTa LARGE . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of different number of relation heads</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we present a simple and effective approach for compressing pretrained Transformers. We employ multi-head self-attention relations to train the student to deeply mimic the self-attention module of its teacher. Our method eliminates the restriction of the number of student's attention heads, which is required to be the same as its teacher for previous work transferring self-attention distributions. Moreover, we show that transferring the self-attention knowledge of an upper middle layer achieves better performance for large-size teacher models. Our student models distilled from BERT and RoBERTa obtain competitive performance on SQuAD 2.0 and the GLUE benchmark, and outperform state-of-the-art methods. For future work, we are exploring an automatic layer selection algorithm. We also would like to apply our method to larger pretrained Transformers.</p><p>Louisiana. Association for Computational Linguistics.</p><p>Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A GLUE Benchmark</head><p>The summary of datasets used for the General Language Understanding Evaluation (GLUE) benchmark 4 <ref type="bibr" target="#b39">(Wang et al., 2019)</ref> is presented in Table <ref type="table" target="#tab_12">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B SQuAD 2.0</head><p>We present the dataset statistics and metrics of SQuAD 2.0 5 <ref type="bibr" target="#b26">(Rajpurkar et al., 2018)</ref> in Table <ref type="table" target="#tab_13">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Hyper-parameters for Fine-tuning</head><p>Extractive Question Answering For SQuAD 2.0, the maximum sequence length is 384. The batch size is set to 32. We choose learning rates from {3e-5, 6e-5, 8e-5, 9e-5} and fine-tune the model for 3 epochs. The warmup ration and weight decay is 0.1 and 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GLUE</head><p>The maximum sequence length is 128 for the GLUE benchmark. We set batch size to 32, choose learning rates from {1e-5, 1.5e-5, 2e-5, 3e-5, 5e-5} and epochs from {3, 5, 10} for different student models. We fine-tune CoLA task with longer training steps (25 epochs). The warmup ration and weight decay is 0.1 and 0.01.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Overview of multi-head self-attention relation distillation. We introduce multi-head self-attention relations computed by scaled dot-product of pairs of queries, keys and values to guide the training of students. In order to obtain self-attention vectors (queries, keys and values) of multiple relation heads, we first concatenate self-attention vectors of different attention heads and then split them according to the desired number of relation heads. For large-size teacher, we transfer the self-attention knowledge of an upper middle layer of the teacher. For base-size teacher, using the last layer achieves better performance. Our student models are named as MINILMv2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Results of 6×384 model (12 attention heads, 12 relation heads) trained using different BERT BASE layers.</figDesc><graphic url="image-1.png" coords="7,96.09,62.81,170.08,127.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results of 6×384 model (6 attention heads, 16 relation heads) trained using different BERT LARGE layers.</figDesc><graphic url="image-2.png" coords="7,96.09,248.69,170.08,127.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>respectively. The self-attention distributions are computed via scaled dot-product of queries and keys. These weights are assigned to the corresponding value vectors to obtain the attention output. |x| represents the length of input sequence. A h and d h indicate the number of attention heads and hidden size. d k is the attention head size. d k × A h is usually equal to d h .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Self-Attention Vectors (An Upper Middle Layer)</head><label></label><figDesc></figDesc><table><row><cell cols="3">Transformer Block L</cell><cell></cell><cell></cell></row><row><cell cols="3">Transformer Block L-1</cell><cell>…</cell><cell cols="2">Transformer Block M</cell></row><row><cell></cell><cell>…</cell><cell></cell><cell>Queries (Q) (ℝ 𝑥 ×𝑑ℎ )</cell><cell></cell><cell>…</cell></row><row><cell cols="3">Transformer Block 4</cell><cell>…</cell><cell cols="2">Transformer Block 3</cell></row><row><cell cols="3">Transformer Block 3</cell><cell>Keys (K) (ℝ 𝑥 ×𝑑ℎ )</cell><cell cols="2">Transformer Block 2</cell></row><row><cell cols="3">Transformer Block 2</cell><cell>…</cell><cell cols="2">Transformer Block 1</cell></row><row><cell cols="3">Transformer Block 1</cell><cell>Values (V) (ℝ 𝑥 ×𝑑ℎ )</cell><cell>𝑥 1 𝑥 2</cell><cell>𝑥 3 𝑥 4</cell><cell>𝑥 5</cell></row><row><cell>𝑥 1 𝑥 2</cell><cell>𝑥 3 𝑥 4</cell><cell>𝑥 5</cell><cell></cell><cell></cell></row><row><cell cols="2">Teacher</cell><cell></cell><cell></cell><cell cols="2">Student</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Self-Attention Vectors (Last Layer) Self-Attention Relations</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Results of MINILMv2 distilled from base-size and large-size teachers on the development sets of GLUE and SQuAD 2.0. We report F1 for SQuAD 2.0, Matthews correlation coefficient for CoLA, and accuracy for other datasets. The GLUE results of DistilBERT are taken from<ref type="bibr" target="#b30">Sanh et al. (2019)</ref>. The rest results of Distil-BERT, TinyBERT 2 , BERT SMALL , Truncated BERT BASE and 6×768 MINILM are taken from<ref type="bibr" target="#b40">Wang et al. (2020)</ref>. BERT SMALL</figDesc><table><row><cell>Model</cell><cell>Teacher</cell><cell cols="9">#Param Speedup SQuAD2 MNLI-m QNLI QQP RTE SST MRPC CoLA Avg</cell></row><row><cell>BERT BASE</cell><cell>-</cell><cell>109M</cell><cell>×1.0</cell><cell>76.8</cell><cell>84.5</cell><cell>91.7</cell><cell>91.3 68.6 93.2</cell><cell>87.3</cell><cell>58.9</cell><cell>81.5</cell></row><row><cell>BERT SMALL</cell><cell>-</cell><cell>66M</cell><cell>×2.0</cell><cell>73.2</cell><cell>81.8</cell><cell>89.8</cell><cell>90.6 67.9 91.2</cell><cell>84.9</cell><cell>53.5</cell><cell>79.1</cell></row><row><cell>Truncated BERT BASE</cell><cell>-</cell><cell>66M</cell><cell>×2.0</cell><cell>69.9</cell><cell>81.2</cell><cell>87.9</cell><cell>90.4 65.5 90.8</cell><cell>82.7</cell><cell>41.4</cell><cell>76.2</cell></row><row><cell>Truncated RoBERTa BASE</cell><cell>-</cell><cell>81M</cell><cell>×2.0</cell><cell>77.9</cell><cell>84.9</cell><cell>91.1</cell><cell>91.3 67.9 92.9</cell><cell>87.5</cell><cell>55.2</cell><cell>81.1</cell></row><row><cell>DistilBERT</cell><cell>BERT BASE</cell><cell>66M</cell><cell>×2.0</cell><cell>70.7</cell><cell>82.2</cell><cell>89.2</cell><cell>88.5 59.9 91.3</cell><cell>87.5</cell><cell>51.3</cell><cell>77.6</cell></row><row><cell>TinyBERT</cell><cell>BERT BASE</cell><cell>66M</cell><cell>×2.0</cell><cell>73.1</cell><cell>83.5</cell><cell>90.5</cell><cell>90.6 72.2 91.6</cell><cell>88.4</cell><cell>42.8</cell><cell>79.1</cell></row><row><cell>6×768 MINILM</cell><cell>BERT BASE</cell><cell>66M</cell><cell>×2.0</cell><cell>76.4</cell><cell>84.0</cell><cell>91.0</cell><cell>91.0 71.5 92.0</cell><cell>88.4</cell><cell>49.2</cell><cell>80.4</cell></row><row><cell>6×384 MINILMv2</cell><cell>BERT BASE</cell><cell>22M</cell><cell>×5.3</cell><cell>72.9</cell><cell>82.8</cell><cell>90.3</cell><cell>90.6 68.9 91.3</cell><cell>86.6</cell><cell>41.8</cell><cell>78.2</cell></row><row><cell>6×384 MINILMv2</cell><cell>BERT LARGE</cell><cell>22M</cell><cell>×5.3</cell><cell>74.3</cell><cell>83.0</cell><cell>90.4</cell><cell>90.7 68.5 91.1</cell><cell>87.8</cell><cell>41.6</cell><cell>78.4</cell></row><row><cell>6×384 MINILMv2</cell><cell>RoBERTa LARGE</cell><cell>30M</cell><cell>×5.3</cell><cell>76.4</cell><cell>84.4</cell><cell>90.9</cell><cell>90.8 69.9 92.0</cell><cell>88.7</cell><cell>42.6</cell><cell>79.5</cell></row><row><cell>6×768 MINILMv2</cell><cell>BERT BASE</cell><cell>66M</cell><cell>×2.0</cell><cell>76.3</cell><cell>84.2</cell><cell>90.8</cell><cell>91.1 72.1 92.4</cell><cell>88.9</cell><cell>52.5</cell><cell>81.0</cell></row><row><cell>6×768 MINILMv2</cell><cell>BERT LARGE</cell><cell>66M</cell><cell>×2.0</cell><cell>77.7</cell><cell>85.0</cell><cell>91.4</cell><cell>91.1 73.0 92.5</cell><cell>88.9</cell><cell>53.9</cell><cell>81.7</cell></row><row><cell>6×768 MINILMv2</cell><cell>RoBERTa LARGE</cell><cell>81M</cell><cell>×2.0</cell><cell>81.6</cell><cell>87.0</cell><cell>92.7</cell><cell>91.4 78.7 94.5</cell><cell>90.4</cell><cell>54.0</cell><cell>83.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Results of 6×768 MINILMv2 distilled form BERT on GLUE test sets and SQuAD 2.0 dev set. The reported results are directly fine-tuned on downstream tasks. We report F1 for SQuAD 2.0, QQP and MRPC, Spearman correlation for STS-B, Matthews correlation coefficient for CoLA and accuracy for the rest.</figDesc><table><row><cell>Model</cell><cell>Teacher</cell><cell cols="9">#Param Speedup SQuAD2 MNLI-m/mm QNLI QQP RTE SST MRPC CoLA STS Avg</cell></row><row><cell>BERT BASE</cell><cell>-</cell><cell>109M</cell><cell>1.0×</cell><cell>76.8</cell><cell>84.6/83.4</cell><cell>90.5</cell><cell>71.2 66.4 93.5</cell><cell>88.9</cell><cell>52.1</cell><cell>85.8 79.3</cell></row><row><cell>BERT LARGE</cell><cell>-</cell><cell>340M</cell><cell>0.3×</cell><cell>81.9</cell><cell>86.7/85.9</cell><cell>92.7</cell><cell>72.1 70.1 94.9</cell><cell>89.3</cell><cell>60.5</cell><cell>86.5 82.1</cell></row><row><cell>MINILMv2</cell><cell>BERT BASE</cell><cell>66M</cell><cell>2.0×</cell><cell>76.3</cell><cell>83.8/83.3</cell><cell>90.2</cell><cell>70.9 69.2 92.9</cell><cell>89.1</cell><cell>46.6</cell><cell>84.3 78.7</cell></row><row><cell cols="2">MINILMv2 BERT LARGE</cell><cell>66M</cell><cell>2.0×</cell><cell>77.7</cell><cell>84.5/84.0</cell><cell>91.5</cell><cell>71.3 69.2 93.0</cell><cell>89.1</cell><cell>48.6</cell><cell>85.1 79.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>The maximum sequence length is set to 512. The dropout rate and weight decay are 0.1 and 0.01. The number of attention heads is 12 for all student models. For BERT LARGE and BERT LARGE-WWM , we use the self-attention knowledge of the 21-th layer to train the student model. The number of relation heads is 48 and 64 for basesize and large-size teacher model, respectively. The student models are initialized randomly.For RoBERTa LARGE , we use similar pre-training datasets as inLiu et al. (</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>, a taskspecific linear layer is added on top of the[CLS]       Comparison between MobileBERT and the same-size MINILMv2 (12 layers, 384 hidden size and 128 embedding size) distilled form BERT LARGE (Whole Word Masking) on GLUE test sets and SQuAD 2.0 dev set.</figDesc><table><row><cell>Model</cell><cell>Teacher</cell><cell cols="9">#Param Speedup SQuAD2 MNLI-m/mm QNLI QQP RTE SST MRPC CoLA STS Avg</cell></row><row><cell>BERT BASE</cell><cell>-</cell><cell>109M</cell><cell>1.0×</cell><cell>76.8</cell><cell>84.6/83.4</cell><cell>90.5</cell><cell>71.2 66.4 93.5</cell><cell>88.9</cell><cell>52.1</cell><cell>85.8 79.3</cell></row><row><cell>MobileBERT</cell><cell>IB-BERT LARGE</cell><cell>25M</cell><cell>1.8×</cell><cell>80.2</cell><cell>84.3/83.4</cell><cell>91.6</cell><cell>70.5 70.4 92.6</cell><cell>88.8</cell><cell>51.1</cell><cell>84.8 79.8</cell></row><row><cell>MINILMv2</cell><cell>BERT LARGE-WWM</cell><cell>25M</cell><cell>2.7×</cell><cell>80.7</cell><cell>85.9/84.6</cell><cell>91.9</cell><cell>71.4 71.9 93.3</cell><cell>89.2</cell><cell>44.9</cell><cell>85.5 79.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Table 5 shows the results of 6×384 model distilled Results of 6×384 model (12 attention heads) distilled from BERT BASE using different number of relation heads.from BERT BASE using different number of relation heads. Using a larger number of relation heads achieves better performance. More fine-grained self-attention knowledge can be captured by using more relation heads, which helps the student to deeply mimic the self-attention module of its teacher.</figDesc><table><row><cell>#Relation Heads</cell><cell>6</cell><cell>12</cell><cell>24</cell><cell>48</cell><cell>96</cell></row><row><cell>MNLI-m</cell><cell cols="5">81.9 82.2 82.2 82.4 82.3</cell></row><row><cell>SQuAD 2.0</cell><cell cols="5">71.9 72.8 72.7 73.0 72.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>2020. Bert-of-theseus: Compressing BERT by progressive module replacing. CoRR, abs/2002.02925. Sergey Zagoruyko and Nikos Komodakis. 2017. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19-27.</figDesc><table><row><cell>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-</cell></row><row><cell>bonell, Ruslan Salakhutdinov, and Quoc V Le. 2019.</cell></row><row><cell>XLNet: Generalized autoregressive pretraining for</cell></row><row><cell>language understanding. In 33rd Conference on</cell></row><row><cell>Neural Information Processing Systems (NeurIPS</cell></row><row><cell>2019).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Summary of the GLUE benchmark.</figDesc><table><row><cell cols="4">Corpus #Train #Dev #Test</cell><cell>Metrics</cell></row><row><cell cols="3">Single-Sentence Tasks</cell><cell></cell><cell></cell></row><row><cell>CoLA</cell><cell>8.5k</cell><cell>1k</cell><cell>1k</cell><cell>Matthews Corr</cell></row><row><cell>SST-2</cell><cell>67k</cell><cell>872</cell><cell>1.8k</cell><cell>Accuracy</cell></row><row><cell cols="4">Similarity and Paraphrase Tasks</cell><cell></cell></row><row><cell>QQP</cell><cell>364k</cell><cell>40k</cell><cell>391k</cell><cell>Accuracy/F1</cell></row><row><cell>MRPC</cell><cell>3.7k</cell><cell>408</cell><cell>1.7k</cell><cell>Accuracy/F1</cell></row><row><cell>STS-B</cell><cell>7k</cell><cell>1.5k</cell><cell cols="2">1.4k Pearson/Spearman Corr</cell></row><row><cell cols="2">Inference Tasks</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MNLI</cell><cell>393k</cell><cell>20k</cell><cell>20k</cell><cell>Accuracy</cell></row><row><cell>RTE</cell><cell>2.5k</cell><cell>276</cell><cell>3k</cell><cell>Accuracy</cell></row><row><cell>QNLI</cell><cell>105k</cell><cell>5.5k</cell><cell>5.5k</cell><cell>Accuracy</cell></row><row><cell>WNLI</cell><cell>634</cell><cell>71</cell><cell>146</cell><cell>Accuracy</cell></row></table><note>4 https://gluebenchmark.com/ 5 http://stanford-qa.com</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Dataset statistics and metrics of SQuAD 2.0.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Knowledge distillation from internal representations</title>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Aguilar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Guo</surname></persName>
		</author>
		<idno>CoRR, abs/1910.03723</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Unilmv2: Pseudo-masked language models for unified language model pre-training</title>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songhao</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12804</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The second PASCAL recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Roy</forename><surname>Bar-Haim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Hoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trang</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00055</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment. Luisa Bentivogli, Ido Dagan</title>
				<meeting>the Second PASCAL Challenges Workshop on Recognising Textual Entailment. Luisa Bentivogli, Ido Dagan</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2009. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Cross-lingual natural language generation via pre-training</title>
		<author>
			<persName><forename type="first">Zewen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
		<idno>CoRR, abs/1909.10481</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Infoxlm: An information-theoretic framework for cross-lingual language model pre-training</title>
		<author>
			<persName><forename type="first">Zewen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saksham</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno>CoRR, abs/2007.07834</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1911.02116</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<idno type="DOI">10.1007/11736790_9</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment, MLCW&apos;05</title>
				<meeting>the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment, MLCW&apos;05<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>CoRR, abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Paraphrasing (IWP2005)</title>
				<meeting>the Third International Workshop on Paraphrasing (IWP2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd Conference on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The third PASCAL recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</title>
				<meeting>the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing<address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR, abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynabert: Dynamic BERT with adaptive width and depth</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention-guided answer distillation for machine reading comprehension</title>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-31">2018. October 31 -November 4, 2018</date>
			<biblScope unit="page" from="2077" to="2086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Tinybert: Distilling BERT for natural language understanding</title>
		<author>
			<persName><forename type="first">Xiaoqi</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR, abs/1909.10351</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Daniel S Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10529</idno>
		<title level="m">Spanbert: Improving pre-training by representing and predicting spans</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
				<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Crosslingual language model pretraining</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The winograd schema challenge</title>
		<author>
			<persName><forename type="first">Hector</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019">Marjan Ghazvininejad,. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">BERT-EMD: many-to-many layer mapping for BERT compression with earth mover&apos;s distance</title>
		<author>
			<persName><forename type="first">Jianquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honghong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohong</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-16">2020. 2020. November 16-20, 2020</date>
			<biblScope unit="page" from="3009" to="3018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">RoBERTa: A robustly optimized BERT pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Xtremedistil: Multi-stage distillation for massive multilingual models</title>
		<author>
			<persName><forename type="first">Subhabrata</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Awadallah</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05">2020. July 5-10, 2020</date>
			<biblScope unit="page" from="2221" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Know what you don&apos;t know: Unanswerable questions for SQuAD</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018-07-15">2018. July 15-20. 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>ICLR 2015</idno>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
				<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Poor man&apos;s BERT: smaller and faster transformer models</title>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahim</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<idno>CoRR, abs/2004.03844</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno>CoRR, abs/1910.01108</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
				<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Mass: Masked sequence to sequence pre-training for language generation</title>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02450</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Patient knowledge distillation for BERT model compression</title>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-03">2019a. November 3-7, 2019</date>
			<biblScope unit="page" from="4322" to="4331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Mobilebert: Task-agnostic compression of bert by progressive knowledge transfer</title>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A simple method for commonsense reasoning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno>ArXiv, abs/1806.02847</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Small and practical BERT models for sequence labeling</title>
		<author>
			<persName><forename type="first">Henry</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Riesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amelia</forename><surname>Archer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-03">2019. November 3-7, 2019</date>
			<biblScope unit="page" from="3630" to="3634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Well-read students learn better: The impact of student initialization on knowledge distillation</title>
		<author>
			<persName><forename type="first">Iulia</forename><surname>Turc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>CoRR, abs/1908.08962</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers</title>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12471</idno>
		<title level="m">Neural network acceptability judgments</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
