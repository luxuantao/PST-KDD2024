<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fine-grained talking face generation with video reinterpretation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-09-12">12 September 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Xin</forename><surname>Huang</surname></persName>
							<email>xhuang@mun.ca</email>
							<idno type="ORCID">0000-0001-7113-5066</idno>
							<affiliation key="aff0">
								<orgName type="institution">Memorial University of Newfoundland</orgName>
								<address>
									<settlement>St. John&apos;s</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingjie</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Memorial University of Newfoundland</orgName>
								<address>
									<settlement>St. John&apos;s</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minglun</forename><surname>Gong</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Guelph</orgName>
								<address>
									<settlement>Guelph</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fine-grained talking face generation with video reinterpretation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-09-12">12 September 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1007/s00371-020-01982-7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Talking face</term>
					<term>Video generation</term>
					<term>Multi-purpose discriminators</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generating a talking face video from a given audio clip and an arbitrary face image has many applications in areas such as special visual effects and human-computer interactions. This is a challenging task, as it requires disentangling semantic information from both input audio clips and face image, then synthesizing novel animated facial image sequences from the combined semantic features. The desired output video should maintain both video realism and audio-lip motion consistency. To achieve these two objectives, we propose a coarse-to-fine tree-like architecture for synthesizing realistic talking face frames directly from audio clips. This is followed by a video-to-word regeneration module to translate the synthesized talking videos back to the words space, which is enforced to align with the input audios. With multi-level facial landmark attentions, the proposed audio-to-video-to-words framework can generate fine-grained talking face videos that are not only synchronous with the input audios but also maintain visual details from the input face images. Multi-purpose discriminators are also adopted for adversarial learning to further improve both image fidelity and semantic consistency. Extensive experiments on GRID and LRW datasets demonstrate the advantages of our framework over previous methods in terms of video quality and audio-video synchronization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatically generating talking face videos under different conditions, such as audio speech, text, and sketch, is a problem of interests in both computer vision and graphics. A talking face contains rich and complex semantic information, and humans are sensitive to subtle artifacts shown on faces. Hence, generating high-quality, audio-corresponding videos based on diverse conditions is a very difficult task. Although significant progress has been made in generating videos using temporal-dependency model <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b25">26]</ref>, realizing photo-realistic visual contents and optimizing generated videos by lip semantic alignment remain challenging.</p><p>The key issue is to learn the shared representation of two modalities (e.g., the given audio and an arbitrary image).</p><p>To achieve this, we explored coarse-to-fine learning module for generating fine-grained talking face video, as well as designed an end-to-end neural architecture built upon temporal-dependent GAN framework, which is conditioned on a face image, facial landmarks, and audio information. The results are then reinterpreted to semantic features and transformed to words information, which is expected to align with the ID of the word associated with the input audio (Fig. <ref type="figure">1</ref>).</p><p>Current still image generation algorithms based on generative adversarial network (GAN) have shown promising results for mapping from natural language feature space to image feature space. For example, Zhang et al. <ref type="bibr" target="#b42">[43]</ref> extend StackGAN <ref type="bibr" target="#b41">[42]</ref> into StackGAN++ <ref type="bibr" target="#b42">[43]</ref>, which uses a treelike structure to progressively generate images from small scale to large scale. Compared to still image generation, animating a still facial image to talking videos in a controllable way is far more challenging, due to the difficulties in bridging domain gap between audio and image sequences, as well as in eliminating artifacts between adjacent frames.</p><p>Most existing works on audio-to-talking face generation can be primarily categorized into two classes: temporalindependent methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">44]</ref> and temporal-dependent ones <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28]</ref>. For example, Chung et al. <ref type="bibr" target="#b4">[5]</ref> adopt an Fig. <ref type="figure">1</ref> Overall architecture of the presented network. Taking an image of the target face and an audio signal as inputs, the network first generates a target face video in a coarse-to-fine manner. The obtained video is then reinterpreted into a vision-audio feature space by a lip-reading model <ref type="bibr" target="#b26">[27]</ref>, which computes word probability distribution through a log-SoftMax layer. The word label with the highest probability is expected to align with the ground truth word that associates to the input audio encoder-decoder model to generate one image for every 0.35 s of the input audio. A common failure phenomenon of temporal-independent model is that the generated image sequences are not always smooth, causing obvious pixel jittering among frames. To address this issue, Song et al. <ref type="bibr" target="#b25">[26]</ref> incorporate valuable temporal information into the recurrent neural network (RNN), whereas Chen et al. <ref type="bibr" target="#b3">[4]</ref> divide the training into two successive steps: training an audio-conditioned face landmark generation model and a landmark-conditioned image generation model. This method establishes a bridge between audio and video by future landmarks prediction. Although it can generate temporally consistent frames in an audio-driven manner, it lacks direct association between input audio and final images. Since the output is constrained using predicted landmarks only and through a single-purpose regression discriminator, the output images can be blurry. Therefore, the lip movements can roughly match input video, but the individual frames lack sharp edges and vivid textures. This is because motion dynamic regions are guided by features which correspond to pixel intensity and the whole quality of each frame image is insufficiently constrained by mean absolute error, i.e., L 1 loss <ref type="bibr" target="#b36">[37]</ref> .</p><p>To tackle these problems, our proposed model adopts sketch-refinement and lip recognition steps to generate the photo-realistic talking face video, which precisely reflects the speech semantics. As illustrated in Fig. <ref type="figure">2</ref>, at each scale level, we adopt multi-purpose discriminators for adversarial learning. The discriminator for each image frame is conditioned on a clip inside the audio that matches to this frame to enhance the semantic association. Our discriminators are trained to distinguish the generated image frames Fig. <ref type="figure">2</ref> The schema of our coarse-to-fine adversarial network, which generates face frames with growing resolutions. Images at each resolution level are associated with a multi-purpose discriminator from the ground truth image frames based on the corresponding semantic audio clips. Furthermore, we apply multi-level attention mechanism in our model to weight image attribute features at different levels so that the generator is guided to emphasize informative phonic clips when generating various regions in each frame. For example, Fig. <ref type="figure">5</ref> shows that at low level, attention information focus on rough shapes such as pose, positions of facial features, etc., whereas at higher level, attention information is refined and details such as lip textures, skin wrinkles, and eyeballs become the focus.</p><p>Lip-reading process can be considered as an inverse problem of the audio-to-lip generation. Given a talking face video, the lip-reading network is performed to output words corresponding to the dynamic lip shapes of the input video. As shown in Fig. <ref type="figure">1</ref>, if a talking video generated by the audioto-video module is consistent with phonetic meaning of the input speech, the reinterpretation by the video-to-words module should output the highest probability for the word that is associated to the input speech. By integrating a lip-reading network, the generator is impelled to produce more precise mouth motion to further narrow the gap between generated results and the ground truth. Motivated by this observation, we propose a novel audio-to-video-to-words framework called AVWnet which exploits the idea of optimizing generation by re-description.</p><p>The contributions of our work can be summarized as follows: (1) establish a multi-level attentive generation network to generate fine-grained talking face, which is conditioned on the embedding of audio clips, example image and landmarks;</p><p>(2) the extended reinterpretation module re-encodes the synthesized video to align its semantic information with the input audio; and (3) multi-purpose discriminators are designed as part of our adversarial learning framework to consistently constrain the quality and semantic correlation of generated video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Talking face modelling Research on talking face modelling was first studied in 1990s <ref type="bibr" target="#b39">[40]</ref>, which establishes the map-ping between acoustic speech features and facial motions. Since then, various approaches have been proposed for audiodriven <ref type="bibr" target="#b11">[12]</ref>, video-driven <ref type="bibr" target="#b29">[30]</ref> or text-driven <ref type="bibr" target="#b33">[34]</ref> generations. Many traditional approaches on this topic are built upon hidden Markov models which reflect the dynamic features of audio and video sequences <ref type="bibr" target="#b0">[1]</ref>. Deng and Neumann <ref type="bibr" target="#b7">[8]</ref> combine target phoneme instances and expressive features to construct the best-matched motion path. Ma and Deng <ref type="bibr" target="#b16">[17]</ref> propose a model to generate speech animations through searching and concatenating best motions. Recently, methods based on deep learning have made great progress. Xie and Liu <ref type="bibr" target="#b37">[38]</ref> introduce a Bayesian network based model to synthesize a speaking mouth. Taylor et al. <ref type="bibr" target="#b28">[29]</ref> adapt a deep neural network for transferring phonetic context to a dynamic lower half of the face. Karras et al. <ref type="bibr" target="#b11">[12]</ref> present an end-to-end training network to learn the mapping between 3D meshes and raw speeches. Fan et al. <ref type="bibr" target="#b8">[9]</ref> generate the lower half region of the face by a bi-directional LSTM. Vougioukas et al. <ref type="bibr" target="#b32">[33]</ref> train a temporal GAN to generate talking faces from raw audios directly. Suwajanakorn et al. <ref type="bibr" target="#b27">[28]</ref> train a one-personbased model to generate the dynamic mouth region, which is then restitched to the original video. Video generation With the booming studies of high-level representation of images <ref type="bibr" target="#b10">[11]</ref>, researchers extend techniques for image synthesis to videos. Currently, generating videos based on different prerequisites has been extensively exploited. For example, how to predict video frames from previous frames has been studied in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. Motivated by the success of generative adversarial networks (GANs) <ref type="bibr" target="#b9">[10]</ref>, some researchers implement video generation based on adversarial learning <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32]</ref> using 3D convolutional layers. Tulyakov et al. <ref type="bibr" target="#b30">[31]</ref> decompose video features to motion and content via a recurrent neural network. Kim and Lee <ref type="bibr" target="#b12">[13]</ref> optimize multicontact character motions through key-pose planning and interpolation. Li et al. <ref type="bibr" target="#b13">[14]</ref> present a text-to-video generation based on variational autoencoders (VAE) and GANs. Suwajanakorn et al. <ref type="bibr" target="#b27">[28]</ref> learn lip landmarks from audios, then synthesize textures from lip landmarks, and finally merge lip textures into the original face. Attention models Attention mechanism has been a hotspot in many research communities ranging from computer vision <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b40">41]</ref> to natural language processing <ref type="bibr" target="#b14">[15]</ref>. Xu et al. <ref type="bibr" target="#b38">[39]</ref> apply the word-level attention to multi-level layers, which underlines different words in different subregions. Pumarola et al. <ref type="bibr" target="#b22">[23]</ref> and Chen et al. <ref type="bibr" target="#b3">[4]</ref> adapt the facial attention masks and base color features to generate the final RGB images. The attention mask determines how much the original image will contribute at each pixel location. We use a similar attention mechanism to discriminatively filter the audiovisual regions. Antitropic structure Our approach is also inspired by existing work on text-to-image generation <ref type="bibr" target="#b23">[24]</ref>, which re-describe the synthesized image using an image captioning model and aims to align the re-described text with the input text. Nevertheless, applying this idea to audio-to-video generation is a more challenging problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed fine-grained speech-to-face model</head><p>To infer the emotions and desired mouth motions of a talking face from a given audio, we need to find a mapping between audio features and face features. This problem can be considered as a conditional temporal GAN problem and solved by minimizing the distance between ground truth video distributions and generated ones <ref type="bibr" target="#b9">[10]</ref>. As shown in Fig. <ref type="figure">3</ref>, AVWnet consists of a multi-level attentional generative network and a reverse lip-reading network. The video generation process exploits the idea of Stackgan++ <ref type="bibr" target="#b42">[43]</ref>, with multi-stage generator and multiform discriminators arranged in a tree-like structure. Inspired by Chen et al. <ref type="bibr" target="#b3">[4]</ref>, we leverage pixel-wise landmark attentions in different stages of the generator. Once all face frames are generated, a reinterpretative module is cascaded to predict word IDs, which are constrained to be consistent with the input audio. In the subsections below, we first elaborate the video generation part, which is followed by explanation on the semantic word regeneration and alignment module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-level attention-based generative network</head><p>Inputs The generative network receives three kinds of inputs which are hybrid features extracted by the following branches.</p><p>Audio feature processing The inputs to audio encoders are mel-frequency cepstral coefficient (MFCC) values. Similar to <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b25">26]</ref>, for every audio sample, we use the last 12 coefficients out of original calculated 13 values. The sample rate of the audio signal is set to 16,000 and the successive window is set to 0.01 s, resulting in 28 time steps. Every input vector can be formulated as a 28 × 12 feature map, where each row embodies MFCC features at each time step. We select a consecutive time step audio sequence A = {A 0 , A 1 , . . . A t } that matches the selected video frames and feed them into a sequence of audio encoders. Each element A i represents an inner clip in this audio sequence. Every audio encoder outputs one audio clip feature:</p><formula xml:id="formula_0">a i = E a (A i ) for i = {0, 1, . . . t}.</formula><p>Landmark feature processing In the training and testing process, given a sequence of facial landmarks L = {L 0 , L 1 , . . . L t }, the landmark encoder E l generates latent landmark features l = {l 0 , l 1 , . . . l t }. We take the distance l − = {l − 0 , l − 1 , . . . l − t } between real landmark distribution l and the example landmark distribution l * = E l (L * ) as one of the condition values for the generative network, where l − = (l −l * ). Real landmark is generated face landmark from real image sequence, and example landmark is generated face Fig. <ref type="figure">3</ref> Overview of our proposed network structure, which consists of a multi-attentional video generation module for talking face video generation and a reinterpretative module for lip-reading from the synthesized video Fig. <ref type="figure">4</ref> Multi-level landmark attention mechanism. The subfigure at topright corner shows how the landmark features ( f attn0 and f attn1 ) are used to blend between input image features (i 32 and i 64 ) and synthesized hidden-layer feature maps (v 0 and v 1 ). Different components in the subfigure are color-coded, where the colors match the corresponding layers in the network landmark from the example image. In the application process, given a random face image and a speech signal, we first employ the landmark generation model in <ref type="bibr" target="#b3">[4]</ref> to get the sequence of predicted landmarks instead of real landmarks, and then take the audio signal, example image, and divergent landmarks as input conditions.</p><p>Image feature processing To realize arbitrary face video generation, our proposed conditional temporal GAN framework needs to be conditioned on image features, so that the facial features and audio information can be further fused together. An example image I * ∈ I ≡ {I 0 , I 1 , . . . I t } is randomly chosen from the source frames and transformed into latent variables i * through an image encoder, i.e., i * = E i (I * ). Outputs We concatenate the audio features a = {a 0 , a 1 , . . . a t }, example image feature i, and landmark difference l − = {l − 0 , l − 1 , . . . , l − t } together, forming a hybrid feature h = {h 0 , h 1 , . . . , h t } as the condition, which is performed by:</p><formula xml:id="formula_1">h = E a (A) ⊕ (E l (L) − E l (L * )) (1) h = E i (I * ) ⊕ h (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where ⊕ is concatenation operation. The network attempts to generate talking face frames </p><formula xml:id="formula_3">Ĩ 0 = { Ĩ 0 0 , Ĩ 0 1 , . . . Ĩ 0 t } with size of 64 × 64 in the first stage, Ĩ 1 = { Ĩ 1 0 , Ĩ 1 1 , . . . Ĩ 1 t }</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-level landmark attentions</head><p>In most of the existing works, single-level-based attention is widely utilized to recalibrate the features <ref type="bibr" target="#b3">[4]</ref>, which usually limits the ability to capture more informative representations for coarse-to-fine process. To overcome this limitation and further reduce jitters between adjacent frames, we design a multi-level attention mechanism, which is imposed on the internal multi-scale features; see Fig. <ref type="figure">4</ref>. The coarse-to-fine attention mechanism helps to precisely rescale and emphasize the dynamic areas that encode speech-related face motions, thus helping produce more sophisticated multi-level features in a fine-grained fashion. Specifically, to calculate the latent attention layers f 0 and f 1 , we integrate landmarks features ( f attn0 , f attn0 ), hidden-layer features (i 32 , i 64 ) of example image, and features v 0 decoded using a Convolutional Recurrent Neural Network (Conv-RNN) along with an upsample process. The overall attention mechanism can be formulated as:</p><formula xml:id="formula_4">f attn0 = σ (H d0 (E l (L * ) ⊕ E l (L))) f attn1 = σ H d1 (E l (L * ) ⊕ E l (L)) v 0 = H D (H cov-rnn (h)) f 0 = ( f attn0 v 0 ) + (1 − f attn0 ) i 32 b 64 = H dup1 ( f 0 ) (3) v 1 = H res (b 64 ) f 1 = ( f attn1 v 1 ) + (1 − f attn1 ) i 64 b 128 = H dup2 ( f 1 )<label>(4)</label></formula><p>where is multiplication operation. H d0 is landmark decoder which can decode the concatenated landmark values to attention feature maps with size of N l × 32 × 32, whereas H d1 produces attention feature maps with size of N l /2×64×64. H conv-rnn is a Conv-RNN network, which generates temporal sequential vectors from the input sequence. Through a deconvolutional operation H D , we can get a latent feature maps v 0 for future generation purpose. After upsample transaction H dup and residual network H res , we obtain the final feature results b 64 and b 128 , which will be used to generate realistic image frames Ĩ 0 of 64 × 64 resolution and Ĩ 1 of 128×128 resolution, respectively. The implementation process is as follows:</p><formula xml:id="formula_5">Ĩ 0 i = f 0 gray f 0 color + (1 − f 0 gray ) I 0 * Ĩ 1 i = f 1 gray f 1 color + (1 − f 1 gray ) I 1 * (5)</formula><p>where the attention mask f gray is one-channel activated convolutional result of feature b and the color content f color is three-channel activated convolutional result of b. Based on the above computations, the model produces image frames that not only retain semantically irrelevant information from the given example face image, but also generate new semantically consistent information according to the regions in the color content, which acts on the positive part of the grayscale attention mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semantic video reinterpretation</head><p>As described above, our proposed model includes a semantic video reinterpretation module, which maps generated image frames to the word space. We here adopt a popular lip-reading framework <ref type="bibr" target="#b26">[27]</ref> for its simplicity. Other more advanced lipreading models <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> can also be integrated in our network and potentially yield better results.</p><p>Relying on a 3D spatiotemporal convolutional network and a Bi-LSTM, the audiovisual speech recognition process is as follows:</p><formula xml:id="formula_6">m I = H res (H STC ( Ĩ )) (6) p = σ (H lstm (m I ) ⊕ H re_lstm (m I )) (7)</formula><p>where the input Ĩ is the frame sequences synthesized by our proposed generative network, H STC is the spatiotemporal convolutional network and H res is a residual block. H lstm and H re_lstm are combined to a two-layer Bi-LSTM network. A following linear layer has been omitted for clarity. σ is a log-SoftMax applied in the last layer, which realizes word-level recognition by producing words probability distribution p.</p><p>To help AVWnet achieve a more stable training process and converge faster, we pre-train this model rather than jointly optimizing it with generative network. The parameters in the pre-trained model are fixed during training of the generative model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-purpose adversarial losses</head><p>To produce videos with increasing resolution, hierarchical adversarial losses are associated with the tree-like structure generator, which plays a dominant role in the performance of our AVWnet. Apart from the conventional single L1/L2 loss, other loss functions should also be incorporated as constraints on the semantic consistency and to boost the generation quality. Specifically, we first transform the matching-aware pair loss <ref type="bibr" target="#b42">[43]</ref> to improve the semantic consistency into the video generation problem. The pixel loss L pi x imposed on mouth region by the mean absolute error and regression-based loss L R <ref type="bibr" target="#b3">[4]</ref> are also combined together to enforce the high-quality generation of mouth region and structural consistency. Note that, to learn coarse-to-fine consistent features at multiple layers, all of these loss terms are independently computed at different generative stages.</p><p>The generator G and discriminator D are trained alternately at each stage of AVWnet. G i is the ith stage of generative network, and it has a corresponding discriminator D i . The discriminator D i takes frame-phonetic clip pairs as their inputs and jointly approximates conditional and unconditional distributions. Discriminator D i has two training objectives: to distinguish whether the input video is real or fake and to classify whether an video-audio fragment condition pair matches or not <ref type="bibr" target="#b42">[43]</ref>. Our discriminator loss function is a combination of conditional loss and unconditional loss. We feed the discriminator with five types of input: I j : the jth frame of real video; Î i j : the jth frame of generated video in the ith stage; (I i j , A j ): real jth video frame with matching jth clip of input audio in the ith stage. ( Î i j , A j ): generated jth video frame with matching jth clip of audio in the ith stage. (I i j , Â j+1 ): real jth video frame with mismatching ( j + 1)th clip of audio in the ith stage.</p><p>The cross-entropy loss for D i function is defined by:</p><formula xml:id="formula_7">L Di = −1/2E I i j ∼ pdatai log D i I i j − 1 2 E Î i j ∼ pGi log 1 − D i Î i j unconditional-loss + −1/3E I i j ∼ pdatai log D i I i j , A j − 1 3 E Î i j ∼ pGi log 1 − D i Î i j , A j conditional-loss − 1 3 E I i j ∼ pdatai log 1 − D i I i j , Â j+1 conditional-loss<label>(8)</label></formula><p>Fig. <ref type="figure">5</ref> Attention masks generated for the coarse (top) and fine (middle) levels and the resulted image frames (bottom). The image resolution is 64 × 64 pixels for f 0 gray and 128 × 128 for f 1 gray . The green boxes highlight that fine-level masks are sharper and more precise around the mouth area, whereas the yellow boxes demonstrate that fine-level masks learned in a later stage contain more detailed textures By combining the cross-entropy loss L D i and the regression loss L R i , the final loss L D for the conjoint discriminators can be formulated as:</p><formula xml:id="formula_8">L D = L D i + λL R i (<label>9</label></formula><formula xml:id="formula_9">)</formula><p>where λ is a hyperparameter to balance the two terms.</p><p>Our generator loss function is a contextual loss, which consists of conditional and unconditional losses. The ith training stage G i is trained by minimizing the loss as follows: <ref type="bibr" target="#b9">(10)</ref> We further utilize an aggregated per time step loss to align the re-interpretated words and the given audios. Mathematically, this loss is defined as:</p><formula xml:id="formula_10">L G i = −1/2E Î i j ∼ pG i log D i Î i j unconditional-loss − 1/2E Î i j ∼ pG i log D i Î i j , A j conditional-loss</formula><formula xml:id="formula_11">L w ( p, C t arg et ) = − t k=0 p[C t arg et ] (<label>11</label></formula><formula xml:id="formula_12">)</formula><p>where p is words probability distribution after the log-SoftMax operation (Eq. ( <ref type="formula">7</ref>)). To generate realistic videos with phonetic conditions, the final objective function of the AVWnet is weighted summation of generator loss, audio reconstruction loss L w , and mouth region pixel loss L pi x :</p><formula xml:id="formula_13">L = L G + ∂L w + βL pi x (<label>12</label></formula><formula xml:id="formula_14">)</formula><p>where ∂ and β are hyperparameters to balance the three terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>Datasets We evaluate the proposed method on two widely used datasets: LRW <ref type="bibr" target="#b5">[6]</ref> contains 500 classes of words spoken by hundred people. In each word class, there are 1000 training video samples, 50 test samples, and 50 validation samples. GRID <ref type="bibr" target="#b6">[7]</ref> contains 33 speakers, each uttering 1000 short phrase. We extract image frames with a sampling rate of 25 FPS, which leads to 31 frames for each LRW video and 75 frames for each GRID video. All face images are cropped to 128 × 128 by aligning to the landmarks. We train on ground truth landmarks that generated by 2D face alignment library <ref type="bibr" target="#b1">[2]</ref> and test under generated example landmark and future landmark sequences predicted by a pre-trained landmark-prediction model <ref type="bibr" target="#b3">[4]</ref>. For audio signal, we transform videos to raw audio format first and then extract MFCC features with window size being 0.01s. Moreover, we pretrain the lip-reading model <ref type="bibr" target="#b26">[27]</ref> based on image frames with size 128 × 128, which are cropped in the same way with our generative model. Evaluation metrics Our proposed approach is evaluated both qualitatively and quantitatively. The quantitative measures we used are: Structural Similarity Index (SSIM) <ref type="bibr" target="#b34">[35]</ref>, Peak Signal-to-Noise Ratio (PSNR), and Landmark Distance Error (LMD) <ref type="bibr" target="#b2">[3]</ref>. SSIM and PSNR aim to assess the quality of our generated videos. LMD calculates the distance between landmarks in generated video and ground truth video, allowing it to evaluate audio-video semantic consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation details</head><p>Basics Our model is implemented on PyTorch and tested on a single Nvidia GeForce GTX 1080 Ti GPU with 50GiB memory. We apply the batch normalization, set a fixed learning rate of 0.0002, and use Adam algorithm as the optimizer.</p><p>Architecture details As illustrated in Fig. <ref type="figure">3</ref>, our generator consists of multiple threads encoders and temporal two-stage decoders with residual connections. The example image encoder transforms input image to a 4N i × 8 × 8 tensor by five convolutional (Conv) layers with residual connections and padding = 1 after the second Conv layer, where N i is the depth of latent image feature maps. The residual block and the third Conv layer produce feature maps with shapes of N i /2 × 64 × 64 and N i × 32 × 32, respectively. These feature maps will be used in the decoders to constrain the final video generation. The audio encoder is decomposed to same 16-stream encoders to manage selected 16 audio segments that correspond to 16 frames of the video. Each audio encoder involves two stages: The first stage consists of five standard Conv layers (kernel size = 3 × 3, stride = 1, padding = 1) and two maxpool layers, which are placed after the 2nd and 5th Conv layers to increase the scale invariance and nonlinearity of the  Similarly, the landmark encoder consists of the same 16stream landmark frame encoders and one example landmark encoder. Each landmark encoder has a linear layer with ReLu activation and a Conv layer, producing landmark features with size N l × 8 × 8, where N l = 2N i . The subtraction result between example landmark features and frame landmark features is concatenated with the audio clip features, forming a hybrid condition for the generator. Associating the condition with example image features, the multiple encoders produce 16-stream inputs for a Conv-RNN layer.</p><p>The Conv-RNN layer predicts 16 frame feature maps, and each frame feature map is fed into 16 identical branched decoders. Each decoder can be divided into four sections.</p><p>Fig. <ref type="figure">8</ref> The detailed comparison between Chen et al. <ref type="bibr" target="#b3">[4]</ref> (top) and our approach (bottom). Green boxes highlight the areas that AVWnet generates more detailed and realistic textures, whereas Chen et al.'s approach yields artifacts since the attention masks are not accurately learned. Yellow boxes highlight the mouth region that AVWnet produces sharper details. In contrast, the mouth region produced by Chen et al. <ref type="bibr" target="#b3">[4]</ref> are blurry</p><p>The initial section consists of a four-layer residual block and transposed convolution layers to improve the resolution of images, producing feature maps with size N i × 32 × 32. The second section aims to generate image frames with scale 64 × 64. It has a residual block and an upsample operation, producing feature maps with size N i /2 × 64 × 64. The third section is a pure residual block containing tensors that have the same size with preceding maps. The last layers of the first section and the third section accept multi-level attentions as discussed in Sect. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and comparison</head><p>Qualitative results As illustrated in Figs. <ref type="figure" target="#fig_2">6, 7</ref>, and 8, we qualitatively compare the results generated by the presented AVWnet with those of two state-of-the-art approaches: Chen et al. <ref type="bibr" target="#b3">[4]</ref> and Chung et al. <ref type="bibr" target="#b4">[5]</ref>. All three methods are trained on LRW dataset and output videos with 128×128 resolution. Chung et al. <ref type="bibr" target="#b4">[5]</ref> adopt a pre-trained VGG-M model on the VGG face dataset <ref type="bibr" target="#b19">[20]</ref>.</p><p>Figure <ref type="figure">5</ref> displays the attention masks obtained for both coarse and fine levels. The fine-level attention masks contain more detailed textures on skin winkles, eye details, and lip edges. As a result, the subsequent generation stage enriches the details in the result frames.</p><p>Figure <ref type="figure" target="#fig_1">6</ref> shows the outputs generated by AVWnet on different input faces. The input audio reads "what happened to her," which is randomly selected from the testing set of LRW dataset. We show inner 18 frames out of 31 generated frames. Despite the wide variety among input faces, AVWnet effectively synthesizes the lip movements that are synchronized to the input audio and nicely matched to the mouth shapes in ground truth video frames.</p><p>Figure <ref type="figure" target="#fig_2">7</ref> compares the results generated by AVWnet with Chen et al. <ref type="bibr" target="#b3">[4]</ref> and Chung et al. <ref type="bibr" target="#b4">[5]</ref>. Both an identity face image from the ground truth and a cartoon figure from the Internet are used for testing. Visual inspection over different frames demonstrates that our model produces sharper talking faces with discriminative lips, teeth, eyes, and detailed skin textures compared with the other two methods (e.g., frames in yellow boxes). The mouth motions are also better matched with the ground truth, which means our outputs can better reflect the input audio (e.g., frames in red boxes). The outputs of existing approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, on the other hand, are more blurry and have less noticeable lip movements. Figure <ref type="figure">8</ref> further compares our work with Chen et al. <ref type="bibr" target="#b3">[4]</ref> on two cases where their results contain blurry regions and obvious artifacts (e.g., spots on the face), whereas our approach outputs sharp and artifact-free images. Quantitative results Using the aforementioned evaluation metrics, we quantitatively compare the performance of our AVWnet with state-of-the-art approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> on two datasets: LRW and GRID. The values of SSIM and PSNR reflect the visual frame quality. The higher the score, the better the visual results are. LMD is the distance between the landmarks of generated frames and the ground truth landmarks, which mediately indicates the semantic-visual consistency. The lower the LMD, the better audio-video synchronization is. For fair comparison, different approaches are evaluated under the same test algorithm and our customized test datasets. Table <ref type="table" target="#tab_0">1</ref> provides the evaluation results, which shows that AVWnet outperforms both existing approaches on all three metrics and for both datasets.</p><p>We also compare the computation time needed by AVWnet and approach <ref type="bibr" target="#b3">[4]</ref> for generating each frame under the same parameter settings and hardware environment (one GeForce GTX 1080 Ti Graphics Card). We apply both methods to produce the same talking video with 31 frames, respectively. Method <ref type="bibr" target="#b3">[4]</ref> can generate one frame in 0.018 s, whereas our method takes 0.025 s. However, as our model involves extra computation of multi-level landmark attention and multi-level discriminators to improve the video quality and accuracy, the slight increase in computational cost is acceptable. To make the survey result more robust, every pair of videos is shown to each participant twice. The participants are asked to score between 0 (worst) and 5 (best) with interval of 0.1 on three aspects: the consistency between lip movements in the videos and input audio (synchronization), the smoothness of the overall video (video smoothness), and the fidelity of each image frame (image quality). The average scores of the two methods on the three aspects are then computed; see Fig. <ref type="figure">9a</ref>. The comparison shows that AVWnet outperforms Chen's method in terms of synchronization and image quality, whereas the video smoothness scores for the two methods are comparable. Additionally, we count the three possible evaluation outcomes for each pair of videos generated by the two methods: AVWnet gets higher scores, Chen et al.'s method gets higher scores, and both methods get equal scores. Figure <ref type="figure">9b</ref> shows the percentages of each outcome on the three aspects. In terms of the image quality, 69% users gave higher scores to AVWnet, compared to 19% gave higher ones to Chen et al.'s model. In terms of synchronization, 56% scored higher for AVWnet and 28% scored higher for Chen et al. <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation studies</head><p>To study the effects of different components and learn the contributions of different losses in our method, we perform an ablation study on the LRW dataset. We investigate three components: Attention Maps f attn0 and f attn1 (AM 0 and AM 1 ), Reinterpretative Module (RIM), and three loss functions (Sect. 3): L D 0 , L D 1 , L R in this section. Table <ref type="table" target="#tab_1">2</ref> shows Fig. <ref type="figure">9</ref> User study on videos generated using the proposed AVWnet and the state-of-the-art method <ref type="bibr" target="#b3">[4]</ref>, which shows AVWnet outperforms <ref type="bibr" target="#b3">[4]</ref> in synchronization and image quality The performances of the algorithm after removing different components are evaluated. Scores with obvious changes when compared with the AVWnet are shown in boldface the comparison results by removing each element at a time.</p><p>To accelerate the process, we simplify the LRW datasets from 500 words to 24 words and test these variant models on these 24 words only. For the same reason, the ground truth facial landmarks, instead of the predicted ones, are used, which leads to higher evaluation scores in some cases. Table <ref type="table" target="#tab_1">2</ref> confirms that the best performance is achieved when all components are used. Specifically, removing the twofold temporal-spatial adversial loss L D 0 and L D 1 leads to noticeable drops in SSIM and PSNR measures, which indicates that L D i is crucial for visual quality. Similarly, removing the RIM and AM causes the LMD values to increase, which means the RIM and multi-attention mechanism are important for lip synchronization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>A fine-grained audio-to-video-to-words network (AVWnet) is presented in this paper for efficient audio-to-video talking face synthesis. Compared to previous models, our AVWnet can generate talking face videos with better audio-lip consistency and higher frame quality. This is achieved because the generative network in AVWnet jointly approximates multiscale conditional and unconditional video distributions, and gradually produces videos in a coarse-to-fine manner. In order to further improve the audio-lip consistency, a reinterpretation module is used to supervise the generator by remapping the generated video to words and enforcing the reconstruction loss. The advantages of our method over existing state-of-the-art methods are demonstrated through both qualitative and quantitative evaluations on two classic datasets, as well as user studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compliance with ethical standards</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>with size of 128 × 128 in the 2nd stage, so that the conditioned result distributions of Ĩ and ground truth frames I are as close as possible, i.e., p( Ĩ |A, L , I * ) ≈ p(I |A, L , I * ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 6</head><label>6</label><figDesc>Fig.<ref type="bibr" target="#b5">6</ref> Video frames (128 × 128 pixels) generated using the same audio clip but different face images sampled from synthetic images, real-world images, celeA, LRW datasets and cartoon characters, respectively. Top</figDesc><graphic url="image-7.png" coords="7,53.65,56.48,487.69,159.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 7</head><label>7</label><figDesc>Fig. 7 Comparisons of videos frames generated by AVWnet, Chen et al. [4] and Chung et al. [5] based on two sets of input audios and faces. The first comparison uses the same face from the ground truth for direct comparison with ground truth frames, whereas the second comparison is tested on a cartoon character shown on the left. Red boxes highlight</figDesc><graphic url="image-8.png" coords="7,53.65,278.30,487.69,243.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 . 1 .</head><label>31</label><figDesc>The last section further improves the image resolution to 128 × 128 by a residual block and an upsampling operation. Our discriminator D involves three parts. The first part contains four 3 × 3 Conv layers with BatchNorms and LeakyReLU activationtrans, which transforms 3×128×128 image frames to 8N d ×16×16 feature maps, where N d = 64. The second part produces 16N d × 4 × 4 maps using downsampling blocks. The third part has a Conv layer of kernel size 3 × 3, stride = 1, padding = 1 with a BatchNorm and LeakyReLU activation, generating a tensor with size 16N d × 4 × 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Quantitative evaluation on LRW and GRID testing datasets Section 4.2 evaluates all synthesized frames individually and hence may not be sufficient for evaluating the whole video. To address this limitation, we conduct a user study between our model and Chen et al.<ref type="bibr" target="#b3">[4]</ref>, which has better performance than Chung et al.<ref type="bibr" target="#b4">[5]</ref>. We randomly select 16 example faces from LRW, CeleA, GRID, and cartoon characters to generate 16 talking videos for each method, which are shuffled before being shown to participants. Same as Chen et al [cite], we use 10 participants. The group that we picked has equal numbers of males and females and involves both undergraduate and graduate students between the ages of 20 and 40.</figDesc><table><row><cell>Method</cell><cell>LRW</cell><cell></cell><cell></cell><cell>GRID</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="6">SSIM PSNR LMD SSIM PSNR LMD</cell></row><row><cell>hung [5]</cell><cell>0.71</cell><cell>28.31</cell><cell>3.19</cell><cell>0.74</cell><cell>28.46</cell><cell>3.03</cell></row><row><cell>Chen [4]</cell><cell>0.75</cell><cell>30.04</cell><cell>2.97</cell><cell>0.77</cell><cell>31.61</cell><cell>2.88</cell></row><row><cell cols="2">Our AVWnet 0.82</cell><cell>31.24</cell><cell>2.84</cell><cell>0.84</cell><cell>32.03</cell><cell>2.79</cell></row><row><cell cols="3">Best scores are shown in boldface</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">4.3 User studies</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Ablation studies on LRW</figDesc><table><row><cell>LRW</cell><cell>AVWnet</cell><cell cols="2">Variant without component</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>AM 0 AM 1</cell><cell>L D0 L D1</cell><cell>L R</cell><cell>RIM</cell></row><row><cell>SSIM</cell><cell>0.839</cell><cell>0.831</cell><cell>0.796</cell><cell>0.836</cell><cell>0.839</cell></row><row><cell>PSNR</cell><cell>32.425</cell><cell>32.418</cell><cell>31.120</cell><cell>32.421</cell><cell>32.423</cell></row><row><cell>LMD</cell><cell>2.775</cell><cell>2.820</cell><cell>2.784</cell><cell>2.801</cell><cell>2.802</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of interest</head><p>To the best of our knowledge, the named authors have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Video rewrite: driving visual speech with audio</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Covell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Slaney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem?(and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1021" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lip movements generation at a glance</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="520" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical crossmodal talking face generation with dynamic pixel-wise loss</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7832" to="7841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">You said that</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jamaludin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02966</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
				<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="87" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An audio-visual corpus for speech perception and automatic speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2421" to="2424" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Expressive speech animation synthesis with phoneme-level controls</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2096" to="2113" />
			<date type="published" when="2008">2008</date>
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A deep bidirectional LSTM approach for video-realistic talking head</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">K</forename><surname>Soong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimed. Tools Appl</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5287" to="5309" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hierarchically-fused generative adversarial network for text to realistic image synthesis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th Conference on Computer and Robot Vision (CRV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Audio-driven facial animation by joint end-to-end learning of pose and emotion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">94</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Keyframe-based multi-contact motion synthesis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Comput</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Video generation from text</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Da-gan: instance-level image translation by deep attention generative adversarial networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wen Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5657" to="5666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A statistical quality model for data-driven speech animation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visual Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1915" to="1927" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05440</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Action-conditional video prediction using deep networks in atari games</title>
		<author>
			<persName><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2863" to="2871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-to-end visual speech recognition with LSTMS</title>
		<author>
			<persName><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2592" to="2596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end audiovisual speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="6548" to="6552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ganimation: anatomically-aware facial animation from a single image</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pumarola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanfeliu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mirrorgan: learning text-toimage generation by redescription</title>
		<author>
			<persName><forename type="first">T</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1505" to="1514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Temporal generative adversarial nets with singular value clipping</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2830" to="2839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Talking face generation by conditional recurrent adversarial network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04786</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Combining residual networks with LSTMS for lipreading</title>
		<author>
			<persName><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04105</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Synthesizing Obama: learning lip sync from audio</title>
		<author>
			<persName><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">95</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A deep learning approach for generalized speech animation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krahe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Face2face: real-time face capture and reenactment of RGB videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2387" to="2395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mocogan: decomposing motion and content for video generation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1526" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="613" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">End-to-end speech-driven realistic facial animation with temporal GANs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Vougioukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="37" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Photo-realistic expressive text to talking head synthesis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blokland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Braunschweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kolluru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Latorre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Maia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yanagisawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>INTERSPEECH</publisher>
			<biblScope unit="page" from="2667" to="2669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">X2face: a network for controlling face generation using images, audio, and pose codes</title>
		<author>
			<persName><forename type="first">O</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sophia Koepke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="670" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Advantages of the mean absolute error (MAE) over the root mean square error (RMSE) in assessing average model performance</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Willmott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Matsuura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clim. Res</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="82" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Realistic mouth-synching for speech-driven talking face using articulatory modelling</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimed</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="500" to="510" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attngan: fine-grained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1316" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Quantitative association of vocal-tract and facial behavior</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yehia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vatikiotis-Bateson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="23" to="43" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Selfattention generative adversarial networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Stackgan: text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Stackgan++: realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1947" to="1962" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Talking face generation by adversarially disentangled audio-visual representation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf</title>
				<meeting>AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9299" to="9306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Xin Huang is a Ph.D. student within the Computer Science program at Memorial University of Newfoundland, Canada. She received the B.E degree and M.Sc. degree in Information and Electrical Engineering from China University of Mining and</title>
		<author>
			<persName><forename type="first">Xuzhou</forename><surname>Technology</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>China</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Hangzhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>China</surname></persName>
		</author>
		<author>
			<persName><surname>China</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014, and the M.Sc. degree from Tianjin University</title>
				<meeting><address><addrLine>Tianjin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>from Zhejiang Sci-Tech University, ; School of Computer Science, University of Guelph</orgName>
		</respStmt>
	</monogr>
	<note>He is currently pursing the Ph.D. degree from Memorial University of Newfoundland, Canada. His research interests include image processing, computer vision and deep learning. Minglun Gong is a full. He obtained his Ph.D. from the University of Alberta in 2003. His research interests include visual computing, image processing and pattern recognition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
