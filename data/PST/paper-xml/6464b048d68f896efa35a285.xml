<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Masked Auto-Encoders Meet Generative Adversarial Networks and Beyond</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhengcong</forename><surname>Fei</surname></persName>
							<email>feizhengcong@meituan.com</email>
						</author>
						<author>
							<persName><forename type="first">Mingyuan</forename><surname>Fan</surname></persName>
							<email>fanmingyuan@meituan.com</email>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Junshi</forename><surname>Huang</surname></persName>
							<email>huangjunshi@meituan.com</email>
						</author>
						<author>
							<persName><roleName>Xiaolin</roleName><forename type="first">Xiaoming</forename><surname>Wei</surname></persName>
							<email>weixiaoming@meituan.com</email>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Meituan</surname></persName>
						</author>
						<title level="a" type="main">Masked Auto-Encoders Meet Generative Adversarial Networks and Beyond</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Masked Auto-Encoder (MAE) pretraining methods randomly mask image patches and then train a vision Transformer to reconstruct the original pixels based on the unmasked patches. While they demonstrates impressive performance for downstream vision tasks, it generally requires a large amount of training resource. In this paper, we introduce a novel Generative Adversarial Networks alike framework, referred to as GAN-MAE, where a generator is used to generate the masked patches according to the remaining visible patches, and a discriminator is employed to predict whether the patch is synthesized by the generator. We believe this capacity of distinguishing whether the image patch is predicted or original is benefit to representation learning. Another key point lies in that the parameters of the vision Transformer backbone in the generator and discriminator are shared. Extensive experiments demonstrate that adversarial training of GAN-MAE framework is more efficient and accordingly outperforms the standard MAE given the same model size, training data, and computation resource. The gains are substantially robust for different model sizes and datasets, in particular, a ViT-B model trained with GAN-MAE for 200 epochs outperforms the MAE with 1600 epochs on fine-tuning top-1 accuracy of ImageNet-1k with much less FLOPs. Besides, our approach also works well at transferring downstream tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, Transformer <ref type="bibr" target="#b60">[62]</ref> has become the de facto standard architecture in computer vision, and has surpassed state-of-the-art Convolutional Neural Network (CNN) <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b56">58]</ref> feature extractors in vision tasks through models such as the Vision Transformer <ref type="bibr" target="#b19">[21]</ref>. Meanwhile, self-supervised learning (SSL) algorithms <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b27">29]</ref> aims to learn transferable representation from unlabeled * The corresponding author. data by performing instance-level pretext tasks, and has been a long-standing target in the vision community. Particularly, masked image modeling (MIM) in SSL for vision transformers has shown remarkably impressive downstream performance in a wide variety of computer vision tasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">28]</ref>, attracting increasing attention. MIM is a simple pretext task that first randomly masks some patches of an image, and then predicts the contents of the masked patches according to the remaining, using various reconstruction targets, e.g., visual tokens <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">19]</ref>, semantic features <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b75">77]</ref> and raw pixels <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b68">70]</ref>. Essentially, it learns the transferable representation by modeling the image structure itself as content prediction. While more effective than conventional pre-training, masked autoencoder modeling approaches still exist some issues: (i) reconstruction optimization with MSE loss leads to blurrier output images than the raw input, it would be better to use a more perceptual loss over pixels to guide the fine-grained seman-tic understanding and representation learning, leading to more plausible synthesized patches; (ii) inner dependency between masked patches is lacked <ref type="bibr" target="#b69">[71]</ref>, i.e., generation of masked image patches may lack the surrounding information. This situation becomes more serious when the image patch masking ratio is large. We alleviate this problem by introducing confident synthesized patches as complementary information during training; (iii) mask-reconstruction methods incur a substantial computation cost because the network only learns from part of the visible patches and misses the information of masked patches.</p><p>In this paper, we propose a Generative Adversarial Networks-based pre-training framework, referred to as GAN-MAE, which contains two components: a generator model learns to reconstruct the masked patches according to visible patches in the encoder-decoder architecture and a discriminator model learns to distinguish real image patches from plausible but synthesized remains. Generally, given an image from training dataset, our method first randomly masks parts of patches and reconstructs them using the rest visible patches with a generator, which serves as a standard MAE model. Then we build the corrupt image as the combination of visible and synthesis patches, which is then fed into the discriminator to predict whether each patch is from raw image or synthesized results. In this manner, the discriminator provides a valid guiding for more delicate image patch modeling. Then, with the development of generator capacity, a key advantage of discriminative task is that it integrates the synthesized patches into corrupt images as complementary information, which fills the missing inner relationship between patches during pre-training. Moreover, we shared the parameters of vision transformer backbone in the generator and discriminator to promote memory reduction, training efficiency, as well as performance enhancement.</p><p>Our experiments follow the same architecture, settings, and pre-training recipe as MAE <ref type="bibr" target="#b26">[28]</ref>, and we find that the simple incorporation of a discriminator consistently outperforms MAE in variant models, e.g., ViT-S, ViT-B, and ViT-L, when fine-tuning for top-1 accuracy of ImageNet classification. We also conduct extensive ablation studies to validate the effectiveness of our core designs in backbone parameter sharing and advarisal training. As pre-training with more epochs usually results in a better downstream performance, we argue that an important consideration for pre-training methods should be computation efficiency as well as absolute downstream performance. From this viewpoint, we also demonstrate that discrimination of pseudoimage patches forces GAN-MAE to train more efficiently than standard MAE. We further provide a comprehensive comparison with MAE in various epochs and various models and show our framework achieves consistently better performance. In particular, as presented in Figure <ref type="figure" target="#fig_0">1</ref>, for the ViT-B model structure, our GAN-MAE achieves compara-ble classification performance with only 200 pre-training epochs vs. standard MAE 1600 pre-training epochs. Furthermore, the GAN-MAE achieves 0.7 points improvement when pre-training 1600 epochs. Finally, we summarize our contribution as follows:</p><p>? We propose a new and effective GAN-alike framework for visual representation self-supervised learning, which to our best knowledge is the first trial of integrating GAN idea into MAE framework. As a generic approach, we suggest that this framework can be easily applied on many other MIM-based tasks.</p><p>? We introduce two core designs: shared weight for the main backbones of generator and discriminator, and an adversarial training process, both of which cost fewer amounts of computing resources while obtaining appreciable performance improvements.</p><p>? Extensive experiments demonstrate that compared with the original MAE, our method is more computeefficient and results in better transfer representation learning on downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Autoencoding. Autoencoder <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b33">35]</ref> is an unsupervised learning technique for neural networks that learns efficient representations by training the network to ignore signal noise. It includes an encoder that maps the original data to a low-dimensional latent embedding and a decoder that recovers the data from the latent embedding, with the goal of learning a compressed knowledge representation. The denoising autoencoder <ref type="bibr" target="#b61">[63,</ref><ref type="bibr" target="#b62">64]</ref> learns to reconstruct clean data points from a noisy version. Numerous efforts have been devoted for image denoising, such as masking pixels <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b62">64]</ref>, inpainting <ref type="bibr" target="#b67">[69]</ref>, removing color channels <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b70">72]</ref>, and shuffling image patches <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b52">54]</ref>. For a broader overview of denoising autoencoder, we refer to <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b59">61]</ref>.</p><p>Masked Image Modeling. Masked language modeling <ref type="bibr" target="#b35">[37,</ref><ref type="bibr" target="#b47">49,</ref><ref type="bibr" target="#b55">57]</ref>, which generalizes well on language understanding and generation tasks, is the domain self-supervised approach in the field of NLP. Similarly, vision transformer <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b48">50]</ref> based masked image modeling (MIM) approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b68">70,</ref><ref type="bibr" target="#b75">77]</ref> for computer vision tasks have also been developed. Generally, these MIM approaches first apply a mask to patches of an image, and then the masked patches are predicted given the visible patches  The works include framework design <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b68">70]</ref>, prediction targets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b64">66,</ref><ref type="bibr" target="#b75">77]</ref>, and integration with visionlanguage representation learning <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b73">75]</ref>. Our work belongs to the first group and introduces a novel GAN framework that discriminates the reconstructed image patches in a standard MAE to learn deeper semantics.</p><p>Generative Adversarial Networks. GANs <ref type="bibr" target="#b24">[26]</ref> are effective at generating high-quality synthetic data. Usually, the generator generates an image, and the discriminator determines whether the input image is a real image or a generated image. Subsequently, many improvements based on the original GAN focused on speeding up the training of the network and improving the quality of the generated images <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b50">52,</ref><ref type="bibr" target="#b72">74]</ref>. These improvements also help GAN achieving a wider range of applications <ref type="bibr" target="#b43">[45,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b65">67]</ref>. Methods based on GAN are also widely used in image-to-image translation <ref type="bibr" target="#b34">[36]</ref>, super-resolution <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b38">40]</ref>, style transfer <ref type="bibr" target="#b14">[15]</ref>, text generation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b71">73]</ref>, and representation learning <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b23">25]</ref>, to name a few. Particularly, <ref type="bibr" target="#b54">[56]</ref> brings some designs of CNN architecture to stabilize the training of GAN framework, after that the discriminator can be directly used as feature extractor in downstream tasks. In contrast, our method proposes to integrate the GAN as assistant for the MIM task, which focuses on the study of better pattern for masking based self-supervised learning. Besides, our strategy of shared parameters provides a unified backbone for better vision representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>In this section, we introduce the GAN-MAE framework in details. At first, we briefly review the conventional masked autoencoder model in Sec. 3.1, and then describe the proposed generator-discriminator pre-training framework in Sec. 3.2. The architecture of our framework is presented in Figure <ref type="figure" target="#fig_1">2</ref>. Finally, we suggest an adversarial training processes under the proposed method and discuss our framework in Sec. 3.3 and Sec. 3.4, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>Masked Autoencoder (MAE) <ref type="bibr" target="#b26">[28]</ref> is a self-supervised approach with a vision transformer encoder and a small transformer decoder, which randomly masks a large portion of input patches, and then reconstructs the masked patches according to the visible patches. Specifically, provided with an image X ? R C?H?W , where C, H and W are the channel number, image height and image width respectively, MAE partitions X into N = H?W P 2 non-overlapping patches with patch size P . In this way, the image is transformed into a sequence of patches X = {x 1 , . . . , x N } with each element x k ? R P 2 ?C . Then, we sample a random set of patch index M in uniform distribution, and split the image patches X into masked patch set</p><formula xml:id="formula_0">X m = {x k |k ? M } and visible patch set X v = {x k |k / ? M }.</formula><p>During training, the MAE encoder inputs X v to achieve the latent representations H v . Then, the MAE decoder attempts to reconstruct X m with the input of interpolating [mask] token embedding into the sequence of latent representations H v according to the index set M , and outputs the reconstructed patches Xm . Finally, MAE optimizes the mean-squared error reconstruction loss on the masked patches as:</p><formula xml:id="formula_1">L mae (X, M, ? mae ) = k?M ||x k -x k || 2 2 ,<label>(1)</label></formula><p>where ? mae represents the parameters of MAE model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">When MAE meet GAN</head><p>In this section, we describe the proposed GAN-MAE framework, as presented in Figure <ref type="figure" target="#fig_1">2</ref>. Generally, our framework consists of two parts, a generator G to reconstruct the masked image patches and a discriminator D to predict the realness of image patches.</p><p>Image Patch Generator. Identical to the standard MAE, the generator follows the encoder-decoder paradigm and is trained to perform masked-image reconstruction task. Given the partitioned image patches X, the generator randomly masks some image patches and encodes the remaining visible patches X v into a sequence of contextualized vector representations H v , based on which the masked patches are reconstructed as Xm . Please refer to Sec. 3.1 for more details. In general, the generation process can be formulated as:</p><formula xml:id="formula_2">M ? Uniform(1, N ),<label>(2)</label></formula><formula xml:id="formula_3">H v = f e (X v , M ),<label>(3)</label></formula><formula xml:id="formula_4">Xm = f d (H v , M ), (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where N is the number of image patches. f e (?) and f d (?) denote the encoder and decoder in a conventional MAE.</p><p>Image Patch Discriminator. For a patch index k and corrupted image sequence X = {X v , Xm }, the discriminator predicts whether the patch token x k is real or synthesized as binary classification task. Specifically, we create the corrupted image X by maintaining the visible patches X v in raw image X and replacing the masked patches with generator predicted result Xm . Note that X v and Xm are absolute complement of set to each other, i.e., X v ? Xm = X and X v ? Xm = ?. Formally, the task of discriminator model can be formulated as:</p><formula xml:id="formula_6">D(X, k) = p disc (y k |X, k), for k ? [1, N ].<label>(5)</label></formula><p>Let the ground-truth classification label sequence Y = {y 1 , . . . , y N } with each element y k ? {0, 1}, where 0 and 1 denote the corresponding image patch is reconstructed from generator or comes from the original image, respectively.</p><p>The training objective of the discriminator can be formulated as:</p><formula xml:id="formula_7">L disc (X, ? disc ) = N k=1 -y k log D(X, k) -(1 -y k )log (1 -D(X, k)).<label>(6)</label></formula><p>Particularly, though the corrupted image is partially unreal, we suggest that this discriminative task can still be benefit to the learning of feature representation with the plausible corrupted image from a well-trained generator, as presented in experiments. Hitherto, due to the GAN-alike strategy for MAE task, we name our framework as GAN-MAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training Scheme</head><p>We explore the training strategy of our proposed GAN-MAE in this section. Particularly, to improve synthesis result, we augment the L-2 reconstruction loss with a percep- tual loss that aims to differentiate the real patches and reconstructed patches as:</p><formula xml:id="formula_8">L adv (X, ? mae ) = log D(X v ) + log(1 -D( Xm )). (7)</formula><p>Please note that Xm is the reconstructed patches by MAE. Therefore, the final objective for the generator comes to minimize the combined loss as:</p><p>Lgen(X, ?mae) = Lmae(X, ?mae) + ?L adv (X, ?mae), <ref type="bibr" target="#b7">(8)</ref> where we compute the adaptive weight ? according to:</p><formula xml:id="formula_9">? = ?[L mae ] ?[L adv ] + ? ,<label>(9)</label></formula><p>?[?] denotes the gradients of different loss function w.r.t. the parameters of last layer in network, and ? = 1e -6 is used for numerical stability. Intuitively, the integration of ? adaptively balances the contributions of two loss functions to the gradients of parameters.</p><p>Based on the aforementioned strategy, the final training algorithm can be formulated in Algorithm 1. Specifically, at each epoch, we conduct the iteration in two steps: (i) train only the generator with L gen ; (ii) Train the discriminator with L disc . During training, we observe that shared weights of generator and discriminator backbones can usually stabilize the training procedure and bring extra-gain in down-stream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Discussion</head><p>Theoretically, the integration of discriminator can be considered as a high-level perceptual loss, which forces the generator to learn better feature representation for the plausible synthesis of masked patches. As the quality of synthesized patches improved, we claim that the introduction of corrupted images, serving as complementary information, provides plausible inner dependency between full image patches for representation learning. Furthermore, with the shared parameters of backbones in generator and discriminator, the GAN-alike framework can be considered as a type of multi-task learning, leading to even better result as presented in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Settings</head><p>In the experiments, we pre-train our GAN-MAE model on the ImageNet-1k [16] and evaluate the performance in end-to-end fine-tuning (FT) pattern for the task of classification, semantic segmentation, object detection and instance segmentation. The evaluation metric of classification is the top-1 validation accuracy on 224?224 cropped input images. The input image is partitioned into 14?14 patches and each patch is of size 16?16. Following the setting of MAE, we only use the standard random cropping and horizontal flipping for data augmentation. To validate the effectiveness of GAN-MAE framework, the used ViT architecture and most hyper-parameters are exactly the same to <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b58">60]</ref>, i.e., ViT-S (12 transformer blocks with dimension 384), ViT-B (12 transformer blocks with dimension 768), and ViT-L (24 transformer blocks with dimension 1024). All version of ViT models are trained with 4096 batch size on 8 V-100 32GB GPUs. We adopt dynamic token masking with the masked positions decided on-the-fly. We use AdamW <ref type="bibr" target="#b36">[38]</ref> optimizer and cosine schedule <ref type="bibr" target="#b49">[51]</ref> with warm up for model training. The learning rate is annealed according to the cosine schedule. Unless stated otherwise, results are evaluated on the dev set. Please refer to appendix A for more training implementation and hyperparameter values for different backbone in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Analysis of GAN-MAE</head><p>We analyze our GAN-MAE framework by proposing and evaluating several extensions to the model. Unless stated otherwise, all these experiments use the same model size as ViT-B and training dataset ImageNet-1K.</p><p>Parameter Sharing of Backbone. In this work, we propose to improve the efficiency of the pre-training by sharing the parameters of the backbone vision transformer between the generator encoder and discriminator. One cause is that the generator and discriminator utilize the same network architecture, and all of the transformer weights can be tied. However, we can release the weight sharing and train the generator and discriminator independently. In between, we  <ref type="table" target="#tab_1">1</ref>, where we employ the adversarial training with the same training epochs. We can see that, generally, the fine-tuning top-1 accuracy of shared weight outperforms the independent generator and discriminator conspicuously, in particular 0.4 point improvements for the generator when pre-training 800 epochs. We hypothesize that GAN-MAE framework benefits from both mask-then-reconstruct and pseudo-patch classification tasks, which can incorporate the visual semantic and consistency understanding.</p><p>It is also surprising that under the GAN process without weight sharing, the discriminator with learning of patch classification can lead to better performance than generator. We suspect that the fine-grained patch classification is more delicate while harder than image-level classification. Thus, we further tried to add an image-level contrastive objective. For this task, we input 50% of the input image unchanged rather than noising them with the generator. We then added a prediction head to the model that predicted if the entire input image was corrupted or not. However, the result didn't improve the final accuracy. In conclusion, we believe that the design of discriminative task, e.g., getting closer to downstream and be more difficult, is an important exploration direction in the future. Another interesting finding is that with the pre-training epochs increase, the independent generator performance boosts, we believe that it is caused by the adversarial training, where the discriminator becomes an effective guiding for generation.</p><p>Training Schemes. We analyze the effect of proposed adversarial training scheme on GAN-MAE with shared backbones. Two training variants are considered as following:</p><p>? Two-stage Training. It is natural to consider to remove the discriminator guiding signal during generator training, which leads to the disentangled optimizing process. Specifically, at each epoch, we do the following steps: train the generator only with L mae and train the discriminator only with L disc . The difference with ad- </p><formula xml:id="formula_10">L gen (? mae ) + ?L disc (? disc ).<label>(10)</label></formula><p>We set ?, the weight for the discriminator objective in the loss to 2.0, as we searched for ? out of <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10]</ref> in early experiments.</p><p>Note that regardless of the form, the nature of sequential training, i.e., the corrupt image built from generator will be fed into discriminator, is not changed. The evaluation results for classification are listed in Table 2. GPU time means pre-training time (hours) on 8 V100 32GB GPUs environment. As we can see, the combined training shows a superior training time while adversarial training slows down as the training procedures of generator and discriminator are isolated. On the other hand, the performance of adversarial training is not better than combined training in the early stages; when pre-training epochs come to 800, the benefits of adversarial training appear. Masking Ratio. Table <ref type="table" target="#tab_4">3</ref> shows the influence of masking ratio in MAE-GAN under different pre-training epochs.</p><p>The optimal ratio for MAE-GAN is identical to 75%, showing a obviously better classification performance compared with same masking ratio in previous work <ref type="bibr" target="#b26">[28]</ref>. Meanwhile, we present several reconstructed images from MAE and GAN-MAE. Although the MIM model can infer missing patches as different yet plausible outputs when mask ratio comes to large, the generator of GAN-MAE can predict the images with more realistic and fine-grained details, e.g., the outline of a mountain peak in first case, which we believe is relative to the learning of useful representation.</p><p>Computation Resource. In terms of pre-training cost, we conduct a computing resource comparison with the baseline MAE on different vision transformer backbones. The results are shown in Table <ref type="table" target="#tab_5">4</ref>. We first point out that GAN-MAE slows down the training process as the discriminator integration, e.g. 317.5h vs. 127.7h for ViT-B model in 800 epochs. Moreover, we also choose to measure computation usage in terms of floating point operations (FLOPs) as it is a metric agnostic to the particular hardware, low-level optimizations, etc. Note that an "operation" is a mathematical operation, not a machine instruction and thop package is used to compute FLOPs in practice. As expected, GAN-MAE employs ? 3 extra theoretical computation cost. Besides, benefits from the weight sharing of backbone, the GAN-MAE incorporates less than 1% addition parameter, which helps to reduce the running memory usage, and provides a convenience to set large batch size. Last but not least, we want to state that although our GAN-MAE method incorporates additional computation resources during training at each epoch, considering the superiority of reduction of epoch number and classification performance improvements, it is fully acceptable and exploration valuable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">ImageNet Classification Comparison</head><p>We compare our GAN-MAE methods with previous state-of-the-art works on the ImageNet-1K classification task. Table <ref type="table">5</ref> reports the top-1 validation accuracy for finetuning results. We can find that compared to the supervised models, trained from scratch, all of the self-supervised pre-training methods achieve significant improvement, sug- Table <ref type="table">5</ref>. End-to-end fine-tuning on ImageNet-1K. We report the fine-tuning top-1 accuracy for classification in different vision transformer architectures and results show that GAN-MAE outperforms previous self-supervised methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Pre-train data Pre-train epochs ViT-S ViT-B ViT-L Supervised <ref type="bibr" target="#b57">[59]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Downstream Tasks</head><p>Semantic Segmentation. We compare our GAN-MAE with supervised as well as state-of-the-art self-supervised models on the widely used dataset ADE20K <ref type="bibr" target="#b74">[76]</ref> for semantic segmentation. Specifically, we use the UperNet framework <ref type="bibr" target="#b66">[68]</ref> in the experiments. We train Upernet for 160K iterations with batch size set as 64 and report the results in Table <ref type="table" target="#tab_8">7</ref>. The evaluation metric is mean Intersection of Union (mIoU) averaged over all semantic categories and the single-scale test results are reported. Importantly, we can see that the proposed GAN-MAE gets superior performance than all the other baselines in the same configuration, further validating the effectiveness of adversarial training with a reconstructed patch discriminator.</p><p>Object Detection and Segmentation. We also perform object detection and instance segmentation, compared with other popular self-supervised methods and the supervised model, on the COCO dataset <ref type="bibr" target="#b45">[47]</ref>. In practice, we choose the Mask R-CNN <ref type="bibr" target="#b28">[30]</ref> framework and adopt FPNs <ref type="bibr" target="#b44">[46]</ref> to scale the feature map into different sizes as introduced in <ref type="bibr" target="#b42">[44]</ref>. The performance is tested on the COCO validation set, following the previous work <ref type="bibr" target="#b16">[18]</ref>. The results are listed in Table <ref type="table" target="#tab_9">8</ref> in terms of box AP metric for object detection and mask AP metric for instance segmentation. Importantly, we can observe that our GAN-MAE model achieves 49.0% for object detection and 43.8% for segmentation, surpassing the previous state-of-the-art BootMAE by 0.5% and 0.4% point, respectively.</p><p>Classification Robutness. Similar to <ref type="bibr" target="#b26">[28]</ref>, we further evaluate the robustness of classification performance on the four ImageNet variants, i.e., ImageNet-C <ref type="bibr" target="#b31">[33]</ref>, ImageNet-A <ref type="bibr" target="#b32">[34]</ref>, ImageNet-R <ref type="bibr" target="#b30">[32]</ref>, and ImageNet-Sketch <ref type="bibr" target="#b63">[65]</ref>, which are common benchmarks to evaluate robustness for perturbations. Table <ref type="table" target="#tab_7">6</ref> demonstrates the robustness comparison with GAN-MAE and MAE using the ViT-B backbone, as well as previous supervised SoTA models. The results illustrate that GAN-MAE outperforms the MAE baseline consistently on all robustness datasets, indicating that the promising of adversarial training in representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed a new self-supervised GAN-alike framework for visual representation learning, where a generator is used to predict masked image patches according to the visible patches and a discriminator is employed to predict whether the patch is from raw image or generated by generator. The key idea is adversarial training a shared vision Transformer to distinguish the input patches from high-quality negative samples, which we believe is beneficial for the understanding of visual conception. It works well while incorporating no much addition parameter. More encouragingly, compared to standard masked image modeling, our GAN-MAE is more compute-efficient, as fewer pre-training epochs result in a better performance on downstream tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Performance comparison in different pre-training epochs for ImageNet-1K Fine-tuning top-1 accuracy. Compared to MAE trained for 1600 epochs, GAN-MAE achieves comparable accuracy with much less training time at 200 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Overview of GAN-MAE framework, where a generator is used to predict the masked patches and a discriminator is employed to classify whether the patches are selected from the raw image or synthesized results. In particular, both of MAE encoder and discriminator are based on the vision Transformer backbone and share parameters for memory reduction and training efficiency.</figDesc><graphic url="image-2.png" coords="3,139.54,109.19,64.50,64.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Qualitative analysis for patch reconstruction. Example results are from ImageNet validation set. For each tuple, we show the raw image, masked image, MAE reconstructed image, and our proposed GAN-MAE reconstructed image from left to right. We can see that the reconstructed images from GAN-MAE are significantly clearer than MAE, which we believe benefits the fine-grained visual semantic understanding.</figDesc><graphic url="image-5.png" coords="7,52.59,72.00,490.05,197.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Adversarial training for GAN-MAE Data: Training data D train , total epoch number N e , GAN-MAE model with generator parameters ? mae and discriminator parameters ? disc ; 1 share weights between generator and discriminator backbones; 2 while n e &lt; N e do</figDesc><table><row><cell>3</cell><cell>for x i ? D train do</cell></row><row><cell>4</cell><cell>? generator training;</cell></row><row><cell>5</cell><cell>sample masking set M i and mask image x i ;</cell></row><row><cell>6</cell><cell>predict masked image patches xi m ;</cell></row><row><cell>7</cell><cell>compute loss L gen ;</cell></row><row><cell>13</cell><cell>end</cell></row><row><cell>14</cell><cell>n e + = 1;</cell></row><row><cell cols="2">15 end</cell></row></table><note><p>8 loss backward for updating ? mae ; 9 ? dicriminator training; 10 construct x i based on x i and xi m ; 11 comput loss L disc ; 12 loss backward for updating ? disc ;</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Effect of parameter sharing in GAN-MAE framework. Results demonstrate that shared parameters for backbone benefits both memory cost and performance improvements.</figDesc><table><row><cell>Models</cell><cell cols="3">Epoch Mask ratio FT</cell></row><row><cell>Generator</cell><cell>800</cell><cell>75%</cell><cell>83.9</cell></row><row><cell cols="2">Discriminator 800</cell><cell>75%</cell><cell>84.2</cell></row><row><cell>Shared</cell><cell>800</cell><cell>75%</cell><cell>84.3</cell></row><row><cell>Generator</cell><cell>1600</cell><cell>75%</cell><cell>84.4</cell></row><row><cell cols="2">Discriminator 1600</cell><cell>75%</cell><cell>84.4</cell></row><row><cell>Shared</cell><cell>1600</cell><cell>75%</cell><cell>84.6</cell></row><row><cell cols="4">can adopt both the parameters of generator and discrimina-</cell></row><row><cell cols="4">tor for downstream learning. The experimental results are</cell></row><row><cell>shown in Table</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Effect of different training schemes.</figDesc><table><row><cell>Models</cell><cell cols="4">Epoch Mask ratio FT GPU Time</cell></row><row><cell>Two-stage</cell><cell>300</cell><cell>75%</cell><cell>82.0</cell><cell>94.3h</cell></row><row><cell>Combined</cell><cell>300</cell><cell>75%</cell><cell>82.2</cell><cell>90.9h</cell></row><row><cell cols="2">Adversarial 300</cell><cell>75%</cell><cell>82.2</cell><cell>118.8h</cell></row><row><cell>Two-stage</cell><cell>800</cell><cell>75%</cell><cell>84.0</cell><cell>252.2h</cell></row><row><cell>Combined</cell><cell>800</cell><cell>75%</cell><cell>84.1</cell><cell>240.5h</cell></row><row><cell cols="2">Adversarial 800</cell><cell>75%</cell><cell>84.3</cell><cell>317.5h</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Effect of different masking ratio in GAN-MAE framework with different pre-training epochs.</figDesc><table><row><cell>Models</cell><cell cols="3">Epoch Mask ratio FT</cell></row><row><cell>MAE</cell><cell>800</cell><cell>75%</cell><cell>83.4</cell></row><row><cell cols="2">GAN-MAE 800</cell><cell>70%</cell><cell>84.3</cell></row><row><cell cols="2">GAN-MAE 800</cell><cell>75%</cell><cell>84.3</cell></row><row><cell cols="2">GAN-MAE 800</cell><cell>80%</cell><cell>84.0</cell></row><row><cell>MAE</cell><cell>1600</cell><cell>75%</cell><cell>83.6</cell></row><row><cell cols="2">GAN-MAE 1600</cell><cell>70%</cell><cell>84.5</cell></row><row><cell cols="2">GAN-MAE 1600</cell><cell>75%</cell><cell>84.6</cell></row><row><cell cols="2">GAN-MAE 1600</cell><cell>80%</cell><cell>84.4</cell></row><row><cell cols="4">versarial training lies in that the training loss of the</cell></row><row><cell cols="3">generator in the first step is changed.</cell><cell></cell></row></table><note><p>? Combined Training. Instead of iterative in total dataset, we can jointly trains the generator and discriminator at each step. That is, for each image X in the training dataset D train , we can directly minimize the combined loss as:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Comparison of computation resource usage during selfsupervied pre-training.</figDesc><table><row><cell cols="2">Backbone Models</cell><cell>FLOPs</cell><cell>Params</cell></row><row><cell>ViT-B</cell><cell>MAE</cell><cell>9.4e9</cell><cell>111.654M</cell></row><row><cell>ViT-B</cell><cell cols="3">GAN-MAE 2.6e10 111.656M</cell></row><row><cell>ViT-L</cell><cell>MAE</cell><cell cols="2">2.0e10 329.238M</cell></row><row><cell>ViT-L</cell><cell cols="3">GAN-MAE 8.0e10 329.240M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Robustness Evaluation on the four ImageNet-variants: ImageNet-C, ImageNet-A, ImageNet-R, and ImageNet-Sketch. Except for ImageNet-C which is measured in terms of mean Corruption Error (mCE), top-1 accuracy is used as the remaining evaluation metric. For simplicity, we denoted IN-C, IN-A, IN-R, In-Skectch correspondingly.</figDesc><table><row><cell>Model</cell><cell cols="4">IN-C (mCE ?) IN-A (top-1 ?) IN-R (top-1 ?) IN-Sketch (top-1 ?)</cell></row><row><cell>Supervised [53]</cell><cell>42.5</cell><cell>35.8</cell><cell>48.7</cell><cell>36.0</cell></row><row><cell>MAE [28]</cell><cell>51.7</cell><cell>35.9</cell><cell>48.3</cell><cell>34.5</cell></row><row><cell>GAN-MAE</cell><cell>49.5</cell><cell>36.8</cell><cell>49.6</cell><cell>35.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Semantic segmentation comparison on the ADE20K dataset for mIoU (%) metric with the ViT-B backbone.</figDesc><table><row><cell>Models</cell><cell cols="3">Pre-train data Epochs mIoU</cell></row><row><cell cols="2">Supervised [28] IN1K w/ labels</cell><cell>300</cell><cell>47.4</cell></row><row><cell>MoCo v3 [14]</cell><cell>IN1K</cell><cell>300</cell><cell>47.3</cell></row><row><cell>BEiT [3]</cell><cell>IN1K+DALLE</cell><cell>800</cell><cell>47.1</cell></row><row><cell>MAE [28]</cell><cell>IN1K</cell><cell>800</cell><cell>47.6</cell></row><row><cell>MAE [28]</cell><cell>IN1K</cell><cell>1600</cell><cell>48.1</cell></row><row><cell>BootMAE [20]</cell><cell>IN1K</cell><cell>800</cell><cell>49.1</cell></row><row><cell>GAN-MAE</cell><cell>IN1K</cell><cell>800</cell><cell>49.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>COCO object detection and segmentation using Mask R-CNN framework with ViT-B backbone.</figDesc><table><row><cell>Models</cell><cell cols="3">Pre-train data AP-box AP-mask</cell></row><row><cell cols="2">Supervised [28] IN1K w/ labels</cell><cell>44.1</cell><cell>39.8</cell></row><row><cell>MoCo v3 [14]</cell><cell>IN1K</cell><cell>44.9</cell><cell>40.4</cell></row><row><cell>BEiT [3]</cell><cell>IN1K+DALLE</cell><cell>46.3</cell><cell>41.1</cell></row><row><cell>MSN [1]</cell><cell>IN1K</cell><cell>46.6</cell><cell>41.5</cell></row><row><cell>iBOT [77]</cell><cell>IN1K</cell><cell>47.3</cell><cell>42.2</cell></row><row><cell>MAE [28]</cell><cell>IN1K</cell><cell>47.2</cell><cell>42.0</cell></row><row><cell>BootMAE [20]</cell><cell>IN1K</cell><cell>48.5</cell><cell>43.4</cell></row><row><cell>GAN-MAE</cell><cell>IN1K</cell><cell>49.0</cell><cell>43.8</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Masked siamese networks for label-efficient learning</title>
		<author>
			<persName><forename type="first">Mahmoud</forename><surname>Assran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Rabbat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.07141</idno>
		<imprint>
			<date type="published" when="2008">2022. 1, 2, 7, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Data2vec: A general framework for self-supervised learning in speech, vision and language</title>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno>PMLR, 2022. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<biblScope unit="page" from="1298" to="1312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Beit: Bert pre-training of image transformers</title>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songhao</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2008">2021. 1, 2, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Blind super-resolution kernel estimation using an internal-gan</title>
		<author>
			<persName><forename type="first">Sefi</forename><surname>Bell-Kligler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning deep architectures for ai</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations and trends? in Machine Learning</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Began: Boundary equilibrium generative adversarial networks</title>
		<author>
			<persName><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Schumm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10717</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9650" to="9660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adversarial text generation via feature-mover&apos;s distance</title>
		<author>
			<persName><forename type="first">Liqun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR, 2020. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Context autoencoder for self-supervised representation learning</title>
		<author>
			<persName><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shentong</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03026</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2008">2021. 1, 7, 8</date>
			<biblScope unit="page" from="9640" to="9649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stargan: Unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minje</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Munyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2009">2018. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>Proc. IEEE CVPR</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adversarial feature learning</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cswin transformer: A general vision transformer backbone with cross-shaped windows</title>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12124" to="12134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Peco: Perceptual codebook for bert pre-training of vision transformers</title>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12710</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Bootstrapped masked autoencoders for vision bert pretraining</title>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.07116</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stacked convolutional denoising autoencoders for feature representation</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lefei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1017" to="1027" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Are large-scale datasets necessary for self-supervised pre</title>
		<author>
			<persName><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv?</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10740</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Maskgan: Better text generation via filling in the</title>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large-scale adversarial training for visionand-language representation learning</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="6616" to="6628" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent-a new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Gheshlaghi Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21271" to="21284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2022">2022. 1, 2, 3, 5, 6, 7, 8, 12</date>
			<biblScope unit="page" from="16000" to="16009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The many faces of robustness: A critical analysis of out-of-distribution generalization</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyak</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8340" to="8349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12261</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Natural adversarial examples</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15262" to="15271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><surname>Kristina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toutanova</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning representations for automatic colorization</title>
		<author>
			<persName><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maire</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="577" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mpvit: Multi-path vision transformer for dense prediction</title>
		<author>
			<persName><forename type="first">Youngwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonghee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Willette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="7287" to="7296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training</title>
		<author>
			<persName><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuejian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11336" to="11344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Liunian</forename><surname>Harold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Da Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">Visualbert: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Benchmarking detection transfer learning with vision transformers</title>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11429</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">High-resolution photorealistic image translation in real-time: A laplacian pyramid translation network</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9392" to="9400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitnick</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pd-gan: Probabilistic diverse gan for image inpainting</title>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyu</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9371" to="9381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond Yk</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Towards robust vision transformer</title>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gege</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuefeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ranjie</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaokai</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12042" to="12051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno>PMLR, 2021. 7</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.09795</idno>
		<title level="m">Three things everyone should know about vision transformers</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Recent advances in autoencoder-based representation learning</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Michael Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.05069</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning robust global representations by penalizing local predictive power</title>
		<author>
			<persName><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songwei</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Masked feature prediction for self-supervised visual pre-training</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="14668" to="14678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Tedigan: Text-guided diverse face image generation and manipulation</title>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujiu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing-Hao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Image denoising and inpainting with deep neural networks</title>
		<author>
			<persName><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Simmim: A simple framework for masked image modeling</title>
		<author>
			<persName><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Adversarial feature matching for text generation</title>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4006" to="4015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Energybased generative adversarial networks</title>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">General facial representation learning in a visual-linguistic manner</title>
		<author>
			<persName><forename type="first">Yinglin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="18697" to="18709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="633" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<author>
			<persName><forename type="first">Jinghao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.07832</idno>
		<title level="m">ibot: Image bert pre-training with online tokenizer</title>
		<imprint>
			<date type="published" when="2008">2021. 1, 2, 3, 7, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
