<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2021 HW-NAS-BENCH: HARDWARE-AWARE NEURAL AR-CHITECTURE SEARCH BENCHMARK</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Under review as a conference paper at ICLR 2021 HW-NAS-BENCH: HARDWARE-AWARE NEURAL AR-CHITECTURE SEARCH BENCHMARK</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>HardWare-aware Neural Architecture Search (HW-NAS) has recently gained tremendous attention for automating the design of DNNs to be deployed into more resource-constrained daily life devices. Despite their promising performance, developing optimal HW-NAS solutions can be prohibitively challenging as it requires cross-disciplinary knowledge in the algorithm, micro-architecture, and device-specific compilation. First, to construct the hardware cost to be incorporated into the NAS process, existing works mostly adopt either pre-collected cost look-up tables or device-specific hardware cost models. The former can be time-consuming due to the needed learning about the device's compilation method and how to set up the measurement pipeline, while the latter is often a barrier for non-hardware experts like NAS researchers. Both of them limit the development of HW-NAS innovations and impose a barrier-to-entry to non-hardware experts. Second, similar to generic NAS, it can be notoriously difficult to benchmark HW-NAS algorithms due to the required significant computational resources and the differences in their adopted search space, hyperparameters, and hardware devices. To this end, we develop HW-NAS-Bench, the first public dataset for HW-NAS research which aims to democratize HW-NAS research to non-hardware experts and make HW-NAS research more reproducible and accessible. To design HW-NAS-Bench, we carefully collected the measured/estimated hardware performance (e.g., energy cost and latency) of all the networks in the search space of both NAS-Bench-201 and FBNet, considering six hardware devices that fall into three categories (i.e., commercial edge devices, FPGA, and ASIC). Furthermore, we provide a comprehensive analysis of the collected measurements in HW-NAS-Bench to provide insights for HW-NAS research. Finally, we demonstrate exemplary user cases where HW-NAS-Bench (1) allows non-hardware experts to perform HW-NAS by simply querying our pre-measured dataset and (2) verify that dedicated device-specific HW-NAS can indeed often provide optimal accuracycost trade-offs. All the codes and data will be released publicly upon acceptance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The recent performance breakthroughs of deep neural networks (DNNs) have attracted an explosion of research in designing efficient DNNs, aiming to bring powerful yet power-hungry DNNs into more resource-constrained daily life devices for enabling various DNN-powered intelligent functions. Among them, HardWare-aware Neural Architecture Search (HW-NAS) has emerged as one of the most promising techniques as it can automate the process of designing optimal DNN structures for the target applications, each of which often corresponds to a different hardware device and requires a different hardware efficiency metric (e.g., prioritizes latency or energy). For example, HW-NAS in <ref type="bibr" target="#b45">(Wu et al., 2019)</ref> develops a differentiable neural architecture search (DNAS) framework and discovers state-of-the-art (SOTA) DNNs balancing both accuracy and hardware efficiency, by incorporating a loss that consists of both the cross-entropy loss that leads to better accuracy and the latency loss that penalizes the network's latency on a target device.</p><p>Despite the promising performance achieved by SOTA HW-NAS, there exist paramount challenges that limit the development of HW-NAS innovations. First, HW-NAS requires the collection of hardware efficiency data corresponding to (all) the networks in the search space. To do so, current practice either pre-collects these data to construct a hardware-cost look-up table or adopts device- specific hardware-cost estimators/models, both of which can be time consuming to obtain and impose a barrier-to-entry to non-hardware experts. This is because it requires knowledge about devicespecific compilation and properly setting up the hardware measurement pipeline for hardware-cost data collection. Second, similar to generic NAS, it can be notoriously difficult to benchmark HW-NAS algorithms due to the required significant computational resources and the differences in their (1) hardware devices, which are specific for HW-NAS, (2) adopted search space, and (3) hyperparameters. Such a difficulty is even higher for HW-NAS considering the numerous choices of hardware devices, each of which can favor very different network structures even under the same target hardware efficiency, as discussed in <ref type="bibr" target="#b11">(Chu et al., 2020)</ref>. While the number of floating-point operations (FLOPs) have been commonly used to estimate the hardware-cost, many works have pointed out that DNNs with fewer FLOPs are not necessarily faster or more efficient <ref type="bibr" target="#b45">(Wu et al., 2019)</ref>. For example, NasNet-A has a comparable complexity in terms of FLOPs as MobileNetV1 yet can have a slower latency than the latter due to NasNet-A's adopted hardware-unfriendly structure.</p><p>It is thus imperative to address the aforementioned challenges in order to make HW-NAS more accessible and reproducible to unfold HW-NAS's full potential. Note that although pioneering NAS benchmark datasets <ref type="bibr" target="#b52">(Ying et al., 2019;</ref><ref type="bibr" target="#b12">Dong &amp; Yang, 2020;</ref><ref type="bibr" target="#b24">Klyuchnikov et al., 2020;</ref><ref type="bibr" target="#b37">Siems et al., 2020;</ref><ref type="bibr">Dong et al., 2020)</ref> have made a significant step towards providing a unified benchmark dataset for generic NAS works, they all either merely provide latency on server-level GPUs (e.g., GTX 1080Ti) or do not provide hardware-cost metrics on real hardware, limiting their applicability to HW-NAS <ref type="bibr" target="#b45">(Wu et al., 2019;</ref><ref type="bibr" target="#b42">Wan et al., 2020;</ref><ref type="bibr" target="#b5">Cai et al., 2018)</ref> which primarily targets commercial edge devices, FPGA, and ASIC. To this end, as shown in Figure <ref type="figure" target="#fig_0">1</ref>, we develop HW-NAS-Bench and make the following contributions in this paper:</p><p>• We have developed HW-NAS-Bench, the first public dataset for HW-NAS research aiming to (1) democratize HW-NAS research to non-hardware experts and (2) provide a unified benchmark for HW-NAS to make HW-NAS research more reproducible and accessible, covering two SOTA NAS search spaces including NAS-Bench-201 and FBNet, with the former being the most popular NAS search space and the latter having been shown to be the most hardware friendly. • We provide hardware-cost data collection pipelines for six commonly used hardware devices that fall into three categories (i.e., commercial edge devices, FPGA, and ASIC), in addition to the measured/estimated hardware-cost (e.g., energy cost and latency) on these devices for all the networks in the search space of both NAS-Bench-201 and FBNet. • We conduct comprehensive analysis of the collected data in HW-NAS-Bench, such as studying the correlation between collected hardware-cost and accuracy-cost trade-offs of all the networks on the six hardware devices, which can provide insights to not only HW-NAS researchers but also DNN accelerator designers. Other researchers can extract other useful insights from HW-NAS-Bench that have not been discussed in this work. • We demonstrate exemplary user cases to show (1) how HW-NAS-Bench can be easily used by non-hardware experts to develop HW-NAS solutions by simply querying the collected data in our HW-NAS-Bench and (2) that dedicated device-specific HW-NAS can indeed often provide optimal accuracy-cost trade-offs, demonstrating the great necessity of HW-NAS benchmark frameworks like our proposed HW-NAS-Bench.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">HARDWARE-AWARE NEURAL ARCHITECTURE SEARCH</head><p>Driven by the growing demand for efficient DNN solutions, HW-NAS has been proposed to automate the search for efficient DNN structures under the target efficiency constraints. For example, <ref type="bibr">(Tan et al., 2019;</ref><ref type="bibr" target="#b23">Howard et al., 2019;</ref><ref type="bibr">Tan &amp; Le, 2019)</ref> adopts reinforcement learning based NAS with a multi-objective reward consisting of both task performance and efficiency, achieving promising results yet suffering from prohibitive search time/cost. In parallel, <ref type="bibr" target="#b45">(Wu et al., 2019;</ref><ref type="bibr" target="#b42">Wan et al., 2020;</ref><ref type="bibr" target="#b5">Cai et al., 2018;</ref><ref type="bibr" target="#b38">Stamoulis et al., 2019)</ref> explore the design space in a differentiable manner following <ref type="bibr" target="#b27">(Liu et al., 2018)</ref> and significantly improve the search efficiency. The promising performance of HW-NAS has motivated a tremendous interest in applying it to more diverse applications <ref type="bibr" target="#b14">(Fu et al., 2020;</ref><ref type="bibr" target="#b6">Chen et al., 2019;</ref><ref type="bibr" target="#b29">Marchisio et al., 2020)</ref> paired with target hardware devices, e.g., EdgeTPU <ref type="bibr" target="#b49">(Xiong et al., 2020)</ref> and NPU <ref type="bibr">(Lee et al., 2020)</ref>, in addition to the widely explored mobile phones.</p><p>As discussed in <ref type="bibr" target="#b11">(Chu et al., 2020)</ref>, different hardware devices can favor very different network structures under the same hardware efficiency metric, and the optimal network structure can differ significantly when considering different application driven hardware efficiency metrics on the same hardware devices. As such, it would ideally lead to the optimal accuracy-efficiency trade-offs if the HW-NAS design is dedicated for the target device and hardware efficiency metrics. However, this requires a good understanding of both device-specific compilation and hardware-cost characterization, imposing a barrier-to-entry to non-hardware experts, such as many NAS researchers, and thus limiting the development of optimal HW-NAS results for numerous applications, each of which often corresponds to a different application driven hardware efficiency and adopts different types of hardware devices. As such, our proposed HW-NAS-Bench will make HW-NAS more friendly to NAS researchers, who are often non-hardware experts, as it consists of comprehensive hardwarecost data in a wide range of hardware devices for all the networks in two SOTA NAS search spaces, expediting the development of HW-NAS innovations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">NEURAL ARCHITECTURE SEARCH BENCHMARKS.</head><p>The importance and difficulty of NAS reproducibility and benchmarking has recently gained increasing attention. Pioneering efforts include <ref type="bibr" target="#b52">(Ying et al., 2019;</ref><ref type="bibr" target="#b12">Dong &amp; Yang, 2020;</ref><ref type="bibr" target="#b24">Klyuchnikov et al., 2020;</ref><ref type="bibr" target="#b37">Siems et al., 2020;</ref><ref type="bibr">Dong et al., 2020)</ref>. Specifically, NAS-Bench-101 <ref type="bibr" target="#b52">(Ying et al., 2019)</ref> develops the first large-scale, open-source architecture dataset for NAS, in which they provide the ground truth test accuracy on CIFAR-10 ( <ref type="bibr" target="#b25">Krizhevsky et al., 2009</ref>) of all the architectures (i.e., 423k) in their search space. Later, NAS-Bench-201 <ref type="bibr" target="#b12">(Dong &amp; Yang, 2020)</ref> further extends NAS-Bench-101 to support more NAS algorithm categories (e.g., differentiable algorithms) and more datasets (e.g., CIFAR-100 <ref type="bibr" target="#b25">(Krizhevsky et al., 2009)</ref> and ImageNet16-120 <ref type="bibr" target="#b10">(Chrabaszcz et al., 2017)</ref>). Most recently, NAS-Bench-301 <ref type="bibr" target="#b37">(Siems et al., 2020)</ref> and NATS-Bench <ref type="bibr">(Dong et al., 2020)</ref> were developed to support benchmarking on larger search spaces. However, all of these works either merely provide latency on the server-level GPU (e.g., GTX 1080Ti) or do not consider hardware efficiency metrics on real hardware, limiting their applicability to HW-NAS <ref type="bibr" target="#b45">(Wu et al., 2019;</ref><ref type="bibr" target="#b42">Wan et al., 2020;</ref><ref type="bibr" target="#b5">Cai et al., 2018)</ref> that primarily target the commercial edge devices, FPGA, and ASIC. This has motivated us to develop our HW-NAS-Bench, aiming to make HW-NAS more accessible and reproducible.</p><p>3 THE PROPOSED HW-NAS-BENCH FRAMEWORK</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">HW-NAS-BENCH'S CONSIDERED SEARCH SPACES</head><p>To ensure a wide applicability, our HW-NAS-Bench considers two representative NAS search spaces: (1) NAS-Bench-201's cell-based search space and (2) FBNet search space. Both contributes valuable aspects to ensure our goal of constructing a comprehensive HW-NAS benchmark framework. Specifically, the former enables HW-NAS-Bench to naturally integrate the ground truth ac-curacy data of all NAS-Bench-201's considered network architectures, while the latter ensures that HW-NAS-Bench includes the most commonly recognized hardware friendly search space.</p><p>NAS-Bench-201 Search Space. Inspired from the search space used in the most popular cell-based NAS, NAS-Bench-201 adopts a fixed cell search space, where each architecture consists of a predefined skeleton with a stack of the searched cell that is represented as a densely-connected directed acyclic graph (DAG). Specifically, it considers 4 nodes and 5 representative operation candidates for the operation set, and varies the feature map sizes and the dimensions of the final Fully-connected layers to handle its considered three datasets (i.e., CIFAR-10, CIFAR-100 <ref type="bibr" target="#b25">(Krizhevsky et al., 2009)</ref>, and ImageNet16-120 <ref type="bibr" target="#b10">(Chrabaszcz et al., 2017)</ref>), leading to a total of 3 × 5 6 = 46875 architectures. Training log and accuracy are provided for each architecture. However, NAS-Bench-201 can not be directly used for HW-NAS as it only includes theoretical cost metrics (i.e., FLOPs and the number of parameters (#Params)) and latency on a server-type GPU (i.e., GTX 1080Ti). HW-NAS-Bench enhances NAS-Bench-201 by providing all the 46875 architectures' measured/estimated hardware-cost on six hardware devices, that are primarily targeted by SOTA HW-NAS works.</p><p>FBNet Search Space. FBNet <ref type="bibr" target="#b45">(Wu et al., 2019)</ref> constructs a layer-wise search space with a fixed macro-architecture, which defines the number of layers and the input/output dimensions of each layer and fixes first and the last three layers with the remaining layers to be searched. In this way, networks in FBNet <ref type="bibr" target="#b45">(Wu et al., 2019)</ref> search space have more regular structure than those in NAS-Bench-201, and have been shown to be more hardware friendly <ref type="bibr" target="#b14">(Fu et al., 2020;</ref><ref type="bibr" target="#b28">Ma et al., 2018)</ref>. The 9 considered re-defined cell candidates and 22 unique positions lead to a total of 9 22 ≈ 10 21 unique architectures. While HW-NAS researchers can develop their search algorithms on top of FBNet <ref type="bibr" target="#b45">(Wu et al., 2019)</ref> search space, tedious efforts are required to collect the hardware-cost look-up tables or models on each target devices. HW-NAS-Bench provides measured/estimated hardware-cost on six hardware devices for all the 10 21 architectures of FBNet search space, aiming to make HW-NAS research more friendly to non-hardware experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">HARDWARE-COST COLLECTION PIPELINE AND THE CONSIDERED DEVICES</head><p>To collect hardware-cost data for all the architectures in both NAS-Bench-201 and FBNet spaces, we construct a generic hardware-cost collection pipeline (see Figure <ref type="figure">2</ref>) to automate the process. Specifically, the pipeline mainly consists of the target devices and corresponding implementation tools (e.g., compilers), where it takes all the networks as its inputs, compile the networks to (1) convert them into the device's required execution format and (2) optimize the execution flow, the latter of which aims to optimize the hardware performance on the target devices. For example, for collecting hardware-cost in Edge GPU, we first set the device in the Max-N mode to fully make use of all available resources following <ref type="bibr" target="#b44">(Wofk et al., 2019)</ref>, and then set up the embedded power rail monitor (Texas Instruments Inc.) to obtain real-measured latency and energy via sysfs (Patrick Mochel and Mike Murphy.) averaged over 50 runs. We can see that the hardware-cost collection process requires various hardware domain knowledge including machine learning development frameworks, device compilation, embedded systems, and device measurements.</p><p>Next, we briefly introduce the six considered hardware devices and the specific configuration required to collect hardware-cost data using each device.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Possible Architectures Efficiency Performance</head><p>Latency Energy   <ref type="formula">2</ref>) optimize the implementation using the official interpreter <ref type="bibr" target="#b22">(Google LLC., 2020)</ref> in Raspi 4, which will be configured in advance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HW-NAS-Bench</head><p>Edge TPU: Edge TPU Dev Board (Google LLC., a) is a dedicated ASIC accelerator developed by Google for targeting Artificial Intelligence (AI) inference for edge applications. Similar to the case when using Raspi 4, all the architectures are converted into TFLite format. After that, an Edge TPU compiler will be used to convert the pre-built TFLite model into a more compressed format which is compatible to the pre-configured Edge TPU runtime environment in the Dev Board.</p><p>Pixel 3: Pixel 3 is one of the latest Pixel mobile phones (Google LLC., e) that are widely used as the target platform by recent NAS works <ref type="bibr" target="#b49">(Xiong et al., 2020;</ref><ref type="bibr" target="#b23">Howard et al., 2019;</ref><ref type="bibr">Tan et al., 2019)</ref>. To collect hardware-cost in Pixel 3, we first convert all the architectures into TFLite, then use TFLite's official benchmark binary files to obtain the latency, when configuring the Pixel 3 to only use big cores for reducing the measurement variance as in <ref type="bibr" target="#b49">(Xiong et al., 2020;</ref><ref type="bibr">Tan et al., 2019)</ref>.</p><p>ASIC-Eyeriss: For collecting hardware-cost data in ASIC, we consider a SOTA ASIC accelerator <ref type="bibr">(Chen et al., 2016)</ref>. Specifically, we adopt SOTA ASIC accelerator performance simulators (1) Accelergy <ref type="bibr" target="#b45">(Wu et al., 2019)</ref>+Timeloop <ref type="bibr" target="#b31">(Parashar et al., 2019)</ref> and (2) DNN-Chip Predictor <ref type="bibr">(Zhao et al., 2020)</ref>, which automatically identify the optimal algorithm-to-hardware mapping methods for each architecture and then provide the estimated hardware-cost when being executed in Eyeriss.</p><p>FPGA: FPGA is a widely adopted AI acceleration platform featuring higher hardware flexibility and decent hardware efficiency. To collect hardware-cost in this platform, we first develop a SOTA chunk based pipeline structure <ref type="bibr">(Zhang et al., 2018;</ref><ref type="bibr" target="#b35">Shen et al., 2017)</ref> implementation, compile all the architectures using standard Vivado HLS toolflow (Xilinx Inc., a), and then obtain the hardware-cost on a Xilinx ZC706 board with Zynq XC7045 SoC (Xilinx Inc., b).</p><p>More details about the pipeline for each device are provided in the Appendix D.</p><p>Note that to estimated the hardware-cost of networks in the FBNet search space <ref type="bibr" target="#b45">(Wu et al., 2019)</ref> when being executed on the commercial category of edge devices (i.e., edge GPU, Raspi 4, edge TPU, and Pixel 3) in our HW-NAS-Bench, we sum up hardware-cost of all unique blocks (i.e., "block" in the FBNet space <ref type="bibr" target="#b45">(Wu et al., 2019)</ref>) within the network architectures. To validate that such an approximation is close to real-measured results, we conduct the experiments summarized in Table <ref type="table" target="#tab_1">2</ref>, which calculates two types of correlation coefficients between the measured and the approximated hardware-cost based on 100 randomly sampled architectures from the FBNet search space. We can see that our approximated costs are highly correlated with the real-measured ones, except for the case on EdgeTPU, which we conjecture is caused by our adopted in-house EdgeTPU compiler <ref type="bibr">(Google LLC., c)</ref>. More visualization results can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ANALYSIS ON HW-NAS-BENCH</head><p>In this section, we provide analysis and visualization of the hardware-cost and corresponding accuracy data (the latter only for architectures in NAS-Bench-201) for all the architectures in the two considered search spaces. Specifically, our analysis and visualization confirm that (1) commonly used theoretical hardware-cost metrics such as FLOPs do not correlate well with measured/estimated hardware-cost;</p><p>(2) hardware-cost of the same architectures can differ a lot when executed on different devices; and (3) device-specific HW-NAS is necessary because optimal architectures resulting from HW-NAS targeting on one device can perform poorly in terms of hardware efficiency when being executed on another device.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CORRELATION BETWEEN COLLECTED HARDWARE-COST AND THEORETICAL ONES</head><p>To confirm whether commonly used theoretical hardware-cost metrics align with real measured/estimated ones, we summarize the calculated correlation between the collected hardware-cost in our HW-NAS-Bench and the theoretical metrics (i.e., FLOPs and #Params), based on data for all the architectures in both search spaces on all the six considered hardware devices where a total of four different datasets are involved.</p><p>As summarized in Tables <ref type="table" target="#tab_3">3 -4</ref>, commonly used theoretical hardware-cost metrics (i.e., FLOPs and #Params) do not always correlate well with measured/estimated hardware-cost for the architectures in both the NAS-Bench-201 and FBNet spaces. For example, there exists at least one coefficient &lt;0.5 on all devices, especially for the cases with measured hardware-cost on commonly considered edge platforms including Edge GPU, Edge TPU, and ASIC-Eyeriss. As such, HW-NAS based on theoretical hardware-cost might lead to sub-optimal results, motivating HW-NAS benchmark frameworks like our HW-NAS-Bench. Note that we consider the Kendall Rank Correlation Coefficients <ref type="bibr" target="#b1">(Abdi, 2007)</ref>, which is a commonly used correlation coefficient in both recent NAS and NAS benchmark frameworks <ref type="bibr" target="#b53">(You et al., 2020;</ref><ref type="bibr" target="#b37">Siems et al., 2020;</ref><ref type="bibr" target="#b50">Yang et al., 2020)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CORRELATION AMONG COLLECTED HARDWARE-COST ON DIFFERENT DEVICES</head><p>To check how much the hardware-cost of the same architectures on different devices correlate, we visualize the correlation between the hardware-cost collected from every two paired devices base on data for all the architectures on both the NAS-Bench-201 and FBNet search spaces with each of the architecture being associated with 9 different hardware-cost metrics.</p><p>The visualization in Figures <ref type="figure">3 -4</ref> indicates that hardware-cost of the same network architectures can differ a lot when being executed on different devices. More specifically, the correlation coefficients can be as small as -0.00 (e.g., Edge GPU latency vs. ASIC-Eyeriss energy), which is a result of the large difference in (1) their underlying hardware architectures and (2) the amount of available resources. Thus, the resulting architecture of HW-NAS targeting one device might perform poorly when being executed on other device, motivating device-specific HW-NAS, and it is crucial to develop comprehensive hardware-cost datasets like our HW-NAS-Bench to enable fast development and optimal results of HW-NAS for different applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">OPTIMAL ARCHITECTURES ON DIFFERENT HARDWARE DEVICES</head><p>To confirm the necessity of performing device-specific HW-NAS from another perspective, we summarize all architectures' test accuracy vs. hardware-cost on ImageNet16-120 of NAS-Bench-201 and focus on the architectures with the optimal accuracy-cost trade-offs.</p><p>As shown in Figure <ref type="figure">5</ref>, such optimal architectures for different devices are not the same. For example, the optimal architectures on Edge GPU (marked as red points) can perform poorly in terms of hardware efficiency in other devices, especially in ASIC-Eyeriss and Edge TPU whose hardware-cost have exactly the smallest correlation coefficient with the hardware-cost measured in the Edge GPU, which is shown in Figure <ref type="figure">3</ref>. Again, this set of analysis and visualization confirms that HW-NAS targeting on one device can perform poorly in terms of hardware efficiency when being executed on another device, thus motivating the necessity of device-specific HW-NAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">USER CASE: BENCHMARK SOTA HW-NAS ALGORITHMS</head><p>In this section, we will demonstrate the user cases of our HW-NAS-Bench to show (1) how nonhardware experts can use it to develop HW-NAS solutions by simply querying the hardware-cost data (2) dedicated device-specific HW-NAS can indeed often lead to optimal accuracy-cost tradeoffs, again showing the need for HW-NAS benchmark frameworks like our HW-NAS-Bench to enable more optimal HW-NAS solutions via device-specific HW-NAS.</p><p>Benchmark Setting. We adopt a SOTA HW-NAS algorithm, ProxylessNAS <ref type="bibr" target="#b5">(Cai et al., 2018)</ref> for this experiment. As an example to use our HW-NAS-Bench, we use ProxylessNAS to search over the FBNet <ref type="bibr" target="#b45">(Wu et al., 2019)</ref> space on CIFAR-100 <ref type="bibr" target="#b25">(Krizhevsky et al., 2009)</ref>, when targeting different devices in our HW-NAS-Bench by simply querying the corresponding device's measured/estimated hardware-cost, which has negligible overhead as compared with the NAS algorithm itself, without the need for hardware expertise or knowledge during the whole HW-NAS.  Table <ref type="table" target="#tab_4">5</ref> illustrates that the searched architectures achieve the lowest latency among all architectures, when the target devices of HW-NAS are the same as the one used to measure the architecture's ondevice inference latency. Specifically, when being executed on Edge GPU, the architecture targeting on Raspi 4 during HW-NAS leads to about a 50% higher latency while the architecture targeting on FPGA during HW-NAS introduces over a 100% higher latency, than the architecture specifically target on Edge GPU during HW-NAS, under the same inference accuracy. This set of experiment show that non-hardware experts can easily use our HW-NAS-Bench to develop optimal HW-NAS solutions, while demonstrating the need of device-specific HW-NAS solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We have developed HW-NAS-Bench, the first public dataset for HW-NAS research aiming to (1) democratize HW-NAS research to non-hardware experts and (2) provide a unified benchmark for HW-NAS to make HW-NAS research more reproducible and accessible. Our HW-NAS-Bench covers two representative NAS search spaces, provides all network architectures' hardware-cost data on six commonly used hardware devices that fall into three categories (i.e., commercial edge devices, FPGA, and ASIC). Furthermore, we conduct comprehensive analysis of the collected data in HW-NAS-Bench, aiming to provide insights to not only HW-NAS researchers but also DNN accelerator designers. Finally, we demonstrate exemplary user cases of HW-NAS-Bench to show (1) how HW-NAS-Bench can be easily used by non-hardware experts to develop HW-NAS solutions by simply querying the collected data and (2) that dedicated device-specific HW-NAS can indeed often provide optimal accuracy-cost trade-offs, demonstrating the great necessity of HW-NAS benchmark frameworks like our proposed HW-NAS-Bench.   <ref type="bibr" target="#b4">(Benesty et al., 2009)</ref>.</p><p>Fig. <ref type="figure" target="#fig_3">6</ref> shows a comparison between the approximated and measured hardware-cost of randomly sampled 100 architectures when being executed on commercial edge devices using the ImageNet and CIFAR-100 datasets, which verifies that our approximation of summing up the performance of the unique blocks is a simple yet quite accurate for providing the hardware-cost for networks in the FBNet space, consistent with our observation in Table <ref type="table" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B COMPARING THE PERDITION OF EYERISS USING ACCELERGY+TIMELOOP AND DNN-CHIP-PREDICTOR</head><p>Both Accelergy <ref type="bibr" target="#b45">(Wu et al., 2019)</ref>+Timeloop <ref type="bibr" target="#b31">(Parashar et al., 2019)</ref> and <ref type="bibr">DNN-Chip Predictor (Zhao et al., 2020)</ref> are able to simulate the latency and energy cost of Eyeriss <ref type="bibr">(Chen et al., 2016)</ref>, a SOTA ASIC design, when giving the network architectures. From Table <ref type="table" target="#tab_6">6</ref>, they nearly give the same estimation of the latency and energy cost (i.e. the mean of differences is 6.096%, the standard deviation of the differences is 0.779%, the Pearson correlation coefficient is 0.9998, and the Kendall Rank correlation coefficient is 0.9633, in term of the average performance) when benchmarked in 3 datasets of NAS-Bench-201. Therefore, we use the average value of their predictions as the estimated Eyeriss latency and energy in our proposed HW-NAS-Bench. We demonstrate our modification on FBNet search space when benchmarking on CIFAR-100 (i.e., the setting in Section 5) via comparing the marco-architecture before and after such modification in Table <ref type="table" target="#tab_7">7</ref>. NVIDIA Edge GPU Jetson TX2 (Edge GPU) (NVIDIA Inc., a) is a commonly used commercial edge device, consisting of a quad-core Arm Cortex-A57, a dual-core NVIDIA Denver2, a 256-core Pascal GPU, and a 8GB 128-bit LPDDR4, for various deep learning applications including classification <ref type="bibr" target="#b26">(Li et al., 2020)</ref>, segmentation <ref type="bibr" target="#b36">(Siam et al., 2018)</ref>, and depth estimation <ref type="bibr" target="#b44">(Wofk et al., 2019)</ref>, targeting IoT and self-driving environments. Although widely-used TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref> and PyTorch <ref type="bibr" target="#b32">(Paszke et al., 2019)</ref> can be used in Edge GPU directly, to achieve faster inference, Ten-sorRT (NVIDIA Inc., b), a C++ library for high-performance inference on NVIDIA GPUs, is more commonly used as the runtime environment in Edge GPU when only benchmarking inference <ref type="bibr" target="#b43">(Wang et al., 2019;</ref><ref type="bibr">NVIDIA Inc., c)</ref>.</p><p>Therefore, we pre-set the Edge GPU in the max-N mode to make full use of the resource on it following <ref type="bibr" target="#b44">(Wofk et al., 2019)</ref>. When plugging Edge GPU into the hardware-cost collection pipeline, we first compile the PyTorch implementations of network architectures in both NAS-Bench-201 and FBNet spaces to TensorRT format models. In this way, we can benefit from the optimized inference implementation within TensorRF runtime environment. And then we can benchmark the architectures in Edge GPU to further measure the energy and latency using the sysfs (Patrick Mochel and Mike Murphy.) of the embedded INA3221 (Texas Instruments Inc.) power rails monitor.</p><p>ergy cost and latency characterization through an integrated mapper that finds the optimal mapping for such layer when being executed in Eyeriss. The inputs to DNN-Chip Predictor are the same as Accelergy+Timeloop, except that we can set the optimization metric as energy/latency/energy-delay product. DNN-Chip Predictor schedules mapping for the optimization metric and generates the estimated hardware-cost. We report the average prediction from the two as the estimated hardware-cost of Eyeriss, and more details can be found in Appendix B.</p><p>D.6 COLLECT PERFORMANCE ON FPGA:</p><p>FPGA is a widely adopted AI acceleration platform which can offer higher flexibility in terms of hardware for accelerating AI applications. For collecting hardware-cost data in FPGA, we construct a SOTA chunk based pipeline structure <ref type="bibr">(Zhang et al., 2018;</ref><ref type="bibr" target="#b35">Shen et al., 2017)</ref> as our FPGA implementation. By configuring multiple sub-accelerators (chunks) and assigning different layers to different sub-accelerators(chunks), we can balance the throughput and hardware resource consumption. To further free up our implantation's potential to reach the performance frontier across different architectures, we additionally configure hardware settings such as the number of PEs, interconnection method of PEs, and tiling/scheduling of the operations, which are commonly adopted by FPGA accelerators <ref type="bibr" target="#b7">(Chen et al., 2017;</ref><ref type="bibr" target="#b54">Zhang et al., 2015;</ref><ref type="bibr" target="#b51">Yang et al., 2016)</ref>. We then compile all the architectures using standard Vivado HLS toolflow (Xilinx Inc., a) and obtain the bottleneck latency, the maximum latency across all sub-accelerators (chunks) of the architectures on a Xilinx ZC706 development board with Zynq XC7045 SoC (Xilinx Inc., b).</p><p>To verify our implementation, we compare our implementation's performance with SOTA FPGA accelerators <ref type="bibr">(Zhang et al., 2018;</ref><ref type="bibr" target="#b47">Xiao et al., 2017)</ref> given the same architecture and dataset as shown in Table <ref type="table" target="#tab_8">8</ref>. We can see that our implementation has SOTA performance and thus provides insightful and trusted hardware-cost estimation for the HW-NAS-Bench. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The overview of our proposed HW-NAS-Bench</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2: Illustrating the hardware-cost collection pipeline applicable to various hardware devices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :Figure 5 :</head><label>345</label><figDesc>Figure 3: Kendall Rank Correlation Coefficient between real measured/estimated hardware-cost in different devices considering the NAS-Bench-201 search space.</figDesc><graphic url="image-19.png" coords="7,157.24,600.26,90.52,90.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Comparison between the approximated and measured hardware-cost on CIFAR-100 (Top) and ImageNet (Bottom), where the red line indicates the fitting line for all the measured data, and R 2 represents the square of the Pearson Correlation Coefficient<ref type="bibr" target="#b4">(Benesty et al., 2009)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Important details about the six hardware devices considered by our HW-NAS-Bench NVIDIA Edge GPU Jetson TX2 (Edge GPU) is a commercial device with a 256-core Pascal GPU and a 8GB LPDDR4 targeting IoT applications (NVIDIA Inc., a). When plugging Edge GPU into the above hardware-cost collection pipeline, we first compile the network architectures in both NAS-Bench-201 and FBNet spaces to (1) convert them to the TensorRT format and (2) optimize the inference implementation within NVIDIA's recommended TensorRT runtime environment, and then execute them in Edge GPU to measure the consumed energy and latency.</figDesc><table><row><cell>Devices</cell><cell>Edge GPU</cell><cell>Raspi 4</cell><cell>Edge TPU</cell><cell>Pixel 3</cell><cell>ASIC-Eyeriss</cell><cell>FPGA</cell></row><row><cell>Collected Metrics</cell><cell cols="4">Latency (ms) Latency (ms) Latency (ms) Latency (ms) Energy (mJ)</cell><cell>Latency (ms) Energy (mJ)</cell><cell>Latency (ms) Energy (mJ)</cell></row><row><cell>Collecting Method</cell><cell>Measured</cell><cell>Measured</cell><cell>Measured</cell><cell>Measured</cell><cell>Estimated</cell><cell>Estimated</cell></row><row><cell>Runtime Environment</cell><cell>TensorRT</cell><cell>TensorFlow Lite</cell><cell>Edge TPU Runtime</cell><cell cols="2">TensorFlow Accelergy+Timeloop / Lite DNN-Chip Predictor</cell><cell>Vivado HLS</cell></row><row><cell>Customizing Hardware?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Category</cell><cell></cell><cell cols="2">Commercial Edge Devices</cell><cell></cell><cell>ASIC</cell><cell>FPGA</cell></row><row><cell>Edge GPU:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Raspi 4: Raspberry Pi 4 (Raspi 4) is the latest Raspberry Pi device (Raspberry Pi Limited.), consisting of a Broadcom BCM2711 SoC and a 4GB LPDDR4. To collect hardware-cost operating on it, we compile the architecture candidates to (1) convert them into TensorFlow Lite (TFLite)<ref type="bibr" target="#b0">(Abadi et al., 2016)</ref> format (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Two types of correlation coefficients (larger means more related) between the measured hardware-cost of the whole architectures and the approximated hardware-cost based on 100 randomly sampled architectures from the FBNet search space.</figDesc><table><row><cell>Correlation Coefficient Types</cell><cell>Datasets</cell><cell cols="5">Latency on Energy on Latency on Latency on Latency on Edge GPU Edge GPU Raspi 4 Edge TPU Pixel 3</cell></row><row><cell>Pearson Correlation Coefficient</cell><cell>CIFAR-100 ImageNet</cell><cell>0.9200 0.8634</cell><cell>0.9116 0.9640</cell><cell>0.9219 0.9897</cell><cell>0.4935 0.7153</cell><cell>0.9324 0.9162</cell></row><row><cell>Kendall Rank Correlation Coefficient</cell><cell>CIFAR-100 ImageNet</cell><cell>0.7373 0.7111</cell><cell>0.7240 0.8379</cell><cell>0.7470 0.9163</cell><cell>0.3551 0.5806</cell><cell>0.8593 0.8064</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Kendall Rank Correlation Coefficient between real measured/estimated hardware-cost and theoretical ones considering the NAS-Bench-201 search space, where coefficients &lt;0.5 are bolded.</figDesc><table><row><cell>Dataset</cell><cell>Metrics</cell><cell cols="4">Edge GPU Latency Energy Latency Raspi 4 Edge TPU Pixel 3 Latency Latency Latency Energy Latency Energy ASIC-Eyeriss FPGA</cell></row><row><cell>CIFAR-10</cell><cell>FLOPs #Params</cell><cell>0.3571 0.4064 0.7394 0.3571 0.4064 0.7394</cell><cell>0.1847 0.1847</cell><cell>0.6823 0.6823</cell><cell>0.4178 0.5359 0.8313 0.8313 0.4178 0.5359 0.8313 0.8313</cell></row><row><cell>CIFAR-100</cell><cell>FLOPs #Params</cell><cell>0.3589 0.4073 0.7384 0.3589 0.4073 0.7384</cell><cell>0.1851 0.1851</cell><cell>0.6844 0.6844</cell><cell>0.4197 0.5360 0.8313 0.8313 0.4197 0.5360 0.8313 0.8313</cell></row><row><cell>ImageNet16-120</cell><cell>FLOPs #Params</cell><cell>0.3544 0.3868 0.6303 0.3544 0.3868 0.6303</cell><cell>0.2635 0.2635</cell><cell>0.7017 0.7017</cell><cell>0.4166 0.5363 0.9205 0.9205 0.4166 0.5363 0.9205 0.9205</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Kendall Rank Correlation Coefficient between real measured/estimated hardware-cost and theoretical ones considering the FBNet search space, where coefficients &lt;0.5 are bolded.</figDesc><table><row><cell>Dataset</cell><cell>Metrics</cell><cell cols="3">Edge GPU Latency Energy Latency Latency Latency Energy Latency Energy Raspi 4 Pixel 3 ASIC-Eyeriss FPGA</cell></row><row><cell>CIFAR-100</cell><cell cols="2">FLOPs #Params -0.0733 0.0202 0.4910 0.0149 0.1564 0.7713</cell><cell>0.8092 0.3734</cell><cell>0.8490 0.7854 0.8710 0.8710 0.4297 0.6455 0.5151 0.5151</cell></row><row><cell>ImageNet</cell><cell>FLOPs #Params</cell><cell>0.4633 0.6094 0.7531 0.0985 0.1840 0.2318</cell><cell>0.7678 0.2357</cell><cell>0.8935 0.7970 0.8643 0.8643 0.3202 0.4140 0.4198 0.4198</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Inference performance comparison of optimal architectures resulting from HW-NAS-Bench when targeting different hardware devices.</figDesc><table><row><cell cols="2">Targeting Device Top-1 Acc.(%) in HW-NAS</cell><cell cols="3">Latency on Edge GPU (ms) Raspi 4 (ms) FPGA (ms) Latency on Latency on</cell></row><row><cell>Edge GPU</cell><cell>74.11</cell><cell>9.96</cell><cell>31.01</cell><cell>20.19</cell></row><row><cell>Raspi 4</cell><cell>73.46</cell><cell>13.88</cell><cell>22.91</cell><cell>15.39</cell></row><row><cell>FPGA</cell><cell>73.51</cell><cell>20.65</cell><cell>25.43</cell><cell>13.96</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Measured Edge GPU Latency Measured Edge GPU Energy Measured Raspi 4 Latency Measured Edge TPU Latency Measured Pixel 3 Latency Measured Edge GPU Latency Measured Edge GPU Energy Measured Raspi 4 Latency Measured Edge TPU Latency Measured Pixel 3 Latency Approximated Edge GPU Latency Approximated Raspi 4 Latency Approximated Pixel 3 Latency Approximated Edge GPU Energy Approximated Edge TPU Latency Approximated Edge GPU Latency Approximated Raspi 4 Latency Approximated Pixel 3 Latency Approximated Edge GPU Energy Approximated Edge TPU Latency ImageNet CIFAR-100</head><label></label><figDesc>Xiaofan Zhang, Junsong Wang, Chao Zhu, Yonghua Lin, Jinjun Xiong, Wen-mei Hwu, and Deming Chen. Dnnbuilder: An automated tool for building high-performance dnn hardware accelerators for fpgas. In Proceedings of the International Conference on Computer-Aided Design, ICCAD '18, New York, NY, USA, 2018. Association for Computing Machinery. ISBN 9781450359504. doi: 10.1145/3240765.3240801. URL https://doi.org/10.1145/ 3240765.3240801.</figDesc><table><row><cell>Cheah Wai Zhao, Jayanand Jegatheesan, and Son Chee Loon. Exploring iot application using rasp-</cell></row><row><cell>berry pi. International Journal of Computer Networks and Applications, 2(1):27-34, 2015.</cell></row><row><cell>Y. Zhao, C. Li, Y. Wang, P. Xu, Y. Zhang, and Y. Lin. Dnn-chip predictor: An analytical performance</cell></row><row><cell>predictor for dnn accelerators with various dataflows and hardware architectures. In ICASSP 2020</cell></row><row><cell>-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp.</cell></row><row><cell>1593-1597, 2020.</cell></row><row><cell>A MORE VISUALIZATION ON MEASURED HARDWARE-COST FOR THE</cell></row><row><cell>FBNET SEARCH SPACE</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>The differences of estimation given by Accelergy<ref type="bibr" target="#b45">(Wu et al., 2019)</ref>+Timeloop<ref type="bibr" target="#b31">(Parashar et al., 2019)</ref> andDNN-Chip Predictor (Zhao et al., 2020)  in 3 datasets of </figDesc><table><row><cell>Datasets</cell><cell>Hardware-cost</cell><cell>Mean of Differences</cell><cell cols="2">Standard Deviation of Pearson Correlation Differences Coefficient TPU</cell><cell>Kendall Rank Correlation Coefficient</cell></row><row><cell>CIFAR-10</cell><cell>Latency Energy</cell><cell>1.648% 10.96%</cell><cell>0.642% 1.035%</cell><cell>0.9999 0.9997</cell><cell>0.9888 0.9374</cell></row><row><cell>CIFAR-100</cell><cell>Latency Energy</cell><cell>1.572% 10.93%</cell><cell>0.611% 1.029%</cell><cell>1.0000 0.9997</cell><cell>0.9888 0.9374</cell></row><row><cell>ImageNet16-120</cell><cell>Latency Energy</cell><cell>1.338% 10.13%</cell><cell>0.520% 0.840%</cell><cell>0.9999 0.9998</cell><cell>0.9888 0.9388</cell></row><row><cell cols="2">Average Performance</cell><cell>6.096%</cell><cell>0.779%</cell><cell>0.9998</cell><cell>0.9633</cell></row><row><cell cols="6">C MINOR MODIFICATIONS ON FBNET SEARCH SPACE WHEN</cell></row><row><cell cols="4">BENCHMARKING ON CIFAR-100</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Left: the marco-architecture of the search space proposed in the FBNet<ref type="bibr" target="#b45">(Wu et al., 2019)</ref> for ImageNet classification. Right: our modified search space to fit the input image size in CIFAR-100 dataset. In the tables, "TBS" means the layer type needs to be searched, Stride denotes the stride of the first block in the stage. The modified parameters are emphasized as bold characters.</figDesc><table><row><cell>Input Shape</cell><cell>Block</cell><cell cols="3">Filter# Block# Stride</cell><cell>Input Shape</cell><cell>Block</cell><cell cols="3">Filter# Block# Stride</cell></row><row><cell>224 2 × 3</cell><cell>3 × 3 conv</cell><cell>16</cell><cell>1</cell><cell>2</cell><cell>32 2 × 3</cell><cell>3 × 3 conv</cell><cell>16</cell><cell>1</cell><cell>1</cell></row><row><cell>112 2 × 16</cell><cell>TBS</cell><cell>16</cell><cell>1</cell><cell>1</cell><cell>32 2 × 16</cell><cell>TBS</cell><cell>16</cell><cell>1</cell><cell>1</cell></row><row><cell>112 2 × 16</cell><cell>TBS</cell><cell>24</cell><cell>4</cell><cell>2</cell><cell>32 2 × 16</cell><cell>TBS</cell><cell>24</cell><cell>4</cell><cell>1</cell></row><row><cell>56 2 × 24</cell><cell>TBS</cell><cell>32</cell><cell>4</cell><cell>2</cell><cell>32 2 × 24</cell><cell>TBS</cell><cell>32</cell><cell>4</cell><cell>2</cell></row><row><cell>28 2 × 32</cell><cell>TBS</cell><cell>64</cell><cell>4</cell><cell>2</cell><cell>16 2 × 32</cell><cell>TBS</cell><cell>64</cell><cell>4</cell><cell>2</cell></row><row><cell>14 2 × 64</cell><cell>TBS</cell><cell>112</cell><cell>4</cell><cell>1</cell><cell>8 2 × 64</cell><cell>TBS</cell><cell>112</cell><cell>4</cell><cell>1</cell></row><row><cell>14 2 × 112</cell><cell>TBS</cell><cell>184</cell><cell>4</cell><cell>2</cell><cell>8 2 × 112</cell><cell>TBS</cell><cell>184</cell><cell>4</cell><cell>2</cell></row><row><cell>7 2 × 184</cell><cell>TBS</cell><cell>352</cell><cell>1</cell><cell>1</cell><cell>4 2 × 184</cell><cell>TBS</cell><cell>352</cell><cell>1</cell><cell>1</cell></row><row><cell>7 2 × 352</cell><cell>1 × 1 conv</cell><cell>1984</cell><cell>1</cell><cell>1</cell><cell>4 2 × 352</cell><cell>1 × 1 conv</cell><cell>1504</cell><cell>1</cell><cell>1</cell></row><row><cell>7 2 × 1984</cell><cell>7 × 7 avgpool</cell><cell>-</cell><cell>1</cell><cell>1</cell><cell cols="2">4 2 × 1504 4 × 4 avgpool</cell><cell>-</cell><cell>1</cell><cell>1</cell></row><row><cell>1504</cell><cell>fc</cell><cell>1000</cell><cell>1</cell><cell>-</cell><cell>1504</cell><cell>fc</cell><cell>100</cell><cell>1</cell><cell>-</cell></row><row><cell cols="8">D DETAILS IN THE PIPELINE TO COLLECT HARDWARE-COST</cell><cell></cell><cell></cell></row><row><cell cols="5">D.1 COLLECT PERFORMANCE ON EDGE GPU:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Our implemented FPGA accelerators for HW-NAS-Bench vs. SOTA FPGA accelerators on VGG16@ImageNet using Zynq XC70Z45 as device.</figDesc><table><row><cell>Resource Utilization</cell><cell>680/900 DSP</cell><cell>824/900 DSP</cell><cell>723/900 DSP</cell></row><row><cell>Performance (GOP/s)</cell><cell>262</cell><cell>230</cell><cell>291</cell></row></table><note>(Zhang et al., 2018)  <ref type="bibr" target="#b47">(Xiao et al., 2017)</ref> Our Implementation</note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 COLLECT PERFORMANCE ON RASPI 4:</head><p>Raspberry Pi 4 (Raspi 4) (Raspberry Pi Limited.) is the latest Raspberry Pi device, which is a popular hardware platform for general purpose IoT applications <ref type="bibr">(Zhao et al., 2015;</ref><ref type="bibr" target="#b3">Basu et al., 2020)</ref> and is able to support deep learning applications with specifically framework designs <ref type="bibr">(Google LLC., f;</ref><ref type="bibr" target="#b55">Zhang et al., 2019;</ref><ref type="bibr" target="#b15">Geiger &amp; Team, 2020)</ref>. We choose the Raspi 4 type with a Broadcom BCM2711 SoC and a 4GB LPDDR4 (Raspberry Pi Limited.). Similar as Edge GPU, Raspi 4 can run architectures in the TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref>, PyTorch <ref type="bibr" target="#b32">(Paszke et al., 2019)</ref> or TensorFlow Lite (Google LLC., f) runtime environments. We utilize TensorFlow Lite (Google LLC., f) which can further boost the efficiency.</p><p>To collect hardware-cost operating on Respi 4, an official TensorFlow Lite interpreter is preconfigured in the Raspi 4, following the settings in <ref type="bibr" target="#b22">(Google LLC., 2020)</ref>. We benchmark the possible architectures in HW-NAS-Bench on Raspi 4 after compiling them to TensorFlow Lite <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref> format to measure the resulting latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 COLLECT PERFORMANCE ON EDGE TPU:</head><p>Edge TPU (Google LLC., a) is a series of dedicated ASIC accelerators developed by Google for targeting AI inference at the edge, which can be used for classification, pose estimation, and segmentation <ref type="bibr" target="#b49">(Xiong et al., 2020;</ref><ref type="bibr">Google LLC., b)</ref> with extremely high efficiency (e.g., 2.32× more efficient than a single SOTA desktop GPU, GTX 2080 Ti, in terms of number of fixed-point operations per watt <ref type="bibr">(Google LLC., d)</ref>). In our proposed collection pipeline, we choose the Dev Board (Google LLC., a) which provides the most functions among all products.</p><p>To collect hardware-cost in Edge TPU, all the architectures to be benchmarked will be converted to TensorFlow Lite (Google LLC., f) format first from Keras <ref type="bibr">(Chollet et al., 2015)</ref> implementation. After that, an in-house compiler (Google LLC., c) will convert the TensorFlow Lite models into a more compressed format. This pipeline uses the least converting tools to make sure as many operations as possible are supported compared with others (e.g., from PyTorch-ONNX <ref type="bibr" target="#b2">(Bai et al., 2020)</ref> implementation). Only latency is collected for Edge TPU since it lacks accurate embedded power rails monitor. We do not consider FBNet' search space for Edge TPU and more details are in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 COLLECT PERFORMANCE ON PIXEL 3:</head><p>Pixel 3 (Google LLC., e) is one of the latest Pixel mobile phones that are widely used as the target platform by recent NAS works <ref type="bibr" target="#b49">(Xiong et al., 2020;</ref><ref type="bibr" target="#b23">Howard et al., 2019;</ref><ref type="bibr">Tan et al., 2019)</ref> and machine learning framework benchmark <ref type="bibr">(Google LLC., f)</ref>. In our implementation, the Pixel 3 is pre-configured to use big cores following the setting in <ref type="bibr" target="#b49">(Xiong et al., 2020;</ref><ref type="bibr">Tan et al., 2019)</ref>. Similar to the case of Raspi 4, we first convert the possible architectures in the search spaces of our proposed HW-NAS-Bench into TensorFlow Lite format and then use the official benchmark binary files to measure the latency for each architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 COLLECT PERFORMANCE ON ASIC-EYERISS:</head><p>For hardware-cost data colection in ASIC, we consider Eyeriss (ASIC-Eyeriss) a SOTA ASIC accelerator <ref type="bibr">(Chen et al., 2016)</ref>. The Eyeriss chip features 168 processing elements (PEs) which are connected through a configurable dedicated on-chip network into a 2D array. A 128KB SRAM is shared by all PEs and further divided into multiple banks, each of which can be assigned to fit the input feature maps or partial sums. Thanks to these configurable hardware settings, we can deploy the optimal algorithm-to-hardware mappings on Eyeriss to minimize the energy or latency by exploiting data reuse for different layers.</p><p>In order to find the optimal mappings and evaluate the performance metrics on Eyeriss, we adopt SOTA performance simulators for DNN accelerators (1) Accelergy <ref type="bibr" target="#b45">(Wu et al., 2019)</ref>+Timeloop <ref type="bibr" target="#b31">(Parashar et al., 2019)</ref> and (2) <ref type="bibr">DNN-Chip Predictor (Zhao et al., 2020)</ref>. Both of the simulators can characterize the Eyeriss' architecture, do mapping exploration, and predict the energy cost and latency metrics. Given the Eyeriss accelerator and layer information (e.g, layer type, feature map size, kernel size) in NAS-Bench-201 and FBNet, Accelergy+Timeloop reports the en-</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} symposium on operating systems design and implementation</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The kendall rank correlation coefficient. Encyclopedia of Measurement and Statistics</title>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Abdi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="508" to="510" />
			<pubPlace>Sage, Thousand Oaks, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://github.com/onnx/onnx" />
		<title level="m">Open neural network exchange</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Raspberry pi 3b+ based smart remote health monitoring system using iot platform</title>
		<author>
			<persName><forename type="first">Samik</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahasweta</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soma</forename><surname>Barman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Communication, Devices and Computing</title>
				<meeting>the 2nd International Conference on Communication, Devices and Computing</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="473" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pearson correlation coefficient</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Benesty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiteng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Israel</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Noise reduction in speech processing</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<title level="m">Proxylessnas: Direct neural architecture search on target task and hardware</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fasterseg: Searching for faster real-time semantic segmentation</title>
		<author>
			<persName><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10917</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JSSC</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="138" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="367" to="379" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
		<ptr target="https://keras.io" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Solid-State Circuits Conference, ISSCC 2016, Digest of Technical Papers</title>
				<imprint>
			<date type="published" when="2015">2016. 2015</date>
			<biblScope unit="page" from="262" to="263" />
		</imprint>
	</monogr>
	<note>Franc ¸ois Chollet et al. Keras</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A downsampled variant of imagenet as an alternative to the cifar datasets</title>
		<author>
			<persName><forename type="first">Patryk</forename><surname>Chrabaszcz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08819</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Discovering multi-hardware mobile models via architecture search</title>
		<author>
			<persName><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Okan</forename><surname>Arikan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Achille</forename><surname>Brighton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berkin</forename><surname>Akin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyog</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08178</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nas-bench-201: Extending the scope of reproducible neural architecture search</title>
		<author>
			<persName><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJxyZkBKDr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarzyna</forename><surname>Musial</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Gabrys</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00437</idno>
		<title level="m">Nats-bench: Benchmarking nas algorithms for architecture topology and size</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Autogan-distiller: Searching to compress generative adversarial networks</title>
		<author>
			<persName><forename type="first">Yonggan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08198</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Larq: An open-source library for training binarized neural networks</title>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Plumerai</forename><surname>Team</surname></persName>
		</author>
		<idno type="DOI">10.21105/joss.01746</idno>
		<ptr target="https://doi.org/10.21105/joss.01746" />
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">45</biblScope>
			<biblScope unit="page">1746</biblScope>
			<date type="published" when="2020-01">January 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Llc</forename><surname>Google</surname></persName>
		</author>
		<author>
			<persName><surname>Edge</surname></persName>
		</author>
		<author>
			<persName><surname>Compiler</surname></persName>
		</author>
		<ptr target="https://coral.ai/docs/dev-board/get-started/" />
		<imprint>
			<biblScope unit="page" from="2020" to="2029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Edge TPU Code Examples</title>
		<author>
			<persName><forename type="first">Llc</forename><surname>Google</surname></persName>
		</author>
		<ptr target="https://coral.ai/examples/#code-examples" />
		<imprint>
			<date type="published" when="2019-11-21">2019-11-21</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Llc</forename><surname>Google</surname></persName>
		</author>
		<author>
			<persName><surname>Edge</surname></persName>
		</author>
		<author>
			<persName><surname>Compiler</surname></persName>
		</author>
		<ptr target="https://coral.ai/docs/edgetpu/compiler/#system-requirements" />
		<imprint>
			<date type="published" when="2020-09-01">2020-09-01</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Llc</forename><surname>Google</surname></persName>
		</author>
		<author>
			<persName><surname>Edge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Faq</surname></persName>
		</author>
		<ptr target="https://coral.ai/docs/edgetpu/faq/" />
		<imprint>
			<date type="published" when="2019-11-21">2019-11-21</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Llc</forename><surname>Google</surname></persName>
		</author>
		<ptr target="https://g.co/kgs/pVRc1Y" />
	</analytic>
	<monogr>
		<title level="j">Pixel</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2020-09-01">2020-09-01</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deploy machine learning models on mobile and IoT devices</title>
		<author>
			<persName><forename type="first">Llc</forename><surname>Google</surname></persName>
		</author>
		<author>
			<persName><surname>Tensorflow Lite</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/lite" />
		<imprint>
			<date type="published" when="2019-11-21">2019-11-21</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Llc</forename><surname>Google</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/lite/guide/python" />
		<title level="m">Tflite python quickstart</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Nas-bench-nlp: Neural architecture search benchmark for natural language processing</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Klyuchnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Trofimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekaterina</forename><surname>Artemova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Salnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.02009</idno>
	</analytic>
	<monogr>
		<title level="m">Jaeseong Lee, Duseok Kang, and Soonhoi Ha. S3nas: Fast npu-aware neural architecture search methodology</title>
				<imprint>
			<date type="published" when="2009">2009. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Halo: Hardwareaware learning to optimize</title>
		<author>
			<persName><forename type="first">Chaojian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyan</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020-09">September 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Nascaps: A framework for neural architecture search to optimize the accuracy and hardware efficiency of convolutional capsule networks</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Marchisio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vojtech</forename><surname>Mrazek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Bussolino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maurizio</forename><surname>Martina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Shafique</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08476</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><surname>Nvidia Inc</surname></persName>
		</author>
		<author>
			<persName><surname>Nvidia Jetson Tx2</surname></persName>
		</author>
		<ptr target="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-tx2/" />
		<title level="m">NVIDIA Inc. Tensorrt, b. NVIDIA Inc. Benchmark tx2 performance in googlenet with tensorrt</title>
				<imprint>
			<date type="published" when="2020-09-01">2020-09-01</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Timeloop: A systematic approach to dnn accelerator evaluation</title>
		<author>
			<persName><forename type="first">Angshuman</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priyanka</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Yakun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><forename type="middle">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangharajan</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brucek</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE international symposium on performance analysis of systems and software (ISPASS)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="304" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">sysfs -The filesystem for exporting kernel objects</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mochel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Murphy</surname></persName>
		</author>
		<ptr target="https://www.kernel.org/doc/Documentation/filesystems/sysfs.txt" />
		<imprint>
			<date type="published" when="2019-11-21">2019-11-21</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title/>
		<ptr target="https://www.raspberrypi.org/products/raspberry-pi-4-model-b/" />
	</analytic>
	<monogr>
		<title level="j">Raspberry Pi Limited. Pixel</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2020" to="2029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Maximizing cnn accelerator efficiency through resource partitioning</title>
		<author>
			<persName><forename type="first">Yongming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Milder</surname></persName>
		</author>
		<idno type="DOI">10.1145/3079856.3080221</idno>
		<ptr target="https://doi.org/10.1145/3079856.3080221" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture, ISCA &apos;17</title>
				<meeting>the 44th Annual International Symposium on Computer Architecture, ISCA &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="535" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A comparative study of real-time semantic segmentation for autonomous driving</title>
		<author>
			<persName><forename type="first">Mennatullah</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Gamal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moemen</forename><surname>Abdel-Razek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Senthil</forename><surname>Yogamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jagersand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
				<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="587" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Nasbench-301 and the case for surrogate benchmarks for neural architecture search</title>
		<author>
			<persName><forename type="first">Julien</forename><surname>Siems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arber</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jovita</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.09777</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Single-path nas: Designing hardware-efficient convnets in less than 4 hours</title>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Stamoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruizhou</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Lymberopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bodhi</forename><surname>Priyantha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Marculescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="481" to="497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<ptr target="http://www.ti.com/product/INA3221" />
		<title level="m">Shunt and Bus Voltage Monitor</title>
				<imprint>
			<publisher>Texas Instruments Inc. INA3221 Triple-Channel, High-Side Measurement</publisher>
			<date type="published" when="2019-11-21">2019-11-21</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Differentiable neural architecture search for spatial and channel dimensions</title>
		<author>
			<persName><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="12965" to="12974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Enriching variety of layer-wise learning information by gradient combination</title>
		<author>
			<persName><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping-Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Wei</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2019-10">Oct 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fastdepth: Fast monocular depth estimation on embedded systems</title>
		<author>
			<persName><forename type="first">Diana</forename><surname>Wofk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangchang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tien-Ju</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sertac</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6101" to="6108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10734" to="10742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Accelergy: An Architecture-Level Energy Estimation Methodology for Accelerator Designs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Yannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM International Conference On Computer Aided Design (ICCAD)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Exploring heterogeneous algorithms for accelerating deep convolutional neural networks on fpgas</title>
		<author>
			<persName><forename type="first">Qingcheng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<idno type="DOI">10.1145/3061639.3062244</idno>
		<ptr target="https://doi.org/10.1145/3061639.3062244" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Design Automation Conference 2017, DAC &apos;17</title>
				<meeting>the 54th Annual Design Automation Conference 2017, DAC &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<ptr target="https://www.xilinx.com/products/boards-and-kits/ek-z7-zc706-g.html,b" />
		<title level="m">Xilinx zynq-7000 soc zc706 evaluation kit</title>
				<imprint>
			<publisher>Xilinx Inc</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Mobiledets: Searching for object detection architectures for mobile accelerators</title>
		<author>
			<persName><forename type="first">Yunyang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyog</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berkin</forename><surname>Akin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14525</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Nas evaluation is frustratingly hard</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">M</forename><surname>Esperanc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HygrdpVKvr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">A systematic approach to blocking convolutional neural networks</title>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaine</forename><surname>Burton Rister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Bhagdikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahar</forename><surname>Kvatinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ardavan</forename><surname>Pedram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Horowitz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Nasbench-101: Towards reproducible neural architecture search</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7105" to="7114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Greedynas: Towards fast one-shot nas with greedy supernet</title>
		<author>
			<persName><forename type="first">Shan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingmin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1999" to="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Optimizing fpga-based accelerator design for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijin</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Cong</surname></persName>
		</author>
		<idno type="DOI">10.1145/2684746.2689060</idno>
		<ptr target="https://doi.org/10.1145/2684746.2689060" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, FPGA &apos;15</title>
				<meeting>the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, FPGA &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">dabnn: A super fast inference framework for binary neural networks on arm devices</title>
		<author>
			<persName><forename type="first">Jianhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
				<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2272" to="2275" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
