<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CoAtGIN: Marrying Convolution and Attention for Graph-based Molecule Property Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xuan</forename><surname>Zhang</surname></persName>
							<email>zhangxuan@mail.sdu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Cheng</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhaoxu</forename><surname>Meng</surname></persName>
							<email>zxmeng18@mail.sdu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Zhenghe</forename><surname>Yang</surname></persName>
							<email>yangzhenghe@lthpc.com</email>
						</author>
						<author>
							<persName><forename type="first">Haitao</forename><surname>Jiang</surname></persName>
							<email>htjiang@sdu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Xuefeng</forename><surname>Cui</surname></persName>
							<email>xfcui@email.sdu.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Technology Shandong University</orgName>
								<address>
									<settlement>Qingdao</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Technology Shandong University</orgName>
								<address>
									<settlement>Qingdao</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Shandong University</orgName>
								<address>
									<settlement>Qingdao</settlement>
									<country>China City, Country</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">LTHPC (Beijing) Technology Company Limited</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Technology Shandong University</orgName>
								<address>
									<settlement>Qingdao</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Technology Shandong University</orgName>
								<address>
									<settlement>Qingdao</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CoAtGIN: Marrying Convolution and Attention for Graph-based Molecule Property Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1101/2022.08.26.505499</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Molecule Property Prediction</term>
					<term>Deep learning</term>
					<term>Graph neural network (GNN)</term>
					<term>k-hop convolution</term>
					<term>linear transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Molecule property prediction based on computational strategy plays a key role in the process of drug discovery and design, such as DFT. Yet, these traditional methods are timeconsuming and labour-intensive, which can't satisfy the need of biomedicine. Thanks to the development of deep learning, there are many variants of Graph Neural Networks (GNN) for molecule representation learning. However, whether the existed well-perform graph-based methods have a number of parameters, or the light models can't achieve good grades on various tasks. In order to manage the trade-off between efficiency and performance, we propose a novel model architecture, CoAtGIN, using both Convolution and Attention. On the local level, khop convolution is designed to capture long-range neighbour information. On the global level, besides using the virtual node to pass identical messages, we utilize linear attention to aggregate global graph representation according to the importance of each node and edge. In the recent OGB Large-Scale Benchmark, CoAtGIN achieves the 0.0933 Mean Absolute Error (MAE) on the large-scale dataset PCQM4Mv2 with only 5.6 M model parameters. Moreover, using the linear attention block improves the performance, which helps to capture the global representation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Throughout the human history of fighting diseases, small molecule drugs have played a vital role as the most reliable approach <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. However, small molecule drug research is confronted with many challenges. It is estimated that 5,000-10,000 molecules need to be identified and validated in the discovery phase for one novel drug <ref type="bibr" target="#b4">[5]</ref>. Considering the long and laborious period, researchers have summarized various guidelines to accelerate the drug discovery process based on drug-like character. For instance, the absorption, distribution, *To whom correspondence should be addressed. Fig. <ref type="figure">1</ref>. Models' performance on the Open Graph Benchmark (OGB). The nodes in the upper-left area represent the models which perform not well enough but with fewer parameters. And the lower right area contains the well-performing model with enormous parameters. As the intersection area, CoAtGIN achieves a good grade while costs only a few parameters. metabolism, excretion, and toxicity (ADMET) are highly related to the acid/base properties <ref type="bibr" target="#b5">[6]</ref>. Moreover, the HUMO-LUMO gap, a quantum chemical property, might influence reactivity, photoexcitation, and charge transport <ref type="bibr" target="#b6">[7]</ref>. Considering thousands of candidates, it is time-consuming and labourintensive to do chemical or biological experiments for each compound.</p><p>With the development of chemistry and computer science, research has shown that the Force Field method can be employed to estimate some properties of molecules with high precision. For example, it is practicable to predict the force within and between molecules. Besides, Density Function theory (DFT) has become a valuable and helpful tool in many scenarios such as physics, chemistry, and materials <ref type="bibr" target="#b7">[8]</ref>. This method contributes to drug discovery by precisely calculating various molecular properties such as the shape of molecules, reactivity, and energy gap <ref type="bibr" target="#b8">[9]</ref>. Despite the superior success of DFT, the running time of the method is still costly for thousands of candidates, especially since it takes hours to calculate one single molecule's properties.</p><p>Recently, artificial intelligence significantly contributed to the development of bio-pharmaceuticals. Several deep learning frameworks have been developed and performed well in the application of small molecule drug discovery <ref type="bibr" target="#b9">[10]</ref>, such as transformer-based BERT <ref type="bibr" target="#b10">[11]</ref> and GPT <ref type="bibr" target="#b11">[12]</ref>. The small molecule is generally represented using SMILES (simplified molecular input line specification) format. Thus, several works employed these well-verified pre-train language models to predict the molecule property. For example, SMILES-BERT <ref type="bibr" target="#b12">[13]</ref> directly applied the BERT-style training strategy to the SMILES sequence using masked language learning. To overcome data scarcity, Chem-BERTa <ref type="bibr" target="#b13">[14]</ref> utilized the multiple equivalent SMILES as input. Furthermore, TokenGT <ref type="bibr" target="#b14">[15]</ref> used vanilla Transformer <ref type="bibr" target="#b15">[16]</ref> to process molecule graph by treating atoms and bonds as independent tokens.</p><p>String-based representations mostly focus on sequence features, which are hard to capture the important topology information of molecules. If we regard atoms and bonds as nodes and edges of graph, using graph neural network (GNN) can help to obtain topology structure information, such as GIN <ref type="bibr" target="#b16">[17]</ref>, GAT <ref type="bibr" target="#b17">[18]</ref>, GIN-Virtual <ref type="bibr" target="#b18">[19]</ref>, Deeper-GCN <ref type="bibr" target="#b19">[20]</ref>. These graph-based methods propagate messages of each node to its neighbours, and message-passing architecture can represent the molecule topology knowledge. However, the deeper and wider GNN-based models required more GPU memory. For example, the Edge-augmented Graph Transformer (EGT) employed a vanilla transformer to process node embeddings and introduced edge channels for each pair of nodes. This novel graph architecture has many model parameters (Fig. <ref type="figure">1</ref>).</p><p>From the above discussion, we can observe that (a) Stringbased methods failed to encode the important topology information directly. GNN architecture is very suitable for processing small molecule drug data. (b) Virtual node passes identical messages to each node, which lacks the ability of global representation. (c) The scale of these methods keeps increasing to improve prediction accuracy (Fig. <ref type="figure">1</ref>), which raises the challenge to manage the trade-off between efficiency and performance. Inspired by these observations, we proposed a novel graph-based architecture called CoAtGIN, the main contribution can be summarized as follows:</p><p>1) We present the k-hop convolution in a graph convolution network for faster message aggregation within one iteration. 2) We present a new way to accomplish global message passing through the graph using the linear transformer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODS</head><p>As shown in Figure <ref type="figure" target="#fig_0">2</ref>, CoAtGIN is composed of L layers. Each layer takes the Node Embedding (NE) and Graph Embedding (GE) as input, and then these two embeddings will be updated for layerwise iteration. Using the embedding block provided by <ref type="bibr" target="#b20">[21]</ref>, we initialize the Node Embedding as the atom type of each node. And the Graph Embeddings are set to zeros before the first layer.</p><p>Although GIN-VN <ref type="bibr" target="#b18">[19]</ref> wants to alleviate the lagging message passing problem between distant node pairs by adding the global virtual node, the broadcast message is identical to all the nodes. To solve this problem, section II-B shows shat we additionally apply the linear transformer as the global convolution block, which is the extension of GIN-VN . Besides taking both the linear attention and virtual node as global convolution block, we modify the traditional aggregation operation in local convolution block to efficiently capture the longrange neighbours' features, which is shown in section II-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Preliminaries</head><p>The Graph Isomorphism Network (GIN) <ref type="bibr" target="#b16">[17]</ref> architecture was proposed by Xu et al. as a simple but powerful approach for the learning of graphs. It is illustrated and proved that GIN could effectively distinguish different graph representations. Moreover, the discriminative power of GIN is as strong as the 1-Weisfeiler-Lehman(1-WL) isomorphism test. Using the multi-layer perceptions (MLPs) to model the multiset functions, the propagation progress of GIN can be described as,</p><formula xml:id="formula_0">h (k) v = MLP (k)   1 + ϵ (k) • h (k−1) v + u∈N (v) h (k−1) u  <label>(1)</label></formula><p>where h</p><formula xml:id="formula_1">(k) v</formula><p>is the feature vector of node v at the k-th layer, MLP (k) represents the k-th layer's the multi-layer perceptions, ϵ (k) denotes a learnable parameter, and N (v) is the node set that contains all the neighbors of node v.</p><p>Nevertheless, the aggregation function shows that only neighbours can query and pass messages in the GIN and traditional graph network. Intuitively, the message through a distant node pair will be noisy and slow because the feature of one takes multiple iterations/layers to reach another. Therefore, Gilmer et al. <ref type="bibr" target="#b18">[19]</ref> came up with a variant, GIN-VIRTUAL, which chose to add a virtual node to the graph with a particular edge type, and the node connects to all other nodes. Each original node in the graph reads and writes to this node in every step of message passing. Adding the virtual node has shown that the method works effectively across a wide range of graph-level prediction datasets <ref type="bibr" target="#b20">[21]</ref>.</p><p>However, as shown in Fig <ref type="figure" target="#fig_0">2 (c</ref>), the virtual node of GIN-VN takes the READOUT as the graph representation and sends the duplicate processed message to every node in the original graph. Although Xu et al. <ref type="bibr" target="#b16">[17]</ref> has proved that taking SUM as the graph level READOUT function can achieve good discriminative power. But each node in the graph also needs to accept different feature representations to optimize its representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Global Convolution</head><p>Besides adding the virtual node, attention mechanism is another way to read and send information across the whole graph. Unlike adding the virtual node, the aggregated feature returned by the attention block differs from node to node. We apply the linear transformer to the global convolution block so that the diversity of the node can be guaranteed. Furthermore, we reserve the virtual node to enable the network to have a good ability both to distinguish different graphs and to gain a more expressive node representation.</p><p>1) Virtual Node: For the virtual node block, the main pipeline maintains the same as GIN-VN. As shown in Figure <ref type="figure" target="#fig_0">2</ref> (c), the node embedding will be reduced the Node Embedding (NE) by the READOUT function (sum operation). All the node embedding will be added up to form the summarized information. The aggregated node feature will pass the feedforward network to get the interpretation of the whole network, which we called Virtual Node Graph Embedding (VGE)</p><p>Added with the older VGE, the embedding enables updating the Graph Embedding of the virtual node and broadcasting the graph embedding for original nodes in the graph. It is notable that the broadcasted embeddings are identical for every node.</p><p>However, different atoms play different roles in all kinds of chemical properties, which requires the network to feedback different information. Meanwhile, the extraordinary distinguish property should be reserved for identifying different molecules.</p><p>2) Linear Transformer: To accomplish this task, the attention mechanism can be used here to afford the responsibility of calculating node-wise graph embedding. The result of the transformer is graph embedding driven by nodes, which we called Attention Graph Embedding (AGE). By this way, both the node-wise and union representation graph embedding can be obtained in each iteration.</p><p>But the time complexity of the vanilla transformer is related to the squared number of nodes. In the desire of an efficient and lightweight model, we choose to use the CosFormer <ref type="bibr" target="#b21">[22]</ref> to realize the attention mechanism for remedying the expensive computation cost, On the basis of the CosFormer, we add the same READ-OUT function in the virtual node block (sum operation) for aggregating the global KV . Besides using the aggregated KV to update the Graph Embedding for linear attention, the matrix is also used to respond to the node-wise Q.</p><p>After the linear attention and virtual block generating the AGE and VGE, the two embeddings are added together as the output of global convolution block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. K-Hop Convolution</head><p>Linear attention and virtual node block are both the way of adding new message passing methods throughout the network. In terms of the vanilla node aggregation operation as shown in eq. ( <ref type="formula" target="#formula_0">1</ref>), only the features within 1-hop distance can be captured.</p><p>By repeating the propagation k times, we can receive the at most k-hop distance information. It makes it possible to interact with the long-range node without the use of a deep Graph Neural Network. And all the aggregation result will be scaled by a learning layer so that the node can capture import neighbours and bonds.</p><p>The ablation study in section III-C shows that the model benefits a lot due to the k-hop convolution. quantum-chemical regression task, which predicts the DFTcalculated HOMO-LUMO energy gap. The HOMO-LUMO energy gap is a human-defined important metric in quantum chemistry, which is strongly associated with many molecule properties such as polarizability and activation energy. Moreover, Mean Absolute Error(MAE) is used to evaluate the benchmark accuracy, and the lower value indicates a better model performance.</p><p>Besides, we instigate the representation of CoAtGIN by visualizing the graph embedding. The result indicates that the graph representation can describe the molecule well, so embedding visualization is continuous.</p><p>At last, we explore whether the modules in our architecture make sense in this quantum-chemical regression task. Based on the observation, further exploration might be obtained by the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparison to the state of the art methods</head><p>As mentioned before, the CoAtGIN model is evaluated in the Large Scale Challenge (LSC) benchmark <ref type="bibr" target="#b20">[21]</ref> PCQM4Mv2 dataset. There are many fancy models using the benchmark as their evaluation metrics. As shown in table I, which includes the state-of-the-art methods and their corresponding asymptotics, number of parameters, and MAE result. The models here can be broadly divided into two types by the model's number of parameters.</p><p>The traditional Graph Neural Network like Graph Convolutional Network (GCN) <ref type="bibr" target="#b22">[23]</ref>, Graph Attention Network (GAT) <ref type="bibr" target="#b17">[18]</ref>, Graph Isomorphism Network (GIN) <ref type="bibr" target="#b16">[17]</ref> and their variants (GCN-VN, GAT-VN, and GIN-VN) have only a few parameters but good performances with about 0.1 MAE.</p><p>However, to achieve the better performance, TokenGT <ref type="bibr" target="#b23">[24]</ref>, GRPE <ref type="bibr" target="#b24">[25]</ref>, EGT <ref type="bibr" target="#b25">[26]</ref> and Graphormer <ref type="bibr" target="#b26">[27]</ref> adopts plenty of fancy modules and architecture. Although the performance does improve, the number of parameters increases severely. Even the slightest TokenGT <ref type="bibr" target="#b23">[24]</ref>, whose number of parameters is 7.2 times larger than the GIN-VN model.</p><p>Although the scale of CoAtGIN is only 5.2M, the model achieves the 0.945 MAE on PCQM4Mv2 dataset which is close to the result of TokenGT. Besides, the asymptotic columns show that the time complexity of CoAtGIN is one of the lowest models in the benchmark. In the words, this experiment shows that CoAtGIN is a lightweight model which can achieve fast inference speed without much loss of performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Investigation of CoAtGIN representation</head><p>We analyse the molecule representation learned by CoAt-GIN using T-distributed Stochastic Neighbor Embedding(t-SNE) <ref type="bibr" target="#b27">[28]</ref> method and the scikit-learn <ref type="bibr" target="#b28">[29]</ref> package. t-SNE can map high-dimensional space into two-dimensional space, showing the embedding information intuitively. We random sample 2000 molecules from PCQM4Mv2 dataset to illustrate what the CoAtGIN learns. From Fig. <ref type="figure" target="#fig_1">3</ref>, the shorter the distance of the molecule representations, the closer the quantumchemical property value is. In other words, similar 2D representations share close molecule properties. For example, the light blue indicates a low HOMU-LUMO gap value, and we can see that these blue dots are clustered together. More interestingly, the 2D embedding representation is continuous in the t-SNE space, which is high relative to the continuous HOMU-LUMO gap value (regression value). This indicates that molecules with similar properties have close features using CoAtGIN for embedding. If we regard CoAtGIN as an effective pre-train strategy, the proposed method will likely perform better in downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation experiments</head><p>To explore the contribution of each module illustrated in Fig. <ref type="figure" target="#fig_0">2</ref> and section II, we evaluate these modules using an ablation study. There are three blockes mentioned in section II: k-hop conv, virtual node, and linear attention. And the ablation experiment is detailed in table II. (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. Firstly, we illustrate the relationship between the performance and the k value selection of k-hop convolution block. From the table II, we can see that the model performance positively correlates with the hop convolution. A higher k value usually brings better performance which means that further node information is helpful to capture better local and topology information. Considering the trade-off between efficiency and performance, larger k values were not validated because the 3-hop neighborhood contains most of the necessary bioinformatic information like torsion angles. Larger k values will lead to the duplicated information problem and make the model larger. Based on this observation, we finally choose k = 3 in the k-hop convolution block considering both the performance and efficiency.</p><p>And then we explore the usage of linear attention and virutal block as shown in the lower part of table II. Table <ref type="table">II</ref> indicates that virtual node and linear attention both play a key role in CoAtGIN. Using any of the linear attention and virutal block in CoAtGIN makes the regression value much more precise. The combination of these two modules further improves the performance. It is convincing that the linear attention and virutal block are both necessary for the network, and learning features from different modules might improve the other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In summary, we propose a novel graph-based architecture, CoAtGIN , to obtain node embedding and graph embedding. Experiments indicate that CoAtGIN achieves competitive performance with relatively high efficiency in the OGB Large-Scale Challenge. The key modules of CoAtGIN include khop conv, virtual node, and linear attention. The k-hop conv can capture better local typology structure information. And the linear transformer is designed to remedy the problem of identical messages from the virtual node. Using both convolution and attention provides a novel way to predict molecule properties.</p><p>In the future, we will continue to explore the strategies for improving CoAtGIN . The rotation angle plays an essential role in the structure and function. Our next direction is to design an effective block to capture the geometric properties of molecules. We also want to understand why CoAtGIN performs better from the biological perspective. In addition, our proposed CoAtGIN can effectively process graph data, so this model could be a good fit to solve protein structurerelated problems. Because we regard residues as nodes, the contact between residues is defined as edge. More exciting applications of CoAtGIN for protein complex prediction and design will be explored.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) The overall pipeline of the model for predicting the HUMO-LUMO gap, which consists of L repeated layers. Each layer can be broadly divided into two blocks, separately Local Convolution and Global Convolution, and classical residuals are used here. (b) Local Convolution block. The novel graph local convolution block named k-hop convolution repeatedly aggregated the neighbours' information k times so that the k-hop nodes can be interacted in each iteration. (c) Global Convolution block. Illustration of virtual node, which reads from and writes to every node in the original graph separately using reduce and broadcast operations. GE (Graph Embedding) information from the last layer involves in the broadcast operation with the output of the feed-forward layer and the result also be used as newer GE. (d) Global Convolution block. Like the most linear transformer, the K, V matrix combines first and Q matrix do dot product with reduced global KV .</figDesc><graphic url="image-2.png" coords="2,48.96,50.54,514.06,161.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3 )</head><label>3</label><figDesc>As shown in fig. 1, we develop a lightweight but wellperformed model named CoAtGIN , which has only 5.2M parameters and achieves 0.0933 Mean Absolute Error (MAE) in the regression task of a large-scale graph dataset (PCQM4Mv2 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. T-SNE visualization of the aggregated nodes' embedding. The color indicates the ground truth of the molecules' HOMO-LUMO gap. The dark blue represents the lowest energy gap and the red color represents the highest value.</figDesc><graphic url="image-3.png" coords="4,321.84,511.39,231.33,160.11" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Small molecules and their impact in drug discovery: A perspective on the occasion of the 125th anniversary of the bayer chemical research laboratory</title>
		<author>
			<persName><forename type="first">H</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Härter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haß</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baerfacker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Drug Discovery Today</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">rzmlp-dta: gmlp network with rezero for sequence-based drug-target affinity prediction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="308" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Structure-based protein-drug affinity prediction with spatial attention mechanisms</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="92" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Edge-gated graph neural network for predicting proteinligand binding affinities</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="334" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">omics&quot;-informed drug and biomarker discovery: opportunities, challenges and future perspectives</title>
		<author>
			<persName><forename type="first">H</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hanison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nirmalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteomes</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The significance of acid/base properties in drug discovery</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Manallack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Prankerd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yuriev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">I</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Chalmers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical Society Reviews</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="485" to="496" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quantum chemical studies of some pyridine derivatives as corrosion inhibitors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ögretir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mihc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bereket</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Molecular Structure: THEOCHEM</title>
		<imprint>
			<biblScope unit="volume">488</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="223" to="231" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Perspective on density functional theory</title>
		<author>
			<persName><forename type="first">K</forename><surname>Burke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of chemical physics</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page">150901</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Ogb-lsc: A large-scale challenge for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.09430</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Geometry-enhanced molecular representation learning for property prediction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="134" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improving language understanding with unsupervised learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Smiles-bert: large scale unsupervised pre-training for molecular property prediction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health informatics</title>
				<meeting>the 10th ACM international conference on bioinformatics, computational biology and health informatics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="429" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Chemberta: large-scale self-supervised pretraining for molecular property prediction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chithrananda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Grand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09885</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Pure transformers are powerful graph learners</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.02505</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deepergcn: All you need to train deeper gcns</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22118" to="22133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">cosformer: Rethinking softmax in attention</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.08791</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Pure transformers are powerful graph learners</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.02505</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Grpe: Relative positional encoding for graph transformer</title>
		<author>
			<persName><forename type="first">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-G</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR2022 Machine Learning for Drug Discovery</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Edgeaugmented graph transformers: Global self-attention is enough for graphs</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Subramanian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.03348</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Do transformers really perform badly for graph representation?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="28877" to="28888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
