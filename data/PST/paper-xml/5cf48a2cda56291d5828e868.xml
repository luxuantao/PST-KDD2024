<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MixMatch: A Holistic Approach to Semi-Supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">David</forename><surname>Berthelot</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
							<email>ncarlini@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Avital</forename><surname>Oliver</surname></persName>
							<email>avitalo@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
							<email>craffel@google.com</email>
						</author>
						<title level="a" type="main">MixMatch: A Holistic Approach to Semi-Supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semi-supervised learning has proven to be a powerful paradigm for leveraging unlabeled data to mitigate the reliance on large labeled datasets. In this work, we unify the current dominant approaches for semi-supervised learning to produce a new algorithm, MixMatch, that guesses low-entropy labels for data-augmented unlabeled examples and mixes labeled and unlabeled data using MixUp. MixMatch obtains state-of-the-art results by a large margin across many datasets and labeled data amounts. For example, on CIFAR-10 with 250 labels, we reduce error rate by a factor of 4 (from 38% to 11%) and by a factor of 2 on STL-10. We also demonstrate how MixMatch can help achieve a dramatically better accuracy-privacy trade-off for differential privacy. Finally, we perform an ablation study to tease apart which components of MixMatch are most important for its success. We release all code used in our experiments. 1  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Much of the recent success in training large, deep neural networks is thanks in part to the existence of large labeled datasets. Yet, collecting labeled data is expensive for many learning tasks because it necessarily involves expert knowledge. This is perhaps best illustrated by medical tasks where measurements call for expensive machinery and labels are the fruit of a time-consuming analysis that draws from multiple human experts. Furthermore, data labels may contain private information. In comparison, in many tasks it is much easier or cheaper to obtain unlabeled data.</p><p>Semi-supervised learning <ref type="bibr" target="#b5">[6]</ref> (SSL) seeks to largely alleviate the need for labeled data by allowing a model to leverage unlabeled data. Many recent approaches for semi-supervised learning add a loss term which is computed on unlabeled data and encourages the model to generalize better to unseen data. In much recent work, this loss term falls into one of three classes (discussed further in Section 2): entropy minimization <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28]</ref>-which encourages the model to output confident predictions on unlabeled data; consistency regularization-which encourages the model to produce the same output distribution when its inputs are perturbed; and generic regularization-which encourages the model to generalize well and avoid overfitting the training data.</p><p>In this paper, we introduce MixMatch, an SSL algorithm which introduces a single loss that gracefully unifies these dominant approaches to semi-supervised learning. Unlike previous methods, MixMatch targets all the properties at once which we find leads to the following benefits: Sharpen … K augmentations ... Then, the average of these K predictions is "sharpened" by adjusting the distribution's temperature. See algorithm 1 for a full description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classify Classify Unlabeled</head><note type="other">Guessed Label Average</note><p>• Experimentally, we show that MixMatch obtains state-of-the-art results on all standard image benchmarks (section 4.2), and reducing the error rate on CIFAR-10 by a factor of 4;</p><p>• We further show in an ablation study that MixMatch is greater than the sum of its parts;</p><p>• We demonstrate in section 4.3 that MixMatch is useful for differentially private learning, enabling students in the PATE framework <ref type="bibr" target="#b35">[36]</ref> to obtain new state-of-the-art results that simultaneously strengthen both privacy guarantees and accuracy.</p><p>In short, MixMatch introduces a unified loss term for unlabeled data that seamlessly reduces entropy while maintaining consistency and remaining compatible with traditional regularization techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>To set the stage for MixMatch, we first introduce existing methods for SSL. We focus mainly on those which are currently state-of-the-art and that MixMatch builds on; there is a wide literature on SSL techniques that we do not discuss here (e.g., "transductive" models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b20">21]</ref>, graph-based methods <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b28">29]</ref>, generative modeling <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b41">42]</ref>, etc.). More comprehensive overviews are provided in <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b5">6]</ref>. In the following, we will refer to a generic model p model (y | x; θ) which produces a distribution over class labels y for an input x with parameters θ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Consistency Regularization</head><p>A common regularization technique in supervised learning is data augmentation, which applies input transformations assumed to leave class semantics unaffected. For example, in image classification, it is common to elastically deform or add noise to an input image, which can dramatically change the pixel content of an image without altering its label <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b9">10]</ref>. Roughly speaking, this can artificially expand the size of a training set by generating a near-infinite stream of new, modified data. Consistency regularization applies data augmentation to semi-supervised learning by leveraging the idea that a classifier should output the same class distribution for an unlabeled example even after it has been augmented. More formally, consistency regularization enforces that an unlabeled example x should be classified the same as Augment(x), an augmentation of itself.</p><p>In the simplest case, for unlabeled points x, prior work <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">40]</ref> adds the loss term</p><formula xml:id="formula_0">p model (y | Augment(x); θ) − p model (y | Augment(x); θ) 2 2 .<label>(1)</label></formula><p>Note that Augment(x) is a stochastic transformation, so the two terms in eq. ( <ref type="formula" target="#formula_0">1</ref>) are not identical. "Mean Teacher" <ref type="bibr" target="#b43">[44]</ref> replaces one of the terms in eq. ( <ref type="formula" target="#formula_0">1</ref>) with the output of the model using an exponential moving average of model parameter values. This provides a more stable target and was found empirically to significantly improve results. A drawback to these approaches is that they use domain-specific data augmentation strategies. "Virtual Adversarial Training" <ref type="bibr" target="#b30">[31]</ref> (VAT) addresses this by instead computing an additive perturbation to apply to the input which maximally changes the output class distribution. MixMatch utilizes a form of consistency regularization through the use of standard data augmentation for images (random horizontal flips and crops).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Entropy Minimization</head><p>A common underlying assumption in many semi-supervised learning methods is that the classifier's decision boundary should not pass through high-density regions of the marginal data distribution.</p><p>One way to enforce this is to require that the classifier output low-entropy predictions on unlabeled data. This is done explicitly in <ref type="bibr" target="#b17">[18]</ref> with a loss term which minimizes the entropy of p model (y | x; θ) for unlabeled data x. This form of entropy minimization was combined with VAT in <ref type="bibr" target="#b30">[31]</ref> to obtain stronger results. "Pseudo-Label" <ref type="bibr" target="#b27">[28]</ref> does entropy minimization implicitly by constructing hard (1-hot) labels from high-confidence predictions on unlabeled data and using these as training targets in a standard cross-entropy loss. MixMatch also implicitly achieves entropy minimization through the use of a "sharpening" function on the target distribution for unlabeled data, described in section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Traditional Regularization</head><p>Regularization refers to the general approach of imposing a constraint on a model to make it harder to memorize the training data and therefore hopefully make it generalize better to unseen data <ref type="bibr" target="#b18">[19]</ref>. We use weight decay which penalizes the L 2 norm of the model parameters <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b45">46]</ref>. We also use MixUp <ref type="bibr" target="#b46">[47]</ref> in MixMatch to encourage convex behavior "between" examples. We utilize MixUp as both as a regularizer (applied to labeled datapoints) and a semi-supervised learning method (applied to unlabeled datapoints). MixUp has been previously applied to semi-supervised learning; in particular, the concurrent work of <ref type="bibr" target="#b44">[45]</ref> uses a subset of the methodology used in MixMatch. We clarify the differences in our ablation study (section 4.2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MixMatch</head><p>In this section, we introduce MixMatch, our proposed semi-supervised learning method. MixMatch is a "holistic" approach which incorporates ideas and components from the dominant paradigms for SSL discussed in section 2. Given a batch X of labeled examples with one-hot targets (representing one of L possible labels) and an equally-sized batch U of unlabeled examples, MixMatch produces a processed batch of augmented labeled examples X ′ and a batch of augmented unlabeled examples with "guessed" labels U ′ . U ′ and X ′ are then used in computing separate labeled and unlabeled loss terms. More formally, the combined loss L for semi-supervised learning is defined as</p><formula xml:id="formula_1">X ′ , U ′ = MixMatch(X , U , T, K, α)<label>(2)</label></formula><formula xml:id="formula_2">L X = 1 |X ′ | x,p∈X ′ H(p, p model (y | x; θ))<label>(3)</label></formula><formula xml:id="formula_3">L U = 1 L|U ′ | u,q∈U ′ q − p model (y | u; θ) 2 2 (4) L = L X + λ U L U</formula><p>(5) where H(p, q) is the cross-entropy between distributions p and q, and T , K, α, and λ U are hyperparameters described below. The full MixMatch algorithm is provided in algorithm 1, and a diagram of the label guessing process is shown in fig. <ref type="figure" target="#fig_0">1</ref>. Next, we describe each part of MixMatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Augmentation</head><p>As is typical in many SSL methods, we use data augmentation both on labeled and unlabeled data. For each x b in the batch of labeled data X , we generate a transformed version xb = Augment(x b ) (algorithm 1, line 3). For each u b in the batch of unlabeled data U , we generate K augmentations ûb,k = Augment(u b ), k ∈ (1, . . . , K) (algorithm 1, line 5). We use these individual augmentations to generate a "guessed label" q b for each u b , through a process we describe in the following subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Label Guessing</head><p>For each unlabeled example in U , MixMatch produces a "guess" for the example's label using the model's predictions. This guess is later used in the unsupervised loss term. To do so, we compute the average of the model's predicted class distributions across all the K augmentations of u b by</p><formula xml:id="formula_4">qb = 1 K K k=1 p model (y | ûb,k ; θ)<label>(6)</label></formula><p>in algorithm 1, line 7. Using data augmentation to obtain an artificial target for an unlabeled example is common in consistency regularization methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>Algorithm 1 MixMatch takes a batch of labeled data X and a batch of unlabeled data U and produces a collection X ′ (resp. U ′ ) of processed labeled examples (resp. unlabeled with guessed labels). q b = Sharpen(q b , T ) // Apply temperature sharpening to the average prediction (see eq. // Combine and shuffle labeled and unlabeled data 13:</p><formula xml:id="formula_5">X ′ = MixUp( Xi, Wi); i ∈ (1, . . . , | X |)</formula><p>// Apply MixUp to labeled data and entries from W 14:</p><formula xml:id="formula_6">U ′ = MixUp( Ûi, W i+| X | ); i ∈ (1, . . . , | Û |)</formula><p>// Apply MixUp to unlabeled data and the rest of W 15: return X ′ , U ′ Sharpening. In generating a label guess, we perform one additional step inspired by the success of entropy minimization in semi-supervised learning (discussed in section 2.2). Given the average prediction over augmentations qb , we apply a sharpening function to reduce the entropy of the label distribution. In practice, for the sharpening function, we use the common approach of adjusting the "temperature" of this categorical distribution <ref type="bibr" target="#b15">[16]</ref>, which is defined as the operation</p><formula xml:id="formula_7">Sharpen(p, T ) i := p 1 T i L j=1 p 1 T j (7)</formula><p>where p is some input categorical distribution (specifically in MixMatch, p is the average class prediction over augmentations qb , as shown in algorithm 1, line 8) and T is a hyperparameter. As T → 0, the output of Sharpen(p, T ) will approach a Dirac ("one-hot") distribution. Since we will later use q b = Sharpen(q b , T ) as a target for the model's prediction for an augmentation of u b , lowering the temperature encourages the model to produce lower-entropy predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">MixUp</head><p>We use MixUp for semi-supervised learning, and unlike past work for SSL we mix both labeled examples and unlabeled examples with label guesses (generated as described in section 3.2). To be compatible with our separate loss terms, we define a slightly modified version of MixUp. For a pair of two examples with their corresponding labels probabilities (x 1 , p 1 ), (x 2 , p 2 ) we compute (x ′ , p ′ ) by</p><formula xml:id="formula_8">λ ∼ Beta(α, α)<label>(8)</label></formula><formula xml:id="formula_9">λ ′ = max(λ, 1 − λ)<label>(9)</label></formula><p>x</p><formula xml:id="formula_10">′ = λ ′ x 1 + (1 − λ ′ )x 2<label>(10)</label></formula><formula xml:id="formula_11">p ′ = λ ′ p 1 + (1 − λ ′ )p 2 (<label>11</label></formula><formula xml:id="formula_12">)</formula><p>where α is a hyperparameter. Vanilla MixUp omits eq. ( <ref type="formula" target="#formula_9">9</ref>) (i.e. it sets λ ′ = λ). Given that labeled and unlabeled examples are concatenated in the same batch, we need to preserve the order of the batch to compute individual loss components appropriately. This is achieved by eq. ( <ref type="formula" target="#formula_9">9</ref>) which ensures that x ′ is closer to x 1 than to x 2 . To apply MixUp, we first collect all augmented labeled examples with their labels and all unlabeled examples with their guessed labels into</p><formula xml:id="formula_13">X = (x b , p b ); b ∈ (1, . . . , B)<label>(12)</label></formula><formula xml:id="formula_14">Û = (û b,k , q b ); b ∈ (1, . . . , B), k ∈ (1, . . . , K)<label>(13)</label></formula><p>(algorithm 1, lines 10-11). Then, we combine these collections and shuffle the result to form W which will serve as a data source for MixUp (algorithm 1, line 12). For each the i th example-label pair in X , we compute MixUp( Xi , W i ) and add the result to the collection X ′ (algorithm 1, line 13). We compute U ′ i = MixUp( Ûi , W i+| X | ) for i ∈ (1, . . . , | Û|), intentionally using the remainder of W that was not used in the construction of X ′ (algorithm 1, line 14). To summarize, MixMatch transforms X into X ′ , a collection of labeled examples which have had data augmentation and MixUp (potentially mixed with an unlabeled example) applied. Similarly, U is transformed into U ′ , a collection of multiple augmentations of each unlabeled example with corresponding label guesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss Function</head><p>Given our processed batches X ′ and U ′ , we use the standard semi-supervised loss shown in eqs. (3) to <ref type="bibr" target="#b4">(5)</ref>. Equation ( <ref type="formula">5</ref>) combines the typical cross-entropy loss between labels and model predictions from X ′ with the squared L 2 loss on predictions and guessed labels from U ′ . We use this L 2 loss in eq. ( <ref type="formula">4</ref>) (the multiclass Brier score <ref type="bibr" target="#b4">[5]</ref>) because, unlike the cross-entropy, it is bounded and less sensitive to incorrect predictions. For this reason, it is often used as the unlabeled data loss in SSL <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b43">44]</ref> as well as a measure of predictive uncertainty <ref type="bibr" target="#b25">[26]</ref>. We do not propagate gradients through computing the guessed labels, as is standard <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Hyperparameters</head><p>Since MixMatch combines multiple mechanisms for leveraging unlabeled data, it introduces various hyperparameters -specifically, the sharpening temperature T , number of unlabeled augmentations K, α parameter for Beta in MixUp, and the unsupervised loss weight λ U . In practice, semi-supervised learning methods with many hyperparameters can be problematic because cross-validation is difficult with small validation sets <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b34">35]</ref>. However, we find in practice that most of MixMatch's hyperparameters can be fixed and do not need to be tuned on a per-experiment or per-dataset basis. Specifically, for all experiments we set T = 0.5 and K = 2. Further, we only change α and λ U on a per-dataset basis; we found that α = 0.75 and λ U = 100 are good starting points for tuning. In all experiments, we linearly ramp up λ U to its maximum value over the first 16,000 steps of training as is common practice <ref type="bibr" target="#b43">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We test the effectiveness of MixMatch on standard SSL benchmarks (section 4.2). Our ablation study teases apart the contribution of each of MixMatch's components (section 4.2.3). As an additional application, we consider privacy-preserving learning in section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation details</head><p>Unless otherwise noted, in all experiments we use the "Wide ResNet-28" model from <ref type="bibr" target="#b34">[35]</ref>. Our implementation of the model and training procedure closely matches that of <ref type="bibr" target="#b34">[35]</ref> (including using 5000 examples to select the hyperparameters), except for the following differences: First, instead of decaying the learning rate, we evaluate models using an exponential moving average of their parameters with a decay rate of 0.999. Second, we apply a weight decay of 0.0004 at each update for the Wide ResNet-28 model. Finally, we checkpoint every 2 16 training samples and report the median error rate of the last 20 checkpoints. This simplifies the analysis at a potential cost to accuracy by, for example, averaging checkpoints <ref type="bibr" target="#b1">[2]</ref> or choosing the checkpoint with the lowest validation error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Semi-Supervised Learning</head><p>First, we evaluate the effectiveness of MixMatch on four standard benchmark datasets: CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b23">[24]</ref>, SVHN <ref type="bibr" target="#b31">[32]</ref>, and STL-10 <ref type="bibr" target="#b7">[8]</ref>. Standard practice for evaluating semi-supervised learning on the first three datasets is to treat most of the dataset as unlabeled and use a small portion as labeled data. STL-10 is a dataset specifically designed for SSL, with 5,000 labeled images and 100,000 unlabeled images which are drawn from a slightly different distribution than the labeled data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Baseline Methods</head><p>As baselines, we consider the four methods considered in <ref type="bibr" target="#b34">[35]</ref> (Π-Model <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">40]</ref>, Mean Teacher <ref type="bibr" target="#b43">[44]</ref>, Virtual Adversarial Training <ref type="bibr" target="#b30">[31]</ref>, and Pseudo-Label <ref type="bibr" target="#b27">[28]</ref>) which are described in section 2. We also use MixUp <ref type="bibr" target="#b46">[47]</ref> on its own as a baseline. MixUp is designed as a regularizer for supervised learning, so we modify it for SSL by applying it both to augmented labeled examples and augmented unlabeled examples with their corresponding predictions. In accordance with standard usage of MixUp, we use a cross-entropy loss between the MixUp-generated guess label and the model's prediction. As advocated by <ref type="bibr" target="#b34">[35]</ref>, we reimplemented each of these methods in the same codebase and applied them to the same model (described in section 4.1) to ensure a fair comparison. We re-tuned the hyperparameters for each baseline method, which generally resulted in a marginal accuracy improvement compared to those in <ref type="bibr" target="#b34">[35]</ref>, thereby providing a more competitive experimental setting for testing out MixMatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10</head><p>For CIFAR-10, we evaluate the accuracy of each method with a varying number of labeled examples from 250 to 4000 (as is standard practice). The results can be seen in fig. <ref type="figure">2</ref>. We used λ U = 75 for CIFAR-10. We created 5 splits for each number of labeled points, each with a different random seed. Each model was trained on each split and the error rates were reported by the mean and variance across splits. We find that MixMatch outperforms all other methods by a significant margin, for example reaching an error rate of 6.24% with 4000 labels. For reference, on the same model, fully supervised training on all 50000 samples achieves an error rate of 4.17%. Furthermore, MixMatch obtains an error rate of 11.08% with only 250 labels. For comparison, at 250 labels the next-best-performing method (VAT <ref type="bibr" target="#b30">[31]</ref>) achieves an error rate of 36.03, over 4.5× higher than MixMatch considering that 4.17% is the error limit obtained on our model with fully supervised learning. In addition, at 4000 labels the next-best-performing method (Mean Teacher <ref type="bibr" target="#b43">[44]</ref>) obtains an error rate of 10.36%, which suggests that MixMatch can achieve similar performance with only 1/16 as many labels. We believe that the most interesting comparisons are with very few labeled data points since it reveals the method's sample efficiency which is central to SSL. CIFAR-10 and CIFAR-100 with a larger model Some prior work <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b1">2]</ref> has also considered the use of a larger, 26 million-parameter model. Our base model, as used in <ref type="bibr" target="#b34">[35]</ref>, has only 1.5 million parameters which confounds comparison with these results. For a more reasonable comparison to these results, we measure the effect of increasing the width of our base ResNet model and evaluate MixMatch's performance on a 28-layer Wide Resnet model which has 135 filters per layer, resulting in 26 million parameters. We also evaluate MixMatch on this larger model on CIFAR-100 with 10000 labels, to compare to the corresponding result from <ref type="bibr" target="#b1">[2]</ref>. The results are shown in table 1. In general, MixMatch matches or outperforms the best results from <ref type="bibr" target="#b1">[2]</ref>, though we note that the comparison still remains problematic due to the fact that the model from <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b1">2]</ref>   <ref type="table">3</ref>: Comparison of error rates for SVHN and SVHN+Extra for MixMatch. The last column ("All") contains the fully-supervised performance with all labels in the corresponding training set. sophisticated "shake-shake" regularization <ref type="bibr" target="#b14">[15]</ref>. For this model, we used a weight decay of 0.0008. We used λ U = 75 for CIFAR-10 and λ U = 150 for CIFAR-100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SVHN and SVHN+Extra</head><p>As with CIFAR-10, we evaluate the performance of each SSL method on SVHN with a varying number of labels from 250 to 4000. As is standard practice, we first consider the setting where the 73257-example training set is split into labeled and unlabeled data. The results are shown in fig. <ref type="figure">3</ref>. We used λ U = 250. Here again the models were evaluated on 5 splits for each number of labeled points, each with a different random seed. We found MixMatch's performance to be relatively constant (and better than all other methods) across all amounts of labeled data. Surprisingly, after additional tuning we were able to obtain extremely good performance from Mean Teacher <ref type="bibr" target="#b43">[44]</ref>, though its error rate was consistently slightly higher than MixMatch's.</p><p>Note that SVHN has two training sets: train and extra. In fully-supervised learning, both sets are concatenated to form the full training set (604388 samples). In SSL, for historical reasons the extra set was left aside and only train was used (73257 samples). We argue that leveraging both train and extra for the unlabeled data is more interesting since it exhibits a higher ratio of unlabeled samples over labeled ones. We report error rates for both SVHN and SVHN+Extra in table <ref type="table">3</ref>. For SVHN+Extra we used α = 0.25, λ U = 250 and a lower weight decay of 0.000002 due to the larger amount of available data. We found that on both training sets, MixMatch nearly matches the fully-supervised performance on the same training set almost immediately -for example, MixMatch achieves an error rate of 2.22% with only 250 labels on SVHN+Extra compared to the fully-supervised performance of 1.71%. Interestingly, on SVHN+Extra MixMatch outperformed fully supervised training on SVHN without extra (2.59% error) for every labeled data amount considered. To emphasize the importance of this, consider the following scenario: You have 73257 examples from SVHN with 250 examples labeled and are given a choice: You can either obtain 8× more unlabeled data and use MixMatch or obtain 293× more labeled data and use fully-supervised learning. Our results suggest that obtaining additional unlabeled data and using MixMatch is more effective, which conveniently is likely much cheaper than obtaining 293× more labels. Note that none of the baselines in table 2 use the same experimental setup (i.e. model), so it is difficult to directly compare the results; however, because MixMatch obtains the lowest error by a factor of two, we take this to be a vote in confidence of our method. We used λ U = 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Ablation Study</head><p>Since MixMatch combines various semi-supervised learning mechanisms, it has a good deal in common with existing methods in the literature. As a result, we study the effect of removing or • removing temperature sharpening (i.e. setting T = 1)</p><p>• using an exponential moving average (EMA) of model parameters when producing guessed labels, as is done by Mean Teacher <ref type="bibr" target="#b43">[44]</ref> • performing MixUp between labeled examples only, unlabeled examples only, and without mixing across labeled and unlabeled examples</p><p>• using Interpolation Consistency Training <ref type="bibr" target="#b44">[45]</ref>, which can be seen as a special case of this ablation study where only unlabeled mixup is used, no sharpening is applied and EMA parameters are used for label guessing.</p><p>We carried out the ablation on CIFAR-10 with 250 and 4000 labels; the results are shown in table <ref type="table" target="#tab_3">4</ref>. We find that each component contributes to MixMatch's performance, with the most dramatic differences in the 250-label setting. Despite Mean Teacher's effectiveness on SVHN (fig. <ref type="figure">3</ref>), we found that using a similar EMA of parameter values hurt MixMatch's performance slightly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Privacy-Preserving Learning and Generalization</head><p>Learning with privacy allows us to measure our approach's ability to generalize. Indeed, protecting the privacy of training data amounts to proving that the model does not overfit: a learning algorithm is said to be differentially private (the most widely accepted technical definition of privacy) if adding, modifying, or removing any of its training samples is guaranteed not to result in a statistically significant difference in the model parameters learned <ref type="bibr" target="#b12">[13]</ref>. For this reason, learning with differential privacy is, in practice, a form of regularization <ref type="bibr" target="#b32">[33]</ref>. Each training data access constitutes a potential privacy leakage, encoded as the pair of the input and its label. Hence, approaches for deep learning from private training data, such as DP-SGD <ref type="bibr" target="#b0">[1]</ref> and PATE <ref type="bibr" target="#b35">[36]</ref>, benefit from accessing as few labeled private training points as possible when computing updates to the model parameters. Semi-supervised learning is a natural fit for this setting.</p><p>We use the PATE framework for learning with privacy. A student is trained in a semi-supervised way from public unlabeled data, part of which is labeled by an ensemble of teachers with access to private labeled training data. The fewer labels a student requires to reach a fixed accuracy, the stronger is the privacy guarantee it provides. Teachers use a noisy voting mechanism to respond to label queries from the student, and they may choose not to provide a label when they cannot reach a sufficiently strong consensus. For this reason, if MixMatch improves the performance of PATE, it would also illustrate MixMatch's improved generalization from few canonical exemplars of each class.</p><p>We compare the accuracy-privacy trade-off achieved by MixMatch to a VAT <ref type="bibr" target="#b30">[31]</ref> baseline on SVHN.</p><p>VAT achieved the previous state-of-the-art of 91.6% test accuracy for a privacy loss of ε = 4.96 <ref type="bibr" target="#b36">[37]</ref>.</p><p>Because MixMatch performs well with few labeled points, it is able to achieve 95.21 ± 0.17% test accuracy for a much smaller privacy loss of ε = 0.97. Because e ε is used to measure the degree of privacy, the improvement is approximately e 4 ≈ 55×, a significant improvement. A privacy loss ε below 1 corresponds to a much stronger privacy guarantee. Note that in the private training setting the student model only uses 10,000 total examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduced MixMatch, a semi-supervised learning method which combines ideas and components from the current dominant paradigms for SSL. Through extensive experiments on semi-supervised and privacy-preserving learning, we found that MixMatch exhibited significantly improved performance compared to other methods in all settings we studied, often by a factor of two or more reduction in error rate. In future work, we are interested in incorporating additional ideas from the semi-supervised learning literature into hybrid methods and continuing to explore which components result in effective algorithms. Separately, most modern work on semi-supervised learning algorithms is evaluated on image benchmarks; we are interested in exploring the effectiveness of MixMatch in other domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Diagram of the label guessing process used in MixMatch. Stochastic data augmentation is applied to an unlabeled image K times, and each augmented image is fed through the classifier.Then, the average of these K predictions is "sharpened" by adjusting the distribution's temperature. See algorithm 1 for a full description.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 : 1 Kk</head><label>11</label><figDesc>Input: Batch of labeled examples and their one-hot labels X = (x b , p b ); b ∈ (1, . . . , B) , batch of unlabeled examples U = u b ; b ∈ (1, . . . , B) , sharpening temperature T , number of augmentations K, Beta distribution parameter α for MixUp. 2: for b = 1 to B do 3: xb = Augment(x b ) // Apply data augmentation to x b 4: for k = 1 to K do 5: ûb,k = Augment(u b ) // Apply k th round of data augmentation to u b p model (y | ûb,k ; θ) // Compute average predictions across all augmentations of u b 8:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>end for 10: X = (x b , p b ); b ∈ (1, . . . , B) // Augmented labeled examples and their labels 11: Û = (û b,k , q b ); b ∈ (1, . . . , B), k ∈ (1, . . . , K) // Augmented unlabeled examples, guessed labels 12: W = Shuffle Concat( X , Û )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>STL- 10</head><label>10</label><figDesc>STL-10 contains 5000 training examples aimed at being used with 10 predefined folds (we use the first 5 only) with 1000 examples each. However, some prior work trains on all 5000 examples. We thus compare in both experimental settings. With 1000 examples MixMatch surpasses both the state-of-the-art for 1000 examples as well as the state-of-the-art using all 5000 labeled examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>also uses more CIFAR-10 and CIFAR-100 error rate (with 4,000 and 10,000 labels respectively) with larger models (26 million parameters).</figDesc><table><row><cell>Method</cell><cell>CIFAR-10</cell><cell>CIFAR-100</cell><cell>Method</cell><cell cols="2">1000 labels 5000 labels</cell></row><row><cell>Mean Teacher [44] SWA [2]</cell><cell>6.28 5.00</cell><cell>-28.80</cell><cell>CutOut [12] IIC [20] SWWAE [48]</cell><cell>--25.70</cell><cell>12.74 11.20 -</cell></row><row><cell>MixMatch</cell><cell cols="2">4.95 ± 0.08 25.88 ± 0.30</cell><cell>CC-GAN 2 [11]</cell><cell>22.20</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>MixMatch</cell><cell>10.18 ± 1.46</cell><cell>5.59</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>STL-10 error rate using 1000-label splits or the entire 5000-label training set. ± 0.26 3.64 ± 0.46 3.27 ± 0.31 3.04 ± 0.13 2.89 ± 0.06 2.59 SVHN+Extra 2.22 ± 0.08 2.17 ± 0.07 2.18 ± 0.06 2.12 ± 0.03 2.07 ± 0.05 1.71</figDesc><table><row><cell>Labels</cell><cell>250</cell><cell>500</cell><cell>1000</cell><cell>2000</cell><cell>4000</cell><cell>All</cell></row><row><cell>SVHN 3.78 Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation study results. All values are error rates on CIFAR-10 with 250 or 4000 labels.adding components in order to provide additional insight into what makes MixMatch performant. Specifically, we measure the effect of • using the mean class distribution over K augmentations or using the class distribution for a single augmentation (i.e. setting K = 1)</figDesc><table><row><cell>Ablation</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/google-research/mixmatch</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank Balaji Lakshminarayanan for his helpful theoretical insights.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning with differential privacy</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Brendan Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security</title>
				<meeting>the 2016 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="308" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Improving consistency-based semi-supervised learning with weight averaging</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05594</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Label Propagation and Quadratic Criterion, chapter 11</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Delalleau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Verification of forecasts expressed in terms of probability</title>
		<author>
			<persName><forename type="first">Glenn</forename><forename type="middle">W</forename><surname>Brier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monthey Weather Review</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep, big, simple neural nets for handwritten digit recognition</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Claudiu Cireşan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ueli</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3207" to="3220" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
				<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The importance of encoding versus training with sparse coding and vector quantization</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Learning augmentation policies from data</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with context-conditional generative adversarial networks</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06430</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Calibrating noise to sensitivity in private data analysis</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kobbi</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Privacy and Confidentiality</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="17" to="51" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning by transduction</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Gammerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volodya</forename><surname>Vovk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence</title>
				<meeting>the Fourteenth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Shake-shake regularization</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Gastaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth International Conference on Learning Representations (Workshop Track)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spike-and-slab sparse coding for unsupervised feature discovery</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Challenges in Learning Hierarchical Models</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Keeping neural networks simple by minimizing the description length of the weights</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><surname>Van Camp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Annual ACM Conference on Computational Learning Theory</title>
				<meeting>the 6th Annual ACM Conference on Computational Learning Theory</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Invariant information distillation for unsupervised image segmentation and clustering</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06653</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Transductive inference for text classification using support vector machines</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transductive learning via spectral graph partitioning</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semisupervised learning with deep generative models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Balaji Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Principled hybrids of generative and discriminative models</title>
		<author>
			<persName><forename type="first">Julia</forename><forename type="middle">A</forename><surname>Lasserre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">P</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Challenges in Representation Learning</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep metric transfer for label propagation with limited annotated data</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08781</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Fixing weight decay regularization in Adam</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin-Ichi</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">On the generalization properties of differential privacy</title>
		<author>
			<persName><forename type="first">Kobbi</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Stemmer</surname></persName>
		</author>
		<idno>CoRR, abs/1504.05800</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with generative adversarial networks</title>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01583</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Realistic evaluation of deep semi-supervised learning algorithms</title>
		<author>
			<persName><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekin</forename><surname>Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3235" to="3246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Semisupervised knowledge transfer for deep learning from private training data</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulfar</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.05755</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Ananth Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Úlfar</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><surname>Erlingsson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08908</idno>
		<title level="m">Scalable private learning with pate</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Variational autoencoder for deep learning of images, labels and captions</title>
		<author>
			<persName><forename type="first">Yunchen</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semisupervised learning with ladder networks</title>
		<author>
			<persName><forename type="first">Antti</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikko</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehran</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Using deep belief nets to learn covariance kernels for Gaussian processes</title>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Best practice for convolutional neural networks applied to visual document analysis</title>
		<author>
			<persName><forename type="first">Patrice</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
				<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Interpolation consistency training for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03825</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12281</idno>
		<title level="m">Three mechanisms of weight decay regularization</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Stacked what-where autoencoders</title>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02351</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
