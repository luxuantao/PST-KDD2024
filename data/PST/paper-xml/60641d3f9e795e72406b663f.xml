<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Keyword-aware Abstractive Summarization by Extracting Set-level Intermediate Summaries</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yizhu</forename><surname>Liu</surname></persName>
							<email>liuyizhu@sjtu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Jia</surname></persName>
							<email>jia_qi@sjtu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Kenny</forename><forename type="middle">Q</forename><surname>Zhu</surname></persName>
							<email>kzhu@cs.sjtu.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Keyword-aware Abstractive Summarization by Extracting Set-level Intermediate Summaries</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3442381.3449906</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Information systems â†’ Summarization Abstractive Summarization</term>
					<term>Alignment</term>
					<term>Set-level Pseudo-summaries</term>
					<term>Reinforcement Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstractive summarization is useful in providing a summary or a digest of news or other web texts and enhancing users reading experience, especially when they are reading on small displays such as mobile phones. However, existing encoder-decoder summarization models have difficulty learning the latent alignment between source documents and summaries because of their vast disparity in length. In this paper, we propose a extractor-abstractor framework in which the keyword-based extractor selects a few sets of salient sentences from the input document and then the abstractor paraphrases these sets of sentences in parallel, which are more aligned to the summary, to generate the final summary. The new extractor and abstractor are pretrained from a set of "pseudo summaries" extracted by specially designed heuristics, and then further trained together in a reinforcement learning framework. The results show that the proposed model generates high-quality summaries with faster training speed and less training memory footprint, and outperforms the state-of-the-art models on CNN/Daily Mail, Webis-TLDR-17, Webis-Snippet-20, WikiHow and DUC-2002 datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Abstractive summarization is the task of creating a short, accurate, and informative summary from a long text document without using the exact sentences from the source. This is useful in generating a snippet or digest for a searched web page or other web text. The essence of summarization is to compress information from the input document and retain only most important information in the output. This process can be seen as aligning the salient information from the source to the output. Recently, encoder-decoder (enc-dec) models with attention mechanism <ref type="bibr" target="#b4">[3,</ref><ref type="bibr" target="#b5">4,</ref><ref type="bibr" target="#b9">8,</ref><ref type="bibr" target="#b10">9,</ref><ref type="bibr" target="#b22">21,</ref><ref type="bibr" target="#b27">26]</ref> have made great progress on abstractive summarization. The attention mechanism is a way of capturing the alignment between input sequences of the encoder and decoder, trying to tell which parts of the source document are relevant to which parts in the summary. However, since lots of non-essential parts in the source document are omitted in the summary, the alignment using only attention mechanism is unsatisfactory. Table <ref type="table">1</ref> shows that two state-of-the-art enc-dec models, i.e., PointGen <ref type="bibr" target="#b27">[26]</ref> and BART <ref type="bibr" target="#b14">[13]</ref>, both frequently make incorrect alignments by either missing some salient parts or including redundancies.</p><p>Source document new delhi, india police have arrested four employees. federal education minister smriti irani was visiting a fabindia outlet in the tourist resort state of goa on friday when she discovered a surveillance camera pointed at the store 's changing room. four employees of the store have been arrested, but its manager was still at large saturday . state authorities found an overhead camera that the minister had spotted and determined that it was indeed able to take photos of customers. authorities sealed off the store and summoned six top officials from fabindia. the arrested staff have been charged with voyeurism and breach of privacy. if convicted, they could spend up to three years in jail . Reference summary federal education minister smriti irani visited a fabindia store in goa , saw cameras. authoroities discovered the cameras could capture photos from the store 's changing room. the four store workers arrested could spend three years each in prison if convicted . PointGen <ref type="bibr" target="#b27">[26]</ref> four employees of a popular indian ethnic chain have been arrested, but its manager was still at large . authorities sealed off the store and summoned six top officials from fabindia.</p><p>BART <ref type="bibr" target="#b14">[13]</ref> federal education minister smriti irani was visiting a fabindia outlet in the tourist resort state of goa . she discovered a surveillance camera pointed at the changing room . four employees of the store have been arrested , but the manager is still at large . the arrested staff have been charged with voyeurism and breach of privacy Table <ref type="table">1</ref>: Summarization results by SOTA models (PointGen and BART). The bold words or phrases are salient information. The underlined parts are redundant information in the output. Some salient information is also missing from the output.</p><p>An alternate view <ref type="bibr" target="#b2">[1,</ref><ref type="bibr" target="#b6">5]</ref> of the summarization process is to paraphrase the salient parts in the source document, i.e., summary sentences are aligned to the salient parts of source document (see Table <ref type="table">1</ref>). This gives rise to a two-stage, extractor-abstractor (ext-abs) framework, which first selects salient sentences from the source (extractor) and then paraphrases the selected ones to generate a summary (abstractor). The ext-abs framework has two advantages: i) the input and output of the abstractor can be better aligned; ii) reduced size of the input to the abstractor reduces both training and inference time.</p><p>To train an ext-abs framework, one has first to generate the intermediate results, i.e., the salient sentences in the input document for all training samples. Since the real salient sentences are not known in practice, we call the intermediate result obtained algorithmically the pseudo summary. Pseudo summaries are used for training both the extractor and the abstractor in the ext-abs framework. As the intermediate result, the low-quality pseudo summaries can bring noises to the model. Better pseudo summaries can reduce the noise and enhance the alignment between encoder and decoder of abstractor. Previously, there are two types of heuristics to create pseudo summaries: sentence-level <ref type="bibr" target="#b6">[5]</ref> and summary-level methods <ref type="bibr" target="#b21">[20,</ref><ref type="bibr" target="#b28">27]</ref>.</p><p>Sentence-level methods assume that there is one unique salient sentence in the source that matches each sentence in the reference summary. To this end, they extract the sentence with the highest ROUGE score <ref type="bibr" target="#b16">[15]</ref> for each reference sentence. This simple assumption gives rise to the design of parallel abstractors (one for each reference sentence) to achieve speed-up. However, the very nature of summarization dictates that a sentence in the summary may be condensed from multiple sentences in the source and not just one. For example, in Table <ref type="table">2</ref>, the first sentence in sentence-level pseudo summary is pertinent to both 1 ğ‘ ğ‘¡ and 2 ğ‘›ğ‘‘ reference sentences in Table <ref type="table">1</ref>, while the second pseudo sentence misses out some information ("changing room") of 2 ğ‘›ğ‘‘ reference sentence. In response to this deficiency, summary-level methods were proposed to select the best combination of a subset of input sentences that maximizes ROUGE score with reference summary as a whole. Nevertheless, they lose the advantage of parallelism in the sentence-level approach. Worse still, when mixing all the sentences together, they treat every token equally in computing the ROUGE, resulting in pseudo summaries that are similar to the reference only by unimportant words. For example, The summary-level pseudo summary in Table <ref type="table">2</ref> doesn't match the information about "authorities" and "arrested", which are more important in the story.</p><p>In this paper, we present a novel set-level matching heuristics that divides the reference summary into a few disjoint clusters of sentences, each of which represents a topic or an aspect, and matches a non-overlapping set of sentences in the source with each cluster of reference sentences. This new heuristics strives to trade off the pros and cons of the previous two approaches. Instead of assuming one-to-one or all-to-all alignment between the pseudo summary and the reference summary, we are assuming a many-to-many alignment, which allows for more flexible alignment while still achieving parallelism using multiple abstractors. When computing the similarity between the pseudo summary and the reference, on top of ordinary ROUGE scores, we emphasize keywords in the reference summary. This amounts to representing summaries not only as a sequence of words but also as a set of important keywords. Accordingly, we Sentence-level 1) federal education minister smriti irani was visiting a fabindia outlet in the tourist resort state of goa on friday when she discovered a surveillance camera pointed at the store's changing room.</p><p>2) state authorities found an overhead camera that the minister had spot -ted and determined that it was indeed able to take photos of customers.</p><p>3) if convicted, they could spend up to three years in jail. Summary-level federal education minister smriti irani was visiting a fabindia outlet in the tourist resort state of goa on friday when she discovered a surveillanc camera pointed at the store's changing room. if convicted, they could spend up to three years in jail. Set-level based on Keywords Set 1) federal education minister smriti irani was visiting a fabindia outlet in the tourist resort state of goa on friday when she discovered a surveillance camera pointed at the changing room. state authorities found an overhead camera that the minister had spotted and determined that it was indeed able to take photos of customers. Set 2) four employees of the store have been arrested. if convicted, they could spend up to three years in jail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2:</head><p>The pseudo summaries produced by different heuristics for the source and reference in Table <ref type="table">1</ref>.</p><p>design a keyword-aware extractor which includes both an ordinary document encoder and a keyword encoder.</p><p>One natural way to connect the extractor and abstractor into an end-to-end trainable model is to use reinforcement learning (RL). Previous ext-abs models use sentence-level <ref type="bibr" target="#b6">[5]</ref> or summary-level <ref type="bibr" target="#b2">[1]</ref> ROUGE scores as the reward. The sentence-level rewards can not properly reflect the quality of overall summary because of overlapping contents <ref type="bibr" target="#b2">[1,</ref><ref type="bibr" target="#b23">22]</ref>, while summary-level rewards ignore the accuracy of the sentences extracted at each step. Therefore, we propose a comprehensive reward which is the weighted sum of sentence/set/summary-level ROUGE scores. This comprehensive reward can help the extractor select sentences that match abstractive reference summaries better.</p><p>In summary, our contributions are as follows:</p><p>(1) Our set-level matching heuristics extracts better pseudo summaries as the training data to pretrain both the extractor and the abstractor, and subsequently allows the abstractor to learn the alignments effectively. (See Section 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">APPROACH</head><p>Our new ext-abs framework is illustrated in Figure <ref type="figure">1</ref>. There are three main components: a keyword-based extractor, an abstractor and a comprehensively rewarded reinforcement learning (RL). As a preprocessing step, we first obtain the set-level pseudo summary from the training data. We then pretrain the keyword-based extractor using the source document and the pseudo summary, and the parallel abstractor using the pseudo summary and the reference summary. Finally, we use RL to bridge the pretrained extractor and abstractor to further finetune the parameters in both models. The RL updates the extractor and abstractor by a comprehensive reward evaluating both the extracted intermediate summaries and abstractive summaries at sentence-level, set-level and summary-level.</p><p>In the rest of this paper, we use the following definitions.</p><p>â€¢ A source document ğ· is a sequence of sentences (ğ‘‘ 0 , ..., ğ‘‘ ğ‘– , ...);</p><p>â€¢ A set-level pseudo summary ğ‘ƒ is a sequence of sentences organized in sets denoted as (ğ‘ 0 0 , ğ‘ Next, we describe the preprocessing of the training data to obtain pseudo summaries, and the key components in the framework. <ref type="foot" target="#foot_0">1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Pre-processing: Set-level Matching Heuristics</head><p>In order to enhance the alignment between pseudo summaries and generated summaries, we propose a set-level matching heuristics to obtain pseudo summaries based on a set of keywords. We use TextRank algorithm <ref type="bibr" target="#b20">[19]</ref> to extract the keywords from the reference summary and obtain the set-level pseudo summary by Algorithm 1. For instance, as shown in Figure <ref type="figure" target="#fig_0">2</ref> we extract the sentence sets covering the most reference keywords (bold) with the highest ROUGE-2 scores from the source document for each reference sentence. Then, if there is an overlap between two extracted sentence sets, the two sets will be merged into one and their reference sentences will also be merged into a longer sentence. In the end, each sentence set in pseudo summary has a corresponding sentence set in reference summary. As a result, in Figure <ref type="figure" target="#fig_0">2</ref> the 1st reference sentence matches source sentence 1), and the best matching for the 2nd reference sentence is the combination of source sentence 1) and 2). The pseudo summary set consisting of source sentence 1) and 2) is corresponding to the combination of 1st and 2nd reference sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Keyword-based Extractor (KE)</head><p>In extractive summarization, we take document ğ· as input and setlevel pseudo summary ğ‘ƒ as output. Our extractor consists of a dual encoder and an aligned pointer decoder. </p><formula xml:id="formula_0">ğ‘œ ğ‘šğ‘ğ‘¥ â† ğ‘œ (ğ‘–ğ‘›ğ‘–ğ‘¡, ğ‘˜ ğ‘– ), ğ‘“ 1 ğ‘šğ‘ğ‘¥ â† ğ‘“ 1(ğ‘–ğ‘›ğ‘–ğ‘¡, ğ‘Ÿ ğ‘– ) ğ· â€² â† ğ· âˆ’ ğ‘–ğ‘›ğ‘–ğ‘¡, r â† ğ‘Ÿ ğ‘– for ğ‘— = 0 â†’ ğ‘™ğ‘’ğ‘›(ğ·) do if ğ‘œ (ğ‘‘ â€² ğ‘— , ğ‘˜ ğ‘– ) &gt; ğ‘œ ğ‘šğ‘ğ‘¥ or (ğ‘œ (ğ‘‘ â€² ğ‘— , ğ‘˜ ğ‘– ) = ğ‘œ ğ‘šğ‘ğ‘¥ and ğ‘“ 1(ğ‘‘ â€² ğ‘— , ğ‘Ÿ ğ‘– ) &gt; ğ‘“ 1 ğ‘šğ‘ğ‘¥ ) then ğ‘ â† ğ‘ âˆª {ğ‘‘ â€² ğ‘— } ğ‘œ ğ‘šğ‘ğ‘¥ â† ğ‘œ (ğ‘, ğ‘˜ ğ‘– ) ğ‘“ 1 ğ‘šğ‘ğ‘¥ â† ğ‘“ 1(ğ‘, ğ‘Ÿ ğ‘– ) ğ‘‘ â€² â† ğ‘‘ â€² âˆ’ ğ‘ for ğ‘— = 0 â†’ ğ‘™ğ‘’ğ‘›(ğ‘‘ â€² ) do if ğ‘“ 1(ğ‘‘ â€² ğ‘— , ğ‘Ÿ ğ‘– ) &gt; ğ‘“ 1 ğ‘šğ‘ğ‘¥ then ğ‘ â† ğ‘ âˆª {ğ‘‘ â€² ğ‘— } ğ‘œ ğ‘šğ‘ğ‘¥ â† ğ‘œ (ğ‘, ğ‘˜ ğ‘– ) ğ‘“ 1 ğ‘šğ‘ğ‘¥ â† ğ‘“ 1(ğ‘, ğ‘Ÿ ğ‘– ) Add ğ‘ into ğ‘ƒ Add r into R while the last two sub-sets in ğ‘ƒ have overlap do</formula><p>Merge last two sub-sets in ğ‘ƒ Merge last two sub-sets in R return ğ‘ƒ, R learns sentence representations using a language model and helps with natural language understanding. Keywords encoder learns keywords representations and guides the decoder to select more accurate sentences. The model is illustrated in Figure <ref type="figure" target="#fig_1">3</ref>. Keywords Encoder. We use TextRank algorithm to receive a sequential list of keywords from the source document, ordered by their original positions in the source. We take convolutional neural network (CNN) model to embed extracted keywords as</p><formula xml:id="formula_1">(k 1 , k 2 , ..., k |k | ),</formula><p>where |ğ¾ | is the number of keywords. The combination of keywords representation and sentence representation enbodies the intuition that the keywords are more important carriers of the salient information and should be treated specially during sentence selection. Document Encoder. We consider two options for the document encoder: training from scratch with BiLSTM document encoder and fine-tuning on pretrained model named HIBERT document encoder. The former is a standard document encoding model, and the latter is the state-of-the-art pretrained model for document encoding.  </p><formula xml:id="formula_2">(â„ 0 , â„ 1 , ..., â„ ğ‘› ) where â„ ğ‘– = [ âˆ’ â†’ â„ ğ‘– ; â† âˆ’ â„ ğ‘– ] is the representation of ğ‘–-th sentence.</formula><p>The HIBERT document encoder is a pretrained encoder <ref type="bibr" target="#b35">[34]</ref>, which contains two Transformer-based sub-encoders. We combine the word embeddings and their corresponding position embeddings as the input and obtain the context sensitive sentence representations (â„ â€² 0 , â„ â€² 1 , ..., â„ â€² ğ‘› ) as the output. Aligned Pointer Decoder. We extend Pointer Network <ref type="bibr" target="#b32">[31]</ref> as the decoder. The pseudo summary consisting of multi-sentence sets is the input of the decoder. We extract a set of keywords for each multi-sentence set in the pseudo summaries and order them based on their positions in the input text. To distinguish the sentences and keywords in different sets, we set the representations for the placeholder &lt;SEP&gt; in pseudo summaries and pseudo keywords. The pseudo summary becomes ğ‘ƒ = (ğ‘ 0 0 , ğ‘ 0 1 , ..., ğ‘ 0 ğ‘— , ğ‘†ğ¸ğ‘ƒ, ğ‘ 1 ğ‘—+1 , ..., ğ‘ƒ ğ‘š ğ‘¥ ), and its keywords become ğ¾ = (ğ‘˜ 0 0 , ğ‘˜ 0 1 , ...ğ‘˜ 0 ğ‘— , ğ‘†ğ¸ğ‘ƒ, ğ‘˜ 1 ğ‘—+1 , ..., ğ‘˜ ğ‘š |ğ‘˜ | ). We randomly initialize the representation of â„ ğ‘†ğ¸ğ‘ƒ for pseudo summary and ğ‘˜ ğ‘†ğ¸ğ‘ƒ for pseudo keywords.</p><p>At each time step ğ‘¡, we take the output of decoder attending to the encoder sentence representations as the predicted vector ğ‘ â„ ğ‘¡ , which is calculated by:</p><formula xml:id="formula_3">ğ‘ â„ ğ‘¡ = ğ‘› ğ‘– ğ›¼ â„ ğ‘–ğ‘¡ ğ‘Š ğ‘1 â„ ğ‘– ğ›¼ â„ ğ‘¡ = softmax(ğ‘£ â„ tanh(ğ‘Š ğ‘”1 ğ‘” ğ‘¡ + ğ‘Š â„1 â„ ğ‘– ))<label>(1)</label></formula><p>where ğ‘” ğ‘¡ is the decoder hidden state at step ğ‘¡. â„ ğ‘– is the sentence representation of ğ‘–-th sentence based on document encoder (BiLSTM or HIBERT). ğ‘ â„ ğ‘¡ is the attention weights based on sentences. ğ‘Š and ğ‘£ in different labels are trainable parameters. Similarly, the keywords vector ğ‘ ğ‘˜ ğ‘¡ can be computed. We compute the current extraction probabilities using predicted sentence vector and keywords vector:</p><formula xml:id="formula_4">ğ‘ (ğ‘¦ ğ‘¡ |ğ‘¦ 1 , .., ğ‘¦ ğ‘¡ âˆ’1 , ğ‘ â„ , ğ‘ ğ‘˜ ) = softmax(ğ‘£ tanh(ğ‘Š ğ‘” ğ‘” ğ‘¡ +ğ‘Š â„ ğ‘ â„ ğ‘¡ +ğ‘Š ğ‘˜ ğ‘ ğ‘˜ ğ‘¡ ))<label>(2)</label></formula><p>where ğ‘¦ ğ‘¡ is the sentence with the highest probability at current step.</p><p>Combinatorial Loss. We propose a combinatorial loss to train extractor, including cross-entropy loss, keywords loss and set loss. The cross-entropy loss reflects the accuracy of one-to-one alignment between extracted sentences and pseudo summaries, which is computed as:</p><formula xml:id="formula_5">ğ¿ ğ‘ğ‘’ = âˆ’ (ğ‘ƒ,ğ· ) âˆˆğ‘‡ log(ğ‘ (ğ‘ƒ |ğ·))<label>(3)</label></formula><formula xml:id="formula_6">ğ‘¤ ! " ğ‘˜ ! " ğ‘˜ " "</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Encoder Aligned Pointer Decoder Keywords Encoder</head><p>CONV CONV where ğ‘‡ is training set with ğ‘ samples. We use keywords loss to emphasize the importance of the related salient information. The probability of keywords extraction is computed as:</p><formula xml:id="formula_7">ğ‘˜ ! ğ‘˜ " CONV CONV CONV CONV ğ‘  ! ğ‘  " ğ‘  # â„ ! â„ " â„ # (a) Conv+BiLSTM (b) HIBERT</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sent Encoder Transformer</head><formula xml:id="formula_8">ğ‘¤ ! ! ğ‘¤ " ! â€¦ EOS ğ‘  ! Doc Encoder Transformer ğ‘  " ğ‘  # ğ‘¤ ! ! ğ‘¤ " ! â€¦ EOS ğ‘¤ " " â€¦ EOS BOS â„ $%&amp; â„ %'( ğ‘˜ ! ! [â„ " , ğ‘˜ " ] [â„ )*+ , ğ‘˜ $%&amp; ] ğ‘˜ " ! [â„ ! , ğ‘˜ ! ] ğ‘˜ $%&amp;</formula><formula xml:id="formula_9">ğ‘ (ğ‘˜ ğ‘¡ |ğ‘˜ 1 , ..., ğ‘˜ ğ‘¡ âˆ’1 , ğ‘) = softmax(ğ‘£ tanh(ğ‘Š â€² ğ‘” ğ‘” ğ‘¡ + ğ‘Š â€² ğ‘˜ ğ‘ ğ‘˜ ğ‘¡ ))<label>(4)</label></formula><p>where ğ‘˜ ğ‘¡ is the predicted keyword at ğ‘¡ step. We compute the keywords loss based on the keywords ground truth ğ¾ and source document ğ· as:</p><formula xml:id="formula_10">ğ¿ ğ‘˜ğ‘’ğ‘¦ = âˆ’ (ğ¾,ğ· ) âˆˆğ‘‡ log(ğ‘ (ğ¾ |ğ·))<label>(5)</label></formula><p>As the output of extractor consists of multi-sentence sets, we need correctly predict &lt;SEP&gt; at the proper positions. For each training sample, we obtain intermediate summary ğ‘„ extracted from source document ğ·, yielded by greedily selecting sentence that maximizes the output probability at each time step. We align the &lt;SEP&gt; of ğ‘ƒ and ğ‘„ in the same position by padding or truncation the sequence of sentence labels in the set of ğ‘ƒ. For example, given pseudo summary ğ‘ƒ = (ğ‘ 0 , ğ‘ 1 , ğ‘†ğ¸ğ‘ƒ, ğ‘ 2 , ğ‘†ğ¸ğ‘ƒ) and extracted intermediate summary ğ‘„ = (ğ‘ 0 , ğ‘†ğ¸ğ‘ƒ, ğ‘ 1 , ğ‘ 2 , ğ‘†ğ¸ğ‘ƒ), we get aligned pseudo summary as ğ‘ƒ â€² = (ğ‘ 0 , ğ‘†ğ¸ğ‘ƒ, ğ‘ 2 , ğ‘†ğ¸ğ‘ƒ, ğ‘†ğ¸ğ‘ƒ). We define the set loss function as:</p><formula xml:id="formula_11">ğ‘™ ğ‘ ğ‘’ğ‘¡ = âˆ’ (ğ‘ƒ â€² ,ğ· ) âˆˆğ‘‡ log(ğ‘ (ğ‘ƒ â€² |ğ·))<label>(6)</label></formula><p>We use the combinatorial loss as follows:</p><formula xml:id="formula_12">ğ¿ ğ‘ğ‘™ = âˆ’ 1 ğ‘ (ğœ† ğ‘ ğ¿ ğ‘ğ‘’ + ğœ† ğ‘˜ ğ¿ ğ‘˜ğ‘’ğ‘¦ + ğœ† ğ‘  ğ¿ ğ‘ ğ‘’ğ‘¡ )<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Abstractor</head><p>The abstractor can paraphrase the inputs in parallel. We take set-level pseudo summaries and their reference summaries as the input and output of abstractor at training. The abstractor is an independent neural network without parameter sharing with the extractor.</p><p>In this work, we take two representative Enc-Dec model options as our abstractor: the standard Enc-Dec model PointerGen <ref type="bibr" target="#b27">[26]</ref> with attention mechanism <ref type="bibr" target="#b19">[18]</ref> and copy mechanism, and a pretrained language model BART <ref type="bibr" target="#b14">[13]</ref> finetuned on our pseudo summaries and reference summaries.</p><p>Special loss. For training, we take pseudo summary ğ‘ƒ as input and reorganized reference summary R as output. Given a pseudo summary, our abstractor deal with the sets in pseudo summary in parallel, so we first compute the cross-entropy loss between ğ‘–-th multi-sentence set of pseudo summary and reference summary</p><formula xml:id="formula_13">ğ¿ â€² ğ‘ğ‘’ (ğ‘–) = âˆ’ log(ğ‘ (r ğ‘– |ğ‘ ğ‘– ))<label>(8)</label></formula><p>Then, we consider all of the sets in a complete summary. The loss of ğ‘–-th set in pseudo summary is as follows:</p><formula xml:id="formula_14">ğ¿ ğ‘ ğ‘ (ğ‘–) = (1 + ğ‘ƒğ‘œğ¿ (ğ‘–)) 2 ğ¿ â€² ğ‘ğ‘’ (ğ‘–) ğ‘ƒğ‘œğ¿ (ğ‘–) = ğ¿ â€² ğ‘ğ‘’ (ğ‘–) ğ‘š ğ‘–=0 ğ¿ â€² ğ‘ğ‘’ (ğ‘–)<label>(9)</label></formula><p>where ğ‘ƒğ‘œğ¿(ğ‘–) is propotion of the loss of ğ‘–-th sentence set over the complete summary. This can strengthen the penalty for worse predicted multi-sentence set in a complete summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Comprehensive Reinforcement Learning</head><p>We apply comprehensive reinforcement learning (CRL) to make ext-abs framework an end-to-end trainable model. We use policy gradient technique to optimize our model and take extractor as the RL agent.</p><p>During training, we first use extractor to obtain an intermediate summary ğ‘„, which is divided into several sentence sets by &lt;SEP&gt;. Then, the abstractor paraphrases the sets in ğ‘„, and connects the rewritten sentences with &lt;SEP&gt; to generate an abstractive summary ğ´. At each time step ğ‘¡, in order to compare the intermediate summary ğ‘„ and pseudo summary ğ‘ƒ, we define a sentence-level reward using ROUGE-L (R-L) score between the sets of ğ‘„ and ğ‘ƒ at the same position.</p><formula xml:id="formula_15">ğ‘… ğ‘ ğ‘’ğ‘› (ğ‘¡ ) = ğ‘…-ğ¿ ğ¹ 1 (ğ‘ ğ‘¡ , ğ‘ ğ‘¡ )<label>(10)</label></formula><p>The sentence-level reward directly measures the accuracy of intermediate summary sentences. As the intermediate sentences and pseudo sentences are both extracted from source document, the R-L, calculating the longest common subsequence (LCS), is the best way to evaluate the intermediate sentences with the pseudo sentences.</p><p>To evaluate the alignment between intermediate summary ğ‘„ and reorganized reference summary R, we propose a set-level reward. We compute the set-level reward by ROUGE-2 (R-2) as:</p><formula xml:id="formula_16">ğ‘… ğ‘ ğ‘’ğ‘¡ (ğ‘¡ ) = ğ‘…-2 ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™ (ğ‘ ğ‘™ , rğ‘™ ), if ğ‘¡ = ğ‘ + |ğ‘ ğ‘™ | ğ‘…-2 ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™ (concat(ğ‘ ğ‘™ ğ‘ ...ğ‘ ğ‘™ ğ‘¡ ), rğ‘™ ), otherwise<label>(11)</label></formula><p>where ğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡ concatenates all the inputs. rğ‘™ is the ğ‘™-th set of sentences in R and |ğ‘ ğ‘™ | is the number of sentences in ğ‘™-th set of ğ‘„. ğ‘ is the index of the first sentence of ğ‘™-th set in ğ‘„. ğ‘¡ = ğ‘ + |ğ‘ ğ‘™ | means that the prediction of ğ‘™-th set in intermediate summary is over. For set-level reward, at step ğ‘¡, we concatenate all of the extracted sentences in ğ‘™-th set as a extracted set ğ¸ ğ‘™ ğ‘¡ and compute the R-2 score between ğ¸ ğ‘™ ğ‘¡ and its corresponding set rğ‘™ in reorganized reference summary. At the end of the prediction of this set, we compare the abstractive summary ğ‘ ğ‘™ generated from ğ‘ ğ‘™ with rğ‘™ . Since reference summary is abstractive which has many variant, the R-2 matching bigram between summaries is more suitable. As the ğ¸ ğ‘™ ğ‘¡ is the part of the input of the abstractor, the ROUGE score reflects the alignment between the input and output of abstractor during test. The higher recall between ğ¸ ğ‘™ ğ‘¡ and rğ‘™ means that the ğ¸ ğ‘™ ğ‘¡ contains more information of rğ‘™ and can predict better abstractive summary.</p><p>Considering the quality of an overall generated summary, we compute summary-level reward as:</p><formula xml:id="formula_17">ğ‘… ğ‘ ğ‘¢ğ‘š (ğ‘¡ ) = ï£± ï£´ ï£´ ï£² ï£´ ï£´ ï£³ ğ‘…-2 ğ¹ 1 (concat(ğ‘ 0 ...ğ‘ ğ‘™ ), concat(r 0 ...r ğ‘™ )), if ğ‘¡ = ğ‘™ 0 |ğ‘ ğ‘™ | ğ‘…-2 ğ¹ 1 (concat(ğ‘ 0 0 ...ğ‘ ğ‘™ ğ‘¡ ), ğ‘…), otherwise<label>(12)</label></formula><p>where ğ‘¡ = ğ‘™ 0 |ğ‘ ğ‘™ | means that prediction of ğ‘™-th set in intermediate summary is over. For summary-level reward, we concatenate all of the extracted sentences as a extracted set ğ¸ ğ‘¡ at each time step ğ‘¡. We use F1 score as reward, because the length should be considered during evaluating the whole generated summary, which can also reflects the alignment of abstrator. At the end of the prediction of each set, we compare the concatenated generated abstractive summary and corresponding reference summary. Especially, while the prediction of model is over, we compute the R-2 F1 score between generated abstractive summary ğ´ and reference summary ğ‘…. The total reward is the combination of above:</p><formula xml:id="formula_18">ğ‘… ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘ğ‘™ğ‘™ = ğ›¾ 1 ğ‘… ğ‘ ğ‘’ğ‘› + ğ›¾ 2 ğ‘… ğ‘ ğ‘’ğ‘¡ + ğ›¾ 3 ğ‘… ğ‘ ğ‘¢ğ‘š (<label>13</label></formula><formula xml:id="formula_19">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EVALUATION</head><p>In this section, we introduce the dataset and experimental setup. We compare our proposed framework, along with its variants, with existing summarizaiton models and demonstrate the advantages of our keyword aware models 2 trained on set-level pseudo summaries. 2 The data and source code are released on https://github.com/YizhuLiu/SetKE_ABS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>In this experiment, we use 5 datasets which are either news, web pages or user generated QAs on the web for training and test. CNN/Daily Mail <ref type="bibr" target="#b11">[10]</ref> (CNNDM) is a popular summarization dataset, which contains 286,817 training pairs, 13,368 validation pairs and 11,487 test pairs. We follow Nallapati <ref type="bibr" target="#b22">[21]</ref> with the data preprocessing and use the non-anonymized version as See et al. <ref type="bibr" target="#b27">[26]</ref>.</p><p>Webis-TLDR-17 Corpus <ref type="bibr" target="#b33">[32]</ref> (Web17), one of the first largescale summarization datasets from social media domain, contains 3 million pairs of content and self-written summaries from Reddit.</p><p>Webis-Snippet-20 Corpus [4] (Web20) contains approximately 3.5 Million (webpage content, abstractive snippet) triples for the task of abstractive snippet generation of web pages. The corpus is compiled from the DMOZ Open Directory Project.</p><p>WikiHow Corpus <ref type="bibr" target="#b13">[12]</ref> (Wiki) is a large-scale dataset using the online WikiHow knowledge base. Each article consists of multiple paragraphs and each paragraph starts with a sentence summarizing it. The dataset contains 200,000 long-sequence pairs. DUC-2002 (DUC) is a test set of 567 document-summary pairs for single-document summarization. We use the models trained on CNNDM to do the test on DUC, which can evaluate the generalizability of the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Setup</head><p>3.2.1 Implementation details. We set batch size as 32 for all training processes. All models are optimized by Adam optimizer. In extractor, we take a single-layer CNN model with 100 dimensions as keywords encoder whose input are randomly initialized with 128-dimensional vectors. For pointer network decoder, we employ LSTM models with 256-dimensional hidden states. We implement our document encoders, BiLSTM encoder and HIBERT encoder, as described by Chen <ref type="bibr" target="#b6">[5]</ref> and Zhang <ref type="bibr" target="#b35">[34]</ref>. We fine-tune HIBERT encoder with learning rate (lr) 5ğ‘’ âˆ’ 5 and warmup steps 4, 000. We set ğœ† ğ‘ = 1.0, ğœ† ğ‘˜ = 0.5, ğœ† ğ‘  = 0.5 (Eq. 7). For abstractor, the lr of PG is 1ğ‘’ âˆ’ 03. We follow Lewise <ref type="bibr" target="#b14">[13]</ref> in fine-tuning BART with ğ‘™ğ‘Ÿ = 3ğ‘’ âˆ’ 05 and warmup = 500. For RL, the lr of RL as 1ğ‘’ âˆ’ 04. We set ğ›¾ ğ‘ ğ‘’ğ‘› = 0.5, ğ›¾ ğ‘ ğ‘’ğ‘¡ = 1.0, ğ›¾ ğ‘ ğ‘¢ğ‘š = 1.0 (Eq. 13) with grid search on validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Models under comparison.</head><p>In this experiments, we evaluate different methods on above datasets. The brief description of these methods are shown in Table <ref type="table" target="#tab_5">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Evaluation Metrics.</head><p>We evaluate the performance of our method by automatic metrics and human evaluation.</p><p>Automatic Metrics. ROUGE scores (F1) include ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-L(R-L) <ref type="bibr" target="#b16">[15]</ref>.</p><p>Human Evaluation. We randomly select 100 samples from each dataset and average the scores by three human annotators who are native or proficient English speakers. <ref type="foot" target="#foot_1">3</ref>â€¢ Manual Alignment Accuracy (manAlign). We rank and score pseudo summaries with three-scale scores based on the informativeness and redundancy of pseudo summary with respect to reference, i.e., better (2.0), equal (1.0) and worse (0.0).  â€¢ Keyword Coverage reflects the accuracy of keywords in generated summary. Given a pair of generated summary and reference summary, we manually extract their keywords and sequence these keywords based on their locations in source. Keyword coverage is computed as the ROUGE-1 precision between generated and reference keywords sequences. â€¢ Readability. We rank summaries generated by our best model and that of BART according to logical consistency with source document and informativeness. The summary should be labeled as better, equal or worse. includes the percentage of the number of summaries with different label to the total summaries.  <ref type="table">2</ref>, the sentence-level pseudo summaries always ignore cross-sentence information. Summary-level pseudo summaries capture the information among sentences and get better ROUGE scores than sentence-level pseudo summaries. However, summary-level pseudo summaries cannot recognize important information in reference summary, which bring noise to the pseudo summaries. The proposed set-level heuristic extracts the most aligned multi-sentence set for one or more reference sentences, which can better align the reference sentences abstracted from multiple source sentences. As the set-level method is based on keywords, the pseudo summaries cover all keywords in reference summaries, significantly reducing salient information lose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Pseudo</head><p>In order to examine the effects of different pseudo summaries on the model, we assume that the extractor is perfect and directly input three kinds of pseudo summaries to train and test the abstractors respectively. As shown in Table <ref type="table" target="#tab_8">4</ref>, the ROUGE scores between these generated summaries and references denote the upper bound of models on different pseudo summaries, which can reflect the alignment between pseudo summaries and reference summaries. The higher ROUGE scores, the more aligned dataset. The ROUGE scores of CNNDM dataset in Table <ref type="table" target="#tab_8">4</ref> are much better than ROUGE scores of other dataset. Since some sentences in reference summaries of CN-NDM dataset are extracted from the source documents, the quality of intermediate results has a greater impact on CNNDM dataset. The improvement of ROUGE scores reflects the enhancement of alignment. Compared with other datasets, the improvement of ROUGE score on Web20 is minimal. The reason is that the length of reference summaries of Web20 dataset is shorter than others, causing the similar pseudo summaries extracted through different heuristics.</p><p>The models trained on set-level pseudo summaries achieve the highest ROUGE scores on all of the datasets. This denotes that the abstractor models can benefit from training on set-level pseudo summaries. Thus, our proposed set-level matching heuristics can produce more aligned training pairs for generation and make the abstractor better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Results</head><p>for the framework. We compare our proposed models with existing models. Following previous work, we take reference summaries in datasets as the ground truth of extractive summaries and abstractive summaries.</p><p>Extractor. We train the extractor on (source document, pseudo summary) pairs. As shown in Table <ref type="table" target="#tab_10">5</ref>, the keyword-based extractor achieves higher ROUGE scores on various datasets. The basic models (PN ğ‘ğ‘‘ and HIBERT ğ‘ğ‘‘ ) with only one document encoder have been improved on ROUGE scores by adding keyword encoder (KE), which demonstrates that KE is useful to guide extractor to select more accurate sentences. After adding combinatorial loss (CL), the ROUGE scores become higher. The reason is that the composition of CL is consistent with the extraction of pseudo summaries and the structure of extractor. Besides, CL containing keywords loss can help extractor to select sentences with more keywords. The ROUGE scores of extracted summaries generated by HIBERT ğ‘ğ‘‘ are higher since the HIBERT ğ‘ğ‘‘ is fine-tuned on a pretrained model which can enhance the language modeling ability. Compared with PN ğ‘ğ‘‘ , the HIBERT ğ‘ğ‘‘ can capture more information about the relationship between inputs of the encoder and the decoder, including keyword information. Therefore, the improvements on different datasets of HI-BERT document encoder (HIBERT ğ‘ğ‘‘ ) are always less than BiLSTM document encoder (PN ğ‘ğ‘‘ ). As shown in Table <ref type="table">6</ref>, compared with the extractor without keyword encoder, the sentences extracted from our keyword-based extractors can capture more keywords of reference summary. However, the extractor without CL always generates duplicate keywords. As shown in Table <ref type="table" target="#tab_13">8</ref>     keywords with less repetition. The reason is the loss function of KE ğ» ğ¼ğ‘ğ‘™ considers the accuracy of the extracted keywords.</p><formula xml:id="formula_20">Models CNN/DM Web17 Web20 Wiki DUC R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L</formula><p>As the extractor is the first step of ext-abs framework, the output of the extractor is very important. As shown in Table <ref type="table" target="#tab_10">5</ref>, with the same abstractor, the ext-abs frameworks with keyword-based extractor get higher ROUGE scores. This shows that the keyword-based extractor can provide better input to abstractor.</p><p>Abstractor. We improve the abstractor by creating a new training set, pseudo summaries, which enhances the alignment between input of encoder and decoder. Table <ref type="table" target="#tab_8">4</ref> shows that the models trained on set-level pseudo summaries generate more accurate summaries. The models with our designed special loss (PG ğ‘ ğ‘™ and BART ğ‘ ğ‘™ ) get higher ROUGE scores on sentence-level and set-level pseudo summaries, because the special loss considers the global information of the summary during parallel summarization. We do not apply special loss to the abstractor trained on summary-level pseudo summaries since the input of abstractor on summary-level pseudo summaries is complete and it cannot be processed in parallel. The summaries generated by pretrained models achieve higher ROUGE scores due to better document representations. As shown in Table <ref type="table">6</ref>, with the same extractor, the abstractor with special loss can generate more informative summaries with less redundancy.</p><p>Comprehensive Reinforcement Learning. As shown in Table <ref type="table" target="#tab_10">5</ref>, we combine our extractor and abstractor in different ways. Compared with BART, PG cannot abstract the extracted sentences effectively and achieves worse ROUGE scores than its connected extractors. The ROUGE scores of summaries generated by keyword-based extractor with BART are lower because the less effective extractor brings more noise to the downstream abstractor. These results show that a good extractor is critical for ext-abs framework. ğ¾ğ¸ ğ» ğ¼ğ‘ğ‘™ -ğµğ´ğ‘…ğ‘‡ ğ‘ ğ‘™ has a lower R-L score on Web20 in Table <ref type="table" target="#tab_10">5</ref> as the sentences in Web20 are very short. This causes that the overlapping of sentence-level longest (a) Reorganized reference summary.</p><p>Set 1. federal education minister smriti irani was visiting a fabindia outlet in the tourist resort state of goa on friday when she discovered a surveillance camera pointed at the changing room. state authorities found an overhead camera that the minister had spotted and determined that it was indeed able to take photos of customers. Set 2. four employees of the store have been arrested. if convicted, they could spend up to three years in jail.</p><p>(b) Extractive summaries of different extractor.</p><p>HIBERT new delhi , india -lrb-cnn -rrb-police have arrested four employees of a popular indian ethnic-wear chain after a minister spotted a security camera overlooking the changing room of one of its stores . federal education minister smriti irani was visiting a fabindia outlet in the tourist resort state of goa on friday when she discovered a surveillance camera at the changing room , police said . HIBERT ğ‘ğ‘‘ Set 1) federal education minister smriti irani was visiting a fabindia outlet in the tourist resort state of goa on friday when she discovered a surveillance camera pointed at the changing room , police said . Set 2) " fabindia is deeply concerned and shocked at this allegation , " the company said in a statement . " we are in the process of investigating this internally and will be cooperating fully with the police . " KE ğ» ğ¼ Set 1) new delhi , india -lrb-cnn -rrb-police have arrested four employees of a popular indian ethnic-wear chain after a minister spotted a security camera overlooking the changing room of one of its stores . Set 2) federal education minister smriti irani was visiting a fabindia outlet in the tourist resort state of goa on friday when she discovered a surveillance camera at the changing room , police said . Set 3) four employees of the store have been arrested , but its manager -a woman -was still at large saturday , said goa police superintendent kartik kashyap . KE ğ» ğ¼ğ‘ğ‘™ Set 1) federal education minister smriti irani was visiting a fabindia outlet in the tourist resort state of goa on friday when she discovered a surveillance camera pointed at the changing room . state authorities launched their investigation right after irani levied her accusation . Set 2) four employees of the store have been arrested . if convicted, they could spend up to three years in jail.</p><p>(c) Abstractive summaries of different end-to-end models.</p><p>BART federal education minister smriti irani was visiting a fabindia outlet in the tourist resort state of goa . she discovered a surveillance camera pointed at the changing room. four employees of the store have been arrested , but the manager is still at large . the arrested staff have been charged with voyeurism and breach of privacy . KE ğ» ğ¼ğ‘ğ‘™ -BART Set 1) federal education minister smriti irani was visiting a fabindia outlet in goa . Set 2) fabindia is concerned and shocked at this allegation. KE ğ» ğ¼ğ‘ğ‘™ -BART ğ‘ ğ‘™ Set 1) police arrested four employees after a minister spotted a security camera . Set 2) federal education minister smriti irani was visiting a fabindia outlet in goa . KE ğ» ğ¼ğ‘ğ‘™ -BART ğ‘ ğ‘™ -CRL Set 1) federal education minister smriti irani was visiting a fabindia store the tourist resort state of goa. she discovered a camera at the changing room. authoroities discovered found it was able to take photos. photos rom the store 's changing room. Set 2) the four store workers could spend three years in jail if convicted.</p><p>Table <ref type="table">6</ref>: The extractive and abstractive summaries for the example in Table <ref type="table">1</ref>.</p><p>common subsequence between reference and generated summary may be slightly lower when their R-1 and R-2 are higher.</p><p>We use RL to connect extractor and abstractor, which makes ext-abs framework an end-to-end trainable model. We observe the changes of different models after adding RL. As shown in Table <ref type="table">7</ref>, after adding sentence-level or summary-level reward, the ROUGE scores of the models on datasets become worse, which demonstrates that it is important to desige a suitable reward for ext-abs framework. The models trained on CRL achieve better ROUGE scores than that trained without RL, which denotes that our CRL can enhance extractor to select more accurate sentences. The ROUGE scores of extractor extended by RL are improved. The CRL bridges the backpropagation from abstractive summary to source document. So the ROUGE-based comprehensive rewards between generated summaries and reference summaries reflect the quality of extracted sentences and generated summaries which can guide the extractor to select correct sentences. The higher ROUGE scores of ext-abs with RL also show that the ext-abs model can benefit from a better extractor.</p><p>As shown in Table <ref type="table" target="#tab_13">8</ref>, our strongest model with CRL (KE ğ» ğ¼ğ‘ğ‘™ -BART ğ‘ ğ‘™ -CRL) outperforms the SOTA abstractive models on all datasets. As BART is the SOTA abstractive summarization model, the ROUGE scores of KE ğ» ğ¼ğ‘ğ‘™ -BART ğ‘ ğ‘™ -CRL are better than the BART but they are close. We take t-test to measure the difference Table <ref type="table" target="#tab_13">8</ref>: ROUGE scores of different end-to-end trainable models on datasets. The scores underlined are statistically significantly better than BART with p &lt; 0.05 according to t-test.</p><p>of ROUGE scores between our model and BART. The p-values on ROUGE scores of the SOTA model BART and Pre-KE ğ‘ğ‘™ -Abs ğ‘ ğ‘™ -CRL of all datasets are less than 0.05, except for Web20. As the reference summary in Web20 is very short and abstract, it is difficult to extract pseudo summary aligned to the reference summary. The performance of models on Web20 is close and not good. This shows that the ext-abs framework with our approaches are effective. As test-only dataset, DUC testing on KE ğ» ğ¼ğ‘ğ‘™ -BART ğ‘ ğ‘™ -CRL gets highest ROUGE scores, which shows that our proposed model has a better generalization. The FastAbs ğ» ğµ in Table <ref type="table" target="#tab_13">8</ref> takes HIBERT as extractor and BART as abstractor. FastAbs ğ» ğµ gets lower ROUGE scores than BART due to its poor alignment of sentence-level training set and its extractor without keyword-based encoder. The best ROUGE scores of our models show that the abstractive models can be improved by locating the salience information. As shown in Table <ref type="table">6</ref>, the summary generated by our model with CRL contains more keywords and becomes more readable.</p><p>Speed and Memory. We take the speed and memory usage of CNNDM as example. As shown in Table <ref type="table" target="#tab_11">9</ref>, we evaluate our models on the speed and memory usage. Based on fune-tuning on the pretrained model or not, we compare our KE ğ‘ğ‘™ -PG ğ‘ ğ‘™ -CRL with PG and KE ğ» ğ¼ğ‘ğ‘™ -BART ğ‘ ğ‘™ -CRL with BART, to be fair. In Table <ref type="table" target="#tab_11">9</ref>(a), KE ğ‘ğ‘™ -PG-CRL is almost 7 times faster in total training time and occupies less memory than PG. We cannot fine-tune BART on GPU RTX-2080ti due to out-of-memory. We test BART based on the released pretrained model. The KE ğ» ğ¼ğ‘ğ‘™ -BART ğ‘ ğ‘™ -CRL performs much better than BART on speed and memory usage. In Abstractive models have to encode long documents with attention model looking at all encoded tokens at each time step, which causes low speed and large memmory usage. As a pointer network, our extractor is faster than most abstractive models. Our models first extract sentence sets from a source and then input them to abstractor. These inputs can be decoded in parallel, which speed up the model. The average length of inputs is shortened from 780 to 100, which reduces the memory usage. The FastAbs and FastAbs ğ» ğµ are faster than our models because they train and test models on sentencelevel pseudo summaries which are shorter than our set-level pseudo summaries. However, the difference is not significant. This is because that the different matching heuristics may extract different sentences for the same sentence in the reference summaries. Besides, as shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>Related work on extractor-abstractor framework and pretrained models for summarization are introduced as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Extractor-Abstractor Framework</head><p>In this paper, we adopt the extractor-abstractor (ext-abs) framework, which has been a popular method for abstractive summarization recently. Unlike the end-to-end models <ref type="bibr" target="#b14">[13,</ref><ref type="bibr" target="#b18">17,</ref><ref type="bibr" target="#b22">21,</ref><ref type="bibr" target="#b25">24,</ref><ref type="bibr" target="#b27">26]</ref> in abstractive summarization, the ext-abs framework trains two enc-dec models, extractor and abstractor. The extractor captures salient content (pseudo summary) of source document, where the pseudo summary can be either sentence-level <ref type="bibr" target="#b6">[5,</ref><ref type="bibr" target="#b12">11,</ref><ref type="bibr" target="#b30">29]</ref> or summary-level <ref type="bibr" target="#b2">[1,</ref><ref type="bibr" target="#b28">27]</ref>, and then abstractor paraphrases the salient content to generate a summary.</p><p>In this paper, we present a set-level matching heuristics to construct the pseudo summaries, better aligned to reference summaries. Extractive models adopt hierarchical neural network as encoder and pointer network as decoder <ref type="bibr" target="#b7">[6,</ref><ref type="bibr" target="#b21">20]</ref>. It is extended with variant models, such as reinforcement learning <ref type="bibr" target="#b23">[22]</ref> and joint scoring <ref type="bibr" target="#b38">[37]</ref>. As transformer preforms excellent on language model, Liu et al. <ref type="bibr" target="#b17">[16]</ref> and Zhang et al. <ref type="bibr" target="#b35">[34]</ref> apply pretrained transformers to extractive summarization. Zhou <ref type="bibr" target="#b39">[38]</ref> and Li et al. <ref type="bibr" target="#b15">[14]</ref> have shown that keywords play an important role in summarization. So we enhanced the extractive models with an additional keyword encoder to get better alignments between pseudo summaries and reference summaries.</p><p>Abstractive models are based on sequence-to-sequence learning <ref type="bibr" target="#b3">[2,</ref><ref type="bibr" target="#b29">28]</ref>. The pointer-generator networks <ref type="bibr" target="#b27">[26]</ref> consisting of copy mechanism and coverage model are the most popular baseline in abstractive summarization. The pretrained transformer language models have success in natural language processing. Through finetuning the pretrained models on summarization task, the quality of generated summaries are improved <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b37">36]</ref>.</p><p>The reinforcement learning (RL) is always used to connect the extractor and abstractor together, which makes an end-to-end trainable model. Chen <ref type="bibr" target="#b6">[5]</ref> and Bae et al. <ref type="bibr" target="#b2">[1]</ref> encourage extractor to select sentences with high ROUGE scores by RL. Sharma et al. <ref type="bibr" target="#b28">[27]</ref> propose an entity-driven encoder and utilize RL with coherent rewards to make abstractor generate readable summaries. Different from previous end-to-end evaluation rewards, we propose a comprehensive reward, taking the intermediate extracted pseudo summary and set-level abstactive summaries into consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pretrained Models for Summarization</head><p>The pretrained transformer language models have success in natural language understanding (NLU). and natural language generation (NLG) NLU models are pretrained on unidirectional and bidirectional prediction. GPT <ref type="bibr" target="#b26">[25]</ref> employs a unidirectional transformer <ref type="bibr" target="#b31">[30]</ref> to predict the sequence. ELMo <ref type="bibr" target="#b24">[23]</ref> learns two unidirectional language models of forward and backward. BERT <ref type="bibr" target="#b8">[7]</ref> uses a bidirectional transformer encoder to predict the masked words. NLG models pretrain on sequence-to-sequence (seq2seq) models. UniLM <ref type="bibr" target="#b9">[8]</ref> is a multi-layer transformer network, including unidirctional, bidirectional and seq2seq language model. BART <ref type="bibr" target="#b14">[13]</ref> takes combines bidirectional transformer encoder and auto-regressive transformer decoder. ProphetNet <ref type="bibr" target="#b25">[24]</ref> trains on the transformer seq2seq model and takes future n-gram prediction as self-supervised. Through fine-tuning the pretrained models or representations on summarization task, the quality of generated summaries can be improved <ref type="bibr" target="#b35">[34]</ref><ref type="bibr" target="#b36">[35]</ref><ref type="bibr" target="#b37">[36]</ref>. PEGASUS <ref type="bibr" target="#b34">[33]</ref> is a new, pretrained model for text summarization, which uses self-supervised objective Gap Sentences Generation to train a transformer seq2seq model. Different from previous pretrained models, it masks sentences rather than smaller continuous text spans. We choose BART which has achieve the SOTA results on summarization tasks as a basic component. Since these pretrained model are encoder-decoder models and can be used in the same context, it has the potential to substitute BART in our ext-abs framework and enjoy similar boost in accuracy and speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>To enhance the alignment between documents and summaries in ext-abs framework, we propose a set-level matching heuristics to extract pseudo summary as training set. We introduce a new ext-abs framework that use comprehensive RL to connect keyword-based extractor and abstractor with pretrained models together. The result shows that our model outperforms the SOTA methods on variant datasets, such as news and web text. Besides, our models are faster and occupy less memory than previous pretrained models during training and testing. In the future, we will improve the extractor and strengthen the relation between extractor and abstractor.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The process of creating Set-level pseudo summary and reorganized reference summary. The words or phrases in bold are keywords. || denotes the sentence boundary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The architacture of baseline model and pretrained model for keyword-aware extractor. ğ‘¤ represents words in corresponding sentences. ğ‘  represents sentence embeddings. â„ ğ‘†ğ¸ğ‘ƒ , â„ ğ¸ğ‘‚ğ· and ğ‘˜ ğ‘†ğ¸ğ‘ƒ are three random initialized vectors for special tokens, which are updated during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>â€¢ A reference summary ğ‘… consists of sentences (ğ‘Ÿ 0 , ğ‘Ÿ 1 ,..., ğ‘Ÿ ğ‘– ,..., The generated abstractive summary ğ´ is a sequence of sentence (ğ‘ 0 , ğ‘ 1 , ..., ğ‘ ğ‘™ , ..., ğ‘ ğ‘€ ) and ğ‘ denotes the set of sentences.â€¢ ğ‘¡ ranges over the time steps in both encoding and decoding.</figDesc><table><row><cell>0 1 , ..., ğ‘ ğ‘™ ğ‘– , ..., ğ‘ ğ‘š ğ‘¥ ) where ğ‘ ğ‘™ ğ‘–</cell></row><row><cell>is the ğ‘– ğ‘¡â„ sentence in the sequence that belongs to the ğ‘™ ğ‘¡â„ set;</cell></row><row><cell>â€¢ The output of extractor, i.e., intermediate summary ğ‘„, is a</cell></row><row><cell>sequence of sentences denoted as (ğ‘ 0 0 , ğ‘ 0 1 ,..., ğ‘ ğ‘™ ğ‘– ,..., ğ‘ ğ‘€ ğ‘¦ ) similar</cell></row><row><cell>to ğ‘ƒ;</cell></row><row><cell>ğ‘Ÿ ğ‘§ );</cell></row><row><cell>â€¢ A reorganized reference summary R consists of sentences (r 0 0 ,</cell></row><row><cell>r 0 1 ,..., rğ‘™ ğ‘– ,..., rğ‘š ğ‘§ ), similar to ğ‘ƒ and ğ‘„;</cell></row><row><cell>â€¢</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The dual decoder has a document encoder and keywords encoder. The document encoder ğ‘™ğ‘’ğ‘›(ğ‘…) do ğ‘‘ ğ‘– is the ğ‘–-th sentence in ğ· ğ‘Ÿ ğ‘– is the ğ‘–-th sentence in ğ‘… ğ‘˜ ğ‘– denotes the keywords of ğ‘Ÿ ğ‘– Initialize ğ‘ â† ğ‘–ğ‘›ğ‘–ğ‘¡ âˆˆ ğ· with highest ğ‘Ÿğ‘’ğ‘ (ğ‘–ğ‘›ğ‘–ğ‘¡, ğ‘Ÿ ğ‘– )</figDesc><table><row><cell cols="2">Algorithm 1: Extraction of Set-level Pseudo Summaries</cell></row><row><cell cols="2">Input: a document ğ·, a reference summary ğ‘…, a set of</cell></row><row><cell>keywords ğ¾</cell><cell></cell></row><row><cell cols="2">Output: pseudo summary ğ‘ƒ and reorganized reference</cell></row><row><cell>summary</cell><cell>R</cell></row><row><cell cols="2">; // ğ· and ğ‘… are each a set of sentences</cell></row><row><cell cols="2">ğ‘™ğ‘’ğ‘›() computes the number of sentences in a text</cell></row><row><cell cols="2">ğ‘Ÿğ‘’ğ‘ () and ğ‘“ 1() compute ROUGE-2 recall and F1 score</cell></row><row><cell>between two texts</cell><cell></cell></row><row><cell cols="2">ğ‘œ () computes the number of overlapping words between the</cell></row><row><cell>two sequences</cell><cell></cell></row><row><cell>for ğ‘– = 0 â†’</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Source Document D Reference Summary R Set-level Reference Summary ! ğ‘… Set-level Pseudo Summary P</head><label></label><figDesc></figDesc><table><row><cell>Source Document D</cell><cell cols="2">Keyword-based</cell><cell>Set-level Intermediate</cell><cell>Parallel</cell></row><row><cell>new delhi, india</cell><cell cols="2">Extractor</cell><cell>Summary Q</cell><cell>Abstractor</cell></row><row><cell>police â€¦â€¦|| federal education minister smriti irani was visiting a fabindia â€¦... || four employees of the store have been arrested â€¦â€¦|| state authorities found an overhead camera that the minister had spotted and â€¦â€¦|| authorities sealed off the store and</cell><cell></cell><cell>Ext</cell><cell>Set 1. federal education minister â€¦.. || state authorities found an overhead the â€¦â€¦ Set 2. if convicted , they could spend up to three years in jail .</cell><cell>Abs Abs</cell><cell>Abstractive Summary A federal education minister â€¦â€¦.|| Shedis covered â€¦â€¦|| authoroities discovered â€¦... || the four store workers could spend â€¦â€¦</cell></row><row><cell>summoned six top officials</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>from fabindia. || the arrested</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>staff have been charged â€¦â€¦|| if convicted , they could spend up to three years in jail . ||â€¦â€¦</cell><cell></cell><cell></cell><cell>â€¦â€¦</cell><cell>Abs</cell><cell>Ground Truth R, P, ! ğ‘…</cell></row><row><cell></cell><cell></cell><cell>Update</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Comprehensive Reward for RL</cell><cell>Scorer</cell></row><row><cell></cell><cell></cell><cell cols="2">1) federal education</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">minister smriti irony</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">visited a fabindia store in</cell><cell></cell></row><row><cell></cell><cell>Maximum</cell><cell cols="2">goa , saw cameras.</cell><cell></cell></row><row><cell></cell><cell>ROUGE</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Maximum</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Keywords</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Overlap</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">3) the four store workers</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">arrested could spend three</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">years each in prison if</cell><cell></cell></row><row><cell></cell><cell></cell><cell>convicted .</cell><cell></cell><cell></cell></row><row><cell>Select &amp; Reorganize</cell><cell></cell><cell>Reorganize</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Set 1) federal education</cell><cell></cell></row><row><cell></cell><cell>Aligned</cell><cell cols="2">ministerâ€¦â€¦ || authoroities discovered â€¦â€¦.</cell><cell></cell></row><row><cell>Set 2) four employees of the â€¦â€¦ || if</cell><cell></cell><cell cols="2">Set 2) the four store</cell><cell></cell></row><row><cell>convicted, they could â€¦â€¦</cell><cell></cell><cell cols="2">workers arrested â€¦â€¦</cell><cell></cell></row></table><note>CompareFigure1: The overview of keyword-aware reinforced extractor-abstractor framework. new delhi......|| 1) federal education minister smriti irani was visiting a fabindia outlet in the tourist resort state of goa on friday when she discovered a surveillance camera pointed at the store 's changing room. || four employees of the store have been arrested ||â€¦â€¦|| 2) state authorities found an overhead camera that the minister had spotted and determined that it was indeed able to take photos of customers. || â€¦...|| 3) if convicted , they could spend up to three years in jail .2) authoroities discovered the cameras could capture photos from the store 's changing room.Set 1) federal education minister â€¦â€¦. store 's changing room || state authorities found an â€¦â€¦.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>The abbreviation and description of different methods.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>and Table 6, KE ğ» ğ¼ğ‘ğ‘™ performs better than KE ğ» ğ¼ as the sentences extracted by KE ğ» ğ¼ğ‘ğ‘™ contain more</figDesc><table><row><cell>Data</cell><cell>Pseudo summary</cell><cell>PG</cell><cell>PG ğ‘ ğ‘™</cell><cell cols="2">R-1 BART BART ğ‘ ğ‘™</cell><cell>PG</cell><cell>PG ğ‘ ğ‘™</cell><cell cols="2">R-2 BART BART ğ‘ ğ‘™</cell><cell>PG</cell><cell>PG ğ‘ ğ‘™</cell><cell cols="2">R-L BART BART ğ‘ ğ‘™</cell><cell>manAlign</cell></row><row><cell></cell><cell>sentence</cell><cell cols="3">48.75 49.70 50.55</cell><cell>50.64</cell><cell cols="3">26.16 26.63 27.34</cell><cell>27.60</cell><cell cols="3">45.98 46.94 47.02</cell><cell>47.59</cell><cell>0.65</cell></row><row><cell>CNNDM</cell><cell cols="2">summary 48.98</cell><cell>-</cell><cell>51.20</cell><cell>-</cell><cell>26.85</cell><cell>-</cell><cell>28.32</cell><cell>-</cell><cell>46.41</cell><cell>-</cell><cell>48.43</cell><cell>-</cell><cell>0.70</cell></row><row><cell></cell><cell>set</cell><cell>49.31</cell><cell>50.2</cell><cell>52.02</cell><cell>52.53</cell><cell cols="3">27.12 27.64 28.66</cell><cell>28.83</cell><cell cols="3">49.34 49.88 48.72</cell><cell>49.12</cell><cell>1.65</cell></row><row><cell></cell><cell>sentence</cell><cell cols="3">19.20 19.44 19.77</cell><cell>20.01</cell><cell>5.04</cell><cell>5.16</cell><cell>5.87</cell><cell>5.98</cell><cell cols="3">16.12 16.26 17.02</cell><cell>17.64</cell><cell>0.75</cell></row><row><cell>Web17</cell><cell cols="2">summary 19.51</cell><cell>-</cell><cell>19.82</cell><cell>-</cell><cell>5.13</cell><cell>-</cell><cell>5.66</cell><cell>-</cell><cell>16.38</cell><cell>-</cell><cell>17.11</cell><cell>-</cell><cell>0.85</cell></row><row><cell></cell><cell>set</cell><cell cols="3">20.34 20.75 21.19</cell><cell>22.02</cell><cell>5.28</cell><cell>5.65</cell><cell>5.97</cell><cell>6.10</cell><cell cols="3">16.76 16.92 17.24</cell><cell>17.80</cell><cell>1.40</cell></row><row><cell></cell><cell>sentence</cell><cell cols="3">19.26 19.24 19.55</cell><cell>19.61</cell><cell>5.07</cell><cell>5.50</cell><cell>6.12</cell><cell>6.14</cell><cell cols="3">17.56 17.96 18.21</cell><cell>18.37</cell><cell>0.94</cell></row><row><cell>Web20</cell><cell cols="2">summary 19.28</cell><cell>-</cell><cell>20.70</cell><cell>-</cell><cell>5.03</cell><cell>-</cell><cell>6.21</cell><cell>-</cell><cell>17.27</cell><cell>-</cell><cell>18.26</cell><cell>-</cell><cell>0.97</cell></row><row><cell></cell><cell>set</cell><cell cols="3">19.30 19.46 21.22</cell><cell>21.43</cell><cell>5.32</cell><cell>5.67</cell><cell>6.34</cell><cell>6.54</cell><cell cols="3">17.58 18.02 18.27</cell><cell>18.45</cell><cell>1.09</cell></row><row><cell></cell><cell>sentence</cell><cell cols="3">27.01 28.17 28.74</cell><cell>29.02</cell><cell cols="3">10.40 11.06 11.98</cell><cell>11.75</cell><cell cols="3">20.79 21.76 21.22</cell><cell>22.78</cell><cell>0.78</cell></row><row><cell>WiKi</cell><cell cols="2">summary 32.28</cell><cell>-</cell><cell>33.47</cell><cell>-</cell><cell>11.27</cell><cell>-</cell><cell>12.32</cell><cell>-</cell><cell>25.25</cell><cell>-</cell><cell>26.12</cell><cell>-</cell><cell>0.86</cell></row><row><cell></cell><cell>set</cell><cell cols="3">34.07 34.76 35.06</cell><cell>35.45</cell><cell cols="3">11.76 12.16 12.37</cell><cell>12.94</cell><cell cols="3">26.22 27.61 27.33</cell><cell>28.02</cell><cell>1.36</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>The ROUGE scores of abstractors trained on pseudo summaries at different levels.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>ğ‘ ğ‘™ 43.12 20.13 39.08 18.75 4.20 14.66 12.71 2.89 11.55 25.70 7.52 20.08 40.24 19.01 36.79</figDesc><table><row><cell cols="2">Only Extractor</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PN ğ‘ğ‘‘</cell><cell></cell><cell>37.02 16.62 33.78 16.17 3.13 10.55</cell><cell>7.81</cell><cell>1.40</cell><cell>7.02</cell><cell>18.65 3.99 14.88 35.43 15.20 32.72</cell></row><row><cell>KE</cell><cell></cell><cell>40.25 18.15 36.46 16.34 3.51 10.39</cell><cell>7.90</cell><cell>1.43</cell><cell>7.10</cell><cell>18.83 4.01 15.07 38.02 16.35 34.81</cell></row><row><cell>KE ğ‘ğ‘™</cell><cell></cell><cell>41.78 18.95 37.33 17.00 3.83 10.76</cell><cell>8.04</cell><cell>1.50</cell><cell>7.33</cell><cell>19.50 5.35 15.62 38.94 17.73 35.75</cell></row><row><cell>HIBERT ğ‘ğ‘‘</cell><cell></cell><cell>41.71 19.35 38.44 18.25 3.95 14.20</cell><cell>7.93</cell><cell>1.55</cell><cell>7.72</cell><cell>20.78 5.79 16.27 38.63 18.04 36.27</cell></row><row><cell>KE ğ» ğ¼</cell><cell></cell><cell>41.70 19.50 38.57 18.32 4.11 14.34</cell><cell>9.01</cell><cell>1.97</cell><cell>8.62</cell><cell>21.22 5.84 16.44 39.17 18.45 36.20</cell></row><row><cell>KE ğ» ğ¼ğ‘ğ‘™</cell><cell></cell><cell>43.01 20.04 39.02 18.69 4.17 14.34</cell><cell>9.34</cell><cell>2.44</cell><cell>8.75</cell><cell>22.50 5.92 16.62 40.07 18.78 36.34</cell></row><row><cell cols="3">Extractor-Abstractor w/o RL</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PN ğ‘ğ‘‘</cell><cell cols="2">PG ğ‘ ğ‘™ BART ğ‘ ğ‘™ 40.12 17.71 32.35 16.04 3.48 10.95 32.75 14.03 30.32 15.88 3.01 10.47</cell><cell>7.65 8.15</cell><cell>1.39 1.50</cell><cell>7.13 8.28</cell><cell>12.71 3.12 19.11 4.92 16.80 36.20 16.38 29.67 9.11 29.07 13.74 24.11</cell></row><row><cell>KE</cell><cell cols="2">PG ğ‘ ğ‘™ BART ğ‘ ğ‘™ 40.65 18.29 33.32 16.14 3.72 12.31 37.42 15.70 34.83 15.66 3.31 10.00</cell><cell>7.38 7.65</cell><cell>1.41 1.44</cell><cell>7.33 7.13</cell><cell>14.83 3.87 13.91 34.20 14.03 29.70 19.66 5.12 16.82 37.20 16.76 30.28</cell></row><row><cell>KE ğ‘ğ‘™</cell><cell cols="2">PG ğ‘ ğ‘™ BART ğ‘ ğ‘™ 40.70 19.27 36.23 17.74 4.05 13.69 38.09 16.61 35.64 16.55 3.75 10.77</cell><cell>7.25 9.73</cell><cell cols="3">1.44 2.12 10.07 20.32 5.77 16.80 34.97 17.21 31.37 7.36 18.85 4.23 16.52 34.88 15.23 31.00</cell></row><row><cell>HIBERT ğ‘ğ‘‘</cell><cell cols="2">PG ğ‘ ğ‘™ BART ğ‘ ğ‘™ 42.48 19.61 39.02 17.77 4.11 14.07 38.45 16.03 33.85 16.78 3.21 10.98</cell><cell>7.36 8.13</cell><cell>1.40 1.59</cell><cell>7.52 7.82</cell><cell>18.83 4.75 16.27 32.16 15.74 33.11 20.03 5.94 16.99 38.76 17.95 36.11</cell></row><row><cell>KE ğ» ğ¼</cell><cell cols="6">PG ğ‘ ğ‘™ BART ğ‘ ğ‘™ 42.19 19.82 38.57 18.53 4.16 14.27 12.16 2.53 11.58 22.17 6.82 18.24 39.73 18.94 36.38 39.62 18.07 33.29 16.22 3.38 11.11 8.47 1.60 8.01 19.14 5.08 16.25 35.18 16.34 34.01</cell></row><row><cell>KE ğ» ğ¼ğ‘ğ‘™</cell><cell>PG ğ‘ ğ‘™ BART</cell><cell>40.63 18.11 36.34 18.59 3.66 12.52</cell><cell>8.37</cell><cell>1.67</cell><cell>7.90</cell><cell>20.47 5.66 16.27 35.66 17.12 34.09</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>The ROUGE scores of extractor and extractor-abstractor without RL.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell>(b), both of our</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table /><note>Total time (T), speed and memory usage (M) of models during training and testing of CNNDM dataset on RTX-2080ti.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 ,</head><label>8</label><figDesc>the ROUGE scores of KE ğ‘ğ‘™ -PG ğ‘ ğ‘™ -CRL and KE ğ» ğ¼ğ‘ğ‘™ -BART ğ‘ ğ‘™ -CRL are better than than FastAbs and FastAbs ğ» ğµ . 3.3.3 Human Evaluation. We compare the readability and keyword coverage of our best model (KE ğ» ğ¼ğ‘ğ‘™ -BART ğ‘ ğ‘™ -CRL) and the SOTA model. As shown in Table 10, our model get the highest readability score and keyword coverage score, which means that our model can generate more informative summaries with more keywords. As shown in Table 6, our model generates more readable summaries. This means that our model improve BART by keywordbased extractor capturing salient and aligned information.</figDesc><table><row><cell>Models</cell><cell>CNNDM Read KC Read KC Read KC Read KC Read KC Web17 Web20 Wiki DUC</cell></row><row><cell cols="2">BART 0.74 0.36 0.80 0.31 0.80 0.20 0.79 0.25 0.73 0.33</cell></row><row><cell>Ours</cell><cell>0.81 0.45 0.86 0.39 0.91 0.28 0.85 0.30 0.88 0.37</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>The Readability(Read) and Keyword Coverage(KC) of generated summaries.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Our framework is flexible with respect to the choice of document encoder and abstractor.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">The Cohen's Kappa coefficient between annotators are 0.68 (manAlign), 0.72 (KC) and 0.64 (Read), indicating substantial agreement.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">Test BART on released model bart.larg.cnn https://github.com/pytorch/fairseq/tree/ master/examples/bart.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was partly supported by the SJTU-CMBCC Joint Research Scheme and SJTU Medicine-Engineering Cross-disciplinary Research Scheme.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abbrev.</head><p>Description Extractive Summarization PN <ref type="bibr" target="#b6">[5]</ref> BiLSTM encoder with pointer decoder PN ğ‘ğ‘‘ BiLSTM encoder with aligned pointer decoder</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>ğ»</surname></persName>
		</author>
		<author>
			<persName><forename type="first">ğ¼ğ‘ğ‘™</forename><surname>Bart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">ğ‘ ğ‘™ -Rl</forename><surname>ğ‘ ğ‘¢ğ‘š</surname></persName>
		</author>
		<idno>40.44 19.44 35.79 18.39 3.97 14.30 12.55 2.66 11.37 22.14 6.98 20.07 34.18 17.75 30.44 KE ğ» ğ¼ğ‘ğ‘™ BART ğ‘ ğ‘™ -CRL 43.57 20.37 40.27 19.46 4.34 16.44 14.46 4.09 14.12 27.01 8.66 20.79 44.46 20.17 36.46</idno>
		<imprint/>
	</monogr>
	<note>Table 7: ROUGE scores of models with different RL</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ke ğ» ğ¼ğ‘ğ‘™ âˆ’</forename><surname>Bart</surname></persName>
		</author>
		<idno>ğ‘ ğ‘™ -CRL 43.57 20.37 40.27 19.46 4.34 16.44 14.46 4.09 14.12 27.01 8.66 21.79 44.46 20.17 36.46</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Summary Level Training of Sentence Rewriting for Abstractive Summarization</title>
		<author>
			<persName><forename type="first">Sanghwan</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taeuk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang-Goo</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on New Frontiers in Summarization</title>
				<meeting>the 2nd Workshop on New Frontiers in Summarization</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyung</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Hyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep Communicating Agents for Abstractive Summarization</title>
		<author>
			<persName><forename type="first">Asli</forename><surname>Ã‡elikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Abstractive Snippet Generation</title>
		<author>
			<persName><forename type="first">Wei-Fan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahbaz</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;20: The Web Conference</title>
				<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-20">2020. 2020. April 20-24, 2020</date>
			<biblScope unit="page" from="1309" to="1319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting</title>
		<author>
			<persName><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the</title>
				<meeting>the 56th Annual Meeting of the</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural Summarization by Extracting Sentences and Words</title>
		<author>
			<persName><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08-07">2016. August 7-12, 2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">June 2-7, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unified Language Model Pre-training for Natural Language Understanding and Generation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-14">2019. 2019. 2019, 8-14 December 2019</date>
			<biblScope unit="page" from="13042" to="13054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An Effective Joint Framework for Document Summarization</title>
		<author>
			<persName><forename type="first">Min</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengkun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenglu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guandong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion of the The Web Conference 2018 on The Web Conference</title>
				<meeting><address><addrLine>Lyon , France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-23">2018. 2018. 2018. April 23-27. 2018</date>
			<biblScope unit="page" from="121" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Teaching Machines to Read and Comprehend</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">TomÃ¡s</forename><surname>KociskÃ½</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss</title>
		<author>
			<persName><forename type="first">Wan-Ting</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chieh-Kai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kerui</forename><surname>Min</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="132" to="141" />
		</imprint>
	</monogr>
	<note>Jing Tang, and Min Sun</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Mahnaz</forename><surname>Koupaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09305</idno>
		<title level="m">Wikihow: A large scale text summarization dataset</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Guiding Generation for Abstractive Text Summarization Based on Key Information Guide Network</title>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
		<title level="s">the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ROUGE: a Package for Automatic Evaluation of Summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text Summarization Branches Out</title>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Text Summarization with Pretrained Encoders</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3728" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Yizhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenny</forename><forename type="middle">Q</forename><surname>Zhu</surname></persName>
		</author>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Controlling Length in Abstractive Summarization Using a Convolutional Neural Network</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Effective Approaches to Attention-based Neural Machine Translation</title>
		<author>
			<persName><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">TextRank: Bringing Order into Text</title>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing , EMNLP 2004, A meeting of SIGDAT, a Special Interest Group of the ACL, held in conjunction with ACL 2004</title>
				<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing , EMNLP 2004, A meeting of SIGDAT, a Special Interest Group of the ACL, held in conjunction with ACL 2004</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents</title>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
				<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-02-04">2017. February 4-9, 2017</date>
			<biblScope unit="page" from="3075" to="3081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Abstractive Text Summarization using Sequence-tosequence RNNs and Beyond</title>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</title>
				<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>CoNLL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note>CÃ­cero Nogueira dos Santos, Ã‡aglar GÃ¼lÃ§ehre, and Bing Xiang</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ranking Sentences for Extractive Summarization with Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-01">2018. June 1-6, 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1747" to="1759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep Contextualized Word Representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training</title>
		<author>
			<persName><forename type="first">Weizhen</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiusheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="2401" to="2410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Improving Language Understanding by Generative Pre-Training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Get To The Point: Summarization with Pointer-Generator Networks</title>
		<author>
			<persName><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">An Entity-Driven Framework for Abstractive Summarization</title>
		<author>
			<persName><forename type="first">Eva</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="3278" to="3289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Abstractive Document Summarization with a Graph-Based Attentional Neural Model</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pointer Networks</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">TL;DR: Mining Reddit to Learn Automatic Summarization</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>VÃ¶lske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahbaz</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on New Frontiers in Summarization</title>
				<meeting>the Workshop on New Frontiers in Summarization<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="59" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization</title>
		<author>
			<persName><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020, Virtual Event (Proceedings of Machine Learning Research</title>
				<meeting>the 37th International Conference on Machine Learning, ICML 2020, Virtual Event ( Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="11328" to="11339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">HIBERT: Document Level Pretraining of Hierarchical Bidirectional Transformers for Document Summarization</title>
		<author>
			<persName><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<title level="s">Long Papers</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5059" to="5069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Extractive Summarization as Text Matching</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan-Jing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6197" to="6208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Searching for Effective Neural Extractive Summarization: What Works and What&apos;s Next</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<title level="s">Long Papers</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural Document Summarization by Jointly Learning to Score and Select Sentences</title>
		<author>
			<persName><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="654" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Selective Encoding for Abstractive Sentence Summarization</title>
		<author>
			<persName><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
