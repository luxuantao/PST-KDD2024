<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Univer-sity of Saskatchewan</orgName>
								<address>
									<postCode>S7N 5E2</postCode>
									<settlement>Saskatoon</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1FD221BBAC722AFE46AD819C77905CD4</idno>
					<idno type="DOI">10.1109/TIP.2016.2528042</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LBP-Based Segmentation of Defocus Blur Xin Yi and Mark Eramian</head><p>Abstract-Defocus blur is extremely common in images captured using optical imaging systems. It may be undesirable, but may also be an intentional artistic effect, thus it can either enhance or inhibit our visual perception of the image scene. For tasks, such as image restoration and object recognition, one might want to segment a partially blurred image into blurred and non-blurred regions. In this paper, we propose a sharpness metric based on local binary patterns and a robust segmentation algorithm to separate in-and out-of-focus image regions. The proposed sharpness metric exploits the observation that most local image patches in blurry regions have significantly fewer of certain local binary patterns compared with those in sharp regions. Using this metric together with image matting and multiscale inference, we obtained high-quality sharpness maps. Tests on hundreds of partially blurred images were used to evaluate our blur segmentation algorithm and six comparator methods. The results show that our algorithm achieves comparative segmentation results with the state of the art and have big speed advantage over the others.</p><p>Index Terms-Defocus, blur, segmentation, LBP, local binary patterns, image restoration, object recognition, out-of-focus, blurred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>D EFOCUS blur in an image is the result of an out-of-focus optical imaging system. In the image formation process, light radiating from points on the focus plane are mapped to a point in the sensor, but light from a point outside the focus plane illuminates a non-point region on the sensor known as a circle of confusion. Defocus blur occurs when this circle becomes large enough to be perceived by human eyes.</p><p>In digital photography, defocus blur is employed to blur background and "pop out" the main subject using largeaperture lenses. However, this inhibits computational image understanding since blurring of the background suppresses details beneficial to scene interpretation. In this case, separation of the blurred and sharp regions of an image may be necessary so that post-processing or restoration algorithms can be applied without affecting the sharp regions, or so that image features are only extracted from in-focus regions.</p><p>Most current image deblurring methods assume that the blur is spatially invariant <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b40">[41]</ref>. Typically, a global blur kernel is estimated and the original image is reconstructed by fitting it to different image priors with maximimum a posteriori estimation. Methods that explicitly model spatially variant blur typically restore small image patches within which blur can be treated as invariant, and restored patches are stitched together <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>. Efficient and accurate detection of blurred or non-blurred regions is useful in several contexts including: 1) in avoiding expensive post-processing of non-blurred regions (e.g. deconvolution) <ref type="bibr" target="#b11">[12]</ref>; 2) in computational photography to identify a blurred background and further blur it to achieve the artistic bokeh effect <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b42">[43]</ref>, particularly for high-depth-of-field cellular phone cameras; and 3) for object recognition in domains where objects of interest are not all-in-focus (e.g. microscopy images) and portions of the object which are blurred must be identified to ensure proper extraction of image features, or, if only the background is blurred, to serve as an additional cue to locate the foreground object and perform object-centric spatial pooling <ref type="bibr" target="#b39">[40]</ref>.</p><p>The purpose of segmentation of defocus blur is to separate blurred and non-blurred regions so that any aforementioned post-processing can be facilitated. Herein, this problem is explicitly addressed without quantifying the extent of blurriness and a new sharpness metric based on Local Binary Patterns (LBP) is introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>The most commonly seen approach for defocus segmentation literature is via local sharpness measurement. There are many works in this area in the past two decades and most of them can be found in the image quality assessment field where images are rated by a single sharpness score that should conform to the human visual perception. These applications only require a single sharpness value to be reported for a single image, thus most of the measures only rely on sharpness around local edges <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b30">[31]</ref> or some distinctive image structures determined in the complex wavelet transform domain <ref type="bibr" target="#b16">[17]</ref>. Similarly, the line spread profile has been adopted for edge blurriness measurement in image recapture detection <ref type="bibr" target="#b45">[46]</ref>. Since most of these metrics are measured around edges, they cannot readily characterize sharpness of any given local image content unless edge-sharpness is interpolated elsewhere as was done in <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b56">[57]</ref>.</p><p>Measures such as higher order statistics <ref type="bibr" target="#b20">[21]</ref>, variance of wavelet coefficients <ref type="bibr" target="#b51">[52]</ref>, local variance image field <ref type="bibr" target="#b52">[53]</ref> have been used directly in segmentation of objects of interest in low-depth-of-field images. These local sharpness metrics are based on local image energy which means that the measures will not only decrease if the energy of the PSF (point spread function) decreases (becomes more blurry), but also decreases if the energy of the image content drops. Thus, a blurry, high-contrast edge region could have a higher sharpness score than an in-focus, low-contrast one. These metrics are suitable for relative sharpness measures, e.g. in focal stacking, but do not behave very well for local sharpness measure across various image contents. This deficiency has already been pointed out in <ref type="bibr" target="#b54">[55]</ref>.</p><p>Recently, Liu et al. <ref type="bibr" target="#b27">[28]</ref> and Shi et al. <ref type="bibr" target="#b42">[43]</ref> proposed a set of novel local sharpness features, e.g. gradient histogram span, kurtosis, for training of a na√Øve Bayes classifier for blur classification of local image regions. The sharpness is interpreted as the likelihood of being classified as sharp patch. Su et al. used singular value decomposition (SVD) of image features to characterize blur and simple thresholding for blurred region detection <ref type="bibr" target="#b43">[44]</ref>. Vu et al. used local power spectrum slope and local total variation for the measure in both the spectral and spatial domains. The final sharpness is the geometric mean of the two measures <ref type="bibr" target="#b50">[51]</ref>.</p><p>Instead of measuring sharpness only based on local information, Shi et al. proposed to learn a sparse dictionary based on a large external set of defocus images and then use it to build a sparse representation of the test image patch. The final measure was the number of non-zero elements of the corresponding words <ref type="bibr" target="#b19">[20]</ref>.</p><p>Depth map estimation is another approach that can also be used for defocus blur segmentation. Zhuo et al. used edge width as a reference for depth measurement under the assumption that edges in blurred regions are wider than those in sharp regions <ref type="bibr" target="#b56">[57]</ref>. They obtained a continuous defocus map by propagating the sharpness measures at edges to the rest of the image using image matting <ref type="bibr" target="#b24">[25]</ref>. Bae and Durand's work is similar, but they computed edge width differently by finding the distance of second derivative extrema of opposite sign in the gradient direction <ref type="bibr" target="#b3">[4]</ref>. These methods tend to highlight edges in places where the blur measure is actually smooth.</p><p>Zhu et al. tried to explicitly estimate the space-variant PSF by analyzing the localized frequency spectrum of the gradient field <ref type="bibr" target="#b55">[56]</ref>. The defocus blur kernel is parameterized as a function of a single variable (e.g. radius for a disc kernel or variance for Gaussian kernel) and is estimated via MAP k estimation <ref type="bibr" target="#b25">[26]</ref>. Similar work can be found in <ref type="bibr" target="#b8">[9]</ref> but the blur kernel is restricted to a finite number of candidates. Khosro et al. estimate the blur kernel locally via blind image deconvolution by assuming the kernel is invariant inside the local block. But instead of fitting the estimated kernel to a parameterized model, they quantized the sharpness through reblurring <ref type="bibr" target="#b4">[5]</ref>. Florent et al. treat the blur kernel estimation as a multilabel energy minimization problem by combining learned local blur evidence with global smoothness constraints <ref type="bibr" target="#b10">[11]</ref>. These methods are inherently slow because of the iterative nature.</p><p>Unlike <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>, we do not intend to construct a depth map. Our goal is only to separate in-focus regions from regions of defocus blur; Also unlike <ref type="bibr" target="#b19">[20]</ref>, we do not rely on external defocus images; in this respect our work is most similar to <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b50">[51]</ref>.</p><p>We postulate that local-based defocus blur segmentation methods to date have been limited by the quality of the sharpness measures which they employ.</p><p>We now review local metrics of image sharpness that have been recently introduced for the segmentation of blurred regions. Generally, they fall into one of three categories: gradient domain metrics, intensity domain metrics, and frequency domain metrics. <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b53">[54]</ref>: The gradient magnitude of sharp images exhibits a heavy-tailed distribution <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b40">[41]</ref> and can be modelled with a two component Gaussian mixture model (GMM):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Gradient Domain Metrics 1) Gradient Histogram Span</head><formula xml:id="formula_0">G = a 1 e - (g-Œº 1 ) 2 œÉ 1 + a 2 e - (g-Œº 2 ) 2 œÉ 2 , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where means Œº 1 = Œº 2 = 0, variance œÉ 1 &gt; œÉ 2 , g is the gradient magnitude, and G is the gradient magnitude distribution in a local region. The component with larger variance is believed to be responsible for the heavy-tailed property. Thus the local sharpness metric is:</p><formula xml:id="formula_2">m GHS = œÉ 1 . (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>2) Kurtosis <ref type="bibr" target="#b42">[43]</ref>: Kurtosis, which captures the "peakedness" of a distribution, also characterizes the gradient magnitude distribution difference. It is defined as:</p><formula xml:id="formula_4">K = E[(g -Œº) 4 ] E 2 [(g -Œº) 2 ] -3<label>( 3 )</label></formula><p>where the first term is the fourth moment around the mean divided by the square of the second moment around the mean. The offset of 3 is to cause the peakedness measure of a normal distribution to be 0. The derived local sharpness metric is:</p><formula xml:id="formula_5">m K = min(ln(K (g x ) + 3), ln(K (g y ) + 3)),<label>(4)</label></formula><p>where g x , g y are gradient magnitudes along x and y axis respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Intensity Domain Metrics</head><p>1) Singular Value Decomposition (SVD) <ref type="bibr" target="#b27">[28]</ref>: An image patch P P P can be decomposed by SVD:</p><formula xml:id="formula_6">P P P = U U U V V V T = n i=1 Œª i u u u i v v v T i ,<label>(5)</label></formula><p>where U U U, V V V are orthogonal matrices, is a diagonal matrix whose diagonal entries are singular values arranged in descending order, u u u i and v v v i are the column vectors of U U U and V V V respectively, and Œª i are the singular values of . It is claimed that large singular values correspond to the rough shape of the patch whereas small singular values correspond to details. The sharpness metric is:</p><formula xml:id="formula_7">m SVD (k) = 1 - k i=1 Œª i n i=1 Œª i , (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>where the numerator is the sum of the k largest singular values.</p><p>2) Linear Discriminant Analysis (LDA) <ref type="bibr" target="#b42">[43]</ref>: By sampling a set of blurred and non-blurred patches, this method finds a transform W W W that maximizes the ratio of the between-class variance S b to the within-class variance S w of the projected data with each variance:</p><formula xml:id="formula_9">S b = 2 j =1 (Œº j -Œº) T (Œº j -Œº), S w = 2 j =1 N j i=1 (x j i -Œº j ) T (x j i -Œº j ),<label>(7)</label></formula><p>where j = 1 represents the blurred class, j = 2 represents the sharp class, x i is the intensity of the i -th pixel, and N j is the number of pixels in the corresponding region (see also <ref type="bibr" target="#b41">[42,</ref><ref type="bibr">Sec. 2.3]</ref>). This is solved by maximizing the ratio</p><formula xml:id="formula_10">det|S b | det|S w |</formula><p>and the resulting column vectors of the projection matrix W W W are the eigenvectors of S -1 w S b . The final metric can be expressed as:</p><formula xml:id="formula_11">m LDA (i ) = w w w T i p p p (<label>8</label></formula><formula xml:id="formula_12">)</formula><p>where w w w i is the i -th column vector of matrix W W W, and p p p is the vectorized patch intensity.</p><p>3) Sparsity <ref type="bibr" target="#b19">[20]</ref>: This measure is based on sparse representation. Each patch is decomposed according to a learned over-complete dictionary which expressed as</p><formula xml:id="formula_13">argmin u u u ||p p p -D D Du u u|| 2 + Œª||u u u|| 1<label>(9)</label></formula><p>where D D D is the learned dictionary on a set of blur image patches. p p p is the vectorized patch intensity and u u u is the coeficients vector, each item of which is the weight used for the reconstruction. The reconstruction of a sharp patch requires more words than blurred patches. Thus the sharpness measure is defined as the number of non-zero elements in u u u, i.e., the L 0 norm of u u u.</p><p>m S = ||u u u|| 0 <ref type="bibr" target="#b9">(10)</ref> 4) Total Variation <ref type="bibr" target="#b50">[51]</ref>: This metric is defined as</p><formula xml:id="formula_14">m TV = 1 4 max Œæ ‚ààP T V (Œæ ) with T V (Œæ ) = 1 255 i, j |x i -x j |<label>(11)</label></formula><p>which is the maximum of the total variation of smaller blocks Œæ (set as 2 √ó 2 in the original paper) inside the local patch P. The coefficient 1 4 is a normalization factor since the largest TV of a 2 √ó 2 block is 4. The author argued that a non-probabilistic application of TV can be used as a measure of local sharpness due to its ability to take into account the degree of local contrast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Frequency Domain Metrics</head><p>1) Power Spectrum <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b50">[51]</ref>: The average of the power spectrum for frequency œâ of an image patch is:</p><formula xml:id="formula_15">J (œâ) = 1 n Œ∏ J (œâ, Œ∏ ) A œâ Œ± (<label>12</label></formula><formula xml:id="formula_16">)</formula><p>where J (œâ, Œ∏ ) is the squared magnitude of the discrete Fourier transform of the image patch in the polar coordinate system, n is the number of quantizations of Œ∏ , and A is an amplitude scaling factor. It was shown that Œ± = 2 for sharp, natural images <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b48">[49]</ref>. Since blurred images contain less energy in the high frequency components, the magnitude of their power spectra tend to fall off much faster with increasing œâ, and the value of Œ± is larger for such images.</p><p>Rather than fitting a linear model to obtain Œ±, the average of the power spectrum can be used instead as an indicator since the power spectra of blurred regions tend to have a steeper slope than for sharp regions, thus have a smaller average power. The metric is:</p><formula xml:id="formula_17">m APS = 1 n œâ Œ∏ J (œâ, Œ∏ )<label>(13)</label></formula><p>In <ref type="bibr" target="#b27">[28]</ref> and <ref type="bibr" target="#b50">[51]</ref>, the authors directly use the fitted spectrum slope Œ± as the measure. However, Ferzli and Karam <ref type="bibr" target="#b42">[43]</ref> claimed that the average power spectrum is more robust to outliers and overfitting, thus we only evaluate m AP S .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Response Behavior of Sharpness Metrics</head><p>Given these sharpness metrics, we conducted a preliminary study to observe how they respond to different local image textures to see if they are limiting progress in blur detection as previously postulated. Since the proposed work is centred on local sharpness measure, this experiment excludes measures that rely on external informations, e.g. m L D A and m S .</p><p>Following the same methodology in <ref type="bibr" target="#b9">[10]</ref>, we assumed there are four common types of textures that appear in natural scenes, a random texture such as grass, a man-made texture, a smooth texture such as sky or fruit surface, and an almost smooth texture such as areas on the road sign (the characteristic of this texture is of low contrast and has more detail than pure smooth regions). Four such exemplar textures are shown in Figure <ref type="figure" target="#fig_0">1</ref>. Gaussian blur of varying severity (œÉ ‚àà [0.1, 10.0]) was applied to these image patches and each metric was computed for each texture and blur level. For the SVD-based metric, we tested with k = 6, that is, m SVD <ref type="bibr" target="#b5">(6)</ref>, but the response is similar for most values of k. The size of image patches were 21 √ó 21 pixels for all metrics.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> shows the response of each metric to each of the four exemplar textures in Figure <ref type="figure" target="#fig_0">1</ref> over the tested range of œÉ . In addition, by evaluating 8000 sharp patches covering different scenes, an aggregate performance of these measures is also shown in Figure <ref type="figure" target="#fig_1">2</ref>. The thick red curve shows the mean response over the 8000 patches and the dashed red curves show higher and lower quartiles (75th and 25th percentile). It can be seen from this figure that, in an aggregate manner, all measures decrease when blur increases (one exception is that m K shows a slight increase after œÉ approaches 5). However, the aggregate data hides responses that are very different from the aggregate with m GHS and m K exhibiting minor to moderate non-monotonicity on some specific textures. Two patches are shown in Figure <ref type="figure" target="#fig_2">3</ref> with two levels of blur. The one with larger œÉ has larger m K .</p><p>A smooth texture should elicit a constant, yet low response to the sharpness metrics since its appearance does not change   with varying degrees of defocus blur, but the yellow curve shows big differences in responses for most of the sharpness metrics, with m GHS , m TV and m SVD exhibiting the least variation. One would also expect that blurry regions would have smaller response than sharp regions, but that is not the case for all metrics. For example, at œÉ = 1.5 the range of values between the higher and lower quartile has a large overlap with range when œÉ = 0. In this respect, m APS has the worse performance. Finally, none of the metrics are wellsuited for measuring low contrast sharp regions, such as the almost smooth region in the example. This is because the low contrast region has very small intensity variance which leads to low gradient and low frequency response. The green and grey curve are almost inseparable for m GHS , m SVD and m TV . This drawback is further shown in Figure <ref type="figure" target="#fig_7">11</ref>. The low contrast yellow region of the road sign does not have a correct response for all measures even if it is in focus.</p><p>In the next section we present our new sharpness metric based on local binary patterns which is monotonic. The range of response values for blur patches has less intersection than that of sharp regions and it has a more appropriate response to low contrast region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED LBP BASED BLUR METRIC</head><p>Local Binary Patterns (LBP) <ref type="bibr" target="#b32">[33]</ref> have been successful for computer vision problems such as texture segmentation <ref type="bibr" target="#b31">[32]</ref>, face recognition <ref type="bibr" target="#b2">[3]</ref>, background subtraction <ref type="bibr" target="#b18">[19]</ref> and recognition of 3D textured surfaces <ref type="bibr" target="#b35">[36]</ref>. The LBP code of a pixel (x c , y c ) is defined as:</p><formula xml:id="formula_18">L B P P,R (x c , y c ) = P-1 p=0 S(n p -n c ) √ó 2 p with S(x) = 1 |x| ‚â• T LBP 0 |x| &lt; T LBP (<label>14</label></formula><formula xml:id="formula_19">)</formula><p>where n c is the intensity of the central pixel (x c , y c ), n p corresponds to the intensities of the P neighbouring pixels located on a circle of radius R centered at n c , and T LBP &gt; 0 is a small, positive threshold in order to achieve robustness for flat image regions as in <ref type="bibr" target="#b18">[19]</ref>. Figure <ref type="figure">4</ref> shows the locations of the neighbouring pixels n p for P = 8 and R = 1. In general, the points n p do not fall in the center of image pixels, so the intensity of n p is obtained with bilinear interpolation.</p><p>A rotation invariant version of LBP can be achieved by performing the circular bitwise right shift that minimizes the value of the LBP code when it is interpreted as a binary number <ref type="bibr" target="#b33">[34]</ref>. In this way, number of unique patterns is reduced to 36. Ojala et al. found that not all rotation invariant patterns sustain rotation equally well <ref type="bibr" target="#b33">[34]</ref>, and so proposed using only uniform patterns which are a subset of the rotation invariant patterns. A pattern is uniform if the circular sequence of bits contains no more than two transitions from one to zero, or zero to one. The non-uniform patterns are then all treated as one single pattern. This further reduces the number of unique patterns to 10 (for 8-bit LBP), that is, 9 uniform patterns, and the category of non-uniform patterns. The uniform patterns are shown in Figure <ref type="figure">5</ref>. In this figure, neighbouring pixels are coloured blue if their intensity difference from centre pixel is larger than T LBP , and we say that it has been "triggered", otherwise, the neighbours are coloured red. Fig. <ref type="figure">6</ref>. LBP code distribution in blurred and sharp regions. Bins 0-8 are the counts of the uniform patterns; bin 9 is the count of non-uniform patterns. Data is sampled from 100 partial blurred images from <ref type="bibr" target="#b41">[42]</ref>.</p><p>Figure <ref type="figure">6</ref> shows the histogram of the nine uniform LBP patterns appearing in the blurred and non-blurred regions of 100 images randomly selected from a publicly available dataset of 704 partially blurred images <ref type="bibr" target="#b41">[42]</ref>, each of which is provided with a hand-segmented groundtruth image denoting the blurred and non-blurred regions. Bin 9 is the number of non-uniform patterns. The frequency of patterns 6, 7, 8, and 9 in blurred regions is noticeably less than that for sharp regions. The intuitive explanation for this is that in smoother areas, most neighbouring pixels will be similar in intensity to n c , and the chance of a neighbour being triggered is lower, making the lower-numbered uniform patterns with fewer triggered neighbours more likely. Examples of the LBP histograms of specific sharp and blurred patches is given in Figure <ref type="figure" target="#fig_4">7</ref> which also exhibit this expected behaviour.</p><p>Our proposed sharpness metric exploits these observations:</p><formula xml:id="formula_20">m LBP = 1 N 9 i=6 n(L B P riu2 8,1 i ) (<label>15</label></formula><formula xml:id="formula_21">)</formula><p>where n(L B P riu2 8,1 i ) is the number of rotation invariant uniform 8-bit LBP pattern of type i , and N is the total number of pixels in the selected local region which serves to normalize the metric so that m LBP ‚àà [0, 1]. One of the advantages of measuring sharpness in the LBP domain is that LBP features are robust to monotonic illumination changes which occur frequently in natural images.</p><p>The threshold T LBP in Equation 14 controls the proposed metric's sensitivity to sharpness. As shown in Figure <ref type="figure" target="#fig_5">8</ref>, by increasing T LBP , the metric becomes less sensitive to sharpness. However, there is a tradeoff between sharpness sensitivity and noise robustness, as shown in Figure <ref type="figure" target="#fig_6">9</ref>.</p><p>In situations where high sensitivity to sharpness is needed, a discontinuity-preserving noise reduction filter such as non-local means <ref type="bibr" target="#b6">[7]</ref> should be employed.</p><p>Figure <ref type="figure" target="#fig_0">10</ref> shows our metric's response to various levels of blur (T LBP = 0.016). There is a sharp fall-off between œÉ = 0.2 and œÉ = 1.0 which makes the intersection of response value range of sharp and blur much smaller than the other metrics. When œÉ approaches 2, responses for all patches shrinks to zero which facilitates segmentation of blurred and sharp regions by simple thresholding. Moreover, almost smooth   <ref type="formula" target="#formula_20">15</ref>) for various values of threshold T LBP . T LBP determines the cutoff for the magnitude of intensity change that is considered an "edge", regardless of edge sharpness. Response of m LBP in the presence of noise. Top: the original image and two copies corrupted by Gaussian noise; bottom: the corresponding sharpness maps. T LBP = 0.016. region elicit a much higher response than smooth region compared with the other metrics. Finally, the metric response is nearly monotonic, decreasing with increasing blur, which should allow such regions to be distinguished with greater accuracy and consistency. Figure <ref type="figure" target="#fig_7">11</ref> shows maps of the local response of our metric and comparators for a sample image. Our metric has the most coherent response and responds the most consistently to low contras regions such as the road sign.</p><p>Table <ref type="table" target="#tab_0">I</ref> shows a comparison of the runtime of m LBP and comparator metrics. Where available, author-supplied code for calculating the metrics was used, otherwise our own Fig. <ref type="figure" target="#fig_0">10</ref>. Our metrics' response to the sample patches shown in Figure <ref type="figure" target="#fig_0">1</ref>. As the same as in Figure <ref type="figure" target="#fig_1">2</ref>, an aggregate response on 8000 sharp patches is also shown with the thick red curve showing the mean response and the dashed red curve showing the higher and lower quartile. The average runtimes are reported in Table <ref type="table" target="#tab_0">I</ref>. The sharpness maps, response curves, and runtimes provide strong qualitative and quantitative evidence that our metric is superior. In the next section we present a blur segmentation method that achieves the state-of-the-art results by employing this metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. NEW BLUR SEGMENTATION ALGORITHM</head><p>This section presents our algorithm for segmenting blurred/sharp regions with our LBP-based sharpness metric; it is summarized in Figure <ref type="figure" target="#fig_8">12</ref>. The algorithm has four main steps: multi-scale sharpness map generation, alpha matting initialization, alpha map computation, and multi-scale sharpness inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multi-Scale Sharpness Map Generation</head><p>In the first step, multi-scale sharpness maps are generated using m LBP . The sharpness metric is computed for a local patch about each image pixel. Sharpness maps are constructed at three scales where scale refers to local patch size. By using an integral image <ref type="bibr" target="#b49">[50]</ref>, sharpness maps may be computed in constant time per pixel for a fixed P and R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Alpha Matting Initialization</head><p>Alpha matting is the process of decomposing an image into foreground and background. The image formation model can be expressed as:</p><formula xml:id="formula_22">I (x, y) = Œ± x,y F(x, y) + (1 -Œ± x,y )B(x, y) (<label>16</label></formula><formula xml:id="formula_23">)</formula><p>where the alpha matte, Œ± x,y , is the opacity value on pixel position (x, y). It can be interpreted as the confidence that a pixel is in the foreground. Typically, alpha matting requires a user to interactively mark known foreground and background pixels, initializing those pixels with Œ± = 1 and Œ± = 0, repectively.</p><p>Interpreting "foreground" as "sharp" and background as "blurred", we initialized the alpha matting process automatically by applying a double threshold to the sharpness maps computed in the previous step to produce an initial value of Œ± for each pixel:</p><formula xml:id="formula_24">mask s (x, y) = ‚éß ‚é™ ‚é® ‚é™ ‚é© 1, if m LBP (x, y) &gt; T m 1 . 0, if m LBP (x, y) &lt; T m 2 . m LBP (x, y), otherwise. (<label>17</label></formula><formula xml:id="formula_25">)</formula><p>where s indexes the scale, that is, mask s (x, y) is the initial Œ±-map at the s-th scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Alpha Map Computation</head><p>The Œ±-map was solved by minimizing the following cost function as proposed by <ref type="bibr">Levin [25]</ref>:</p><formula xml:id="formula_26">E(Œ±) = Œ± T L L LŒ± + Œª(Œ± -Œ±) T (Œ± -Œ±)<label>(18)</label></formula><p>where Œ± is the vectorized Œ±-map, Œ± = mask i (x, y) is one of the vectorized initialization alpha maps from the previous step, and L L L is the matting Laplacian matrix. The first term is the regulation term that ensures smoothness, and the second term is the data fitting term that encourages similarity to Œ±.</p><p>For more details on Equation <ref type="formula" target="#formula_26">18</ref>, readers are referred to <ref type="bibr" target="#b24">[25]</ref>.</p><p>The alpha matting was applied at each scale as shown in Figure <ref type="figure" target="#fig_8">12</ref>. The final alpha map at each scale is denoted as Œ± s , s = 1, 2, 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Multi-Scale Inference</head><p>After determining the alpha map at three different scales, a multi-scale graphical model was adopted to make the final decision <ref type="bibr" target="#b42">[43]</ref>. The total energy on the graphical model is expressed as:</p><formula xml:id="formula_27">E(h) = 3 s=1 i |h s i -ƒ•s i | +Œ≤ ‚éõ ‚éù 3 s=1 i j ‚ààN s i |h s i -h s j | + 2 s=1 i |h s i -h s+1 i | ‚éû ‚é†<label>(19)</label></formula><p>where ƒ•s i = Œ± s i is the alpha map for scale s at pixel location i that was computed in the previous step, and h s i is the sharpness to be inferred. The first term on the right hand side is the unary term which is the cost of assigning sharpness value h s i to pixel i in scale s. The second is the pairwise term which enforces smoothness in the same scale and across different scales. The weight Œ≤ regulates the relative importance of these two terms. Optimization of Equation 19 was performed using loopy belief propagation <ref type="bibr" target="#b29">[30]</ref>. The output of the algorithm is h 3 which is the inferred sharpness map at the largest scale. This is a grayscale image, where higher intensity indicates greater sharpness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. BLUR SEGMENTATION ALGORITHM EVALUATION</head><p>Our blur segmentation algorithms were tested using a public blurred image dataset consisting of 704 partially blurred images and accompanying hand-segmented ground truth images <ref type="bibr" target="#b41">[42]</ref>. Each image was segmented into sharp and blurred regions using the processes described in Section IV. Sharpness metric m LBP was computed with T LBP = 0.016. The sharpness map scales were square local regions of 11 √ó 11, 15 √ó 15, and 21 √ó 21 pixels. The thresholds used in the alpha matting step were T m 1 = 0.3 and T m 2 = 0.01. Weight Œ≤ = 0.5 was used in the multi-scale inferencing step.</p><p>We compared our algorithm to six comparator methods briefly mentioned in Section II of which we now remind the reader. Su et al. simply calculated a sharpness map using m SVD <ref type="bibr" target="#b43">[44]</ref>; Vu combined both spectral and spatial sharpness (as called S 1 and S 2 in the original paper) using a geometric mean <ref type="bibr" target="#b50">[51]</ref>. Shi14 used all of m GHS , m K , m LDA , m APS together with a na√Øve Bayes classifier and multi-scale inference model <ref type="bibr" target="#b42">[43]</ref>. Shi15 formed a sparse representation of image The curves were obtained by thresholding the sharpness maps with threshold varying in the range of [0, 255]. Note that our method achieves the highest precision when recall is larger than 0.8. This comparison might be unfair for Zhu since their segmentation is based on graph cut rather than thresholding of the depth map. Therefore we compared their graph cut segmented binary map in the F-measure section. patches using a learned dictionary for the detection of slight perceivable blur <ref type="bibr" target="#b19">[20]</ref>. Zhuo computed a depth map based on edge width <ref type="bibr" target="#b56">[57]</ref>. Zhu estimated the space-variant PSF by statistical modelling of the localized frequency spectrum of the gradient field <ref type="bibr" target="#b55">[56]</ref>.</p><p>All the output of these methods are grayscale images where greater intensity indicates greater sharpness, and all (except for Zhu) use simple thresholding, T seg , as a final step to produce a segmentation, as in our own algorithm. The parameters for the comparator algorithms were to the defaults as in their original code. Since we were unable to get the original code for Zhu's algorithm <ref type="bibr" target="#b55">[56]</ref>, which belongs to Adobe Systems Inc., the results shown here were produced by our own implementation of the algorithm as described in the published paper. The depth map was normalized by 8 (since the coherence labels are in the range of [0,8]) and inverted to get the sharpness map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Precision and Recall</head><p>Precision and recall curves were generated for each algorithm by varying the threshold used to produce a segmentation of the final sharpness maps (i.e. similar to <ref type="bibr" target="#b42">[43]</ref>).</p><formula xml:id="formula_28">precision = R ‚à© R g R , r ecall = R ‚à© R g R g (<label>20</label></formula><formula xml:id="formula_29">)</formula><p>where R is the set of pixels in the segmented blurred region and R g is the set of pixels in the ground truth blurred region. Figure <ref type="figure" target="#fig_9">13</ref> shows the precision and recall curves for each method with the threshold T seg sampled at every integer within the interval [0, 255]. Our algorithm achieves higher precision than the comparator algorithms when recall is above 0.8. Moreover, the proposed sharpness metric alone achieves results comparable to Shi15. Figure <ref type="figure" target="#fig_10">14</ref> shows the sharpness maps (prior to final thresholding) for each algorithm for a few sample images. Our method is superior than the others under various background and blurs. We attribute errors mainly to the shortcomings of the sharpness metrics used by local based methods-Shi14, Vu, Su (Section II). Moreover, our detection maps contain mostly high-or low-confidence values which can be more correctly thresholded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. F-Measure</head><p>In another experiment, we used an image dependent adaptive threshold proposed by <ref type="bibr" target="#b0">[1]</ref> for the segmentation with the threshold defined as: Precision, Recall and F-measure for adaptive thresholds. The result of Zhu is achieved by using graph cut instead of simple thresholding as suggested in their paper. Note that by using a smaller threshold (i.e. T seg = 0.3), our method can achieve comparative performance ( precision = 0.863, recall = 0.868, F-measure = 0.864) with Zhu. where, W, H are the width and height of the final sharpness map I . Then, similar to <ref type="bibr" target="#b34">[35]</ref>, the weighted harmonic mean measure or F-measure of precision and recall was computed for comparison. The definition is as follows:</p><formula xml:id="formula_30">T seg = 2 W √ó H W x=1 H y=1 I (x, y)<label>(21)</label></formula><formula xml:id="formula_31">F Œ≤ = (1 + Œ≤ 2 ) √ó precision √ó r ecall Œ≤ 2 √ó precision + r ecall<label>(22)</label></formula><p>Here, Œ≤ 2 was set to 0.3 as in <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b34">[35]</ref> . Note that, the segmentation map of Zhu was produced by graph cut instead of simple thresholding of the depth map. The parameters we used were the same as suggested in their paper which are Œª 0 = 1000, œÉ Œª = 0.04, œÑ = 2. Exemplar segmentation maps of images in Figure <ref type="figure" target="#fig_10">14</ref> is shown in Figure <ref type="figure" target="#fig_12">16</ref>.</p><p>The reason our performance is worse than Zhu in Figure <ref type="figure" target="#fig_11">15</ref> is that the adaptive threshold is not the best threshold for our method. The best precision and recall we can achieve, as can be seen in Figure <ref type="figure" target="#fig_9">13</ref>, is precision = 0.863, r ecall = 0.868 which is comparative with the one Zhu have achieved. However, even if only the adaptive-thresholded results were compared, our method ranked first among the comparators.</p><p>A run time comparison of the complete segmentation algorithms is shown in Table <ref type="table" target="#tab_1">II</ref>. The same setup was used for the measurement of runtime as in Table <ref type="table" target="#tab_0">I</ref>. Given m LBP 's performance in the precision and recall curve and F-measure, it has a significant speed advantage over the others. The time for our complete segmentation algorithm is mostly spent on the matting and multi-scale inference. It ranks the fourth among all these methods.</p><p>Finally, we give some examples of our algorithm applied to images other than those in our evaluation data set. Microscopy optics often have low depth of field and form an important class of images for blur detection. Figure <ref type="figure" target="#fig_13">17</ref> shows examples of our algorithm applied to such images. The first is a plant seed <ref type="bibr" target="#b38">[39]</ref> whose roughly spherical shape results in a ringshaped in-focus region. The other image is a microorganism <ref type="bibr" target="#b36">[37]</ref> in fresh water. The threshold T LBP for the sharpness metric was set to 0.012 and 0.04 respectively. Note how well our segmentation results conformed to the visual perception of the image sharpness. Additional results can be seen in the supplementary file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Discussion</head><p>You may have noticed the jagged boundary of our segmentation map when there is a distinctive depth discontinuity between the foreground and background. This is because the sharpness is measured locally. It is inevitable to incorporate regions with various extents of sharpness by using a local window, especially around edges where the depth discontinuity occurs. Therefore, the sharp area is enlarged in the alpha matting initialization step (step B). Zhu solved this problem by taking smoothness and color edge information into consideration in the coherence labeling step but would also fail in cases where depth changes gradually.</p><p>There are certain situations that can cause our method to fail. Our method has difficulty differentiating an in-focus smooth region and a blurred smooth region since only a limited small size of local neighbour is considered, but this is a problem that will be inherently challenging for any algorithm. If noise level in the image is low, this problem can be overcome to some extent by reducing the T LBP threshold. In addition, for object recognition purposes, this drawback would not weaken the feature representation too much since smooth regions contain little to no useful discriminating texture. An example of this type of failure case and proposed remedy can be seen in Figure <ref type="figure" target="#fig_15">18(a)</ref>.</p><p>Another failure case occurs due to image noise, but it can be mitigated by applying a noise reducing filter as mentioned in section III. An example of this type of failure and the proposed remedy is shown in Figure <ref type="figure" target="#fig_15">18(b)</ref>.</p><p>The selection of T LBP is essential for obtaining a satisfactory segmentation. It controls how much sharp area would appear in the final segmentation result. For a image with little to no noise, T LBP 0.016 should produce a reasonable result. Lowering the value would cause the inclusion of more low contrast sharp regions. For a image corrupted by noise, a noise reduction procedure should be employed.</p><p>The proposed metric was inspired by the statistical difference of local binary patterns of a set of partial blurred images. Since the source of blurriness is mainly defocus blur,    our metric currently is only capable of detecting defocus blur. Given that there are other type of blurriness such as those introduced by low qualities of lens and materials in image systems and motion blur, it would be worth studying the blur model due to the properties of optical devices <ref type="bibr" target="#b23">[24]</ref> and at the same time exploring properties of different patterns such as the non-uniform binary patterns and local ternary pattern (LTP) <ref type="bibr" target="#b44">[45]</ref> on blur regions of different type. Moreover, the ideas used in noise-resistant LBP (NRLBP) <ref type="bibr" target="#b37">[38]</ref>, which treats pixels susceptible to noise as having uncertain state and then determines the corresponding bit value based on the other bits of the LBP code, might worth borrowing if explicit handling of noise in blur detection is desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We have proposed a very simple yet effective sharpness metric for blur segmentation. This metric is based on the distribution of uniform LBP patterns in blur and non-blur image regions. The direct use of the local raw sharpness measure can achieve comparative results to the stat-of-the-art defocus segmentation method that based on sparse representation, which shows the potential of local based sharpness measures. By integrating the metric into a multiscale information propagation frame work, it can achieve comparative results with the state-of-the-art. We have shown that the algorithm's performance is maintained when using an automatically and adaptively selected threshold T seg . Our sharpness metric measures the number of certain LBP patterns in the local neighbourhood thus can be efficiently implemented by integral images. If combined with real-time matting algorithms, such as GPU implementations of global matting <ref type="bibr" target="#b17">[18]</ref>, our method would have significant speed advantage over the other defocus segmentation algorithms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Four commonly appeared textures in natural scenes.</figDesc><graphic coords="4,307.91,58.73,125.90,94.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Responses of different measures. The thick red curve shows the mean performance over 8000 patches and the dashed red line shows the higher and lower quartile. The responses to 4 exemplar patches are shown in blur, cyan, green, grey curves respectively.</figDesc><graphic coords="4,176.99,530.45,51.62,51.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. An example of the non-monotonicity of the sharpness measure m K . The patches showing here are the almost smooth patch under two levels of Gaussian blur as marked by black dots in m K response in Figure 2.</figDesc><graphic coords="4,119.99,530.45,51.50,51.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. 8-bit LBP with P = 8, R = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7.Histogram of LBP patterns in three different patches which are sampled from blurred (A), sharp (B), and transitive (C) areas respectively. In the ground truth image, white denotes the sharp region and black the blurred region.</figDesc><graphic coords="6,53.03,248.57,77.18,51.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Response of m LBP (Equation15) for various values of threshold T LBP . T LBP determines the cutoff for the magnitude of intensity change that is considered an "edge", regardless of edge sharpness.</figDesc><graphic coords="6,53.03,436.49,77.18,51.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9.Response of m LBP in the presence of noise. Top: the original image and two copies corrupted by Gaussian noise; bottom: the corresponding sharpness maps. T LBP = 0.016.</figDesc><graphic coords="6,53.03,502.37,77.18,51.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Metric responses for a sample image for different sharpness metrics.</figDesc><graphic coords="7,177.71,154.01,121.70,81.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Our blur segmentation algorithm. The main steps are shown on the left; the right shows each image generated and its role in the algorithm. The output of the algorithm is h 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Precision and recall curves for different methods on the blur dataset.The curves were obtained by thresholding the sharpness maps with threshold varying in the range of [0, 255]. Note that our method achieves the highest precision when recall is larger than 0.8. This comparison might be unfair for Zhu since their segmentation is based on graph cut rather than thresholding of the depth map. Therefore we compared their graph cut segmented binary map in the F-measure section.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Results achieved by different blur detection methods. Final sharpness maps, prior to thresholding for segmentation, are shown.</figDesc><graphic coords="9,151.91,573.65,77.42,51.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 15 .</head><label>15</label><figDesc>Fig.<ref type="bibr" target="#b14">15</ref>.Precision, Recall and F-measure for adaptive thresholds. The result of Zhu is achieved by using graph cut instead of simple thresholding as suggested in their paper. Note that by using a smaller threshold (i.e. T seg = 0.3), our method can achieve comparative performance ( precision = 0.863, recall = 0.868, F-measure = 0.864) with Zhu.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Binary segmentation map comparison with Zhu et al.</figDesc><graphic coords="11,162.47,115.61,77.42,51.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 17</head><label>17</label><figDesc>Fig. 17. Our algorithm applied to microscopy images. Top row: original images; bottom row: final sharpness maps.</figDesc><graphic coords="11,56.51,304.13,96.26,87.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Fig. 17. Our algorithm applied to microscopy images. Top row: original images; bottom row: final sharpness maps.</figDesc><graphic coords="11,159.11,304.13,132.50,87.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. Blur segmentation algorithm failure cases and mitigation. (a) failure due to ambiguity of smooth regions. (b) failure due to noise.</figDesc><graphic coords="11,63.47,495.05,51.50,67.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I RUNTIME</head><label>I</label><figDesc>COMPARISON OF VARIOUS METRICS. NOTE THAT THE SPEED</figDesc><table /><note><p><p>OF OUR METRIC CAN BE BOOSTED BY USING INTEGRAL IMAGE WHICH MAKES THE COMPLEXITY INDEPENDENT</p>OF THE SIZE OF LOCAL REGION implementations were used. All implementations were in MATLAB and were unoptimized. 10 randomly selected images with approximate size of 640 √ó 480 pixels were tested on a Mac with 2.66 GHz intel core i5 and 8 GB memory.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II RUN</head><label>II</label><figDesc>TIME COMPARISON OF DIFFERENT BLUR SEGMENTATION METHODS. THE TIME FOR OUR METHOD IS BASED ON A MEX IMPLEMENTATION OF m LBP</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank Jianping Shi for making their blur detection dataset available for the public. They would also like to thank the authors of <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b56">[57]</ref> for releasing their code.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Xin Yi received the M.S. degrees in mechanical engineering and automatic control from Northwestern Polytechnical University, Xi'an, China, in 2012. He is currently pursuing the Ph.D. degree in computer science with the University of Saskatchewan.</p><p>His research interests include image segmentation and object recognition.</p><p>Mark Eramian received the degrees (Hons.) in computer science and geophysics and the Ph.D. degree in computer science from the University of Western Ontario, London, ON, Canada, in 1997 and 2002, respectively.</p><p>He has been a Faculty Member with the Department of Computer Science, University of Saskatchewan, Saskatoon, SK, Canada, since 2002, where he is currently an Associate Professor. His research has been in the area of image processing and computer vision with a particular interest in image segmentation.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards HST restoration with a space-variant PSF, cosmic rays and other missing data</title>
		<author>
			<persName><forename type="first">H.-M</forename><surname>Adorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Restoration HST Images Spectra-II</title>
		<meeting>Restoration HST Images Spectra-II</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="72" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Face description with local binary patterns: Application to face recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietik√§inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2037" to="2041" />
			<date type="published" when="2006-12">Dec. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Defocus magnification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="571" to="579" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A novel approach for partial blur detection and segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bahrami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Multimedia Expo (ICME)</title>
		<meeting>IEEE Int. Conf. Multimedia Expo (ICME)</meeting>
		<imprint>
			<date type="published" when="2013-07">Jul. 2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A computational method for the restoration of images with an unknown, spatially-varying blur</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bardsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jefferies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Plemmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Exp</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1767" to="1782" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Color and spatial structure in natural scenes</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">R</forename><surname>Moorhead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Opt</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="157" to="170" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Analyzing spatially-varying blur</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zickler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="2512" to="2519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Motion blur removal from photographs</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. Elect. Eng. Comput. Sci., Massachusetts Inst. Technol</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to estimate and remove non-uniform image blur</title>
		<author>
			<persName><forename type="first">F</forename><surname>Couzinie-Devy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="1075" to="1082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Removing partial blur in a single image</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="2544" to="2551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Removing camera shake from a single photograph</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="787" to="794" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A no-reference objective image sharpness metric based on the notion of just noticeable blur (JNB)</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ferzli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Karam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="717" to="728" />
			<date type="published" when="2009-04">Apr. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Relations between the statistics of natural images and the response properties of cortical cells</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Amer. A</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2379" to="2394" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scanning singular-valuedecomposition method for restoration of images with space-variant blur</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Fish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grochmalicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Pike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Amer. A</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="464" to="469" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image sharpness assessment based on local phase coherence</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M A</forename><surname>Salama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2798" to="2810" />
			<date type="published" when="2013-07">Jul. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A global sampling method for alpha matting</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="2049" to="2056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A texture-based method for modeling the background and detecting moving objects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heikkila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietik√§inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="657" to="662" />
			<date type="published" when="2006-04">Apr. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Just noticeable defocus blur detection and estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="657" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Segmenting a low-depth-of-field image using morphological filters and region merging</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1503" to="1511" />
			<date type="published" when="2005-10">Oct. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast image deconvolution using hyperlaplacian priors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1033" to="1041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Blind deconvolution using a normalized sparsity measure</title>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Blind image deconvolution</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kundur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hatzinakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="43" to="64" />
			<date type="published" when="1996-05">May 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A closed-form solution to natural image matting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="242" />
			<date type="published" when="2008-02">Feb. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Understanding and evaluating blind deconvolution algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="1964" to="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient marginal likelihood optimization in blind deconvolution</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="2657" to="2664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image partial blur detection and classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A no-reference perceptual blur metric</title>
		<author>
			<persName><forename type="first">P</forename><surname>Marziliano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dufaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
		<idno>III-57-III-60</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Process</title>
		<meeting>Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Loopy belief propagation for approximate inference: An empirical study</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Conf. Uncertainty Artif. Intell</title>
		<meeting>15th Conf. Uncertainty Artif. Intell</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="467" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A no-reference image blur metric based on the cumulative probability of blur detection (CPBD)</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Narvekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Karam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2678" to="2683" />
			<date type="published" when="2011-09">Sep. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised texture segmentation using feature distributions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietik√§inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="477" to="486" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A comparative study of texture measures with classification based on featured distributions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietik√§inen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="59" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietik√§inen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>M√§enp√§√§</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002-07">Jul. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="733" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">View-based recognition of real-world textures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pietik√§inen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nurmela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>M√§enp√§√§</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="323" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Corral. water Project</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ochoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Herv√≠as</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename></persName>
		</author>
		<ptr target="https://www.flickr.com/photos/microagua/" />
		<imprint>
			<date type="published" when="2015-07">Jul. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Noise-resistant local binary pattern with an embedded error-correction mechanism</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4049" to="4060" />
			<date type="published" when="2013-10">Oct. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Seed identification dataset</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neudorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Canada Food Inspection Agency</title>
		<meeting><address><addrLine>Saskatoon, SK, Canada, Tech</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Object-centric spatial pooling for image classification</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision</title>
		<meeting><address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">High-quality motion deblurring from a single image</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">73</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Blur Detection Dataset</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="http://www.cse.cuhk.edu.hk/~leojia/projects/dblurdetect/dataset.html" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Discriminative blur detection features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
			<biblScope unit="page" from="2965" to="2972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Blurred image region detection and classification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th ACM Int. Conf. Multimedia</title>
		<meeting>19th ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1397" to="1400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Enhanced local texture feature sets for face recognition under difficult lighting conditions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1635" to="1650" />
			<date type="published" when="2010-06">Jun. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An image recapture detection algorithm based on learning dictionaries of edge profiles</title>
		<author>
			<persName><forename type="first">T</forename><surname>Thongkamwitoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Muammar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-L</forename><surname>Dragotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Security</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="953" to="968" />
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Identification and restoration of spatially variant motion blurs in sequential images</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Trussell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="123" to="126" />
			<date type="published" when="1992-01">Jan. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Image restoration of space variant blurs by sectioned methods</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Trussell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Hunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess. (ICASSP)</meeting>
		<imprint>
			<date type="published" when="1978-04">Apr. 1978</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="196" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Modelling the power spectra of natural images: statistics and information</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Der Schaaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Van Hateren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="2759" to="2770" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001-12">Dec. 2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="511" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">S 3 : A spectral and spatial measure of local perceived sharpness in natural images</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="934" to="945" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Unsupervised multiresolution segmentation for images with low depth of field</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wiederhold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="85" to="90" />
			<date type="published" when="2001-01">Jan. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Automatic object segmentation in images with low depth of field</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pyun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Process</title>
		<meeting>Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2002-06">Jun. 2002</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="805" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Automatic blur region segmentation approach using image matting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal, Image Video Process</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1173" to="1181" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Measuring spatially varying blur and its application in digital image restoration</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ph.D. dissertation, Dept. Elect. Eng., Univ. California</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<pubPlace>Santa Cruz, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Estimating spatially varying defocus blur from a single image</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4879" to="4891" />
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Defocus map estimation from a single image</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1852" to="1858" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
