<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Stochastic Natural Gradient Method for One-Shot Neural Architecture Search</title>
				<funder>
					<orgName type="full">SECOM Science and Technology Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Youhei</forename><surname>Akimoto</surname></persName>
							<email>&lt;akimoto@cs.tsukuba.ac.jp&gt;</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Tsukuba</orgName>
								<address>
									<country>&amp; RIKEN AIP</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shinichi</forename><surname>Shirakawa</surname></persName>
							<email>shinichi-bg@ynu.ac.jp&gt;.</email>
							<affiliation key="aff1">
								<orgName type="institution">Yokohama National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nozomu</forename><surname>Yoshinari</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Yokohama National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kento</forename><surname>Uchida</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Yokohama National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shota</forename><surname>Saito</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Yokohama National University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">SkillUp AI Co</orgName>
								<address>
									<country>Ltd</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kouhei</forename><surname>Nishida</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Shinshu University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive Stochastic Natural Gradient Method for One-Shot Neural Architecture Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>High sensitivity of neural architecture search (NAS) methods against their input such as stepsize (i.e., learning rate) and search space prevents practitioners from applying them out-of-the-box to their own problems, albeit its purpose is to automate a part of tuning process. Aiming at a fast, robust, and widely-applicable NAS, we develop a generic optimization framework for NAS. We turn a coupled optimization of connection weights and neural architecture into a differentiable optimization by means of stochastic relaxation. It accepts arbitrary search space (widely-applicable) and enables to employ a gradient-based simultaneous optimization of weights and architecture (fast). We propose a stochastic natural gradient method with an adaptive step-size mechanism built upon our theoretical investigation (robust). Despite its simplicity and no problem-dependent parameter tuning, our method exhibited near state-of-theart performances with low computational budgets both on image classification and inpainting tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Neural architecture search (NAS) is a promising way to automatically find a reasonable neural network architecture and one of the most popular research topics in deep learning. The success of deep learning impresses people from outside machine learning communities and attracts practitioners to apply deep learning to their own tasks. However, they face different difficulties when applying deep learning. One difficulty is to determine the neural architecture for their previously-unseen problem. NAS is a possible solution to this difficulty.</p><p>Work published before 2017 often frames NAS as a hyperparameter optimization, where an architecture's performance is measured by the validation error obtained after the training of the weights under a fixed architecture <ref type="bibr" target="#b17">(Real et al., 2017;</ref><ref type="bibr" target="#b19">Suganuma et al., 2017;</ref><ref type="bibr">Zoph &amp; Le, 2017)</ref>. More recent studies <ref type="bibr" target="#b3">(Brock et al., 2018;</ref><ref type="bibr" target="#b18">Shirakawa et al., 2018;</ref><ref type="bibr" target="#b16">Pham et al., 2018;</ref><ref type="bibr" target="#b9">Liu et al., 2019;</ref><ref type="bibr" target="#b22">Xie et al., 2019;</ref><ref type="bibr" target="#b4">Cai et al., 2019)</ref>, on the other hand, optimize the weights and the architecture simultaneously within a single training by treating all possible architectures as subgraphs of a supergraph. These approaches are called one-shot architecture search or one-shot NAS. They break through the bottleneck of the hyper-parameter optimization approaches, namely, high computational cost for each architecture evaluation, and enable to perform NAS on a standard personal computer, leading to gathering more potential applications.</p><p>Research directions of NAS fall into three categories <ref type="bibr" target="#b6">(Elsken et al., 2019)</ref>: performance estimation (how to estimate the performance of architectures), search space definition (how to define the possible architectures), and search strategy (how to optimize the architecture). In the last direction, promising approaches transform a coupled optimization of weights and architectures into optimization of a differentiable objective by means of continuous relaxation <ref type="bibr" target="#b9">(Liu et al., 2019;</ref><ref type="bibr" target="#b22">Xie et al., 2019)</ref> or stochastic relaxation <ref type="bibr" target="#b18">(Shirakawa et al., 2018;</ref><ref type="bibr" target="#b16">Pham et al., 2018)</ref>. A gradient descent or a natural gradient descent strategy with an existing adaptive step-size mechanism or a constant step-size is then employed to optimize weights and architecture simultaneously. However, optimization performance is sensitive against its inputs such as step-size (i.e., learning rate) and search space, limiting its application to unseen tasks.</p><p>To achieve a robust NAS, we develop a generic optimization framework for one-shot NAS. Our strategy is based on stochastic relaxation. We generalize the work by <ref type="bibr" target="#b18">Shirakawa et al. (2018)</ref> to enable arbitrary types of architecture variables including categorical variables, ordinal (such as real or integer) variables, and their mixture. We develop a unified optimization framework for our stochastic relaxation based on the so-called stochastic natural gradient <ref type="bibr" target="#b1">(Amari, 1998)</ref>. Our theoretical investigation derives a condition on the stepsize for our objective value to improve monotonically every iteration. We propose a step-size adaptation mechanism to approximately satisfy the condition. It significantly relaxes the performance sensitivity on the inputs and makes the overall framework rather flexible.</p><p>Our contributions are summarized as follows: (i) Our framework can treat virtually arbitrary types of architecture variables as long as one can define a parametric family of probability distributions on it; (ii) We propose a step-size adaptation mechanism for the stochastic natural gradient ascent, improving the optimization speed as well as its robustness against the hyper-parameter tuning. The default values are prepared for all introduced hyper-parameters, and they need not be touched even when the architecture search space changes; (iii) The proposed approach can enjoy parallel computer architecture, while it is comparable or even faster than existing approaches even on serial implementation; and (iv) Our strategy is rather simple, allowing us theoretical investigation, based on which we develop the step-size adaptation mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Our Approach</head><p>In this paper we address the following optimization problem</p><formula xml:id="formula_0">max x?X , c?C f (x, c) ,<label>(1)</label></formula><p>where f : X ? C ? R is the objective function that is differentiable with respect to (w.r.t.) x ? X and is blackbox w.r.t. c ? C. The domain X of x is a subset of n x dimensional real space R nx , whereas C can be either categorical, continuous, or their product space. Our objective is to simultaneously optimize x and c by possibly utilizing the gradient ? x f . In the context of NAS, x, c and f represent connection weights, architecture parameters, a criterion to be maximized such as negative loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Stochastic Relaxation</head><p>We turn the original optimization problem into an optimization of differentiable objective J by means of stochastic relaxation. For this purpose, we introduce a family of prob-</p><formula xml:id="formula_1">ability distributions P = {P ? : ? ? ? ? R n ? } defined on C.</formula><p>We suppose that for any c ? C the family of probability distribution contains a sequence of the distributions that approaches the Dirac-Delta distribution ? c concentrated at c. Moreover, we suppose that any P ? ? P admits the density function p ? w.r.t. the reference measure dc on C, and the log-density is differentiable w.r.t. ? ? ?. The stochastic relaxation of f given P is defined as follows</p><formula xml:id="formula_2">J(x, ?) := c?C f (x, c)p ? (c)dc = E p ? [f (x, c)] . (2)</formula><p>Maximization of J coincides with maximization of f , as</p><formula xml:id="formula_3">sup ??? J(x, ?) = sup c?C f (x, c) = f (x, c * )</formula><p>, where the supremum of J(x, ?) is attained by the limit of the sequence {?} where P ? converges to ? c * .</p><p>The stochastic relaxation J inherits nice properties of f . For example, if f (x, c) is convex and/or Lipschitz continuous w.r.t. x, then so is J(x, ?) w.r.t. x, respectively. Moreover, the stochastic relaxation J is differentiable w.r.t. both x and ? under mild conditions as follows</p><formula xml:id="formula_4">? x J(x, ?) = E p ? [? x f (x, c)] (3) ? ? J(x, ?) = E p ? [f (x, c)? ? ln(p ? (c))] .<label>(4)</label></formula><p>Stochastic Relaxation with Exponential Family: An exponential family consists of probability distributions whose density is expressed as h(c)?exp(?(?) T T (c)-?(?)), where T : C ? R n ? is the sufficient statistics, ? : ? ? R n ? is the natural parameter of this family, and ?(?) is the normalization factor. For the sake of simplicity, we limit our focus on the case h(c) = 1. If we choose the parameter ? so that ? = E p ? [T (c)], it is called the expectation parameters of this family. Under the expectation parameters, the natural gradient of the log-likelihood reduces to ? ln(p ? (c)) = T (c)?. The inverse of Fisher informa-</p><formula xml:id="formula_5">tion matrix is F -1 (?) = E[(T (c) -?)(T (c) -?) T ],</formula><p>and is typically expressed as an analytical function of ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Alternating Gradient Ascent</head><p>Let x t and ? t represent the parameter values at iteration t.</p><p>We maximize (2) by alternating optimization, namely,</p><formula xml:id="formula_6">x t+1 = argmax x?X J(x, ? t ) (5) ? t+1 = argmax ??? J(x t+1 , ?) .<label>(6)</label></formula><p>Alternating steepest ascent is a way to avoid repeatedly solving computationally heavy optimization (5) and ( <ref type="formula" target="#formula_6">6</ref>).</p><p>Stochastic Gradient Ascent on X : Update step (5) is replaced with the gradient ascent w.r.t. a metric A on X with possibly time-dependent step-size x ,</p><formula xml:id="formula_7">x t+1 = x t + x A? x J(x t , ? t ) .<label>(7)</label></formula><p>Here A may change over t, leading to (quasi-) second order update. For fixed ? t , it has been widely investigated in literature, and convergence of x to a stationary point ? x J(x, ? t ) = 0 is guaranteed under different conditions.</p><p>Monotone improvement of J is easily derived under different conditions. An example result is as follows.</p><formula xml:id="formula_8">Proposition 1. Assume that J(x, ? t ) is ? A -Lipschitz smooth w.r.t. x: ? x J(x , ? t ) -? x J(x, ? t ) A ? L x - x A . (This is satisfied if f (x, c</formula><p>) is so for all c.) Then, for</p><p>x &lt; 2/L, we have the monotone improvement</p><formula xml:id="formula_9">J(x t+1 , ? t ) -J(x t , ? t ) ? ( x -(L/2) 2 x ) ? x J(x t , ? t ) 2 A &gt; 0 . (8)</formula><p>In our situation, the gradient ? x J(x t , ? t ) is not tractable.</p><p>Instead, we estimate it by Monte-Carlo (MC) using ? x J(x t , c i ) with independent and identically distributed (i.i.d.) samples c i ? P ? t (i = 1, . . . , ? x ), namely,</p><formula xml:id="formula_10">G x (x t , ? t ) = 1 ? x ?x i=1 ? x f (x t , c i ) .<label>(9)</label></formula><p>The strong law of large numbers shows lim ?x?? G x (x t , ? t ) = ? x J(x t , ? t ) almost surely under mild conditions. The number ? x of MC samples determines the trade-off between the accuracy and the computational cost.</p><p>We replace ? x J(x t , ? t ) with G x (x t , ? t ), leading to a stochastic gradient ascent, for which adaptation mechanisms for the step-size x are developed.</p><p>Stochastic Natural Gradient Ascent on ?: Update step (6) is replaced with the natural gradient ascent with gradient normalization and step-size ? ,</p><formula xml:id="formula_11">? t+1 = ? t + ? ?? J(x t+1 , ? t ) (10) ? = ? ? / ?? J(x t+1 , ? t ) F(? t ) ,<label>(11)</label></formula><p>where ?? = F(? t ) -<ref type="foot" target="#foot_0">1</ref> ? ? . It can be approximately understood as the trust region method under the Kullback-Leibler (KL-) divergence with trust region radius ? ? .</p><p>As the natural gradient is not analytically obtained, we use MC to obtain its approximation</p><formula xml:id="formula_12">G ? (x t+1 , ? t ) = 1 ? ? ? ? i=1 f (x t+1 , c i )(T (c i ) -? t ) , (<label>12</label></formula><formula xml:id="formula_13">)</formula><p>where c i are i.i.d. from P ? t . The parameter update follows</p><formula xml:id="formula_14">? t+1 = ? t + ? G ? (x t+1 , ? t ) (13) ? = ? ? / G ? (x t+1 , ? t ) F(? t ) .<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Adaptive Stochastic Natural Gradient</head><p>In general, the step-size of a stochastic gradient algorithm plays one of the most important roles in performance and optimization time. Different adaptive step-size mechanisms have been proposed such as Adam <ref type="bibr" target="#b8">(Kingma &amp; Ba, 2015)</ref>. However, our preliminary empirical study shows a specific adaptation mechanism for ? is required to have robust performance. In the following, we first investigate the theoretical properties of the stochastic natural gradient ascent introduced above, then we introduce an adaptation mechanism for the trust-region radius ? ? .</p><p>Theoretical Background: For problems without x, i.e., fully black-box optimization of f (c), the natural gradient ascent (10) of the stochastic relaxation (2) of function f (c) is known as the information geometric optimization (IGO) <ref type="bibr" target="#b13">(Ollivier et al., 2017)</ref> algorithm. For the case of exponential family with expectation parameters, <ref type="bibr" target="#b0">Akimoto &amp; Ollivier (2013)</ref> have shown that (10) leads to a monotone increase of J(?), summarized as follows. 1</p><p>Proposition 2 (Theorem 12 of <ref type="bibr" target="#b0">(Akimoto &amp; Ollivier, 2013)</ref>).</p><p>Assume that min c?C f (c) &gt; 0. Then, (10) satisfies</p><formula xml:id="formula_15">ln J(? t + ? ?? J(? t )) -ln J(? t ) ? (( ? J(? t )) -1 -1)D ? (? t + ? ?? J(? t ), ? t ) , (<label>15</label></formula><formula xml:id="formula_16">)</formula><p>where D ? is KL-divergence on ?.</p><p>Proposition 2 gives us a very useful insight into the stepsize ? . It says that ? &lt; 1/J(? t ) leads to improvement in J value as long as the parameter follows the exact natural gradient. Together with Proposition 1, it implies the monotone improvement of alternating update of x and ? when the exact gradients are given. However, in our situation, the natural gradient in ( <ref type="formula">10</ref>) is not tractable and one needs to approximate it with MC. Then, the monotone improvement is not guaranteed. A promising feature of our framework is that the MC approximate G ? (x t+1 , ? t ) of the natural gradient ?? J(? t ) can be made arbitrarily accurate by taking the number of MC samples ? ? to ?.</p><p>The following proposition shows that the loss in J is bounded if the divergence is bounded. Its proof can be found in supplementary material.</p><p>Proposition 3. Assume that min c?C f (c) &gt; 0 and let f * = max c?C f (c). Then, ln J(? )ln J(?) ? -f * J(?) D ? (? , ?) for any ? and ? .</p><p>As a straight-forward consequence of the above two propositions, we obtain a sufficient conditions for the stochastic natural gradient ascent to improve J monotonically. This is the baseline of our proposal.</p><formula xml:id="formula_17">Theorem 4. Assume that min c?C f (c) &gt; 0 and let f * = max c?C f (c). For any &gt; 0, if D ? (?, ? t + ?? J(? t )) ? ?D ? (? t + ?? J(? t ), ? t ) holds for some ? &gt; 0, we have ln J(?) -ln J(? t ) ? 1 -? f * -J(? t ) J(? t ) D ? (? t + ?? J(? t ), ? t ) . (<label>16</label></formula><formula xml:id="formula_18">)</formula><p>In particular, if &lt; (?f * + J(? t )) -1 holds, J(?) &gt; J(? t ).</p><p>If we replace ? with ? t+1 defined in (13), we obtain a sufficient condition for the stochastic natural gradient update (13) to lead to monotone improvement, namely,</p><formula xml:id="formula_19">D ? (? t+1 , ? t + ? ?? J(x t+1 , ? t )) ? ?D ? (? t + ? ?? J(x t+1 , ? t ), ? t ) . (17)</formula><p>This can be satisfied for any ? &gt; 0 by taking a sufficiently large ? ? as G ? (x t+1 , ? t ) is a consistent estimator of ?? J(x t+1 , ? t ) and the left hand side (LHS) is O(? -1 ? ). However, if ? (or ? ? ) is sufficiently small, monotone improvement at each iteration is too strict and one might only need to guarantee the improvement over ? &gt; 0 iterations, where ? ? 1/? ? . To derive an insightful formula, we put aside the mathematical rigor in the following. Let ?t ? ? = G ? (x t+1 , ? t ) for short. We continue to consider a problem without x (or x is fixed). A common argument borrowed from stochastic approximation (e.g., <ref type="bibr" target="#b2">Borkar (2008)</ref>) states that if ? is so small that the parameter vector stays near ? t and ?t+i ? ? are considered i.i.d. for i = 0, . . . , ? -1, the parameter vector after ? steps will be approximated as</p><formula xml:id="formula_20">? t+? -? t ? ? ? E[ ?t ? ? ] + ? ? ? -1 i=0 1 ? ( ?t+i ? ? -E[ ?t ? ? ]) .</formula><p>If we replace ? t+1 with ? t+? and with ? ? in ( <ref type="formula">17</ref>) and apply the approximation of the KL-divergence by the Fisher information matrix, we obtain</p><formula xml:id="formula_21">? -1 i=0 ?t+i ? ? -E[ ?t ? ? ] ? ? 2 F(? t ) ?Tr(Cov( ?t ? ? )F(? t )) as ? ?? ? ?? E[ ?t ? ? ] 2 F(? t ) ,</formula><p>The LHS tends to the variance of ?t ? ? measured w.r.t. the Fisher metric and is upper bounded by (f * ) 2 n ? /? ? . That is, ? ? and/or ? ? should be adapted so that</p><formula xml:id="formula_22">E[ ?t ? ? ] 2 F(? t ) Tr(Cov( ?t ? ? )F(? t )) ? 1 ?? ? ?(? ? ) .<label>(18)</label></formula><p>In words, the signal-to-noise ratio (LHS of ( <ref type="formula" target="#formula_22">18</ref>)) must be greater than a constant proportional to ? ? .</p><p>Adaptive Stochastic Natural Gradient: We develop an algorithm that approximately satisfies the above-mentioned condition by adapting the trust region ? ? . The above condition can be satisfied by increasing ? ? while ? ? is fixed, and the same idea as described below can be used to adapt the number ? ? of MC samples. The reason we adapt ? ? rather than ? ? is to update connection weights x more frequently (x is updated after every ? ? forward network processes). If multiple GPUs are available, one can set ? ? = ? x = #GPUs and enjoy parallel computation, allowing to keep x and ? ? (hence ? as well) higher as the stochastic gradient becomes more reliable.</p><p>Algorithm 1 ASNG-NAS</p><formula xml:id="formula_23">Require: x 0 , ? 0 {initial search points} Require: ? = 1.5, ? 0 ? = 1, ? x = ? ? = 2 1: ? = 1, ? = 0, s = 0, t = 0 2: repeat 3: ? ? = ? 0 ? /?, ? = ? ? /n 1/2 ? 4:</formula><p>compute G x (x t , ? t ) by ( <ref type="formula" target="#formula_10">9</ref>) and update x t+1 using G x (x t , ? t ) 5:</p><p>compute G ? (x t+1 , ? t ) by ( <ref type="formula" target="#formula_12">12</ref>), update ? t+1 with (13), then force ? t+1 ? ? by projection 6:</p><formula xml:id="formula_24">s ? (1 -?)s + ?(2 -?) F(? t ) 1 2 G ? (x t+1 ,? t ) G ? (x t+1 ,? t ) F(? t ) 7: ? ? (1 -?) 2 ? + ?(2 -?) 8: ? ? min(? max , ? exp(?(? -s 2 /?))) 9: until termination conditions are met</formula><p>We introduce the accumulation of the stochastic natural gradient as follows</p><formula xml:id="formula_25">s (t+1) = (1 -?)s (t) + ?(2 -?)F(? t ) 1 2 ?t ? ? , (19) ? (t+1) = (1 -?) 2 ? (t) + ?(2 -?) ?t ? ? 2 F(? t ) , (20)</formula><p>where s (0) = 0 and ? (0) = 0. To understand the effect of s and ?, we consider the situation that x and ? ? are small enough that x t and ? t stay at (x, ?). Then,</p><formula xml:id="formula_26">s t approaches (2 -?)/?E[F(? t ) 1 2 ?? ? ]+?, where ? is a random vector with E[?] = 0 and Cov(?) = F(? t ) 1 2 Cov( ?? ? )F(? t ) 1 2 , and ? t approximates E[ ?? ? 2 F(? t ) ] = E[ ?? ? ] 2 F(? t ) + Tr(Cov( ?t ? ? )F(? t )).</formula><p>If we set ? ? ? ? and adapt ? ? or ? ? to keep s (t+1) 2 /? (t+1) ? ? for some ? &gt; 1, it approximately achieves</p><formula xml:id="formula_27">E[ ?? ? ] 2 F(? t ) Tr(Cov( ?t ? ? )F(? t )) ? E[ ?? ? ] 2 F(? t ) E[ ?? ? 2 F(? t ) ] ? ? 2 -2? s (t+1) 2 ? (t+1) -1 ? ?(? -1) 2 -2? ? ?(? ? ) .</formula><p>It results in satisfying (18).</p><p>The adaptation of ? ? is then done as follows:</p><formula xml:id="formula_28">? ? ? ? ? exp ? s (t+1) 2 /? -? (t+1) . (<label>21</label></formula><formula xml:id="formula_29">)</formula><p>This tries to keep s (t+1) 2 /? (t+1) ? ? by adapting ? ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Adaptive Stochastic Natural Gradient-based NAS</head><p>The proposed optimization method for problem (1), called Adaptive Stochastic Natural Gradient-based NAS (ASNG-NAS), is summarized in Algorithm 1. Here, we summarize some implementation remarks. One is that instead of accumulating F(? t ) 1 2 ?t ? ? and ?t</p><formula xml:id="formula_30">? ? 2 F(? t ) separately in (19)</formula><p>and ( <ref type="formula">20</ref>), we accumulate F(? t ) 1 2 ?t ? ? / ?t ? ? F(? t ) in s and ? ? 1. In our preliminary experiments, we found it more stable. The other point is that the average function value is subtracted from the function value in the stochastic natural gradient computation (12) when ? ? = 2. This is a well-known technique to reduce the estimation variance of gradient while the expectation is unchanged (e.g., <ref type="bibr" target="#b7">Evans &amp; Swartz (2000)</ref>). Since we normalize the stochastic natural gradient when the parameter is updated, it is equivalent to transform <ref type="table"></ref>and<ref type="table">(0, 0</ref></p><formula xml:id="formula_31">f 1 = f (x t+1 , c 1 ) and f 2 = f (x t+1 , c 2 ) to (1, -1) if f 1 &gt; f 2 , (-1, 1) if f 1 &lt; f 2 ,</formula><formula xml:id="formula_32">) if f 1 = f 2</formula><p>(in this case, we skip the update and start the next iteration). When ? ? &gt; 2, we similarly transform</p><formula xml:id="formula_33">f i = f (x t+1 , c i ) in (12) to 1 if f i is in top ? ? /4 , -1 if it is in bottom ? ? /4</formula><p>, and 0 otherwise. By doing so, we obtain invariance to a strictly increasing transformation of f , and we observed significant speedup in many cases in our preliminary study.</p><p>To instantiate ASNG-NAS, we prepare an exponential family defined on</p><formula xml:id="formula_34">C. If C is a set of categorical vari- ables (C = 1, m 1 ? ? ? ? ? 1, m nc ), one can simply use categorical distribution parameterized by the proba- bility [?] i,j = [? i ] j of i-th categorical variable to be j (1 - mi-1 j=1 [?] i,j is the probability of [c] i = m i ). Then, T (c) = (T 1 ([c] 1 ), . . . , T nc ([c] nc )), where T i : 1, m i ? [0, 1] mi-1</formula><p>is the one-hot representation without the last element, and</p><formula xml:id="formula_35">F(?) = diag(F 1 (? 1 ), . . . , F nc (? nc )), where F i (? i ) = diag(? i ) -1 + (1 - mi-1 j=1 [? i ] j ) -1 11 T . If C</formula><p>is a set of ordinal variables, e.g., C ? R nc , our choice will be</p><formula xml:id="formula_36">P ? = N (? 1 , ? 2 1 ) ? ? ? ? ? N (? nc , ? 2 nc ) and ? = (? 1 , ? 2 1 + ? 2 1 , . . . , ? nc , ? 2 nc + ? 2 nc ). Then, we have T (c) = (T 1 ([c] 1 ), . . . , T nc ([c] nc )) with T i ([c] i ) = ([c] i , [c] 2 i ),<label>and</label></formula><formula xml:id="formula_37">F(?) is a block-diagonal matrix with block size 2 whose i-th block is [? 2 i , 2? i ? 2 i ; 2? i ? 2 i , 4? 2 i ? 2 i + 2? 4 i ] -1</formula><p>. Integer variables can be treated similarly. If C is a product of categorical and ordinal variable spaces, we can use their product distribution. A desired ? 0 realizes the maximal entropy in ? unless one has a prior knowledge. Moreover, ? should be restricted to avoid degenerated distribution. E.g., for categorical distribution, we lower bounds [?] i,j by ? min i = (n c (m i -1)) -1 . See the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments and Results</head><p>We investigate the robustness of ASNG on an artificial test function in ?3.1. We then apply ASNG-NAS to the architecture search for image classification and inpainting in ?3.2 and ?3.3. To compare the quality of the obtained architecture and the computational cost, we adopt the same search spaces as in previous works. The experiments were done with a single NVIDIA GTX 1080Ti GPU, and ASNG-NAS is implemented using PyTorch </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Toy Problem</head><p>We consider the following selective squared error function composed of continuous variable x ? R D?K and categorical variables c. For each i (1 ? i ? D), we denote the one-hot vector of i-th categorical variable in c by h i (c) ? {0, 1} K . The objective function to be minimized is</p><formula xml:id="formula_38">f (x, c) = E z ? ? D i=1 K j=1 h ij (c) (x ij -z i ) 2 + j -1 K ? ? ,</formula><p>where the underlying distribution of z is N (0, K -2 I). This function switches the active variables in x by the categorical variables c. The global optima locate at h i (c) = [1, 0, . . . , 0] for i = 1, . . . , D, x 1 = [0, . . . , 0], and arbitrary variables of x i for i = 2, . . . , D. To mimic NN training, we approximate the expectation by using a sample z, which is drawn from N (0, K -2 I) every parameter update.</p><p>We use the stochastic gradient descent (SGD) with a momentum of 0.9 to optimize x and anneal the step-size x by cosine scheduling <ref type="bibr" target="#b11">(Loshchilov &amp; Hutter, 2017)</ref>, which we also use for the latter experiments. We initialize x ? N (0, I) and the distribution parameter ? = (1/K)1. We regard it successfully optimized if a solution with the actual objective value less than K -1 + DK -2 is sampled before 10 5 iterations. We report the number of iterations to sample the target value divided by the success rate over 100 runs as the performance measure. We have tested different combinations of D and K and observed similar results. We report the results for D = 30 and K = 5 as a typical one. Figure <ref type="figure" target="#fig_0">1</ref> compares ASNG, SNG (stochastic natural gradient with constant step-size), and Adam <ref type="bibr" target="#b8">(Kingma &amp; Ba, 2015)</ref>-a standard step-size adaptation for DNNs-with different initial step-size. We replace the gradient in Adam with the normalized natural gradient as it is used in ASNG since we found in our preliminary studies that Adam does not work properly with its default. For SNG and Adam one needs to fine tune the step-size, otherwise they fail to locate the optimum. On the other hand, ASNG relaxes the sensitivity against ? 0 ? . The robustness of ASNG on the choice of ? is evaluated in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Architecture Search for Image Classification</head><p>Dataset: We use the CIFAR-10 dataset and adopt the standard preprocessing and data augmentation as done in the previous works, e.g., <ref type="bibr" target="#b9">Liu et al. (2019)</ref>; <ref type="bibr" target="#b16">Pham et al. (2018)</ref>. During the architecture search, we split the training dataset into halves as D = {D x , D ? } as done in <ref type="bibr" target="#b9">Liu et al. (2019)</ref>. The gradients ( <ref type="formula" target="#formula_10">9</ref>) and ( <ref type="formula" target="#formula_12">12</ref>) are calculated using mini-batches from D x and D ? , respectively. We use the same mini-batch samples among the different architecture parameters in ( <ref type="formula" target="#formula_10">9</ref>) and ( <ref type="formula" target="#formula_12">12</ref>) to get accurate gradients. Note that we do not need the back-propagation for calculating (12). Namely, the computational cost of the ? update is less than that of x.</p><p>Search Space: The search space is based on the one in <ref type="bibr" target="#b16">Pham et al. (2018)</ref>, which consists of models obtained by connecting two motifs (called normal cell and reduction cell) repeatedly. Each cell consists of B (= 5) nodes and receives the outputs of the previous two cells as inputs. Each node receives two inputs from previous nodes, applies an operation to each of the inputs, and adds them. Our search space includes 5 operations: identity, 3 ? 3 and 5 ? 5 separable convolutions <ref type="bibr" target="#b5">(Chollet, 2017)</ref>, and 3 ? 3 average and max poolings. The separable convolutions are applied twice in the order of ReLU-Conv-BatchNorm. We select a node by 4 categorical variables representing 2 outputs of the previous nodes and 2 operations applied to them. Consequently, we treat 4B-dimensional categorical variables for each cell. After deciding B nodes, all of the unused outputs of the nodes are concatenated as the output of the cell. The number of the categorical variables is n c = 40, and the dimension of ? becomes n ? = 140.</p><p>Training Details: In the architecture search phase, we optimize x and ? for 100 epochs (about 40K iterations) with a mini-batch size of 64. We stack 2 normal cells (N = 2) and set the number of channels at the first cell to 16. We use SGD with a momentum of 0.9 to optimize weights x. The step-size x changes from 0.025 to 0 following the cosine schedule <ref type="bibr" target="#b11">(Loshchilov &amp; Hutter, 2017)</ref>. After the architecture search phase, we retrain the network with the most likely architecture, ? = argmax c p ? (c), from scratch, which is a commonly used technique <ref type="bibr" target="#b3">(Brock et al., 2018;</ref><ref type="bibr" target="#b9">Liu et al., 2019;</ref><ref type="bibr" target="#b16">Pham et al., 2018)</ref> to improve final performance. In the retraining stage, we can exclude the redundant (unused) weights. Then, we optimize x for 600 epochs with a mini-batch size of 80. We stack 6 normal cells (N = 6) and increase the number of channels at the first cell so that the model has the nearly equal number of weight parameters to 4 million. We report the average (avg.) and standard deviation (std.) among 3 independent experiments.</p><p>Result and Discussion: Table <ref type="table" target="#tab_0">1</ref> compares the search cost and the test error of different NAS methods. The bottom 5 methods adopt similar search spaces, hence showing the performance differences due to search algorithms. The avg. and std. of ASNG-NAS are those of architecture search + retraining (whole NAS process), whereas the values for the other methods are taken from the references and have different meanings. E.g., the values for DARTS and SNAS are the avg. and std. of 10 independent retraining of the best found architecture among 4 NAS processes.</p><p>We clearly see the trade-off between search cost and final performance. The more accurate the performance estimation of neural architecture is (as in NASNet and NAONet), the better final performance is obtained at the risk of speed. Among relatively fast NAS methods (ENAS, DARTS, SNAS, and ASNG-NAS), ASNG-NAS is the fastest and achieves a competitive error rate. The reason of speed difference between these algorithms is discussed in ?4. We observed that the probability vector ? of the categorical distribution converges to a certain category. More precisely, the average value of the max j [?] i,j reaches around 0.9 at the 50th epoch. The architecture of the best model obtained by ASNG-NAS is found in supplementary material.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> compares the test error w.r.t. elapsed time in the architecture search phase. The test accuracy of the most likely architecture ? is plotted for ASNG-NAS. DARTS (mix) and DARTS (fix) are the architectures obtained by the same run of DARTS. The former is the one mixing all possible operations with real-valued structure parameters and is the one optimized during the architecture search, whereas the latter is the one that takes the operations with the highest weights and is the one used after the architecture search. We see that DARTS (fix) do not improve the test accuracy during the architecture search phase and the retraining is a must. DARTS (mix) achieves better performance than ASNG-NAS in the end, but the obtained architecture is not one-hot and is computationally expensive. ASNG-NAS shows the best performance for small time budgets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Architecture Search for Inpainting</head><p>Dataset: We use the CelebFaces Attributes Dataset (CelebA) <ref type="bibr" target="#b10">(Liu et al., 2015)</ref>. The data preprocessing and augmentation method is the same as <ref type="bibr" target="#b20">Suganuma et al. (2018)</ref>. We use three different masks to generate images with missing regions; a central square block mask (Center); a random pixel mask where 80% of the pixels were randomly masked (Pixel), and a half image mask where either the vertical or horizontal half of the image is randomly selected (Half). Following <ref type="bibr" target="#b20">Suganuma et al. (2018)</ref>, we use two standard eval-  uation measures: the peak-signal to noise ratio (PSNR) and the structural similarity index (SSIM) <ref type="bibr" target="#b21">(Wang et al., 2004)</ref> to evaluate the restored images. Higher values of these measures indicate a better image restoration.</p><p>Search Space: The search space we use is based on <ref type="bibr" target="#b20">Suganuma et al. (2018)</ref> for comparison. The architecture encoding is slightly different but it can represent the exact same network architectures. We employ the symmetric convolutional autoencoder (CAE) as a base architecture. A skip connection between the convolutional layer and the mirrored deconvolution layer can exist. We prepare six types of layers: the combination of the kernel sizes {1 ? 1, 3 ? 3, 5 ? 5} and the existence of the skip connection. The layers with different settings do not share weight parameters.</p><p>We implement two ASNG-NAS algorithms with only categorical variables (ASNG-NAS (Cat)) and with mixed categorical and ordinal (integer) variables (ASNG-NAS (Int)) to demonstrate the flexibility of the proposed approach. The former encodes the layer type, channel size, and connections for each hidden layer, and the connection for the output layer using categorical variables. We select the output channel size of each of 20 hidden layers from {64, 128, 256}. The latter encodes the kernel size and the channel size by integers in 1, 3 (corresponding to {1 ? 1, 3 ? 3, 5 ? 5}) and 64, 256 . We employ the Gaussian distribution as described in ?2.4. Sampled variables are clipped to <ref type="bibr">[1, 3] and [64, 256]</ref> and rounded to integers (only for architecture evaluation). The dimension of ? amounts to n ? = 214 for ASNG-NAS (Cat) and n ? = 174 for ASNG-NAS (Int).</p><p>Training Details: We use the mean squared error (MSE) as the loss function and a mini-batch size of 16. In the architecture search phase, we use SGD with momentum with the same setting in ?3.2, while we use Adam in the retraining phase. We apply gradient clipping with the norm of 5 to prevent a too long gradient step. The maximum numbers of iterations are 50K and 500K in the architecture search and retraining phases, respectively. The setting of the retraining is the same as in <ref type="bibr" target="#b20">Suganuma et al. (2018)</ref>. Differently from the previous experiment, we retrain the obtained architecture without any change in this experiment.</p><p>Result and Discussion: Table <ref type="table" target="#tab_1">2</ref> shows the comparison of PSNR and SSIM. The performances of ASNG-NAS are better than CE, SII, and BASE on all mask types and comparable to E-CAE. <ref type="bibr" target="#b20">Suganuma et al. (2018)</ref> reported that E-CAE spent approximately 12 GPU days (3 days with 4 GPUs) for the architecture search and retraining. On the other hand, the average computational times of ASNG-NAS were less than 1 GPU days. ASNG-NAS (Cat) took approximately 6 hours for the architecture search and 14 hours for the retraining on average, whereas the average retraining time of ASNG-NAS (Int) was reduced to 11 hours. This is because the architectures obtained by ASNG-NAS (Int) tended to have a small number of channels compared to ASNG-NAS (Cat) that selects from the predefined three channel sizes.</p><p>In conclusion, ASNG-NAS achieved practically significant speedup over E-CAE without compromising the final performance. The flexibility of ASNG-NAS has been shown as well. The capability of ASNG-NAS to treat mixed categorical and ordinal variables potentially decreases the number of the architecture parameters (good for speed) and enlarges the search space (good for performance).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work and Discussion</head><p>ASNG-NAS falls into one-shot NAS. On this line of the research, three different existing approaches are reported. The 1st category is based on a meta-network. SMASH <ref type="bibr" target="#b3">(Brock et al., 2018)</ref> employs HyperNet that takes an architecture  <ref type="bibr" target="#b15">(Pathak et al., 2016)</ref> and the semantic image inpainting <ref type="bibr">(Yeh et al., 2017)</ref>, which are the human-designed CNN. E-CAE refers to the model obtained by the architecture search method using the evolutionary algorithm <ref type="bibr" target="#b20">(Suganuma et al., 2018)</ref>. BASE is the same depth of the best architecture obtained by E-CAE but having 64 channels and 3 ? 3 filters in each layer, along with a skip connection. The 2nd category is based on continuous relaxation. DARTS <ref type="bibr" target="#b9">(Liu et al., 2019)</ref> extends essentially categorical architecture parameters (selection of operations and connections) to a real-valued vector by considering a linear combination of outputs of all possible operations. This enables gradient descent both on the connection weights and the weights for the linear combination. This seminal work is followed by further improvements <ref type="bibr" target="#b22">(Xie et al., 2019;</ref><ref type="bibr" target="#b4">Cai et al., 2019)</ref>. An advantage of ASNG-NAS is that DARTS requires to compute all possible operations and connections to perform backprop, whereas we only require to process sub-networks with sampled architectures c, hence ASNG-NAS is faster. This advantage is reflected in Table <ref type="table" target="#tab_0">1</ref>. Another advantage is its flexibility in the sense that the continuous relaxation of DARTS requires the output of all possible operations to live in the same domain to add them.</p><p>The last category is based on stochastic relaxation, which is another approach enabling to use gradient descent. <ref type="bibr" target="#b18">Shirakawa et al. (2018)</ref> has introduced it to model connections and types of activation functions in multi-layer perceptrons. They are encoded by a binary vector c and Bernoulli distribution is considered as the underlying distribution of c. The probability parameters of Bernoulli distribution is updated by SNG. We improve their work in the following directions: generalization to arbitrary architecture parameters (categorical, ordinal, or their mixture), theoretical investigation of monotone improvement, robustness against its input parameter by introducing a step-size adaptation mechanism.</p><p>This paper focused on the optimization framework for NAS.</p><p>One can easily incorporate a different search space and a different performance estimation method into our framework. The step-size adaptation mechanism eases hyper-parameter tuning when different components are introduced. The ability to treat ordinal variables such as the number and size of filters and the number of layers accepts more flexible search space. In existing studies they are modeled by categorical variables by choosing a few representative numbers beforehand. Moreover, the ordinal variables potentially decreases the dimension of architecture parameters. When multiple GPUs are available, ASNG-NAS can easily enjoy them by increasing ? x and ? c and distributing them. In our preliminary study, we found that the larger they are, the greater step-size are allowed and the step-size adaptation automatically increases it. Our simple formulation allows theoretical investigation, which we think is missing in the current NAS research fields. Further theoretical investigation will contribute better understanding and further improvement of NAS.</p><p>One-shot NAS including our method does not optimize parameters involved in learning process such as the step-size for weight update. It is because their effects do not appear in the one-shot loss and will not be optimized effectively. If we employ hyper-parameter optimizers such as Bayesian optimization to optimize these parameters while each training process is replaced by our method, both architectures and other hyper-parameters could be optimized. The fast and robust properties of our method will be useful to combine one-shot NAS and hyper-parameter optimizer. This is an important direction towards automation of deep learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Results on the selective squared error function for x 0.05 and 0.0005. Median values over 100 runs are reported. Missing values indicate the setting never succeeded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Transitions of test error against elapsed time in the architecture search phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of different architecture search methods on CIFAR-10. The search cost indicates GPU days for architecture search excluding the retraining cost.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell></row><row><cell>Method NASNet-A (Zoph et al., 2018) NAONet (Luo et al., 2018)</cell><cell cols="2">Search Cost Params (GPU days) (M) 1800 3.3 200 128</cell><cell>Test Error (%) 2.65 2.11</cell><cell>test accuracy</cell><cell>40 60</cell><cell>ASNG-NAS ENAS</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DARTS (mix)</cell></row><row><cell>ProxylessNAS-G (Cai et al., 2019)</cell><cell>4</cell><cell>5.7</cell><cell>2.08</cell><cell></cell><cell>20</cell><cell>DARTS (fix)</cell></row><row><cell>SMASHv2 (Brock et al., 2018)</cell><cell>1.5</cell><cell>16.0</cell><cell>4.03</cell><cell></cell><cell></cell></row><row><cell>DARTS second order (Liu et al., 2019) DARTS first order (Liu et al., 2019)</cell><cell>4 1.5</cell><cell>3.3 3.3</cell><cell>2.76 (?0.09) 3.00 (?0.14)</cell><cell></cell><cell>0</cell><cell>10 elapsed time (hour) 20</cell></row><row><cell>SNAS (Xie et al., 2019)</cell><cell>1.5</cell><cell>2.8</cell><cell>2.85 (?0.02)</cell><cell></cell><cell></cell></row><row><cell>ENAS (Pham et al., 2018)</cell><cell>0.45</cell><cell>4.6</cell><cell>2.89</cell><cell></cell><cell></cell></row><row><cell>ASNG-NAS</cell><cell>0.11</cell><cell>3.9</cell><cell>2.83 (?0.14)</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on the inpainting tasks. CE and SII indicate the context encoder</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>ASNG-NAS (Cat)   encodes all architecture parameters into categorical variables, whereas ASNG-NAS (Int) encodes the kernel and channel sizes into integer variables. The values of CE, SII, BASE, and E-CAE are referenced from<ref type="bibr" target="#b20">Suganuma et al. (2018)</ref>. The weights of HyperNet is then optimized by backprop while c is randomly chosen during architecture search. ENAS<ref type="bibr" target="#b16">(Pham et al., 2018)</ref> employs a recurrent neural network (RNN) to generate a sequence of categorical variables c representing neural architecture. It optimizes the weights and the RNN weights alternatively. ENAS and ASNG-NAS are different in that the latter directly introduces a probability distribution behind c while the former employ RNN. The advantage of ASNG-NAS over metanetwork based approaches is that we do not need to design the architecture of a meta-network, which may be a tedious task for practitioners.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">PSNR [dB] / SSIM</cell></row><row><cell>Mask</cell><cell>CE</cell><cell>SII</cell><cell>BASE</cell><cell cols="2">E-CAE (12 GPU days) (0.84 GPU days) ASNG-NAS (Cat) ASNG-NAS (Int) (0.75 GPU days)</cell></row><row><cell cols="5">Center 28.5 / 0.912 19.4 / 0.907 27.1 / 0.883 29.9 / 0.934</cell><cell>29.2 / 0.903</cell><cell>29.3 / 0.911</cell></row><row><cell>Pixel</cell><cell cols="3">22.9 / 0.730 22.8 / 0.710 27.5 / 0.836</cell><cell>27.8 / 0.887</cell><cell>28.4 / 0.905</cell><cell>28.6 / 0.909</cell></row><row><cell>Half</cell><cell cols="3">19.9 / 0.747 13.7 / 0.582 11.8 / 0.604</cell><cell>21.1 / 0.771</cell><cell>20.5 / 0.779</cell><cell>20.6 / 0.779</cell></row><row><cell cols="4">c as its input and returns the weights for the network with</cell><cell></cell></row><row><cell>architecture c.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The statement is simplified so as not to introduce additional notation. Note that if f (c) is lower bounded, considering f (c) -minc?C f (c) in (2) instead of f is sufficient to meet the condition of Proposition</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>2. This modification only adds an offset to the J value without affecting the gradient.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work is partially supported by the <rs type="funder">SECOM Science and Technology Foundation</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Objective improvement in information-geometric optimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Akimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ollivier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOGA XII &apos;13: Proceedings of the twelfth workshop on Foundations of genetic algorithms</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013-01">jan 2013</date>
			<biblScope unit="volume">XII</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Natural Gradient Works Efficiently in Learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="276" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Stochastic approximation: a dynamical systems viewpoint</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Borkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SMASH: One-Shot Model Architecture Search through HyperNetworks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Direct Neural Architecture Search on Target Task and Hardware</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><surname>Proxylessnas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Xception: Deep Learning with Depthwise Separable Convolutions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural Architecture Search: A Survey</title>
		<author>
			<persName><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">55</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Approximating integrals via Monte Carlo and deterministic methods</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Swartz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable Architecture Search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Learning Face Attributes in the Wild</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic Gradient Descent with Warm Restarts</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural Architecture Optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7827" to="7838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Information-Geometric Optimization Algorithms: A Unifying Picture via Invariance Principles</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ollivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Auger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="564" to="628" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Autodiff Workshop in Thirty-first Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Context Encoders: Feature Learning by Inpainting</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient Neural Architecture Search via Parameter Sharing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 35th International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4095" to="4104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large-Scale Evolution of Image Classifiers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 34th International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2902" to="2911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic Optimization of Neural Network Structures Using Probabilistic Modeling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shirakawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Iwata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Akimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4074" to="4082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Genetic Programming Approach to Designing Convolutional Neural Network Architectures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Suganuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shirakawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nagao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Genetic and Evolutionary Computation Conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="497" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploiting the Potential of Standard Convolutional Autoencoders for Image Restoration by Evolutionary Search</title>
		<author>
			<persName><forename type="first">M</forename><surname>Suganuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ozay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 35th International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4778" to="4787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image Quality Assessment: From Error Visibility to Structural Similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SNAS: Stochastic Neural Architecture Search</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Semantic Image Inpainting</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
