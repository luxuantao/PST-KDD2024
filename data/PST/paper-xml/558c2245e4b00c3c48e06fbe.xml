<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Autonomous MAV Flight in Indoor Environments using Single Image Perspective Cues</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Cooper</forename><surname>Bills</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Joyce</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
						</author>
						<title level="a" type="main">Autonomous MAV Flight in Indoor Environments using Single Image Perspective Cues</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2E583219F6FF8077A2ECB3E87C71451E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the problem of autonomously flying Miniature Aerial Vehicles (MAVs) in indoor environments such as home and office buildings. The primary long range sensor in these MAVs is a miniature camera. While previous approaches first try to build a 3D model in order to do planning and control, our method neither attempts to build nor requires a 3D model. Instead, our method first classifies the type of indoor environment the MAV is in, and then uses vision algorithms based on perspective cues to estimate the desired direction to fly. We test our method on two MAV platforms: a co-axial miniature helicopter and a toy quadrotor. Our experiments show that our vision algorithms are quite reliable, and they enable our MAVs to fly in a variety of corridors and staircases.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>We consider flying a Miniature Aerial Vehicle (MAV) such as a quadrotor equipped with a camera (Figure <ref type="figure" target="#fig_0">1</ref>) in indoor environments such as corridors and stairs (Figure <ref type="figure" target="#fig_1">2</ref>).</p><p>Miniature aerial vehicles (MAVs) that can fly autonomously in unstructured indoor GPS-denied environments such as homes and offices would be useful for exploration, rescue, surveillance and entertainment. These aerial vehicles need to be small and lightweight, so a passive, low-power long-range sensor is more suitable than power-hungry sensors such as LIDAR <ref type="bibr" target="#b0">[1]</ref>. Our solution is to use a small camera as the primary long-range sensor.</p><p>However, the 2D visual information from a camera is difficult to use for navigation in a 3D world. Some algorithms (such as visual SLAM <ref type="bibr" target="#b1">[2]</ref>) attempt to infer 3D structure from multiple images. This is challenging because reliable odometry is not available for aerial robots and building a 3D model of the environment takes considerable computation power <ref type="bibr" target="#b2">[3]</ref>. Furthermore, the constructed 3D model often has holes for textureless surfaces such as walls. In our work, we instead rely on perspective cues from a single image, which require very little computational power and can be used by a controller directly without building a 3D model.</p><p>In the past, vision-based algorithms have been shown to fly in simple environments such as corridors <ref type="bibr" target="#b3">[4]</ref>. The ability to travel between different floors is a big advantage for aerial robots over ground-based robots. To the best of our knowledge, our work is the first one that considers flying in previously unseen staircase environments using vision.</p><p>We can categorize almost all indoor environments as belonging to one of the following categories: long-narrow spaces (e.g. corridors), areas with non-uniform height (e.g. stairs), open spaces (e.g. atriums), small enclosed spaces (e.g.</p><p>Cooper Bills, Joyce Chen and Ashutosh Saxena are with the Department of Computer Science, Cornell University, USA. csb88@cornell.edu,{yuhsin,asaxena}@cs.cornell.edu  offices). For each of these environments, there are certain perspective cues that give us information about the 3D structure, and hence depths in different directions. Previously, such single image cues <ref type="bibr" target="#b4">[5]</ref> have been successfully used in obstacle avoidance for ground robots <ref type="bibr" target="#b5">[6]</ref>. In our work, we first classify the type of environment the MAV is in, and for each case we compute the perspective cues. These cues are then used to navigate the MAV within the environment.</p><p>We test our approach in several buildings, including areas never seen before by our algorithms. The testing environments included several corridors and staircases, which often involved turns; for example, a turn may connect two corridors or two flights of stairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Previous work on aerial robots can be arranged as follows: non-vision vs. vision sensors, with a focus on stabilization or navigation, in both outdoor and indoor environments. In indoor environments, the perception and navigation problem is more important because they are GPS denied and more constrained; while in outdoor environments, the focus has been on designing control algorithms <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-vision sensors:</head><p>There is extensive work on autonomous quad-rotor navigation using active sensors such as laser range scanners, sonar, and infra-red. Roberts et al. <ref type="bibr" target="#b10">[11]</ref> used infrared and ultrasonic sensors to fly a quad-rotor indoor in a large (7 Ã— 6m) room, but it cannot do long-range sensing. Achtelik et al. <ref type="bibr" target="#b0">[1]</ref> used a laser rangefinder and a stereo camera for quadrotor navigation, with the eventual goal of combining the two complementary sensors. Custom-built quadrotors are used in order to support heavy sensors or additional battery weight necessary to support active, power-hungry sensors; however, most miniature MAVs can carry only a light-weight and low-power sensor such as a camera. Vision sensors: Recently, there is an interest in building MAVs that use vision to fly in common indoor environments, particularly because vision offers long-range sensing with low power and less weight, allowing smaller aerial robots to be built. Nicoud et al. <ref type="bibr" target="#b11">[12]</ref> discussed the tradeoffs of designing indoor helicopters while Schafroth et al. <ref type="bibr" target="#b12">[13]</ref> designed various test benches for micro helicopters and designed a dual-rotor single-axis helicopter with an omnidirectional camera. Mejias et al. <ref type="bibr" target="#b13">[14]</ref> used vision to land a helicopter while avoiding power lines. Zingg et al. <ref type="bibr" target="#b3">[4]</ref> presented optical flow based algorithms for navigating an MAV in corridors. Stabilization and Control: Research that used purely vision-based algorithms for stabilization and pose estimation of quad-rotors include using specialized cameras that can be programmed to refocus at certain depths or heavier stereo cameras. E.g., Moore et al. <ref type="bibr" target="#b14">[15]</ref> used stereo cameras for autonomous flight. Johnson <ref type="bibr" target="#b15">[16]</ref> used vision to make a quadrotor hover stably indoors. Among the most recent work on UAV flight, work involving vision-based stabilization and pose estimation of the aerial vehicle include <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, but these works consider only stabilization, not navigation. Navigation: Even when simple lightweight cameras are used, work on navigation often depends on known patterns or environments. For example, Tournier et al. <ref type="bibr" target="#b19">[20]</ref> used Moire patterns pasted in the environment to estimate the attitude and position of quad-rotor vehicles. Soundararaj, Prasanth and Saxena <ref type="bibr" target="#b20">[21]</ref> and Courbon et al. <ref type="bibr" target="#b21">[22]</ref>) used vision to fly in known environments; however their method does not apply to scenarios where full visual databases are not available. Mori et al. <ref type="bibr" target="#b22">[23]</ref> used markers to stably hover a co-axial helicopter and go from one marker to another. Other Related Works: In other related work on indoor navigation, Michels, Saxena and Ng <ref type="bibr" target="#b5">[6]</ref> used an on-board camera for autonomous obstacle avoidance in a small RC car driving at high speeds. They compute image features that capture single image distance cues (Make3D, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>) to predict distances to obstacles. Our work is motivated by this work; however their obstacle avoidance algorithms for a ground robots do not directly apply to obstacle detection for MAVs.</p><p>One can also build a map of the environment using Visual SLAM (e.g., <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>), in which the map is built using images captured from a camera. Ribnick et al. <ref type="bibr" target="#b28">[29]</ref> estimate positions and velocities from monocular views. Such methods for estimating position/location from visual landmarks have been used in many robots which navigate on 2D ground. Celik et al. <ref type="bibr" target="#b1">[2]</ref> use vision to reconstruct 3D properties of the environment for UAV path-planning; this technique only applies to situations where there are strong feature points that could be tracked from frame to frame, and would not apply in many indoor environments that are devoid of trackable features (e.g., walls). Our approach does not explicitly perform 3D reconstruction and hence is less computationally intensive.</p><p>Our vision-based algorithm does not require highresolution cameras; in fact, our algorithm is robust enough that a camera resolution of 128 Ã— 128 pixels suffices, making our algorithm attractive for even smaller aerial robots. For example, extremely small robots such as 25mm long micromechanical flying insects <ref type="bibr" target="#b29">[30]</ref> can use miniature cameras. Most flying platforms have a camera installed (e.g., <ref type="bibr" target="#b30">[31]</ref>), and therefore our algorithms could also be used on them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. HARDWARE PLATFORM</head><p>Our primary platform is the Parrot AR.Drone quadrotor (Figure <ref type="figure" target="#fig_0">1</ref>). This inexpensive toy, currently available to the general public and contains two cameras (one facing forward, the other horizontally downwards), a sonar height sensor, and an onboard computer running proprietary software for communication and command handling. Commands and images are exchanged via a WiFi ad-hoc connection between our host machine and the AR.Drone. Our algorithms run on the host machine-a 2010 Apple Macbook Pro (2.66GHz Intel Core i7, 4GB RAM), running Ubuntu 10.04.</p><p>We get an image of resolution of 320Ã—240 pixels from the AR.Drone's forward-facing camera, and get 88 Ã— 72 pixels form the downward-facing camera. Due to the limitations of the proprietary software on the drone, the only way to receive both images is to overlay one on top of the other. Therefore, we cannot use the full image data on the front camera, as we extract the bottom camera's image from it.</p><p>As a secondary platform, a co-axial hobby helicopter was used (The Blade CX2 Coaxial Micro Helicopter, shown in Figure <ref type="figure" target="#fig_2">3</ref>) with a Spectrum DX6i 2.4GHz transmitter to communicate. The camera on this platform is a miniature onboard KX141 camera, with a 795Ã—596 resolution. However, we choose to use the AR.Drone as our primary platform for it's stability and previously developed API.</p><p>On both platforms, three sonars (LV MaxSonar-EZ0) are used for proximity detection on the sides and front of the MAV, and one additional sonar sensor for height detection. We transmit the short-range sensor data back to the host computer using an XBee transmitter module. These inexpensive MAV platforms are less stable (i.e., they drift quite a lot) and are not capable of performing acrobatic maneuvers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. APPROACH</head><p>One possible approach to navigation is to create a 3D model of the environment from camera images (using techniques such as structure-from-motion <ref type="bibr" target="#b1">[2]</ref>) and then perform path-planning. Such an approach requires high camera resolution and often constructs incomplete models of the environment <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b31">[32]</ref>. Furthermore, even if complete models of the environment could be created, creating these models is computationally expensive. This causes unacceptable delays between the perception and the action of the MAV, thus severely limiting the MAV's autonomous abilities.</p><p>We note that a tight connection between perception and action is crucial in attaining the quick real-time responses required to navigate in constrained indoor environments. Our approach is also motivated by the studies on insect flight that demonstrate houseflies do not explicitly comprehend the 3D structure of their surroundings, but rather react to basic stimuli such as optical flow <ref type="bibr" target="#b32">[33]</ref>. Therefore in our approach, the vision algorithms are closely coupled to the control, minimizing delay.</p><p>We surveyed the types of indoor environments found in common home and office buildings. We found that the majority of the indoor environments belong to the following categories: long spaces (e.g., corridors), staircases, and other (such as rooms and corners). Each type of environment has a certain structure that lends itself to simple navigation rules. For example, in order to fly in a corridor, we only need to figure out the distance from the ground/walls and the direction of the end of the corridor (Figure <ref type="figure" target="#fig_4">4</ref>). When facing a staircase, we only need to know the center of the staircase and which direction to fly in (Figure <ref type="figure" target="#fig_5">5</ref>).</p><p>Given images of the current environment, our classifier detects the environment type. Depending on the classifier result, the MAV follows different strategies: for corridors and stairs, it relies on perspective cues to navigate; and for corners, it instead relies on short-range sensors to navigate. In the future, this setup can be augmented with additional algorithms for new environments and thus expanded beyond the scope of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ALGORITHMS</head><p>Indoor environments are comprised of long straight parallel lines, and from the MAV's perspective each environment type has unique visual cues. Our algorithms are designed to take advantage of these perspective cues to navigate the MAV though the environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Corridor</head><p>When in a corridor, the goal of the MAV is usually to fly to one end of the corridor. The ends of the corridor can be observed as vanishing points in images from the MAV's camera. Vanishing points have been used successfully for estimating 3D structure from a single image <ref type="bibr" target="#b33">[34]</ref>. In indoor environments, vanishing points can be found consistently and therefore we use them to locate the end of the corridor.</p><p>In detail, we first compute the edges of the image using the Canny edge detector, and then use the probabilistic Hough transform to find long lines. In a corridor environment, the long lines along the corridor converge towards a vanishing point at the end of the corridor. Because of errors, all the lines do not intersect at exactly one point, so we look for the region in the image which has the highest density of pairwise line intersections (Figure <ref type="figure" target="#fig_4">4</ref>). To do this, we divide the image plane into a 11 Ã— 11 grid G and count how many line intersections fall into each grid element. Let (a * , b * ) be the grid element that has the highest number of intersections.</p><p>More formally, let</p><p>â€¢ l : a line in the image, discovered via Hough transform.</p><p>Let L be the number of lines found in the image, and l âˆˆ [0, L). The line l can be represented by parameters</p><formula xml:id="formula_0">m l , b l as y = m l x + b l . â€¢ (x k , y k ) âˆˆ R 2</formula><p>+ : the coordinates of the intersection of two lines. There are K total intersections. In detail, if the lines i and j do not intersect, let (x k , y k ) = (âˆž, âˆž). If they do, then we get them by solving </p><formula xml:id="formula_1">y k = m i x k + b i = m j x k + b j . â€¢ G:</formula><formula xml:id="formula_2">G a,b = K k=1 1{a â‰¤ nx k w &lt; a + 1, b â‰¤ ny k h &lt; b + 1}</formula><p>where w is the width of the image, and h is the height of the image. The grid with the maximum number of intersections is:</p><formula xml:id="formula_3">(a * , b * ) = arg max (a,b) G a,b</formula><p>Therefore the initial estimate of the vanishing point is:</p><formula xml:id="formula_4">(x * , y * ) = w n (a * + 0.5), h n (b * + 0.5)</formula><p>However this estimate is noisy, and therefore we average the actual location of the intersections lying near (x * , y * ) in order to accurately estimate the vanishing point. N represents the set of points lying close to (x * , y * ):</p><formula xml:id="formula_5">N = {s âˆˆ [0, K) : ||(x s , y s ) -(x * , y * ))|| 2 â‰¤ Î´}</formula><p>where Î´ is a distance threshold. We then compute the new estimate of the vanishing point as: If the pair-wise intersections are closely concentrated, then it indicates a high confidence in the estimate of the vanishing point; therefore we compute the variance:</p><formula xml:id="formula_6">(x, È³) = 1 |N | sâˆˆN (x s , y s )</formula><formula xml:id="formula_7">Ïƒ 2 = 1 |N | sâˆˆN ||(x s , y s ) -(x, È³)|| 2 2 (1)</formula><p>It is important that the algorithm has low false positive rate (i.e. it does not report a vanishing point when none is actually present) because the MAV requires precision control. We use the number of pair-wise intersections found (i.e., G a * ,b * ), the variance (Ïƒ 2 ) and the presence/absence of vanishing point in the previous frame in order to compute the probability that there is a vanishing point in the current frame. If the previous frames in the video sequence had strong vanishing points, the current frame is likely to have a vanishing point. Furthermore, the current frame's vanishing point is likely to be close to those of the previous frames. We construct a probability model with Gaussians to model the noise in the location of the vanishing point. The probability of the location of the vanishing point X t is given by the product of two conditional probabilities: P (X t |X t+1 ) and P (X t |I), i.e., we model it as a Markov Model, and solve it using standard Viterbi algorithm <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Staircase</head><p>In order to fly up a staircase, the MAV needs to advance and ascend carefully without hitting the stairs, and remain in the center of the staircase.</p><p>Our goal is to find the center of the staircase, so the MAV can follow the staircase using its low-level controller.</p><p>We start by detecting the horizontal lines that form the staircase with the Canny edge detector and probabilistic Hough transform to acquire line segments. Typically, an image has many horizontal lines in an image, and therefore we need to specifically look for a sequence of horizontal lines that represent the staircase. We do so by classifying the line segments as horizontal or vertical and looking for the largest horizontal-line cluster in the Hough transform space. Our estimate of finding the stairs is based on the number of lines found in the cluster. Once the cluster of horizontal lines is detected, we take the mean of the lines' endpoints; this gives the desired direction the quadrotor should go to-we show this by the red vertical line in Figure <ref type="figure" target="#fig_5">5</ref>.</p><p>When an MAV, flying upwards, reaches the top of a flight of stairs, the forward-facing camera of the MAV can no longer see the stairs; the stairs are underneath the MAV, out of view. In order to detect that the MAV is above a flight of stairs and to increase its navigational accuracy, we use the downward-facing camera on the MAV. Again, using a Hough transform, we extract the long lines in the image, then filter them. The filter picks out lines within 45 degrees of horizontal (perpendicular to the MAV's direction of travel). Two examples are shown in Figure <ref type="figure" target="#fig_6">6</ref>.</p><p>Because most detectable lines in indoor environments are at right angles, the slopes of the resulting lines are used to fine-tune the MAV's heading, with the goal to make the lines detected perpendicular to the MAV's direction. The location of the detected lines is also used to help center the MAV on a desired path. This fine-tuning helps the MAV return to a desired path if it drifts off course.  There are two major components to control the MAV's heading: Yaw and Drift. Yaw is the adjustment to the rotational direction of the MAV. We compute the average slope of the filtered lines from the downward facing camera (and call it Slope). Let P os be the distance of the desired direction (i.e., the red vertical line in Figure <ref type="figure" target="#fig_5">5</ref>) from the center of the image in the front-facing stair algorithm. We adjust the Yaw as: âˆ†Yaw = Î± 1 * Slope + Î± 2 * P os where Î± 1 , Î± 2 are the gain parameters.</p><p>Drift is the side-to-side movement of the MAV. Finding the drift adjustment using the bottom image is a little more difficult, as we only want the MAV to drift if it is properly oriented. We thus reintroduce the average slope from above, combined with the average offset (from center) of the line midpoints in the downward-facing camera. Using these, and a similar scaling parameter Î², we obtain the following:</p><formula xml:id="formula_8">âˆ†Drift = Î² * offset 1 -|slope|</formula><p>Because we are dividing offset by 1-|slope|, âˆ†Drift will approach 0 as slope approaches 1 or -1 (45 degrees from horizontal), and will peak when slope is 0. This reduces the drift correction in situations when the MAV is not properly oriented. The drift correction is especially useful for returning to the desired path when the close range sensors cannot detect anything (i.e. stairs with an open railing).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Environment Classifier</head><p>We used two algorithms for classifying which environment the MAV is currently in. The first one uses GIST features with a learning algorithm, and the second uses "confidence" values generated by the algorithms described above. GIST Classifier: We first extract the GIST descriptor <ref type="bibr" target="#b35">[36]</ref> of each image by shrinking the input image down to 128 Ã— 128. We then use SVM <ref type="bibr" target="#b36">[37]</ref> for classifying indoor images into these three categories: (1) corridor, (2) stairway, (3) room (neither). GIST descriptors are well-suited for this task because they directly measure the global distribution of oriented line segments in an image, which takes advantage of the long lines found in indoor images. This classifier is used with the helicopter platform, but the quadrotor uses our second algorithm below. "Confidence" Classifier: In this classifier, "confidence" estimates are gathered from the stair and corridor vision algorithms. These confidences are based on the number of lines found in the respective cluster. The confidence values are then compared, and the highest (above a threshold) is deemed the current environment the MAV is in. For example, in the corridor algorithm, if 80% of the lines cross the vanishing point, the algorithm will return a high confidence. Where as if only 20% of the lines cross the vanishing point, it will have a low confidence. If all of the vision algorithms return a confidence below a threshold, then the current environment is classified as unknown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Corners and Unknown Environments</head><p>Connecting stairs and corridors are corners and rooms where our vision algorithms would not be able to reliably find perspective cues. This is often because only a blank empty wall is visible by the front-facing camera. In such areas, the MAV's goal is to explore the area to find adjacent environments to fly to where the vision algorithms may work.</p><p>In these areas, the system falls back to an exploration algorithm as follows. When the MAV first enters a corner, it keeps on moving forward until the front-facing short-range sensor reports proximity to a wall. Then, it uses the side short-range sensors to turn to the most open area. While turning, it checks the front short-range sensors, and will continue moving if there is enough space. If it does not find an open space, it turns back in the other direction while still looking for an open space. This avoids the MAV turning completely around, unless it has actually reached a closed corner. If at any time another environment is detected, the corner algorithm will give control to the respective vision algorithm(s). We found this approach to work surprisingly well in most situations. VI. CONTROL Our control algorithm is a set of low-level proportionalderivative controllers that adjust the MAV's throttle, yaw, pitch, and roll. The low-level controllers work together to stabilize the helicopter. (The quadrotor has some stabilization in its proprietary low-level controllers.) Navigation to reach Goal: The corridor and staircase vision algorithms give a value in the range [-1, 1]; a value closer to -1 indicates that the goal is to the left of the MAV, and a value closer to 1 indicates the goal is to the right. (A value of 0 means the goal is exactly in the center.) This was used to control the yaw, pitch and drift of the MAVs. (Similar to Section V-B.) Height Control: In most situations, there is a -margin within which it is fine to fly the MAV. (I.e., we don't care if the MAV flies closer or farther from the ground, just as long as it does not get too close or far from it.) For our helicopter, we use a PD controller with a non-linear response (flat within the -margin). For our quadrotor, we provided the output of the sonar height estimator to low-level controllers using a P -controller with similar response. Short-range proximity sensors: The short-range sonar range sensors work reliably for upto approximately 1.5m. If any of the front or side sensors detects a proximity to an obstacle for more than a few milliseconds, then the low-level controllers move the MAV in the opposite direction of the detected obstacle, until a threshold distance is reached. The proximity sensors take priority over the vision algorithms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. GIST Classification Experiments</head><p>We use our GIST classifier to classify which environment the MAV is currently in. We trained our GIST classifier on training images of 464 corridors, 339 stairs, and 484 rooms in a building (Upson Hall at Cornell University). We used a one-against-all method of training/testing. We then tested the GIST classifier on images taken in a different building (Phillips Hall at Cornell University). For testing, we had 314 corridor images, 269 staircase images and 414 room images. Table <ref type="table" target="#tab_0">I</ref> shows the performance of the classifier.</p><p>Note that even though the GIST classifier makes some errors, in practice, other parts of the algorithm correct for them because of the two following reasons. First, our corridor and stair classifiers also report the confidence in their goalpredictions (see Section V-A). Second, we get a series of images over time and one mistake by the classifier will not be detrimental for the MAV's navigation overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Testing Environments and Conditions</head><p>Our tests were conducted in a variety of indoor environments, shown in Figure <ref type="figure" target="#fig_7">7</ref>. For a corridor test to be considered successful, the MAV must progress at least 100ft (many tests progressed further). For a stairs test to be considered successful, the MAV must start behind the first step of the staircase and go up the stairs beyond the final step. <ref type="foot" target="#foot_0">1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Helicopter Experiments</head><p>Our first platform was a co-axial helicopter. In our experiments, we tested two different corridors and two different stairs (in different buildings). Table <ref type="table" target="#tab_1">II</ref> shows the quantitative results and Figure <ref type="figure" target="#fig_8">8</ref> shows some screenshots. While our experiments demonstrate that our vision algorithms are quite robust and work on multiple platforms, this platform suffered from severe drift problems. Therefore we performed our full experiments on the quadrotor platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Quadrotor Experiments</head><p>Our second platform was a AR Drone quadrotor. Table <ref type="table" target="#tab_1">III</ref> shows our experimental results of flying the quadrotor in three different corridors and three different stairs, in a total of 41 experiments. These environments were quite varied in appearance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corridors:</head><p>We achieve an overall success rate of 92% on the quadrotor platform. Furthermore, we found our corridor algorithm to consistently detect the vanishing point 100% of the time in corridor 1 and corridor 2, but only 90% of the time in the corridor 3, as it was dimly lit with little contrast. Flying in a corridor is challenging because the wind from the quadrotor's blades creates turbulence. Because of this, gentle contact with the wall was made in a few tests (especially in corridor 3 which was very narrow). This situation could be fixed using a more robust short-range sensor system. Results for all quadrotor experiments are given for both contact and non-contact outcomes.</p><p>Staircases: Flying in a staircase is more challenging than flying in a corridor. First, the turbulence is significantly greater, thereby affecting stability. Second, staircases are much narrower giving less margin for error in the controller. Lastly, the quadrotor changes its pitch in order to go up the stairs which changes the angle of the front-facing camera.</p><p>In our experiments, the quadrotor can traverse a full flight of stairs successfully 87% of the time. Some of the stairs included two flights of stairs, where the quadrotor had to turn-once reaching on the middle level, it turned using "unknown environment" mode, until it saw the next set of stairs to go up.</p><p>The video showing some of these test flights is available at:</p><p>http://www.cs.cornell.edu/ Ëœasaxena/MAV</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSIONS</head><p>We presented a set of vision algorithms to autonomously fly a Miniature Aerial Vehicle (MAV) in common indoor environments based on single image perspective cues. We use classification algorithms to detect the type of environment, and then for each environment we extract perspective cues for estimating the desired direction for the MAV to fly. These vision algorithms are shown to be robust and can be applied to many different types of MAVs, enabling them to traverse corridors, stairs, and corners they have never seen before. Our method requires only a small, light-weight camera and therefore it is a good solution for MAVs with payload and power restrictions. Furthermore, these algorithms require significantly less computational power, enabling the MAV to quickly react and navigate new indoor environments.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Our indoor quadrotor Parrot AR Drone.</figDesc><graphic coords="1,337.68,161.60,195.84,126.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An example of an environment where we want to fly the MAV in. (Left) Picture of a staircase. (Right) Overhead map of the staircase. Note narrow spaces, unknown terrain, and turns at the end of the staircase.</figDesc><graphic coords="1,437.34,311.61,110.16,87.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Our co-axial indoor platform (inset shows the camera used).</figDesc><graphic coords="2,313.20,54.00,244.80,151.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>The n Ã— n grid that tiles the image plane (n = 11 in our experiments). G a,b represents the number of line intersections falling in the grid element (a, b). (a, b âˆˆ [0, n) are integers.). I.e.,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (Top) Original corridor images. (Bottom) Corridor processed with Canny edge detector and Hough transform to find vanishing point using our grid-based approach. The vanishing point is marked with a blue circle. Note that in the rightmost image, the confidence of the vanishing point estimate would be low because the variance Ïƒ 2 would be high (see text).</figDesc><graphic coords="4,245.78,183.60,87.83,136.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (Top) Original images of staircases. (Bottom) Image with bold red line marking location of staircase. Green line marks center of image for reference. For example, if the red line is to left of green line, the MAV should turn left.</figDesc><graphic coords="5,79.76,176.40,163.20,122.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. (Top) Original images from bottom camera. (Bottom) Detected lines with filter applied. Accepted lines are green, rejected are red. The left set shows the MAV in a situation needing yaw correction, the right needing drift correction.</figDesc><graphic coords="5,93.32,345.30,166.16,129.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Corridor and stair environments tested. (Top row) Corridors, (Bottom row) staircases. Note the variations in their appearance.</figDesc><graphic coords="8,101.05,160.78,134.31,100.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Screenshots of our MAVs flying in different environments. First and last image show co-axial helicopter and the others show our quadrotor platform.</figDesc><graphic coords="8,54.69,426.33,142.03,115.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I .</head><label>I</label><figDesc>Performance of the GIST classifier for different environments.</figDesc><table><row><cell>Environment</cell><cell cols="3">Number of test images False positive False negative</cell></row><row><cell>Corridor</cell><cell>314</cell><cell>1.3%</cell><cell>8.9%</cell></row><row><cell>Staircase</cell><cell>269</cell><cell>9.7%</cell><cell>0.7%</cell></row><row><cell>Room</cell><cell>414</cell><cell>8.9%</cell><cell>1.0%</cell></row><row><cell></cell><cell cols="2">VII. EXPERIMENTS</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II .</head><label>II</label><figDesc>Co-axial Helicopter: Experimental Performance. This corridor was very narrow -barely 3x the quadrotor's width. Furthermore, there was strong air currents because of air conditioning.</figDesc><table><row><cell>Environment</cell><cell></cell><cell>No. of</cell><cell></cell><cell>Vision</cell><cell>Success rate</cell></row><row><cell></cell><cell></cell><cell>experiments</cell><cell cols="2">success rate</cell><cell>(full flight)</cell></row><row><cell cols="2">Corridors (2 different)</cell><cell>4</cell><cell></cell><cell>100%</cell><cell>75%</cell></row><row><cell cols="2">Staircases (2 different)</cell><cell>10</cell><cell></cell><cell>70%</cell><cell>50%</cell></row><row><cell cols="6">*Note that this platform suffers significantly from stability problems and</cell></row><row><cell cols="3">does not have downward facing camera.</cell><cell></cell><cell></cell></row><row><cell cols="6">TABLE III. Quadrotor: Experimental Performance.</cell></row><row><cell>Environment</cell><cell>No. of</cell><cell>Vision</cell><cell></cell><cell cols="2">Success rate Success rate</cell></row><row><cell></cell><cell cols="3">experiments success rate</cell><cell cols="2">(no contact)</cell><cell>(full flight)</cell></row><row><cell>Corridor 1</cell><cell>9</cell><cell>100%</cell><cell></cell><cell>78%</cell><cell>100%</cell></row><row><cell>Corridor 2</cell><cell>7</cell><cell>100%</cell><cell></cell><cell>42%</cell><cell>86%</cell></row><row><cell>Corridor 3</cell><cell>10</cell><cell>90%</cell><cell></cell><cell>20%*</cell><cell>90%</cell></row><row><cell>Corr. Total</cell><cell>26</cell><cell>96%</cell><cell></cell><cell>46%</cell><cell>92%</cell></row><row><cell>Staircase 1</cell><cell>5</cell><cell>80%</cell><cell></cell><cell>80%</cell><cell>80%</cell></row><row><cell>Staircase 2</cell><cell>6</cell><cell>83%</cell><cell></cell><cell>67%</cell><cell>83%</cell></row><row><cell>Staircase 3</cell><cell>4</cell><cell>100%</cell><cell></cell><cell>60%</cell><cell>100%</cell></row><row><cell>Stair Total</cell><cell>15</cell><cell>87%</cell><cell></cell><cell>67%</cell><cell>87%</cell></row><row><cell>*</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The Parrot AR.Drone has an automated take-off procedure, however we found it to be unreliable in constrained indoor environments. Due to this, about half of our experiments began with manual take-off by the experimenter, and after the quadrotor reached a height of more than</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2ft, the control was given to the vision algorithm.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Tung S. Leung, Arjun Prakash and Henry Chan for their help.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Stereo vision and laser odometry for autonomous helicopters in gps-denied indoor environments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Achtelik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Prentice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>SPIE Unmanned Systems Technology XI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Monocular vision slam for indoor aerial vehicles</title>
		<author>
			<persName><forename type="first">K</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Clausman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Somani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiview stereo for community photo collections</title>
		<author>
			<persName><forename type="first">M</forename><surname>Goesele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mav navigation through indoor corridors using optical flow</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zingg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">High speed obstacle avoidance using monocular vision and reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Michels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An application of reinforcement learning to aerobatic helicopter flight</title>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Quigley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Aggressive landing maneuvers for unmanned aerial vehicles</title>
		<author>
			<persName><forename type="first">E</forename><surname>Feron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bayraktar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIAA GN&amp;C</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Control logic for automated aerobatic flight of miniature helicopter</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gavrilets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Martinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mettler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Feron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIAA GN&amp;C</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning for control from multiple demonstrations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Quadrotor using minimal sensing for autonomous indoor flight</title>
		<author>
			<persName><forename type="first">J</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stirling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Zufferey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Floreano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>EMAV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Toward indoor flying robots</title>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Nicoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Zufferey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">From the test benches to the first prototype of the mufly micro helicopter</title>
		<author>
			<persName><forename type="first">D</forename><surname>Schafroth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bouabdallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bermes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JIRS</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="245" to="260" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Two seconds to touchdown vision-based controlled forced landing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mejias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Usher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Corke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Campoy</surname></persName>
		</author>
		<editor>IROS</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A stereo vision system for uav guidance</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J D</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thurrowgood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Soccol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Vision-assisted control of a hovering air vehicle in an indoor setting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>Bringham Young University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A visual navigation system for autonomous flight of micro air vehicles</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kendoul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nonami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Autonomous altitude estimation of a uav using a single onboard camera</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Andersh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Morellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papanikolopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mettler</surname></persName>
		</author>
		<editor>IROS</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Dynamic visual servoing of a small scale autonomous helicopter in uncalibrated environments</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baoquan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<editor>IROS</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Estimation and control of a quadrotor vehicle using monocular vision and moirre patterns</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tournier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valenti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>How</surname></persName>
		</author>
		<editor>AIAA GN&amp;C</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Autonomous indoor helicopter flight using a single onboard camera</title>
		<author>
			<persName><forename type="first">S</forename><surname>Soundararaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sujeeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visual navigation of a quadrotor aerial vehicle</title>
		<author>
			<persName><forename type="first">J</forename><surname>Courbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mezouar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Guenard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Martinet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Vision-based guidance control of a small-scale unmanned helicopter</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hirata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinoshita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">3-d depth reconstruction from a single still image</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Make3d: Learning 3D Scene Structure from a Single Still Image</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="824" to="840" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">An image-to-map loop closing method for monocular slam</title>
		<author>
			<persName><forename type="first">B</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Newmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tardos</surname></persName>
		</author>
		<editor>IROS</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fab-map: Probabilistic localisation and mapping in the space of appearance</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Newmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="647" to="665" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visual slam for flying vehicles</title>
		<author>
			<persName><forename type="first">B</forename><surname>Steder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Grisetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1088" to="1093" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Estimating 3d positions and velocities of projectiles from monocular views</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ribnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Atev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papanikolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="938" to="944" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Flapping flight for biomimetic robotic insects: Part ii-flight control design</title>
		<author>
			<persName><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schenato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="789" to="803" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Ar.drone</title>
		<author>
			<persName><surname>Parrot</surname></persName>
		</author>
		<ptr target="http://ardrone.parrot.com/" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Depth estimation using monocular and stereo cues</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Vision: a computational investigation into the human representation and processing of visual information</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<publisher>W.H. Freeman</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Single view metrology</title>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="148" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Support vector learning for interdependent and structured output spaces</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
