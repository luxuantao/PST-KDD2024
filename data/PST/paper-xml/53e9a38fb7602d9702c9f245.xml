<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Design Principles for Effective Knowledge Discovery from Big Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Edmon</forename><surname>Begoli</surname></persName>
							<email>begolie@ornl.gov</email>
							<affiliation key="aff0">
								<orgName type="department">Computational Sciences &amp; Engineering Division</orgName>
								<orgName type="institution">Oak Ridge National Laboratory Oak Ridge</orgName>
								<address>
									<region>Tennessee</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><surname>Horey</surname></persName>
							<email>horeyjl@ornl.gov</email>
							<affiliation key="aff0">
								<orgName type="department">Computational Sciences &amp; Engineering Division</orgName>
								<orgName type="institution">Oak Ridge National Laboratory Oak Ridge</orgName>
								<address>
									<region>Tennessee</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Design Principles for Effective Knowledge Discovery from Big Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">539A6F76C2E9FBE0CDC16EAFCD6DE82D</idno>
					<idno type="DOI">10.1109/WICSA-ECSA.212.32</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Big data phenomenon refers to the practice of collection and processing of very large data sets and associated systems and algorithms used to analyze these massive datasets. Architectures for big data usually range across multiple machines and clusters, and they commonly consist of multiple special purpose sub-systems. Coupled with the knowledge discovery process, big data movement offers many unique opportunities for organizations to benefit (with respect to new insights, business optimizations, etc.). However, due to the difficulty of analyzing such large datasets, big data presents unique systems engineering and architectural challenges. In this paper, we present three system design principles that can inform organizations on effective analytic and data collection processes, system organization, and data dissemination practices. The principles presented derive from our own research and development experiences with big data problems from various federal agencies, and we illustrate each principle with our own experiences and recommendations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Knowledge Discovery from Data (KDD) <ref type="bibr" target="#b0">[1]</ref> refers to a set of activities designed to extract new knowledge from complex datasets. The KDD process is often interdisciplinary and spans computer science, statistics, visualization, and domain expertise (Figure <ref type="figure" target="#fig_0">1</ref>). In recent years, large quantities of data have become increasingly available at significant volumes (petabytes or more). Such data have many sources including online activities (social networking, social media), telecommunications (mobile computing,call statistics), scientific activities (simulations, experiments, environmental sensors), and the collation of traditional sources (forms, surveys). Consequently KDD has become strategically important for large business enterprises, government organizations, and research institutions. However, effectively producing knowledge from massive datasets remain challenging, especially for large enterprise organizations comprised of multiple sub-organizations (each of whom may have their own internal processes, formats, etc.). Effective KDD therefore requires effective organizational and technological practices to be in place. Specifically, knowledge discovery processes are comprised of:</p><p>• Data collection, storage and organization practices • Understanding and effective application of the modern data analytic methods (including tools) • Understanding of the problem domain and the nature, structure and meaning of the underlying data This paper outlines empirically derived principles for the establishment of effective architectures for knowledge discovery over big data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head><p>The principles described in this paper are derived from the experiences and outcomes of various real world projects at Oak Ridge National Laboratory (ORNL). ORNL collaborates with several state and federal agencies on big data projects <ref type="bibr" target="#b1">[2]</ref>; typically ORNL receives data, is expected to analyze this data with domain experts, and to present the results via multiple avenues (i.e., web interface, reports, etc). Often the types of analysis to be performed are not explicitly defined and ORNL must explore a variety of potential methods. On occasion, ORNL is also asked to re-evaluate the current state of an agencies' internal big data architecture and strategy. In a recent example, an agency approached ORNL to develop new platforms for comprehensive and flexible data analysis. In addition to improving current analytic and business intelligence functions, the initiatives objective was to modernize, simplify, and streamline data collection, organization, and analysis of nationally significant datasets. The agency's existing methods were considered costly (using many proprietary components), somewhat antiquated (using older mainframe systems), and unable to meet rapidly increasing demand. The rest of the paper describes some key lessons learned in the development of our own infrastructure and the development of infrastructure for other agencies. We believe that the application of some core principles can yield economical, comprehensive, flexible, and secure solutions for the federal government's big data needs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PRINCIPLES</head><p>Knowledge discovery, as any form of discovery, is serendipitous. Discovery of new knowledge might occur if the right factors are present and aligned. Factors such as intuition, acuteness, and probability of observation are difficult to control. Others, such as comprehensibility and organization of data, its layout, availability of proper tools, and domain expertise are controllable. Our design principles are largely concerned with maximizing the controllable factors and thereby enabling researchers to explore, analyze, and interact with data in as easy manner as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Principle 1: Support a Variety of Analysis Methods</head><p>Knowledge discovery and modern data science employs methods from distributed programming, data mining, statistical analysis, machine learning, visualization, and human-computer interaction (amongst others). These methods often employ vastly different tools and techniques. For example, programmers may use Java to write distributed computation (i.e., Hadoop <ref type="bibr" target="#b2">[3]</ref>), while statisticians may feel more comfortable using R, SAS, etc. Many analysts will interact with SQL during the lifetime of the application. Depending on the nature of the analysis, different tools and expertise may be applied at different times. Indeed, it has been our experience that it is better to support a variety of tools rather than forcing users to use a limited set of tools (that many may be unfamiliar with, etc.). The architecture must therefore support a variety of methods and analysis environments. In the following, we detail some specific methods that are frequently used in our projects.</p><p>1) Statistical Analysis: Statistical analysis is concerned with both summarizing large datasets (i.e., average, min, etc.) and in defining models for prediction. In our experience, such analysis is often the first step in understanding the data. However most statistical tools (i.e., R, SAS) prefer to compute over numerical and categorical data organized in a tabular, columnoriented fashion. This often requires an extensive parsing and organization step, especially for unstructured datasets. In our current systems, we provide a variety of statistical tools, including column-oriented relational databases (i.e., SQL), R, and Python (i.e., NumPy, SciPy).</p><p>2) Data Mining and Machine Learning: Data mining is concerned with automatically discovering useful models and patterns in large datasets. Data mining consists of a large set of statistical and programmatic techniques (i.e., clustering, topic discovery, etc.). Machine learning employs both data mining and statistical techniques (amongst others) with the explicit goal of enabling machines to understand a set of data. Techniques may be supervised (i.e. with human assistance) or unsupervised. Often these techniques are used in conjunction with statistical methods for elucidating complex relationships (non-linear). For these tasks, we provide tools including MADLib <ref type="bibr" target="#b3">[4]</ref>(in conjunction with EMC Greenplum <ref type="bibr" target="#b4">[5]</ref>) and the Hadoop Mahout library.</p><p>3) Visualization and Visual Analysis: Visual analytics is an emerging field in which massive datasets are presented to users in visually compelling ways with the hope that users will be able to discover interesting relationships. Visual analytics requires generating many visualizations (often interactively) across many datasets. For our large visualization systems <ref type="bibr" target="#b5">[6]</ref> and web-based interactive systems <ref type="bibr" target="#b6">[7]</ref>, we employ a combination of high bandwidth file systems and pre-computed analytic artifacts stored as visualization friendly objects such as JSON representations. In addition to providing a variety of data analysis methods, a comprehensive KDD architecture must supply a means of storing and processing the data at all stages of the pipeline (from initial ingest to serving results). While a single storage mechanism may suffice for small data volumes (i.e., local filesystem), this is more problematic for large-scale data analysis. Historically many organizations have relied on large relational databases to accomplish this. However, we argue that different types of analysis and the intermediate data structures required by these (e.g. graphs for social network analysis) call for specialized data management systems (Figure <ref type="figure" target="#fig_1">2</ref>). Others have also recognized that the time of the single style database that fits all needs is gone <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Principle 2: One Size Does Not Fit All</head><p>1) Data Preparation and Batch Analytics: Data preparation is the first step in the analytics pipeline (Figure <ref type="figure" target="#fig_2">3</ref>). At this stage, the data may contain errors, missing values, and is often in an unusable format (i.e., a compressed binary format). In our experience, Hadoop is an ideal tool for this stage. Hadoop is a collection of Java-based open source software inspired by Google's BigTable <ref type="bibr" target="#b8">[9]</ref>, Google File System <ref type="bibr" target="#b9">[10]</ref> and MapReduce <ref type="bibr" target="#b10">[11]</ref>. It includes a MapReduce component (for distributed computation) and a scalable storage component, Hadoop File System (HDFS), that can often replace costly SAN devices. Hadoop sub-projects such as Hive and HBase offer additional data management solutions for storing structured and semistructured data sets. In our systems we rely on HDFS as a data landing platform and use Hive as our batch-oriented data warehouse.</p><p>2) Processing Structured Data: Often the product of the data preparation stage is a set of highly structured, relational data. Although Hadoop can process such data (via Hive), we have found distributed analytic databases <ref type="bibr" target="#b11">[12]</ref> to be useful for storing and analyzing such data. These databases (i.e., EMC Greenplum, HP Vertica <ref type="bibr" target="#b12">[13]</ref>) are highly optimized for large reads, aggregations, and statistical processing. They often store data in a column-oriented fashion (vs row-oriented) and distribute data over multiple machines to scale large sizes. Because these systems employ SQL for querying, these databases can also serve as backends for mainstream Business Intelligence software (and thus simplifying visual interaction).</p><p>3) Processing Semi-structured Data: Not all data can be easily modeled using relational techniques. For example, hierarchical documents, graphs, and geospatial data. Such data is extremely useful for social network analysis, natural language processing, and semantic web analysis. We provide HBase <ref type="bibr" target="#b13">[14]</ref> and Cassandra <ref type="bibr" target="#b14">[15]</ref> for hierarchical, key-value data organization. For graph analysis, we employ both open-source tools (e.g., Neo4j <ref type="bibr" target="#b15">[16]</ref>) and proprietary hardware solutions (e.g., Cray's uRiKa platform <ref type="bibr">[17]</ref>). Finally, for geospatial data we employ open-source tools (e.g., PostGIS, GeoTools) and proprietary tools (e.g., ESRI software). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Principle 3: Make Data Accessible</head><p>While the previous principles address data analysis and organization, the final principle addresses the end product (which is often highly summarized data and insights). Specifically, it has been our experience that making the results accessible and easy to understand is paramount. Three approaches we have used to accomplish this are using open, popular standards, adoption of lightweight, web-oriented architectures, and exposing results via a web-based API.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Use Open, Popular Standards:</head><p>Presenting results to users involves a complex set of software (databases, application servers, and web interfaces). While this space has been involving quickly, it has been our goal to use well supported frameworks. For example employing open-source databases (i.e., PostgreSQL), application servers (i.e., Java, Node.js), and nimble Javascript frameworks (jQuery, etc.). Each of these systems should communicate using standard protocols (i.e., REST, ODBC/JDBC, JSON).</p><p>2) Use Lightweight Architectures: Users demand rich, interactive experiences when using web interfaces. While traditional methods of building web services suffice (i.e., J2EE), our experience has been that new lightweight architectures (i.e., Rails <ref type="bibr" target="#b16">[18]</ref>, Django [19], Node.js <ref type="bibr" target="#b17">[20]</ref>) simplify the construction of web applications. This, in turn, means that we can create rich applications quickly and on demand. Since many of these applications rely on open-source tools, we can be confident that our applications will be able to run on a variety of platforms.</p><p>3) Expose Results using an API: While downloading results in a document makes sense for many scenarios, users now demand more flexible methods to interact with data systems. Specifically users now expect rich web-enabled APIs to download, interact, and visualize data. Actual processing may happen on the server or in the client (via Javascript). By exposing data via an API, it becomes easier for disparate systems to interact and enables users to create additional analysis tools. In that regard, we were inspired by Yahoo's BOSS Web Services <ref type="bibr" target="#b18">[21]</ref> as a good model of how to expose rich web service APIs. Currently, we offer data feeds as OData <ref type="bibr" target="#b19">[22]</ref> compliant web services that represent either enumerable data entities or results of the analytical post-processing. For each instance of an API we provide interface documentation and extensive examples. As part of this effort, we have learned that, in order to foster rapid adoption of API based architecture, it is paramount to offer extensive documentation describing both API usage and the underlying datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. IMPLEMENTATION</head><p>We have integrated these design three principles into our own knowledge discovery infrastructure. We use this infrastructure to address various federal agency needs and to serve as a reference implementation for agencies that may want their own infrastructure. When ORNL executes work for others, the data provided for us typically arrive in a raw format (often from their own internal systems) that's not necessarily amenable to analysis (i.e., Cobol EBCDIC). Depending on the data volume, the data is either transmitted over the internet (using a secure channel) or via encrypted NAS drives. We process this data using Hadoop and may perform initial analysis using Hive. For data that requires additional structured analysis (i.e., SQL), we place the data into a distributed database. For other data formats, we use our cloud computing platform (CloudStack <ref type="bibr" target="#b20">[23]</ref>) to instantiate necessary datastores such as Cassandra. Other cloud platform also enables us to create virtual machines for ad-hoc analysis, parsing, and visualization. In accordance to the principles outlined, all datasets are exposed via RESTful APIs using the OData standard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. FUTURE WORK</head><p>Although our infrastructure is used for real-world applications, we treat these systems as a research platform and expect it to continuously evolve as the state-of-the-art advances. We have developed our knowledge discovery principles during the course of implementing these applications and standing up our own systems. However, there is still much to do and many open architectural questions. Some immediate ones include:</p><p>• How do we take advantage of cloud computing to instantiate big data services in an optimal manner (i.e., to reduce cost, maximize performance)? • How do we automate and formalize the process of instantiating the entire data analysis pipeline? • How do we track provenance and handle security as the data flows through the analysis pipeline? • What additional storage and analysis systems do we need? For example, do we need a Hadoop-for-graphs? What is the role of in-memory systems? We are currently examining these questions at ORNL while continuing to further refine our existing systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>The big data movement has energized the software architecture world, and it has introduced complex, interesting questions for the community. As organizations continue to collect more data at this scale, formalizing the process of big data analysis will become paramount. In this paper, we introduced several principles that we believe can guide organizations into developing a sound, useful, and flexible data analysis pipeline. These principles are a result of experience and lessons learned from our own big data applications at ORNL. We have instantiated these principles in our own infrastructure and have found the principles to be useful guides.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Elements of the knowledge discovery process.</figDesc><graphic coords="1,348.03,230.35,137.05,131.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Knowledge Discovery Architecture Reference Implementation -KD Fabric</figDesc><graphic coords="2,359.01,241.66,148.56,131.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Transformation of data through specialized datastores.</figDesc><graphic coords="3,70.33,461.79,91.60,92.48" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>978-0-7695-4827-2/12 $26.00 © 2012 IEEE DOI 10.1109/WICSA-ECSA.212.32</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Advances in knowledge discovery and data mining</title>
		<author>
			<persName><forename type="first">U</forename><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Piatetsky-Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Uthurusamy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fact sheet: Big data across the federal government</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kalil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Office of Science and Technology Policy</title>
		<imprint>
			<date type="published" when="2012-03">March 2012</date>
		</imprint>
	</monogr>
	<note>Executive Office of the President</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>White</surname></persName>
		</author>
		<title level="m">Hadoop: The definitive guide</title>
		<imprint>
			<publisher>Yahoo Press</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The MADlib analytics library or MAD skills, the SQL</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schoppmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fratkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gorajek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Welton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Greenplum database community edition</title>
		<ptr target="http://www.greenplum.com/products/community-edition" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Parallel visualization for GIS applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sorokine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings GeoComputation</title>
		<meeting>GeoComputation</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">3 -JavaScript based visualization library</title>
		<ptr target="http://mbostock.github.com/d3/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">One size fits all: An idea whose time has come and gone</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stonebraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Cetintemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Engineering, 2005. ICDE 2005. Proceedings. 21st International Conference on</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="2" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Big data: The next frontier for innovation, competition and productivity</title>
		<author>
			<persName><forename type="first">J</forename><surname>Manyika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bughin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dobbs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Roxburgh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Byers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-05">May, 2011</date>
			<publisher>McKinsey Global Institute</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The google file system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gobioff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="29" to="43" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MapReduce: simplified data processing on large clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="113" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Column-Stores vs. Row-Stores: how different are they really</title>
		<author>
			<persName><forename type="first">D</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hachem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="967" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Hp vertica database community edition</title>
		<ptr target="http://vertica.com/community" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Apache hbase</title>
		<ptr target="http://hbase.apache.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cassandra: A decentralized structured storage system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lakshman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="35" to="40" />
			<date type="published" when="2010-04">Apr. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Cray yarcdata urika appliance</title>
		<ptr target="http://www.cray.com/products/Urika.aspx" />
		<imprint/>
	</monogr>
	<note>Neo4j graph database</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Ruby on rails</title>
		<ptr target="http://rubyonrails.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Node.js</title>
		<ptr target="http://nodejs.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Yahoo! search build your own service (BOSS)</title>
		<ptr target="http://developer.yahoo.com/search/boss/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Open data protocol</title>
		<ptr target="http://www.odata.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Cloudstack</title>
		<ptr target="http://www.cloud.com" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
