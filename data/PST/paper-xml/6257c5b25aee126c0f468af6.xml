<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SPHERICAL MESSAGE PASSING FOR 3D MOLECULAR GRAPHS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yi</forename><surname>Liu</surname></persName>
							<email>yiliu@tamu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University College Station</orgName>
								<address>
									<postCode>77843</postCode>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Limei</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University College Station</orgName>
								<address>
									<postCode>77843</postCode>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
							<email>mengliu@tamu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University College Station</orgName>
								<address>
									<postCode>77843</postCode>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuchao</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University College Station</orgName>
								<address>
									<postCode>77843</postCode>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuan</forename><surname>Zhang</surname></persName>
							<email>xuan.zhang@tamu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University College Station</orgName>
								<address>
									<postCode>77843</postCode>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bora</forename><surname>Oztekin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University College Station</orgName>
								<address>
									<postCode>77843</postCode>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University College Station</orgName>
								<address>
									<postCode>77843</postCode>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SPHERICAL MESSAGE PASSING FOR 3D MOLECULAR GRAPHS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider representation learning of 3D molecular graphs in which each atom is associated with a spatial position in 3D. This is an under-explored area of research, and a principled message passing framework is currently lacking. In this work, we conduct analyses in the spherical coordinate system (SCS) for the complete identification of 3D graph structures. Based on such observations, we propose the spherical message passing (SMP) as a novel and powerful scheme for 3D molecular learning. SMP dramatically reduces training complexity, enabling it to perform efficiently on large-scale molecules. In addition, SMP is capable of distinguishing almost all molecular structures, and the uncovered cases may not exist in practice. Based on meaningful physically-based representations of 3D information, we further propose the SphereNet for 3D molecular learning. Experimental results demonstrate that the use of meaningful 3D information in SphereNet leads to significant performance improvements in prediction tasks. Our results also demonstrate the advantages of SphereNet in terms of capability, efficiency, and scalability. Our code is publicly available as part of the DIG library (https://github.com/divelab/DIG).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In many real-world studies, structured objects such as molecules are naturally modeled as graphs <ref type="bibr" target="#b20">(Gori et al., 2005;</ref><ref type="bibr" target="#b55">Wu et al., 2018;</ref><ref type="bibr" target="#b44">Shervashidze et al., 2011;</ref><ref type="bibr" target="#b12">Fout et al., 2017;</ref><ref type="bibr" target="#b34">Liu et al., 2020;</ref><ref type="bibr" target="#b54">Wang et al., 2020)</ref>. With the advances of deep learning, graph neural networks (GNNs) have been developed for learning from graph data <ref type="bibr" target="#b26">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b11">Defferrard et al., 2016;</ref><ref type="bibr" target="#b50">Veličković et al., 2018;</ref><ref type="bibr" target="#b58">Zhang et al., 2018;</ref><ref type="bibr" target="#b57">Xu et al., 2019;</ref><ref type="bibr" target="#b14">Gao &amp; Ji, 2019;</ref><ref type="bibr" target="#b15">Gao et al., 2018;</ref><ref type="bibr">2020)</ref>. Currently, the message passing scheme <ref type="bibr" target="#b18">(Gilmer et al., 2017;</ref><ref type="bibr" target="#b40">Sanchez-Gonzalez et al., 2020;</ref><ref type="bibr" target="#b51">Vignac et al., 2020;</ref><ref type="bibr" target="#b2">Battaglia et al., 2018)</ref> is one of the commonly used architectures for realizing GNNs. In this work, we aim at developing a novel message passing method for 3D graphs. Generally, a 3D molecular graph contains 3D coordinates for each atom given in the Cartesian system along with the graph structure <ref type="bibr" target="#b33">(Liu et al., 2019;</ref><ref type="bibr" target="#b48">Townshend et al., 2019;</ref><ref type="bibr" target="#b1">Axelrod &amp; Gomez-Bombarelli, 2020)</ref>. Different types of relative 3D information can be derived from 3D molecular graphs, and they can be important in molecular learning, such as bond lengths, angles between bonds <ref type="bibr" target="#b41">(Schütt et al., 2017;</ref><ref type="bibr" target="#b28">Klicpera et al., 2020b)</ref>.</p><p>We first investigate complete representations of 3D molecules. This requires the graph structure to be uniquely defined by relative 3D information. To this end, we conduct formal analyses in the spherical coordinate system (SCS) <ref type="bibr" target="#b6">(Chen et al., 2019)</ref>, and show that relative location of each atom in a 3D graph is uniquely determined by three geometries, including distance, angle, and torsion. However, such completeness needs to involve edge-based 2-hop information, leading to excessively high computational complexity. To circumvent the computational cost, we propose a novel message passing scheme, known as the spherical message passing (SMP), for fast and accurate 3D molecular learning. Our SMP is efficient and approximately complete in representing 3D molecules. First, we design a novel strategy to compute torsion, which only considers edge-based 1-hop information, thus substantially reducing training complexity. This enables the generalization of SMP to large-scale molecules. In addition, we show that our SMP can distinguish almost all 3D graph structures. The uncovered cases seem rarely appear in practice. By naturally using relative 3D information and a novel torsion, SMP yields predictions that are invariant to translation and rotation of input graphs.</p><p>We apply the SMP to real-world molecular learning, where meaningful physical representations are needed. Geometries (d, θ, φ) specified by SMP are then physically represented by Ψ(d, θ, φ), which can be a solution to the Schrödinger equation, as described in Sec. 4. Based on this, we develop the spherical message passing neural networks, known as the SphereNet, for 3D molecular learning. We conduct experiments on various types of datasets including OC20, QM9, and MD17. Results show that, compared with baseline methods, SphereNet achieves the best performance without increasing the computing budget. Ablation study reveals contributions and necessity of different types of 3D information, including distance, angle, and torsion. Particularly, we compare with a complete message passing scheme that can distinguish all 3D graph structures but involves edge-based 2-hop information. Experimental results show that SphereNet achieves comparable performance but reduces running time by 4 times. This suggests the use of SphereNet in practice rather than the complete message passing scheme, whose computational complexity prevents its use on large molecules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">COMPLETE REPRESENTATIONS OF MOLECULES</head><p>Equivariant graph neural networks (EGNNs) represent one research area for 3D molecular graphs, as introduced in Sec. 5.1. These methods usually take coordinates in the Cartesian coordinate system (CCS) for all atoms as the raw input. Hence, all the network layers need to be carefully designed to be equivariant. The computing of some equivariant components is expensive, like spherical harmonics and Clebsh-Gordan coefficients <ref type="bibr" target="#b47">(Thomas et al., 2018;</ref><ref type="bibr" target="#b13">Fuchs et al., 2020)</ref>. In addition, the complicated SE(3) group representations may not be necessary for molecular learning where final representations are generally required to be invariant. In this work, we focus on the other category of methods that take relative position information purely as input to graph learning models. Relative 3D information could be distance or angle, which is inherently invariant to translation and rotation of input molecules. It is natural to consider such information in the spherical coordinate system (SCS). We start by investigating the structure identification of 3D molecules in the SCS. For any point in the SCS, its location is specified by a 3-tuple <ref type="bibr">(d, θ, φ)</ref>, where d, θ, and φ denote the radial distance, polar angle, and the azimuthal angle, respectively. When modeling 3D molecular graphs in the SCS, any atom i can be the origin of a local SCS, and d, θ, and φ naturally become the bond length, the angle between bonds, and the torsion angle, respectively. Thus, the relative location of each neighboring atom of atom i can be specified by the corresponding tuple <ref type="bibr">(d, θ, φ)</ref>. Similarly, the relative location of each atom in the 3D molecular graph can be determined, leading to the identified structure, which is naturally invariant to translation and rotation of the input graph. The SCS can be easily converted from the Cartesian coordinate system, thus, the tuple (d, θ, φ) can be easily obtained. As in Fig. <ref type="figure" target="#fig_0">1</ref>, we use the chemical structure of the hydrogen peroxide (H 2 O 2 ) to show how d, θ, and φ are vital for the molecular structure identification. It is obvious that the structure is uniquely defined by the three bond lengths d 1 , d 2 , d 3 , the two bond angles θ 1 , θ 2 , and the torsion angle φ. Note that the input may not contain all pairwise distances (all possible bond lengths). This is because the atomic connectivity is usually based on real chemical bonds and cut-off distances. The cut-off distance is usually set as a hyperparameter. It is hard to guarantee that the cut-off is larger than any pairwise distance in a molecule. Hence, in this example, H-H bond length may not be considered as input if the cut-off is small. Setting a proper cut-off is even harder for other complicated and large molecules where a distance between two atoms could be large. In addition, considering all pairwise distances will cause severe redundancies, dramatically increasing the computational complexity. The model also easily gets confused by excessive noise, leading to unsatisfactory performance. From the perspective of completeness, using all pairwise distance is not capable of recognizing the chirality property. To overcome the above challenges, we use a combination of distance, angle, and torsion for rigorous design and accurate learning. Apparently, the two O-H bonds can rotate around the O-O bond without changing any of the bond lengths and bond angles. In this situation, however, the torsion angle φ changes and the structure of the H 2 O 2 varies accordingly. The importance of torsion angle has also been demonstrated in related research domains. <ref type="bibr" target="#b17">Garg et al. (2020)</ref> formally shows that the torsion along with the port numbering can improve the expressive power of GNNs in distinguishing geometric graph properties, such as girth and circumference, etc. Other studies <ref type="bibr" target="#b24">(Ingraham et al., 2019;</ref><ref type="bibr" target="#b45">Simm et al., 2020)</ref> reveal that protein sequences and molecules can be accurately generated by considering the torsion in the given 3D structures. In this work, we propose SMP to systematically consider distance, angle, and torsion for approximately complete representation learning of 3D molecular graphs. Note that by using angle and torsion, SMP can easily recognize the chirality property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>O H H O</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SPHERICAL MESSAGE PASSING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MESSAGE PASSING SCHEME</head><p>Currently, the class of message passing neural networks (MPNNs) <ref type="bibr" target="#b18">(Gilmer et al., 2017)</ref> are one of the most widely used architectures for GNNs. Based upon the completeness analyses in Sec. 2, we propose to perform message passing in the spherical coordinate system (SCS), resulting in a novel and efficient scheme known as spherical message passing (SMP). We show that message passing schemes used in existing methods, such as SchNet and DimeNet, are special cases of SMP.</p><p>We first formally define a 3D molecular graph, which is usually represented as a 4-tuple G = (u, V, E, P ). The u ∈ R du is a global feature vector for the molecular graph G. V = {v i } i=1:n is the set of atom features, where each v i ∈ R dv is the feature vector for the atom i. E = {(e k , r k , s k )} k=1:m is the set of edges, where each e k ∈ R de is the feature vector, r k is the index of the receiver atom, and s k is the index of the sender atom for the edge k. P = {r h } h=1:n is the set of 3D Cartesian coordinates that contains 3D spatial information for each atom. In addition, we let E i = {(e k , r k , s k )} r k =i,k=1:m denote the set of edges that point to the atom i, and N i denote the indices of incoming nodes of atom i. The outputs after a message passing process include the updated global feature vector u ′ ∈ R du , the updated atom features V ′ = {v ′ i } i=1:n , and the updated edges</p><formula xml:id="formula_0">E ′ = {(e ′ k , r k , s k )} k=1:</formula><p>m . An illustration of the message aggregation scheme for SMP is provided in Fig. <ref type="figure" target="#fig_1">2 (a)</ref>. Apparently, the embedding of the atom r k is obtained by aggregating each incoming message e k . The message e k is updated based on E s k , the set of incoming messages pointing to the atom s k . Let q denote the sender atom of any message in E s k . Hence, we can define a local SCS, where s k serves as the origin, and the direction of the message e k naturally serves as the z-axis. We define a neighboring atom o of s k as the reference atom. Thus, the reference plane is formed by three atoms s k , r k , and o. For atom q, its location is uniquely defined by the tuple (d, θ, φ), as shown in Fig. <ref type="figure" target="#fig_1">2 (a)</ref>. Specifically, d determines its distance to the atom s k , θ specifies its direction to update the message e k . The torsion angle φ is formed by the defined reference plane and the plane spanned by s k , r k , and q. Intuitively, as an advanced message passing architecture in spherical coordinates for 3D graphs, SMP specifies relative location for any neighboring atom q by considering all the distance, angle, and torsion information, leading to more comprehensive representations for 3D molecular graphs.</p><p>Generally, the atom s k may have several neighboring atoms, which we denote as q 1 , ..., q t . It is easy to compute the corresponding bond lengths and bond angles for these t atoms. The SMP computes torsion angles by projecting all the t atoms to the plane that is perpendicular to e k and intersect with s k . Then on this plane, the torsion angles are formed in a predefined direction, such as the anticlockwise direction. By doing this, any atom naturally becomes the reference atom for its next atom in the anticlockwise direction. Notably, the sum of these t torsion angles is 2π. A simplified case is illustrated in Fig. <ref type="figure" target="#fig_1">2 (b</ref>). The atom s k has three neighboring atoms q 1 , q 2 , and q 3 ; q 3 is the reference atom for q 1 , and they form φ 1 ; q 1 is the reference atom for q 2 , and they form φ 2 ; similarly, q 2 is the reference atom for q 3 , and they form φ 3 . It is obvious that the sum of φ 1 , φ 2 , and φ 3 is 2π. As the torsion is defined relatively, q 1 can be picked arbitrarily, which will not affect the output of the message passing scheme, as we perform summation when aggregating information to s k from its neighbors q 1 , q 2 , and q 3 . Notably, by designing each atom to be the reference atom of the next one in the predefined direction, invariance is effectively achieved because reference atom is naturally relative. In addition, our method computes torsion within edge-based 1-hop neighborhood. Even though a torsion angle involves four atoms, our design avoids the number of torsion angles to be exponential, but makes it the same as the number of neighboring atoms. Hence, it is efficient and will not cause time or memory issues. Formally, the proposed SMP can be defined in the SCS as</p><formula xml:id="formula_1">e ′ k = ϕ e e k , v r k , v s k , E s k , ρ p→e {r h } h=r k ∪s k ∪Ns k , v ′ i = ϕ v (v i , ρ e→v (E ′ i )) , u ′ = ϕ u (u, ρ v→u (V ′ )) ,<label>(1)</label></formula><p>where ϕ e , ϕ v , and ϕ u are three information update functions on edges, atoms, and the whole graph, respectively. ρ e→v and ρ v→u aggregate information between different types of geometries. Especially, in SMP, the 3D information in P is converted and incorporated to update each message e k . Hence, SMP employs another position aggregation function ρ p→e for message update. Notably, the main difference between our SMP scheme defined in Eq. 1 and the GN framework in Battaglia et al. ( <ref type="formula">2018</ref>) is the inclusion of 3D information P . In line with the research area described in Sec. 5.1.2, we focus on such 3D information and develop a systematic solution to incorporate it completely and efficiently. Detailed description of these functions is given in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">COMPLETENESS VERSUS EFFICIENCY</head><p>Figure <ref type="figure" target="#fig_3">3</ref>: An illustration of cases that SMP can and cannot distinguish.</p><p>All the neighboring nodes of s k are projected to the plate perpendicular to the message of interest. We assume all the distances and angles are fixed (the molecules can be more easily distinguished otherwise). Hence, all the angle shown are torsion angles and they are formed in the anticlockwise direction. (a) and (b) are chiral and SMP can distinguish them. This is because in (a), q ′ 1 (90</p><formula xml:id="formula_2">• ), q ′ 2 (60 • ), q ′ 3 (120 • ), q ′ 4 (90 • ); in (b), q ′ 1 (60 • ), q ′ 2 (120 • ), q ′ 3 (90 • ), q ′ 4 (90 • ). SMP cannot distinguish (b) and (c) but this scenario may not exist in nature. ∠q ′ 1 s k q ′ 2 in (b) and ∠q ′ 1 s k q ′</formula><p>3 in (c) usually are different as q ′ 2 and q ′ 3 are different atoms and the corresponding distances and angles are the same.</p><p>The identification criteria described in Sec. 2 can fully determine the structure of a 3D molecule, but involves edge-based 2-hop information. Hence, the computational complexity is as sizeable as O(nk 3 ), where n is the number of atoms, and k denotes the average number of neighboring atoms for each center atom. Unfortunately, such design can hardly generalize to large molecular graphs. To this end, we propose SMP as an efficient and scalable scheme to realize message passing in SCS. Our SMP only involves edge-based 1hop information, thus the time complexity is reduced to O(nk 2 ). This enables the application of SMP to large molecules, like the newly released OC20 data <ref type="bibr" target="#b5">(Chanussot et al., 2020)</ref>. We rigorously investigate the completeness of SMP and show that it can distinguish even complex geometric properties such as chirality, as indicated by Fig. <ref type="figure" target="#fig_3">3</ref> (a) and <ref type="bibr">Fig. 3 (b)</ref>. As SMP uses the last atom as the reference atom (like q 2 is the reference atom for q 3 in Fig. <ref type="figure" target="#fig_1">2 (b</ref>)) in a predefined direction, the relative order between adjacent atoms is considered while the absolute order is neglected. Hence, SMP can not distinguish the two molecules illustrated by <ref type="bibr">Fig. 3 (b)</ref> and Fig. <ref type="figure" target="#fig_3">3 (c</ref>). However, this scenario may not exist in nature. This is also demonstrated in experiments that our SMP achieves comparable performance with the complete representations, while the latter induces huge time complexity and severe memory issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">RELATIONS WITH PRIOR MESSAGE PASSING METHODS</head><p>When developing message passing methods for 3D graphs, sphere message passing is an advanced scheme where the relative location of each atom is more specified. The development for 3D graphs with relative information is still in early stage. To our best knowledge, there exist several notable methods in literature, and they can be viewed as special cases of the SMP, as they capture partial 3D position information. For example, the SchNet and PhysNet consider distance, and the DimeNet encodes directional information. Formally, these methods can be perfectly fit the Spherical scheme defined in Eq. 1. We describe the details of these methods in Appendix C. Notably, compared with prior models, SMP provides a rigorous justification on its completeness with failure cases clearly described. Importantly, SMP is developed based on the identification analyses of 3D molecular graphs. Hence, it aims at learning complete data representations for 3D molecular graphs, rather than simply including extra 3D information (like angle or torsion).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SPHERENET</head><p>The obtained 3-tuple (d, θ, φ) indicates the relative location of any atom in a 3D molecular graph. However, it cannot serve as the direct input to neural networks as it lacks meaningful representations. Essentially, molecules are quantum systems thus the representation design needs to follow physics laws. An important aspect is to choose appropriate basis functions that transform the 3-tuple (d, θ, φ) into physically-based representations. Several basis functions have been explored in <ref type="bibr" target="#b23">Hu et al. (2021)</ref>; <ref type="bibr" target="#b28">Klicpera et al. (2020b)</ref>, including MLP, Gaussian and sine functions, spherical Bessel basis, and spherical harmonics. Especially, spherical Bessel is shown to be the best basis for encoding distance, and spherical harmonics is the most appropriate one for encoding angle <ref type="bibr" target="#b23">(Hu et al., 2021;</ref><ref type="bibr" target="#b28">Klicpera et al., 2020b)</ref>. We denote the final representation as Ψ(d, θ, φ). Referring to theories in Griffiths &amp; Schroeter (2018); <ref type="bibr" target="#b9">Cohen et al. (2019)</ref>; <ref type="bibr" target="#b28">Klicpera et al. (2020b)</ref>, one form of the representation can be expressed as</p><formula xml:id="formula_3">Ψ(d, θ, φ) = j ℓ β ℓn c d Y m ℓ (θ, φ), where j ℓ (•) is a spherical Bessel function of order ℓ, Y m</formula><p>ℓ is a spherical harmonic function of degree m and order ℓ, c denotes the cutoff, β ℓn is the n-th root of the Bessel function of order ℓ. We also have ℓ</p><formula xml:id="formula_4">∈ [0, • • • , L − 1], m ∈ [−ℓ, • • • , ℓ] and n ∈ [1, • • • , N ].</formula><p>L and N denote the highest orders for the spherical harmonics and spherical Bessel functions, respectively. They are hyperparameters in experimental settings. In addition, we can derive two simplified representations Ψ(d) and Ψ(d, θ) from Ψ(d, θ, φ).</p><p>Based upon the spherical message passing scheme described in Sec. 3 and physical representations, we build the SphereNet for molecular learning. Apparently, SphereNet can produce data representations that are both accurate and physically meaningful. By incorporating the position information in spherical coordinates, SphereNet also generates predictions invariant to translation and rotation of input molecules. Following the architecture design in the research line stated in Sec. 5.1.2, our network is composed of an input block, several interaction blocks, and an output block. For clear description, we assume the message e k for the edge k in Fig. <ref type="figure" target="#fig_1">2</ref> and Eq. ( <ref type="formula" target="#formula_1">1</ref>) is the message for update. The update process and detailed architecture for the SphereNet are provided in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">METHODS FOR 3D MOLECULAR GRAPHS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">EQUIVARIANT GRAPH NEURAL NETWORKS</head><p>One research line for 3D molecular graphs is equivariant graph neural networks (EGNNs) including tensor field networks (TFNs) <ref type="bibr" target="#b47">(Thomas et al., 2018)</ref>, SE(3)-transformers <ref type="bibr" target="#b13">(Fuchs et al., 2020)</ref>, PaiNN <ref type="bibr" target="#b42">(Schütt et al., 2021)</ref>, NequIP <ref type="bibr" target="#b3">(Batzner et al., 2021)</ref>, Noisy Nodes <ref type="bibr" target="#b19">(Godwin et al., 2022)</ref>, etc. The raw input of these methods usually contains the absolute information, such as coordinates in the Cartesian coordinate system. In intermediate layers, absolute information could be decomposed into partial absolute information and partial relative information as needed. A simple example is that a vector can be decomposed into its orientation (absolute) and length (relative) <ref type="bibr" target="#b47">(Thomas et al., 2018;</ref><ref type="bibr" target="#b42">Schütt et al., 2021)</ref>. Apparently, network components of these methods should be carefully designed to be equivariant. The preliminary work like TFNs were developed for 3D point clouds. However, it is demonstrated that for molecules whose downstream tasks usually require the systems to be invariant, the complicated SE(3) group representations are not necessary but S 2 representations are enough <ref type="bibr" target="#b29">(Klicpera et al., 2021)</ref>. Moreover, their performance on molecular tasks is not satisfactory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">INVARIANT GRAPH NEURAL NETWORKS</head><p>Another category of methods purely take relative 3D information as input, such as distances between atoms, angles between bonds, angles between planes, etc. Hence, the network is naturally invariant. The development of these methods is in early stage, and existing studies focus on leveraging different geometries. The SchNet <ref type="bibr" target="#b41">(Schütt et al., 2017)</ref> incorporates the distance information during the information aggregation stage by using continuous-filter convolutional layers. The PhysNet (Unke &amp; Meuwly, 2019) integrates both the atom features and distance information in the proposed interaction block. The DimeNet <ref type="bibr" target="#b28">(Klicpera et al., 2020b)</ref> is developed based on the PhysNet and moves a step forward by considering directional information in the interaction block. The GemNet <ref type="bibr" target="#b29">(Klicpera et al., 2021)</ref> is proposed recently for universal molecular representations. The OrbNet <ref type="bibr" target="#b38">(Qiao et al., 2020)</ref> combines distance information with the atomic orbital theory to design important SAAO features as inputs to GNNs. Generally, the use of 3D position information usually results in improved performance. However, existing methods simply include additional geometries such as distance and angle, and there lacks a rigorous justification on how different geometries contribute to the information aggregation process. We conduct formal analysis and show that all the distance, angle, and torsion are necessary for 3D molecular identification, based on which we propose SphereNet to generate more powerful molecular representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">METHODS FOR OTHER OBJECTS MODELED AS GRAPHS</head><p>Besides molecules, many other data objects are also represented as graphs, such as 3D point clouds <ref type="bibr" target="#b22">(Guo et al., 2020;</ref><ref type="bibr" target="#b46">Simonovsky &amp; Komodakis, 2017;</ref><ref type="bibr" target="#b43">Shen et al., 2018;</ref><ref type="bibr" target="#b31">Landrieu &amp; Simonovsky, 2018)</ref> and meshes <ref type="bibr" target="#b4">(Bronstein et al., 2021;</ref><ref type="bibr" target="#b10">De Haan et al., 2020;</ref><ref type="bibr" target="#b37">Perraudin et al., 2019)</ref>. When modeling 3D point clouds as 3D graphs, points are represented as nodes and connections between points are directed edges. Existing methods mainly capture distance information from local neighborhood in 3D space. In DGCNN <ref type="bibr" target="#b53">(Wang et al., 2019b)</ref>, a novel layer namely EdgeConv is proposed to aggregate distance-based edges features for node learning. In <ref type="bibr" target="#b30">Landrieu &amp; Boussaha (2019)</ref>, neighborhood radius along with spatial orientation is incorporated in local point embedding. The work <ref type="bibr" target="#b52">Wang et al. (2019a)</ref> proposes a graph attention convolution for 3D point clouds. Generally, these methods can be fit into our message passing scheme defined in Eq. 1. The work De Haan et al. ( <ref type="formula">2020</ref>) is an exemplary study that formulates meshes as graphs with considering geometrical information. The used convolutional kernel depends on the angle between a predefined reference edge and any other edge projected to the tangent plane for each vertex. It focuses on the design of gauge equivariance rather than the learning of complete geometry information. In this work, we study the complete learning of 3D molecules and leave extensive studies on other data types as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTAL STUDIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">EXPERIMENTAL SETUP</head><p>We apply our SphereNet to three benchmark datasets, including Open Catalyst 2020 (OC20) <ref type="bibr" target="#b5">(Chanussot et al., 2020)</ref>, QM9 <ref type="bibr" target="#b39">(Ramakrishnan et al., 2014)</ref>, and MD17 <ref type="bibr" target="#b7">(Chmiela et al., 2017)</ref>. Baseline methods include <ref type="bibr">PPGN (Maron et al., 2019)</ref>, SchNet <ref type="bibr" target="#b41">(Schütt et al., 2017)</ref>, PhysNet (Unke &amp; Meuwly, 2019), Cormorant <ref type="bibr" target="#b0">(Anderson et al., 2019)</ref>, PaiNN <ref type="bibr" target="#b42">(Schütt et al., 2021)</ref>, NequIP <ref type="bibr" target="#b3">(Batzner et al., 2021)</ref>, MGCN <ref type="bibr" target="#b35">(Lu et al., 2019)</ref>, DimeNet <ref type="bibr" target="#b28">(Klicpera et al., 2020b)</ref>, DimeNet++ <ref type="bibr" target="#b27">(Klicpera et al., 2020a)</ref>, GemNet <ref type="bibr" target="#b29">(Klicpera et al., 2021)</ref>, CGCNN <ref type="bibr" target="#b56">(Xie &amp; Grossman, 2018)</ref>, and sGDML <ref type="bibr" target="#b8">(Chmiela et al., 2018)</ref>. Detailed configurations of all the models used in the following sections are provided in the supplementary material. Unless otherwise specified, for all the baseline methods, we report the results taken from the referred papers or provided by the original authors. For the SphereNet, all models are trained using the Adam optimizer <ref type="bibr" target="#b25">(Kingma &amp; Ba, 2014)</ref>. The optimal hyperparameters are obtained by grid search. Network configurations and search space for all models are provided in Appendix D. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">OC20</head><p>The Open Catalyst 2020 (OC20) dataset is a newly released large-scale dataset for catalyst discovery and optimization <ref type="bibr" target="#b5">(Chanussot et al., 2020)</ref>. It comprises millions of DFT relaxations across huge chemical structure space such that machine learning models can be fully trained. We focus on the IS2RE task in this work, and description of the data is provided in Appendix E. Results for CGCNN, SchNet, and DimeNet++ are provided in <ref type="bibr" target="#b5">Chanussot et al. (2020)</ref>. The original GemNet paper does not contain results on the OC20 dataset, We use the publicly available code from the OC Project website<ref type="foot" target="#foot_0">1</ref> to produce results for GemNet-T. We report evaluation results of fixed epochs for SphereNet. Following a setting in <ref type="bibr" target="#b5">Chanussot et al. (2020)</ref>, we use the direct approach and all the training data for training models. The used metrics are the energy MAE and the percentage of Energies within a Threshold (EwT) of the ground truth energy. Table <ref type="table" target="#tab_0">1</ref> shows that the SphereNet achieves the best performance on 3 out of 4 splits and the average in terms of energy MAE. For EwT, SphereNet is the best on all the 4 splits. Specifically, it reduces the average energy MAE by 0.019, which is 3.10% of the second best model. In addition, it improves the average EwT from 3.42% to 3.64%, which is a large margin considering the inherently low EwT values.</p><p>Notably, ForceNet <ref type="bibr" target="#b23">(Hu et al., 2021)</ref> and GemNet <ref type="bibr" target="#b29">(Klicpera et al., 2021)</ref> are recently proposed for quantum system learning. A prominent advantage for ForceNet is its high efficiency and scalability to large molecules. ForceNet focuses on S2EF thus there are no original results for the IS2RE task. However, DimeNet++ is slightly better than ForceNet in terms of performance, and our SphereNet outperforms DimeNet++ significantly. GemNet has two variants GemNet-T and GemNet-Q. GemNet-T considers distance and angle information as input, and contains an effective architecture with novel network components, such as bilinear layers and scaling factors. We can see GemNet-T is similar as DimeNet++ in terms of performance. GemNet-Q is claimed to be able to capture universal representations of molecules. However, it considers edge-base 2-hop information and the time complexity is extremely high. It may not be configured properly on the large catalyst molecules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">QM9</head><p>We apply the SphereNet to the QM9 dataset, which is widely used for predicting various properties of molecules. It consists organic molecules composed of up to 9 heavy atoms. Thus, this test can examine the power of the SphereNet for similar quantum chemistry systems. The dataset is original split into three sets, where the training set contains 110,000, the validation set contains 10,000, and the test set contains 10,831 molecules. For energy-related properties, the training processes use the unit eV. All hyperparameters are tuned on the validation set and applied to the test set. We compare our SphereNet with baselines using mean absolute error (MAE) for each property and the overall mean standarized MAE (std. MAE) for all the 12 properties. The comparison results are summarized in Table <ref type="table" target="#tab_1">2</ref>. SphereNets achieves best performance on 5 properties and the second best performance on 3 properties. It also improves the overall mean std. MAE of the QM9 dataset from 0.98 to 0.91 and sets the new state of the art. Notably, the most recent method PaiNN uses the same data splits as SphereNet in terms of sample numbers. Its final performance is the average of three different runs on three random splits. We follow such settings and run SphereNet on four properties including ϵ HOMO , ϵ LUMO , U 0 , and µ. The corresponding results are 22.9 ± 0.2, 18.8 ± 0.2, 6.28 ± 0.05, and 0.0243 ± 0.00, respectively. It is obvious that these results are highly close to those in Table <ref type="table" target="#tab_1">2</ref>, thus, we can draw consistent conclusions. The MD17 dataset is used to examine the expressive power of SphereNet for molecular dynamics simulations. Following the settings in <ref type="bibr" target="#b41">Schütt et al. (2017)</ref>; <ref type="bibr" target="#b28">Klicpera et al. (2020b)</ref>, we train a separate model for each molecule to predict atomic forces. We use 1000 samples for training, and each of the eight molecules has both the validation and test sets. Note that all the baseline models employ a joint loss of forces and conserved energy during training. In the original SchNet <ref type="bibr" target="#b41">(Schütt et al., 2017)</ref> and DimeNet <ref type="bibr" target="#b28">(Klicpera et al., 2020b)</ref> papers, the authors set the weight of force over energy (WoFE) to 100, while the NequIP <ref type="bibr" target="#b3">(Batzner et al., 2021)</ref> and GemNet <ref type="bibr" target="#b29">(Klicpera et al., 2021)</ref> papers use a weight of 1000. As the WoFE tends to affect the force prediction significantly, we perform SphereNet with both WoFE values for fair comparisons. PaiNN <ref type="bibr" target="#b42">(Schütt et al., 2021)</ref> uses neither 100 nor 1000 as WoFE, so we do not compare with it on MD17. The results for forces are reported in Table <ref type="table" target="#tab_2">3</ref>. Note that for Benzene, all models are evaluated on Benzene17, thus, the result for sGDML is 0.20 rather than 0.06 (Benzene18). We can observe from the table that when WoFE is 100 for all models, SphereNet consistently outperforms SchNet and DimeNet by largin margins. Notably, sGDML is one of the original work that created the MD17 dataset with carefully-designed features. Compared with sGDML, SphereNet performs better on four and worse on the other four molecules, which is similar to DimeNet. One reason is sGDML incorporates molecular symmetries to boost precision, and different molecules have different symmetries. However, sGDML has poorer generalization power to larger datasets without hand-engineered features. In addition, SphereNet achieves much better overall std. MAE than sGDML. When using the same WoFE that is 1000, SphereNet achieves similar results with GemNet in spite of that GemNet-T is of high complexity and contains carefully designed network components for performance boost.  <ref type="bibr" target="#b29">Klicpera et al. (2021)</ref>. We conduct experiments on MD17, reporting performance and average running time for all the 8 molecules per epoch using the same computing infrastructure (Nvidia GeForce RTX 2080 TI 11GB). Results are shown in Table <ref type="table" target="#tab_3">4</ref>, from which we can observe that on either backbone network, SMP achieves very similar results with Q-MP. However, the time cost is much less than SMP, which indicates it is much more efficient than Q-MP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">COMPLETENESS VERSUS EFFICIENCY</head><p>Based on analyses in Sec. 3.2, SMP can distinguish almost all molecular structures, and the failure cases may not exist in nature. Hence, SMP performs similarly with Q-MP even though the latter is complete theoretically but not scalable in practice. We further compare the efficiency between SphereNet and other models in terms of parameters and time cost in Appendix F. SphereNet uses similar computing budget with others but achieves the best performance. The proposed SMP considers all the distance, angle, and torsion, leading to more powerful data representations. We investigate contributions of different geometries to demonstrate the advances of our SMP. We remove torsion information from SMP which we denote as "SMP w/o φ"; we further remove angle information which we denote as "SMP w/o (θ, φ)". The three message passing strategies are integrated to the same architecture with other network parts remaining the same. We evaluate these models on four molecules of MD17. Table <ref type="table" target="#tab_4">5</ref> shows that SMP outperforms SMP w/o φ, and SMP w/o φ outperforms "SMP w/o (θ, φ)". These results demonstrate the effectiveness of angle and torsion information used in the SMP. The best performance of SMP further reveals that SMP represents an accurate scheme for 3D graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">ABLATION STUDY</head><p>In addition, we provide visualization results for SphereNet filters in Appendix G to further show that all the distance, angle, and torsion information determine the structural semantics of filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>3D information is important for molecules but there lacks a principled message passing framework to consider it. We first propose the spherical message passing as a unifying and efficient scheme that can achieve approximately complete representations of molecules without increasing computing budget.</p><p>Based on SMP and meaningful physical representations, SphereNet is presented, and experiments on various types of datasets demonstrates its capability, efficiency, and scalibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REPRODUCIBILITY STATEMENT</head><p>Detailed experimental setup is provided in Appendix D. Implementation hyper-parameters of SphereNet on all the three datasets OC20, QM9, and MD17 are given in Table <ref type="table">6</ref>, Table <ref type="table">7</ref>, and Table <ref type="table" target="#tab_6">8</ref>, respectively. Code is integrated in the DIG library <ref type="bibr" target="#b32">(Liu et al., 2021</ref>) and available at https://github.com/divelab/DIG.</p><p>Spherical Message Passing for 3D Molecular Graphs: Appendix</p><formula xml:id="formula_5">Interaction Block Angle Ψ(d, θ ) LB2 LB2 Messages E sk + σ(LB) Message e k σ(LB) Input Block Distance Ψ(d ) v sk v rk | | LB2 e k σ(LB) σ(LB) σ(LB) e k + Output Block LB2 e′ k ⊙ Σ LB2 Residual + Spherical Message Passing σ(LB) σ(LB) ⊙ ⊙ σ(LB) ⊙ Σ Residual x 2 v rk e′ k Distance Ψ(d ) LB2 Torsion Angle Ψ(d, θ, φ) Distance Ψ(d )</formula><p>Figure <ref type="figure">4</ref>: Architecture of SphereNet. LB2 denotes a linear block with two linear layers, σ(LB) denotes a linear layer followed by an activation function, ∥ denotes concatenation, and ⊙ denotes element-wise multiplication. Each LB2 aims at canceling bottlenecks by performing downprojection, followed by upprojection. Hence, it is related to three hyperparameters; these are, input embedding size, intermediate size, and output embedding size. Each linear block LB is related to hyperparameters including input embedding size and output embedding size. Description of each block is in Sec. B. The function ϕ e is applied to each edge k and outputs the updated edge vector e ′ k . The indices of the input geometries to ϕ e are illustrated in Fig. <ref type="figure" target="#fig_2">5 (a)</ref>. Correspondingly, the inputs include the old edge vector e k , the receiver node vector v r k , the sender node vector v s k , the set of edges E s k that point to the node s k , and the 3D position information for all the nodes connected by the edge k and edges in E s k with the index set as r k ∪ s k ∪ N s k . The function ρ p→e aggregates 3D information from these nodes to update the edge k. The function ϕ v is used for per-node update and generates the new node vector v ′ i for each node i. An illustration of the indices of the inputs to ϕ v is provided in Fig. <ref type="figure" target="#fig_2">5 (b</ref>). The inputs include the old node vector v i , the set of edges E ′ i that point to the node i, and 3D information for all the related nodes (the index set is i ∪ N i ). The functions ρ e→v is applied to aggregate the input edge features for updating the node i. The function ϕ u is used to update the global graph feature, while the function ρ v→u aggregates information from all the edge features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A UPDATE FUNCTIONS IN SMP</head><formula xml:id="formula_6">' ' ( ' ) ! ( ! (a) (b) * ( ! + ! ) * )</formula><p>The three information update functions ϕ e , ϕ v , and ϕ u can be implemented in different ways, such as using neural networks and mathematical operations. In SMP, the 3D information in P is converted and incorporated to update each message e k . Hence, SMP uses ρ p→e compute position representations for edges. Note that as absolute Cartesian coordinates stored in P are not invariant to translation and rotation, they are not used as immediate inputs to machine learning models. The position aggregation function can be flexibly adapted to generate invariant representations. For example, ρ p→e in Eq. ( <ref type="formula" target="#formula_1">1</ref>) can be adapted to a spherical Bessel basis function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B INFORMATION UPDATE AND ARCHITECTURE OF SPHERENET</head><p>we assume the message e k for the edge k is the center message for update. The input block generates the initial message for the edge k, and takes only the distance representation Ψ(d) as the input. Each interaction block updates the message for the edge k. The inputs include messages for all the neighboring edges, and all three representations, including Ψ(d, θ, φ), Ψ(d, θ), and Ψ(d) based on the edge k and its neighboring edges. The output block first takes both the distance representation and the current message for k as inputs. Then the feature vector of the receiver atom for the edge k (atom r k in Fig. <ref type="figure" target="#fig_1">2</ref> and Eq. ( <ref type="formula" target="#formula_1">1</ref>)) is obtained by aggregating all the messages pointing to it, where other messages have a similar update process as e k .</p><p>Detailed architecture of SphereNet is provided in Fig. <ref type="figure">4</ref>. Specifically, SphereNet is composed of an input block, followed by multiple interaction blocks and output blocks. For the purpose of simplicity, the architecture is explained by updating the receiver note r k of the message e k , as described in Eq. ( <ref type="formula" target="#formula_1">1</ref>) and Sec. 4 in main paper.</p><p>Input Block aims at constructing initial message e k for the edge k. Inputs include the distance representation Ψ(d) for edge k, initial node embeddings v s k , v r k for the sender node s k , and the receiver node r k . The distance information is encoded by using a LB2 block. Interaction Block updates the message e k with incorporating all the three physical representations. Input 3D information includes the distance embedding Ψ(d), the angle Ψ(d, θ), and the torsion Ψ(d, θ, φ). The initial embedding sizes for them are L, N × L, and N 2 × L, respectively. Other inputs are old message e k and the set of messages E s k that point to the sender node s k . Similar to the input block, each type of 3D information is encoded by using a block LB2. Note that each ⊙ indicates the element-wise multiplication between the corresponding 3D information represented as a vector and each message in the set E s k . Thus, each neighboring message of e k is gated by the encoded 3D information. The aggregates all the gated messages in E s k to a vector, which is added to the transformation of the old message e k as the updated message e ′ k . The transformation branch for old message e k is composed of several nonlinear layers and residual blocks, as shown in Fig. <ref type="figure">4</ref>. Output Block aggregates all the incoming messages to update the feature for node r k . Each incoming message has the same update process as e k by interaction blocks. For the purpose of clear illustration, we use e ′ k to represent each updated incoming message, which is further gated by the distance representation vector Ψ(d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C RELATIONS WITH PRIOR MESSAGE PASSING METHODS</head><p>Our SMP is a specific architecture for 3D graphs and is formally defined in Eq. (1). Especially, the message passing schemes used in some existing models can be viewed as special cases of SMP as they only encode partial 3D information. In this section, we clearly give the used functions as well as their inputs for each existing method. Basically, we describe how each method realizes Eq. (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 SCHNET, SCHÜTT ET AL. (2017)</head><p>In SchNet, the used aggregation function to encode 3D position information is ρ p→e ({r h } h=r k ∪s k ) = Ψ (∥r r k − r s k ∥), which converts the position information to an embedding of distance with radial basis functions. In addition to the ρ p→e function, the ϕ e function used is NN (NN (v r k ) ⊙ NN (Ψ (∥r r k − r s k ∥))), where NN denotes a neural network and ⊙ denotes the element-wise multiplication. The ρ e→v function is</p><formula xml:id="formula_7">(e ′ k ,r k ,s k )∈E ′ i e ′ k . The ϕ v function is v i + (e ′ k ,r k ,s k )∈E ′ i e ′ k .</formula><p>The global feature u is updated based on the final node features V T and the function is ϕ u = i=1:n NN v T i . Formally, the update process is expressed as</p><formula xml:id="formula_8">e ′ k =ϕ e (v r k , ρ p→e ({r h } h=r k ∪s k )) =ϕ e (v r k , Ψ (∥r s k − r r k ∥)) =NN (NN (v r k ) ⊙ NN (Ψ (∥r r k − r s k ∥))) , v ′ i =ϕ v (v i , ρ e→v (E ′ i )) =ϕ v   vi, (e ′ k ,r k ,s k )∈E ′ i e ′ k    =v i + (e ′ k ,r k ,s k )∈E ′ i e ′ k , u =ϕ u ρ v→u V T = i=1:n NN v T i .<label>(2)</label></formula><p>C.2 PHYSNET, UNKE &amp; MEUWLY (2019)</p><p>PhysNet uses distance between atoms as an important feature and proposes more powerful neural networks for chemical applications. The position aggregation function is</p><formula xml:id="formula_9">ρ p→e ({r h } h=r k ∪s k ) = Ψ (∥r r k − r s k ∥)</formula><p>, where Ψ is any radial basis function with a smooth cutoff. For the information update functions, the ϕ e function is σ</p><formula xml:id="formula_10">(W 1 ) σ (v s k ) ⊙ W 2 Ψ (∥r r k − r s k ∥), the ϕ v function is NN W 3 ⊙ v i + NN σ (W 4 ) σ (v i ) + (e ′ k ,r k ,s k )∈E ′ i e ′ k and the ϕ u function is u + i=1:n NN (v ′ i ).</formula><p>Here NN denotes a neural network, W 1 , W 2 , W 3 , W 4 are learnable weight matrices, σ is an activate function, and ⊙ denotes the element-wise multiplication. PhysNet is expressed as </p><formula xml:id="formula_11">e ′ k =ϕ e (v s k , ρ p→e ({r h } h=r k ∪s k )) =ϕ e (v s k , Ψ (∥r r k − r s k ∥)) =σ (W 1 ) σ (v s k ) ⊙ W 2 Ψ (∥r r k − r s k ∥) , v ′ i =ϕ v (v i , ρ e→v (E ′ i )) =ϕ v   vi, (e ′ k ,r k ,s k )∈E ′ i e ′ k    =NN   W3 ⊙ v i + NN   σ (W 4 ) σ (v i ) + (e ′ k ,r k ,s k )∈E ′ i e ′ k       , u ′ =ϕ u (ρ v→u (V ′ ) , u) =u + i=1:n NN (v ′ i ) .<label>(3)</label></formula><formula xml:id="formula_12">′ k = e ′ k,1 ∥e ′ k,2 with e ′ k,1 = NN e k,1 + NN σW 1 e k,1 + (ej ,rj ,sj )∈Es k W 2 Ψ (d j , θ jk ) (W 3 Ψ (d j ) ⊙ σW 4 e j,1</formula><p>)</p><formula xml:id="formula_13">and e ′ k,2 = W 5 Ψ (d j ) ⊙ e ′ k,1</formula><p>, where NN denotes a neural network, W 1 , W 2 , W 3 , W 4 , W 5 are different weight matrices, and σ is an activation function. The ρ e→v function is</p><formula xml:id="formula_14">(e ′ k ,r k ,s k )∈E ′ i e ′ k,2 and the ϕ v is NN (e ′ k ,r k ,s k )∈E ′ i e ′ k,2</formula><p>. The ρ v→u is i=1:n v ′ i and the ϕ u is u + i=1:n v ′ i . Note that ρ p→v , ρ p→u , ρ e→u functions are not required in DimeNet. The whole model is expressed as</p><formula xml:id="formula_15">e k = (e k,1 ∥e k,2 ) , ρ p→e = (Ψ (d) ∥Ψ (d, θ)) , e ′ k,1 =ϕ e e k , E s k , ρ p→e {r h } h=r k ∪s k ∪Ns k =NN   e k,1 + NN   σW 1 e k,1 + (ej ,rj ,sj )∈Es k W 2 Ψ (d j , θ jk ) (W 3 Ψ (d j ) ⊙ σW 4 e j,1 )     , e ′ k,2 =W 5 Ψ (d j ) ⊙ e ′ k,1 , v ′ i =ϕ v (ρ e→v (E ′ i )) =NN    (e ′ k ,r k ,s k )∈E ′ i e ′ k,2    , u ′ =ϕ u (u, ρ v→u (V ′ )) =u + i=1:n v ′ i .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D EXPERIMENTAL SETUP</head><p>For all the models used in three datasets, we set input embedding size = 256 and output embedding size = 64 for both LB2 and LB blocks. For each separate model, we first perform warmup on initial learning rate. Then two learning rate strategies, including ReduceLROnPlateau and StepLR, are used for training. For StepLR, the learning rate is decayed by the decay ratio every fixed epochs represented as step size. We do not use weight decay or dropout for all models. Some hyperparameters are fixed values, and some are tuned by grid search. Values/search space of hyperparameters for OC20, QM9, and MD17 are provided in Table <ref type="table">6</ref>, Table <ref type="table">7</ref>, and Table <ref type="table" target="#tab_6">8</ref>, respectively. Optimized hyperparameters are tuned on validation sets and applied to test sets for QM9 and MD17. For OC20, optimized hyperparameters are obtained on the ID split within max epochs, and then applied to the other three splits. Pytorch is used to implement all methods. For QM9 and MD17 datasets, all models are trained using one NVIDIA GeForce RTX 2080 Ti 11GB GPU. For the OC20 dataset, all models are trained using four NVIDIA RTX A6000 48GB GPUs.</p><p>E OC20 DATA DESCRIPTION  <ref type="bibr">24,</ref><ref type="bibr">943,</ref><ref type="bibr">24,</ref><ref type="bibr">961,</ref><ref type="bibr">24,</ref><ref type="bibr">963,</ref><ref type="bibr">24,</ref><ref type="bibr">987</ref>, respectively. The average number of atoms per structure is 77.75.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F EFFICIENCY STUDY OF SPHERENET</head><p>We study the efficiency of SphereNet by comparing with other models regarding number of parameters and time cost per epoch using the same computing infrastructure (Nvidia GeForce RTX 2080 TI </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G SPHERENET FILTER VISUALIZATION</head><p>We visualize SphereNet filters from a learned SphereNet model. Specifically, we port learned weights for the block LB2 after the torsion embedding Ψ(d, θ, φ) in Fig. <ref type="figure">4</ref>. For each location represented by a tuple (d, θ, φ), the initial embedding size is N 2 × L. The computation for the above LB2 is W 1 (W 2 Ψ(d, θ, φ)), which results in the new embedding size of 64 for each location (d, θ, φ). We then perform sampling on locations in 3D space for visualizing weights as SphereNet filters. The visualization results are provided in Fig. <ref type="figure" target="#fig_4">6</ref>. We set sampling rate in the torsion direction to be π/4, thus, there are eight samples in the torsion direction. There are totally 64 elements for each location, and we randomly pick 6 elements. Apparently, among the distance, angle and torsion, considering any one when fixing the other two, the structural value of filters will be different when the one of interest changes. It essentially shows that all the distance, angle, and torsion information determine the structural semantics of filters. This further demonstrates that SMP enables the learning of different 3D information for improved representations. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The chemical structure of the H 2 O 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a). The message aggregation scheme for the spherical message passing. (b). An illustration for computing torsion angles in the spherical message passing architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Illustrations of the functions ϕ e (a) and ϕ v (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>C. 3</head><label>3</label><figDesc>DIMENET, KLICPERA ET AL. (2020B) DimeNet explicitly considers distances between atoms and directions of directed edges. The aggregation functions on the position information is ρ p→e = (Ψ (d) ∥Ψ (d, θ)), where ∥ denotes concatenation, Ψ (d) and Ψ (d, θ) are the same basis functions used in SphereNet as introduced in Sec. 4. Specifically, Ψ (d) denotes the representation of the distance based on spherical Bessel function, and Ψ (d, θ) denotes the representation of distance and angle based on spherical Bessel function and spherical harmonics. For other functions, the ϕ e function used is e</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visualization of six SphereNet filters. Each row corresponds to a filter with torsion angles 0, π/4, π/2, 3π/4, π, 5π/4, 3π/2, and 7π/4 from left to right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparisons between SphereNet and other models on IS2RE in terms of energy MAE and the percentage of EwT of the ground truth energy. Results reported for models trained on the All training dataset. The best results are shown in bold.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Energy MAE [eV] ↓</cell><cell></cell><cell></cell><cell></cell><cell>EwT ↑</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>ID</cell><cell cols="4">OOD Ads OOD Cat OOD Both Average</cell><cell>ID</cell><cell cols="4">OOD Ads OOD Cat OOD Both Average</cell></row><row><cell>CGCNN</cell><cell>0.6203</cell><cell>0.7426</cell><cell>0.6001</cell><cell>0.6708</cell><cell>0.6585</cell><cell>3.36%</cell><cell>2.11%</cell><cell>3.53%</cell><cell>2.29%</cell><cell>2.82%</cell></row><row><cell>SchNet</cell><cell>0.6465</cell><cell>0.7074</cell><cell>0.6475</cell><cell>0.6626</cell><cell>0.6660</cell><cell>2.96%</cell><cell>2.22%</cell><cell>3.03%</cell><cell>2.38%</cell><cell>2.65%</cell></row><row><cell cols="2">DimeNet++ 0.5636</cell><cell>0.7127</cell><cell>0.5612</cell><cell>0.6492</cell><cell>0.6217</cell><cell>4.25%</cell><cell>2.48%</cell><cell>4.40%</cell><cell>2.56%</cell><cell>3.42%</cell></row><row><cell>GemNet-T</cell><cell>0.5561</cell><cell>0.7342</cell><cell>0.5659</cell><cell>0.6964</cell><cell>0.6382</cell><cell>4.51%</cell><cell>2.24%</cell><cell>4.37%</cell><cell>2.38%</cell><cell>3.38%</cell></row><row><cell>SphereNet</cell><cell>0.5632</cell><cell>0.6682</cell><cell>0.5590</cell><cell>0.6190</cell><cell>0.6024</cell><cell>4.56%</cell><cell>2.70%</cell><cell>4.59%</cell><cell>2.70%</cell><cell>3.64%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparisons between SphereNet and other models in terms of MAE and the overall mean std. MAE on QM9. '-' denotes no results are reported in the referred papers for the corresponding properties. The best results are shown in bold and the second best results are shown with underlines.</figDesc><table><row><cell cols="12">Property Unit PPGN SchNet PhysNet Cormorant MGCN DimeNet DimeNet++ PaiNN SphereNet</cell></row><row><cell>µ</cell><cell>D</cell><cell></cell><cell cols="3">0.047 0.033 0.0529</cell><cell>0.13</cell><cell cols="2">0.0560 0.0286</cell><cell>0.0297</cell><cell cols="2">0.012 0.0245</cell></row><row><cell>α</cell><cell>a 0</cell><cell>3</cell><cell cols="3">0.131 0.235 0.0615</cell><cell cols="3">0.092 0.0300 0.0469</cell><cell>0.0435</cell><cell cols="2">0.045 0.0449</cell></row><row><cell>ϵ HOMO</cell><cell cols="2">meV</cell><cell>40.3</cell><cell>41</cell><cell>32.9</cell><cell>36</cell><cell>42.1</cell><cell>27.8</cell><cell>24.6</cell><cell>27.6</cell><cell>22.8</cell></row><row><cell>ϵ LUMO</cell><cell cols="2">meV</cell><cell>32.7</cell><cell>34</cell><cell>24.7</cell><cell>36</cell><cell>57.4</cell><cell>19.7</cell><cell>19.5</cell><cell>20.4</cell><cell>18.9</cell></row><row><cell>∆ϵ</cell><cell cols="2">meV</cell><cell>60.0</cell><cell>63</cell><cell>42.5</cell><cell>60</cell><cell>64.2</cell><cell>34.8</cell><cell>32.6</cell><cell>45.7</cell><cell>31.1</cell></row><row><cell>R 2</cell><cell>a 0</cell><cell>2</cell><cell cols="3">0.592 0.073 0.765</cell><cell>0.673</cell><cell cols="2">0.110 0.331</cell><cell>0.331</cell><cell>0.066</cell><cell>0.268</cell></row><row><cell>ZPVE</cell><cell cols="2">meV</cell><cell>3.12</cell><cell>1.7</cell><cell>1.39</cell><cell>1.98</cell><cell>1.12</cell><cell>1.29</cell><cell>1.21</cell><cell>1.28</cell><cell>1.12</cell></row><row><cell>U 0</cell><cell cols="2">meV</cell><cell>36.8</cell><cell>14</cell><cell>8.15</cell><cell>28</cell><cell>12.9</cell><cell>8.02</cell><cell>6.32</cell><cell>5.85</cell><cell>6.26</cell></row><row><cell>U</cell><cell cols="2">meV</cell><cell>36.8</cell><cell>19</cell><cell>8.34</cell><cell>-</cell><cell>14.4</cell><cell>7.89</cell><cell>6.28</cell><cell>5.83</cell><cell>6.36</cell></row><row><cell>H</cell><cell cols="2">meV</cell><cell>36.3</cell><cell>14</cell><cell>8.42</cell><cell>-</cell><cell>14.6</cell><cell>8.11</cell><cell>6.53</cell><cell>5.98</cell><cell>6.33</cell></row><row><cell>G</cell><cell cols="2">meV</cell><cell>36.4</cell><cell>14</cell><cell>9.40</cell><cell>-</cell><cell>16.2</cell><cell>8.98</cell><cell>7.56</cell><cell>7.35</cell><cell>7.78</cell></row><row><cell>c v</cell><cell cols="5">cal mol K 0.055 0.033 0.0280</cell><cell cols="3">0.031 0.0380 0.0249</cell><cell>0.0230</cell><cell cols="2">0.024 0.0215</cell></row><row><cell cols="2">std. MAE %</cell><cell></cell><cell cols="2">1.84 1.76</cell><cell>1.37</cell><cell>2.14</cell><cell>1.86</cell><cell>1.05</cell><cell>0.98</cell><cell>1.01</cell><cell>0.91</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparisons between SphereNets and other models in terms MAE of forces on MD17. WoFE indicates weight of force over energy in loss functions. Results of all baseline models are directed taken or adapted (if the unit varies) from the original papers, and SphereNet uses two WoFEs in line with the original papers of different baselines for fair comparisons. The best results are shown in bold and the second best results are shown with underlines.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">WoFE = 100</cell><cell></cell><cell></cell><cell cols="2">WoFE = 1000</cell><cell></cell></row><row><cell>Molecule</cell><cell cols="8">sGDML SchNet DimeNet SphereNet NequIP GemNet-T GemNet-Q SphereNet</cell></row><row><cell>Aspirin</cell><cell>0.68</cell><cell>1.35</cell><cell>0.499</cell><cell>0.430</cell><cell>0.353</cell><cell>0.220</cell><cell>0.217</cell><cell>0.209</cell></row><row><cell>Benzene</cell><cell>0.20</cell><cell>0.31</cell><cell>0.187</cell><cell>0.178</cell><cell>0.186</cell><cell>0.145</cell><cell>0.145</cell><cell>0.147</cell></row><row><cell>Ethanol</cell><cell>0.33</cell><cell>0.39</cell><cell>0.230</cell><cell>0.208</cell><cell>0.204</cell><cell>0.086</cell><cell>0.088</cell><cell>0.091</cell></row><row><cell cols="2">Malonaldehyde 0.41</cell><cell>0.66</cell><cell>0.383</cell><cell>0.340</cell><cell>0.328</cell><cell>0.155</cell><cell>0.160</cell><cell>0.172</cell></row><row><cell>Naphthalene</cell><cell>0.11</cell><cell>0.58</cell><cell>0.215</cell><cell>0.178</cell><cell>0.105</cell><cell>0.055</cell><cell>0.051</cell><cell>0.048</cell></row><row><cell>Salicylic acid</cell><cell>0.28</cell><cell>0.85</cell><cell>0.374</cell><cell>0.360</cell><cell>0.242</cell><cell>0.127</cell><cell>0.125</cell><cell>0.113</cell></row><row><cell>Toluene</cell><cell>0.14</cell><cell>0.57</cell><cell>0.216</cell><cell>0.155</cell><cell>0.102</cell><cell>0.060</cell><cell>0.060</cell><cell>0.054</cell></row><row><cell>Uracil</cell><cell>0.24</cell><cell>0.56</cell><cell>0.301</cell><cell>0.267</cell><cell>0.173</cell><cell>0.097</cell><cell>0.104</cell><cell>0.106</cell></row><row><cell>std. MAE</cell><cell>1.11</cell><cell>2.38</cell><cell>1.10</cell><cell>0.97</cell><cell>0.79</cell><cell>0.45</cell><cell>0.45</cell><cell>0.44</cell></row><row><cell>6.4 MD17</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparisons bewtween SMP and Q-MP on MD17 using two backbone networks.</figDesc><table><row><cell>The message passing scheme Q-MP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>in GemNet represents the edge-based</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2-hop geometric message passing and can generate complete representations</cell><cell></cell><cell cols="4">SphereNet Backbone GemNet Backbone</cell></row><row><cell>of 3D molecular graphs. We study the</cell><cell>Molecule</cell><cell>SMP</cell><cell>Q-MP</cell><cell>SMP</cell><cell>Q-MP</cell></row><row><cell>capability and efficiency of the pro-posed SMP by comparing with Q-MP. Specifically, We use the same back-bone network for these two MP meth-ods for fair comparisons. We exten-sively use two backbones, which are the SphereNet backbone introduced in Sec. 4 and the GemNet backbone pro-posed in</cell><cell cols="2">Aspirin Benzene Ethanol Malonaldehyde 0.172 0.209 0.147 0.091 Naphthalene 0.048 Salicylic acid 0.113 Toluene 0.054 Uracil 0.106 Time/Epoch (s) 324</cell><cell>0.247 0.153 0.102 0.168 0.057 0.125 0.043 0.106 1270</cell><cell>0.225 0.144 0.089 0.169 0.063 0.111 0.052 0.098 295</cell><cell>0.231 0.149 0.083 0.176 0.062 0.114 0.063 0.113 1185</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparisons among three message passing strategies on the same SphereNet architecture on the partial MD17 dataset.</figDesc><table><row><cell>Molecule</cell><cell cols="3">SMP w/o (θ, φ) SMP w/o φ SMP</cell></row><row><cell>Ethanol</cell><cell>0.249</cell><cell>0.22</cell><cell>0.208</cell></row><row><cell>Malonaldehyde</cell><cell>0.550</cell><cell>0.360</cell><cell>0.340</cell></row><row><cell>Naphthalene</cell><cell>0.372</cell><cell>0.205</cell><cell>0.178</cell></row><row><cell>Toluene</cell><cell>0.446</cell><cell>0.182</cell><cell>0.155</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>There exist three tasks including S2EF, IS2RS, and IS2RE. In this work, we focus on IS2RE that predicts structure's energy in the relaxed state. It is the most common task in catalysis as relaxed energies usually influence the catalyst activity. The dataset for IS2RE is originally split into training/validation/test sets. There are 460,318 structures in the training dataset in total. The test label is not publicly available. Performance is evaluated on the validation set, which has four splits including In Domain (ID), Out of Domain Adsorbates (OOD Ads), Out of Domain catalysts (OOD cat), and Out of Domain Adsorbates and catalysts (OOD Both), where numbers of structures are</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Values/search space for hyperparameters on MD17.</figDesc><table><row><cell>Hyperparameters</cell><cell>Values/search space</cell></row><row><cell>Interaction block -distance LB2 intermediate size</cell><cell>4, 8, 16</cell></row><row><cell>Interaction block -angle LB2 intermediate size</cell><cell>4, 8, 16</cell></row><row><cell>Interaction block -torsion LB2 intermediate size</cell><cell>4, 8, 16</cell></row><row><cell># of interaction blocks</cell><cell>2, 3, 4, 5</cell></row><row><cell># of RBFs N</cell><cell>6</cell></row><row><cell># of spherical harmonics L</cell><cell>3, 5, 7</cell></row><row><cell>Cutoff distance</cell><cell>4, 5, 6</cell></row><row><cell>Batch size</cell><cell>1, 2, 4, 16, 32</cell></row><row><cell>Initial learning rate</cell><cell>1e-4, 5e-4, 1e-3</cell></row><row><cell>Learning rate strategy</cell><cell>StepLR</cell></row><row><cell>Learning rate decay ratio</cell><cell>0.4, 0.5, 0.6</cell></row><row><cell>Learning rate step size</cell><cell>50, 100, 200</cell></row><row><cell>Max # of Epochs</cell><cell>500, 1000, 2000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>Efficiency comparisons between SphereNet and other models in terms of number of parameters and time cost per epoch using the same infrastructure.</figDesc><table><row><cell></cell><cell cols="5">SchNet DimeNet DimeNet++ GemNet-T SphereNet</cell></row><row><cell cols="4">#Param. 185,153 2100,070 1887,110</cell><cell cols="2">2040,194 1898,566</cell></row><row><cell>Time (s)</cell><cell>100</cell><cell>840</cell><cell>240</cell><cell>290</cell><cell>340</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/Open-Catalyst-Project/ocp</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported in part by National Science Foundation grant IIS-1908198 and National Institutes of Health grant 1R21NS102828. We thank Hannes Stärk for his valuable suggestions and discussions when developing the methods.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>). Experiments are conducted on the property U 0 of QM9 and results are shown in Table <ref type="table">9</ref>.</p><p>It is obvious that SphereNet uses similar computational resources as DimeNet++ and GemNet-T, and is much more efficient than DimeNet. The main reason could be we develop an efficient way to compute torsion, as introduced in Sec. <ref type="bibr">3 and Fig. 2 (b)</ref>. Moreover, GemNet-Q cannot run on QM9 using the infrastructure as mentioned above.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cormorant: Covariant molecular neural networks</title>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Truong-Son</forename><surname>Hy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33st International Conference on Neural Information Processing Systems</title>
				<meeting>the 33st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="14537" to="14546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Geom: Energy-annotated molecular conformations for property prediction and molecular generation</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Gomez-Bombarelli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05531</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Peter W Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Se (3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Batzner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tess</forename><forename type="middle">E</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">P</forename><surname>Mailoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mordechai</forename><surname>Kornbluth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Molinari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Kozinsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03164</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taco</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><surname>Veličković</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13478</idno>
		<title level="m">Geometric deep learning: Grids, groups, graphs, geodesics, and gauges</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Lowik</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammed</forename><surname>Shuaibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgane</forename><surname>Riviere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Heras-Domingo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caleb</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09990</idno>
		<title level="m">The open catalyst 2020 (oc20) dataset and community challenges</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Clusternet: Deep hierarchical cluster network with rigorously rotation-invariant representation for point cloud analysis</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruijia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
				<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4994" to="5002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Machine learning of accurate energy-conserving molecular force fields</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Huziel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Sauceda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristof</forename><forename type="middle">T</forename><surname>Poltavsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science advances</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">e1603015</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards exact molecular dynamics simulations with machine-learned force fields</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Huziel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Sauceda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><surname>Tkatchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gauge equivariant convolutional networks and the icosahedral cnn</title>
		<author>
			<persName><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berkay</forename><surname>Kicanaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gauge equivariant mesh cnns: Anisotropic convolutions on geometric graphs</title>
		<author>
			<persName><forename type="first">Pim</forename><surname>De Haan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Protein interface prediction using graph convolutional networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basir</forename><surname>Shariat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asa</forename><surname>Ben-Hur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
				<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6533" to="6542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Se (3)-transformers: 3d rototranslation equivariant attention networks</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volker</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph U-nets</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 36th International Conference on Machine Learning</title>
				<meeting>The 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large-scale learnable graph convolutional networks</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Topology-aware graph pooling networks</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09834</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generalization and representational limits of graph neural networks</title>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3419" to="3430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Simple gnn regularisation for 3d molecular property prediction and beyond</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Godwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schaarschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">L</forename><surname>Gaunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE International Joint Conference on Neural Networks</title>
				<meeting>2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Introduction to quantum mechanics</title>
		<author>
			<persName><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darrell</forename><forename type="middle">F</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><surname>Schroeter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep learning for 3d point clouds: A survey</title>
		<author>
			<persName><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Forcenet: A graph neural network for large-scale quantum calculations</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammed</forename><surname>Shuaibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anuroop</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01436</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generative models for graphbased protein design</title>
		<author>
			<persName><forename type="first">John</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15794" to="15805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast and uncertainty-aware directional message passing for non-equilibrium molecules</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shankari</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><forename type="middle">T</forename><surname>Margraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS-W</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Directional message passing for molecular graphs</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janek</forename><surname>Groß</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Gemnet: Universal directional graph neural networks for molecules</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08903</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Point cloud oversegmentation with graph-structured deep metric learning</title>
		<author>
			<persName><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Boussaha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7440" to="7449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dig: a turnkey library for diving into graph deep learning research</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youzhi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaochen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shurui</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingtun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">240</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">N-gram graph: Simple unsupervised representation for graphs, with applications to molecules</title>
		<author>
			<persName><forename type="first">Shengchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehmet</forename><forename type="middle">F</forename><surname>Demirel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8464" to="8476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep learning of high-order interactions for protein interface prediction</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="679" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Molecular property prediction: A multilevel quantum interactions modeling perspective</title>
		<author>
			<persName><forename type="first">Chengqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenya</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peize</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lixin</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1052" to="1060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Provably powerful graph networks</title>
		<author>
			<persName><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadar</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="2153" to="2164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deepsphere: Efficient spherical convolutional neural network with healpix sampling for cosmological applications</title>
		<author>
			<persName><forename type="first">Nathanaël</forename><surname>Perraudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Kacprzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Sgier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Astronomy and Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="130" to="146" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Orbnet: Deep learning for quantum chemistry using symmetry-adapted atomic-orbital features</title>
		<author>
			<persName><forename type="first">Zhuoran</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Welborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animashree</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Frederick R Manby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">124111</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Quantum chemistry structures and properties of 134 kilo molecules</title>
		<author>
			<persName><forename type="first">Raghunathan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Pavlo O Dral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Von</forename><surname>Anatole</surname></persName>
		</author>
		<author>
			<persName><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to simulate complex physics with graph networks</title>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Godwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8459" to="8468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Schnet: A continuous-filter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName><forename type="first">Kristof</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huziel</forename><surname>Enoc Sauceda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="991" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Equivariant message passing for the prediction of tensorial properties and molecular spectra</title>
		<author>
			<persName><surname>Kristof T Schütt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Oliver T Unke</surname></persName>
		</author>
		<author>
			<persName><surname>Gastegger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.03150</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mining point cloud local structures by kernel correlation and graph pooling</title>
		<author>
			<persName><forename type="first">Yiru</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Reinforcement learning for molecular design guided by quantum mechanics</title>
		<author>
			<persName><forename type="first">Gregor</forename><surname>Simm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Pinsler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Miguel Hernández-Lobato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8959" to="8969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Nathaniel</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tess</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lusann</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Kohlhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08219</idno>
		<title level="m">Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">End-to-end learning on 3d protein structure for interface prediction</title>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Townshend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Suriana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Dror</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="15642" to="15651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Physnet: A neural network for predicting energies, forces, dipole moments, and partial charges</title>
		<author>
			<persName><forename type="first">T</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Unke</surname></persName>
		</author>
		<author>
			<persName><surname>Meuwly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical theory and computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3678" to="3693" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Building powerful and equivariant graph neural networks with message-passing</title>
		<author>
			<persName><forename type="first">Clement</forename><surname>Vignac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15107</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Graph attention convolution for point cloud semantic segmentation</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaolin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenman</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="page" from="10296" to="10305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions On Graphics (tog)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Advanced graph and sequence neural networks for molecular property prediction and drug discovery</title>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youzhi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaochen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.01981</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName><forename type="first">Zhenqin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caleb</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aneesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">C</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">145301</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
