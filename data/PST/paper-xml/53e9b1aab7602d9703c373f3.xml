<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Balanced Scheduling: Instruction Scheduling When Memory Latency is Uncertain</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">R</forename><surname>Kerns</surname></persName>
							<email>kerns@pobox.com</email>
							<affiliation key="aff0">
								<orgName type="department">Sand Point Engineering Mercer Island</orgName>
								<address>
									<postCode>98040</postCode>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Susan</forename><forename type="middle">J</forename><surname>Eggers</surname></persName>
							<email>eggers@cs.washington.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Seattle</orgName>
								<address>
									<postCode>98115</postCode>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Balanced Scheduling: Instruction Scheduling When Memory Latency is Uncertain</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Both of us came to the problem of scheduling instructions from the perspective of computer architects, rather than compiler writers. Susan Eggers was an Assistant Professor, whose previous research was primarily in the design and performance of bus-based shared memory multiprocessors. A major concern in building these machines was avoiding operations and reducing latencies of instructions that utilized the machine's major bottleneck, the processor-memory interconnect. In these days this interconnect was a very slow, one-operation-at-a-time bus with a keep-the-bus communication protocol. Daniel Kerns was a graduate student, whose interests and previous experience were in designing instruction set microcode and microcode scheduling tools for Data General minicomputers and the Multiflow Trace.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Balanced Scheduling algorithm was developed in the context of single-issue, in-order processors, whose instruction issue logic scheduled instructions in their program-generated order. Recent innovations in the memory subsystem, in the form of nonblocking caches, provided these processors with some latency-hiding capability for long memory accesses. When a nonblocking cache missed on a data load, it returned execution control back to the CPU, which executed the instructions behind the load until it encountered the load use instruction. At the same time, the nonblocking cache obtained the load data from another level in the memory hierarchy. The parallel activity hid much or all of the load latency from the executing program.</p><p>Compiler technology at the time assumed that operation latencies were not only fixed, but optimistically fixed, gambling that the shortest latency option would always occur. A classic example was the assumption that a load would always hit in the level-one cache and the delay to obtaining its value would always be the time of a cache hit. If the assumption turned out to be wrong, the actual latency might be a hundred times longer, i.e., the time required to access higher levels of the memory hierarchy, including a nondeterministic amount for bus and memory contention. Given the small size of level-one caches at the time (around 4KB), the optimistic value used by the code scheduler could not be counted on a sizeable percentage of the time. Since the bus was the only path to memory, memory contention was probable. And given the discrepancy between hit and miss times, misses had a large, negative effect on performance.</p><p>20 Years of the ACM/SIGPLAN Conference on Programming Language <ref type="bibr">Design and Implementation (1979</ref><ref type="bibr">-1999</ref><ref type="bibr">): A Selection, 2003</ref><ref type="bibr">. Copyright 2003 ACM 1-58113-623-4 $5.00</ref> This presented a dilemma for compiler technology that was based on fixed latencies. If a code scheduler optimistically assumed that loads would hit in the level-one cache and therefore scheduled loads and uses back-to-back, the processor stalled on cache misses; if it pessimistically assumed that loads missed and scheduled independent instructions between loads and their first use, it would needlessly tie up load destination registers, perhaps inducing register spills.</p><p>Given our computer architecture perspective, this seemed to present an opportunity to the compiler. We decided to tackle the problem of scheduling code for dynamically determined latencies by exposing both the variation in instruction latencies and the concurrency in the memory subsystem hardware to the compiler. With this knowledge, the code scheduler could distribute independent instructions behind the loads, in essence covering as many uncertain cases as possible.</p><p>The approach presented a change of view for code schedulers in two different respects. The first was a shift away from simply detecting dependences between instructions and prioritizing instruction scheduling to execute critical paths quickly. The second shifted from basing scheduling priorities on fixed latencies of a particular implementation. Instead the code scheduler focused on detecting available concurrency in the code and using it to take advantage of concurrency in the underlying hardware. The result was an instruction scheduler that was more tolerant of variable memory latencies and was robust across different memory subsystem implementations. It was especially effective when coupled with compiler optimizations that increased ILP [LE95].</p><p>We know of some impact in the industrial sector. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Balanced Scheduling was incorporated into compilers at Intel Corp. and Tera Computer Co. (now Cray Computers) and into the memory-safe software at Colusa Inc. (now part of Microsoft Corp.)</figDesc><table><row><cell>References</cell></row><row><cell>[LE95] J.L. Lo and S.J. Eggers, "Improving Balanced</cell></row><row><cell>Scheduling with Compiler Optimizations that Increase</cell></row><row><cell>Instruction-level Parallelism," International Symposium on</cell></row><row><cell>Programming Language Design and Implementation (June,</cell></row><row><cell>1995).</cell></row></table><note>ACM SIGPLAN 515Best ofPLDI 1979PLDI  -1999     </note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Best of PLDI 1979-1999   </note>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
