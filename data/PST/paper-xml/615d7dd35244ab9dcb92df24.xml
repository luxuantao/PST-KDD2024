<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Effective gene expression prediction from sequence by integrating long-range interactions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Å½iga</forename><surname>Avsec</surname></persName>
							<email>avsec@google.com</email>
							<idno type="ORCID">0000-0002-7790-8936</idno>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vikram</forename><surname>Agarwal</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Calico Life Sciences</orgName>
								<orgName type="institution" key="instit2">South San Francisco</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Visentin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joseph</forename><forename type="middle">R</forename><surname>Ledsam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<address>
									<settlement>Google, Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Agnieszka</forename><surname>Grabskabarwinsk</surname></persName>
							<idno type="ORCID">0000-0001-7782-3548</idno>
						</author>
						<author>
							<persName><forename type="first">Kyle</forename><forename type="middle">R</forename><surname>Taylor</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yannis</forename><surname>Assael</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">John</forename><surname>Jumper</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
							<email>pushmeet@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Kelley</surname></persName>
							<idno type="ORCID">0000-0001-7782-3548</idno>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Calico Life Sciences</orgName>
								<orgName type="institution" key="instit2">South San Francisco</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Effective gene expression prediction from sequence by integrating long-range interactions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1038/s41592-021-01252-x</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>odels that predict gene expression and chromatin states from DNA sequences hold the promise to better understand transcriptional regulation and how it is affected by the many noncoding genetic variants associated with human diseases and traits. These models complement population-based association studies, which are often limited to common variants and struggle to disentangle causality from association due to linkage disequilibrium (LD). Additionally, experimental validation of human genetic variants is laborious and limited to cell types or tissues that can be recapitulated in the laboratory, making it intractable to test all variants of interest in the relevant biological contexts. Although sequence-based computational models can in principle overcome these challenges, their accuracy is still limited 1-4 , making expression prediction from sequence a critical unsolved problem.</p><p>Deep convolutional neural networks (CNNs) achieve the current state of the art at predicting gene expression from DNA sequences for the human and mouse genomes <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref> . However, to make predictions, these models are only able to consider sequence elements up to 20 kb away from the transcription start site (TSS) because the locality of convolutions limits information flow in the network between distal elements. Many well-studied regulatory elements, including enhancers, repressors, and insulators, can influence gene expression from far greater than 20 kb away 5 . Thus, increasing information flow between distal elements is a promising path to increase predictive accuracy.</p><p>In this work, we introduce a neural network architecture based on self-attention towards this goal. We frame the machine learning problem as predicting thousands of epigenetic and transcriptional datasets in a multitask setting across long DNA sequences. Training on most of the human and mouse genomes and testing on held out sequences, we observed improved correlation between predictions and measured data relative to previous state-of-the-art models without self-attention. We demonstrate more effective use of long-range information, as benchmarked by CRISPRi enhancer assays. The model also produces more accurate predictions of muta-tion effects, as measured by direct mutagenesis assays and population eQTL studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Enformer improves gene expression prediction. We developed a new model architecture named Enformer (a portmanteau of enhancer and transformer) to predict gene expression and chromatin states in humans and mice from DNA sequences (Fig. <ref type="figure">1a</ref> and Extended Data Fig. <ref type="figure">1</ref>). Transformers are a class of deep learning models that have achieved substantial breakthroughs in natural language processing (NLP) <ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7</ref> and were also recently applied to model short DNA sequences 8 . They consist of attention layers that transform each position in the input sequence by computing a weighted sum across the representations of all other positions in the sequence. Attention weight between any two positions depends on the embeddings of their current representation vectors and the distance between them. This allows the model, for example, to refine the prediction at a TSS by gathering information from all relevant regions, such as enhancers regulating the gene. Since each position directly attends to all other positions in the sequence, they allow for a much better information flow between distal elements. By contrast, convolutional layers require many successive layers to reach distal elements due to their local receptive field. Using transformer layers allowed us to substantially increase the receptive field, reaching distal regulatory elements up to 100 kb away while still being able to effectively integrate their information. By contrast, previous state-of-the-art models Basenji2 or ExPecto only reach elements up to 20 kb away (Extended Data Fig. <ref type="figure">1</ref>). This increase in the receptive field is important because it greatly expands the number of relevant enhancers seen by the model from 47% (&lt;20 kb) to 84% (&lt;100 kb) as estimated from the proportions of high-confidence enhancergene pairs <ref type="bibr" target="#b8">9</ref> .</p><p>Enformer substantially outperformed the previous best model, Basenji2, for predicting RNA expression as measured by Cap Analysis Gene Expression 10 (CAGE) at the TSS of human</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Articles</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nature Methods</head><p>protein-coding genes, with the mean correlation increasing from 0.81 to 0.85 (Fig. <ref type="figure">1b, left</ref>). This performance increase is twice as large as the performance increase between Basenji1 (ref. <ref type="bibr" target="#b2">3</ref> ) and Basenji2 (ref. <ref type="bibr" target="#b1">2</ref> ) and closes one-third of the gap to experimental-level accuracy, estimated at 0.94 (Extended Data Fig. <ref type="figure" target="#fig_1">2</ref>). Gene expression predictions also better captured tissue-or cell-type specificity (Fig. <ref type="figure">1b</ref>, right), including for closely related samples (Extended Data Fig. <ref type="figure">3</ref>). The performance improvement was consistent across all four types of genome-wide tracks, including CAGE measuring transcriptional activity, histone modifications, TF binding, and DNA accessibility in various cell types and tissues for held-out chromosomes (Fig. <ref type="figure">1c</ref>). The performance improvement was largest for CAGE, possibly because tissue-specific gene expression strongly depends on distal elements <ref type="bibr" target="#b10">11</ref> . The improvement in prediction accuracy was also qualitatively evident when visualizing observed and predicted tracks of the genome (Fig. <ref type="figure">1d</ref>). Enformer also yielded greater predictive accuracy than ExPecto 1 , a model trained to predict gene expression levels measured by RNA-seq, for both across-genes (0.850 versus 0.812 Spearman r) and across-tissues (0.451 versus 0.368 Spearman r) evaluation (Extended Data Fig. <ref type="figure" target="#fig_2">4</ref>). These results confirm that the Enformer architecture advances prediction accuracy for both a broad range of epigenetic marks and gene expression from DNA sequence.</p><p>To pinpoint the benefit of attention layers compared with the dilated convolutions used in Basenji2, we replaced attention layers with dilated convolutions and tuned the learning rate for optimal performance. Attention layers outperformed dilated convolutions across all model sizes, numbers of layers, and numbers of training data points (Extended Data Fig. <ref type="figure">5a</ref>). The larger receptive field was indeed crucial, because we observed a large performance drop when restricting the receptive field of Enformer to that of Basenji2 by replacing global attention layers with local ones (Extended Data Fig. <ref type="figure">5b</ref>). We note that increasing the number of parameters improved model performance, consistent with recent advances in NLP <ref type="bibr" target="#b6">7</ref> . Enformer uses custom relative positional basis functions in the transformer layers to more easily distinguish between proximal and distal regulatory elements, and to distinguish positions upstream and downstream of the TSS. Both properties provided a noticeable performance improvement over the typically used relative basis functions and absolute positional encodings in the NLP literature (Extended Data Fig. <ref type="figure">6a,b</ref>). Overall, these results confirm that attention layers are better suited than dilated convolutions for gene expression prediction.</p><p>Enformer attends to cell-type-specific enhancers. To better understand what sequence elements Enformer is utilizing when making predictions, we computed two different gene expression contribution scores -input gradients (gradient Ã input) <ref type="bibr" target="#b11">12</ref> and attention weights (Methods and Supplementary Fig. <ref type="figure">1</ref>) -for several genes with CRISPRi-validated enhancers <ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13</ref> . Contribution scores highlight the input sequences that are most predictive for the expression of a particular gene <ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15</ref> . In silico mutagenesis and gradient Ã input are tissue-or cell-type-specific, since they are computed with respect to a particular output CAGE sample (for example, K562). By contrast, attention weights are internal to the model and are shared among all tissue and cell-type predictions. We inspected the contribution scores of several genes and observed that they correlated with histone H3 acetylated at K27 (H3K27ac) and highlighted not only local promoter regions, but also distal enhancers more than 20 kb away (Fig. <ref type="figure" target="#fig_1">2a</ref> and Supplementary Figs. <ref type="figure" target="#fig_1">2 and 3</ref>). By contrast, the contribution scores of Basenji2 were zero for sequences beyond 20 kb from the TSS due to the limited receptive field, thereby missing several enhancers. This example suggests that Enformer is indeed looking at biologically relevant regions, such as enhancers beyond 20 kb, when making predictions, and that gene expression contribution scores could be used to prioritize relevant enhancers.</p><p>Linking candidate enhancers identified via biochemical annotations <ref type="bibr" target="#b15">16</ref> to target genes is an important and unsolved problem <ref type="bibr" target="#b4">5</ref> . Computational models have historically produced low accuracy owing to the combination of noisy labels and class imbalance. To systematically evaluate the ability of contribution scores to pinpoint relevant enhancers for a particular gene, we compared several contribution scores across all tested enhancer-gene pairs in two large-scale CRISPRi studies performed on the K562 cell line <ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13</ref> . In these experiments, CRISPRi was used to suppress the activity of more than 10,000 candidate enhancers and measure their effect on gene expression.</p><p>Enformer contribution scores prioritized validated enhancergene pairs with higher accuracy than Basenji2 contribution scores or random scores across almost all relative distances and different types of contribution scores (Fig. <ref type="figure" target="#fig_1">2b</ref>, Enformer versus Basenji2 versus Random). The performance of Enformer was comparable to, and in some cases even better than, the ABC score <ref type="bibr" target="#b12">13</ref> , a state-of-the-art method recently proposed specifically for enhancer prioritization. This is remarkable because the ABC score relies on experimental data, such as a HiC-based interaction frequency and H3K27ac as input (Fig. <ref type="figure" target="#fig_1">2b</ref>, blue versus green, and Extended Data Fig. <ref type="figure">7a</ref>), whereas Enformer uses only DNA sequence as input and was never trained to explicitly locate enhancers. This allows Enformer to also be used for arbitrary sequence variations lacking experimental data. Cell-type-specific contribution scores yielded a higher prioritization performance than cell-type-agnostic ones, suggesting that the model was using different enhancer sequences in different cell types as expected (Extended Data Fig. <ref type="figure">7c</ref>). Thus, Enformer contribution scores are an effective strategy to prioritize candidate enhancers in cell types used for model training.</p><p>Next, we asked whether the model has learned about another important class of regulatory elements: insulator elements, which separate two topologically associating domains (TADs) and minimize enhancer-promoter crosstalk between the two. We inspected the attention matrices (which were more efficient to compute relative to input gradients owing to the many output targets) of sequences Fig. <ref type="figure">1</ref> | Enformer improves gene expression prediction in held-out genes by using a larger receptive field. a, enformer is trained to predict human and mouse genomic tracks at 128-bp resolution from 200 kb of input DNA sequence. by using transformer modules instead of dilated convolutions, it achieves a five times larger receptive field able to detect sequence elements 100 kb away, compared with 20 kb for basenji2 (ref. <ref type="bibr" target="#b1">2</ref> ) or exPecto 1 (extended Data Fig. <ref type="figure">1</ref>). b, enformer outperforms basenji2 in gene expression prediction from sequence both across genes and across CAGe experiments for protein-coding genes. Test set performance was measured by Pearson correlation of CAGe gene expression (log(1 + x) transformed) computed across genes for each CAGe experiment (left) or across CAGe experiments for each test gene stratified by the observed expression variance across experiments (right). Average performance for each model is shown in the corners. bootstrapped s.d. of these estimates is 0.004 for 'Across genes'. Gene expression values were obtained by summing up the observed or predicted CAGe read counts at all unique TSS locations of the gene. Values for each CAGe experiment were standardized to have zero mean and variance of 1 across genes. c, enformer consistently outperforms basenji2 across all 4 assay types (columns) as measured by Pearson correlation computed across all 128-bp binned genomic positions in the human test set for 5,313 predicted tracks (points). both models were trained and evaluated on the same dataset. enformer performance was significantly higher across all plots in b and c) (paired Wilcoxon P &lt; 10 â38 ). d, representative example of observed and predicted genomic tracks (log 10 scale) at CD44 gene locus located in the test-set region with high disagreement between enformer and basenji2 predictions (Methods). For each experiment, all three tracks share the same y axis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Articles</head><p>Nature Methods centered at TAD boundaries, and compared them with attention from sequences with no particular alignment. From the perspective of the query position, Enformer paid more attention to TAD boundaries than to random positions (vertical red stripe, Fig. <ref type="figure" target="#fig_1">2c</ref>) and less attention to regions on the opposite side of the boundary (off-diagonal blue blocks, Fig. <ref type="figure" target="#fig_1">2c</ref>), consistent with the biological observation of reduced inter-TAD interactions. Both of these two patterns were statistically significant across 1,500 tested sequences (Fig. <ref type="figure" target="#fig_1">2d</ref>, ' Across TAD' and 'Key at TAD boundary'). One of the key motifs at TAD boundaries that the model used to make DNase and CAGE predictions was CTCF, which was found to be associated with both positive and negative contribution scores (Extended Data Fig. <ref type="figure">8</ref>). Overall, these results suggest that the model has not only learned about the role of tissue-specific enhancers and promoters, but also about insulator elements and their role in inhibiting information flow between genomic compartments.</p><p>Enformer improves variant effect prediction on eQTL data. A central goal of this research is to predict the influence of genetic variants on cell-type-specific gene expression, in order to inform  <ref type="bibr" target="#b12">13</ref> ; enformer attention weight averaged across all layers and heads for a query placed at the main TSS of HNRNPA1 gene (position 0); and gradient Ã input 12 contribution scores computed with regard to the K562 CAGe track at the main TSS position for enformer and basenji2. b, enhancer-gene pair classification performance (CrISPri-validated versus nonvalidated candidate enhancers), stratified by relative distance, as measured by auPrC on two CrISPri datasets <ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13</ref> for different methods, models, and contribution scores (Methods). AbC score* (H3K27ac/distance) denotes the approximate version of the AbC score <ref type="bibr" target="#b12">13</ref> lacking Hi-C data, which exhibits similar performance (extended Data Fig. <ref type="figure">7a</ref>). Colored bars depict the median auPrC, and error bars show the 25th and 75th percentiles obtained by sampling 80% of enhancer-gene pairs 100 times without replacement. The auPrC metric is sensitive to class imbalance, which differs between the two datasets (1:10 for Gasperini 9 and 1:4 for Fulco <ref type="bibr" target="#b12">13</ref> ). c, Average attention matrix difference of enformer between 1,500 sequences centered at a topologically associating domain (TAD) boundary and 1,500 sequences from the validation set without any particular centering. Attention matrices were averaged across all layers, heads, and sequences. red stripe in the center at key = 0 means that the model is attending more to the TAD boundary than by chance. blue regions in off-diagonal quadrants mean that the model is attending less across the TAD boundary. d, Attention is significantly lower across TAD boundaries (center), significantly higher at TAD boundaries (right), and shows no significant difference within them (left), as compared with 1,500 random genomic sequences. Distributions show attention across all sequences in specific attention matrix parts shown in red. P values were computed with the two-sided Mann-Whitney U test. The box plots mark the median, upper and lower quartiles, and 1.5Ã interquartile range (whiskers); outliers are shown as points (n = 1,500 for each violin plot).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Articles</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nature Methods</head><p>fine-mapping of the many thousands of noncoding associations with phenotypes of interest from genome-wide association studies (GWAS). Computational models that predict regulatory activity from DNA sequences can process distinct alleles and compare predictions to score genetic variants <ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref> . A successful model would be able to produce the results of a gene expression quantitative trait loci (eQTL) study without having to measure hundreds to thousands of individual gene expression profiles. Thus, we studied eQTLs discovered by the GTEx project across dozens of human tissues to validate model predictions <ref type="bibr" target="#b19">20</ref> . The primary challenge of such validation is the influence of co-occurrences between variants (that is, linkage disequilibrium) in the profiled population, which transfers the causal eQTL's effect to nearby co-occurring variants' measurements. Signed linkage disequilibrium profile (SLDP) regression is a technique developed to measure the genome-wide statistical concordance between signed variant annotations (such as our model predictions) and GWAS summary statistics (such as GTEx eQTLs) while accounting for linkage disequilibrium (Methods) <ref type="bibr" target="#b20">21</ref> . For 379 of 648 (59.4%) CAGE datasets, the maximum SLDP Z-score across GTEx tissues (representing the most likely closest sample match) increased for Enformer predictions relative to Basenji2. Enformer maximum Z-scores increased by greater than one s.d. for 228 CAGE datasets, relative to 46 decreased by one. The maximum Z-score increased on average from 6.3 to 6.9 (Fig. <ref type="figure">3a</ref>). Note that we do not expect increased SLDP Z-scores for CAGE samples without a relevant GTEx tissue match. We observed a qualitative improvement in the tissue similarity of the top-ranked CAGE sample for GTEx tissues, exemplified by increased SLDP Z-scores for muscle samples to GTEx skeletal muscle and adipose samples for GTEx subcutaneous adipose tissue (Fig. <ref type="figure">3b,c</ref>). We also found that Enformer variant effect predictions for DNase hypersensitivity had greater SLDP concordance with GTEx than an alternative method called DeepSEA Beluga, used in ExPecto 1 (Extended Data Fig. <ref type="figure">9</ref>). Thus, Enformer predictions for noncoding-variant activity appear to improve primarily for samples with similar cell-type composition, in line with our observations of improved tissue and cell-type specificity for held-out sequences.</p><p>Although linkage disequilibrium generally results in GTEx eQTL associations that can be attributed only to a set of frequently co-occurring variants, the latest GTEx release includes many thousands of associations in loci with simple linkage patterns, which have been fine-mapped to a single high-probability causal variant <ref type="bibr" target="#b21">22</ref> . To assess the utility of Enformer predictions for identifying causal variants, we defined a classification task for each tissue to discriminate likely causal variants (causal probability &gt; 0.9, as determined by the population-based fine-mapping model SuSiE <ref type="bibr" target="#b22">23</ref> ) from likely spurious eQTLs (causal probability &lt; 0.01), which were matched for the eGene when possible (Methods). We represented each variant by its prediction difference vector (that is, evaluating the reference minus alternative allele, summed across the sequence) for all 5,313 human datasets, and trained random forest classifiers. Enformer predictions enabled a more accurate classifier for 47 of 48 GTEx tissues (Fig. <ref type="figure">3d</ref>), increasing the mean area under the receiver operating characteristic curve (auROC) from 0.729 to 0.747. This improvement was consistent across all distances from the TSS (Fig. <ref type="figure">3e</ref>), suggesting that the model not only better represents variants likely overlapping long-range enhancers (enabled by the larger receptive field), but also more effectively parses promoters and short-range enhancers. The Enformer model was also more accurate at predicting the direction of expression change of these fine-mapped eQTLs than was Basenji2 (Extended Data Fig. <ref type="figure">10</ref>).</p><p>One example variant where the Enformer eQTL probability prediction increased relative to Basenji2 is rs11644125, which lies within an intron ~35 kb downstream of the TSS of NLRC5, a gene involved in viral immunity and the cytokine response (Fig. <ref type="figure">3f</ref>). The variant has been statistically fine-mapped as likely to cause changes in monocyte and lymphocyte blood cell counts <ref type="bibr" target="#b23">24</ref> . According to GTEx, the minor allele T decreases gene expression of NLRC5 in whole blood relative to the major allele C. Enformer correctly predicts reduced NLRC5 expression from the upstream TSS in many relevant CAGE samples, including PBMCs. Using in silico mutagenesis of the local region (Methods), we observed that the variant rs11644125 modulates the known motif of the transcription factor SP1 (ref. <ref type="bibr" target="#b23">24</ref> ). Enformer predictions suggest perturbed SP1 binding in hematopoietic cells that alters NLRC5 expression as a mechanism for these traits.</p><p>Enformer improves MPRA mutation effect prediction. Finally, we evaluated Enformer's performance on a second, independent variant effect prediction task using a dataset in which massively parallel reporter assays (MPRAs) directly measured the functional effect of genetic variants through saturation mutagenesis of several enhancers and promoters in a variety of cell types <ref type="bibr" target="#b24">25</ref> . We used the same training and test sets as the CAGI5 competition <ref type="bibr" target="#b25">26</ref> , enabling us to directly benchmark Enformer's performance relative to those of submissions from other groups. Methods derived from other groups deploy a heterogeneous set of approaches, ranging from the use of the deltaSVM strategy <ref type="bibr" target="#b26">27</ref> , the CADD framework <ref type="bibr" target="#b27">28</ref> , and regression models using features derived from a combination of conservation information and deep learning predictions from DeepBind <ref type="bibr" target="#b28">29</ref> and DeepSEA 18 (Group 3, Group 5, and Group 7) <ref type="bibr" target="#b25">26</ref> . For each variant, we evaluated its effect as the predicted difference between the reference and alternative allele, retrieving 5,313 features. Next, we compared two approaches: (1) we used these features to train a lasso regression model on the provided training set for each gene, and (2) we preselected a subset of features corresponding to cell-type-matched and cell-type-agnostic predictions of changes in CAGE and DNase, and generated a summary statistic of the features (that is, without additional training).</p><p>Evaluating these two approaches on each gene's test set revealed that lasso regression with Enformer predictions as features had the best average correlation across all loci, among seven alterna-Fig. <ref type="figure">3</ref> | Enformer improves variant effect prediction on eQTL data as measured by sLDP regression and fine-mapped variant classification. a, We computed genome-wide statistical concordance between variant effect predictions for individual CAGe datasets and GTex eQTL summary statistics using SLDP <ref type="bibr" target="#b20">21</ref> across all variants in the 1000 Genomes dataset. Taking the GTex tissue with max Z-score for each sample, enformer predictions achieved greater Z-scores for 59.4% of samples, and 228 are greater by more than one s.d. (versus 46 for basenji2). each point represents one of the 638 CAGe samples. We used one-sided binomial tests to compute the P values in the top row panels. b,c, Studying SLDP in skeletal muscle (b) and subcutaneous adipose (c) GTex tissues indicated that biologically relevant CAGe datasets (shown in blue) improve between basenji2 and enformer. d, We trained random forest classifiers to discriminate between fine-mapped GTex eQTLs and matched negative variants in each of 48 tissues (Methods). Features derived from enformer enabled more accurate classifiers than basenji2 features for 47 of 48 tissues. e, We computed auPrC for variants in four roughly equally sized TSS distance bins. Violin plots represent measures for the n = 48 tissues (white dots represent the median, thick bars the interquartile range, and thin bars the entire data range). enformer improved accuracy at all distances (one-sided paired Wilcoxon P &lt; 1 Ã 10 -4 ). f, enformer prediction for rs11644125 improved relative to basenji2 (data not shown) by better capturing its influence on an NLRC5 TSS ~35 kb upstream. rs11644125 is associated with monocyte and lymphocyte counts in the UK biobank and fine-mapped to &gt;0.99 causal probability <ref type="bibr" target="#b23">24</ref> . In silico mutagenesis of the region surrounding rs11644125 revealed an affected SP1 transcription factor motif <ref type="bibr" target="#b38">39</ref> .</p><p>tive submissions from the competition (Fig. <ref type="figure" target="#fig_2">4a</ref>). Moreover, using the Enformer predictions directly as scores, without training, performed comparably to the lasso-trained model and also outperformed the other submissions. This includes the sequence-based predictor deltaSVM <ref type="bibr" target="#b26">27</ref> , which was trained on independent DNase and histone H3 monomethylated at K4 (H3K4me1) data derived from matched cell types <ref type="bibr" target="#b24">25</ref> . The lasso-trained Enformer exceeded the performance of Group 3, the winning team from CAGI5 (P = 0.002, paired, one-sided Mann-Whitney U test, Fig. <ref type="figure" target="#fig_2">4b</ref>). Visualization of the predictions that required no additional training revealed that Enformer faithfully captured the effects of two out of four transcription-factor-binding sites for the LDLR locus (Fig. <ref type="figure" target="#fig_2">4c</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Articles</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nature Methods</head><p>Enformer highlighted an additional binding site that had lower effect sizes, but still showed a significant difference. By contrast, del-taSVM successfully predicted only one binding site but missed the other three, overall exhibiting 50% reduced Pearson and Spearman correlations to the measured effects relative to Enformer. For this locus, cell-type-matched predictions mirrored cell-type-agnostic predictions, indicating that the binding sites which were detected likely corresponded to general transcription factors present in most cell types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>A long-standing problem in regulatory genomics is that of predicting gene expression purely from DNA sequence. With a novel transformer architecture, we have made a significant improvement by greatly expanding the receptive field and increasing the information flow between distal elements. In this way, the model can better capture biological phenomena such as enhancers regulating promoters despite a large DNA-sequence distance between the two. This led to a substantial performance increase in tissue-and cell-type-specific gene expression prediction correlation from 0.81 to 0.85, one-third of the way toward the experimental-level accuracy of 0.94 estimated from replicates. This improvement in predictive accuracy translated to improved models for two key problems of biological relevance: enhancerpromoter prediction and noncoding variant effect prediction. We observed that the model pays attention to enhancers and considers insulators when making gene expression predictions, suggesting that it has learned canonical distal regulation patterns. Using the Enformer model, we can more accurately predict whether a natural variant or CRISPR-perturbed enhancer will cause a notable expression change than can previous approaches. By relying solely on DNA sequences as input, Enformer has several advantages over alternative variant effect prediction methods: (1) unlike most methods <ref type="bibr" target="#b24">25</ref> , it is capable of signed prediction of activating or repressive mutations; (2) by not relying explicitly on nucleotide conservation statistics, as the majority of tools do <ref type="bibr" target="#b24">25</ref> , its predictions are not limited to conserved enhancers, which comprise a small proportion of all enhancers <ref type="bibr" target="#b29">30</ref> ; and (3) it can make predictions for arbitrary sequences, which enables the synthetic design of enhancers that are optimized to exhibit cell-type specificity <ref type="bibr" target="#b30">31</ref> . Altogether, these advances and advantages open exciting avenues to study the expanding catalogs of genetic variants linked to disease and enhancer biology in development and evolution.</p><p>Several paths to further improve model accuracy appear promising. Machine-learning success depends on the training data, so increasing the resolution and quality of the target tracks <ref type="bibr" target="#b14">15</ref> , and curating data from additional organisms 2 , would likely boost performance. Recent work demonstrated that the highly structured 3D DNA contacts, which greatly influence long-range gene regulation, are predictable from the underlying DNA sequence <ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33</ref> .  <ref type="bibr" target="#b24">25</ref> , on test sets for 15 loci curated for the CAGI5 competition <ref type="bibr" target="#b25">26</ref> . Shown above the horizontal break is the performance of five methods that required no additional fine-tuning on each locus; shown below is that of eight methods that were additionally trained on the CAGI5 training sets. b, Pearson correlations of each locus for predictions derived from the enformer versus the winning team of the CAGI5 competition. Average performance for each model is shown in the corners. enformer shows a significant performance improvement (P = 0.002, paired, one-sided Mann-Whitney U test). c, example saturation mutagenesis data from the LDLR promoter locus. Shown in the top row is the reference sequence scaled to the mean effect size among all alternative mutations, with measured effect sizes of individual variants in the second row. Two of the four significant elements match known motifs <ref type="bibr" target="#b38">39</ref> , and the two unknown motifs partially resemble the SP1 binding motif. Shown in the bottom two rows are the predictions on the full dataset using methods from a that required no additional fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Articles</head><p>Nature Methods</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Model architecture. The Enformer architecture consists of three parts: (1) 7 convolutional blocks with pooling, (2) 11 transformer blocks, and (3) a cropping layer followed by final pointwise convolutions branching into 2 organism-specific network heads (Extended Data Fig. <ref type="figure">1</ref>). Enformer takes as input one-hot-encoded DNA sequence (A = [1,0,0,0], C = [0,1,0,0], G = [0,0,1,0], T = [0,0,0,1], N = [0,0,0,0]) of length 196,608 bp and predicts 5,313 genomic tracks for the human genome and 1,643 tracks for the mouse genome, each of length 896 corresponding to 114,688 bp aggregated into 128-bp bins. The convolutional blocks with pooling first reduce the spatial dimension from 196,608 bp to 1,536 so that each sequence position vector represents 128 bp (although the convolutions do observe nucleotides in the adjacent pooled regions). The transformer blocks then capture long-range interactions across the sequence. The cropping layer trims 320 positions on each side to avoid computing the loss on the far ends because these regions are disadvantaged because they can observe regulatory elements only on one side (toward the sequence center) and not the other (the region beyond the sequence boundaries). Finally, the two output heads predict organism-specific tracks. The Enformer architecture is similar to the state-of-the-art model Basenji2 (ref. <ref type="bibr" target="#b1">2</ref> ). However, the following changes helped us improve and exceed its performance: Enformer uses transformer blocks instead of dilated convolutions, attention pooling instead of max pooling, twice as many channels, and 1.5 times longer input sequence (197 kb instead of 131 kb). The detailed model architecture, including the selected hyperparameters, is shown in Extended Data Fig. <ref type="figure">1</ref>. Attention pooling summarizes a contiguous chunk of the input sequence x full k:k+Lp = x â R LpÃC across L p positions for each of the C channels and returns the output value hâR C as follows:</p><formula xml:id="formula_0">hj = â i exp ( x i â¢ w j ) xij â i exp ( x i â¢ w j ) ,</formula><p>where i indexes sequence position in the pooling window, which is weighted by the exponentiated dot product x i ã»w j and w â R C Ã K is a matrix of learned weights. We apply attention pooling to contiguous chunks of the original input sequence using window size L p = 2 and stride of 2. We initialize w to 2 Ã 1, where 1 is the identity matrix to prioritize the larger value, making the operation similar to max pooling. This initialization gave slightly better performance than did random initialization or initialization with zeros, representing average pooling. We use multi-head attention (MHA) layers to share information across the sequence and model long-range interactions, such as those between promoters and enhancers. Each head has a separate set of weights w q âR CÃK , w k âR CÃK , and w v âR CÃV which transform the input sequence xâR LÃC into queries q i =x i w q , keys k j =x j w k , and values v j =x j w v . Queries represent the current information at each position and keys represent the information each position will be looking for to attend to. Their dot product plus the relative positional encodings R ij forms the attention matrix, which is computed as a ij = softmax(q i k j T /âK + R ij ), where the entry a ij represents the amount of weight query at position i puts on the key at position j. Values represent the information that each position will propagate forward to positions that attend to it. Each single attention head computes its output as a weighted sum across all input positions: av. This allows each query position to use information across the whole sequence. The multiple heads compute with independent parameters, and we concatenate the outputs from each head to form the final layer output followed by a linear layer to combine them. Our layers used 8 heads, value size of 192, and key/query size of 64.</p><p>MHA applications in NLP typically operate directly on the input sequence, tokenized into words and embedded in a richer embedding space. The convolution tower preceding MHA in the Enformer model serves to perform an analogous operation of embedding nucleotide segments and contributes a compelling inductive bias for adjacent nucleotides to function together in motifs. We chose to compute at 128-bp resolution because it roughly represents a well-studied length of regulatory elements that contain several motifs and is an appropriate bin size at which to aggregate the experimental data to be predicted. Finer resolution has potential benefits when the data support it <ref type="bibr" target="#b14">15</ref> , but would extend the sequence length entering the quadratic complexity MHA and make the model engineering intractable on currently available hardware.</p><p>To inject positional information, we add relative positional encodings 40 R ij to the q i k j T attention term as formulated in the Transformer-XL paper <ref type="bibr" target="#b40">41</ref> . Relative positional encodings provide a parameterized baseline for how actively two positions in the sequence should influence each other during the layer's transformation as a function of their pairwise distance. Specifically, we use R ij = q i r T i-j + u k T j + v r T i-j , where r i-j = w R f(i -j) is a linear function of different relative basis functions f(i -j), and u and v are the position-agnostic embeddings used to evaluate the preference for specific keys (u) or relative distances (v). We use three different basis function classes for f(i -j), as visualized in Extended Data Fig. <ref type="figure">5b:</ref> 1.</p><formula xml:id="formula_1">f exponential i (r) = e</formula><p>â log(2) r r1/2,i , where r 1/2,i is placed linearly in the log-space between 3 and sequence length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>f central mask i</p><formula xml:id="formula_2">(r) = { 1, if r â¤ 2 i 0, otherwise 3. f gamma i (r) = Gamma ( r|Î± = Î¼ i Ï 2 , Î² = Î¼ 2 i Ï 2</formula><p>)</p><p>, where Gamma(r|É,Î²) is the gamma probability distribution function. ð i is placed linearly from (sequence length / number of features) to sequence length and Ï = sequence length / (2 Ã number of features).</p><p>For each basis function, we use a symmetric f(|x|) and asymmetric sign(x) Ã f(|x|) version to introduce directionality. We use the same number of relative positional basis functions as the value size of MHA (192). The 192 basis functions are equally divided among the basis function classes and the symmetric versus asymmetric versions thereof. With 3 basis function classes, each basis function class provides 64 positional features (32 symmetric and 32 asymmetric).</p><p>Dropout rates of 0.01 and 0.05 were used for positional encoding features and the final attention matrix respectively in MHA. All other dropout rates are annotated in Extended Data Fig. <ref type="figure">1a</ref>.</p><p>Model training and evaluation. The model was trained, evaluated, and tested on the same targets, genomic intervals, and using the same Poisson negative log-likelihood loss function as Basenji2 (ref. <ref type="bibr" target="#b1">2</ref> ). Briefly, the cross-species training/ validation/test sets were constructed using the following procedure to partition homologous sequences into the same set. First, we divided both the human and mouse genomes into 1 Mb regions. We constructed a bipartite graph, in which the vertices represent these regions. Next, we placed edges between 2 regions if they have &gt;100 kb of aligning sequence in the hg38-mm10 syntenic net format alignment downloaded from the UCSC Genome Browser <ref type="bibr" target="#b41">42</ref> . Finally, we partitioned connected components in the bipartite graph randomly into training, validation, and test sets.</p><p>The dataset contains 34,021 training, 2,213 validation, and 1,937 test sequences for the human genome, and 29,295 training, 2,209 validation, and 2,017 test sequences for the mouse genome. For the human genome, each example contains 2,131 transcription factor (TF) chromatin immunoprecipitation and sequencing (ChIP-seq), 1,860 histone modification ChIP-seq, 684 DNase-seq or ATAC-seq, and 638 CAGE tracks (total 5,313, Supplementary Table <ref type="table">2</ref>). For the mouse genome, each example contains 308 TF ChIP-seq, 750 histone modification ChIP-seq, 228 DNase-seq or ATAC-seq, and 357 CAGE tracks (total 1,643, Supplementary Table <ref type="table">3</ref>). We modified the Basenji2 dataset by extending the input sequence to 196,608 bp from the original 131,072 bp using the hg38 reference genome.</p><p>To train a model simultaneously on human and mouse genomes, we alternated between a batch containing data from the human genome and the mouse genome. The main Enformer model with 1,536 channels was implemented in Sonnet v2, TensorFlow (v2.4.0), and was trained on 64 TPU v3 cores with batch size of 64 (1 per core) for 150,000 steps (approximately 3 days) using all-reduce gradient aggregation across the cores at every step. Batch normalization statistics were also aggregated across multiple replicas using 0.9 momentum. We used the Adam optimizer from Sonnet v2 (ref. <ref type="bibr" target="#b42">43</ref> ) with a learning rate of 0.0005 and default settings for other hyperparameters: Î² 1 = 0.9, Î² 2 = 0.999, Îµ = 1 Ã 10 -8 . The optimal learning rate was discovered by grid search yielding the highest performance on the validation set. We linearly increased the learning rate from 0 to target value in the first 5,000 steps of training. We clipped gradients to a maximum global norm of 0.2. We used the same data augmentation as Basenji2 (ref. <ref type="bibr" target="#b1">2</ref> ) during training by randomly shifting the input sequence by up to 3 bp and reverse-complementing the input sequence while reversing the targets. Finally, we fine-tuned the Enformer model on human data for 30,000 steps using a lower learning rate of 0.0001.</p><p>We used the pretrained Basenji2 model for all main model comparisons and retrained an equivalent model for ablation and hyperparameter sweeps shown in Extended Data Fig. <ref type="figure">5</ref>. In these comparative analyses, we used 768 channels (1/2 of the original Enformer model obtained by using a value size of 96 in MHA), 131 kb input sequence, and batch size 32 trained on 32 TPU v3 cores. We did not fine-tune these models on the human data. For models using dilated convolutions instead of transformer blocks, we used a higher learning rate of 0.02 without ramp up of the learning rate. As for Enformer, the optimal learning rate was discovered by grid search yielding the highest performance on the validation set. All models were trained for 500,000 steps while only storing the model with the highest Spearman correlation of CAGE TSS gene expression across genes averaged across experiments computed on the validation set every 1,000 steps. We used the validation set for hyperparameter selection and the test set for Basenji2 comparison. We considered two evaluation metrics: (1) Pearson correlation computed across all 128-bp binned genomic positions in the validation/ test set for each output track; and (2) Pearson correlation of CAGE gene expression values (log(1 + x)-transformed and standardized across genes for each experiment) of all protein-coding genes in the validation/test set computed either for each CAGE experiment across genes (main metric) or across CAGE experiments for each gene (shown in Fig. <ref type="figure">1b</ref>). Observed and predicted gene expression values were obtained by summing up the observed/predicted CAGE read counts at all unique TSS locations of the gene. For each TSS location, we used the 128-bp bin overlapping the TSS as well as the two neighboring bins (3 bins in total). We used test-time augmentation during model evaluation: we averaged the predictions from 8 sequences randomly augmented the same way as during training (â¤3 bp shifts and reverse-complementation). We only evaluated the performance of our model on the test set once to generate Fig. <ref type="figure">1</ref> and did not use the test set during model development.</p><p>NATuRE METhoDs | www.nature.com/naturemethods To select a representative example, we visualized the top 10 transcripts with highest discrepancy between Enformer and Basenji2 performance on the ' Across CAGE experiments' metric measuring tissue specificity for 33% of the most tissue-specific genes. We picked the sixth transcript in the list (ENST00000524922) because it cleanly showed differences across all three categories of genomic tracks (DNA accessibility, histone modifications, and gene expression).</p><p>Enhancer prioritization. We obtained a set of enhancer-gene pairs tested using a CRISPRi assay perturbing the enhancer of interest while measuring the expression change of the gene in K562 cells from two studies: Gasperini et al. <ref type="bibr" target="#b8">9</ref> using scRNA-seq to measure expression changes, and Fulco et al. <ref type="bibr" target="#b12">13</ref> using Flow-FISH. We transformed the enhancer and gene coordinates from hg19 to hg38 using the UCSC liftOver web tool <ref type="bibr" target="#b41">42</ref> . Each enhancer-gene pair contains a label denoting whether a significant expression change was induced after CRISPRi treatment. We denoted the set of all enhancers as 'candidate' enhancers and those that showed a change in expression as 'validated' enhancers. We evaluated different methods on their ability to classify or prioritize enhancer-gene pairs that exhibited a significant expression change using area under precision-recall curve (auPRC) <ref type="bibr" target="#b12">13</ref> .</p><p>To prioritize enhancer-gene pairs with sequence-based models, we computed three different scores: gradient Ã input, attention, and in silico mutagenesis (ISM). For each enhancer-gene pair, we determined the major TSS of the gene by taking the highest predicted CAGE value in K562 using Enformer. We extracted the DNA sequence centered at the main TSS and computed the following different enhancer-gene scores:</p><p>1. Gradient Ã input: We computed the absolute value of the gradient of the CAGE targets (either using the K562-specific CAGE targets or all CAGE targets, Extended Data Fig. <ref type="figure">7c</ref>) at the TSS with regard to the input reference sequence nucleotide. Note that since our input sequence is one-hot encoded, taking the input gradient of the nonzero channel (the reference nucleotide), is equivalent to computing gradient Ã input attributions <ref type="bibr" target="#b11">12</ref> . We note that 'CAGE at TSS' always means summing the absolute gradient values from three adjacent bins, as is also done in gene-focused model evaluation. The three bins include the bin overlapping the TSS and one flanking bin on each side. The enhancer-gene score was obtained by summing the absolute gradient Ã input scores in the 2-kb window centered at the enhancer. 2. Attention: We first averaged transformer attention matrices across all heads and layers. We extracted the row corresponding to the query index positioned at the TSS, so that keys correspond to different spatial positions and the attention values specify how much the model attended to these positions when making predictions for the TSS. We only computed this contribution score for Enformer. The enhancer-gene score was obtained by summing the attention scores in the 2-kb window centered at the enhancer. 3. ISM: The in silico mutagenesis enhancer-gene score was computed by comparing K562 CAGE predictions at the TSS from the reference sequence with predictions from modified sequence where the 2-kb enhancer sequence was replaced by a random sequence: |f(modified) -f(reference)|.</p><p>To reproduce the ABC score introduced in Fulco et al. <ref type="bibr" target="#b12">13</ref> , we obtained the BigWig of H3K27ac ChIP-seq data in K562 from ENCODE with file accession ENCFF779QTH and DNase with file accessions ENCFF413AHU and ENCFF936BDN. We summed the normalized reads from replicates. For each track and enhancer, we summed up the signal at the enhancer in a fixed window of 2 kb centered at the enhancer. This fixed and broader window yielded better performance compared to the variable window size of ~500 bp as used in the original ABC score (Extended Data Fig. <ref type="figure" target="#fig_2">4a</ref>).</p><p>GTEx SLDP. We predicted the effect of a genetic variant on various annotations by computing a forward pass through the model using the reference and alternative alleles, subtracting their difference, and summing outputs across the sequence to obtain a signed score for each training dataset. We averaged scores computed using the forward and reverse complement sequence and small sequence shifts to the left and right. We computed scores for all 1000 Genomes SNPs.</p><p>We used SLDP <ref type="bibr" target="#b19">20</ref> to estimate the functional correlation between these scores and GTEx v7a summary statistics for 48 tissues while accounting for population linkage disequilibrium structure (Supplementary Information).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-mapped GTEx classification.</head><p>To study specific eQTLs without needing to consider LD, we studied statistical fine-mapping of GTEx v8 using the SuSiE method <ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23</ref> . We focused on variants with posterior inclusion probability (PIP) in a credible causal set &gt;0.9, which ranged from a minimum of 166 variants for substantia nigra to 2,740 for tibial nerve. We arranged a classification task to discriminate between these positive causal variants and a matched set of negative variants. When available, we chose a negative variant matched to each causal variant from the set with PIP &lt; 0.01 but |Z-score| &gt; 4 tested for the same gene. When unavailable for the same gene, we chose from the set with PIP &lt; 0.01 and |Z-score| &gt; 6 genome-wide.</p><p>To determine how informative different variant annotations are, we trained separate random forest classifiers for each tissue to distinguish causal from noncausal variants using eight-fold crossvalidation. We selected the default hyperparameters of the scikit-learn 0.22 implementation after finding negligible accuracy gains from modifying them <ref type="bibr" target="#b43">44</ref> . However, owing to the large number of features derived from the training datasets, setting the maximum features considered per decision tree split to log 2 of the total number of features greatly improved the computational efficiency. We fit 100 iterations of stochastic crossvalidation shuffling and random forest fitting to delineate a low-variance estimate of model accuracy. We performed statistical tests comparing two different model feature sets by comparing the 8 Ã 100 distinct test set auROCs.</p><p>For signed GTEx analysis, we benchmarked model predictions on the basis of their ability to discriminate causal variants that increase versus decrease gene expression. In this analysis, we removed variants that affect gene expression in opposite directions for different cis-genes. We manually matched FANTOM5 CAGE sample descriptions to the GTEx tissues. We skipped cases with more than three possible matches. In cases with two or three possible matches, we chose the CAGE sample with the best average concordance between the Basenji2 and Enformer predictions. We computed auROC statistics by ranking causal variants by their signed prediction for that sample.</p><p>Benchmarking variant effect predictions on saturation mutagenesis data. We acquired training and test sets as well as the predictive accuracies of individual competition participants from the CAGI5 competition <ref type="bibr" target="#b25">26</ref> (M. Kircher, personal communication, https://genomeinterpretation.org/content/expression-variants). For each variant and locus, we evaluated its effect as the predicted difference between the reference and alternative allele summed in four flanking bins representing 512 bp, producing 5,313 features based on the human datasets. All CAGE features were log-transformed after adding a pseudocount of 1 prior to computing this difference. For each allele, we averaged predictions for the forward and reverse-complemented sequence. We scaled the features from the test set with scaling factors computed on the features from the training set, such that the training features had a mean of 0 and s.d. of 1. Following our previous work <ref type="bibr" target="#b44">45</ref> , we then trained a lasso regression model for each locus using these features and the corresponding training set. The strength of the regularization was controlled by a single Î» parameter, which was optimized using tenfold crossvalidation for each training set using the cv.glmnet function of the glmnet library in R.</p><p>For our training-free comparisons, we selected the subset of features corresponding to cell-type-matched and cell-type-agnostic predictions of changes in CAGE and DNase. For the cell-type-agnostic models, we used the subset of all 638 CAGE or 674 DNase features (Supplementary Table <ref type="table">2</ref>). For the cell-type-matched models, we additionally required the CAGE/DNase features to contain the following substrings: (1) 'HepG2' for F9, LDLR, and SORT1, (2) 'K562' for GP1BB, HBB, HBG1, and PKLR, and (3) 'HEK293' for HNF4A, MSMB, TERT (performed in HEK293T cells), and MYCrs6983267. For several loci, a perfectly matched DNase or CAGE sample did not exist. We therefore selected the most closely matched feature based on the following substrings: (1) 'pancreas' for ZFAND3, (2) 'glioblastoma' for TERT (performed in GBM cells), (3) 'keratinocyte' for IRF6, and (4) 'SK-MEL' for IRF4. For each locus, we extracted the features matching the aforementioned substrings, and used the first principal component (PC) of the indicated features as our summary statistic, inverting the sign of the PC if it was negatively correlated to the mean of the features.</p><p>Reporting Summary. Further information on research design is available in the Nature Research Reporting Summary linked to this article.</p><p>Pre-trained Enformer model is available on TF-Hub so that users can easily run it on new data: https://tfhub.dev/deepmind/enformer/1. We also plan to release it in the Kipoi model repository <ref type="bibr" target="#b46">47</ref> . We provide code examples (enformer-usage.ipynb) on how to use that model to score genetic variants. Finally, we provide variant effect predictions for all frequent variants in the 1000 genomes cohort (MAF&gt;0.5% in any population) here, with an open creative-commons CC-BY 4.0 license. To make these predictions more accessible, we distilled the 5,313 features into 20 highly informative variant scores using PCA (Methods) to keep the released file sizes manageable (&lt;1 GB in total for 10 M variants, instead of 100 GB) while retaining high predictive accuracy (GTEx fine-mapping classification auROC of 0.743 compared to 0.747 using all features). The most prominent change in gene expression due to the lipopolysaccharide treatment was also captured by the enformer model. Observed matrix was hierarchically clustered for both rows and columns. enformer predicted heatmap follows the same row and column ordering as the observed matrix. c) Predictive performance in the test set for CAGe gene expression fold change for all within-group pairs from a (y-axis) compared to the observed gene expression correlation between two pairs (x-axis). Fold change of highly correlated CAGe samples is more difficult to predict. d) enformer shows higher fold-change predictive performance compared to basenji2. Extended Data Fig. <ref type="figure">9</ref> | Enformer achieves greater and more specific sLDP concordance to GTEx than DeepsEA. To compare enformer and DeepSeA beluga (convolutional neural network used in exPecto) variant effect predictions, we manually matched DNase datasets that both models were trained on, finding 100 confident matches. We computed genome-wide statistical concordance between variant effect predictions for these DNase datasets and GTex eQTL summary statistics using SLDP across all variants in the 1000 genomes dataset. a) We scatter plotted all DNase sample and GTex tissue z-scores for the DeepSeA and enformer predictions, observing that the enformer scores are greater for 60.8%. each point corresponds to (DNase sample, GTex tissue) pair. Only some (DNase sample, GTex tissue) pairs are biologically well-matched, while the majority are. b) Since many DNase samples profile fibroblasts we specifically plotted all DNase sample z-scores for the GTex fibroblast summary statistics. We colored the DNase fibroblast samples in red, revealing that they are the highest scoring and most improved in the enformer model relative to DeepSeA. This suggests that enformer variant effect predictions are more tissue specific, since one would expect to obtain the highest Z-score for these matched samples shown in red.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extended Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NATuRE</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>RNATuRE METhoDs | VOL 18 |</head><label>18</label><figDesc>Pearson for CAGE at gene TSS (test set) R Pearson across all positions per track (test set) OCTOber 2021 | 1196-1203 | www.nature.com/naturemethods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 |</head><label>2</label><figDesc>Fig.2| Enformer attends to cell-type-specific enhancers, enabling enhancer prioritization. a, HNRNPA1 locus showing: predicted CAGe expression in K562; measured H3K27ac highlighting active enhancers; candidate (light gray) and CrISPri-validated enhancers (dark gray) exhibiting significant HNRNPA1 expression changes from Fulco et al.<ref type="bibr" target="#b12">13</ref> ; enformer attention weight averaged across all layers and heads for a query placed at the main TSS of HNRNPA1 gene (position 0); and gradient Ã input 12 contribution scores computed with regard to the K562 CAGe track at the main TSS position for enformer and basenji2. b, enhancer-gene pair classification performance (CrISPri-validated versus nonvalidated candidate enhancers), stratified by relative distance, as measured by auPrC on two CrISPri datasets<ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13</ref> for different methods, models, and contribution scores (Methods). AbC score* (H3K27ac/distance) denotes the approximate version of the AbC score<ref type="bibr" target="#b12">13</ref> lacking Hi-C data, which exhibits similar performance (extended Data Fig.7a). Colored bars depict the median auPrC, and error bars show the 25th and 75th percentiles obtained by sampling 80% of enhancer-gene pairs 100 times without replacement. The auPrC metric is sensitive to class imbalance, which differs between the two datasets (1:10 for Gasperini 9 and 1:4 for Fulco<ref type="bibr" target="#b12">13</ref> ). c, Average attention matrix difference of enformer between 1,500 sequences centered at a topologically associating domain (TAD) boundary and 1,500 sequences from the validation set without any particular centering. Attention matrices were averaged across all layers, heads, and sequences. red stripe in the center at key = 0 means that the model is attending more to the TAD boundary than by chance. blue regions in off-diagonal quadrants mean that the model is attending less across the TAD boundary. d, Attention is significantly lower across TAD boundaries (center), significantly higher at TAD boundaries (right), and shows no significant difference within them (left), as compared with 1,500 random genomic sequences. Distributions show attention across all sequences in specific attention matrix parts shown in red. P values were computed with the two-sided Mann-Whitney U test. The box plots mark the median, upper and lower quartiles, and 1.5Ã interquartile range (whiskers); outliers are shown as points (n = 1,500 for each violin plot).</figDesc><graphic url="image-2.png" coords="4,91.56,365.74,96.91,86.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 |</head><label>4</label><figDesc>Fig. 4 | Enformer improves noncoding variant effect prediction as measured by saturation mutagenesis experiments. a, Correlation of variant effect predictions with experimental values, as measured by saturation mutagenesis MPrAs<ref type="bibr" target="#b24">25</ref> , on test sets for 15 loci curated for the CAGI5 competition<ref type="bibr" target="#b25">26</ref> . Shown above the horizontal break is the performance of five methods that required no additional fine-tuning on each locus; shown below is that of eight methods that were additionally trained on the CAGI5 training sets. b, Pearson correlations of each locus for predictions derived from the enformer versus the winning team of the CAGI5 competition. Average performance for each model is shown in the corners. enformer shows a significant performance improvement (P = 0.002, paired, one-sided Mann-Whitney U test). c, example saturation mutagenesis data from the LDLR promoter locus. Shown in the top row is the reference sequence scaled to the mean effect size among all alternative mutations, with measured effect sizes of individual variants in the second row. Two of the four significant elements match known motifs<ref type="bibr" target="#b38">39</ref> , and the two unknown motifs partially resemble the SP1 binding motif. Shown in the bottom two rows are the predictions on the full dataset using methods from a that required no additional fine-tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 1 | 2 | 3 |</head><label>123</label><figDesc>Enformer model architecture and comparison to Basenji2. a) From left to right: enformer model architecture, 'dilated' architecture used in ablation studies obtained by replacing the transformer part of the model with dilated convolutions, and basenji22. Output shapes (without batch dimensions) are shown as tuples on the right side of the blocks. The number of trainable parameters for different parts of enformer are shown on the left side of the blocks. The two main hyperparameters of the model are the number of transformer/dilated layers, L, and the number of channels, C. All models have the same two output heads as shown on the enformer at the bottom. The number of channels in the convolutional tower Ci was increased by a constant multiplication factor to reach C channels starting from C/2 (or 0.375*C for basenji2) in 6 layers. For dilated layers, we increased the dilation rate Di by a factor of 1.5 at every layer (rounded to the nearest integer). b) Definition of different network blocks in terms of basic neural network layers. MHA denotes multi-headed attention using relative positional encodings with kq representing the number of key/query size, v representing the value size and h the number of heads. Number of relative positional basis functions is equal to value size v. NATuRE METhoDs | www.nature.com/naturemethods Extended Data Fig. Replicate level accuracy. a) Gene expression correlation (log(1+x) pearsonr) for each CAGe track across protein-coding genes comparing experimental-level accuracy computed in two ways (estimated and direct) to enformer. For 'replicates direct', CAGe replicate experiments were partitioned into two groups and compared against each other. For 'replicates estimated', a predictive model was used to impute CAGe values of a particular track from all other tracks. NATuRE METhoDs | www.nature.com/naturemethods Extended Data Fig. 3 | See next page for caption. NATuRE METhoDs | www.nature.com/naturemethods Extended Data Fig. Predictive performance for treated samples. a) Groups of CAGe experiments where the biological samples were perturbed in different ways. b) Observed and predicted gene expression matrices (log(1+x) transformed) for CD14+ monocytes and genes in the held-out test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Extended Data Fig. 5 | 6 |</head><label>56</label><figDesc>METhoDs | www.nature.com/naturemethods Extended Data Fig. 4 | Enformer predicts mRNA-seq more accurately than ExPecto. a) Test set predictive performance comparison of a linear model trained on top of enformer CAGe predictions from the major TSS (y-axis) and exPecto (x-axis) computed either across genes (first column) or across tissues (second column). Gene expression matrix was normalized across genes to have zero mean and unit variance for each tissue. enformer was re-trained only on the human genome using the same training chromosomes for this comparison (Methods). b) Same as a), but using enformer predictions averaged across all TSS of the gene. c) Observed versus enformer-predicted gene expression values for all 990 test genes in 6 rNA-seq samples. Comparison to dilated convolutions. a) enformer with original transformer layers (extended Data Fig. 1a left) performs better than enformer with dilated convolutions (extended Data Fig. 1a center) across different model sizes and training dataset subsets as measured CAGe gene expression correlation in the validation set (same metric as in Fig. 1b across genes). At 15 dilated layers, the model starts to reach outside of the input sequence range (receptive field of 224,263 bp). Note that all the evaluations here are limited by TPU memory preventing you from using more layers or channels. b) Performance comparison to basenji2 (left) and enformer (right) to enformer with the same receptive field (44 kb) as basenji2 by either allowing a fixed attention radius of 16 across all layers where query can attend to at most 16 positions away (enformer 769: Fixed radius: 16) or by exponentially increasing the respective field in the same way as the dilation rate in basenji2. enformer768 was trained with the same number of 768 channels and 131 kb input sequences as basenji2, whereas enformer uses two times more channels and 1.5 times longer sequence. Same evaluation metrics are shown as in Fig. 1. NATuRE METhoDs | www.nature.com/naturemethods Extended Data Fig. Custom relative positional encoding functions are required for good predictive performance. a) relative positional encoding basis function options for the transformer model. Sine/cosine basis functions are frequently used in the NLP literature for both absolute or relative positional encodings. enformer uses a concatenation of exponential, gamma and central_mask relative positional encodings. For each basis function, a symmetric f(|x|) and asymmetric sign(x) * f(|x|) basis function will be used to introduce directionality and thereby inform the model of what is upstream or downstream of the TSS. each basis function is visualized with a different color. b) Validation set performance as measured by CAGe gene expression correlation across protein coding genes (top; same metric as in Fig. 1b across genes) or across all positions (bottom; same metric as displayed in Fig. 1c CAGe) for models trained with different classes of positional encoding functions in the transformer. Custom relative positional encodings show better performance than using standard sin/cos basis functions or using absolute positional encodings, likely because they can better capture the decreasing importance of enhancers with increased distance. Also, symmetric only (f(|x|) version shows much lower performance than using both, symmetric and asymmetric versions. All models use the same 96 total number of basis functions. each positional encoding configuration was trained with multiple different random seeds. red points denote runs with lower performance than the y-axis limits. NATuRE METhoDs | www.nature.com/naturemethods Extended Data Fig. 7 | Tissue-specific contribution scores are required for good enhancer-recall performance. a) enhancer-gene ranking performance comparison of different AbC score versions, including the original score which uses DNase, H3K27ac, and Hi-C data. H3K27ac / distance is a good if not even slightly better proxy for the AbC score. The alternative versions use a fixed and 2 kb wide aggregation window whereas the original uses a dynamic peak width depending on the DNase peak width. b) Attention-based contribution score as a function of distance at all enhancer-gene pairs in both studied datasets. c) Contribution scores that are cell-type-specific (shown in green, achieved by computing the contribution scores w.r.t. cell-type-specific target variables) outperform cell-type agnostic contribution scores (shown in orange). Colored bars in a and c depict the median auPrC with error bars at the 25th and 75th percentile obtained by sampling 80% of enhancer-gene pairs 100 times without replacement. NATuRE METhoDs | www.nature.com/naturemethods Extended Data Fig. 8 | TF-MoDIsco motifs at TAD boundaries. a,b) Motifs obtained by TF-MoDISco from gradient Ã input DNase (a) or CAGe (b) contribution scores at 1,500 TAD boundaries. Motif title contains: TF name of the closest motif match from HOCOMOCO v11 database, metacluster and pattern id returned by TF-MoDISco, number of seqlets supporting the motifs, and Tomtom q-value for the closest motif match (lower means better match). For each motif, the information content of the position frequency matrix (PFM) is visualized for the database motif in the top row and for the actual TF-MoDISco motif in the second row. Third row for each motif shows the contribution weight matrix (CWM)15,47 which can be negative. Shown are the top 6 motifs for each contribution score sign with sufficiently close match to a known motif (q-value&lt;1e-5) and support from at least 200 seqlets. Interestingly, the CTCF motif was discovered for both CAGe and DNase in both contribution score signs, suggesting that it can influence them in a positive or negative manner. NATuRE METhoDs | www.nature.com/naturemethods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-6.png" coords="13,43.24,55.56,508.80,136.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-8.png" coords="16,43.24,55.56,508.80,256.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-9.png" coords="17,43.24,55.56,508.80,306.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-10.png" coords="18,49.45,55.56,496.37,299.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-12.png" coords="20,43.96,55.56,507.36,312.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-13.png" coords="21,43.26,55.56,508.75,240.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-14.png" coords="22,43.24,55.56,508.80,339.48" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">NATuRE METhoDs | VOL 18 | OCTOber 2021 | 1196-1203 | www.nature.com/naturemethods</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1">NATuRE METhoDs | www.nature.com/naturemethods</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank M. Kircher (Berlin Institute of Health) for sharing saturation mutagenesis MPRA datasets and variant effect predictions with us as well as A. Pritzel, A. W. R. Nelson, A. Patterson, A. Obika, C. Meyer, D. Hassabis, M. Dunlop, N. Latysheva, N. Alic, S.-J. Dunn, S. Petersen, T. Niccoli, T. Sargeant, and T. Back for their contributions and support.</p></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data availability</head><p>Gene annotation was obtained from https://www.gencodegenes.org/ (v32). Basenji2 training, validation, and test data was obtained from https://console.cloud.google. com/storage/browser/basenji_barnyard/data. Processed CRISPRi data for Fulco et al 2019 <ref type="bibr" target="#b12">13</ref> was obtained from supplementary material and for Gasperini et al 2019 <ref type="bibr" target="#b8">9</ref> from GEO accession GSE120861. H3K27ac ChIP-seq data in K562 used for analysis in Fig. <ref type="figure">2</ref> was obtained from https://www.encodeproject.org/ with file accession ENCFF779QTH and DNase with file accessions ENCFF413AHU and ENCFF936BDN. TAD boundaries processed by Fudenberg et al 2020 <ref type="bibr" target="#b31">32</ref> were obtained from https://console.cloud.google.com/storage/browser/basenji_hic/ insulation. Fine-mapped eQTLs are available from the supplementary material of Wang et al 2021 <ref type="bibr" target="#b21">22</ref> and the negative set from https://console.cloud.google.com/storage/ browser/dm-enformer/data/gtex_fine. We acquired training and test sets as well as the predictive accuracies of individual competition participants from the CAGI5 competition <ref type="bibr" target="#b25">26</ref> (M. Kircher, personal communication, https://genomeinterpretation. org/content/expression-variants). For comparison to ExPecto, we used the provided data from https://github.com/FunctionLab/ExPecto/tree/master/resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code availability</head><p>All components of our core algorithm, including the full model architecture and example code to train and evaluate the model are available under the open source Apache 2.0 license at the following URL: https://github.com/deepmind/ deepmind-research/tree/master/enformer. The code is also archived at Zenodo https://doi.org/10.5281/zenodo.5098375 <ref type="bibr" target="#b45">46</ref> . In addition, layer components of the model are now available in the existing Basenji repository for biological sequence deep learning at https://github.com/calico/basenji also under the open source Apache 2.0 license.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Artful combination of these models with our own could improve Enformer's modeling of insulators and distal regulation. A limitation of the current approach is that we can model and predict only for cell types and assays in the training data and cannot generalize to new cell types or assays. Parallel research has begun to address this shortcoming via representation learning of cell types and assays and could make use of the Enformer architecture in the future <ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35</ref> . The sensitivity of the model to genetic variants could be further improved by training upon the growing number of functional genomic datasets, such as those derived from CRISPR perturbations and massively parallel reporter assays. Currently, the small size of these datasets has limited their usage only to model evaluation. Finally, we anticipate that recent improvements in the computational efficiency <ref type="bibr" target="#b35">36</ref> of transformer models together with better hardware will allow us to further scale-up the models.</p><p>In the future, Enformer could be systematically applied to fine-map existing GWAS studies <ref type="bibr" target="#b21">22</ref> , prioritize rare or de novo variants observed for rare disorders <ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38</ref> , and impute regulatory activity across species to study cis-regulatory evolution 2 . To foster these downstream applications, we have made the pretrained Enformer model openly available along with code examples demonstrating its use. Furthermore, we have precomputed effect predictions for all frequent variants in the 1000 Genomes dataset and made them openly available. We hope that our model will stimulate an improved understanding of gene-regulatory architecture and facilitate the development of improved diagnostic tools for diseases of genetic origin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>online content</head><p>Any methods, additional references, Nature Research reporting summaries, source data, extended data, supplementary information, acknowledgements, peer review information; details of author contributions and competing interests; and statements of data and code availability are available at https://doi.org/10.1038/ s41592-021-01252-x. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author contributions</head><p>Å½.A., J.R.L., D.R.K., and P.K. initiated the project. Å½.A., V.A., and D.R.K. conceived of the study and designed the analyses. Å½.A. designed the model with help from D.V., J.J., and P.K.. Å½.A., D.V., K.R.T., and Y.A. implemented the model. Å½.A. performed model performance analyses, A.G-B. and Å½.A. performed enhancer prioritization analysis, D.R.K. performed variant effect analysis on population genetic data, and V.A. performed variant effect analyses for MPRA data. J.R.L., J.J., P.K., and D.R.K. supervised the study. Å½.A., V.A., and D.R.K. prepared the manuscript with input from all authors. The authors received no specific external funding for this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Competing interests</head><p>Å½.A., A.G-B., K.R.T., Y.A., J.J., and P.K. are employed by DeepMind. V.A., and D.R.K. are employed by Calico Life Sciences. J.R.L. is employed by Google. The remaining authors declare no competing interests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional information</head><p>Extended data is available for this paper at https://doi.org/10.1038/s41592-021-01252-x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary information</head><p>The online version contains supplementary material available at https://doi.org/10.1038/s41592-021-01252-x.</p><p>Extended Data Fig. <ref type="figure">10</ref> | Enformer outperforms Basenji2 on eQTL sign prediction. For each of the GTex tissues, we manually matched FANTOM5 CAGe sample descriptions to choose a single matched dataset (Methods). We then arranged a classification task to discriminate between fine-mapped causal eQTLs in which the minor allele increases gene expression versus eQTLs in which the minor allele decreases gene expression. We computed aurOC statistics by ranking causal variants by their signed prediction for the corresponding sample. To consider the influence of variant distance to TSS, we compute aurOC in four bins of roughly equal size. Across tissues and TSS distances, enformer predictions usually achieve more accurate classification of eQTL sign than basenji2 predictions. We display six example tissues with large numbers of fine-mapped eQTLs and with clear correspondence between CAGe and GTex tissues. Violin plots show the aurOC distribution of 100 bootstrap samples from the full set of variants. (The white dot represents the median, the thick gray bar in the center represents the 25%-75% percentile range and the thin line represents the entire data range.) Dashed lines represent the mean aurOC over all distances. both models struggle with variants beyond the promoter (TSS distance &gt; 1,000), highlighting an important problem for future research.</p><p>NATuRE METhoDs | www.nature.com/naturemethods</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning sequence-based ab initio prediction of variant effects on expression and disease risk</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Genet</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1171" to="1179" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cross-species regulatory sequence activity prediction</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Kelley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">e1008050</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sequential regulatory activity prediction across chromosomes with convolutional neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Kelley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Res</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="739" to="750" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predicting mRNA abundance directly from genomic sequence using deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shendure</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell Rep</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">107663</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards a comprehensive catalogue of validated and target-linked human enhancers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gasperini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shendure</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Genet</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="292" to="310" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DNABERT: pre-trained bidirectional encoder representations from transformers model for DNA-language in genome</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Davuluri</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btab083</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/btab083" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A genome-wide framework for mapping gene regulation via cellular genetic screens</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gasperini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">176</biblScope>
			<biblScope unit="page" from="377" to="390" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A promoter-level mammalian expression atlas</title>
	</analytic>
	<monogr>
		<title level="m">FANTOM Consortium and the RIKEN PMI and CLST (DGT)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">507</biblScope>
			<biblScope unit="page" from="462" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The selection and function of cell type-specific enhancers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Heinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Romanoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Benner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Mol. Cell Biol</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="144" to="154" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning important features through propagating activation differences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shrikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Greenside</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kundaje</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3145" to="3153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Activity-by-contact model of enhancer-promoter regulation from thousands of CRISPR perturbations</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Fulco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Genet</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="1664" to="1669" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep learning: new computational modelling techniques for genomics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Eraslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Å½</forename><surname>Avsec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gagneur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Theis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Genet</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="389" to="403" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Base-resolution models of transcription-factor binding reveal soft motif syntax</title>
		<author>
			<persName><forename type="first">Å½</forename><surname>Avsec</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41588-021-00782-6</idno>
		<ptr target="https://doi.org/10.1038/s41588-021-00782-6" />
	</analytic>
	<monogr>
		<title level="j">Nat. Genet</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Expanded encyclopaedias of DNA elements in the human and mouse genomes</title>
		<author>
			<persName><forename type="first">Project</forename><surname>Encode</surname></persName>
		</author>
		<author>
			<persName><surname>Consortium</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">583</biblScope>
			<biblScope unit="page" from="699" to="710" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">gkmSVM: an R package for gapped-kmer SVM</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ghandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="2205" to="2207" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Predicting effects of noncoding variants with deep learning-based sequence model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">G</forename><surname>Troyanskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="931" to="934" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Basset: learning the regulatory code of the accessible genome with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Rinn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Res</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="990" to="999" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The GTEx Consortium. The GTEx Consortium atlas of genetic regulatory effects across human tissues</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Consortium</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">369</biblScope>
			<biblScope unit="page" from="1318" to="1330" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Detecting genome-wide directional effects of transcription factor binding on polygenic disease risk</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Reshef</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Genet</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1483" to="1493" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Leveraging supervised learning for functionally informed fine-mapping of cis-eQTLs identifies an additional 20,913 putative causal eQTLs</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">3394</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A simple new approach to variable selection in regression, with application to genetic fine mapping</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Carbonetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stephens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="1273" to="1300" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Functionally informed fine-mapping and polygenic localization of complex trait heritability</title>
		<author>
			<persName><forename type="first">O</forename><surname>Weissbrod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Genet</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1355" to="1363" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Saturation mutagenesis of twenty disease-associated regulatory elements at single base-pair resolution</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kircher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schubach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">3583</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Integration of multiple epigenomic marks improves prediction of variant impact in saturation mutagenesis reporter assay</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shigaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hum. Mutat</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1280" to="1291" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A method to predict the impact of regulatory variants from DNA sequence</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Genet</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="955" to="961" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">CADD: predicting the deleteriousness of variants throughout the human genome</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rentzsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shendure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kircher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="D886" to="D894" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Predicting the sequence specificities of DNA-and RNA-binding proteins by deep learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alipanahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Delong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Weirauch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biotechnol</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="831" to="838" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Enhancer evolution across 20 mammalian species</title>
		<author>
			<persName><forename type="first">D</forename><surname>Villar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page">554</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A generative neural network for maximizing fitness and diversity of synthetic DNA and protein sequences</title>
		<author>
			<persName><forename type="first">J</forename><surname>Linder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell Syst</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="49" to="62" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Predicting 3D genome folding from DNA sequence with Akita</title>
		<author>
			<persName><forename type="first">G</forename><surname>Fudenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Pollard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1111" to="1117" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">DeepC: predicting 3D genome folding using megabase-scale transfer learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Schwessinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1118" to="1124" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Avocado: a multi-scale deep tensor factorization method learns a latent representation of the human epigenome</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Durham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">81</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Integrating regulatory DNA sequence and gene expression to predict genome-wide chromatin accessibility across cellular contexts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Perricone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kundaje</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="108" to="116" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Efficient transformers: a survey</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2009.06732" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Genomic analyses implicate noncoding de novo variants in congenital heart disease</title>
		<author>
			<persName><forename type="first">F</forename><surname>Richter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Genet</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="769" to="777" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Whole-genome deep-learning analysis identifies contribution of noncoding mutations to autism risk</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Genet</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="973" to="980" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Quantifying similarity between motifs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Stamatoyannopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">R24</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
		<title level="s">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="464" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics 2978-2988</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics 2978-2988</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The Human Genome Browser at UCSC</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Kent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="996" to="1006" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Open sourcing Sonnet -a new library for constructing neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Reynolds</surname></persName>
		</author>
		<ptr target="https://deepmind.com/blog/open-sourcing-sonnet" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scikit-learn: machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A systematic evaluation of the design and context dependencies of massively parallel reporter assays</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1083" to="1091" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Enformer (Version 3.0) (Zenodo</title>
		<author>
			<persName><forename type="first">Å½iga</forename><surname>Avsec</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5098375</idno>
		<ptr target="https://doi.org/10.5281/zenodo.5098375" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The Kipoi repository accelerates community exchange and reuse of predictive models for genomics</title>
		<author>
			<persName><forename type="first">Å½</forename><surname>Avsec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biotechnol</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="592" to="600" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
