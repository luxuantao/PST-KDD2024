<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Statistically Significant Pattern Mining with Ordinal Utility</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Thien</forename><forename type="middle">Q</forename><surname>Tran</surname></persName>
							<email>thientquang@mdl.cs.tsukuba.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Tsukuba</orgName>
								<address>
									<settlement>Riken AIP</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kazuto</forename><surname>Fukuchi</surname></persName>
							<email>fukuchi@cs.tsukuba.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Tsukuba</orgName>
								<address>
									<settlement>Riken AIP</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Youhei</forename><surname>Akimoto</surname></persName>
							<email>akimoto@cs.tsukuba.ac.jp</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Tsukuba</orgName>
								<address>
									<settlement>Riken AIP</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Sakuma</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Tsukuba</orgName>
								<address>
									<settlement>Riken AIP</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Statistically Significant Pattern Mining with Ordinal Utility</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394486.3403215</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Significant pattern mining</term>
					<term>multiple testing</term>
					<term>high-utility pattern</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Statistically significant patterns mining (SSPM) is an essential and challenging data mining task in the field of knowledge discovery in databases (KDD), in which each pattern is evaluated via a hypothesis test. Our study aims to introduce a preference relation into patterns and to discover the most preferred patterns under the constraint of statistical significance, which has never been considered in existing SSPM problems. We propose an iterative multiple testing procedure that can alternately reject a hypothesis and safely ignore the hypotheses that are less useful than the rejected hypothesis. One advantage of filtering out patterns with low utility is that it avoids consumption of the significance budget by rejection of useless (that is, uninteresting) patterns. This allows the significance budget to be focused on useful patterns, leading to more useful discoveries.</p><p>We show that the proposed method can control the familywise error rate (FWER) under certain assumptions, that can be satisfied by a realistic problem class in SSPM.We also show that the proposed method always discovers a set of patterns that is at least equally or more useful than those discovered using the standard Tarone-Bonferroni method SSPM.Finally, we conducted several experiments with both synthetic and real-world data to evaluate the performance of our method. As a result, in the experiments with real-world datasets, the proposed method discovered a larger number of more useful patterns than the existing method for all five conducted tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS Concepts</head><p>â€¢ Information systems â†’ Association rules; â€¢ Mathematics of computing â†’ Hypothesis testing and confidence interval computation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Statistically significant pattern mining (SSPM) is the task of finding patterns that statistically occur more often in the data for one class than for another. Different from other pattern mining frameworks, in SSPM, the discovery of each pattern is evaluated via a statistical hypothesis test: a process to obtain a p-value which quantifies the probability that the association observed in the data is due to chance. The goal of SSPM is to maximize the number of true discoveries, i.e., minimize type-II error while controlling the number of false discoveries, i.e., controlling type-I error. With its statistical guarantee, SSPM is important and widely applied in fields such as genetics, healthcare, and market analysis. For example, in healthcare, discoveries of treatment combinations that are significantly efficacious could improve healthcare service quality. Similarly, medical scientists are interested in finding significant patterns of gene alleles associated with the onset of disease.</p><p>In this study, we focus on the utility of patterns, an aspect that is not considered in existing SSPM works. Pattern utility is essential in many applications spanning medicine, finance, and e-commerce, among others <ref type="bibr" target="#b3">[4]</ref>. For example, assume that we have the Adult dataset in which each transaction contains several demographic attribute values as explanatory variables and a binary target class (if income &gt; 50K or not). In general, SSPM aims to find patterns of demographic attributes (e.g., "university graduate" and "works 60 hours/week") that are significantly associated with "income &gt; 50K". Herein, we further introduce a preference relation between patterns and aim to find out the most useful patterns under the constraint of statistical significance. Continuing with the same example, assume that we are more interested in patterns that realize "higher income with less education" than "higher income with higher education"; we can define patterns with "less education" as being more preferred than patterns with "higher education".</p><p>This paper serves to propose SPUR (Significant Pattern mining with Utility Relationship), a method that aims to discover statistically significant patterns with the highest utility. There are three main challenges that must be solved to achieve this goal.</p><p>First, our goal is different from the standard goal of SSPM. Generally, SSPM aims to discover as many patterns as possible, while controlling type-I error. Our goal is to discover the patterns that have the highest utility while still controlling type-I error. Therefore, existing SSPM methods, such as Tarone-Bonferroni (T-Bonferroni), are not necessarily practical for our setting. Since these methods do not consider preferences for different patterns, they may waste the significance budget for controlling the type-I error of patterns which are not useful. Here, we propose that after discovering a pattern, we can ignore all the patterns that are less useful than the discovered pattern. We expect that by ignoring less useful patterns, we can discover more useful patterns without violating the type-I error constraint.</p><p>Fig. <ref type="figure" target="#fig_0">1</ref> demonstrates the statistically significant patterns in the Adult dataset found by the existing T-Bonferroni method (top) and our SPUR method (bottom). Specifically, we show the discovered patterns of "education level" and "work hour" while fixing other demographic features. We defined "less education" and "less hoursper-week" as being more preferred than "higher education" and "more hours-per-week". As we see from the figure, in two different demographic conditions, our method can find more preferred (i.e., "less education and less hours-per-week" but "higher income") patterns than the existing method with fewer rejections.</p><p>Second, since each discovery in SSPM is evaluated via a statistical test, the proposed approach is only valid if we can develop an iterative multiple-testing procedure that can safely ignore less useful hypotheses after rejecting a hypothesis. By "safely", we mean that it does not violate the type-I error constraint. To the best of our knowledge, no existing multiple-testing procedure can fulfill this requirement. We discovered that this is achievable for a specific SSPM problem class. We propose a method that can control the FWER under a particular assumption about the independence of p-values, in that the p-values of false null hypotheses and true null hypotheses are mutually independent. In the SSPM context, this assumption can be satisfied when Fisher's exact test is used, and the setup of patterns guarantees that there is no overlap of samples between distinct patterns (see Section 3.3 for more details). We note that such a setting fits well for most of our intended applications.</p><p>Finally, because we often have to consider numerous patterns in the SSPM task, the number of hypotheses in the multiple-testing problem is also large. Generally, when the number of tests is large, simple multiple-testing adjustments, such as the Bonferroni correction, become too conservative for discovering significant patterns. Recently, many studies focus on the Tarone's trick <ref type="bibr" target="#b19">[19]</ref>, which takes account of "untestable" hypotheses, resulting in more discoveries while still controlling the FWER. Thus, to be practical in the SSPM setting, our method must also be able to leverage the Tarone's trick.</p><p>Our contributions in this work are as follows:</p><p>â€¢ We introduce the problem of discovering statistically significant patterns with the highest utility, given the ordinal utility between patterns. Then, we propose an iterative multipletesting method that can reject more useful hypotheses and carefully ignore less useful hypotheses at each iteration. â€¢ We prove that the proposed method can control the FWER under a particular assumption (independence of p-values between true and false hypotheses), which can be satisfied in a realistic SSPM problem class. â€¢ We prove that the proposed method can always achieve a result that has a higher utility compared to T-Bonferroni, a standard SSPM method. â€¢ We conduct several experiments using both synthetic and real-world datasets. For the real-world experiment, our method achieves a discovery result with higher utility for all five conducted tasks.</p><p>The remainder of the paper is organized as follows. We introduce some related works and fundamental concepts in Section 2 and 3. Then, we formally define our problem setting in Section 4. Details concerning the proposed method are provided in Section 5. We prove the theoretical guarantee for FWER and improvement of utility in Section 6. Next, we demonstrate that the proposed method outperforms other methods in discovering useful patterns in Section 7, using both synthetic and real-world data. Finally, brief conclusions are put forward in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The most fundamental challenge in SSPM is the explosion in the number of patterns due to the number of factors. Some methods have been proposed to overcome this challenge in terms of improving discovery power and navigating complexity. Early works in this respect were proceeded in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">22]</ref>. Then, Limitless Arity Multipletesting Procedure (LAMP) -a method which can efficiently discover significant patterns in the higher-order were proposed in <ref type="bibr" target="#b20">[20]</ref>. LAMP is designed with a combination of the Tarone's trick <ref type="bibr" target="#b19">[19]</ref> and the association rule mining algorithm Apriori <ref type="bibr" target="#b0">[1]</ref>. Other studies have attempted to improve or extend LAMP to other settings <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b21">21]</ref>.</p><p>There is also a body of literature focused on other types of tests or other aspects of the problem. For example, <ref type="bibr" target="#b11">[11]</ref> studied the Westfall-Young permutation test for the purpose of dealing with the dependence between patterns. <ref type="bibr" target="#b9">[10]</ref> worked on Cochran-Mantel-Haenszel and <ref type="bibr" target="#b13">[13]</ref> focused on Barnard's exact test. Moreover, <ref type="bibr" target="#b7">[8]</ref> focused in the statistical emerging pattern mining problem, <ref type="bibr" target="#b23">[23]</ref> considered the hypotheses stream problem, while <ref type="bibr" target="#b18">[18]</ref> focused on finding significant interactions between continuous features. However, to the best of our knowledge, no SSPM studies have hitherto focused on the utility of patterns.</p><p>In pattern mining, many studies have focused on the utility of patterns, namely utility-oriented pattern mining (UPM). UPM is an essential task with numerous applications in finance, medicine, and e-commerce, among others <ref type="bibr" target="#b3">[4]</ref>. Many UPM approaches have been proposed, expanding to various subfields, including high-utility </p><formula xml:id="formula_0">ğ‘¦ ğ‘– = 1 ğ‘ ğ‘  ğ‘› 1 âˆ’ ğ‘ ğ‘  ğ‘› 1 ğ‘¦ ğ‘– = 0 ğ‘› ğ‘  âˆ’ ğ‘ ğ‘  ğ‘› ğ· âˆ’ ğ‘› ğ‘  âˆ’ (ğ‘› 1 âˆ’ ğ‘ ğ‘  ) ğ‘› ğ· âˆ’ ğ‘› 1 Total ğ‘› ğ‘  ğ‘› ğ· âˆ’ ğ‘› ğ‘  ğ‘› ğ·</formula><p>item sets <ref type="bibr" target="#b24">[24]</ref>, high-utility rules <ref type="bibr" target="#b8">[9]</ref>, and maximal high utility <ref type="bibr" target="#b17">[17]</ref>. However, no studies on UPM have offered a statistical guarantee for discovered patterns.</p><p>3 Preliminaries</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Statistical association testing</head><p>Statistical association testing is a procedure for testing whether two random variables are statistically dependent, or in other words, associated. In the context of pattern mining, this is a procedure to test whether the presence of a pattern is related with the occurrence of a specific event, represented by a class label. Suppose that we have a dataset ğ· = {ğ‘¡ 1 , ğ‘¡  <ref type="table" target="#tab_0">1</ref>. Here, ğ‘› ğ· is the total number of transactions, and ğ‘› 1 is the number of transactions with label ğ‘¦ = 1. Moreover, ğ‘› ğ‘  is the support for pattern ğ‘ ; i.e., the number of transactions that contain pattern ğ‘  and ğ‘ ğ‘  is the support for pattern ğ‘  among class ğ‘¦ = 1. A widely used independence test in SSPM is Fisher's exact test, which is a conditioned test in which ğ‘› ğ· , ğ‘› 1 , ğ‘› ğ‘  are assumed to be fixed <ref type="bibr" target="#b2">[3]</ref>. Under the null hypothesis of no association between ğ‘‹ ğ‘  and ğ‘¦, the count cell ğ‘ ğ‘  follows a hypergeometric distribution ğ‘ƒ (. | ğ‘› ğ· , ğ‘› 1 , ğ‘› ğ‘  ). Thus, the probability of observing the current table can be calculated as follows:</p><formula xml:id="formula_1">ğ‘ƒ (ğ‘ ğ‘  | ğ‘› ğ· , ğ‘› 1 , ğ‘› ğ‘  ) = ğ‘› ğ· ğ‘ ğ‘  ğ‘› ğ· âˆ’ğ‘› 1 ğ‘› ğ‘‡ âˆ’ğ‘¥ ğ‘‡ ğ‘› ğ· ğ‘› ğ‘‡ .</formula><p>Consequently, the p-value, i.e., the probability of observing a contingency table that is equally or more extreme as the observed table under the null hypotheses, can be obtained as</p><formula xml:id="formula_2">ğ‘ (ğ‘ ) val = min{ğ‘› ğ· ,ğ‘› ğ‘  } ğ‘˜=ğ‘ ğ‘  ğ‘ƒ (ğ‘˜ |ğ‘› ğ· , ğ‘› 1 , ğ‘› ğ‘  ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>If the p-value ğ‘ (ğ‘ )</head><p>val â‰¤ ğ›¼ holds for some significance level ğ›¼, we can reject the null hypothesis of no association, and conclude that pattern ğ‘  is associated with outcome ğ‘¦ under the significance level ğ›¼. Through the significance testing procedure, the probability of falsely rejecting a true null hypothesis, i.e., the probability of falsely discovering a false pattern, is controlled under the desired significance level ğ›¼.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multiple testing</head><p>In the previous section, we described an individual test for a pattern. However, in association mining, we have to consider many patterns, which means that multiple hypotheses have to be tested at the same time. If each test is conducted independently with a significance level of ğ›¼, the probability that at least one false discovery occurs would be much larger than ğ›¼. Typically, in such a case, it is necessary to control the overall error of all tested hypotheses. Two criteria are widely used in this respect: the familywise error rate (FWER) and false discovery rate (FDR). In what follows, we focus on controlling the FWER, which is defined as the probability of making at least one false rejection. Assume that we want to test a hypotheses set ğ» = {â„ 1 , . . . , â„ |ğ» | }. Letting ğ‘‡ âŠ† ğ» be the subset of true null hypotheses and ğ‘… âŠ† ğ» be the set of hypotheses that were rejected by the multiple-testing procedure, FWER is defined as follows: Definition 3.1 (Familywise error rate (FWER)):</p><formula xml:id="formula_3">FWER = Pr ğ‘… âˆ© ğ‘‡ â‰  âˆ…</formula><p>The most straightforward method to control the FWER is the Bonferroni correction, which uses a corrected rejection threshold ğ›¿ = ğ›¼/ğ‘š for each test. However, when the number of hypotheses is large, the Bonferroni correction can become too conservative. Recently, in the context of SSPM, many methods starting with LAMP <ref type="bibr" target="#b20">[20]</ref> have leveraged the Tarone's trick <ref type="bibr" target="#b19">[19]</ref> to exclude untestable hypotheses that will never be significant. To be specific, for tests in which the test statistics are discrete, we can evaluate the minimal attainable p-value for that test. When ğœ“ (â„) is the minimal attainable p-value, hypothesis â„ will never be rejected by a threshold ğœ if ğœ“ (â„) &gt; ğœ. In the case of Fisher's exact test, the minimal attainable p-value for a pattern ğ‘  is obtained as</p><formula xml:id="formula_4">ğœ“ (ğ‘ ) = ğ‘ (ğ‘˜ | ğ‘› ğ· , ğ‘› 1 , ğ‘› ğ‘  ) where ğ‘˜ = min{ğ‘› 1 , ğ‘› ğ‘  }.</formula><p>Using ğœ“ (ğ‘ ), regardless of the count cell ğ‘ ğ‘  , if ğœ“ (ğ‘ ) &gt; ğ›¿, the hypothesis related to pattern ğ‘  will never be significant for significance level ğ›¿. In SSPM, ğ‘› 1 can be considered as fixed because the data are commonly given beforehand. Moreover, since ğ‘› ğ‘  = ğ‘– ğ‘‹ ğ‘–,ğ‘  , pattern ğ‘  can be ignored if there are too few transactions that contain pattern ğ‘ , i.e., if the support for that pattern is too small. Since the number of hypotheses to be considered decreases with Tarone's trick, a larger rejection threshold can be used and results in more discoveries. Formally, the T-Bonferroni method leverages the Tarone's trick by setting the rejection threshold ğœ Tarone as follows:</p><formula xml:id="formula_5">ğœ Tarone = max{ğœ | ğœ |ğœ… (ğœ)| â‰¤ ğ›¼ }.</formula><p>where ğœ… (ğœ) = {â„ | ğœ“ (â„) â‰¤ ğœ } is the testable hypothesis set regarding the rejection threshold ğœ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Assumption and the target problem class</head><p>Next, we introduce the key assumption that is necessary for our method followed by the target problem class of the proposed method. In other words, we assume that the p-values obtained from the true patterns and the false patterns are mutually independent<ref type="foot" target="#foot_0">1</ref> . We next claim that Assumption 3.1 holds for certain types of SSPM problems by the following proposition. As a counter-example, Assumption 3.1 does not hold for a set ğ‘† which contains both pattern {ğ‘} and pattern {ğ‘, ğ‘}, since a transaction that contains {ğ‘, ğ‘} will also contain {ğ‘}. It is noted that the categorical dataset setup fits well for most of our intended applications and is adopted for our real-world experiment in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Problem setting</head><p>To formally define our problem setting, we first introduce the concept of ordinal utility and clarify our goal of finding significant patterns with the highest utility. Next, we define a criterion for evaluating the goodness of the discovered result and discuss the limitation of existing approaches in our setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ordinal utility and the dominating subset</head><p>We focus on the ordinal utility between patterns, i.e., which of two options is better. This differs from cardinal utility, which would consider how good the two options are and thus how much better one option is compared to the other. We chose ordinal utility for our setting because, in SSPM, assigning a utility score for each pattern is not always practical for patterns with multiple items. By contrast, using (partial) ordinal utility, we can easily define a flexible utility relationship between patterns, possibly using their items. We use ğ‘  1 â‰» ğ‘  2 and ğ‘  1 â‰ˆ ğ‘  2 to denote that "pattern ğ‘  1 is useful to pattern ğ‘  2 " and "pattern ğ‘  1 is equally useful as pattern ğ‘  2 ", respectively. Moreover, since each pattern ğ‘  is evaluated via a hypothesis test â„ ğ‘  in SSPM, we similarly use â„ ğ‘  1 â‰» â„ ğ‘  2 and â„ ğ‘  1 â‰ˆ â„ ğ‘  2 to denote the ordinal utility between hypotheses. A preference order of utility for a pattern set ğ‘† is defined in the following. Such a preference order can be predefined based on the background knowledge or preferences of the user. For example, suppose that we are considering a dataset on medication usage in which each transaction includes a combination of drugs used by a patient and a binary class cured or not-cured. Moreover, we assume that we can define the total cost ğ‘ğ‘œğ‘ ğ‘¡ (ğ‘ ) and the adverse effect level ğ‘ğ‘‘ğ‘£ (ğ‘ ) for each drug combination pattern ğ‘  as ordinal levels. A user who prefers patterns with a lower cost and less adverse effects can define the ordinal utility between patterns as We show an example of a pattern set ğ‘… = {ğ‘, ğ‘, ğ‘, ğ‘‘, ğ‘’, ğ‘“ } in Figure <ref type="figure" target="#fig_4">2</ref> where each point ğ‘, . . . , ğ‘“ represents the utility of that pattern, i.e., the cost and the adverse effect levels. The colored rectangle next to each point represents the area that is less useful than that pattern. From the foregoing definition, we have that â„ ğ‘ â‰º â„ ğ‘‘ , â„ ğ‘’ â‰º â„ ğ‘ , etc. We notice that there are pairs of patterns such as â„ ğ‘ and â„ ğ‘‘ in Figure <ref type="figure" target="#fig_4">2</ref> that are not comparable. In other words, we do not require the preference order â‰º to be complete. This design choice is especially useful for the pattern mining setting, where each pattern contains multiple items, and multiple aspects of the pattern can be considered.</p><formula xml:id="formula_6">ğ‘ğ‘œğ‘ ğ‘¡ (ğ‘  1 ) = ğ‘ğ‘œğ‘ ğ‘¡ (ğ‘  2 ) âˆ§ ğ‘ğ‘‘ğ‘£ (ğ‘  1 ) = ğ‘ğ‘‘ğ‘£ (ğ‘  2 ) â‡â‡’ â„ ğ‘  1 â‰ˆ â„ ğ‘  2 ğ‘ğ‘œğ‘ ğ‘¡ (ğ‘  1 ) &lt; ğ‘ğ‘œğ‘ ğ‘¡ (ğ‘  2 ) âˆ§ ğ‘ğ‘‘ğ‘£ (ğ‘  1 ) â‰¤ ğ‘ğ‘‘ğ‘£ (ğ‘  2 ) =â‡’ â„ ğ‘  1 â‰» â„ ğ‘  2 ğ‘ğ‘œğ‘ ğ‘¡ (ğ‘  1 ) â‰¤ ğ‘ğ‘œğ‘ ğ‘¡ (ğ‘  2 ) âˆ§ ğ‘ğ‘‘ğ‘£ (ğ‘  1 ) &lt; ğ‘ğ‘‘ğ‘£ (ğ‘  2 ) =â‡’ â„ ğ‘  1 â‰» â„ ğ‘  2 .</formula><p>In SSPM, the utility of a pattern can be defined using the items included in that pattern. For example, in the medication example, let ğ‘ğ‘œğ‘ ğ‘¡ (ğ‘¡) and ğ‘ğ‘‘ğ‘£ (ğ‘¡) be the ordinal level of the cost and the adverse effect level for a drug ğ‘¡. We can define a utility function ğ‘ˆ : ğ‘† â†’ ğ‘ ğ‘‘ as</p><formula xml:id="formula_7">ğ‘ˆ (ğ‘ ) = ğ‘ğ‘œğ‘ ğ‘¡ (ğ‘ ) ğ‘ğ‘‘ğ‘£ (ğ‘ ) = ğ‘¡ âˆˆğ‘  ğ‘ğ‘œğ‘ ğ‘¡ (ğ‘¡) max ğ‘¡ âˆˆğ‘  ğ‘ğ‘‘ğ‘£ (ğ‘ )</formula><p>.</p><p>Next, we recall that our goal is to discover significant patterns with the highest utility. Hence, the goodness of a discovered pattern set is determined only by the patterns with the highest utility in that set. We call such a subset the (utility) dominating subset. </p><formula xml:id="formula_8">(ğ¾) = {ğ‘  âˆˆ ğ¾ | ğ‘  â€² âˆˆğ¾ , ğ‘  â‰º ğ‘  â€² }</formula><p>We also illustrate this concept by showing the dominating set ğ‘‘ğ‘œğ‘š(ğ‘…) for ğ‘… = {ğ‘, ğ‘, ğ‘, ğ‘‘, ğ‘’, ğ‘“ } in Figure <ref type="figure" target="#fig_4">2</ref> (right). We see that since ğ‘ â‰º â„ (and also ğ‘, ğ‘‘), we have ğ‘ âˆ‰ ğ‘‘ğ‘œğ‘š(ğ‘…). Similarly, we also have that ğ‘’ âˆ‰ ğ‘‘ğ‘œğ‘š(ğ‘…) and ğ‘‘ âˆ‰ ğ‘‘ğ‘œğ‘š(ğ‘…) and we can obtain the dominating subset ğ‘‘ğ‘œğ‘š(ğ‘…) = {ğ‘, ğ‘, ğ‘‘ }.</p><p>Our setting also requires that the discovered patterns must be true patterns. That is, letting the set of true patterns (the set of false null hypotheses) be ğ¹ , the best discovery result that can be achieved is ğ‘‘ğ‘œğ‘š(ğ¹ ). However, in the SSPM setting, the number of discoveries is often limited because we have to control the number of false discoveries. Hence, it is often not possible to achieve ğ‘‘ğ‘œğ‘š(ğ¹ ), especially when the number of true patterns is large. Thus, the practical goal in our setting is to discover a pattern set whose utility is as close as ğ‘‘ğ‘œğ‘š(ğ¹ ) as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Criterion for utility evaluation</head><p>Here, we introduce a metric to compare the goodness of two pattern sets, or in other words, two rejected hypothesis sets. This is then used to compare the utilities between discovered set ğ‘‘ğ‘œğ‘š(ğ‘…) and optimal set ğ‘‘ğ‘œğ‘š(ğ¹ ). We also use this criterion to compare the results of different methods, which is required in the real-world setting, with unknown ğ¹ . Definition 4.3 (Utility measure): Given two pattern sets ğ¾ and ğ¾ â€² , the utility measure from ğ¾ â€² to ğ¾ is denoted by ğ· ğ‘¢ (ğ¾ âˆ¥ğ¾ â€² ).</p><formula xml:id="formula_9">ğ· ğ‘¢ (ğ¾ âˆ¥ğ¾ â€² ) = |{â„ âˆˆ ğ‘‘ğ‘œğ‘š(ğ¾) | â„ â€² âˆˆğ‘‘ğ‘œğ‘š (ğ¾ â€² ) , â„ âª¯ â„ â€² }|.</formula><p>Intuitively, ğ· ğ‘¢ (ğ¾ âˆ¥ğ¾ â€² ) is the number of more useful patterns in ğ‘‘ğ‘œğ‘š(ğ¾) that are not included in ğ‘‘ğ‘œğ‘š(ğ¾ â€² ). We note that ğ· ğ‘¢ (ğ¾ âˆ¥ğ¾ â€² ) can be asymmetric, that is, can be different from ğ· ğ‘¢ (ğ¾ â€² âˆ¥ğ¾). In Figure <ref type="figure" target="#fig_1">3</ref>, we illustrate this utility measure for two sets ğ¾ = {ğ‘, ğ‘, ğ‘, ğ‘‘ } and ğ¾ â€² = {ğ‘, ğ‘, ğ‘’, ğ‘“ }. In this case, ğ‘‘ğ‘œğ‘š(ğ¾) = {ğ‘, ğ‘, ğ‘‘ } and ğ‘‘ğ‘œğ‘š(ğ¾ â€² ) = {ğ‘, ğ‘“ }. Thus, ğ· ğ‘¢ (ğ¾ âˆ¥ğ¾ â€² ) = 2 and ğ· ğ‘¢ (ğ¾ â€² âˆ¥ğ¾) = 0. Definition 4.4 (Utility comparison): Given two pattern sets, ğ¾ and ğ¾ â€² , we use ğ¾ â‰» ğ¾ â€² to denote that ğ¾ is more useful than ğ¾ â€² , while ğ¾ â‰ˆ ğ¾ â€² denotes that ğ¾ is equally as useful as ğ¾ â€² . Here, the utility comparison is defined as follows: It should be noted that there are cases such that two pattern sets are not comparable. Specifically, two pattern sets ğ¾ and ğ¾ â€² are not comparable if ğ· ğ‘¢ (ğ¾ âˆ¥ğ¾ â€² ) â‰  âˆ… and ğ· ğ‘¢ (ğ¾ â€² âˆ¥ğ¾) â‰  âˆ…. This is the case that ğ¾ includes some more useful patterns that are not included in ğ¾ â€² while at the same time ğ¾ â€² also includes some more useful patterns that are not included in ğ¾. Since our goal is to discover as many useful patterns as possible, we consider both ğ¾ and ğ¾ â€² as profitable results, even when ğ· ğ‘¢ (ğ¾ âˆ¥ğ¾ â€² ) &gt; ğ· ğ‘¢ (ğ¾ â€² âˆ¥ğ¾).</p><formula xml:id="formula_10">ğ· ğ‘¢ (ğ¾ âˆ¥ğ¾ â€² ) = 0 âˆ§ ğ· ğ‘¢ (ğ¾ â€² âˆ¥ğ¾) = 0 â‡â‡’ ğ¾ â‰ˆ ğ¾ â€² ğ· ğ‘¢ (ğ¾ âˆ¥ğ¾ â€² ) &gt; 0 âˆ§ ğ· ğ‘¢ (ğ¾ â€² âˆ¥ğ¾) = 0 â‡â‡’ ğ¾ â‰» ğ¾ â€² ğ· ğ‘¢ (ğ¾ âˆ¥ğ¾ â€² ) &gt; 0 âˆ§ ğ· ğ‘¢ (ğ¾ â€² âˆ¥ğ¾) &gt; 0 â‡â‡’ not comparable.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Existing approaches and their limitations</head><p>The most simple approach to our problem is to use standard multipletesting procedures such as Bonferroni or Holm <ref type="bibr" target="#b5">[6]</ref> to obtain significant patterns, then filter out less useful patterns. The limitation of this approach is obvious: these procedures treat all hypotheses equally regardless of their utility. These methods are only developed to maximize the number of rejections |ğ‘…| under the constraint of FWER, i.e., to minimize type-II error while controlling type-I error. By contrast, our setting directly focuses on the dominating subset ğ‘‘ğ‘œğ‘š(ğ‘…), only. Consequently, from a utility perspective, many rejections of less useful patterns in ğ‘… is meaningless, resulting in a waste of the significance budget for controlling the type-I error of hypotheses which are not useful.</p><p>A better way forward may be a weighted approach, such as the weighted Bonferroni procedure <ref type="bibr" target="#b14">[14]</ref>. In this procedure, a weight is assigned to each hypothesis based on its importance. Hence, more important hypotheses have a higher chance of being rejected. However, if small weights are unfortunately assigned to false null hypotheses, many false null hypotheses will not be rejected, because the rejection thresholds are too strict. In the worst case, the weighted procedure might falsely accept all the hypotheses, i.e., miss all the true patterns. Considering that the weights must be assigned before conducting statistical tests to properly control FWER and no one knows which hypotheses are true null hypotheses, the weighted approach does not necessarily work well in practice.</p><p>5 Proposed method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The proposed method: SPUR</head><p>In our setting, after discovering a pattern set ğ‘…, discovering any pattern ğ‘  that is less useful than a pattern in ğ‘…, i.e., âˆƒğ‘  â€² âˆˆ ğ‘…, ğ‘  â€² â‰» ğ‘ , does not increase the result's utility. Thus, once a hypothesis â„ ğ‘  is rejected, we can ignore all less useful hypotheses â„ ğ‘  â€² â‰º â„ ğ‘  . Here, "ignoring a hypothesis" means "accepting the hypothesis". Because we have to guarantee the type-I error when rejecting a hypothesis, we can save the significance budget by rejecting more useful hypotheses and, at the same time, accepting less useful ones.</p><p>We propose an iterative multiple-hypotheses procedure that can conduct such an adaptive rejection process while controlling the FWER under Assumption 3.1. In particular, our method repeatedly (1) rejects the most significant hypothesis in the candidate hypothesis set and (2) ignores all hypotheses in the candidate set that are less useful than the rejected hypothesis in the last step. Next, we explain our algorithm along with the pseudocode provided in Algorithm 1. We also note that FWER control by our adaptive procedure is not obvious and we will discuss this in detail in Section 6.2.</p><p>Initialization and notation: First, we initialize the significance budget and the rejection set as ğ›¿ 1 = ğ›¼, ğ‘… 0 = âˆ…. Here, ğ›¿ ğ‘¡ is used to control the FWER of the procedure at iteration ğ‘¡. We define ğ» ğ‘¡ as the set of the remaining candidate hypotheses at iteration ğ‘¡ and let ğ» 1 = ğ» . Furthermore, we also define ğœ… ğ‘¡ (ğœ) = {â„ âˆˆ ğ» ğ‘¡ | ğœ“ (â„) â‰¤ ğœ } as the set of testable hypotheses at the ğ‘¡-th iteration for a rejection threshold ğœ.</p><p>Rejection procedure: At each iteration ğ‘¡, we decide whether to reject a hypothesis in the candidate set ğ» ğ‘¡ as follows. (1) We obtain the smallest p-value ğ‘ ğ‘Ÿ ğ‘¡ = min â„ âˆˆğ» ğ‘¡ ğ‘ â„ and the corresponding hypothesis â„ ğ‘Ÿ ğ‘¡ = argmin â„ âˆˆğ» ğ‘¡ ğ‘ â„ . We also assume that â„ ğ‘Ÿ ğ‘¡ can be decided by a predefined rule if there is more than one hypothesis realizing the smallest p-value. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">An intuitive explanation</head><p>In SPUR, the rejection threshold ğœ ğ‘¡ and significance budget ğ›¿ ğ‘¡ are managed in a way that is not so intuitive. We will prove that such updates are necessary to control the FWER properly in Section 6. Before providing the formal proof, we give a more intuitive account of our rejection rule by considering a simple case: the case without the Tarone's trick. In other words, we assume that âˆ€â„ âˆˆ ğ» : ğœ“ (â„) = 0. We have the following proposition.</p><p>Proposition 5.1 (SPUR in limited case): Assume for any â„ âˆˆ ğ» , ğœ“ (â„) = 0, then the rejection rule of SPUR at step 7 can be rewritten as</p><formula xml:id="formula_11">ğ‘ ğ‘Ÿ ğ‘¡ â‰¤ ğ›¼ âˆ’ ğ‘¡ âˆ’1 ğ‘–=1 ğ‘ ğ‘Ÿ ğ‘– (|ğ» ğ‘– | âˆ’ |ğ» ğ‘–+1 | âˆ’ 1) |ğ» ğ‘¡ | =â‡’ reject â„ ğ‘Ÿ ğ‘¡ ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’ =â‡’ stop.</formula><p>Proof. Appendix B. â–¡</p><p>In our simple case, after ignoring less useful hypotheses, we decrease the significance budget by an amount of ğ‘ ğ‘Ÿ ğ‘– (|ğ» ğ‘– | âˆ’ |ğ» ğ‘–+1 | âˆ’ 1). We note that |ğ» ğ‘– | âˆ’ |ğ» ğ‘–+1 | âˆ’ 1 is exactly the number of ignored hypotheses due to the rejection of â„ ğ‘Ÿ ğ‘– . This term came from the modification of ğ›¿ ğ‘¡ after each rejection, i.e., step 11 in Algorithm 1. Moreover, in Section 7, we demonstrate that this modification is necessary to properly control FWER by showing that the rejection threshold ğœ ğ‘¡ = ğ›¼ |ğ» ğ‘¡ | fails to control the FWER. We also remark that when no hypotheses can be ignored, i.e., âˆ€ğ‘–, ğ‘ ğ‘Ÿ ğ‘– (|ğ» ğ‘– |âˆ’|ğ» ğ‘–+1 âˆ’1|) = 0, our method reduces to the step-down Holm method <ref type="bibr" target="#b5">[6]</ref>. The full SPUR procedure is obtained by leveraging the Tarone's trick to additionally consider the testability of the candidate hypotheses set at each iteration.</p><p>6 Theoretical analysis</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">FWER guarantee</head><p>First, we show that the proposed method can control the FWER properly under Assumption 3.1 by Theorem 6.1. A more detailed proof is relegated to Appendix A. <ref type="foot" target="#foot_1">2</ref>Theorem 6.1 (FWER controlling): Under Assumption 3.1, the proposed SPUR procedure conducted with significance level ğ›¼ controls the familywise error rate at ğ›¼. Here, we discuss why Assumption 3.1 is needed. We first remark that we reject the hypothesis â„ ğ‘Ÿ ğ‘¡ at iteration ğ‘¡ if ğ‘ ğ‘Ÿ ğ‘¡ â‰¤ ğœ ğ‘¡ . In SPUR, because ğœ ğ‘¡ relies on the p-values of the rejected hypotheses in the previous steps, it is burdensome to directly evaluate the probability of rejecting a true hypothesis, due to the complicated dependence of p-values between hypotheses. This characteristic makes the evaluation of FWER challenging for our procedure. We overcame this challenge using the following observations. First, we remark that our testing framework is an iterative procedure. Thus, we can evaluate FWER by considering the probability of SPUR firstly rejecting a true hypothesis at an iteration. Actually, we show that FWER is equivalent to the sum of such a probability for all possible iterations. By viewing FWER this way, when evaluating the probability that SPUR firstly rejects a true hypothesis at iteration ğ‘‡ ğ¸ , we can regard that all rejected hypotheses in previous steps ğ‘¡ &lt; ğ‘‡ ğ¸ are false hypotheses. Moreover, any variables at any iteration ğ‘¡ &lt; ğ‘‡ ğ¸ , including ğ›¿ ğ‘¡ , ğœ ğ‘¡ , ğ» ğ‘¡ , â„ ğ‘Ÿ ğ‘¡ , ğ‘ ğ‘Ÿ ğ‘¡ , are determined only by the p-values of false hypotheses {ğ‘ â„ } â„ âˆˆğ¹ . Furthermore, ğ» ğ‘‡ ğ¸ , ğ›¿ ğ‘‡ ğ¸ , and ğœ ğ‘‡ ğ¸ are also determined only by {ğ‘ â„ } â„ âˆˆğ¹ since these variables are obtained using only the variables of iteration ğ‘‡ ğ¸ âˆ’ 1. Hence, the only variable that relies on both the p-values of true and false hypotheses is â„ ğ‘Ÿ ğ‘‡ ğ¸ . Then, using Assumption 3.1, we can apply the Tarone's trick to control the probability of rejecting a true hypothesis in ğ» ğ‘‡ ğ¸ with due regard to the threshold ğœ ğ‘‡ ğ¸ and show that FWER is upper-bounded by significant level ğ›¼.</p><p>Finally, it should be noted that our method can also be used as a general multiple testing framework for any problem that satisfies Assumption 3.1, such as multi-center studies or subset analysis in which the statistics between hypotheses are independent <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Utility guarantee</head><p>Next, we give a guarantee on the utility of the proposed method compared to the T-Bonferroni method in Theorem 6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 6.2 (Comparison with T-Bonferroni):</head><p>For arbitrary preference order â‰º, let ğ‘… be rejected by SPUR and ğ‘… Tarone be the rejection set by T-Bonferroni; then ğ‘… âª° ğ‘… Tarone .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. Appendix B. â–¡</head><p>To show this theorem, we first claim that in SPUR, we always obtain a rejection threshold ğœ ğ‘¡ that is not smaller than the rejection threshold of the last step ğœ ğ‘¡ âˆ’1 . Lemma 6.1 (Monotonically increasing ğœ ğ‘¡ ): Let ğœ 0 = 0, for any iteration ğ‘¡ â‰¤ ğ‘‡ SPUR of the SPUR process: ğœ ğ‘¡ âˆ’1 â‰¤ ğœ ğ‘¡ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. Appendix B. â–¡</head><p>Recall that the proposed method always rejects the most significant hypothesis â„ ğ‘Ÿ ğ‘¡ then ignores other less useful hypotheses in ğ» ğ‘¡ regarding â„ ğ‘Ÿ ğ‘¡ . Thus, the necessary condition for a hypothesis â„ * âˆˆ ğ» to be ignored is that there exists another hypothesis â„ âˆˆ ğ» that (1) is more significant than â„ * , i.e., ğ‘ â„ &lt; ğ‘ * â„ , and (2) is at least as useful as â„ * , i.e., â„ âª° â„ * . In our proof, we show that this condition does not hold for any hypothesis â„ * included in the dominating set ğ‘‘ğ‘œğ‘š(ğ‘… Tarone ) discovered by T-Bonferroni. Moreover, using Lemma 6.1 and the fact that the ğœ 1 = ğœ ğ‘‡ ğ‘ğ‘Ÿğ‘œğ‘›ğ‘’ , we can show that SPUR always rejects all the hypotheses in ğ‘‘ğ‘œğ‘š(ğ‘… Tarone ). Thus, ğ‘… âŠ‡ ğ‘‘ğ‘œğ‘š(ğ‘… Tarone ) and Theorem 6.2 can then be shown using the utility comparison between hypotheses sets. This guarantee of utility improvement is the most critical advantage of SPUR compared to the weighted approaches. As discussed in Section 4.3, the discovery result by weighted approaches can be heavily affected if the false hypotheses are assigned small weights. In the worst case, the weighted approach would be inferior to methods that do not consider utility, e.g., T-Bonferroni. In contrast, although SPUR is also expected to achieve better performance when the utility of the false hypotheses is high, it still guarantees a utility that is not less useful than T-Bonferroni even when the utility of false hypotheses is low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiment</head><p>In this section, we evaluate SPUR using a synthetic experiment and three real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Synthetic experiment</head><p>We conduct the first synthetic experiment with two goals. First, we verify if SPUR can adequately control FWER. Particularly, we demonstrate that adjusting the significance budget by ğ›¿ ğ‘¡ +1 = ğ›¿ ğ‘¡ âˆ’ ğœ ğ‘¡ (ğ‘ ğ‘Ÿ ğ‘¡ âˆ’ ğ‘ ğ‘Ÿ ğ‘¡ âˆ’1 ) + ğ‘ ğ‘Ÿ ğ‘¡ is necessary to control the FWER by showing that obtaining the significance budget as ğ›¿ ğ‘¡ +1 = ğ›¿ ğ‘¡ would violate the FWER. Second, we verify if SPUR can correctly reject hypotheses with higher utility compared to other methods. We confirm our discussion in Section 6.2 about the limitation of the weighted approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Experiment setting:</head><p>We consider a set of 100 hypotheses ğ» = {â„ 1 , â„ 2 , . . . , â„ 100 }, where 20 of them are false null hypotheses. Let ğ¹ be the set of false hypotheses and let the preference be â„ ğ‘– â‰» â„ ğ‘— for ğ‘– &lt; ğ‘—, i.e., the hypothesis with a smaller index be more useful. We consider the following ğ‘—=1 with 20 samples for each hypothesis â„ ğ‘– , where ğ‘¥ ğ‘– ğ‘— âˆ¼ N (ğœ‡ = 0, ğœ = 0.75) if â„ ğ‘– âˆˆ ğ‘‡ and ğ‘¥ ğ‘– ğ‘— âˆ¼ N (ğœ‡ = 0.5, ğœ = 0.75) if â„ ğ‘– âˆˆ ğ¹ . We remark that the p-values obtained in this setting satisfy Assumption 3.1 although this setting does not employ Fisher's exact test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Comparative methods and criterion</head><p>We compare our method with the following three methods.</p><p>â€¢ Bonferroni: the T-Bonferroni method with ğœ = ğ›¼/|ğ» | â€¢ w-Bonferroni: the weighted Bonferroni method where hypothesis â„ ğ‘– 's weight ğ‘¤ ğ‘– is assigned to be the number of hypotheses that are less useful than â„ ğ‘– , i.e., ğ‘¤ ğ‘– = 100 âˆ’ ğ‘– + 1. â€¢ invalid-SPUR: the version of SPUR where ğ›¿ ğ‘¡ +1 = ğ›¿ ğ‘¡ For each setting, we generated 10,000 datasets, applied each generated dataset to the four methods, and then evaluated the FWER and the utility of the rejected set ğ‘… (only for the runs with no type-I error). To evaluate utility, we first define ğ‘Ÿğ‘ğ‘›ğ‘˜ (â„) as the utility ranking of a false hypothesis â„ within the false hypothesis set ğ¹ . For example, in the medium utility setting, we have ğ‘Ÿğ‘ğ‘›ğ‘˜ (â„ 1 ) = 1, ğ‘Ÿğ‘ğ‘›ğ‘˜ (â„ 6 ) = 2. Here, the smaller the ğ‘Ÿğ‘ğ‘›ğ‘˜ (.), the more useful that rejected set is. The utility of the rejected set ğ‘… is the ranking of the most useful hypothesis in ğ‘….</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.3">Results and discussion</head><p>The FWER and the average number of rejections according to the four methods are given in Table <ref type="table" target="#tab_3">2</ref>. From the table, we observe that Bonferroni, w-Bonferroni, and SPUR succeed in controlling the FWER in all three settings. On the other hand, invalid-SPUR failed to control the FWER for the medium utility setting. This result demonstrates that management of the significance budget ğ›¿ ğ‘¡ at line 11 of SPUR is necessary for controlling the FWER. Figure <ref type="figure" target="#fig_8">4</ref> is a cumulative histogram of the utility rank ğ‘Ÿğ‘ğ‘›ğ‘˜ (ğ‘…) of the rejection set ğ‘… by the three methods: Bonferroni, w-Bonferroni, and SPUR after 10,000 runs. For each possible value ğ‘˜ âˆˆ {1, . . . , 20} of ğ‘Ÿğ‘ğ‘›ğ‘˜ (ğ‘…), we count how many times each method obtained a rejection set ğ‘… so that ğ‘Ÿğ‘ğ‘›ğ‘˜ (ğ‘…) â‰¤ ğ‘˜. For example, considering ğ‘˜ = 2 in the medium utility setting, we count how many times each method rejected hypothesis â„ 1 or â„ 6 correctly. In Figure <ref type="figure" target="#fig_8">4</ref>, the xand y-axes represent ğ‘˜ and the count of correct rejection at value ğ‘˜, respectively.</p><p>First, in all the settings, SPUR always achieved a rejection set that is not less useful than the Bonferroni rejection set, as is guaranteed in Theorem 6.2. Moreover, we observed that SPUR could achieve a rejection set with a higher utility if the false hypotheses have high utility. This is because when the utility of false hypotheses is high, once a false hypothesis gets rejected, SPUR can ignore many nonuseful hypotheses, which are both less useful and less significant than the rejected false hypothesis. As a result, SPUR can achieve a rejected set with higher utility.</p><p>On the other side, w-Bonferroni, which assigns larger weights for hypotheses with higher utility, works well in the high and medium utility settings. However, in the low utility setting, this method performs poorly and even worse than the Bonferroni. This can be explained in terms of the dependence between the assigned weight and the p-value in the weighted approach. In particular, if the weights of the false hypotheses are small, their rejection thresholds could become too strict, and these hypotheses could not be rejected. In contrast, SPUR guarantees that the utility of the rejected set is at least useful as the result of Bonferroni even in the low utility setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Real-world datasets</head><p>We conduct real-world data experiments to confirm if SPUR can discover more useful patterns in a real-world situation where the Tarone's trick must be considered. We adopted the three datasets Adult <ref type="bibr" target="#b6">[7]</ref>, Crash Report <ref type="bibr" target="#b16">[16]</ref>, and Crime <ref type="bibr" target="#b15">[15]</ref> for five mining tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Data preprocessing and task setup</head><p>All three datasets consist of some explanatory variables and a target class. To elaborate, the Crash Report dataset <ref type="bibr" target="#b16">[16]</ref> is a dataset of traffic accidents, including information about speed limits, weather, light-conditions, etc., and a class ğ‘¦ = 1 indicates if someone was injured. The Crime dataset <ref type="bibr" target="#b15">[15]</ref> consists of criminal records with information about the place, street, and time along with a class for the crime category: against a person, property, or society. For this dataset, we set up three tasks using each of these categories as the target class. Finally, the Adult dataset <ref type="bibr" target="#b6">[7]</ref> contains several demographic attributes and a class ğ‘¦ = 1 of income &gt; 50K for different individuals.</p><p>To set up the candidate pattern set and the null hypotheses in each dataset, we focused on some variables and defined the mining task based on these variables. We translate the values of these variables into items by categorizing them with some predefined rules. For example, the hours-per-week attribute in the Adult dataset is categorized into [&lt; 20], [20 âˆ’ 29], . . . , [â‰¥ 60]. A valid pattern is a combination of all explanatory variables, with one item for each variable. This setup guarantees that there is no overlap of samples between patterns, and we adopted the Fisher's exact test for all five tasks (one-sided for the Crash and Adult datasets, two-sided for the Crime dataset). Our goal is to discover the combinations of levels that are significantly associated with the target class. Details about the adopted variables are shown in Table <ref type="table">4</ref> of Appendix C.</p><p>To define the ordinal utility between patterns, we first divide the explanatory variables into two groups: utility variables and family variables. Specifically, we state ğ¾ â‰» ğ¾ â€² if their family variables are identical, and the utility variables of ğ¾ is more useful than of ğ¾ â€² . For example, for the Adult dataset, we let (sex, work-class, and occupation) be the family variables and hours-per-week, education be the utility variables. Here, based on prior knowledge that "a higher income correlates with higher education level and more work hours", we oppositely prefer patterns with lower education level and less work hours. That is, we aim to discover patterns that are unexpectedly associated with the class &gt; 50K. Furthermore, the family variables are also effective in finding useful patterns in various situations of (sex, work-class, and occupation). In addition, we prefer sounded safe for the Crash task and occurs in midday patterns for the Crime dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Results and discussion</head><p>In Table <ref type="table" target="#tab_4">3</ref>, we show the number of discoveries |ğ‘… S | by SPUR, |ğ‘… T | by T-Bonferroni, and the utility measure ğ· ğ‘¢ (ğ‘… S âˆ¥ğ‘… T ), for different significance levels ğ›¼. We do not show ğ· ğ‘¢ (ğ‘… T âˆ¥ğ‘… S ), because ğ· ğ‘¢ (ğ‘… T âˆ¥ğ‘… S ) = 0 for all settings, i.e., because SPUR always discovered a pattern set that is not less useful than T-Bonferroni, as is guaranteed by Theorem 6.2. Especially, ğ· ğ‘¢ is large for the Crash, Property, and Society tasks, indicating that SPUR discovered a large number of high-utility patterns that cannot be discovered by T-Bonferroni.</p><p>On the other hand, the utility measure ğ· ğ‘¢ (ğ‘… S âˆ¥ğ‘… T ) is comparatively small for the Adult and Person tasks. We explain this by considering the number of discovered patterns |ğ‘… ğ‘‡ | by T-Bonferroni. First, in these tasks, |ğ‘… ğ‘‡ | is small compared to other tasks. Moreover, even when we increase the significance level ğ›¼, only a few additional patterns are discovered. In such a case, since the number of discoverable patterns is small in the first place, even if SPUR can achieve a larger rejection threshold by ignoring less useful hypotheses, the number of newly discovered patterns would not increase. In other words, for cases such as the Crash, Property, and Society tasks, SPUR is especially functional because many useful true patterns were not discovered by T-Bonferroni due to the FWER constraint, while there are many patterns in ğ‘… ğ‘‡ which are not useful.</p><p>Next, we focus on the discovery process of two methods by using the Adult task as an example. In Figure <ref type="figure" target="#fig_0">1</ref>, we show the discoveries by two methods with ğ›¼ = 0.05 for two families (Male, Self-emp, Profspecialty) and (Male, Private, Exec-managerial). We also show the sorted indexes of p-values for discovered patterns by each method. As we can see from Figure <ref type="figure" target="#fig_0">1</ref>, by considering both the significance and the utility of discovered patterns, SPUR can efficiently expand its dominance of patterns with fewer members of rejection. By contrast, with no consideration of utility, T-Bonferroni wastes its significance budget in rejecting less useful hypotheses. As a result, SPUR can discover more useful patterns that T-Bonferroni fails to discover. This advantage is not limited to discovering patterns in the same family, but is also helpful in discovering patterns of other families. In fact, we obtained a number of pattern families that only SPUR could discover.</p><p>In Figure <ref type="figure" target="#fig_0">1</ref>, as discovered by SPUR, a male (private) executive/manager who works 40-50 hours a week is likely to have an income &gt; 50K even if he has just graduated from high school, while the required education level found by T-Bonferroni is to graduate from a college. Moreover, a male who graduated from a professional school and is working in a (self-employed) professional specialty is likely to have an income of &gt; 50K even if he only works for 30-40 hours a week, while the requirement discovered by T-Bonferroni is 40-50 hours a week. Other than those patterns, in the Crash dataset, SPUR discovered many patterns that seemed safe but are still associated with an accident and injury, which cannot be discovered by T-Bonferroni. In this way, SPUR can discover patterns that are significant, and at the same time, preferred by the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this study, we focused on the utility of patterns in the SSPM context. We introduced the problem of discovering statistically significant patterns with the highest utility, giving the ordinal utility between patterns. We proposed an iterative multiple-testing framework that alternately rejects a hypothesis and safely ignores hypotheses which are less useful than the rejected one. This enables the discovery of more useful patterns. We theoretically showed the FWER guarantee under a particular assumption and the utility guarantee of the proposed method. Finally, we conducted several experiments with both synthetic and real-world datasets to demonstrate that the proposed method is capable of discovering more useful patterns under the constraint of type-I error. To show this theorem, we first define the stochastic process obtained by the proposed method SPUR. </p><formula xml:id="formula_12">{ğ‘ â„ } â„ âˆˆğ‘‡ {ğ‘ â„ } â„ âˆˆğ¹ ğ‘‹1 â€¢ â€¢ â€¢ ğ‘‹ğ‘¡ â€¢ â€¢ â€¢ ğ‘‹ ğ‘‡ğ‘˜ ğ¸1 â€¢ â€¢ â€¢ ğ¸ğ‘¡ â€¢ â€¢ â€¢ ğ¸ ğ‘‡ğ‘˜</formula><formula xml:id="formula_13">{ğ‘… âˆ© ğ‘‡ â‰  âˆ…} = {ğ‘‡ ğ¸ â‰¤ |ğ» |}.</formula><p>Here, ğ¸ ğ‘¡ = {â„ ğ‘Ÿ ğ‘¡ âˆˆ ğ‘‡ , ğ‘ ğ‘Ÿ ğ‘¡ â‰¤ ğœ ğ‘¡ } is the event of rejecting a true hypothesis at the ğ‘¡-th iteration and ğ‘‡ ğ¸ = min{ğ‘¡ âˆˆ [|ğ» |] : ğ‘¡ â‰¤ ğ‘‡ SPUR âˆ§ ğ¸ ğ‘¡ } is the first iteration that a true hypotheses got rejected. Lemma A.1 claims that the FWER is actually the probability of the first Type-I error occurs at some step ğ‘¡ 0 â‰¤ |ğ» |. To show the FWER controlling, we have to consider the relationship between the p-values of the true hypotheses, the false hypotheses, and the rejection threshold at each iteration. However, such the dependence are complicated in the SPUR process {ğ‘‹ ğ‘¡ } ğ‘‡ SPUR ğ‘¡ =1 . Thus, we instead consider an alternative stochastic process that only depends on the false hypotheses and show the FWER controlling by analyzing this process. In addition, we illustrate the relation of entities in the SPUR process using a graph shown in Figure <ref type="figure" target="#fig_9">5</ref> where each ğ‘ ğ‘‡ * ğ‘¡ is obtained using the value ğ‘Œ ğ‘¡ of the false hypotheses based process.  </p><formula xml:id="formula_14">} ğ‘‡ FALSE ğ‘¡ =1 if {ğ‘ â„ } â„ âˆˆğ¹ ğ‘Œ1 â€¢ â€¢ â€¢ ğ‘Œğ‘¡ â€¢ â€¢ â€¢ ğ‘Œğ‘‡ ğ‘˜ ğ‘ ğ‘‡ * 1 ğ¸ * 1 â€¢ â€¢ â€¢ ğ‘ ğ‘‡ * 1 ğ¸ * ğ‘¡ â€¢ â€¢ â€¢ ğ‘ ğ‘‡ * ğ‘˜ ğ¸ * ğ‘‡ğ‘˜ {ğ‘ â„ } â„ âˆˆğ‘‡<label>Figure</label></formula><formula xml:id="formula_15">Pr ğ‘‡ * ğ¸ â‰¤ |ğ» | | {ğ‘ â„ } â„ âˆˆğ¹ â‰¤ ğ‘¡ 0 &lt;ğ‘˜ |ğœ… * ğ‘¡ 0 (ğ‘ ğ‘Ÿ * ğ‘¡ 0 ) âˆ© ğ‘‡ |(ğ‘ ğ‘Ÿ * ğ‘¡ 0 âˆ’ ğ‘ ğ‘Ÿ * ğ‘¡ 0 âˆ’1 ) + |ğœ… * ğ‘˜ (ğ‘ ğ‘Ÿ * ğ‘˜ ) âˆ© ğ‘‡ |(ğœ * ğ‘˜ âˆ’ ğ‘ ğ‘Ÿ * ğ‘˜âˆ’1 ).</formula><p>Actually, the proposed algorithm SPUR is designed to guarantee that the right side in the inequation of Lemma A.3 always less than ğ›¼, as stated in Lemma A.4.</p><p>Lemma A.4: Using the same definition of Lemma A.2 and let ğ‘˜ = ğ‘‡ False , the following holds: Proof. Since âˆ€ â„ âˆˆğ» ğœ“ (â„) = 0, we have that</p><formula xml:id="formula_16">ğ‘¡ 0 &lt;ğ‘˜ |ğœ… * ğ‘¡ 0 (ğ‘ ğ‘Ÿ * ğ‘¡ 0 ) âˆ© ğ‘‡ |(ğ‘ ğ‘Ÿ * ğ‘¡ 0 âˆ’ ğ‘ ğ‘Ÿ * ğ‘¡ 0 âˆ’1 ) + |ğœ… * ğ‘˜ (ğœ * ğ‘¡ ) âˆ© ğ‘‡ |(ğœ * ğ‘˜ âˆ’ ğ‘ ğ‘Ÿ * ğ‘˜âˆ’1 ) â‰¤ ğ›¼ .</formula><formula xml:id="formula_17">âˆ€ ğ‘¡ , âˆ€ ğœ â‰¥0 , ğœ… ğ‘¡ (ğœ) = |{â„ âˆˆ ğ» ğ‘¡ |ğœ“ (â„) â‰¤ ğœ }| = |ğ» ğ‘¡ |.</formula><p>We also have</p><formula xml:id="formula_18">ğœ ğ‘¡ = max{ğœ : (ğœ âˆ’ ğ‘ ğ‘Ÿ ğ‘¡ âˆ’1 )|ğ» ğ‘¡ | â‰¤ ğ›¿ ğ‘¡ } = ğ›¿ ğ‘¡ |ğ» ğ‘¡ | + ğ‘ ğ‘Ÿ ğ‘¡ âˆ’1 ğœ ğ‘¡ = ğ›¿ ğ‘¡ /(ğœ ğ‘¡ âˆ’ ğ‘ ğ‘Ÿ ğ‘¡ âˆ’1 ) = |ğ» ğ‘¡ | Thus, ğœ ğ‘¡ = ğ›¿ ğ‘¡ + ğ‘ ğ‘Ÿ ğ‘¡ âˆ’1 |ğ» ğ‘¡ | |ğ» ğ‘¡ | = ğ›¼ âˆ’ ğ‘¡ âˆ’1 ğ‘–=1 (ğ‘ ğ‘Ÿ ğ‘– âˆ’ ğ‘ ğ‘Ÿ ğ‘–âˆ’1 )|ğ» ğ‘– | âˆ’ ğ‘ ğ‘Ÿ ğ‘– + |ğ» ğ‘¡ |ğ‘ ğ‘Ÿ ğ‘¡ âˆ’1 |ğ» ğ‘¡ | = ğ›¼ âˆ’ ğ‘¡ âˆ’1 ğ‘–=1 ğ‘ ğ‘Ÿ ğ‘– (|ğ» ğ‘– | âˆ’ |ğ» ğ‘–+1 | âˆ’ 1) |ğ» ğ‘¡ | .</formula><p>In the last line, we rewrite the formula by grouping terms for each ğ‘ ğ‘Ÿ ğ‘¡ and used the fact that ğ‘ ğ‘Ÿ 0 = 0. This concludes our proof. â–¡ B.2 Proof of Proposition 3.1</p><p>Proof. Without the loss of generality, we consider two patterns ğ‘  and ğ‘  â€² where â„ ğ‘  âˆˆ ğ‘‡ and â„ â€² ğ‘  âˆˆ ğ¹ . We adopt the notions in Section 3 and assume that the observed dataset is ğ· = {ğ‘¡ ğ‘– , ğ‘¦ ğ‘– } ğ‘› ğ· ğ‘–=1 âˆ¼ D. We represent this dataset regarding ğ‘  and ğ‘  â€² as ğ· = {ğ‘¥ ğ‘–,ğ‘  , ğ‘¥ Proof. Let ğ‘… be the rejected set by SPUR. Moreover, let ğœ Tarone and ğ‘… Tarone be the rejection threshold and the rejected set by T-Bonferroni, respectively. We have that Since ğ‘… ğ‘ Tarone âˆª ğ‘… Tarone = ğ» , â„ âˆˆğ» â„ âª° â„ * âˆ§ ğ‘ â„ &lt; ğ‘ â„ * On the other hand, âˆƒ â„ âˆˆğ» â„ âª° â„ * âˆ§ ğ‘ â„ &lt; ğ‘ â„ * is the necessary condition for the hypothesis â„ * to be ignored in the procedure SPUR. Thus, no hypotheses in ğ‘‘ğ‘œğ‘š(ğ‘… Tarone ) will be ignored by SPUR. Moreover, we have ğœ 1 = ğœ Tarone since ğ» 1 = ğ» and ğ‘ 0 = 0 and ğœ ğ‘˜ â‰¥ ğœ 1 â‰¥ ğœ Tarone for any iteration ğ‘˜ by Lemma 6.1. Thus, ğ‘ * ğ‘˜ â‰¤ ğœ Tarone â‰¤ ğœ ğ‘˜ and it implies that ğ‘‘ğ‘œğ‘š(ğ‘… Tarone ) âŠ† ğ‘…. This concludes our proof. â–¡ C Appendix: Reproducibility Table <ref type="table">4</ref>: Adopted variables for each dataset and the number of categorizing levels for each variable (in parentheses).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>family-vars utility-vars target-class</head><p>Crash <ref type="bibr" target="#b16">[16]</ref> street-type (5) traffic-control (2) crash-type (2) time <ref type="bibr" target="#b24">(24)</ref> speed-limit (7) ğ‘¦ = 1 (injured) weather (3) light-condition (3) Adult <ref type="bibr" target="#b6">[7]</ref> sex (2) hours-per-week (6) income (2) work-class <ref type="bibr" target="#b2">(3)</ref> education (10) ğ‘¦ = 1 (&gt; 50K) occupation <ref type="bibr" target="#b15">(15)</ref> Crime <ref type="bibr" target="#b15">[15]</ref> place <ref type="bibr" target="#b12">(12)</ref> time <ref type="bibr" target="#b12">(12)</ref> crime-type (3) street ( <ref type="formula">14</ref>) ğ‘¦ = property/person/society</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Difference in discovered patterns and sorted indexes of p-values for two family A = (Male, Self-emp, Profspecialty) and B = (Male, Private, Exec-managerial) by the existing method T-Bonferroni and our method SPUR.</figDesc><graphic url="image-1.png" coords="1,358.88,202.55,156.16,158.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Assumption 3 .</head><label>3</label><figDesc>1 (p-values independence): For any hypotheses pair â„ ğ‘¡ âˆˆ ğ‘‡ and â„ ğ‘“ âˆˆ ğ¹ , their p-values ğ‘ â„ ğ‘¡ and ğ‘ â„ ğ‘“ are independent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Proposition 3 . 1 :</head><label>31</label><figDesc>In the setting of Section 3.1 and 3.2, suppose pvalues of Fisher's exact test are considered. Given ğ·, if for any two distinct patterns ğ‘ , ğ‘  â€² âˆˆ ğ‘†, {ğ‘¡ âˆˆ ğ· | ğ‘  âˆˆ ğ‘¡ } âˆ© {ğ‘¡ âˆˆ ğ· | ğ‘  â€² âˆˆ ğ‘¡ } = âˆ… holds, Assumption 3.1 holds. Proof. Appendix B. â–¡ Proposition 3.1 claims that Assumption 3.1 can be satisfied by two requirements. First, it requires that Fisher's exact test is used, which is a popular independent test in SSPM contexts. Second, it requires that the pattern set is designed to separate the dataset into non-overlapping subsets. This second requirement can be satisfied in several scenarios, for example â€¢ Categorical dataset: consider a dataset with several categorical attributes (ğ‘¥ 1 , ğ‘¥ 2 , . . . , ğ‘¥ ğ‘š ) and a target label ğ‘¦. Assumption 3.1 holds by any pattern set ğ‘† âŠ† ğ¼ 1 Ã— â€¢ â€¢ â€¢ Ã— ğ¼ ğ‘š , where ğ¼ ğ‘‘ is the set of possible values for variable ğ‘¥ ğ‘‘ . â€¢ Transaction dataset with fixed transaction size: consider a dataset that for any transaction ğ‘¡ ğ‘– âˆˆ ğ·, the number of items in ğ‘¡ ğ‘– is ğ‘˜ for a fixed ğ‘˜, i.e., ğ‘— âˆˆğ¼ ğ‘¥ ğ‘– ğ‘— = ğ‘˜. Assumption 3.1 holds by any pattern set ğ‘† âŠ† ğ¼ ğ‘˜ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Definition 4 . 1 (</head><label>41</label><figDesc>Preference order of utility): A preference order â‰º on a pattern set ğ‘† = {ğ‘  1 , . . . , ğ‘  |ğ‘† | } is a transitive binary relation in which (ğ‘†, â‰º) is a partially ordered set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Preference order and the dominating set</figDesc><graphic url="image-2.png" coords="4,376.90,271.20,120.12,70.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Definition 4 . 2 (</head><label>42</label><figDesc>(Utility) dominating subset): For a pattern set ğ¾ = {ğ‘  1 , . . . , ğ‘  ğ‘˜ }, we call ğ‘‘ğ‘œğ‘š(ğ¾) âŠ† ğ¾ the (utility) dominating subset of ğ¾ if ğ‘‘ğ‘œğ‘š</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Utility measure between two pattern sets ğ¾ = {ğ‘, ğ‘, ğ‘, ğ‘‘ } and ğ¾ â€² = {ğ‘, ğ‘, ğ‘‘, ğ‘’}</figDesc><graphic url="image-3.png" coords="5,53.80,500.45,240.25,72.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( 2 )</head><label>2</label><figDesc>We obtain ğœ ğ‘¡ = max{ğœ : (ğœ âˆ’ ğ‘ ğ‘Ÿ ğ‘¡ âˆ’1 )|ğœ… ğ‘¡ (ğœ)| â‰¤ ğ›¿ ğ‘¡ } as the rejection threshold to decide whether to reject â„ ğ‘Ÿ ğ‘¡ . (3) If ğ‘ ğ‘Ÿ ğ‘¡ â‰¤ ğœ ğ‘¡ , we reject â„ ğ‘Ÿ ğ‘¡ by adding it to the rejection set, i.e., ğ‘… ğ‘¡ +1 = ğ‘… ğ‘¡ âˆ© {â„ ğ‘Ÿ ğ‘¡ }. We also ignore (accept) all less useful hypotheses in ğ» ğ‘¡ by setting the next candidate set as ğ» ğ‘¡ +1 = {â„ âˆˆ ğ» ğ‘¡ | â„ â‰» â„ ğ‘Ÿ ğ‘¡ }. Finally, we modify the significance budget as ğ›¿ ğ‘¡ +1 = ğ›¿ ğ‘¡ âˆ’ ğœ ğ‘¡ (ğ‘ ğ‘Ÿ ğ‘¡ âˆ’ ğ‘ ğ‘Ÿ ğ‘¡ âˆ’1 ) + ğ‘ ğ‘Ÿ ğ‘¡ , where ğœ ğ‘¡ = ğ›¿ ğ‘¡ /(ğœ ğ‘¡ âˆ’ ğ‘ ğ‘Ÿ ğ‘¡ âˆ’1 ). (4) If ğ‘ ğ‘Ÿ ğ‘¡ &gt; ğœ ğ‘¡ , we stop and return the rejection set ğ‘… = ğ‘… ğ‘¡ âˆ’1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Cumulative histogram of utility rank.</figDesc><graphic url="image-4.png" coords="7,322.84,608.77,228.23,93.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Relation graph of {ğ‘‹ ğ‘¡ } ğ‘‡ SPUR ğ‘¡ =1 (shaded) and event ğ¸ ğ‘¡ (we let ğ‘˜ = ğ‘‡ False in the graph) A Appendix: Proof of Theorem 6.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Definition A. 3 (</head><label>3</label><figDesc>Alternative true hypotheses sequence): A set of r.v. {ğ‘ ğ‘‡ * ğ‘¡ } ğ‘‡ Falseğ‘¡ =1 is said to be an alternative true hypotheses sequence obtained from the false hypotheses based process {ğ‘Œ ğ‘¡</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Theorem 6 .</head><label>6</label><figDesc>1 is then shown by combining the above lemmas. B Appendix: Other proofs B.1 Proof of Proposition 5.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>ğœ</head><label></label><figDesc>Tarone = max{ğœ | ğœğœ… (ğœ) â‰¤ ğ›¼ } ğ‘… Tarone = {ğ‘ â„ â‰¤ ğœ Tarone } â„ âˆˆğ» .Letting â„ âˆˆ ğ‘… ğ‘ Tarone = ğ» \ ğ‘… Tarone , the following holds for a hypothesis â„ * âˆˆ ğ‘‘ğ‘œğ‘š(ğ‘… Tarone ):â„ âˆˆğ‘… Tarone , â„ âª° â„ * (since â„ * âˆˆ ğ‘‘ğ‘œğ‘š(ğ‘… Tarone ))and â„ âˆˆğ‘… ğ‘ Tarone , ğ‘ â„ &lt; ğ‘ â„ * (since â„ * âˆˆ ğ‘… Tarone ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>2 Ã— 2 contingency table for pattern S ğ‘‹ ğ‘–,ğ‘  = 1 ğ‘‹ ğ‘–,ğ‘  = 0 Total</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>ğ‘– indicates whether the corresponding items present in the transaction; i.e., ğ‘¥ ğ‘– ğ‘— = 1 if item ğ‘— appear in transaction ğ‘¡ ğ‘– . We call a set of items ğ‘  âŠ† ğ¼ a pattern and define the indication variable ğ‘‹ ğ‘–,ğ‘  = ğ‘— âˆˆğ‘  ğ‘¥ ğ‘– ğ‘— for pattern ğ‘ . Here, ğ‘‹ ğ‘–,ğ‘  = 1 if pattern ğ‘  appears in transaction ğ‘¡ ğ‘– and ğ‘‹ ğ‘–,ğ‘  = 0 otherwise.The association of pattern ğ‘  and target class ğ‘¦ = 1 can be investigated by conducting an independence test with the null hypothesis ğ» 0 : ğ‘‹ ğ‘  âŠ¥ğ‘¦ via a 2 Ã— 2 contingency table as in Table</figDesc><table /><note>2 , . . . , ğ‘¡ ğ‘› ğ· } that contains ğ‘› ğ· transactions defined in the universe of ğ‘š items ğ¼ = {1, . . . , ğ‘š}. Here, a transaction ğ‘¡ ğ‘– can be described by a vector x ğ‘– of length ğ‘š and a binary-class label ğ‘¦ ğ‘– . The vector x</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Algorithm 1 :</head><label>1</label><figDesc>SPUR Data: Hypothesis set ğ» , statistical level ğ›¼ Result: Reject set ğ‘… 1 ğ‘¡ â† 1, ğ‘… 0 â† âˆ…; 2 ğ» 1 â† ğ», ğ›¿ 1 â† ğ›¼, ğ‘ ğ‘Ÿ 0 â† 0; 3 while ğ» ğ‘¡ â‰  âˆ… do 4 ğœ ğ‘¡ â† max{ğœ : (ğœ âˆ’ ğ‘ ğ‘Ÿ ğ‘¡ âˆ’1 )|ğœ… ğ‘¡ (ğœ)| â‰¤ ğ›¿ ğ‘¡ } ; // threshold 5 ğ‘ ğ‘Ÿ ğ‘¡ â† min â„ âˆˆğ» ğ‘¡ ğ‘ â„ ; ğ» ğ‘¡ +1 â† {â„ âˆˆ ğ» ğ‘¡ | â„ â‰» â„ ğ‘Ÿ ğ‘¡ } ; // ignore unuseful 10 ğœ ğ‘¡ â† ğ›¿ ğ‘¡ /(ğœ ğ‘¡ âˆ’ ğ‘ ğ‘Ÿ ğ‘¡ âˆ’1 ); 11 ğ›¿ ğ‘¡ +1 â† ğ›¿ ğ‘¡ âˆ’ ğœ ğ‘¡ (ğ‘ ğ‘Ÿ ğ‘¡ âˆ’ ğ‘ ğ‘Ÿ ğ‘¡ âˆ’1 ) + ğ‘ ğ‘Ÿ ğ‘¡ ; // modify budget</figDesc><table><row><cell>6</cell><cell>â„ ğ‘Ÿ ğ‘¡ â† argmin â„ âˆˆğ» ğ‘¡ ğ‘ â„ ;</cell><cell></cell></row><row><cell>7</cell><cell>if ğ‘ ğ‘Ÿ ğ‘¡ â‰¤ ğœ ğ‘¡ then</cell><cell></cell></row><row><cell>8</cell><cell>ğ‘… ğ‘¡ +1 â† ğ‘… ğ‘¡ âˆ© {â„ ğ‘Ÿ ğ‘¡ } ;</cell><cell>// reject</cell></row><row><cell>9</cell><cell></cell><cell></cell></row><row><cell>12</cell><cell>else</cell><cell></cell></row><row><cell>13</cell><cell>return ğ‘… ğ‘¡ âˆ’1 ;</cell><cell></cell></row><row><cell>14</cell><cell>end</cell><cell></cell></row><row><cell>15</cell><cell>ğ‘¡ â† ğ‘¡ + 1</cell><cell></cell></row><row><cell cols="2">16 end</cell><cell></cell></row><row><cell cols="2">17 return ğ‘… ğ‘¡ âˆ’1 ;</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>FWER and the average number of rejections. ğ¹ has High utility : ğ¹ = {â„ 1 , â„ 2 , . . . , â„ 20 } (2) ğ¹ has Medium utility : ğ¹ = {â„ 1 , â„ 6 , . . . , â„ 96 } (3) ğ¹ has Low utility : ğ¹ = {â„ 81 , â„ 82 , . . . â„ 100 }. Since we do not consider the Tarone's trick in this experiment, we adopt the z-test and set up the null hypothesis ğ» 0 : ğœ‡ = 0 for all hypotheses. Moreover, we generate a dataset ğ· ğ‘– = {ğ‘¥ ğ‘– ğ‘— } 20</figDesc><table><row><cell></cell><cell></cell><cell>FWER</cell><cell cols="3">Average number of rejects</cell></row><row><cell></cell><cell cols="4">High Medium Low High Medium</cell><cell>Low</cell></row><row><cell>SPUR</cell><cell>0.006</cell><cell>0.042</cell><cell>0.048 3.157</cell><cell>2.265</cell><cell>1.934</cell></row><row><cell>Bonferroni</cell><cell>0.039</cell><cell>0.041</cell><cell>0.041 3.493</cell><cell>3.488</cell><cell>3.480</cell></row><row><cell cols="2">w-Bonferroni 0.032</cell><cell>0.038</cell><cell>0.049 4.504</cell><cell>3.199</cell><cell>1.803</cell></row><row><cell cols="2">invalid-SPUR 0.006</cell><cell>0.056</cell><cell>0.048 3.268</cell><cell>2.378</cell><cell>1.947</cell></row><row><cell cols="6">three settings where each is named by the usefulness of the false</cell></row><row><cell>hypotheses set.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(1)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Number of patterns discovered by T-Bonferroni |ğ‘… ğ‘‡ | and SPUR |ğ‘… ğ‘† | along with the utility measure ğ· ğ‘¢ = ğ· ğ‘¢ (ğ‘… ğ‘† âˆ¥ğ‘… ğ‘‡ ), for different ğ›¼ ğ›¼ = 0.01 ğ›¼ = 0.05 ğ›¼ = 0.1 |ğ‘… ğ‘‡ | |ğ‘… ğ‘† | ğ· ğ‘¢ |ğ‘… ğ‘‡ | |ğ‘… ğ‘† | ğ· ğ‘¢ |ğ‘… ğ‘‡ | |ğ‘… ğ‘† | ğ· ğ‘¢</figDesc><table><row><cell>Crash</cell><cell>83</cell><cell cols="3">75 12 120</cell><cell>94</cell><cell cols="4">6 136 106 13</cell></row><row><cell>Adult</cell><cell>45</cell><cell>26</cell><cell>0</cell><cell>57</cell><cell>32</cell><cell>2</cell><cell>63</cell><cell>36</cell><cell>3</cell></row><row><cell cols="6">Property 275 142 10 315 156</cell><cell cols="4">8 336 164 10</cell></row><row><cell>Person</cell><cell>133</cell><cell>73</cell><cell cols="2">2 156</cell><cell>83</cell><cell cols="2">2 163</cell><cell>87</cell><cell>1</cell></row><row><cell>Society</cell><cell cols="2">320 142</cell><cell cols="3">3 348 156</cell><cell cols="3">7 361 164</cell><cell>9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Definition A.1 (SPUR process): A SPUR (stochastic) process is a stochastic process {ğ‘‹ ğ‘¡ } ğ‘‡ SPUR ğ‘¡ =1 stopped at stopping time ğ‘‡ SPUR where ğ‘‹ ğ‘¡ = (ğ›¿ ğ‘¡ , ğ‘… ğ‘¡ , ğ‘ ğ‘Ÿ ğ‘¡ , â„ ğ‘Ÿ ğ‘¡ , ğœ ğ‘¡ , ğœ ğ‘¡ ) are obtained from the algorithm SPUR and ğ‘‡ SPUR = min{ğ‘¡ âˆˆ [|ğ» |] : ğ‘ ğ‘Ÿ ğ‘¡ &gt; ğœ ğ‘¡ }. We remark that since we can obtain ğ» ğ‘¡ = {â„ âˆˆ ğ» |âˆ€ â„ ğ‘Ÿ âˆˆğ‘… ğ‘¡ âˆ’1 , â„ â‰» â„ ğ‘Ÿ }, we do not need to include ğ» ğ‘¡ in ğ‘‹ ğ‘¡ . Using the stochastic process {ğ‘‹ ğ‘¡ } ğ‘‡ SPUR ğ‘¡ =1 , we next rewrite the event of rejecting at least one true hypotheses, i.e., the event of occuring a Type-I error. Lemma A.1: Consider a SPUR process {ğ‘‹ ğ‘¡ } ğ‘‡ SPUR ğ‘¡ =1 , let ğ¸ ğ‘¡ = {â„ ğ‘Ÿ ğ‘¡ âˆˆ ğ‘‡ , ğ‘ ğ‘Ÿ ğ‘¡ â‰¤ ğœ ğ‘¡ } and ğ‘‡ ğ¸ = min{ğ‘¡ âˆˆ [|ğ» |] : ğ‘¡ â‰¤ ğ‘‡ SPUR âˆ§ğ¸ ğ‘¡ }, the following holds:</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>.</head><label></label><figDesc>Definition A.2 (False hypotheses based process): A false hypotheses based process is a stochastic process {ğ‘Œ ğ‘¡ } ğ‘‡ FALSE</figDesc><table><row><cell cols="2">stopped at ğ‘¡ ) are obtained ğ‘¡ =1 ğ‘¡ , ğœ  *  ğ‘¡ , ğœ  *  ğ‘¡ , â„ ğ‘Ÿ  *  ğ‘¡ , ğ‘ ğ‘Ÿ  *  ğ‘¡ , ğ‘…  *  stopping time ğ‘‡ False where ğ‘Œ ğ‘¡ = (ğ›¿  *</cell></row><row><cell cols="2">from the algorithm SPUR with the following modification:</cell></row><row><cell>ğ‘ ğ‘Ÿ  *  ğ‘¡ = min â„ âˆˆğ»  *  ğ‘¡ âˆ©ğ¹</cell><cell>ğ‘ â„ and â„ ğ‘Ÿ  *  ğ‘¡ = argmin â„ âˆˆğ»  *  ğ‘¡ âˆ©ğ¹ ğ‘ â„ .</cell></row><row><cell cols="2">Next, we define a sequence of true hypotheses' p-values {ğ‘ ğ‘‡  *  ğ‘¡ } ğ‘‡ False ğ‘¡ =1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>6: Relation graph of {ğ‘Œ ğ‘¡ } ğ‘¡ (we let ğ‘˜ = ğ‘‡ False in the graph) for ğ‘¡ â‰¤ ğ‘‡ FALSE : ğ‘ ğ‘‡ * ğ‘¡ = ğ‘“ (ğ‘Œ ğ‘¡ , {ğ‘ â„ } â„ âˆˆğ‘‡ ) = min Since at each step, SPUR rejects the most significant hypothesis â„ ğ‘Ÿ ğ‘¡ = ğ‘šğ‘–ğ‘› â„ âˆˆğ» ğ‘¡ ğ‘ â„ , the event {â„ ğ‘Ÿ ğ‘¡ âˆˆ ğ‘‡ } (and {â„ ğ‘Ÿ ğ‘¡ âˆˆ ğ¹ }) depends on the comparison of ğ‘šğ‘–ğ‘› â„ âˆˆğ» ğ‘¡ âˆ©ğ¹ ğ‘ â„ and ğ‘šğ‘–ğ‘› â„ âˆˆğ» ğ‘¡ âˆ©ğ‘… ğ‘ â„ . We next consider this comparison via the false hypotheses based process and the alternative true hypotheses sequence, while claims its relation to Lemma A.1. ğ‘¡ â‰¤ ğ‘‡ False âˆ§ ğ¸ * ğ‘¡ } and ğ‘‡ ğ¸ as defined in Lemma A.1, ğ‘‡ ğ¸ â‰¥ ğ‘‡ * ğ¸ almost surely. We also give an illustration on the events ğ¸ * ğ‘¡ and their relationship with other entities in Figure 6. We have that Pr ğ‘… âˆ© ğ‘‡ â‰  âˆ… â‰¤ Pr ğ‘‡ ğ¸ â‰¤ |ğ» | â‰¤ Pr ğ‘‡ * ğ¸ â‰¤ |ğ» | from Lemma A.2. Moreover, since Pr ğ‘‡ * ğ¸ â‰¤ |ğ» | = E {ğ‘ â„ } â„âˆˆğ¹ Pr ğ‘‡ * ğ¸ â‰¤ |ğ» | | {ğ‘ â„ } â„ âˆˆğ¹ . we next find the upper bound of Pr ğ‘‡ * ğ¸ â‰¤ |ğ» | | {ğ‘ â„ } â„ âˆˆğ¹ . Lemma A.3: Using the same definition of Lemma A.2 and let ğ‘˜ = ğ‘‡ False , under Assumption 3.1, the following holds:</figDesc><table><row><cell>event ğ¸  *</cell><cell cols="2">ğ‘‡ False ğ‘¡ =1 (shaded), {ğ‘ ğ‘‡  *  ğ‘¡ } ğ‘‡ False ğ‘¡ =1 and</cell></row><row><cell></cell><cell>â„ âˆˆğ»  *  ğ‘¡ âˆ©ğ‘‡</cell><cell>ğ‘ â„ .</cell></row><row><cell cols="3">Lemma A.2: Consider {ğ‘Œ ğ‘¡ } ğ‘‡ False ğ‘¡ =1 and {ğ‘ ğ‘‡  *  ğ‘¡ } ğ‘‡ False ğ‘¡ =1 as defined in Def-inition A.2 and A.3. Let ğ¸  *  ğ‘¡ = {ğ‘ ğ‘‡  *  ğ‘¡ â‰¤ ğ‘ ğ‘Ÿ  *  ğ‘¡ , ğ‘ ğ‘‡  *  ğ‘¡ â‰¤ ğœ  *  ğ‘¡ } and ğ‘‡  *  ğ¸ =</cell></row><row><cell>min{ğ‘¡ âˆˆ [|ğ» |] :</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>ğ‘–,ğ‘  â€² , ğ‘¦ ğ‘– } where ğ‘¥ ğ‘–,ğ‘  = 1 if pattern ğ‘  is included in transaction ğ‘–.Since Fisher's exact test assumes that the margin distribution ğ‘› 1 , ğ‘› ğ· and ğ‘› ğ‘  , ğ‘› ğ‘  â€² are fixed, the only remaining r.v. in obtaining the p-value are ğ‘ ğ‘  and ğ‘ ğ‘  â€² , whereğ‘ ğ‘  = ğ‘– âˆˆ [ğ‘› ğ· ],ğ‘¥ ğ‘  ğ‘– =1 ğ‘¦ ğ‘– and ğ‘ ğ‘  â€² = ğ‘– âˆˆ [ğ‘› ğ· ],ğ‘¥ ğ‘  â€² ğ‘– =1 ğ‘¦ ğ‘– . Letting ğ¼ ğ‘  = {ğ‘– âˆˆ [ğ‘› ğ· ], ğ‘¥ ğ‘  ğ‘– = 1}, ğ¼ ğ‘  â€² = {ğ‘– âˆˆ [ğ‘› ğ· ], ğ‘¥ ğ‘  â€² ğ‘– = 1}, from the design of ğ‘  and ğ‘  â€² , we have ğ¼ ğ‘  âˆ© ğ¼ ğ‘  â€² = âˆ…. Hence, ğ‘ ğ‘  and ğ‘ ğ‘  â€² are obtained using mutually distinguish transactions. We next consider the independence of (ğ‘¥ ğ‘  ğ‘– , ğ‘¦ ğ‘  ğ‘– ) and(ğ‘¥ ğ‘  â€² ğ‘– â€² , ğ‘¦ ğ‘  â€² ğ‘– â€² ) for ğ‘– â‰  ğ‘– â€² : Pr ğ‘¥ ğ‘  â€² ğ‘– â€² , ğ‘¦ ğ‘  â€² ğ‘– â€² | ğ‘¥ ğ‘  ğ‘– , ğ‘¦ ğ‘  ğ‘– = Pr ğ‘¦ ğ‘  â€² ğ‘– â€² | ğ‘¥ ğ‘  â€² ğ‘– â€² , ğ‘¥ğ‘  ğ‘– , ğ‘¦ ğ‘  ğ‘– Pr ğ‘¥ ğ‘  â€² ğ‘– â€² | ğ‘¥ ğ‘  ğ‘– , ğ‘¦ ğ‘  ğ‘– = Pr ğ‘¦ ğ‘  â€² ğ‘– â€² , ğ‘¥ ğ‘  â€² On the other hand, since â„ ğ‘  is a true hypotheses, we have ğ‘¥ ğ‘  ğ‘– âŠ¥ğ‘¦ ğ‘  ğ‘– . Pr ğ‘¥ ğ‘  ğ‘– , ğ‘¦ ğ‘  ğ‘– = Pr ğ‘¥ ğ‘  ğ‘– Pr ğ‘¦ ğ‘  ğ‘– Hence, Pr ğ‘¥ ğ‘  â€² ğ‘– â€² , ğ‘¦ ğ‘  â€² ğ‘– â€² | ğ‘¥ ğ‘  ğ‘– , ğ‘¦ ğ‘  ğ‘– = Pr ğ‘¥ ğ‘  â€² ğ‘– â€² , ğ‘¦ ğ‘  â€² ğ‘– â€² Pr ğ‘¥ ğ‘  â€² ğ‘– â€² , ğ‘¥ ğ‘  ğ‘– Pr ğ‘¥ ğ‘  â€² ğ‘– â€² | ğ‘¥ ğ‘  ğ‘– Pr ğ‘¥ ğ‘  ğ‘– Pr ğ‘¥ ğ‘  ğ‘– = Pr ğ‘¥ ğ‘  â€² ğ‘– â€² , ğ‘¦ ğ‘  â€² ğ‘¥ ğ‘  â€² ğ‘– â€² , ğ‘¦ ğ‘  â€² ğ‘– â€² âŠ¥ğ‘¦ ğ‘  ğ‘– | ğ‘¥ ğ‘  ğ‘– . Moreover, Fisher's exact test assumes that |{ğ‘– : ğ‘¥ ğ‘  ğ‘– = 1}| = ğ‘› ğ‘  for any dataset ğ· = {ğ‘¡ ğ‘– , ğ‘¦ ğ‘– } ğ‘› ğ· ğ‘–=1 âˆ¼ D. We have {ğ‘¥ ğ‘  â€² ğ‘– â€² , ğ‘¦ ğ‘  â€² ğ‘– â€² } ğ‘– âˆˆğ¼ ğ‘  âŠ¥{ğ‘¥ ğ‘  ğ‘– , ğ‘¦ ğ‘  ğ‘– } ğ‘– âˆˆğ¼ ğ‘  | ğ‘› ğ‘  , ğ‘› ğ‘  â€² , ğ‘› ğ· =â‡’ ğ‘ ğ‘  âŠ¥ğ‘ ğ‘  â€² | ğ‘› ğ‘  , ğ‘› ğ‘  â€² , ğ‘› ğ· =â‡’ {ğ‘ â„ } â„ âˆˆğ‘‡ âŠ¥{ğ‘ â„ } â„ âˆˆğ¹ | ğ‘› ğ‘  , ğ‘› ğ‘  â€² , ğ‘› ğ· . Proof. First, we remind that ğœ ğ‘¡ and ğœ ğ‘¡ +1 are obtained as follows ğœ ğ‘¡ = max{ğœ : (ğœ âˆ’ ğ‘ ğ‘Ÿ ğ‘¡ âˆ’1 )|ğœ… ğ‘¡ (ğœ)| â‰¤ ğ›¿ ğ‘¡ } ğœ ğ‘¡ +1 = max{ğœ : (ğœ âˆ’ ğ‘ ğ‘Ÿ ğ‘¡ )|ğœ… ğ‘¡ +1 (ğœ)| â‰¤ ğ›¿ ğ‘¡ +1 }. Because ğ» ğ‘¡ +1 âŠ‚ ğ» ğ‘¡ and |ğœ… ğ‘¡ (ğœ ğ‘¡ )| â‰¤ ğ›¿ ğ‘¡ ğœ ğ‘¡ âˆ’ğ‘ ğ‘Ÿ ğ‘¡ âˆ’1 , we have |ğœ… ğ‘¡ +1 (ğœ ğ‘¡ )| &lt; |ğœ… ğ‘¡ (ğœ ğ‘¡ )| â‰¤ ğ›¿ ğ‘¡ ğœ ğ‘¡ âˆ’ ğ‘ ğ‘Ÿ ğ‘¡ âˆ’1 =â‡’ |ğœ… ğ‘¡ +1 (ğœ ğ‘¡ )|(ğœ ğ‘¡ âˆ’ ğ‘ ğ‘Ÿ ğ‘¡ ) &lt; ğ›¿ ğ‘¡ ğ›¿ ğ‘¡ +1 = ğ›¿ ğ‘¡ âˆ’ ğœ ğ‘¡ (ğ‘ ğ‘Ÿ ğ‘¡ âˆ’ ğ‘ ğ‘Ÿ ğ‘¡ âˆ’1 ) + ğ‘ ğ‘Ÿ ğ‘¡ = ğ›¿ ğ‘¡ ğœ ğ‘¡ âˆ’ ğ‘ ğ‘Ÿ Thus, |ğœ… ğ‘¡ +1 (ğœ ğ‘¡ )|(ğœ ğ‘¡ âˆ’ ğ‘ ğ‘Ÿ ğ‘¡ ) &lt; ğ›¿ ğ‘¡ +1 âˆ’ ğ‘ ğ‘Ÿ ğ‘¡ â‰¤ ğ›¿ ğ‘¡ +1 and ğœ ğ‘¡ +1 â‰¥ ğœ ğ‘¡ due to the maximal operation.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>B.3 Proof of Lemma6.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ğœ ğ‘¡ âˆ’ ğ‘ ğ‘Ÿ ğ‘¡</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ğ‘¡ âˆ’1 ğœ ğ‘¡ âˆ’ ğ‘ ğ‘Ÿ</cell><cell>.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>On the other hand,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ğ‘¡ ğ‘¡ âˆ’1 ğœ ğ‘¡ âˆ’ ğ‘ ğ‘Ÿ</cell><cell>+ ğ‘ ğ‘Ÿ ğ‘¡ .</cell></row><row><cell></cell><cell>ğ‘– â€²</cell><cell cols="2">Pr ğ‘¥ ğ‘  â€² ğ‘– â€² , ğ‘¥ ğ‘  ğ‘–</cell></row><row><cell></cell><cell>Pr ğ‘¥ ğ‘  â€² ğ‘– â€² | ğ‘¥ ğ‘  ğ‘– , ğ‘¦ ğ‘  ğ‘–</cell><cell cols="2">Pr ğ‘¥ ğ‘  ğ‘– | ğ‘¦ ğ‘  ğ‘–</cell></row><row><cell>=</cell><cell cols="2">Pr ğ‘¦ ğ‘  â€² ğ‘– â€² , ğ‘¥ ğ‘  â€² ğ‘– â€² ğ‘– â€² | ğ‘¥ ğ‘  ğ‘– Pr ğ‘¥ ğ‘  ğ‘– | ğ‘¦ ğ‘  ğ‘– Pr ğ‘¥ ğ‘  â€²</cell><cell>Pr ğ‘¥ ğ‘  â€² ğ‘– â€² , ğ‘¥ ğ‘  ğ‘– Pr ğ‘¥ ğ‘  ğ‘– | ğ‘¦ ğ‘  ğ‘–</cell><cell>.</cell></row><row><cell></cell><cell></cell><cell>ğ‘– â€²</cell><cell></cell></row><row><cell></cell><cell></cell><cell>.</cell><cell></cell></row><row><cell></cell><cell cols="2">Pr ğ‘¥ ğ‘ </cell><cell></cell></row><row><cell></cell><cell></cell><cell>ğ‘–</cell><cell></cell></row><row><cell cols="2">Thus, This concludes our proof.</cell><cell></cell><cell></cell><cell>â–¡</cell></row></table><note>â–¡ B.4 Proof of Theorem 6.2</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Stated differently, we assume that the test statistics obtained from the true patterns and the false patterns are mutually independent and the p-values are determined using these test statistics.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">The completed proof is available at https://github.com/dizzyvn/SPUR/ Research Track Paper KDD '20, August 23-27, 2020, Virtual Event, USA</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partly supported by KAKENHI (Grants-in-Aid for Scientific Research) Grant Numbers JP19H04164 and JP18H04099. We are also grateful to Professor Takeuchi for providing useful discussions and advices.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast algorithms for mining association rules</title>
		<author>
			<persName><forename type="first">Rakesh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakrishnan</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th int. conf. very large data bases, VLDB</title>
				<meeting>20th int. conf. very large data bases, VLDB</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">1215</biblScope>
			<biblScope unit="page" from="487" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the adaptive control of the false discovery rate in multiple testing with independent statistics</title>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Benjamini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yosef</forename><surname>Hochberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of educational and Behavioral Statistics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="60" to="83" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Aylmer</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName><surname>Fisher</surname></persName>
		</author>
		<title level="m">Statistical methods for research workers. Statistical methods for research workers</title>
				<imprint>
			<date type="published" when="1950">1950. 1950</date>
		</imprint>
	</monogr>
	<note>llth ed. revised</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Wensheng</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Chun-Wei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Fournier-Viger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han-Chieh</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">S</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10511</idno>
		<title level="m">A survey of utility-oriented pattern mining</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Kingfisher: an efficient algorithm for searching for both positive and negative dependency rules with statistical significance measures</title>
		<author>
			<persName><forename type="first">Wilhelmiina</forename><surname>HÃ¤mÃ¤lÃ¤inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and information systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="383" to="414" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A simple sequentially rejective multiple test procedure</title>
		<author>
			<persName><forename type="first">Sture</forename><surname>Holm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian journal of statistics</title>
		<imprint>
			<biblScope unit="page" from="65" to="70" />
			<date type="published" when="1979">1979. 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Ronny</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Becker</surname></persName>
		</author>
		<title level="m">Adult Data Set</title>
				<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Statistical emerging pattern mining with multiple testing correction</title>
		<author>
			<persName><forename type="first">Junpei</forename><surname>Komiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masakazu</forename><surname>Ishihata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroki</forename><surname>Arimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takashi</forename><surname>Nishibayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin-Ichi</forename><surname>Minato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="897" to="906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Utilitybased association rule mining: A marketing solution for cross-selling</title>
		<author>
			<persName><forename type="first">Dongwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung-Hyuk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songchun</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with applications</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2715" to="2725" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Felipe</forename><surname>Llinares-LÃ³pez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laetitia</forename><surname>Papaxanthos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><surname>Bodenham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damian</forename><surname>Roqueiro</surname></persName>
		</author>
		<title level="m">COPDGene Investigators, and Karsten Borgwardt</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Genome-wide genetic heterogeneity discovery with categorical covariates</title>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1820" to="1828" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast and memory-efficient significant pattern mining via permutation testing</title>
		<author>
			<persName><forename type="first">Felipe</forename><surname>Llinares-LÃ³pez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahito</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laetitia</forename><surname>Papaxanthos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining</title>
				<meeting>the 21th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="725" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A fast method of statistical assessment for combinatorial hypotheses based on frequent itemset enumeration</title>
		<author>
			<persName><forename type="first">Shin-Ichi</forename><surname>Minato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeaki</forename><surname>Uno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koji</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aika</forename><surname>Terada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Sese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="422" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Pellegrina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Riondato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Vandin</surname></persName>
		</author>
		<title level="m">SPuManTE: Significant Pattern Mining with Unconditional Testing</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ensemble-adjusted p values</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page">540</biblScope>
			<date type="published" when="1983">1983. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mcg Esb Service</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Crash Reporting -Drivers Data</title>
		<author>
			<persName><surname>Mcg Esb Service</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient algorithms for mining maximal high utility itemsets from data streams with different models</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Bai-En Shie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="12947" to="12960" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Finding statistically significant interactions between continuous features</title>
		<author>
			<persName><forename type="first">Mahito</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
				<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3490" to="3498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A modified Bonferroni method for discrete data</title>
		<author>
			<persName><surname>Robert E Tarone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="page" from="515" to="522" />
			<date type="published" when="1990">1990. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Statistical significance of combinatorial regulations</title>
		<author>
			<persName><forename type="first">Aika</forename><surname>Terada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariko</forename><surname>Okada-Hatakeyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koji</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Sese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="12996" to="13001" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Significant pattern mining with confounding variables</title>
		<author>
			<persName><forename type="first">Aika</forename><surname>Terada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koji</forename><surname>Tsuda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="277" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discovering significant patterns</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A multiple test correction for streams and cascades of statistical hypothesis tests</title>
		<author>
			<persName><forename type="first">I</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">FranÃ§ois</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><surname>Petitjean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1255" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mining itemset utilities from transaction databases</title>
		<author>
			<persName><forename type="first">Hong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><forename type="middle">J</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data &amp; Knowledge Engineering</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
