<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unified Dynamic Convolutional Network for Super-Resolution with Variational Degradations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-04-15">15 Apr 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yu-Syuan</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Roy</forename><surname>Tseng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Tseng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hsien-Kai</forename><surname>Kuo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yi-Min</forename><surname>Tsai</surname></persName>
						</author>
						<author>
							<persName><roleName>Hsinchu, Taiwan</roleName><forename type="first">Mediatek</forename><surname>Inc</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Syuan</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ulia</forename><surname>Tseng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yi-Min</forename><surname>Tsai}</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mediatek</forename><surname>Com</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Conv</forename><surname>Conv</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Relu</forename><surname>Conv</surname></persName>
						</author>
						<author>
							<persName><surname>Conv</surname></persName>
						</author>
						<title level="a" type="main">Unified Dynamic Convolutional Network for Super-Resolution with Variational Degradations</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-04-15">15 Apr 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2004.06965v1[eess.IV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep Convolutional Neural Networks (CNNs) have achieved remarkable results on Single Image Super-Resolution (SISR). Despite considering only a single degradation, recent studies also include multiple degrading effects to better reflect real-world cases. However, most of the works assume a fixed combination of degrading effects, or even train an individual network for different combinations. Instead, a more practical approach is to train a single network for wide-ranging and variational degradations. To fulfill this requirement, this paper proposes a unified network to accommodate the variations from interimage (cross-image variations) and intra-image (spatial variations). Different from the existing works, we incorporate dynamic convolution which is a far more flexible alternative to handle different variations. In SISR with nonblind setting, our Unified Dynamic Convolutional Network for Variational Degradations (UDVD) is evaluated on both synthetic and real images with an extensive set of variations. The qualitative results demonstrate the effectiveness of UDVD over various existing works. Extensive experiments show that our UDVD achieves favorable or comparable performance on both synthetic and real images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Single image super-resolution (SISR) has posed a great challenge of image quality in the recent advance of computer vision. The goal of SISR is to reconstruct a High-Resolution (HR) image from a Low-Resolution (LR) image. This inverse property makes it a highly ill-posed problem. Deep Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> have been widely adopted to solve SISR problem, and achieve significant success. Nevertheless, most of the methods assume a single fixed combination of degrading effects, e.g., blurring and bicubicly downsampling. Such assumption limits their capability to handle practical cases with multiple degradations.</p><p>Several CNN base methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> are proposed in the context of SISR with multiple degradations. These methods address this problem with very diverse settings and formulations. Shocher et al. <ref type="bibr" target="#b20">[21]</ref> train a small image-specific network to deal with different degradations ε=1.6, σ=0 ε=0.2,σ=0 ε=1.6, σ=10 ε=0.2, σ=10 Input Image LR RCAN <ref type="bibr" target="#b11">[12]</ref> ZSSR <ref type="bibr" target="#b20">[21]</ref> UDVD (Ours) HR Figure <ref type="figure">1</ref>. The SISR problem with variational degradations. Image castle (102061) in BSD100 <ref type="bibr" target="#b39">[40]</ref> with scale factor 2 is used. The HR image applies Gaussian blur with variant kernel width ε, bicubicly downsampling and white Gaussian noise with variant level σ to generate LR image.</p><p>for a certain image. In this approach, an individual network has to be trained whenever there is variation of degradations, e.g., different degrading effects across images. To address the problem of variational degradations, Zhang et al. <ref type="bibr" target="#b19">[20]</ref> propose SRMD and train a single network to handle multiple variations, including blur and noise. Note that the types of degrading effect are predefined, which is also known as non-blind setting. To the best of our knowledge, SFTMD <ref type="bibr" target="#b23">[24]</ref> is one of the most recent works for blind setting, which proposes Spatial Feature Transform (SFT) and Iterative Kernel Correction (IKC) to deal with a limited set of blind degradations. Following the most related work, SRMD <ref type="bibr" target="#b19">[20]</ref>, this paper adopts the same setting and formulation in which a single unified network is trained for variational degradations.</p><p>To handle variations of degrading effects, a unified network is expected to accommodate two types of variations, cross-image variations (inter-image) and spatial variations (intra-image). Fig. <ref type="figure">1</ref> illustrates an example of the problem of variational degradations in SISR. In Fig. <ref type="figure">1</ref>, different degrading effects are applied to different regions of a benchmarking image in BSD100 <ref type="bibr" target="#b39">[40]</ref>. Compare to the methods trained with a fixed degradation setting, RCAN <ref type="bibr" target="#b11">[12]</ref> and ZSSR <ref type="bibr" target="#b20">[21]</ref>, the proposed method achieves similar quality (red patches) while effectively adapt to other variations (green, purple and blue patches in Fig. <ref type="figure">1</ref>). On the other hand, in RCAN <ref type="bibr" target="#b11">[12]</ref> and ZSSR <ref type="bibr" target="#b20">[21]</ref>, unsatisfying quality can be observed due to its unawareness of variations. Further discussions and comparisons with a wide range of existing works will be addressed in Section 4.</p><p>Different from the most directly comparable approach, SRMD <ref type="bibr" target="#b19">[20]</ref>, this paper exploits dynamic convolutions to better solve the non-blind SISR problem with variational degradations. Dynamic convolution is a far more flexible operation than the standard one. A standard convolution learns kernels that minimize the error across all pixel locations at once. While, dynamic convolution uses perpixel kernels generated by the parameter-generating network <ref type="bibr" target="#b24">[25]</ref>. Moreover, the kernels of standard convolution are content-agnostic which are fixed after training. In contrast, the dynamic ones are content-adaptive which adapt to different input even after training. By the aforementioned properties, dynamic convolution demonstrates itself a better alternative to handle variational degradations. In this paper, we incorporate dynamic convolutions and propose a Unified Dynamic Convolutional Network for Variational Degradations (UDVD).</p><p>The contributions of this work are summarized as follows: <ref type="bibr" target="#b0">(1)</ref> We propose UDVD, a unified dynamic convolutional network for non-blind SISR with variational degradations. <ref type="bibr" target="#b1">(2)</ref> We further propose two types of dynamic convolutions to improve performance. And we integrate multistage loss to gradually refine images throughout the consecutive dynamic convolutions. (3) We perform comprehensive analysis of the performance impact of dynamic convolutions and investigate a number of configurations of dynamic convolutions. Extensive experiments show that the proposed UDVD achieves favorable or comparable performance on both synthetic and real images.</p><p>This paper is organized as follows. Section 2 discusses related works. Section 3 introduces the proposed UDVD and its implementation details. Section 4 presents experimental results. Conclusions are discussed in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Single Image Super-Resolution. In the past few years, various CNNs based techniques had been proposed for SISR. Among these works, SRCNN <ref type="bibr" target="#b0">[1]</ref> was the first to adopt a three-layer CNN architecture and achieved superior performance against the previous non-CNN works. Inspired by SRCNN, Kim et al. proposed a deeper network VDSR <ref type="bibr" target="#b1">[2]</ref> which contains 20 convolution layers and added global residual connection for residual learning <ref type="bibr" target="#b2">[3]</ref>. The authors in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> investigated the use of recursive blocks to increase the depth with parameter sharing. However, the bicubicly interpolated LR images used in these methods cause additional computation cost. To address this problem, FSRCNN <ref type="bibr" target="#b6">[7]</ref> and ESPCNN <ref type="bibr" target="#b7">[8]</ref> directly mapped LR images to HR images by adding transpose convolution layers and sub-pixel convolution layers at the end of the network. To cope with the ever decreasing of input resolution, EDSR <ref type="bibr" target="#b8">[9]</ref> and RDN <ref type="bibr" target="#b9">[10]</ref> leveraged an even larger model. DBPN <ref type="bibr" target="#b10">[11]</ref> further proposed iterative upsample and downsample unit for large scaling factors. Recently, RCAN <ref type="bibr" target="#b11">[12]</ref> integrated residual-in-residual structure and channel attention mechanism to weight channel-wise features.</p><p>Multiple Degradations. Among the studies of SISR, most of the works were trained on a single and fixed degradation e.g., bicubic downsampling, which strongly limits the applicability in practical scenarios. In the context of multiple and diverse degradations, Shocher et al. <ref type="bibr" target="#b20">[21]</ref> trained a small image-specific network to deal with different degradations for a certain image. The authors in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> used Generative Adversarial Networks (GANs) to tackle degradations in an unsupervised way. Zhang et al. <ref type="bibr" target="#b19">[20]</ref> proposed SRMD, a single network to handle multiple degradations, including blur and noise. Gu et al. <ref type="bibr" target="#b23">[24]</ref> proposed SFTMD, and applied Spatial Feature Transform (SFT) and Iterative Kernel Correction (IKC) for a subset of blind degradations. Different from the most directly comparable approach, SRMD <ref type="bibr" target="#b19">[20]</ref>, we exploit dynamic convolutions to better solve the SISR problem with variational degradations. When compared to SRMD <ref type="bibr" target="#b19">[20]</ref>, the proposed method achieves favorable performance on both synthetic and real images.</p><p>Dynamic Kernel Network. In recent researches, dynamic kernels in convolution layer had been widely-used in many applications. Brabandere et al. <ref type="bibr" target="#b24">[25]</ref> firstly exploited parameter-generating network to generate dynamic kernels for every pixel. Dynamic kernel network had made the trained network more flexible and gained successes in various applications, such as denoising <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>, video super-resolution <ref type="bibr" target="#b29">[30]</ref> and video interpolation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. This paper adopts a simple fully-convolution backbone to predict per-pixel kernels for dynamic convolutions. With the experiments in Section 4, this paper validates that the flexibility of dynamic convolutions can be used to tackle variational degradations of SISR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In this section, the problem of variational degradations of SISR is formulated. We then introduce the architecture and implementation details of the proposed UDVD. Next, the design of different types of dynamic convolutions and multistage loss are further discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>This paper focuses on SISR with the degrading effects, including blurring, noise and downsampling. These degrading effects can simultaneously happen to a practical use case <ref type="bibr" target="#b15">[16]</ref>. The degradation process is formulated as: where I HR and I LR indicates HR and LR image respectively, k represents a blur kernel, n stands for additive noise, ⊗ marks convolution operation, and ↓ s denotes downsampling operation with scale factor s. We consider Isotropic Gaussian blur kernel, which is one of the widely used kernels in recent studies <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b15">16]</ref>. For additive noise, most of the studies adopt Additive White Gaussian Noise (AWGN) with covariance (noise level) <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24]</ref>. Bicubic downsampler is considered for downsampling operation. By controlling the parameters of degrading effects, one can synthesize more realistic degradations for SISR training.</p><formula xml:id="formula_0">I LR = (I HR ⊗ k) ↓ s + n,<label>(1)</label></formula><p>Non-blind setting. In this paper, a non-blind setting is adopted. Assume given ground truth degradations, nonblind results provide the upper bounds for blind methods in which the degradations are estimated. Such bounding observation are supported as shown by Table <ref type="table" target="#tab_1">2</ref> of <ref type="bibr" target="#b20">[21]</ref>, Table 1 of <ref type="bibr" target="#b23">[24]</ref> and Table <ref type="table" target="#tab_0">1</ref> of <ref type="bibr" target="#b22">[23]</ref>. As mentioned above, improvements in non-blind setting elevate the performance upper bound for blind methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23]</ref>. Any degradation estimation methods can be prepended to extend our method on blind setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Unified Dynamic Convolutional Network for Variational Degradation (UDVD)</head><p>The framework of the UDVD is illustrated in Fig. <ref type="figure" target="#fig_0">2</ref>. The framework consists of a feature extraction network and a refinement network. The feature extraction network manages to extract high-level features of the input image, such as global context, local details and so on. The refinement network is then learn to enhance and upsample the image together with the extracted high-level features.</p><p>Training with Variational Degradations. Given a HR image, the degrading process is executed as follows, apply-ing isotropic Gaussian blur kernel of size p × p, bicubicly downsampling the image, and finally adding AWGN with noise level σ. The generated LR image is of size C×H×W , where C denotes the number of channels, H and W denote the height and width of the image. Similar to <ref type="bibr" target="#b19">[20]</ref>, we project the blur kernel to a t-dimensional vector by using Principal Component Analysis (PCA) technique. We then concatenate an extra dimension of noise level σ to get a (1 + t) vector. Such vector is then stretched to get a degradation map D of size (1 + t) × H × W . Last, we concatenate the LR image I 0 with the degradation map D of size (C + 1 + t) × H × W as input for UDVD. Note that t is set to 15 by default.</p><p>Feature Extraction Network. In UDVD, the input is first forwarded to the feature extraction network to extract highlevel features. And then, the high-level features and the input image are sent to the refinement network to generate HR image. The feature extraction network contains N residual blocks which are composed of convolutions and Rectified Linear Units (ReLU), as shown in Fig. <ref type="figure" target="#fig_0">2</ref>. In this work, the kernel size of convolution layers is set to 3 × 3, and the channels is set to 128.</p><p>Refinement Network. With the extracted feature maps, the refinement network further allocates M dynamic blocks for feature transformation. Note that a dynamic block can be optionally extended to perform upsampling with a specific rate r. The implementation details of dynamic convolutions will be covered in Section 3.3. In a dynamic block m, as illustrated in Fig. <ref type="figure" target="#fig_0">2</ref>, the input image I m−1 is sent to three 3 × 3 convolution layers with 16, 16 and 32 channels respectively, and then concatenated with the high-level feature maps F from the feature extraction network. The resultant feature maps are then forwarded to two paths.  The first path is a 3 × 3 convolution layer to predict perpixel kernels. The generated per-pixel kernels are stored in a tensor with k × k in channel dimension, where k is kernel size for per-pixel kernels (Fig. <ref type="figure" target="#fig_2">3(a)</ref>). When upsampling is of interested, the channel dimension is k × k × r × r, where r is upsample rate (Fig. <ref type="figure" target="#fig_2">3(b)</ref>). Note that k is set to 5 for the default setting. The predicted per-pixel kernels are then used to perform dynamic convolution operation on I m−1 to generate the output O m .</p><p>The second path contains two 3 × 3 convolution layer with 16 and 3 channels to generate the residual image R m for enhancing high frequency details as describe in <ref type="bibr" target="#b29">[30]</ref>. The residual image R m is then added to the output of dynamic convolution operation O m for output image I m . Note that sub-pixel convolution layer is used to align the resolutions between paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Types of Dynamic Convolutions</head><p>Fig. <ref type="figure" target="#fig_2">3</ref> illustrates two types of dynamic convolutions. In general, typical dynamic convolutions are used when input and output resolution are identical, shown in Fig. <ref type="figure" target="#fig_2">3(a)</ref>. Depending on use cases, upsampling can also be integrated into dynamic convolution as shown in Fig. <ref type="figure" target="#fig_2">3(b)</ref>.</p><p>Dynamic Convolution. In a typical dynamic convolution, convolutions are conducted by using per-pixel kernels K of kernel size k × k. Such operation can be expressed as:</p><formula xml:id="formula_1">I out (i, j) = ∆ u=−∆ ∆ v=−∆ K i,j (u, v) • I in (i − u, j − v), (2)</formula><p>where I in and I out represent input and output image respectively. i and j are the coordinates in image, u and v are the coordinates in each K i,j . Note that ∆ = k/2 . These perpixel kernels perform weighted sum across nearby pixels and enhance the image quality pixel by pixel. In default setting, there are H × W kernels and the corresponding weights are shared across channels. By introducing an additional dimension C with Eq. 2, dynamic convolution can be extended for independent weights across channels.</p><p>Integration with Upsampling. To integrate with upsampling, r 2 convolutions are conducted on the same corresponding patch to create r×r new pixels. The mathematical form of such operation is defined as:</p><formula xml:id="formula_2">I out (i × r + x, j × r + y) = r x=0 r y=0 ∆ u=−∆ ∆ v=−∆ K i,j,x,y (u, v) • I in (i − u, j − v),<label>(3)</label></formula><p>where x and y are in the coordination of each r × r output block (0 ≤ x, y ≤ r − 1). Here, the resolution of I out is r times the resolution of I in . We exploit r 2 HW kernels to generate rH × rW pixels as I out . When integrated with upsampling, the weights are shared across channels to avoid the curse of dimensionality <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Multistage Loss</head><p>Similar to previous work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b33">34]</ref>, we adopt a multistage loss at the outputs of dynamic blocks. The losses are calculated in between the HR image I HR and the intermediate images at each dynamic block. The loss is defined as:</p><formula xml:id="formula_3">Loss = M m=1 F (I m , I HR ) (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where M is the number of dynamic blocks and F is loss function such as L 2 loss and perceptual loss. To obtain a high quality resultant image, we then minimize the sum of the losses from each dynamic block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we discuss the experimental results and setups. Section 4. <ref type="bibr" target="#b0">1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Training Setups</head><p>We collect high-quality 2K images from DIV2K <ref type="bibr" target="#b34">[35]</ref> and Flickr2K <ref type="bibr" target="#b35">[36]</ref> for training. The degraded images are synthesized according to Eq. 1. As listed in Eq. 1, the process applies a sequence of degrading effects on high-quality images, saying blurring, bicubicly downsampling and then adding noise. Following previous works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24]</ref>, we use isotropic Gaussian blur kernels. The range of kernel width is set to [0. We conduct all the experiments on a server equipped with NVIDIA RTX 2080 Ti GPU.</p><p>We validate UDVD on datasets Set5 <ref type="bibr" target="#b37">[38]</ref>, Set14 <ref type="bibr" target="#b38">[39]</ref>, BSD100 <ref type="bibr" target="#b39">[40]</ref> and real images <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>. Same as in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20]</ref>, all models are trained on RGB space. The evaluations of PSNR and SSIM metrics are on the Y channel in YCbCr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison of UDVD Configurations</head><p>Table <ref type="table" target="#tab_0">1</ref> summarizes the quantitative comparisons of different UDVD configurations. The baseline configuration contains only the feature extraction network (15 residual blocks) and a sub-pixel layer. Whereas, in the refinement network, different configurations of dynamic convolutions are compared. In these configurations, D represents the block containing a typical dynamic convolution (as shown in Fig. <ref type="figure" target="#fig_2">3(a)</ref>), and U marks the integration of upsampling (listed in Fig. <ref type="figure" target="#fig_2">3(b)</ref>). As in Table <ref type="table" target="#tab_0">1</ref>, dynamic blocks improve the performance over the baseline configuration. Especially, the UD configuration achieves better performance over its counterparts. The ablation experiments in Table <ref type="table" target="#tab_0">1</ref> also demonstrate the effectiveness of the proposed multistage loss. In the following of this paper, we use UDVD UDD for scale factors 2 and 3. While, UDVD UUDD (two separated upsampling steps) is used for scale factor 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Visualizing Dynamic Kernels</head><p>UDVD generates dynamic kernels to adapt to both image contents and different degradations. Fig. <ref type="figure">4(a)</ref> demonstrates that different kernels are generated for different contents. Likewise, Fig. <ref type="figure">4(b)</ref> shows that the generated kernels further adapt to the applied degradations. As shown in Fig. <ref type="figure">4</ref>(c), the adaptation behavior is varying among different degradations regardless of contents. These observations confirm that UDVD is capable of handling spatial variations by generating dynamic kernels considering the spatial differences of content and degradation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experiments on Synthetic Images</head><p>We evaluate the proposed UDVD with a number of applicational settings, covering both variational and fixed degradations. Synthetic images are generated for these settings. Note that we compared UDVD with a comprehensive set of other methods in non-blind setting only. Details are elaborated in the following paragraphs.</p><p>Variations of Multiple Degradations. The proposed UDVD are evaluated with diverse variations of degrading effects, including bicubicly downsampling, isotropic Gaussian blur kernels and AWGN. We consider scale factors 2, 3, and 4 for bicubicly downsampling. For isotropic Gaussian blur, kernel widths 0.2, 1.3 and 2.6 are considered. Noise levels 15 and 50 are considered for AWGN. Table <ref type="table" target="#tab_1">2</ref> compares UDVD to previous works on widely-used dataset, Set5 <ref type="bibr" target="#b37">[38]</ref>, Set14 <ref type="bibr" target="#b38">[39]</ref> and BSD100 <ref type="bibr" target="#b39">[40]</ref>. Due to the unawareness of multiple degradations, RDN <ref type="bibr" target="#b9">[10]</ref> and RCAN <ref type="bibr" target="#b11">[12]</ref> produce unsatisfying PSNR when compared to other methods. For multiple degradations based methods, we compare UDVD to two well known methods, IR-CNN <ref type="bibr" target="#b16">[17]</ref>, and SRMD <ref type="bibr" target="#b19">[20]</ref>. As in  <ref type="table" target="#tab_1">2</ref>. Average PSNR values on variations of multiple degradations. We use the provided official code to compute the results, except IRCNN. For IRCNN, results are extracted from the publication <ref type="bibr" target="#b16">[17]</ref>. The best results are highlighted in red color. Spatial Variations of Degradations. In order to validate the advantages of dynamic convolution, we further extend the experiments to consider spatial variations of degradations. The degraded LR images are synthesized with the kernel width and noise level, which are gradually increasing in the corresponding range [0.2, 2] and <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">50]</ref> from left to right. We compare our UDVD to the most related work, SRMD <ref type="bibr" target="#b19">[20]</ref>, which handles the spatial variations without dynamic convolutions. As in Table <ref type="table" target="#tab_2">3</ref>, UDVD constantly outperforms SRMD for all the settings on all the dataset, Set5 <ref type="bibr" target="#b37">[38]</ref>, Set14 <ref type="bibr" target="#b38">[39]</ref> and BSD100 <ref type="bibr" target="#b39">[40]</ref>. In summary, UDVD delivers a noticeable PSNR improvement over SRMD. The qualitative comparison is also illustrated in Fig. <ref type="figure">5</ref>. In Fig.  <ref type="table">4</ref>. Average PSNR values on noise-free degradations. We use the official code to compute the results, except SFTMD and IRCNN. For SFTMD and IRCNN, results are extracted from the publications <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b16">17]</ref>. The best two results are highlighted in red and blue colors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Noise-Free Degradations and Variations. In this experiment, we train UDVD for noise-free degradations (remove noise degradations) to enable extensive comparisons to other works. Table <ref type="table">4</ref> summarizes the PSNR results for noise-free UDVD and the competitive methods.Compare to SRMDNF <ref type="bibr" target="#b19">[20]</ref> and SFTMD <ref type="bibr" target="#b23">[24]</ref>, UDVD achieves comparable results in most cases and outperforms both the methods when large scale factor on BSD100 <ref type="bibr" target="#b39">[40]</ref>. Note that SFTMD <ref type="bibr" target="#b23">[24]</ref> benefits mostly from the Spatial Feature Transform (SFT) which applies an affine transformation on the degradation information rather than concatenating it with input image. Without losing generality, UDVD can also be extended to adopt SFT layers for further improvements.</p><p>Fixed Degradations. To compare with state-of-the-art fixed degradation based methods, we evaluate UDVD on two widely used fixed degradations BI and DN. BI only includes bicubicly downsampling. While DN applies bicubicly downsample and then add AWGN with noise level 30. Table <ref type="table" target="#tab_4">5</ref> lists PSNR and SSIM results on BI and DN with scale factor 3. In Table <ref type="table" target="#tab_4">5</ref>, except SRMD <ref type="bibr" target="#b19">[20]</ref> and UDVD, all the rest methods train different and specific models for BI and DN respectively. While SRMD <ref type="bibr" target="#b19">[20]</ref> and UDVD exploits a single model to handle both BI and DN degradations. With only a single model, UDVD still produces competitive results on BI and superior results on DN. This verifies that a single UDVD can adapt to various degradations and achieve promising results.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Experiments on Real Images</head><p>Besides the experiments of synthetic test images in Section 4.4, we further extend the experiments to real images. There is no ground-truth degradations for real images. Hence, manual grid search on degradation parameters is performed as in <ref type="bibr" target="#b19">[20]</ref> to obtain visually satisfying results. Fig. <ref type="figure">6</ref> illustrates the qualitative comparisons on the widely used real image chip <ref type="bibr" target="#b40">[41]</ref> and frog <ref type="bibr" target="#b41">[42]</ref> for UDVD, RCAN <ref type="bibr" target="#b11">[12]</ref>, ZSSR <ref type="bibr" target="#b20">[21]</ref> and SRMD <ref type="bibr" target="#b19">[20]</ref>. Most of the com-pared methods produce noticeable artifacts. RCAN produces blur edges and cannot deal with noise. ZSSR tends to produce over-smoothed results. Although SRMD successfully removes these artifacts, but it fails to recover sharp edges. In contrast, the proposed UDVD reconstructs a sharper and clearer quality. Fig. <ref type="figure">7</ref> illustrates images reconstructed by UDVD with different degradation estimations. One can observe that different patches (blue and red) have favorable reconstructed results on different degradation estimations. This confirms the existence of variational degradations in real images. In summary, Fig. <ref type="figure">6</ref> and Fig. <ref type="figure">7</ref> confirm the effectiveness of UDVD for real images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In summary, this paper presents a Unified Dynamic Convolutional Network for Variational Degradations (UDVD). We further introduce two types of dynamic convolutions to improve performance. Multistage loss is also applied to gradually refine images throughout the consecutive dynamic convolutions. With only a single network, the proposed UDVD efficiently handles a wide range of degradation variations for real-world images, including cross-image and spatial variations. UDVD is evaluated on both synthetic and real images with diverse variations of degrading effects. Comprehensive experiments are conducted to compare UDVD with various existing works. Through qualitative results, we confirm the effectiveness of dynamic convolutions over various existing works. Extensive experiments show that the proposed UDVD achieves favorable or comparable performance on both synthetic and real images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The network architecture of the proposed UDVD framework.</figDesc><graphic url="image-22.png" coords="3,337.19,167.50,58.07,58.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Dynamic Convolution with Upsampling</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Types of dynamic convolutions. (a) A typical dynamic convolution refines the image quality while keeps resolution. (b) A dynamic convolution further upsamples the image with a specific upsample rate r.</figDesc><graphic url="image-31.png" coords="4,155.20,286.86,75.49,75.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>2, 3.0], and the kernel size is fixed to 15 × 15. For noise, we use AWGN with noise level in range [0, 75]. Uniform sampling is used to generate all the parameters. During training, degraded LR images are cropped into patches of size 48×48. While the corresponding HR images are cropped into patches of 96×96, 144×144 and 192×192 for scale factors 2, 3, and 4, respectively. The patches are augmented by randomly horizontal flipping, vertical flipping, and 90 o rotating. We set the mini-batch size to 32. Adam optimizer [?] is used together with the multistage L2 loss introduced in Section 3.4. The learning rate is initialized to 10 −4 and decreased by half for every 2 × 10 5 steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>ε=1. 6 ,Figure 4 .</head><label>64</label><figDesc>Figure 4. The predicted per-pixel kernels in second dynamic block of UDVD. (a) The kernels learned from lr image. (b) The kernels learned from image with spatially variant degradation of Gaussian blur kernel width ε and noise level σ. (c) The absolute difference between (a) and (b).</figDesc><graphic url="image-33.png" coords="5,308.92,79.66,89.10,98.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>* indicates a unified model for BI and DN. We use the provided official code to compute the results, except SRCNN and VDSR. For SRCNN and VDSR, results are extracted from the publications<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10]</ref>. The best two results are highlighted in red and blue colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Figure 6. The qualitative results of real image chip with scale factor 4 (first row) and frog with scale factor 3 (second row).</figDesc><graphic url="image-97.png" coords="8,99.00,349.07,98.28,100.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>elaborates the details of dataset and training setups. Different configurations of the proposed UDVD are compared in Section 4.2. Section 4.3 illustrates the learned dynamic kernels. Section 4.4 evaluates UDVD with different applicational settings using synthetic images. Real image evaluations are in Section 4.5. Average PSNR and SSIM values for various UDVD configurations on Set5. Degradation parameters include scaling factor ×3, kernel width 1.3 and noise level 15 . The best results are highlighted in red color.</figDesc><table><row><cell>Methods</cell><cell>PSNR</cell><cell>SSIM</cell></row><row><cell>Baseline</cell><cell cols="2">29.45 0.8350</cell></row><row><cell>UDVD U</cell><cell cols="2">29.56 0.8384</cell></row><row><cell>UDVD DU</cell><cell cols="2">29.58 0.8393</cell></row><row><cell>UDVD UD</cell><cell cols="2">29.61 0.8400</cell></row><row><cell cols="3">UDVD UDD w/o multistage loss 29.58 0.8385</cell></row><row><cell>UDVD UDD</cell><cell cols="2">29.67 0.8410</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 ,</head><label>2</label><figDesc>UDVD constantly achieves better PSNR, even at the hardest test case (kernel width 2.6 and noise level 50).</figDesc><table><row><cell>Methods</cell><cell cols="2">Kernel width Noise level</cell><cell>×2</cell><cell>Set5 ×3</cell><cell>×4</cell><cell>×2</cell><cell>Set14 ×3</cell><cell>×4</cell><cell>×2</cell><cell>BSD100 ×3</cell><cell>×4</cell></row><row><cell>RDN [10]</cell><cell></cell><cell></cell><cell cols="9">26.23 25.57 24.48 25.44 24.40 23.45 25.03 24.04 23.13</cell></row><row><cell>RCAN [12]</cell><cell></cell><cell></cell><cell cols="3">26.05 25.46 24.83</cell><cell>25.3</cell><cell cols="5">24.29 23.64 24.95 23.92 23.33</cell></row><row><cell>IRCNN [17]</cell><cell>0.2</cell><cell>15</cell><cell cols="3">32.60 30.08 28.35</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SRMD [20]</cell><cell></cell><cell></cell><cell cols="9">32.76 30.43 28.79 30.14 27.82 26.48 29.23 27.11 25.95</cell></row><row><cell>UDVD</cell><cell></cell><cell></cell><cell cols="9">32.96 30.68 29.04 30.43 28.14 26.82 29.38 27.27 26.08</cell></row><row><cell>RDN [10]</cell><cell></cell><cell></cell><cell cols="9">25.01 24.98 24.33 24.08 23.92 23.39 23.85 23.67 23.09</cell></row><row><cell>RCAN [12]</cell><cell></cell><cell></cell><cell>24.9</cell><cell cols="8">24.94 24.58 24.04 23.88 23.53 23.84 23.62 23.26</cell></row><row><cell>IRCNN [17]</cell><cell>1.3</cell><cell>15</cell><cell cols="3">29.96 28.68 27.71</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SRMD [20]</cell><cell></cell><cell></cell><cell cols="9">30.98 29.43 28.21 28.34 27.05 26.06 27.52 26.45 25.63</cell></row><row><cell>UDVD</cell><cell></cell><cell></cell><cell cols="9">31.16 29.67 28.43 28.63 27.36 26.37 27.64 26.58 25.74</cell></row><row><cell>RDN [10]</cell><cell></cell><cell></cell><cell cols="9">23.18 23.28 23.07 22.34 22.40 22.31 22.44 22.52 22.35</cell></row><row><cell>RCAN [12]</cell><cell></cell><cell></cell><cell cols="7">23.13 23.29 23.24 22.34 22.41 22.42 22.47</cell><cell>22.5</cell><cell>22.48</cell></row><row><cell>IRCNN [17]</cell><cell>2.6</cell><cell>15</cell><cell cols="3">26.44 25.67 24.36</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SRMD [20]</cell><cell></cell><cell></cell><cell cols="9">28.48 27.55 26.82 26.18 25.58 25.06 25.81 25.29 24.86</cell></row><row><cell>UDVD</cell><cell></cell><cell></cell><cell cols="9">28.73 27.80 26.98 26.48 25.87 25.33 25.93 25.41 24.96</cell></row><row><cell>RDN [10]</cell><cell></cell><cell></cell><cell cols="9">17.23 16.85 16.51 17.04 16.58 16.21 16.90 16.38 15.99</cell></row><row><cell>RCAN [12]</cell><cell></cell><cell></cell><cell cols="8">17.08 16.13 16.64 16.84 15.68 16.35 16.66 15.54</cell><cell>16.1</cell></row><row><cell>IRCNN [17]</cell><cell>0.2</cell><cell>50</cell><cell cols="3">28.20 26.25 24.95</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SRMD [20]</cell><cell></cell><cell></cell><cell cols="9">28.51 26.48 25.18 26.70 25.01 23.95 26.13 24.74 23.86</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="9">28.63 26.65 25.34 27.00 25.32 24.24 26.27 24.87 23.98</cell></row><row><cell>RDN [10]</cell><cell></cell><cell></cell><cell cols="9">16.97 16.70 16.41 16.75 16.45 16.14 16.64 16.29 15.95</cell></row><row><cell>RCAN [12]</cell><cell></cell><cell></cell><cell cols="9">16.82 15.98 16.54 16.55 15.56 16.28 16.42 15.47 16.06</cell></row><row><cell>IRCNN [17]</cell><cell>1.3</cell><cell>50</cell><cell cols="3">26.69 25.20 24.42</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SRMD [20]</cell><cell></cell><cell></cell><cell cols="9">27.43 25.82 24.77 25.63 24.47 23.64 25.26 24.33 23.63</cell></row><row><cell>UDVD</cell><cell></cell><cell></cell><cell cols="9">27.54 25.99 24.92 25.88 24.75 23.91 25.36 24.45 23.74</cell></row><row><cell>RDN [10]</cell><cell></cell><cell></cell><cell cols="9">16.50 16.31 16.08 16.30 16.09 15.88 16.29 16.03 15.77</cell></row><row><cell>RCAN [12]</cell><cell></cell><cell></cell><cell>16.36</cell><cell>15.6</cell><cell cols="7">16.22 16.12 15.24 16.02 16.07 15.23 15.88</cell></row><row><cell>IRCNN [17]</cell><cell>2.6</cell><cell>50</cell><cell cols="3">22.98 22.16 21.43</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SRMD [20]</cell><cell></cell><cell></cell><cell cols="9">25.85 24.75 23.98 24.32 23.53 22.98 24.30 23.68 23.18</cell></row><row><cell>UDVD</cell><cell></cell><cell></cell><cell cols="9">26.00 24.85 24.11 24.60 23.81 23.23 24.41 23.79 23.27</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Average PSNR values on spatial variations of degradations. We use the official code of SRMD to compute its results. The best results are highlighted in red color.</figDesc><table><row><cell></cell><cell cols="2">Kernel width Noise level</cell><cell>×2</cell><cell>Set5 ×3</cell><cell>×4</cell><cell>×2</cell><cell>Set14 ×3</cell><cell>×4</cell><cell>×2</cell><cell>BSD100 ×3</cell><cell>×4</cell></row><row><cell>SRMD [20] UDVD</cell><cell>[0.2, 2]</cell><cell>5</cell><cell cols="8">33.56 31.67 30.01 29.79 28.06 26.77 27.54 26.00 24.98 33.77 32.00 30.63 30.74 28.93 27.76 29.61 27.99 26.90</cell></row><row><cell>SRMD [20] UDVD</cell><cell>0.2</cell><cell>[5, 50]</cell><cell cols="8">29.70 27.80 26.53 28.02 26.20 24.98 27.39 25.75 24.81 30.78 28.61 27.15 28.90 26.85 25.62 27.98 26.20 25.19</cell></row><row><cell>SRMD [20] UDVD</cell><cell>[0.2, 2]</cell><cell>[5, 50]</cell><cell cols="8">28.53 27.06 26.05 26.97 25.62 24.66 26.46 25.33 24.59 29.37 27.74 26.57 27.63 26.28 25.23 26.89 25.71 24.95</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Figure5. The visual results of image zebra (first row) and image bamboo (second row) with scale factor 3 on spatially variant blur and noise degradations. .<ref type="bibr" target="#b12">13</ref> 31.96 33.33 30.04 28.35 32.05 28.97 27.49 SFTMD w/o SFT [24] 31.74 30.90 29.40 27.57 26.40 26.18 27.24 26.43 26.34 SFTMD [24] 38.00 34.57 32.39 33.68 30.47 28.77 32.09 29.09 27.58 UDVD 38.01 34.49 32.31 33.64 30.44 28.78 32.19 29.18 27.70 ZSSR<ref type="bibr" target="#b20">[21]</ref> </figDesc><table><row><cell>PSNR/SSIM</cell><cell>24.50/0.7148</cell><cell></cell><cell cols="2">26.03/0.7546</cell><cell></cell><cell cols="2">28.06/0.8005</cell><cell></cell><cell cols="2">28.45/0.8052</cell></row><row><cell>PSNR/SSIM</cell><cell>20.56/0.3831</cell><cell></cell><cell cols="2">17.68/0.3068</cell><cell></cell><cell cols="2">22.33/0.4608</cell><cell></cell><cell cols="2">22.52/0.4856</cell></row><row><cell>(a) Ground Truth</cell><cell>(b) Bicubic</cell><cell></cell><cell cols="2">(c) RCAN [12]</cell><cell></cell><cell cols="2">(d) SRMD [20]</cell><cell></cell><cell cols="2">(e) UDVD(Ours)</cell></row><row><cell>Methods</cell><cell>Kernel width</cell><cell>×2</cell><cell>Set5 ×3</cell><cell>×4</cell><cell>×2</cell><cell>Set14 ×3</cell><cell>×4</cell><cell>×2</cell><cell>BSD100 ×3</cell><cell>×4</cell></row><row><cell>ZSSR [21]</cell><cell></cell><cell cols="9">34.94 29.29 28.87 31.04 28.05 27.15 31.42 28.24 26.68</cell></row><row><cell>IRCNN [17]</cell><cell></cell><cell cols="3">37.43 33.39 31.02</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SRMDNF [20] IRCNN [17] SRMDNF [20] SFTMD w/o SFT [24]</cell><cell cols="10">0.2 37.79 341.3 33.37 28.67 27.44 31.31 27.34 26.15 30.31 27.30 25.95 36.01 33.33 31.01 ------37.44 34.17 32.00 33.20 30.08 28.42 31.98 29.03 27.53 30.88 30.33 29.11 27.16 25.84 25.93 26.84 25.92 26.20</cell></row><row><cell>SFTMD [24]</cell><cell></cell><cell cols="9">37.46 34.53 32.41 33.39 30.55 28.82 32.06 29.15 27.64</cell></row><row><cell>UDVD</cell><cell></cell><cell cols="9">37.36 34.52 32.37 33.39 30.50 28.85 32.00 29.23 27.75</cell></row><row><cell>ZSSR [21]</cell><cell></cell><cell cols="9">29.89 27.80 27.69 27.72 26.42 26.06 27.32 26.47 25.92</cell></row><row><cell>IRCNN [17]</cell><cell></cell><cell cols="3">32.07 31.09 30.06</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SRMDNF [20] SFTMD w/o SFT [24]</cell><cell>2.6</cell><cell cols="9">34.12 33.02 31.77 30.25 29.33 28.26 29.23 28.35 27.43 24.22 28.44 28.64 22.99 24.19 25.63 23.07 24.42 25.99</cell></row><row><cell>SFTMD [24]</cell><cell></cell><cell cols="9">34.27 33.22 32.05 30.38 29.63 28.55 29.35 28.41 27.47</cell></row><row><cell>UDVD</cell><cell></cell><cell cols="9">33.74 33.15 31.99 30.08 29.58 28.55 28.93 28.49 27.55</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">5, without considering spatial varia-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">tions, RCAN can only handle a fixed degradation (red block</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">represents lighter degradation), but fails at the other one</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">(blue block represents heavier degradation). Although both</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">SRMD and UDVD are capable to deal with spatial vari-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">ations, UDVD still produces sharper and clearer recon-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">structed image. These results demonstrate that UDVD is</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">capable to handle not only multiple degradations but also</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">their spatial variations.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Bicubic 30.39/0.8308 24.01/0.5369 27.55/0.7271 22.87/0.4724 27.21/0.6918 22.92/0.4449 SRCNN [1] 32.75/0.8944 25.01/0.6950 29.30/0.8074 23.78/0.5898 28.41/0.7736 23.76/0.5538 VDSR [2] 33.67/0.9150 25.20/0.7183 29.78/0.8244 24.00/0.6112 28.83/0.7893 24.00/0.5749 SRMD * [20] 33.86/0.9232 28.30/0.8123 29.80/0.8342 26.34/0.7025 28.87/0.8001 25.84/0.6561 Average PSNR/SSIM values on fixed degradations.</figDesc><table><row><cell>Methods</cell><cell>BI</cell><cell>Set5</cell><cell>DN</cell><cell>BI</cell><cell>Set14</cell><cell>DN</cell><cell>BI</cell><cell>BSD100</cell><cell>DN</cell></row><row><cell>RDN [10]</cell><cell cols="9">34.71/0.9280 28.47/0.8151 30.57/0.8447 26.60/0.7107 29.26/0.8079 25.93/0.6573</cell></row><row><cell>RCAN [12]</cell><cell cols="2">34.74/0.9299</cell><cell>-</cell><cell cols="2">30.65/0.8482</cell><cell>-</cell><cell cols="2">29.32/0.8111</cell><cell>-</cell></row><row><cell>UDVD  *</cell><cell cols="9">33.99/0.9240 28.52/0.8186 30.04/0.8371 26.65/0.7146 28.94/0.8016 25.99/0.6632</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image superresolution using deep convolutional networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2790" to="2798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision (ICCV)</title>
				<meeting>IEEE Int. Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4549" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accelerating the superresolution convolutional neural network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision (ECCV)</title>
				<meeting>European Conf. Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition Wksp</title>
				<meeting>IEEE Conf. Computer Vision and Pattern Recognition Wksp</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1132" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008">2018. 1, 2, 5, 6, 8</date>
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep backprojection networks for super-resolution</title>
		<author>
			<persName><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1664" to="1673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision (ECCV)</title>
				<meeting>European Conf. Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008">2018. 1, 2, 5, 6, 7, 8</date>
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision Wksp. (ECCVW)</title>
				<meeting>European Conf. Computer Vision Wksp. (ECCVW)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast and accurate image super-resolution with deep laplacian pyramid networks</title>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Single-image superresolution: A benchmark</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision (ECCV)</title>
				<meeting>European Conf. Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning deep cnn denoiser prior for image restoration</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Conditioned regression models for non-blind single image superresolution</title>
		<author>
			<persName><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rüther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision (ICCV)</title>
				<meeting>IEEE Int. Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="522" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning a single convolutional super-resolution network for multiple degradations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008">2018. 1, 2, 3, 5, 6, 7, 8</date>
			<biblScope unit="page" from="3262" to="3271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">zero-shot&quot; superresolution using deep internal learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008">June 2018. 1, 2, 3, 7, 8</date>
			<biblScope unit="page" from="3118" to="3126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised image super-resolution using cycle-in-cycle generative adversarial networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition Wksp</title>
				<meeting>IEEE Conf. on Computer Vision and Pattern Recognition Wksp</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="814" to="81409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Blind superresolution kernel estimation using an internal-gan</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bell-Kligler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Blind super-resolution with iterative kernel correction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007">2019. 1, 2, 3, 5, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Neural Information Processing Systems (NIPS)</title>
				<meeting>Conf. Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Kernelpredicting convolutional networks for denoising monte carlo renderings</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bako</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vogels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Novák</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harvill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Derose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rousselle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="97" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Burst denoising with kernel prediction networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2502" to="2510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Denoising with kernel prediction and asymmetric loss functions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vogels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rousselle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Röthlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harvill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Novák</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="124" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning deformable kernels for image and video denoising</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1904">1904.06903. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep video superresolution network using dynamic upsampling filters without explicit motion compensation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive convolution</title>
		<author>
			<persName><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2270" to="2279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive separable convolution</title>
		<author>
			<persName><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision (ICCV)</title>
				<meeting>IEEE Int. Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dynamic conditional networks for few-shot learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision (ECCV)</title>
				<meeting>European Conf. Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations (ICLR)</title>
				<meeting>Int. Conf. Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition Wksp</title>
				<meeting>IEEE Conf. Computer Vision and Pattern Recognition Wksp</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1122" to="1131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image superresolution: Methods and results</title>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition Wksp</title>
				<meeting>IEEE Conf. Computer Vision and Pattern Recognition Wksp</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1110" to="1121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations (ICLR)</title>
				<meeting>Int. Conf. Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Low-complexity single-image superresolution based on nonnegative neighbor embedding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Alberi-Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conf. (BMVC)</title>
				<meeting>British Machine Vision Conf. (BMVC)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conf. Curves and Surfaces</title>
				<meeting>International Conf. Curves and Surfaces</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision (ICCV)</title>
				<meeting>IEEE Int. Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2001">2001. 1, 5, 6, 7</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Image upsampling via imposed edge statistics</title>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (TOG)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">95</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The noise clinic: a blind image denoising algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lebrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Colom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Processing On Line (IPOL)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="54" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
