<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PRODIGY: Enabling In-context Learning Over Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-05-21">21 May 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qian</forename><surname>Huang</surname></persName>
							<email>qhwang@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
							<email>hyren@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Chen</surname></persName>
							<email>pengc@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gregor</forename><surname>Kr?manc</surname></persName>
							<email>gregor.krzmanc@ijs.si</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Ljubljana</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Zeng</surname></persName>
							<email>dzeng@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
							<email>pliang@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PRODIGY: Enabling In-context Learning Over Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-05-21">21 May 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2305.12600v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In-context learning is the ability of a pretrained model to adapt to novel and diverse downstream tasks by conditioning on prompt examples, without optimizing any parameters. While large language models have demonstrated this ability, how in-context learning could be performed over graphs is unexplored. In this paper, we develop Pretraining Over Diverse In-Context Graph Systems (PRODIGY), the first pretraining framework that enables in-context learning over graphs. The key idea of our framework is to formulate in-context learning over graphs with a novel prompt graph representation, which connects prompt examples and queries. We then propose a graph neural network architecture over the prompt graph and a corresponding family of in-context pretraining objectives. With PRODIGY, the pretrained model can directly perform novel downstream classification tasks on unseen graphs via in-context learning. We provide empirical evidence of the effectiveness of our framework by showcasing its strong in-context learning performance on tasks involving citation networks and knowledge graphs. Our approach outperforms the in-context learning accuracy of contrastive pretraining baselines with hard-coded adaptation by 18% on average across all setups. Moreover, it also outperforms standard finetuning with limited data by 33% on average with in-context learning. * indicates equal contribution.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In-context learning is a novel and one of the most intriguing capabilities of language models <ref type="bibr" target="#b0">[1]</ref>. It refers to the capability of a pretrained model to perform novel and diverse tasks directly at the prediction time when prompted with just a few examples, without the need to update the model weights. For example, a person may describe the new task (e.g., question answering, machine translation, or code generation) using natural language and demonstrate it to the language model with several prompt examples. The language model then directly without any model training or finetunning performs the task.</p><p>However, how to enable in-context learning for diverse graph machine learning tasks, such as identifying misinformation spreader in social networks <ref type="bibr" target="#b13">[14]</ref> and product suggestions across online e-commerce websites <ref type="bibr" target="#b20">[21]</ref>, still remain unexplored and challenging. An in-context learner for graphs should be able to solve novel tasks on novel graphs. For example, give music product </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? ?</head><p>Queries #:</p><formula xml:id="formula_0">Graph $: (A) (C) (B)</formula><p>Few-shot Prompting Figure <ref type="figure">1</ref>: In-context few-shot prompting over graphs with prompt graph for edge classification in PRODIGY. (A) Given the source graph G, we provide prompt examples S that consist of the input head/tail nodes and their labels, as well as the queries. (B) For each datapoint from both prompt examples and the queries, we first construct its data graph G D by retrieving context from the source graph G. (C) Then we create a task graph to capture the connection between each datapoint and each label, which includes a data node v x for each datapoint and a label node v y for each label in Y. Each pair of data and label nodes are connected with edge attributes corresponding to their binary labels. recommendations on Spotify when being trained on Amazon purchasing graph. The first challenge here is how to formulate and represent node-, edge-and graph-level tasks over graphs with a unified task representation that allows the model to solve diverse tasks without the need for retraining or parameter tuning. In other words, the key challenge is: what is an analog of natural language prompting for graph machine learning tasks? The second challenge is how to design model architecture and pretraining objectives that enable models to achieve in-context learning capability across diverse tasks and diverse graphs in the unified task representation. Existing graph pretraining methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13]</ref> only aim to learn a good graph encoder and require fine-tuning to adapt to different tasks, while existing meta-learning methods over graphs <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25]</ref> only aim to generalize across different tasks within the same graph. On the other hand, achieving in-context learning requires tackling the more difficult setting of generalizing across the graphs and tasks without finetuning.</p><p>Here we present a general approach for solving these two challenges for classification tasks on graphs:</p><p>(1) prompt graph, an in-context graph task representation, and (2) Pretraining Over Diverse In-Context Graph Systems (PRODIGY), a framework for pretraining an in-context learner over prompt graphs.</p><p>We propose prompt graph (Figure <ref type="figure">1</ref>) to provide unified way to represent diverse node-, edge-and graphlevel machine learning tasks. Prompt graph first contextualizes the input nodes/edges on which we make prediction (including both the prompt examples and the queries), then connects them with additional label nodes, such that the prompt examples are interconnected with queries. Such a unified representation allows us to specify diverse graph machine learning tasks to the same model regardless of the graph size.</p><p>PRODIGY then designs both model architecture and pretraining objectives with the prompt graph in-context task formulation, such that the model is pretrained to solve tasks across a wide range of tasks and graphs, and can continue to do so out-of-the-box. We design a graph architecture that utilizes graph neural networks to learn node/edge representations and an attention mechanism to communicate over prompt graph. Furthermore, we propose a family of in-context pretraining objectives over prompt graph. In particular, this includes a novel self-supervised pretraining task, neighbor matching, where we classify which neighborhood a node or edge belongs to.</p><p>We use PRODIGY framework to pretrain on citation networks (MAG240M <ref type="bibr" target="#b4">[5]</ref>) and knowledge graphs (Wiki <ref type="bibr" target="#b21">[22]</ref>). We then show that such model (without any retraining) provides strong performance on in-context paper category classification and knowledge graph completion tasks on novel graphs it was never trained on (arXiv, ConceptNet, FB15K-237, NELL) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23]</ref>. Specifically, PRODIGY improves upon contrastive pretraining baselines with hard-coded adaptation for in-context setup by 18% on average across all datasets and numbers of labels to classify among. Moreover, it also outperforms standard finetuning with limited data by 32.6% on average with in-context learning. It even outperforms the state-of-the-art few-shot learning methods trained on the testing downstream graph with pure in-context learning. Finally, we further demonstrate that our methods achieve increasingly higher performance with more examples in the prompt even beyond what it was pretrained with, which shows that the model really learns to learn from context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">In-context Learning over Graphs</head><p>In this work, we specifically focus on in-context learning for node and edge classification tasks on graphs with few-shot prompting, which are the forms of the most standard and important graph machine learning tasks. In this section, we introduce the concrete classification tasks over graphs and few-shot prompting over them with our in-context task representation prompt graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Classification Tasks over Graphs</head><p>We define a graph as G = (V,E,R), where V, E, R represent the set of nodes, edges and relations. An edge e = (u,r,v) ? E consists of a subject u ? V, a relation r ? R and an object v ? V.</p><p>Given a set of classes Y, a standard classification task is predicting the labeling y ? Y of each input x ? X . A node-level classification task is similar but each input is a single node in G, i.e., X = V, with the additional auxiliary information of the entire graph G. For example, over a citation network consisting of authors and papers, a node-level classification task could be predicting the primary institution of each author. Similarly, an edge-level classification task is predicting the best labeling of potential edges formed by any pair of nodes, i.e., X = V ?V. A common special case is that the classes are the same as the relations Y = R, such as predicting the relation between entities over knowledge graphs. More generally, the same definitions can be extended to subgraph and graph-level classification tasks, where the input data x may consist of more nodes and edges, and essentially represents a subgraph of G.</p><p>Since we are interested in tasks of different types/levels, we design a unified formulation, where the space of the input X consists of graphs, i.e., x i ? X ,x i = (V i ,E i ,R i ). For node classification task, G i only consists of the input node that we aim to make predictions on, i.e., |V i | = 1 and |E i | = 0; for edge classification task, it consists of (subject, object) pair, i.e., |V i | = 2 and |E i | = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Few-shot Prompting</head><p>Here we define in-context learning setup for classification tasks over graphs with few-shot prompting. For a k-shot prompt with a downstream m-way classification tasks with |Y| = m classes, we use a small number of input-label pairs S = {(x i ,y i )} m?k i=1 as prompt examples of the task specification, such that there are k input-label pairs with label y for each y ? Y. We also give the model a set of queries Q = {x i } n i=1 that we want to predict labels for. We emphasize an important difference of classification tasks on graphs from language and other modalities. Namely, since all input datapoints are nodes/edges/subgraphs from the larger source graph G, this graph contains critical information and provides contexts for the inputs, e.g., the local neighborhood of the input node that we aim to predict. Hence, besides S and Q, we also need to include the source graph G in the prompt.</p><p>Given the above information as the prompt, the pretrained model should be able to directly output the predicted labels for each datapoint in Q via in-context learning. Thus, how to formulate the information as a unified and efficient form of input poses a unique challenge and affects the model architecture. Below, we present our in-context task formulation prompt graph designed to do so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Prompt Graph Representation</head><p>Inspired by <ref type="bibr" target="#b1">[2]</ref>, we propose prompt graph as a unified representation of a k-shot prompt over graphs for an m-way classification task (Figure <ref type="figure">1</ref>). A prompt graph is composed of data graphs and a task graph: Data graph. To construct a prompt graph, we first perform contextualization of each datapoint x i = (V i ,E i ,R i ) in S and Q in the source graph G to form data graphs. The goal is to gather more information about the x i from the source graph G without having to represent the entire source graph explicitly. There are many potential designs for contextualization, from explicitly retrieving subgraphs to implicitly using embedding-based methods. Here we construct data graph G D i by sampling k-hop neighborhood of V i in G. In other words,</p><formula xml:id="formula_1">G D i = (V D i , E D i , R D i ) ? k i=0 Neighbor(V i , G, i), where V i ? V D i ? V, E i ? E D i ? E, R i ? R D i ? R,</formula><p>and Neighbor is a function that returns the exact i-hop neighbors of each node in V i . With this data graph G D i , we call the node set that corresponds to the nodes in V i before contextualization input node set, e.g., the target node to classify in node classification task and the pair of nodes in link prediction task.</p><p>Task graph. After contextualizing each datapoint to a data graph G D , we then construct task graph G T to better capture the connection and relationship among the inputs and the labels. For each data graph G D i from the previous stage, we have a data node v xi that represents each input; for each label, we have a label node v yi . So overall, a task graph contains m?k+n data nodes (m?k prompt examples and n queries) and m label nodes, as shown in Figure <ref type="figure">1</ref>. Now we add edges between the data nodes and the label nodes: For the query set, since we do not know the labels of each graph, we add single directional edges from all label nodes to each datapoint in the query set, i.e., each query data node v xi will be connected to all the label nodes as shown by the yellow edges in Figure <ref type="figure">1</ref>; For the prompt examples, we connect each data node to all the label nodes, where the edge with the true labels is marked as T while the others are marked as F, as shown by the green and red edges in Figure <ref type="figure">1</ref> respectively. Together we propose the prompt graph that consists of both data graphs and a task graph. Prompt graph effectively captures the relationship between input data x i and the label y i through the context captured in data graph G D i and the data node v xi and the label node v yi in the task graph G T . It is also possible to extend prompt graph to non-classification tasks and free-form text prompting. For example, for numerical regression (e.g. molecular energy prediction) and other free-form generation tasks (e.g. text generation), one can extend our task graph to contain vector values on the edges to represent y i . Then different label nodes would represent different prediction tasks. To support more general forms of prompting, one can include additional task information and instructions in the feature of label nodes, and additional description paired with each datapoint in the global feature in data graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Pretraining to Enable In-context Learning</head><p>So far given a few-shot prompt for a classification task over graphs, we have defined a prompt graph representation for it that captures relationships between the prompt examples, queries, and labels. Now we need to design a pretraining strategy that can pretrain a generalizable model capable of in-context learning. We assume access to a pretraining graph G pretrain that is independent of the source graph G for the downstream task.</p><p>In this section, we introduce PRODIGY, a general pretraining framework over G pretrain that is designed specifically for enabling in-context learning over downstream classification tasks without any additional finetuning steps on arbitrary graphs. Our framework PRODIGY has two main components: model architecture over prompt graph and in-context pretraining objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Message Passing Architecture over prompt graph</head><p>Next we introduce our model architecture over the prompt graph consisting of two submodules: Data graph Message Passing. First, we apply a message passing GNN module M D that learns node representation E for nodes in each G D .</p><formula xml:id="formula_2">E ? R |V D |?d = M D (G D ) (<label>1</label></formula><formula xml:id="formula_3">)</formula><p>where d is the embedding dimension. M D can be implemented in multiple ways, such as using Graph Convolutional Network (GCN) or Graph Attention Networks (GAT) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>To read out a single embedding G i for each data graph, we perform another aggregation step to pool node embeddings. For node classification tasks, we take the updated node representation of the single input node that we aim to predict, i.e.:</p><formula xml:id="formula_4">G i = E Vi<label>(2)</label></formula><p>For link prediction tasks, we concatenate the pair of nodes, which we want to predict a link between, as well as a max pooling over all node representations following <ref type="bibr" target="#b9">[10]</ref> with an additional linear projection layer at the end to convert the embedding size back to d.</p><formula xml:id="formula_5">G i = W T (E v1?Vi ||E v2?Vi ||max(E i ))+b,<label>(3)</label></formula><p>where || represents concatenation, W ? R 3d?d is a learnable weight matrix and b is the learnable bias.</p><p>Task graph Message Passing. Note in the previous step there is no communication between different datapoints in S and Q. Now we would like to communicate between them via message passing over the task graph G T . We apply another GNN M T on the task graph to obtain updated representation of data nodes and label nodes.</p><formula xml:id="formula_6">H = M T (G T ) (4)</formula><p>where H is the obtained embedding per node. The initial embedding of data node v xi is G i and the embedding of label node v yi can either be initialized with random Gaussian or additional information available about the labels. Each edge also has two binary features e ij that indicate 1) whether the edge comes from an example or a query, and 2) the edge type of T or F. For M T , we use an attention-based GNN, where each node performs attention to other nodes at each layer. See the architecture detail in the appendix C.</p><p>The goal of this step is to learn a better representation of the label nodes using the support examples and propagate label information back to the support and query graph representation for a more task-specific graph representation.</p><p>Prediction Read Out. Finally, we readout the classification logits O i by taking cosine similarity between each pair of query graph representation and label representation, as in contrastive learning:</p><formula xml:id="formula_7">O i = [cosine_similarity(H xi ,H y ),?y ? Y]<label>(5)</label></formula><p>Note that we could perform the two message passing steps for multiple rounds to have more communication between x i and learn a better representation. One key insight is that different in-context prompt examples share information through the label nodes, which can be seen as an information bottleneck.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">In-context Pretraining Objectives</head><p>In order to pretrain the model for solving the downstream graph tasks in-context, we propose a set of in-context pretraining objectives. The goal is to pretrain the graph model using a large pretraining graph G pretrain independent of the downstream task graph, such that the model can directly be applied on downstream tasks with in-context learning.</p><p>Our main design principle is that we formulate each pretraining objective in an in-context learning way. Most previous graph pretraining objectives only pretrain a shared graph encoder to perform various tasks with task-specific heads, so they require finetuning for another task-specific head over each downstream task. In contrast, we explicitly construct in-context pretraining tasks in prompt graph form and pretrain the model to solve diverse tasks in-context with the same set of weights, such that it can perform in-context learning directly over downstream tasks.</p><p>Below, we detail our proposed family of in-context pretraining objectives in terms of three components: 1) pretraining task generation, including few-shot prompt (i.e. Figure <ref type="figure">1(A)</ref>) and corresponding labels, 2) converting generated few-shot prompt to prompt graph format (i.e. Figure <ref type="figure">1(B,</ref><ref type="figure">C</ref>)) with augmentation, and 3) pretraining loss over the generated prompt graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Pretraining Task Generation</head><p>We propose two methods to generate pretraining tasks from the pretraining graph G pretrain in the form of few-shot prompts: neighbor matching and multi-task.</p><p>Neighbor Matching. Given the pretraining graph, we construct self-supervised in-context pretraining tasks with the goal of classifying which local neighborhood a node belongs to, where each local neighborhood is defined by the example nodes belonging to that neighborhood. Intuitively, we sample multiple subgraphs from the pretraining graph G pretrain as the local neighborhoods, and we say a node belongs to a local neighborhood if it is in the sampled subgraph.</p><p>Formally, we denote NM k,m as a sampler that generates m-way neighbor matching tasks, where each includes a k-shot prompt (G pretrain ,S NM ,Q NM ) (see subsection 2.2 and Figure <ref type="figure">1(A)</ref>) and the labels of the queries. For simplicity of the notation, we will include the labels in Q NM as paired with the inputs:</p><formula xml:id="formula_8">(G pretrain ,S NM ,Q NM ) ? NM k,m (G pretrain )<label>(6)</label></formula><p>To generate these, we first sample m nodes from the pretraining graph G pretrain , where each of the sampled node corresponds to one class.</p><formula xml:id="formula_9">C = {c i } m i=1 c i ? Uniform(V pretrain )<label>(7)</label></formula><p>For each sampled node/class c i , we sample k different nodes from its exact l-hop neighbors. These k nodes serve as examples of label c i . We also sample additional n m nodes similarly for each label c i to form the query set. Formally,</p><formula xml:id="formula_10">N i = Neighbor(c i ,G pretrain ,l)<label>(8)</label></formula><formula xml:id="formula_11">S i = {(x j ,y j = c i )} k j=1 x j ? Uniform(N i )<label>(9)</label></formula><formula xml:id="formula_12">Q i = {(x j ,y j = c i )} n m j=1 x j ? Uniform(N i )<label>(10)</label></formula><p>In such a way, we constructed a neighbor matching pretraining task sample in the format of a few-shot prompt</p><formula xml:id="formula_13">(G pretrain ,S NM = S i ,Q NM = Q i ).</formula><p>The neighbor matching task generation process outlined above is specifically applicable when the downstream tasks are also node classification. When the downstream task is link prediction, we may adapt the above neighbor matching tasks to over edges correspondingly. Specifically, we can expand each sampled input node x i to an edge by randomly sampling an edge that contains x i . Then, instead of classifying to which neighborhood a node in the query set belongs, now the neighbor matching task is to classify to which neighborhood an edge in the query set belongs.</p><p>Multi-task. When the pretraining graphs have node or edge-level labeling f (x i ) = y i ? Y for some x i ? V pretrain or E pretrain , we can further leverage this signal to perform supervised pretraining. Similar to neighbor matching, the key is to construct such supervised pretraining tasks in the format of few-shot prompts and corresponding labels.</p><formula xml:id="formula_14">(G pretrain ,S MT ,Q MT ) ? MT k,m (G pretrain ,f )<label>(11)</label></formula><p>For node classification tasks, we first sample m labels from the whole label set. Then, for each label, we directly sample k nodes as support examples and n m nodes with labels in this set as query examples.</p><formula xml:id="formula_15">C = {c i } m i=1 c i ? Uniform(Y)<label>(12)</label></formula><formula xml:id="formula_16">S i = {(x j ,y j = c i )} k j=1 x j ? Uniform({x i |f (x i ) = c i })<label>(13)</label></formula><formula xml:id="formula_17">Q i = {(x j ,y j = c i )} n m j=1 x j ? Uniform({x i |f (x i ) = c i })<label>(14)</label></formula><p>We then construct a task with the few-shot prompt as (G pretrain ,S MT = S i ,Q MT = Q i ). For link prediction, we directly use the edge type function as</p><formula xml:id="formula_18">f , i.e. f ((v 1 ,v 2 )) = r ?? (v 1 ,r,v 2 ) ? E.</formula><p>With this f , we may directly sample m edge types and construct pretraining tasks in a similar way as above.</p><p>The benefit of such a supervised pretraining objective is that it could directly resemble the format of downstream tasks, compared with neighbor matching objective, which may only serve as a surrogate. However, it requires extra labels if f is not part of G pretrain , e.g. node classification labels that may not exist for some pretraining graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Prompt graph generation with augmentation</head><p>After we obtained the few-shot prompts and labels for either of the two tasks (NeighborMatching and multi-task), we need to construct the prompt graph for pretraining. In addition to the standard construction process described in subsection 2.3, we add an additional augmentation step to augment</p><formula xml:id="formula_19">! ! " ! # " ! $ " ! % " ? ? ?</formula><p>Prompt Examples !: Queries #:</p><p>?</p><formula xml:id="formula_20">2 hops neighbors Input ! Label "</formula><p>Figure <ref type="figure">2</ref>: Neighbor matching pretraining task generation. The key idea of neighbor matching is to classify whether a node/edge is in the local neighborhood of a set of sampled nodes (as labels). Given a set of sampled nodes and their two-hop neighbors as prompt examples, we aim to classify whether a query node is also a two-hop neighbor of each sampled node.</p><p>the data graphs as inspired by Contrastive Learning. The key insight is to corrupt data graph such that the pretrained model learns representation invariant to various corruptions.</p><p>Here we demonstrate how we adopt graph augmentation techniques during the construction of prompt graph from a few-shot prompt generated from G pretrain . We first still sample the k-hop neighbor subgraph of each sample G i in the prompt examples and queries:</p><formula xml:id="formula_21">G D i ? k j=1</formula><p>Neighbor(G i ,G pretrain ,j). Then we adopt the following two augmentation techniques to create augmented data graph G aug i , including (1) node dropping, and (2) node feature masking <ref type="bibr" target="#b23">[24]</ref>. For node dropping, we randomly drop nodes from the k-hop neighbor subgraph and take the remaining graph as G aug i = DropNode(G D i ). For node feature masking, we randomly mask the feature of a subset of nodes with value zero to create</p><formula xml:id="formula_22">G aug i = MaskNode(G D i ).</formula><p>With the augmented data graphs for each datapoint in the prompt examples and the queries, we may accordingly construct the task graph G T by creating a data node v xi for each augmented data graphs and the label node v yi as introduced in subsection 2.3. Combining data graphs with task graph, we obtain the prompt graph formulation with augmentation for the few-shot prompt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Pretraining Loss</head><p>Finally, we pretrain the model with the cross-entropy objectives over generated prompt graphs:</p><formula xml:id="formula_23">(G pretrain ,S NM ,Q NM ) ? NM k,m (G pretrain ) (15) (G pretrain ,S MT ,Q MT ) ? MT k,m (G pretrain ,f ) (16) L = E xi?QNM CE(O NM,i ,y NM,i )+ E xi?QMT CE(O MT,i ,y MT,i )<label>(17)</label></formula><p>where O NM,i is the logits produced by our model over input of G aug i and G T produced from Q NM , as described in subsection 3.1; y NM,i is the corresponding label of x i in Q NM ; Similar for MT terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets. For pretraining, we use two datasets: MAG240M <ref type="bibr" target="#b4">[5]</ref>, a large scale citation network with 122 million nodes and 1.3 billion edges; and Wiki, a knowledge graph (KG) constructed from Wikipedia <ref type="bibr" target="#b21">[22]</ref> with 4.8 million nodes and 5.9 million edges. After the model is pretrained we evaluate its in-context learning capability on diverse classification tasks over 4 graphs: arXiv [6], ConceptNet <ref type="bibr" target="#b15">[16]</ref>, FB15K-237 <ref type="bibr" target="#b22">[23]</ref>, NELL <ref type="bibr" target="#b22">[23]</ref>. We use subsets of knowledge graph datasets same as in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref>. For arXiv, the downstream task is an m-ways node classification task that predicts the paper category. For knowledge graph datasets (ConceptNet, FB15K-237, NELL), the downstream task is an m-ways relation type classification task that predicts the relationship connecting the two input nodes.</p><p>Evaluation. We pretrain our model on MAG240M and Wiki and then we evaluate the in-context learning performance on differnt downstream datasets that belong to similar domain as the pretraining dataset (e.g., pretraining on Wiki and evaluating on ConceptNet, FB15K-237, and NELL). Each of the downstream classification datasets has its original train, validation, and test splits. To simulate the situation where there are a limited amount of labeled data in the downstream task, we randomly select 10 nodes (or edges) from the training split per way as the prompt examples with known labels. Then, we construct a k-shot prompt for test nodes (or edges) from the test split by randomly selecting k examples per way from these available examples. This allows us to test the model's ability to learn in-context relationships and perform well on classification tasks with truly limited known labels. By default we use k = 3 shots in our experiments.</p><p>Methods and Baselines. We consider three versions of our proposed framework PRODIGY: 1) PG-NM, which uses neighbor matching task for pretraining; 2) PG-MT, which employs multi-task pretraining; and 3) full PRODIGY, which combines the previous two methods. To augment the data, we use DropNode and MaskNode augmentations with a probability of 0.5 per node for each method.</p><p>We consider three baselines for comparison: 1) NoPretrain, which uses a randomly-initialized model with the same architecture as our pretrained models; 2) Contrastive <ref type="bibr" target="#b23">[24]</ref>, which employs a standard contrastive learning method with the same augmentation as above and uses a hard-coded nearest neighbor algorithm to adapt to our in-context learning setting. Specifically, we classify the query by comparing its pretrained embedding against the average embedding of the example inputs of each class. 3) Finetune <ref type="bibr" target="#b6">[7]</ref>, which trains an additional linear classification head on top of the graph encoder pretrained with contrastive learning, following the standard practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">In-Context Learning Results</head><p>We first evaluate the in-context learning capability for node classification and link prediction with various numbers of ways (i.e. number of classes to classify among).</p><p>Strong in-context learning performance. The results demonstrate that our method PRODIGY consistently outperforms all other baselines in this setting. It achieves the highest average accuracy across all ways on arXiv, with an average improvement of 28.6% and up to 48% over the best baseline of Contrastive. Over KGs, PRODIGY also outperforms contrastive learning on average by 12.2%. PRODIGY also demonstrates similar-to-better performance compared to Finetune, which requires additional training on downstream tasks. On arXiv, we see an average improvement of 77.7% over all ways. This can be attributed to the diverse set of pretraining tasks incorporated in PRODIGY, which allows the model to avoid overfitting on specific tasks and learn in-context.</p><p>Self-supervised pretraining PG-NM bridges different tasks. In particular, we highlight that the pure self-supervised pretraining method PG-NM produces significantly higher in-context learning performance over arXiv than baselines, even though the model is pretrained on different tasks from the downstream task. This advantage can be further leveraged by pretraining on even larger-scale unlabeled datasets. On the other hand, PG-MT follows the supervised pretraining objective that directly resembles the format of downstream tasks. On KGs, this allows PG-MT to adapt better to downstream task even sometimes compared to the full PRODIGY ( marked by underlines), while PG-NM might have overfitted to the incorrect strategy of only identifying co-occurring nodes. Yet, PG-MT performs worse on arXiv potentially due to less diversity. The full PRODIGY, which ensembles the two, achieves more diversity than either single task and therefore achieves the best performance over both worlds.</p><p>Outperforming meta-learning method trained on test graph. Finally, we compare PG-NM in-context learning performance against state-of-the-art meta-learning method TENT <ref type="bibr" target="#b19">[20]</ref> over the downstream test graph arXiv. We evaluate the average 3-ways classification tasks performance over only test labels, since TENT trains on train labels from arXiv. PG-NM achieves 69.07% over the 65.13% of TENT, even though PG-NM has never been trained on any paper category classification  task during pretraining. This demonstrates the power of self-supervised pretraining over large amount of data compared to supervised meta-learning over the limited labeled data (train labels in arXiv).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablations</head><p>Aside from PG-NM and PG-MT, we also conduct ablation studies on various configurations of the selfsupervised objective PG-NM as described in 3.2. See the full results in Appendix E and Table <ref type="table" target="#tab_4">4</ref>. Overall, the ablation results reveal that using all of the elements together results in the highest performance. Specifically, attribute prediction (see appendix A) has the greatest impact on PG-NM's performance, as its removal results in an average 7% drop across all ways, shown in the 'No-Attr' column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation using different numbers of in-context examples</head><p>We investigate our method's ability to learn from the context by analyzing its performance as the number of prompt examples changes. Figure <ref type="figure" target="#fig_0">3</ref> shows the result on ConceptNet. See full results on other datasets in Appendix F. As the number of prompt examples increases, the margin of our proposed PG models over the baseline increases. This supports the hypothesis that the PRODIGY models can more effectively learn the unknown task by reasoning about the common characteristics of prompt examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Scaling with Data Size</head><p>Finally, we explore how the model scales with more pretraining data. The result on arXiv in a 5-ways setting is illustrated in Figure <ref type="figure" target="#fig_1">4</ref>. It shows that the Contrastive baseline saturates quickly and its performance fluctuates as trained over more pretraining data. Instead, PRODIGY consistently shows an improvement in performance as more data is pretrained on, since the pretraining tasks are harder and more diverse.</p><p>5 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">In-context Learning of Large Language Models</head><p>Pretrained large language models can make predictions for diverse downstream tasks directly by prompting with a few examples of the task or more generally any textual instructions. This ability is called in-context learning. Comparing to previous language encoder models like BERT <ref type="bibr" target="#b3">[4]</ref>, it drastically reduces the adaptation effort comparing to fine-tuning, and has demonstrated strong performance in a broad range of models and tasks. Our work extends this success similarly to graph data compared to the current pretrained graph encoders, such that a single pretrained model can be adapted to different classification tasks over different graphs without additional fine-tuning but only few-shot prompting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Pretraining on Graphs</head><p>There are many existing works on pretraining over graphs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13]</ref>. However, they all follow the general paradigm of learning a good graph encoder that can perform certain pretraining tasks, such as masked feature prediction <ref type="bibr" target="#b6">[7]</ref> and paired graph classification <ref type="bibr" target="#b23">[24]</ref>. To adapt to any downstream tasks, it then requires finetuning a classification head on top of the encoder with large amount of task specific data for each downstream task. In contrast, we explore pretraining methods for inducing general in-context learning ability, such that the pretrained model can be directly used for various downstream tasks with no gradient updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Meta Learning on Graphs</head><p>Another closely related line of works is meta-learning methods over graphs that aim to address standard few shot learning problems over graphs <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25]</ref>. However, existing meta-learning methods are only designed and tested for generalizing across different tasks on the same graph: the methods are trained on a set of training tasks on a graph, then tested over a disjoint but similar set of test tasks over the same graph. They are shown to exhibit optimal performance only when trained on similar curated tasks <ref type="bibr" target="#b9">[10]</ref>. Different from this, our work explicitly focuses on the in-context learning performance, i.e. model performance on graphs and tasks completely different from the pretraining without additional fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduce PRODIGY, the first framework that enables in-context learning on graphs. A model that is pretrained using PRODIGY can seamlessly execute a new classification task over new graphs represented by prompt graph. It markedly surpasses the performance of other baseline models with in-context learning, even those that employ finetuning, in both the node and edge classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Attribute Prediction Loss</head><p>For each augmented DataGraph G aug , the certain node features F v are masked during MaskNode augmentation. Therefore, we can reconstruct them using the learned embedding E v with a MLP and train with MSE reconstruction Loss. </p><formula xml:id="formula_24">L attr (G aug ) = 1 |V D | v MSE(F v ,MLP(E v ))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Dataset Statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Task Graph GNN Architecture</head><p>For the GNN over task graph M T , we use an attention-based GNN, where each node performs attention to other nodes at each layer:</p><formula xml:id="formula_25">? ij = MLP W T q H l i ||W T k H l j ||e ij<label>(18)</label></formula><formula xml:id="formula_26">? ij = exp(? ij ) k?N (i)?{i} exp(? ik )<label>(19)</label></formula><formula xml:id="formula_27">H l+1 i = ReLU ? ? BN ? ? H l i +W T o j?N (i)?{i} ? ij W T v H l j ? ? ? ?<label>(20)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Hyperparameters</head><p>D.1 Model Architecture, MAG240M and arxiv</p><p>We initialize the node features in citation network datasets using a pretrained language model (RoBERTa <ref type="bibr" target="#b11">[12]</ref> base model trained on NLI and STSB).The architecture of our PromptGraph model in all of our proposed methods for citation network datasets (full PRODIGY, PG-NM, and PG-MT) and the baseline (NoPretrain), consists of two message passing layers, M D , over the DataGraph and one message passing layer, M T , over the TaskGraph. These layers are defined in Section 3.1.</p><p>For the Contrastive method, the architecture includes two message passing layers, M d , over the DataGraph, and a contrastive learning component that is defined in Section 4.1. The mode for Finetune is the same as the Contrastive method, with the addition of a linear layer head over the output of the two M d layers, also described in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Model Architecture, knowledge graph datasets</head><p>We initialize node and edge features in knowledge graph datasets using a pretrained language model (MPNet <ref type="bibr" target="#b14">[15]</ref>). The architecture of our PromptGraph model in all of our proposed methods for knowledge graph datasets (full PRODIGY, PG-NM, and PG-MT) and the baseline (NoPretrain), consists of two message passing layers, M D , over the DataGraph, an aggregator as described by Equation 3, and two message passing layers, M T , over the TaskGraph, which only pass messages along the positive and query edges. These layers are defined in Section 3.1.</p><p>For the Contrastive method, the architecture includes two message passing layers, M d , over the DataGraph, an aggregator as described by Equation 3 and a contrastive learning component that is defined in Section 4.1. The mode for Finetune is the same as the Contrastive method, with the addition of a linear layer head over the output of the two M d layers, also described in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Training, MAG240M</head><p>The following describes our pretraining setup:</p><p>The pretraining task we used consisted of 30 ways, 3 shots, and 4 queries per task. This specific task configuration was carefully selected to strike a balance between complexity and diversity in the training data, without overwhelming the GPU memory.</p><p>We checkpoint the model every 500 steps.</p><p>Our pretraining setup included a model with an input dimension of 768 and an embedding dimension of 256, batch size of 1, and the AdamW optimizer with a learning rate of 1?10 -3 and weight decay of 1? 10 -3 , a pretraining task with 30 ways, 3 shots, and 4 queries per task, and checkpointing every 500 steps. This consistent configuration was applied across all the methods for fair comparison. Our full PRODIGY setup, on average, involves sampling 1 Neighbor Matching task per 1 multitask pretraining tasks.</p><p>For our evaluation process, we computed zero-shot transfer performance of the model on the test set, using the pretraining checkpoint at the 10,000 step of pretraining. The evaluation was conducted on 500 test tasks, with batch size of 5, measured on the downstream task of graph classification accuracy.</p><p>To maintain consistency, we kept the number of shots and queries constant at 3 for all evaluation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Training, Wiki</head><p>The following describes our pretraining setup:</p><p>Our pretraining setup included a model with an input dimension of 768 and an embedding dimension of 256, the AdamW optimizer with a learning rate of 1?10 -3 and weight decay of 1?10 -3 , a pretraining task with 30 ways, 3 shots, and 4 queries per task, using a batch size of 10, and checkpointing every 500 steps. This specific task configuration was carefully selected to strike a balance between complexity and diversity in the training data, without overwhelming the GPU memory. This consistent configuration was applied across all the methods for fair comparison. Our full PRODIGY setup involves sampling one neighbor matching task per 50 multitask pretraining tasks.</p><p>For our evaluation process, we computed zero-shot transfer performance of the model on the test set, using the pretraining checkpoint at the 8,000 step of pretraining. The evaluation was conducted on 500 test tasks, with batch size of 1, measured on the downstream task of graph classification accuracy. To maintain consistency, we kept the number of shots and queries constant at 3 for all evaluation tasks. We sample 1-hop neighbourhoods for ConceptNet and FB15K-237 and 2-hop neigbourhoods for NELL and Wiki.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Ablation on Table 4 for the PG-NM setting</head><p>In Table <ref type="table" target="#tab_4">4</ref>, we ablate on various configurations of the self-supervised objective PG-NM. As described in Section 3.2, PG-NM is composed of an attribute prediction loss, dropnode and zeronode augmentations, with the default setting of sampling 3 shots neighbor matching tasks. Our best setting, referred to as simply "PG-NM", is also shown in Table <ref type="table" target="#tab_1">1</ref> and comprises of attribute prediction, dropnode and zeronode augmentations, with the default setting of 3 shots.</p><p>The ablation results reveal that using all of these elements together results in the highest performance. Specifically, attribute prediction has the greatest impact on PG-NM's performance, as its removal results in an average 7% drop across all ways, as shown in the 'No-Attr' column.</p><p>Removing the dropnode and zeronode augmentations results in an average 3% drop across all ways, as shown in the No Aug' column. Removing both attribute prediction and augmentations results in performance that is similar to just removing attribute prediction alone, which is also roughly a 7% drop across all ways, as shown in the 'No Attr, Aug' column. Additionally, we found that decreasing the number of shots to 1 from the default setting of 3 resulted in an average 3.5% drop across all ways, as shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Evaluation using different numbers of shots</head><p>We show evaluation using different numbers of shots, as shown in Figures  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Scaling with Data Size</head><p>We explore how the model scales with more pretraining tasks. Note that we use the number of train steps as a proxy because the model sees more pretraining tasks as the training proceeds with almost no redundancy (0.20% for 10k tasks). The result on arXiv in a 5-ways setting is illustrated in  pretraining tasks. Instead, PRODIGY consistently shows an improvement in performance as more data is pretrained on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Compute</head><p>We use one NVIDIA A100-SXM4-80GB GPU for all our experiments. One pretrain run of 10k steps takes 3 to 4 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Broader Impacts</head><p>Our work aims to extend the success of in-context learning to graphs and start building toward graph foundation models. This would allow cost-effective and accurate predictions, especially in domains where labeled data is scarce and long tail such as network anomaly detection, rare disease diagnosis/treatment, supply chain disruption, and recommendations for new users. However, overreliance on prior knowledge from pretraining could also lead to increased social bias and unfair benefits to the dominate groups. To mitigate this, pretraining data should be diverse and well-balanced, and the pretrained models should be tested on downstream tasks over different groups and subdistributions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: In-context learning accuracy on Concept-Net in a 4-ways setting wrt. the number of prompt examples (shots).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: With more training steps and data on arXiv (5-ways), PRODIGY keeps improving while the baseline (Contrastive) saturates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: In-context learning accuracy on NELL in a 20-ways setting wrt. the number of prompt examples (shots).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>It shows that the Contrastive baseline saturates quickly and its performance fluctuates given more</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: In-context learning accuracy on arXiv in a 5-ways setting wrt. the number of prompt examples (shots).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>In-context learning accuracy (%) on arXiv paper category classification on 500 sampled test tasks with 3-shot prompts. PRODIGY was pretrained on MAG240M and is then applied in-context to arXiv, which has completely different structure and a different set of paper categories. PG-NM and PG-MT are ablations of PRODIGY.</figDesc><table><row><cell cols="3">Classes NoPretrain Contrastive</cell><cell>PG-NM</cell><cell>PG-MT</cell><cell>PRODIGY</cell><cell>Finetune</cell></row><row><cell>3</cell><cell cols="5">33.16 ? 0.30 65.08 ? 0.34 72.50 ? 0.35 65.64 ? 0.33 73.09 ? 0.36</cell><cell>65.42 ? 5.53</cell></row><row><cell>5</cell><cell cols="5">18.33 ? 0.21 51.63 ? 0.29 61.21 ? 0.28 51.97 ? 0.27 61.52 ? 0.28</cell><cell>53.49 ? 4.61</cell></row><row><cell>10</cell><cell>9.19 ? 0.11</cell><cell cols="4">36.78 ? 0.19 46.12 ? 0.19 37.23 ? 0.20 46.74 ? 0.20</cell><cell>30.22 ? 3.77</cell></row><row><cell>20</cell><cell>4.72 ? 0.06</cell><cell cols="4">25.18 ? 0.11 33.71 ? 0.12 25.91 ? 0.12 34.41 ? 0.12</cell><cell>17.68 ? 1.15</cell></row><row><cell>40</cell><cell>2.62 ? 0.02</cell><cell cols="4">17.02 ? 0.07 23.69 ? 0.06 17.19 ? 0.08 25.13 ? 0.07</cell><cell>8.04 ? 3.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>In-context learning accuracy (%) on ConceptNet, FB15K-237 and NELL (from top to bottom) on 500 sampled test tasks with 3-shot prompts. PRODIGY was pretrained on Wiki, which has completely different node and relation types from graphs it is then applied on in-context. ? 0.57 84.08 ? 0.54 80.53 ? 0.58 84.79 ? 0.51 87.02 ? 0.44 87.22 ? 12.75 10 18.82 ? 0.31 76.54 ? 0.45 72.77 ? 0.48 78.5 ? 0.44 81.06 ? 0.41 71.90 ? 5.90 20 7.42 ? 0.16 66.56 ? 0.35 62.82 ? 0.36 69.82 ? 0.34 72.66 ? 0.32 66.19 ? 8.46 40 3.04 ? 0.07 57.44 ? 0.24 49.59 ? 0.22 53.55 ? 0.23 60.02 ? 0.22 55.06 ? 4.19</figDesc><table><row><cell cols="3">Classes NoPretrain Contrastive</cell><cell>PG-NM</cell><cell>PG-MT</cell><cell>PRODIGY</cell><cell>Finetune</cell></row><row><cell>4</cell><cell>30.4 ? 0.63</cell><cell>44.01 ? 0.61</cell><cell cols="3">46.94 ? 0.61 51.78 ? 0.63 53.97 ? 0.63</cell><cell>53.85 ? 9.29</cell></row><row><cell>5</cell><cell>33.54 ? 0.61</cell><cell>81.35 ? 0.58</cell><cell cols="2">80.35 ? 0.57 89.15 ? 0.46</cell><cell>88.02 ? 0.48</cell><cell>82.01 ? 12.83</cell></row><row><cell>10</cell><cell>20.0 ? 0.35</cell><cell>70.88 ? 0.48</cell><cell cols="2">71.68 ? 0.45 82.26 ? 0.40</cell><cell>81.1 ? 0.39</cell><cell>71.97 ? 6.16</cell></row><row><cell>20</cell><cell>9.2 ? 0.18</cell><cell>59.8 ? 0.35</cell><cell>59.9 ? 0.35</cell><cell cols="2">73.47 ? 0.32 72.04 ? 0.33</cell><cell>64.01 ? 4.66</cell></row><row><cell>40</cell><cell>2.5 ? 0.08</cell><cell>49.39 ? 0.23</cell><cell cols="3">46.82 ? 0.21 58.34 ? 0.22 59.58 ? 0.22</cell><cell>57.27 ? 3.33</cell></row><row><cell>5</cell><cell>33.44</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">: Dataset statistics</cell><cell></cell></row><row><cell>Dataset</cell><cell cols="3"># Nodes # Edges # Classes</cell></row><row><cell>MAG240M</cell><cell>122M</cell><cell>1.3B</cell><cell>153</cell></row><row><cell>Wiki</cell><cell>4.8M</cell><cell>5.9M</cell><cell>639</cell></row><row><cell>arXiv</cell><cell>169K</cell><cell>1.2M</cell><cell>40</cell></row><row><cell>ConceptNet</cell><cell>791K</cell><cell>2.5M</cell><cell>14</cell></row><row><cell>FB15K-237</cell><cell>15K</cell><cell>268K</cell><cell>200</cell></row><row><cell>NELL</cell><cell>69K</cell><cell>181K</cell><cell>291</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation of PG-NM on arXiv. ? 0.35 69.13 ? 1.09 65.74 ? 1.12 68.98 ? 1.09 66.53 ? 1.12 63.60 ? 1.06 5 61.21 ? 0.29 57.49 ? 0.92 52.78 ? 0.90 57.50 ? 0.85 53.89 ? 0.92 51.27 ? 0.69 10 46.12 ? 0.19 42.03 ? 0.60 37.99 ? 0.63 42.43 ?</figDesc><table><row><cell>Ways</cell><cell>PG-NM</cell><cell>3 ?1 shot</cell><cell>No Attr</cell><cell>No Aug</cell><cell cols="2">No Attr, Aug No Attr, Aug, M T</cell></row><row><cell>3</cell><cell cols="4">72.50 0.64</cell><cell>38.87 ? 0.59</cell><cell>37.62 ? 0.34</cell></row><row><cell>20</cell><cell cols="4">33.71 ? 0.11 30.18 ? 0.38 26.60 ? 0.36 30.89 ? 0.38</cell><cell>27.50 ? 0.36</cell><cell>27.44 ? 0.17</cell></row><row><cell>40</cell><cell cols="4">23.69 ? 0.07 21.44 ? 0.22 18.03 ? 0.21 21.97 ? 0.24</cell><cell>18.52 ? 0.22</cell><cell>19.69 ? 0.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>3, 5, and 6, 7.    </figDesc><table><row><cell>accuracy</cell><cell>0.52 0.64 0.80 0.76 0.72 0.68 0.60 0.56</cell><cell></cell><cell></cell><cell></cell><cell>FB15K-237 -20 way</cell><cell>Contrastive PRODIGY</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>4</cell><cell>8</cell><cell cols="2">12 16 20 24 28 shots</cell></row><row><cell cols="7">Figure 5: In-context learning accuracy on FB15K-237 in a 20-ways setting wrt. the number of prompt</cell></row><row><cell>examples (shots).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>accuracy</cell><cell>0.57 0.60 0.69 0.81 0.78 0.75 0.72 0.66 0.63</cell><cell>0</cell><cell>4</cell><cell>8</cell><cell cols="2">shots 12 16 20 24 28 NELL -20 way Contrastive PRODIGY</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Relational multi-task learning: Modeling relations between data and tasks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Representation Learning (ICLR)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Meta relational learning for few-shot link prediction in knowledge graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01515</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>ArXiv, abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.09430</idno>
		<title level="m">Ogb-lsc: A large-scale challenge for machine learning on graphs</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12265</idno>
		<title level="m">Strategies for pre-training graph neural networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gpt-gnn: Generative pre-training of graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1857" to="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Graph meta learning via local subgraphs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Few-shot relational reasoning via connection subgraph pretraining</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>abs/2210.06722</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>Roberta</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gcc: Graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Identifying possible rumor spreaders on twitter: A weak supervised learning approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">MPNet: Masked and Permuted Pre-training for Language Understanding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="16857" to="16867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName><forename type="first">R</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">One-shot relation learning for knowledge graphs via neighborhood aggregation and paths encoding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Asian and Low-Resource Language Information Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Graph attention networks</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Task-adaptive few-shot node classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Task-adaptive few-shot node classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Graph neural networks in recommender systems: A survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">One-shot relational learning for knowledge graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">One-shot relational learning for knowledge graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5812" to="5823" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Few-shot knowledge graph completion</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3041" to="3048" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
