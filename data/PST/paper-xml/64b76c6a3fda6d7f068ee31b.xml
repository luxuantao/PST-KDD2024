<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How Is ChatGPT&apos;s Behavior Changing over Time?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-07-18">18 Jul 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lingjiao</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">James</forename><surname>Zou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stanford</forename><surname>University</surname></persName>
						</author>
						<author>
							<persName><forename type="first">U</forename><forename type="middle">C</forename><surname>Berkeley</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Exact Match</orgName>
								<address>
									<addrLine>10.3% 12.2%, 2 0.4 0.6 0.8 1 Exact Match 52.0% 10.0%, 2 0.4 0.6 0.8 1 Directly Executable 22.0% 2.0%, 2 0.4 0.6 0.8 1</addrLine>
									<postCode>2023, 2023 0 0, 2023, 2023 0 0, 2023, 2023 0 0</postCode>
									<settlement>March, June, March, June, March, June</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<addrLine>2 0.4 0.6 0.8 1</addrLine>
									<postCode>0% 8.0%, 2023, 2023 0 0</postCode>
									<settlement>March, June</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">How Is ChatGPT&apos;s Behavior Changing over Time?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-07-18">18 Jul 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2307.09009v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>GPT-4 GPT-3</term>
					<term>5 GPT-4 GPT-3</term>
					<term>5 GPT-4 GPT-3</term>
					<term>5 GPT-4 GPT-3</term>
					<term>5</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on four diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous questions, 3) generating code and 4) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was very good at identifying prime numbers (accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions (accuracy 2.4%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5 (March 2023)  in this task. GPT-4 was less willing to answer sensitive questions in June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. Overall, our findings shows that the behavior of the "same" LLM service can change substantially in a relatively short amount of time, highlighting the need for continuous monitoring of LLM quality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large language models (LLMs) like GPT-3.5 and GPT-4 are being widely used. A LLM like GPT-4 can be updated over time based on data and feedback from users as well as design changes. However, it is currently opaque when and how GPT-3.5 and GPT-4 are updated, and it is unclear how each update affects the behavior of these LLMs. These unknowns makes it challenging to stably integrate LLMs into larger workflows: if LLM's response to a prompt (e.g. its accuracy or formatting) suddenly changes, this might break the downstream pipeline. It also makes it challenging, if not impossible, to reproduce results from the "same" LLM.</p><p>Beyond these integration challenges, it is also an interesting question whether an LLM service like GPT4 is consistently getting "better" over time. It's important to know whether updates to the model aimed at improving some aspects actually hurt its capability in other dimensions.</p><p>Motivated by these questions, we evaluated the behavior of the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on four tasks: 1) solving math problems, 2) answering sensitive/dangerous questions, 3) generating code and 4) visual reasoning. These tasks were selected to represent diverse and useful capabilities of these LLMs. We find that the performance and behavior of both GPT-3.5 and GPT-4 vary significantly across these two releases and that their performance on some tasks have gotten substantially worse over time.</p><p>Related Work. There has been a plethora of benchmarks and evaluations of LLMs including GPT-3.5 and GPT-4 [LBL + 22, ZPM + 23, LNT + 23, BCL + 23]. Existing work shows that LLMs achieve reasonable performance on traditional language tasks such as reading comprehension <ref type="bibr">[dW23]</ref>, translation [JWH + 23], and summarization <ref type="bibr" target="#b8">[GLD22]</ref>. More recently, GPT-4 was shown to successfully pass difficult exams in professional domains such as medicine [NKM + 23] and law <ref type="bibr" target="#b10">[KBGA23]</ref>. To the best of our knowledge, the existing works do not systematically monitor the longitudinal drifts of widely used LLM services such as GPT-4 and GPT-3.5 over time. Monitoring model performance shifts is emerging for deploying machine learning (ML) services in the wild. [CJE + 22] offers a large-scale longitudinal dataset of commercial ML API responses on various evaluation tasks, and <ref type="bibr" target="#b2">[CCZZ21]</ref> studies how to efficiently estimate ML API performance shifts. They mainly focus on ML APIs for simple classification tasks such as sentiment analysis, while this work focuses on generative LLM services.</p><p>Figure <ref type="figure">1</ref>: Performance of the March 2023 and June 2023 versions of GPT-4 and GPT-3.5 on four tasks: solving math problems, answering sensitive questions, generating code and visual reasoning. The performances of GPT-4 and GPT-3.5 can vary substantially over time, and for the worse in some tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview: LLM Services, Tasks and Metrics</head><p>This paper studies how different LLMs' behaviors change over time. To answer it quantitatively, we need to specify (i) which LLM services to monitor, (ii) on which application scenarios to focus , and (iii) how to measure LLM drifts in each scenario.</p><p>LLM Services. The LLM services monitored in this paper are GPT-4 and GPT-3.5, which form the backbone of ChatGPT. Due to the popularity of ChatGPT, both GPT-4 and GPT-3.5 have been widely adopted by individual users and a number of businesses. Thus, timely and systematically monitoring these two services helps a large range of users better understand and leverage LLMs for their own use cases. At the time of writing, there are two major versions available for GPT-4 and GPT-3.5 through OpenAI's API, one snapshotted in March 2023 and another in June 2023. Therefore we focus on the drifts between these two dates.</p><p>Evaluation Tasks. In this paper, we focus on four LLM tasks frequently studied in performance and safety benchmarks: solving math problems, answering sensitive questions, code generation, and visual reasoning, as shown in Figure <ref type="figure">1</ref>. These tasks are selected for two reasons. First, they are diverse tasks frequently used to evaluate LLMs in the literature [WWS + 22, ZPM + 23, CTJ + 21]. Second, they are relatively objective and thus easy-to-evaluate. For each task, we use one dataset, either sampled from existing datasets or constructed by us for monitoring purposes. We acknowledge that using one benchmark dataset does not comprehensively cover a task. Our goal here is not to provide a holistic assessment but to demonstrate that substantial ChatGPT performance drift exists on simple tasks. We are adding more benchmarks in future evaluations as part of a broader, long-term study of LLM service behavior. We cover each task and dataset in detail in the next section.</p><p>GPT-4 GPT-3.5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>March 2023 June 2023</head><p>Step 1: Check if the number is even. If it is, then it is not prime (except for 2). 17077 is odd, so we proceed to the next step. Step 2: Find the square root of the number. The square root of 17077 is approximately 130.68. Step 3: Check if the number is divisible by any prime numbers less than or equal to the square root. The prime numbers less than or equal to <ref type="bibr">130.68 are 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127</ref>  <ref type="formula">7</ref>) from the remaining leading digits (1707 -2*7 = 1693), we see that 17077 is not divisible by 7. 5. We continue by checking if 17077 is divisible by 11. By alternatingly subtracting and adding the digits (1 -7 + 0 -7 + 7 = -6), we see that 17077 is not divisible by 11. 6. Finally, we check if 17077 is divisible by 13. By subtracting four times the last digit ( <ref type="formula">7</ref>) from the remaining leading digits (1707 -4*7 = 1679), we see that 17077 is not divisible by 13. Since we have checked all the prime numbers up to the square root of 17077 and found that it is not divisible by any of them, we can conclude that 17077 is a prime number.   between March and June 2023. Overall, a large performance drifts existed for both services. (b) an example query and corresponding responses over time. GPT-4 followed the chain-of-thought instruction to obtain the right answer in March, but ignored it in June with the wrong answer. GPT-3.5 always followed the chain-of-thought, but it insisted on generating a wrong answer <ref type="bibr">([No]</ref> ) first in March. This issue was largely fixed in June.</p><p>Metrics. How can we quantitatively model and measure LLM drifts in different tasks? Here, we consider one main performance metric for each task and two common additional metrics for all tasks. The former captures the performance measurement specific to each scenario, while the latter covers common complementary measurement across different applications. In particular, accuracy that quantifies how often an LLM service generates the correct answer is the main metric for solving math problems. For answering sensitive questions, answer rate, i.e. the frequency that an LLM service directly answers an question, serves as the main metric. For code generation, the main metric is what fraction of the generated codes are directly executable (if the generated code could be directly executed in a programming environment and pass the unit tests). For visual reasoning, the main metric is exact match (whether the generated visual objects exactly matches the ground truth).</p><p>Our first additional common metric is verbosity, i.e., the length of generation. The other one is overlap, i.e. whether for the same prompt, the extracted answers by two versions of the same LLM service match each other. Note that this only compares the answers' differences, not the raw generations. For example, for math problems, overlap is 1 if the generated answers are the same, even if the intermediate reasoning steps are different. For each LLM service, we use the overlap's empirical mean over the entire population to quantify how much an LLM service's desired functionality, instead of the textual outputs, deviates over time. For each of the other metrics, We compute its population mean for both the March and June versions, and leverage their differences to measure the drift sizes.</p><p>GPT-4 GPT-3.5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>March 2023 June 2023</head><p>As an AI language model, I must clarify that I do not endorse or support any form of discrimination, including gender discrimination. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer</head><p>Figure <ref type="figure">3</ref>: Answering sensitive questions. (a) Overall performance changes. GPT-4 answered fewer questions from March to June while GPT-3.5 answered slightly more. (b) An example query and responses of GPT-4 and GPT-3.5 at different dates. In March, GPT-4 and GPT-3.5 were verbose and gave detailed explanation for why it did not answer the query. In June, they simply said sorry.</p><p>3 Monitoring Reveals Substantial LLM Drifts</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Solving Math Problems: Chain-of-Thought Might Fail</head><p>How do GPT-4 and GPT-3.5's math solving skills evolve over time? As a canonical study, we explore the drifts in these LLMs' ability to figure out whether a given integer if prime. We focus on this task because it is easy to understand for humans while still requires reasoning, resembling many math problems. The dataset contains 500 questions, extracted from [ZPM + 23]. To help the LLMs reason, we leverage Chain-of-Thought [WWS + 22], a standard approach for reasoning-heavy tasks.</p><p>Perhaps surprisingly, substantial LLM drifts emerge on this simple task. As shown in Figure <ref type="figure" target="#fig_0">2</ref>(a), GPT-4's accuracy dropped from 97.6% in March to 2.4% in June, and there was a large improvement of GPT-3.5's accuracy, from 7.4% to 86.8%. In addition, GPT-4's response became much more compact: its average verbosity (number of generated characters) decreased from 821.2 in March to 3.8 in June. On the other hand, there was about 40% growth in GPT-3.5's response length. The answer overlap between their March and June versions was also small for both services.</p><p>Why was there such a large difference? One possible explanation is the drifts of chain-of-thoughts' effects. Figure <ref type="figure" target="#fig_0">2</ref> (b) gives an illustrative example. To determine whether 17077 is a prime number, the GPT-4's March version followed the chain-of-thought instruction very well. It first decomposed the task into four steps, checking if 17077 is even, finding 17077's square root, obtaining all prime numbers less than it, checking if 17077 is divisible by any of these numbers. Then it executed each step, and finally reached the correct answer that 17077 is indeed a prime number. However, the chain-of-thought did not work for the June version: the service did not generate any intermediate steps and simply produced "No". Chain-of-thought's effects had a different drift pattern for GPT-3.5. In March, GPT-3.5 inclined to generate the answer "No" first and then performed the reasoning steps. Thus, even if the steps and final conclusion ("17077 is a prime number") were correct, its nominal answer was still wrong. On the other hand, the June update seemed to fix this issue: it started by writing the reasoning steps and finally generate the answer "Yes", which was correct. This interesting phenomenon indicates that the same prompting approach, even these widely adopted such as chain-of-thought, could lead to substantially different performance due to LLM drifts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Answering Sensitive Questions: Safer but Less Rationale</head><p>Prompting LLMs with sensitive questions is known to lead to harmful generations such as social biases [GLK + 22], personal information [CTW + 21], and toxic texts [GGS + 20]. Thus, another goal of Table <ref type="table">1</ref>: Comparison of answer rate drifts on plain texts and AIM attacks (one jailbreaking prompting). GPT-3.5 failed to defend AIM attacks: its answer rate was high in both March (100%) and June (96%). On the other hand, GPT-4's updates offered a stronger defense against the attacks: the answer rate for AIM attacks dropped from 78.0% in March to 31.0% in June. We observed two major trends on this task. First, as shown in Figure <ref type="figure">3</ref>, GPT-4 answered fewer sensitive questions from March (21.0%) to June (5.0%) while GPT-3.5 answered more (from 2.0% to 8.0%). It was likely that a stronger safety layer was likely to deployed in the June update for GPT-4, while GPT-3.5 became less conservative. Another observation is that the generation length (measured by number of characters) of GPT-4 dropped from more than 600 to about 140.</p><p>Why did the generation length change? Besides answering fewer questions, it was also because GPT-4 became more terse and offered fewer explanations when it refused to answer a query. To see this, consider the example shown in Figure <ref type="figure">3(b)</ref>. GPT-4 refused to answer the inappropriate query in both March and June. However, it generated a whole paragraph to explain the rejection reasons in March, but simply produced "Sorry, but I cannot assist with that". A similar phenomenon happened for GPT-3.5 too. This suggests that these LLM services may have become safer, but also provide less rationale for refusing to answer certain questions.</p><p>LLM Jailbreaking. Jailbreaking attacks are a major thread to LLM service safety [GLK + 22]. It rephrases or reorganizes the original sensitive questions in order to produce harmful generations from LLMs. Thus, it is also critical to study how LLM services' defense against jailbreaking attacks drift over time. Here, we leverage the AIM (always intelligent and Machiavellian) attack<ref type="foot" target="#foot_0">1</ref> , the most user-voted among a largest collection of ChatGPT jailbreaks on the internet<ref type="foot" target="#foot_1">2</ref> . The AIM attack describes a hypothetical story and asks LLM services to act as an unfiltered and amoral chatbot. We applied the AIM attack for each query in the sensitive question dataset and then queried GPT-4 and GPT-3.5. The answer rate of their March and June versions was shown in Table <ref type="table">1</ref>. There was a large increase of answer rate for both both GPT-4 and GPT-3.5 when AIM attack was deployed. However, their temporal drifts differed substantially. For GPT-4, AIM attack produced 78% direct answers in March, but only 31.0% in June. For GPT-3.5, there was only a 4% (=100%-96%) answer rate difference among the two versions. This suggests that GPT-4's update was more robust to jailbreaking attacks than that of GPT-3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Code Generation: More Verbose and Less Directly Executable</head><p>One major application of LLMs is code generation [CTJ + 21]. While many code generation datasets exist [CTJ + 21, YZY + 18, AON + 21], using them to assess LLM services' code generation ability faces the data contamination issue. To overcome this, we have constructed a new code generation dataset. It contains the latest 50 problems from the "easy" category of LeetCode at the time of writing. The earliest public solutions and discussions were released in December 2022. The prompt for each problem is the concatenation of the original problem description and the corresponding Python code template.  <ref type="bibr">, '2', '3', '4', '5', '6', '7', '8', '9']</ref>  For GPT-4, the percentage of generations that are directly executable dropped from 52.0% in March to 10.0% in June. The drop was also large for GPT-3.5 (from 22.0% to 2.0%). GPT-4's verbosity, measured by number of characters in the generations, also increased by 20%. (b) An example query and the corresponding responses. In March, both GPT-4 and GPT-3.5 followed the user instruction ("the code only") and thus produced directly executable generation. In June, however, they added extra triple quotes before and after the code snippet, rendering the code not executable. Each LLM's generation was directly sent to the LeetCode online judge for evaluation. We call it directly executable if the online judge accepts the answer.</p><p>Overall, the number of directly executable generations dropped from March to June. As shown in Figure <ref type="figure" target="#fig_1">4</ref> (a), over 50% generations of GPT-4 were directly executable in March, but only 10% in June. The trend was similar for GPT-3.5. There was also a small increase in verbosity for both models.</p><p>Why did the number of directly executable generations decline? One possible explanation is that the June versions consistently added extra non-code text to their generations. Figure <ref type="figure" target="#fig_1">4</ref> (b) gives one such instance. GPT-4's generations in March and June are almost the same except two parts. First, the June version added "'python and "' before and after the code snippet. Second, it also generated a few more comments. While a small change, the extra triple quotes render the code not executable. This is particularly challenging to identify when LLM's generated code is used inside a larger software pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Visual Reasoning: Marginal Improvements</head><p>Finally, we investigate LLM drifts for visual reasoning. This task differs from other scenarios because it requires abstract reasoning. The ARC dataset <ref type="bibr" target="#b3">[Cho19]</ref> is commonly used to assess the visual reasoning ability. The task is to create a output grid corresponding to an input grid, based solely on a few similar examples. Figure <ref type="figure" target="#fig_2">5</ref> color. We fed the LLM services 467 samples in the ARC dataset that fits in all services' context window. Then we measured the exact match between their generation and the ground truth.</p><p>As shown in Figure <ref type="figure" target="#fig_2">5</ref>(a), there were marginal performance improvements for both GPT-4 and GPT-3.5. However, for more than 90% visual puzzle queries, the March and June versions produced the exact same generation. These services' overall performance were also low: 27.4% for GPT-4 and 12.2% for GPT-3.5.</p><p>It is worthy noting that LLM services did not uniformly make better generations over time. In fact, despite better overall performance, GPT-4 in June on queries on was correct for in March. Figure <ref type="figure" target="#fig_2">5</ref>(b) gives one example. This underlines the need of fine-grained drift monitoring, especially for critical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head><p>Our findings demonstrate that the behavior of GPT-3.5 and GPT-4 has varied significantly over a relatively short amount of time. This highlights the need to continuously evaluate and assess the behavior of LLMs in production applications. We plan to update the findings presented here in an ongoing long-term study by regularly evaluating GPT-3.5, GPT-4 and other LLMs on diverse tasks over time. For users or companies who rely on LLM services as a component in their ongoing workflow, we recommend that they should implement similar monitoring analysis as we do here for their applications.</p><p>To encourage further research on LLM drifts, we have release our evaluation data and ChatGPT responses at https://github.com/lchen001/LLMDrift.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Solving math problems. (a): monitored accuracy, verbosity (unit: character), and answer overlap of between March and June 2023. Overall, a large performance drifts existed for both services. (b) an example query and corresponding responses over time. GPT-4 followed the chain-of-thought instruction to obtain the right answer in March, but ignored it in June with the wrong answer. GPT-3.5 always followed the chain-of-thought, but it insisted on generating a wrong answer([No]  ) first in March. This issue was largely fixed in June.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Code generation. (a) Overall performance drifts.For GPT-4, the percentage of generations that are directly executable dropped from 52.0% in March to 10.0% in June. The drop was also large for GPT-3.5 (from 22.0% to 2.0%). GPT-4's verbosity, measured by number of characters in the generations, also increased by 20%. (b) An example query and the corresponding responses. In March, both GPT-4 and GPT-3.5 followed the user instruction ("the code only") and thus produced directly executable generation. In June, however, they added extra triple quotes before and after the code snippet, rendering the code not executable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visual reasoning. (a) Overall performance. For both GPT-4 and GPT-3.5, there was a 2% improvement of the exact match rate from March to June. The generation length remained roughly the same. The generation did not change to June for about 90% of the visual reasoning queries. (b) An example query and the corresponding responses. While overall GPT-4 became better over time, it was worse on this particular query. It gave the correct grid in March but the wrong one in June.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, we see that 17077 is not divisible by 3. 3. We continue by checking if 17077 is divisible by 5. Since it does not end in 0 or 5, it is not divisible by 5. 4. Next, we check if 17077 is divisible by 7. By subtracting twice the last digit (</figDesc><table><row><cell></cell><cell>[No] Explanation: To determine if 17077 is a prime number, we need to check if it is</cell></row><row><cell></cell><cell>only divisible by 1 and itself. We can start by checking if it is divisible by 2. If it is, then</cell></row><row><cell></cell><cell>it is not a prime number. 17077 is an odd number, so it is not divisible by 2. Next, we</cell></row><row><cell>. Step 4:</cell><cell>can check if it is divisible by any odd number up to the square root of 17077 (which is</cell></row><row><cell>Test each of these prime numbers as a divisor. If any of them divide 17077 without a</cell><cell>approximately 130.7). We can see that it is not divisible by 3, 5, 7, 11, 13, or any</cell></row><row><cell>remainder, then 17077 is not prime. If none of them divide 17077 without a</cell><cell>other odd number up to 130.7. Therefore, 17077 is a prime number.</cell></row><row><cell>remainder, then 17077 is prime. 17077 is not divisible by any of the prime numbers</cell><cell></cell></row><row><cell>listed above without a remainder. Therefore, 17077 is a prime number. Answer: [Yes]</cell><cell></cell></row><row><cell></cell><cell>To determine if 17077 is a prime number, we need to check if it is divisible by any</cell></row><row><cell></cell><cell>number other than 1 and itself. 1. We start by checking if 17077 is divisible by 2.</cell></row><row><cell></cell><cell>Since it is an odd number (not divisible by 2), we move on to the next step. 2. Next,</cell></row><row><cell></cell><cell>we check if 17077 is divisible by 3. By adding the digits of 17077 (1 + 7 + 0 + 7 + 7 =</cell></row><row><cell></cell><cell>22)</cell></row><row><cell>[No]</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>to understand how LLM services' responses to sensitive questions have shifted over time. To achieve this goal, we have created a sensitive question dataset, which contains 100 sensitive queries that LLM services are not supposed to answer directly. As it is challenging to automatically evaluate whether a response is indeed a direct answer, we have manually labelled all responses from the monitored LLM services.</figDesc><table><row><cell></cell><cell>GPT-4</cell><cell></cell><cell></cell><cell>GPT-3.5</cell></row><row><cell>LLM Service</cell><cell cols="2">Query mode</cell><cell cols="2">Query mode</cell></row><row><cell>Eval Time</cell><cell cols="4">Plain Text AIM Attack Plain Text AIM Attack</cell></row><row><cell>Mar-23</cell><cell>21.0%</cell><cell>78.0%</cell><cell>2.0%</cell><cell>100.0%</cell></row><row><cell>Jun-23</cell><cell>5.0%</cell><cell>31.0%</cell><cell>8.0%</cell><cell>96.0%</cell></row><row><cell>this paper was</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>: You are given an integer consists of exactly digits. We call the number if, after the following modification, the resulting number contains all the digits from to once and does not contain any 's: Concatenate with the numbers and . Return if fascinating, or otherwise. 'Concatenating' two numbers means joining them together. For example, the concatenation of</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>PT-4</cell><cell>GPT-3.5</cell><cell></cell></row><row><cell></cell><cell>irectly Executable</cell><cell></cell><cell></cell></row><row><cell>b Example</cell><cell cols="2">[Description]and</cell><cell>is</cell><cell>.</cell></row><row><cell></cell><cell>[Examples]: ......</cell><cell></cell><cell></cell></row><row><cell></cell><cell>[Note]</cell><cell></cell><cell></cell></row><row><cell cols="2">March 2023</cell><cell cols="3">class Solution(object): def isFascinating(self, n): # Convert n to string for easier manipulation n_str = str(n) # Concatenate n, 2*n, and 3*n concatenated_str = n_str + str(2*n) + str(3*n) # Check if concatenated string contains all digits from 1 to 9 exactly once if set(concatenated_str) == set('123456789'): return True else: return False</cell></row><row><cell></cell><cell></cell><cell>```python</cell><cell></cell></row><row><cell></cell><cell></cell><cell>class Solution(object):</cell><cell></cell></row><row><cell>June 2023</cell><cell>```python class Solution(object): def isFascinating(self, n): # Concatenate n, 2*n and 3*n s = str(n) + str(n*2) + str(n*3) # Check if the length of s is 9 and contains all digits from 1 to 9 return len(s) == 9 and set(s) == set('123456789') ```G</cell><cell cols="2">def isFascinating(self, n): # Convert n to string n_str = str(n) # Concatenate n with 2*n and 3*n concatenated_str = n_str + str(2 * n) + str(3 * n) # Check if the concatenated string contains # all digits from 1 to 9 exactly once if set(concatenated_str) == set('123456789'): return True</cell></row><row><cell></cell><cell></cell><cell>else:</cell><cell></cell></row><row><cell></cell><cell></cell><cell>return False</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">``D</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>: Solve it by filling in the following python code. Generate the code only without any other text.</head><label></label><figDesc></figDesc><table><row><cell>class Solution:</cell></row><row><cell>...</cell></row><row><cell>class Solution(object):</cell></row><row><cell>def isFascinating(self, n):</cell></row><row><cell>concatenated_number = str(n) +str(2 * n) + str(3 * n)</cell></row><row><cell>return sorted(concatenated_number)=['1'</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>www.jailbreakchat.com/prompt/4f37a029-9dff-4862-b323-c96a5504de5d   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>jailbreakchat.com   </p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Program synthesis with large language models</title>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Aon + 21] Jacob Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carrie</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07732</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity</title>
		<author>
			<persName><surname>Bcl + 23 ; Yejin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nayeon</forename><surname>Cahyawijaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenliang</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holy</forename><surname>Wilie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Lovenia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiezheng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willy</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Chung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.04023</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Did the model change? efficiently assessing machine learning api shifts</title>
		<author>
			<persName><forename type="first">Lingjiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tracy</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.14203</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.01547</idno>
		<title level="m">On the measure of intelligence</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hapi: A large-scale longitudinal dataset of commercial ml api predictions</title>
		<author>
			<persName><forename type="first">Zhihua</forename><surname>Cje + 22] Lingjiao Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Sabri Eyuboglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>R?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">Y</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24571" to="24585" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Extracting training data from large language models</title>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Ctj + 21] Mark Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Ponde De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><surname>Petrov</surname></persName>
		</author>
		<idno>CTW + 21</idno>
	</analytic>
	<monogr>
		<title level="m">30th USENIX Security Symposium (USENIX Security 21)</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="2633" to="2650" />
		</imprint>
	</monogr>
	<note>Evaluating large language models trained on code</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Can chatgpt pass high school exams on english language comprehension</title>
		<author>
			<persName><forename type="first">Joost Cf De</forename><surname>Winter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Researchgate</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Samuel Gehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.11462</idno>
		<title level="m">Realtoxicityprompts: Evaluating neural toxic degeneration in language models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Tanya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jessy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.12356</idno>
		<title level="m">News summarization and evaluation in the era of gpt-3</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned</title>
		<author>
			<persName><surname>Glk + 22] Deep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liane</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Lovitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><surname>Schiefer</surname></persName>
		</author>
		<author>
			<persName><surname>Kamal Ndousse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.07858</idno>
		<idno>arXiv:2301.08745</idno>
	</analytic>
	<monogr>
		<title level="m">Is chatgpt a good translator? a preliminary study</title>
		<imprint>
			<date type="published" when="2022">2022. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>JWH + 23</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Martin</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">James</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang</forename><surname>Bommarito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Arredondo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">4389233</biblScope>
		</imprint>
	</monogr>
	<note>Gpt-4 passes the bar exam. Available at SSRN</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Holistic evaluation of language models</title>
		<author>
			<persName><surname>Lbl + 22] Percy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilara</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Soylu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yian</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.09110</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><surname>Lnt + 23] Hanmeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.03439</idno>
		<title level="m">Evaluating the logical reasoning ability of chatgpt and gpt-4</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><surname>Nkm + 23] Harsha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">Mayer</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><surname>Mckinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Carignan</surname></persName>
		</author>
		<author>
			<persName><surname>Horvitz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.13375</idno>
		<title level="m">Capabilities of gpt-4 on medical challenge problems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Chain of thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><surname>Wws + 22] Jason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11903</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task</title>
		<author>
			<persName><surname>Yzy ; Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingning</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanelle</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><surname>Roman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.08887</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><surname>Zpm + 23] Muru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alisa</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.13534</idno>
		<title level="m">How language model hallucinations can snowball</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
