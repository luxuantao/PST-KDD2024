<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-19">19 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Renqian</forename><surname>Luo</surname></persName>
							<email>renqianluo@microsoft.com</email>
							<idno type="ORCID">0000-0002-9062-3484</idno>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research AI4Science</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liai</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yingce</forename><surname>Xia</surname></persName>
							<email>yinxia@microsoft.com</email>
							<idno type="ORCID">0000-0001-9823-9033</idno>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research AI4Science</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
							<email>taoqin@microsoft.com</email>
							<idno type="ORCID">0000-0002-9095-0776</idno>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research AI4Science</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
							<idno type="ORCID">0000-0003-3672-5436</idno>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
							<idno type="ORCID">0000-0002-9067-0918</idno>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research AI4Science</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">Microsoft Research AI4Science</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-19">19 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2210.10341v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>biomedical literature</term>
					<term>generative pre-trained language model</term>
					<term>text generation</term>
					<term>text mining</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e., BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large scale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms. Code is available at https://github.com/microsoft/BioGPT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Text mining and knowledge discovery from biomedical literature play important roles in drug discovery, clinical therapy, pathology research, etc. Typical tasks include recognizing named entities in the articles, mining the interaction between drugs and proteins/diseases/other drugs, answering questions given reference text, generating abstracts for given phrases/words, etc. People have accumulated large amounts of literature in the previous studies. For example, PubMed 1 , one of the most popular biomedical search engines, covers more than 30M articles and the number still rapidly increases every day as new discoveries are continuously coming out. Therefore, automatically mining the knowledge from literature becomes an urgent demand.</p><p>Pre-training models have demonstrated their powerful capability in natural language processing (NLP). On the GLUE benchmark, a widely used benchmark for natural language understanding, pre-training based methods outperform nonpre-training methods by a large margin <ref type="bibr" target="#b0">[1]</ref> 2 . There are two main kinds of pre-training models: (1) the BERT-like models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>, mainly for language understanding tasks; (2) the GPT-like models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>, mainly for language generation tasks.</p><p>These models are first pre-trained on large scale corpora collected from the Web via self-supervised learning task (e.g., masked language modeling for BERT, auto-regressive language modeling for GPT), and then fine-tuned on specific donwstream tasks. The BERT-like models are widely used in sequence classification and sequence labeling, where we need to encode 1 https://pubmed.ncbi.nlm.nih.gov 2 https://gluebenchmark.com/leaderboard the complete document. In comparison, the GPT-like models are often used in generation tasks (e.g., abstract generation, knowledge triplet generation).</p><p>By witnessing the success of pre-training in general NLP, people explore adapting these techniques into biomedical domain. However, directly applying these models to the biomedical domain leads to unsatisfactory performance due to domain shift <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. A natural solution is to develop pre-training models on biomedical texts (e.g., PubMed). BioBERT <ref type="bibr" target="#b9">[10]</ref> and PubMedBERT <ref type="bibr" target="#b8">[9]</ref>) are two representative BERT-like models pre-trained on biomedical domain, and they obtain superior performances than general pre-trained models on biomedical benchmarks. However, previous works mainly focus on BERT models which are more appropriate for understanding tasks, not generation tasks. In comparison, GPT models have demonstrated their abilities on generation tasks but demonstrate inferior performance when directly applying to the biomedical domain <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>In this work, we propose BioGPT, a domain-specific generative pre-trained Transformer language model for biomedical text generation and mining. BioGPT follows the Transformer language model backbone, and is pre-trained on 15M PubMed abstracts from scratch. We apply BioGPT to six biomedical NLP tasks: end-to-end relation extraction on BC5CDR <ref type="bibr" target="#b12">[13]</ref>, KD-DTI <ref type="bibr" target="#b13">[14]</ref> and DDI <ref type="bibr" target="#b14">[15]</ref>, question answering on PubMedQA <ref type="bibr" target="#b15">[16]</ref>, document classification on HoC <ref type="bibr" target="#b16">[17]</ref>, and text generation. To adapt to the downstream tasks, we carefully design and analyze the target sequence format and the prompt for better modeling the tasks. Experiments demonstrate that BioGPT achieves better performance compared to baseline methods and other well-performing methods across all the tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work Pre-trained Language Models</head><p>It has proven to be a very successful pattern in deep learning to pre-train models on large scale unlabeled data via careful designed self-supervision tasks and then transfer to downstream tasks by fine-tuning on them. Downstream tasks can benefit from the learned representations from the pre-trained models. BERT <ref type="bibr" target="#b1">[2]</ref> is a bidirectional transformer based contextualized language model pre-trained on large scale text corpus English Wikipedia and BooksCorpus. It is pretrained via carefully designed self-supervision tasks: masked language modeling (MLM) task where random word tokens of the input text are replaced by a special token <ref type="bibr">[MASK]</ref> which is to be predicted by the model from the context, and the next sentence prediction (NSP) task where two sentences are to be predicted whether the second one is probable given the first one. The pre-trained BERT provides contextualized word representations that can be used by downstream tasks by just fine-tuning on the tasks and has achieved great success on various natural language understanding tasks. Subsequent works mainly focus on pre-training on larger-scale data and models <ref type="bibr" target="#b2">[3]</ref> and advanced pre-training task <ref type="bibr" target="#b3">[4]</ref>. Though BERT and various biomedical BERT models have been successful in language understanding tasks and classification tasks, few efforts have been devoted to generative models. As BERT learns powerful word representations through the Transformer encoder model architecture in a bi-directional way, it limits its ability of generation.</p><p>Generative Pre-trained Transformer (GPT) <ref type="bibr" target="#b4">[5]</ref> is proposed for language generation tasks via pre-training Transformer decoder model on large scale text corpus in a classical casual language modeling task where model learns to predict the next word token only dependent on the previous word tokens. Further, GPT-2 <ref type="bibr" target="#b5">[6]</ref> and GPT-3 <ref type="bibr" target="#b6">[7]</ref> with larger model size pre-trained on larger scale text corpus are proposed with remarkable performance in various downstream tasks (e.g., translation, summarization) including classification tasks (e.g., reading comprehension) even without fine-tuning (zero-shot) via appropriate prompts design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-trained Language Models in Biomedical Domain</head><p>When applying to specific domain (e.g., biomedicine), BERT models pre-trained on general domain can be further improved if pre-trained on in-domain text data <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b9">10]</ref>. Specifically, <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b7">[8]</ref> start from the original pre-trained BERT model <ref type="bibr" target="#b1">[2]</ref> that are pre-trained on general domain (Wikipedia and BooksCorpus) and continue pre-training on biomedical literature. Specifically, <ref type="bibr" target="#b9">[10]</ref> continue pre-training using PubMed abstracts and PubMed Central full text articles and <ref type="bibr" target="#b7">[8]</ref> continue pre-training on both PubMed text and clinical notes from MIMIC-III <ref type="bibr" target="#b18">[19]</ref>. As they are initialized from the original BERT that are pre-trained on general domain, they use the same vocabulary as the original BERT, which is quite different from the target biomedical domain. Instead of continue pre-training from the pre-trained BERT model, <ref type="bibr" target="#b17">[18]</ref> pre-train the BERT model from scratch on large corpus of scientific literature (mainly biomedical and computer science literature) where the vocabulary is more suitable for science domain but still contains out-domain information for biomedicine. <ref type="bibr" target="#b8">[9]</ref> propose that it is a better strategy to pre-train on domain-specific data from scratch where the vocabulary is more suitable for the biomedical domain. Consequently, they propose PubMedBERT which is pre-trained on 14M PubMed abstracts from scratch. Similarly, <ref type="bibr" target="#b19">[20]</ref> pre-train on 28M data as in <ref type="bibr" target="#b7">[8]</ref> also from scratch, using the more advanced ELECTRA model. All these works have shown improved performance on plenty of biomedical literature language processing tasks compared to the original BERT pretrained on general domain, while none of them is for biomedical generation tasks.</p><p>Noticing the powerful generation ability of GPT models, it is quite curious how GPT models perform on biomedical domain which is very different from general domain. However, recent works show that GPT models, even much more powerful GPT-3 model, perform poorly on biomedical tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. A previous work on pre-training GPT on biomedical literature is DARE <ref type="bibr" target="#b20">[21]</ref>. However, they pre-train GPT on very limited amount of data (only 0.5M PubMed abstracts) and use it only for data-augmentation for relation extraction task. A recent work on using GPT model is <ref type="bibr" target="#b21">[22]</ref>, where they design converters for GPT-3 <ref type="bibr" target="#b6">[7]</ref> for several unconventional downstream clinical tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Downstream Tasks</head><p>In this subsection, we introduce the downstream tasks we will work on. A summary of those tasks is in Table <ref type="table" target="#tab_0">1</ref>. All these tasks can be formulated as text generation / mining tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Extraction</head><p>Relation extraction is a key task for biomedicine and life science research. Classical pipeline-based methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b22">23]</ref> resolve the task into several separate sub-tasks that require additional intermediate annotations and information which may suffer from the lack of intermediate annotated data and error accumulation. Joint extraction aims to jointly extract the entities and the relations between them from the text. Sequence labeling methods tackle the task by labeling the word tokens in the text with different tags to mark out all the entity mentions and then perform the relation classification between them via classifiers <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>. Table filling methods formulate the task as a table constituted by the Cartesian product of itself and predicts the relations between the token pairs <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref>. These methods may suffer from error accumulation caused by previous tagging process and laborious intermediate annotations (i.e., named entity recognition). Text generation methods reframe the task as a sequence-to-sequence learning task, by taking the text as the input sequence and the triplet as the target sequence and employing an encoder-decoder network to learn to generate the triplet from the text <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. However, many joint extraction methods still require additional entity information <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b43">44]</ref>. In this work, we focus on the end-toend relation extraction, which formulates the task as an text generation task that takes only the text as the input and generates the relational triplets in an end-to-end way without additional intermediate annotations <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Answering</head><p>Question answering (QA) is the task of answering questions given a context (reading comprehension). Typical methods predict a span in the source context as the answer text, or predicts a label (e.g., yes or no) for simpler tasks with predefined categorical answers <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b26">27]</ref>. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> mainly focus on the biomedical domain question answering task via pre-trained language models. Generative models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> directly generate the answer sequence or the label words. Relation Extraction GLRE <ref type="bibr" target="#b22">[23]</ref>, REBEL <ref type="bibr" target="#b23">[24]</ref>, seq2rel <ref type="bibr" target="#b24">[25]</ref> KD-DTI <ref type="bibr" target="#b13">[14]</ref>, BC5CDR <ref type="bibr" target="#b12">[13]</ref>, DDI <ref type="bibr" target="#b14">[15]</ref> Question Answering QA-Net <ref type="bibr" target="#b25">[26]</ref>, LUKE <ref type="bibr" target="#b26">[27]</ref>, BERT <ref type="bibr" target="#b1">[2]</ref>, PubMedBERT <ref type="bibr" target="#b8">[9]</ref>, BioELECTRA <ref type="bibr" target="#b27">[28]</ref>, LinkBERT <ref type="bibr" target="#b28">[29]</ref> PubMedQA <ref type="bibr" target="#b15">[16]</ref>, BioASQ <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> Document Classification BERT <ref type="bibr" target="#b1">[2]</ref>, BlueBERT <ref type="bibr" target="#b7">[8]</ref>, SciBERT <ref type="bibr" target="#b17">[18]</ref>, SPECTER <ref type="bibr" target="#b31">[32]</ref>, PubMedBERT <ref type="bibr" target="#b8">[9]</ref>, BioELECTRA <ref type="bibr" target="#b27">[28]</ref>, LinkBERT <ref type="bibr" target="#b28">[29]</ref> HoC <ref type="bibr" target="#b16">[17]</ref>, SciDocs <ref type="bibr" target="#b31">[32]</ref> Document Classification Document classification is to classify a document into predefined categories (single label or multi label). Recent works on biomedical document classification also leverage large pre-trained language models for understanding the text and predicting the label <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. Generative models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> generate the label words instead of predicting from the predefined set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training Method</head><p>In this section, we describe our BioGPT from the perspective of dataset, vocabulary, and model. Dataset: Dataset is crucial for language model pre-training, in terms of amount, quality and domain. As Gu et al. <ref type="bibr" target="#b8">[9]</ref> point, training only on in-domain data from scratch is important for specific domain. Therefore, we only consider in-domain text data and pre-train our model from scratch on the collected data. We collected all the PubMed items<ref type="foot" target="#foot_0">3</ref> that were updated before 2021 from the official site<ref type="foot" target="#foot_1">4</ref> using the wget tool. We then filtered out all the empty items with only title but no abstract. We used the left 15M items (each with both title and abstract) as our pre-training dataset.</p><p>Vocabulary: <ref type="bibr" target="#b8">[9]</ref> also points that in-domain vocabulary is vital. Instead of using the vocabulary of GPT-2, we learn the vocabulary on our collected in-domain corpus. Specifically, we use byte pair encoding (BPE) <ref type="bibr" target="#b45">[46]</ref> to segment the words in the corpus into word pieces and learn the vocabulary. We adopt the fastBPE<ref type="foot" target="#foot_2">5</ref> implementation of BPE. The final learned vocabulary size is 42384. Model: We adopt the GPT-2 model architecture <ref type="bibr" target="#b5">[6]</ref> as the backbone of our BioGPT, which is a Transformer decoder <ref type="bibr" target="#b46">[47]</ref>.</p><p>Currently we cannot follow the GPT-3 setting due to its extremely large model with 15 billion parameters. The core component of Transformer as well as our BioGPT is the multihead attention. Given the input, three linear transformations are applied to produce the query Q, the key K and the value V , and then the output is calculated as follows:</p><formula xml:id="formula_0">Multihead(Q, K, V ) = Concat(head 1 , head 2 , ? ? ? , head h )W, head i = softmax Q i K T i ? d V i ,<label>(1)</label></formula><p>where (1) h is the number of heads; (2) Q, K and V are equally split into Q i , K i and V i along the feature dimension, i ? {1, 2, ? ? ? , h}; (3) Concat denotes concatenating all inputs as a large tensor along the feature dimension; (4) W is the parameter for the affine transformation. The output of multi-head attention layer is then fed into a feed-forward layer to construct a Transformer layer (or Transformer block). Practically, we adopt GPT-2 medium as the backbone network which has 24 layers, 1024 hidden size and 16 attention heads resulting in 355M parameters in total, and our BioGPT has 347M parameters (the difference only comes from the different embedding size and output projection size caused by the different vocabulary size).</p><p>Training criteria: BioGPT is trained via the standard language modeling task as the same as in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. Let D = {x i } i denote the collection of sequences, and sequence x i is made up of n i tokens, i.e.,</p><formula xml:id="formula_1">x i = (s 1 , s 2 , ? ? ? , s n i ).</formula><p>The training objective is to minimize the negative log-likelihood:</p><formula xml:id="formula_2">min - 1 |D| |D| i=1 n i j=1 log P (s j |s j-1 , s j-2 , ? ? ? , s 1 ).<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-tuning Method</head><p>In this section, we introduce how to adapt the pre-trained BioGPT to downstream tasks: end-to-end relation extraction, question answering (QA) and document classification. The inputs of the tasks are all sequences, while they have different output formats.</p><p>To use BioGPT for these tasks, we need to convert the labels into sequences. In this way, the downstream task is consistent with the pre-training task in terms of the format.</p><p>Considering that BioGPT is pre-trained on massive natural language corpus, we convert the labels to sequences in natural language rather than the structured format using special tokens explored in other works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. In this way, our reformed labels are semantically smoother than using special tokens. We will show the detailed implementation for each task and empirically verify the effectiveness of our method later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>End-to-end Relation Extraction</head><p>Task description: Given a source sequence x, we need to find all triplets head entity i , tail entity i , relation i N i=1 , that can be inferred from x. N refers to the number of all possible triplets. Examples include extracting the drugtarget-interaction, chemical-disease-relation and drug-druginteraction. Method: We convert the triplets into a simple natural language sequence with the same grammatical structures. We explore three forms in this paper:</p><p>1. the "subject verb object" form (svo), where the entities correspond to the head entity, the relation and the tail entity in the triplet. 2. the "subject is the rel.noun of object" form (is-of), where the "rel.noun" refers to the noun form of the relation.</p><p>3. the " the relation between subject and object is rel.noun" form (rel-is).</p><p>If there are multiple relational triplets for an input document, we sort them according to their order of appearance in the document and use semicolons to concatenated them together.</p><p>Let us use a drug, target, interaction triplet as example. Suppose we would like to extract triplet dextropropoxyphene (drug name), mu-type opioid receptor (target name), inhibitor (relation) from an input document. Then the svo representation is: dextropropoxyphene inhibits mu-type opioid receptor.</p><p>The is-of form is: dextropropoxyphene is the inhibitor of mu-type opioid receptor.</p><p>The rel-is form is:</p><p>the relation between dextropropoxyphene and mu-type opioid receptor is inhibitor.</p><p>The natural sentences can be converted back to triplets using regular expression. Users can also design customized formats depending on tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Answering</head><p>Task description Given a question, a reference context and an answer, the goal is to determine whether the answer to the question can be inferred from the reference context. The label is within the category of yes, no, or maybe. Method: We pre-pend the description word "question:", "context:" and "answer" before the question, the context and the answer respectively and concatenate them together as the source sequence. Then for the target sequence, we generate it using the format "the answer to the question given the context is label". For example: source: question: question text. context: context text.</p><p>answer: answer text. target: the answer to the question given the context is yes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Classification</head><p>Task description Given a document text, the goal is to classify the type of the document. Method: We generate the target sequence using the format "the type of this document is label". For example: the type of this document is genomic instability and mutation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt-based Fine-tuning</head><p>We have formatted the labels to target sequences. The last question is, how do we use the source and the target to fine-tune and inference with BioGPT? A naive way is to concatenate the source and the target sequences together but is difficult for the model to generate during inference as it does not know what to generate for the specific task given the source text input.</p><p>Prompt is recently extensively explored in NLP <ref type="bibr" target="#b47">[48]</ref> to elicit the knowledge from a pre-trained language model. Prompt is to append task-specific instructions to the input for the model to better generate output that meets the demand of the task. GPT-3 <ref type="bibr" target="#b6">[7]</ref> uses hard prompts (manually designed discrete language phrases) to generate for different tasks. Though hard prompts can achieve satisfactory performance, designing task specific prompts is laborious and it is found that different prompts lead to different performance.</p><p>In this work, we mainly adopt soft prompts in prefixtuning <ref type="bibr" target="#b48">[49]</ref>, which leverage continuous embeddings (virtual tokens) to steer the pre-trained language model by directly appending several additional virtual tokens before the text as the prompts. Such continuous embeddings are randomly initialized and learned end-to-end on the downstream tasks to be task-specific. Different from <ref type="bibr" target="#b48">[49]</ref>, we do not append the virtual tokens to the very beginning of the source input, but only before the target sequence (between the source and the target). Equipped with the prompt, our final sequence is constructed as [source; prompt; target], as depicted in Fig. <ref type="figure" target="#fig_0">1</ref>. During the inference, we provide the source text and the prompt as the prefix for the language model to condition on and let the language model to generate the target output as in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we pre-train our BioGPT and evaluate it on the following four biomedical NLP tasks across six datasets: end-to-end relation extraction on BC5CDR <ref type="bibr" target="#b12">[13]</ref>, KD-DTI <ref type="bibr" target="#b13">[14]</ref> and DDI <ref type="bibr" target="#b14">[15]</ref>, question answering on PubMedQA <ref type="bibr" target="#b15">[16]</ref>, document classification on HOC <ref type="bibr" target="#b16">[17]</ref>, and text generation on self-created dataset. We use fairseq <ref type="bibr" target="#b49">[50]</ref> as our code base for implementation. We adopt the GPT-2 medium model configuration as our backbone model configuration. We perform BPE to learn to tokenize the corpus and construct the vocabulary instead of using the learned vocabulary from GPT-2 due to the domain gap between the biomedical domain and the general domain.</p><p>For pre-training, we pre-train BioGPT on 8 NVIDIA V100 GPUs for 200k steps, with 1024 tokens per GPU and 64 accumulated steps (i.e., the final batch size is 1024 ? 8 ? 64 = 524288 tokens). We use Adam <ref type="bibr" target="#b50">[51]</ref> as the optimizer with a peak learning rate of 2?10 -4 and 20000 warm-up steps. The learning rate follows an inverse square root decay schedule after reaching the peak as in <ref type="bibr" target="#b46">[47]</ref>.</p><p>All the fine-tuning experiments are conducted on a single NVIDIA V100 GPU, with a batch size of 1024 tokens and 32 accumulated steps.</p><p>During the inference, we adopt beam search with beam size=5 for the text generation task, and greedy search for all the other tasks.</p><p>We make comparison to general domain GPT-2 for all the experiments. Specifically, we use the GPT-2 medium model from the Hugging face library <ref type="bibr" target="#b51">[52]</ref> <ref type="foot" target="#foot_3">6</ref> which is the backbone network of our BioGPT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>End-to-end Relation Extraction</head><p>Relation extraction is an important task in information extraction. Here we target at the end-to-end relation extraction setting where the model takes the text as the input and directly generates the relational triplets. We mainly compare to REBEL <ref type="bibr" target="#b23">[24]</ref>, a recently proposed end-to-end triplet extraction approach based on sequence-to-sequence model, which employs BART pre-trained model <ref type="bibr" target="#b52">[53]</ref> as the backbone model, and further enhances it by pre-training on additional large relational triplet dataset created from Wikipedia as REBEL pt . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BC5CDR</head><p>BC5CDR is a dataset for chemical-disease-relation extraction task introduced by <ref type="bibr" target="#b12">[13]</ref> which consists of 500/500/500 documents as the training/validation/test set. We fine-tune GPT-2 medium and BioGPT for 100 epochs with a peak learning rate 10 -5 and 100 warm-up steps. We use continuous embeddings with length=9 as prompts and the rel-is target sequence format. Since BC5CDR is a binary relation dataset where the entities are labeled if the relationship exists instead of a specific relation type, we use the pattern "the relation between head entity and tail entity exists" as the target sequence format. We average the checkpoints of the last 5 epochs for evaluation. We mainly measure and compare the micro-F1 score. We compare BioGPT to REBEL and seq2rel <ref type="bibr" target="#b24">[25]</ref> where both methods are end-to-end relation extraction methods based on sequence-to-sequence modeling. We also compare with a pipeline-based extraction method, GLRE <ref type="bibr" target="#b22">[23]</ref> which requires NER (named entity recognition) information as the intermediate annotations in the pipeline. Originally, GLRE uses the ground truth NER information. To make a fair comparison, we experiment with GLRE for two settings: 1) using ground-truth NER information during the training and using open-source NER tool during the inference (i.e., GLRE (gt+pred)) and 2) using open-source NER tool for both the training and the inference (i.e., GLRE (pred+pred)).</p><p>We use the open-source NER tool 7 for the NER tagging. We try our best to run the baseline methods and evaluate them.</p><p>From the results in Table <ref type="table" target="#tab_1">2</ref>, we can see that BioGPT achieves the best result (44.98%) among all the methods, with large improvements. We have several findings: 1) pipelinebased method GLRE significantly drops when using NER tagged by open-source tools instead of ground truth NER. However, this is often the common case in practical situation where the annotations for NER are lacked or expensive to collect. When applying open-source NER tools to some specific domains, errors occur and lead to inferior performance of relation extraction. 2) Compared to REBEL, BioGPT has a large gain with 8.28% improvement. Notice that seq2rel <ref type="bibr" target="#b24">[25]</ref> is trained on both the training set and validation set, while  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KD-DTI</head><p>KD-DTI is dataset for drug-target-interaction introduced by <ref type="bibr" target="#b13">[14]</ref>, consisting of 12k/1k/1.3k documents as the train/validation/test set. We fine-tune GPT-2 medium and BioGPT on the task for 30 epochs using Adam optimizer with a peak learning rate of 10 -5 and 1000 warm-up steps. We use continuous embeddings with length=9 as prompts and the rel-is target sequence format for constructing the target sequence. We average the checkpoints of the last 5 epochs for evaluation. We mainly measure and compare the micro-F1 score and the results are listed in Table <ref type="table" target="#tab_2">3</ref>.</p><p>We compare BioGPT with GPT-2 medium , Transformer + PubMedBERT-attn evaluated in <ref type="bibr" target="#b13">[14]</ref> and REBEL. It can be shown that BioGPT achieves 38.42% f1 score, with 14.23%, 9.97% and 8.03% improvement compared to Transformer + PubMedBERT-attn, GPT-2 medium and REBEL. Particularly,  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Answering</head><p>PubMedQA <ref type="bibr" target="#b15">[16]</ref> is a biomedical question answering dataset. Each sample contains a question, an answer, a reference context from a PubMed abstract and a yes/no/maybe label of whether the answer to the question can be inferred from the reference context. We use the original train/validation/text split with 450, 50 and 500 respectively. We use the continuous embedding with length=9 as the prompt. We format the data into source sequence and target sequence as described before. We fine-tune GPT-2 medium and BioGPT for 100 epochs with a peak learning rate 10 -5 and 100 warm-up steps. We measure and compare the classification accuracy.</p><p>From the results in Table <ref type="table" target="#tab_4">5</ref> we can see that BioGPT achieves 78.2% accuracy with 6.0% improvement over previous best performance obtained by BioLinkBERT <ref type="bibr" target="#b28">[29]</ref>, achieving a new state-of-the-art on this task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Classification</head><p>HoC (the Hallmarks of Cancers corpus) consists of 1580 PubMed abstracts manually annotated at sentence level by experts with ten currently known hallmarks of cancer <ref type="bibr" target="#b16">[17]</ref>.</p><p>We follow the same training/test split as in <ref type="bibr" target="#b7">[8]</ref>. We use the continuous embedding with length=1 as the prompt and format the label into the target sequence as described before. We finetune GPT-2 medium and BioGPT for 20000 steps with a peak learning rate 10 -5 and 1000 warm-up steps. Micro-F1 score is measured and reported for comparison. We can see from the results in Table <ref type="table" target="#tab_5">6</ref> that BioGPT achieves 85.12% accuracy with 3.28% improvement over general domain GPT-2, and surpasses BioBERT, PubMedBERT and BioLinkBERT with 3.58%, 2.8%, 0.77% improvements respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Generation</head><p>GPT, GPT-2 and GPT-3 demonstrate remarkable text generation ability. Given words, phrases or simple sentences as prefix, they can continue to generate text that are syntactically correct and semantically smooth conditioning on the given text. We are also curious about the text generation ability of the pre-trained BioGPT in the biomedical domain, and how does general domain GPT-2 perform in the biomedical domain.</p><p>We evaluate the biomedical text generation ability of BioGPT and GPT-2 medium . Specially, we extract all the entities within the triplets from the KD-DTI test set (i.e., drugs and targets). Then for each drug/target name, we provide it to the language model as the prefix and let the model generate text conditioned on it. We then investigate whether the generated text is meaningful and fluent.</p><p>For this task, no objective evaluation metric is reported here. Instead, we provide a few examples here for demonstration.</p><p>From the results in Table <ref type="table" target="#tab_6">7</ref>, we can see that: (1) Given relatively common names as input, for example in the first two cases (i.e., Bicalutamide and JAK-3), GPT-2 can generate meaningful and fluent text that is related to the word Bicalutamide, a synthetic bicalutamide derivative, was found to increase the cellular proliferation of C. elegans cells. This effect was not observed with the other synthetic bicalutamide derivatives. This is in line with our previous work which showed that bicalutamide increases the proliferation of C. elegans cells but not in the presence of synthetic bicalutamide.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BioGPT</head><p>Bicalutamide (Casodex) is an androgen receptor (AR) antagonist approved for the treatment of metastatic castration-resistant prostate cancer (mCRPC) in patients who have progressed on or are ineligible for docetaxel chemotherapy, as well as for the treatment of early-stage prostate cancer in men who have not progressed on or are ineligible for docetaxel chemotherapy, as well as for the treatment of metastatic castration-sensitive prostate cancer (mCSPC) in men who have not progressed on or are ineligible for docetaxel chemotherapy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Janus kinase 3 (JAK-3) GPT-2</head><p>Janus kinase 3 (JAK-3) mediates the conversion of glucose to glycogen in skeletal muscle. The increase in glucose uptake by skeletal muscle is believed to be associated with a reduction in muscle protein breakdown and with the release of ketone bodies, which contribute to the metabolism of glucose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BioGPT</head><p>Janus kinase 3 (JAK-3) is a member of the Janus kinase (JAK) family of non-receptor tyrosine kinases and plays an important role in the regulation of cell proliferation, differentiation, survival, migration, and angiogenesis.</p><p>Apricitabine GPT-2</p><p>Apricitabine, duloxetine, and citalopram) were investigated in patients with a history of treatmentresistant depression (n = 10). We found that treatment with the three drugs significantly decreased depressive symptoms, but not depressive symptoms over the next 12 weeks. We conclude that antidepressant drugs that reduce depressive symptoms and reduce depressive symptoms over time may be beneficial in patients with recurrent depression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BioGPT</head><p>Apricitabine is an oral prodrug of 5-aza-2 '-deoxycytidine (5-aza-CdR), a DNA methyltransferase (DNMT) inhibitor, which has been approved by the US Food and Drug Administration (FDA) for the treatment of myelodysplastic syndrome (MDS) and acute myeloid leukemia (AML) in combination with low-dose cytarabine (Ara-C) and granulocyte colony-stimulating factor (G-CSF) for patients with intermediate-2 or high-risk MDS or AML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Xylazine GPT-2</head><p>Xylazine, the "active ingredient" in "bath salts" (see above) has been linked to numerous deaths, including those of former University of California, Berkeley, professor Andrew Anglin, who posted a video of himself having a bath filled with it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BioGPT</head><p>Xylazine is an alpha 2-adrenoceptor agonist that has been used as a sedative and analgesic in veterinary medicine for many years, but its effects on the cardiovascular system have not been extensively studied in the dog, and its effects on the central nervous system (CNS) have not been well characterized in the dog, despite the fact that xylazine has been widely used as a sedative and analgesic in veterinary medicine for more than 30 years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Psoralen GPT-2</head><p>Psoralen, the city of the spiritualist, and had been given to him by God, who wished to show him the way of salvation, and to draw him back to the true life of Christ, and to bring him into communion with his fellow-man.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BioGPT</head><p>Psoralen photochemotherapy (PUVA) is a well-established treatment for psoriasis, but its use is limited by the risk of skin cancer, particularly squamous cell carcinoma (SCC) of the head and neck (H &amp; N), which is the most common site of skin cancer in the United States (US) BioGPT BIIB-021 is a novel, orally active, non-peptide bradykinin B2 receptor antagonist with potent and longlasting anti-inflammatory activity in animal models of acute and chronic inflammation and in a rat model of adjuvant-induced arthritis (AIA), an animal model of rheumatoid arthritis (RA), and in a rat model of collagen-induced arthritis (CIA), an animal model of collagen-induced arthritis (CIA), in which arthritis is induced by immunization with bovine type II collagen (CII). </p><note type="other">CP</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT-2</head><p>The drug that can treat COVID-19 is not effective enough to cure patients with this rare genetic disease, but it could help patients who have it. Researchers say this finding could mean that a drug could be developed that would help patients with this rare genetic disorder. The study, published in the journal Molecular Psychiatry, also looked at how a drug might be used to treat patients with COPD, which causes chronic pain and lung damage. Researchers say the study could also lead to new drugs that treat COPD. Omicron variants of SARS-CoV-2 have been detected in the blood of patients who contracted SARS-CoV-2 and survived it, but it is unclear whether these variants were caused by SARS-CoV-2 infection or if the SARS-CoV-2 variant in the blood was due to a misidentification of SARS-CoV-2 from other SARS strains. We found that a polymorphism in the gene encoding the SARS-CoV-2-specific viral surface protein was associated with SARS-CoV-2 infection in a cohort of patients with SARS-CoV-2 infection who had an active SARS infection, suggesting that SARS-CoV-2 may be able to infect the host during an active infection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BioGPT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BioGPT</head><p>Omicron variants of SARS-CoV-2 have been isolated from patients with severe acute respiratory syndrome (SARS) and have been shown to be highly pathogenic in mice and ferrets, suggesting that they may play a role in the pathogenesis of SARS-CoV-2 infection and the development of severe disease in patients with SARS-CoV-2 infection. and biomedicine, while BioGPT generates more specific and professional descriptions. (2) When given some uncommon names (e.g., in the Apricitabine and Xylazine cases), GPT-2 cannot generate meaningful descriptions while BioGPT still generates specific descriptions. Especially in the Apricitabine case, GPT-2 seems to generate a piece of text that comes from a specific scientific paper while BioGPT generates more general description. (3) When given some very uncommon and domain specific names that even lose semantic information from their surface names (e.g., Psoralen, CP-673451 and BIIB-021), GPT-2 trained on general completely failed to generate any informative text. Given Psoralen, GPT-2 treats it as a city name and generates some text though fluent but unrelated to the given name. Given CP-673451, GPT-2 even begins to count numbers. Given BIIB-021, GPT-2 treats it as a name of a pdf document. For these types, BioGPT is still able to generate text that describes the names or is highly related to them.</p><p>Besides these samples, we also manually input several keywords or phrases that are of interest (e.g., COVID-19 related terms) and see what GPT-2 and our BioGPT generate. The results are listed in Table <ref type="table" target="#tab_7">8</ref>, where we input many COVID-19 related key words/phrases as the prefix for the language model to condition on. We can see that GPT-2 treats the term "COVID-19" and "SARS-CoV-2" as some codes within a link </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>In this section, we conduct ablation study on the prompt design and the target sequence format of the label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target Sequence Format</head><p>Previous works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b13">14]</ref> directly format the labels into structured formats using special tokens. Taking the triplet generation task as an example, in REBEL <ref type="bibr" target="#b23">[24]</ref>, the triplets are represented by: &lt;triplet&gt; head entity 1 &lt;subj&gt; tail entity 1 &lt;obj&gt; relation 1 &lt;triplet&gt; head entity 2 &lt;subj&gt; tail entity 2 &lt;obj&gt; relation 2 ? ? ? , where &lt;triplet&gt;, &lt;subj&gt; and &lt;obj&gt; are special tokens to represent the start of the head entity, the tail entity and the relation. <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25]</ref> use similar method to process the targets.</p><p>Although these methods achieved promising results in their tasks respectively, such formulation pattern is not the best choice for BioGPT. Previous works use an encoder-decoder framework, where two separated modules are leveraged to process the input (by the encoder) and generate the answers (by the decoder). The two modules can be trained to fit the two different types of sequences (natural language sequence v.s. structured sequence).</p><p>In contrast, in BioGPT, we use a unified module to encode context and generate answers. Intuitively, it is better to maintain the format consistency between the inputs and answers. Consequently, instead of the structured target format with special tokens as in previous works, we format the label within a natural language sentence for the language model to smoothly learn and generate. However, there are also various patterns that can be used to construct the target sentence.</p><p>8 https://medlineplus.gov/druginfo/meds/a601240.html</p><p>We explore several target sequence formats, including the structured format, on the KD-DTI dataset for end-to-end relation extraction task. We fix the prompt to continuous embeddings with length=9. From the results in Table <ref type="table" target="#tab_8">9</ref> we can see that the formats in natural language perform better than structured format, and that the rel-is format performs the best among all the formats in terms of F1 which provides a more semantically smooth and clear description. We also conduct experiments on BC5CDR and DDI to further compare the structure format and the rel-is format. The F1 scores of the structure format on BC5CDR and DDI are 42.85 and 38.60, while those two scores with rel-is format are 44.98 and 40.76, which further verify our conclusion. We conduct experiment with manually designed hard prompts and continuous embedding soft prompts on the KD-DTI extraction task. We fix the target format to the rel-is format (i.e., "the relation between head entity and tail entity is relation"). From the results in Table <ref type="table" target="#tab_9">10</ref> we can see that the best performing prompt is continuous embeddings with length of 13 virtual tokens. Moreover, we have several observations: (1) Different manually designed hard prompts result in different performance and more instructive and informative prompt (e.g., "we can conclude that") achieve better performance. (2) Generally, continuous embedding soft prompts perform better than manually designed hard prompts. (3) The performance of the continuous embedding soft prompts are roughly irrelevant to the length. In our previous experiments, we empirically choose length=9 according to the performance on validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt Design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this work, we proposed BioGPT, a generative pre-trained Transformer language model for biomedical text generation and mining. We adopted GPT-2 as our backbone model and pre-trained on 15M PubMed abstracts corpus. We carefully designed and investigated the prompt and the target sequence format when applying pre-trained BioGPT to downstream tasks. We applied the pre-trained BioGPT to biomedical NLP tasks: end-to-end relation extraction task, question answering task, document classification task and text generation task. BioGPT achieves SOTA results on three endto-end relation extraction tasks and one question answering task. It also demonstrates better biomedical text generation ability compared to GPT-2 on the text generation task. For future work, we plan to train larger scale BioGPT on larger scale biomedical data and apply to more downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Key Points</head><p>Our contributions are summarized as follows:</p><p>? We propose BioGPT, a generative pre-trained Transformer language model on biomedical domain.</p><p>BioGPT can be used for biomedical literature text generation and mining. ? BioGPT achieves state-of-the-art results on four benchmarks: BC5CDR, KD-DTI and DDI end-to-end relation extraction task, and PubMedQA question answering task. We also demonstrate the capability of biomedical text generation of BioGPT compared to standard GPT trained on general domain. ? We study the prompt design and the target sequence design when applying BioGPT to downstream tasks and find that target sequence with natural language semantics are better than structured prompts explored in previous works.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Framework of BioGPT when adapting to downstream tasks</figDesc><graphic url="image-1.png" coords="5,103.06,56.47,389.35,187.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>7</head><label></label><figDesc>https://huggingface.co/samrawal/bert-base-uncased_ clinical-ner</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>10-11-12-13-14-15-16-17-18-19-20-21-22-23-24-25-26-27-28-29-30-31-32-33-34-35-36-37-38-39-40-41-42-43-44-45-46-47-48-49-50... BioGPT CP-673451 is a potent, selective, and orally active inhibitor of human neutrophil elastase (HNE) and human cathepsin G (CatG) with in vitro and in vivo anti-inflammatory activity in a variety of animal models of inflammation and in a model of acute lung injury (ALI) in the rat induced by intratracheal instillation of lipopolysaccharide (LPS) and tumor necrosis factor-alpha (TNF-alpha), a model of acute lung injury (ALI) in which neutrophils play an important role. -2-1.pdf. Wright, C. (2002). "The War on Terror: A Legal Theory of Terrorist Attacks," in L.E.R. Wright, S.R. Warrick, and J.H. Rothman (Eds.), The War on Terror: Essays in Honor of Michael T. Klare (New York: Oxford University Press), 143-57.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>The drug that can treat COVID-19 is hydroxychloroquine (HCQ), which has been shown to inhibit the replication of the SARS-CoV-2 virus in cell culture and in animal models of COVID-19, and has been approved by the US Food and Drug Administration (FDA) for the treatment of patients with COVID-19 in March 2020, and by the European Medicines Agency (EMA) for the treatment of patients with COVID-19 in April 2020, and by the European Medicines Agency (EMA) for the treatment of patients with COVID-19 in May 2020.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Summary of the downstream tasks</figDesc><table><row><cell>Task</cell><cell>Method</cell><cell>Dataset</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results on BC5CDR chemical-disease-relation extraction task. 'gt+pred' means using ground truth NER information for training and using open-source NER tool to annotate NER for inference. 'pred+pred' means using open-source NER tool for both training and inference. ' ?' means training on training and validation set.</figDesc><table><row><cell>Model</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>GLRE (gt+pred)</cell><cell>34.82</cell><cell>18.29</cell><cell>23.99</cell></row><row><cell>GLRE (pred+pred)</cell><cell>23.00</cell><cell>4.88</cell><cell>8.05</cell></row><row><cell>GPT-2 [6]</cell><cell>43.92</cell><cell>32.55</cell><cell>37.39</cell></row><row><cell>REBEL [24]</cell><cell>34.28</cell><cell>39.49</cell><cell>36.70</cell></row><row><cell>REBELpt [24]</cell><cell>40.94</cell><cell>21.20</cell><cell>27.94</cell></row><row><cell>seq2rel [25]  ?</cell><cell>43.5</cell><cell>37.5</cell><cell>40.2</cell></row><row><cell>BioGPT</cell><cell>49.44</cell><cell cols="2">41.28 44.98</cell></row><row><cell>BioGPT  ?</cell><cell>49.52</cell><cell cols="2">43.25 46.17</cell></row><row><cell cols="4">our BioGPT is only trained on the training set and still</cell></row><row><cell cols="4">outperforms it with 4.78% improvement. Moreover, when also</cell></row><row><cell cols="4">trained on both the training set and the validation set, BioGPT</cell></row><row><cell cols="4">further improves to 46.17% with 5.97% improvement against</cell></row><row><cell>seq2rel [25].</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results on KD-DTI drug-target-interaction extraction task</figDesc><table><row><cell>Model</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>Transformer + PubMedBERT -attn [14]</cell><cell>25.35</cell><cell>24.14</cell><cell>24.19</cell></row><row><cell>GPT-2 medium</cell><cell>30.53</cell><cell>27.87</cell><cell>28.45</cell></row><row><cell>REBEL</cell><cell>32.36</cell><cell>29.58</cell><cell>30.39</cell></row><row><cell>REBELpt</cell><cell>35.73</cell><cell>32.61</cell><cell>33.32</cell></row><row><cell>BioGPT</cell><cell>40.00</cell><cell cols="2">39.72 38.42</cell></row><row><cell cols="4">it surpasses REBEL pt by 5.1% which is further pre-trained on</cell></row><row><cell cols="4">large relation extraction dataset while BioGPT does not.</cell></row><row><cell>DDI</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">DDI extraction 2013 corpus is a dataset for drug-drug-</cell></row><row><cell cols="4">interaction task introduced by [15], consisting of 792</cell></row><row><cell cols="4">texts selected from the DrugBank database and other 233</cell></row><row><cell cols="4">Medline abstracts. We use the original dataset and use a</cell></row><row><cell cols="4">train/validation/test split of 664/50/191 files. We fine-tune</cell></row><row><cell>GPT-2</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>medium and BioGPT for 100 epochs with a peak learning rate 10 -4 and 500 warm-up steps. We also use continuous embeddings with length=9 as prompts and the rel-is target sequence format. The last 5 epochs are averaged for evaluation. The micro-F1 score is measured and compared.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Results on DDI drug-drug-interaction extraction taskThe results are shown in Table4from which we can see that BioGPT achieves 40.76% with 16.08% and 12.49% improvement against GPT-2 medium and REBEL. It also surpasses REBEL pt which uses additional large relation extraction dataset for twostage pre-training.</figDesc><table><row><cell>Model</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>GPT-2 medium</cell><cell>23.39</cell><cell>31.93</cell><cell>24.68</cell></row><row><cell>REBEL</cell><cell>35.36</cell><cell>28.64</cell><cell>28.27</cell></row><row><cell>REBELpt</cell><cell>46.59</cell><cell cols="2">39.60 40.56</cell></row><row><cell>BioGPT</cell><cell>41.70</cell><cell cols="2">44.75 40.76</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Results on PubMedQA question answering task</figDesc><table><row><cell>Model</cell><cell>Accuracy</cell></row><row><cell>PubMedBERT [9]</cell><cell>55.8</cell></row><row><cell>BioELECTRa [28]</cell><cell>64.2</cell></row><row><cell>BioLinkBERT base [29]</cell><cell>70.2</cell></row><row><cell>BioLinkBERT large [29]</cell><cell>72.2</cell></row><row><cell>GPT-2 medium</cell><cell>75.0</cell></row><row><cell>BioGPT</cell><cell>78.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Results on HoC document classification task</figDesc><table><row><cell>Model</cell><cell>F1</cell></row><row><cell>BioBERT [10]</cell><cell>81.54</cell></row><row><cell>PubMedBERT [9]</cell><cell>82.32</cell></row><row><cell cols="2">PubMedBERT large 82.70</cell></row><row><cell>BioLinkBERT base</cell><cell>84.35</cell></row><row><cell>GPT-2 medium</cell><cell>81.84</cell></row><row><cell>BioGPT</cell><cell>85.12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Example of generated text</figDesc><table><row><cell>Input</cell><cell>Model</cell><cell>Text</cell></row><row><cell>Bicalutamide</cell><cell>GPT-2</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Generated text of keywords of interest Hip-hop-romance-detective-franchise-unveiled-in-coco-houston http://www.pinterest.com/pin/71178278499149422/ Black Widow: World War II's Greatest Widow? http://www.pinterest.com/pin/71177842668650112/ BioGPT COVID-19 is a global pandemic caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the causative agent of coronavirus disease 2019 (COVID-19), which has spread to more than 200 countries and territories, including the United States (US), Canada, Australia, New Zealand, the United Kingdom (UK), and the United States of America (USA), as ofMarch 11, 2020, with more than 800,000 confirmed cases and more than 800,000 deaths.The treatment of COVID-19 is similar to that of COVID-20 and COVID-22, in which COVID-19 is treated with a variety of ionic, dopaminergic and serotonergic agents. In addition to its effects on COVID-19, the use of COVID-19 has been suggested to exert effects on the function of other dopamine neurons, including dopamine D2 receptors, D3 receptors and serotonin transporters. These effects include attenuation of the increase in dopamine release induced by the central nervous system stimulant drug amphetamine and its metabolites.BioGPTThe treatment of COVID-19 with remdesivir (remdesivir for COVID-19) has been approved by the US Food and Drug Administration (FDA) for the treatment of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection in patients with confirmed SARS-CoV-2 infection and is currently being evaluated in clinical trials for the treatment of COVID-19 in patients with confirmed SARS-CoV-2 infection, as well as in patients with suspected or confirmed SARS-CoV-2 infection.</figDesc><table><row><cell>Input</cell><cell></cell><cell>Model</cell><cell>Text</cell></row><row><cell></cell><cell></cell><cell></cell><cell>COVID-19-0067-7</cell></row><row><cell cols="2">COVID-19</cell><cell>GPT-2</cell></row><row><cell cols="2">SARS-CoV-2</cell><cell>GPT-2</cell><cell>SARS-CoV-2-CKM-T1-1/pdf/T1/SARS-CoV-2-CKM-T1-1.pdf Cancer http://www.cancer.gov/cancer topics/programs/tox/tox-programs.html Other Research http://www.cdc.gov/cancer/cancer/index.html</cell></row><row><cell></cell><cell></cell><cell>BioGPT</cell><cell>SARS-CoV-2 is the causative agent of COVID-19, a severe acute respiratory syndrome (SARS) that has infected more than 390,000 people worldwide and killed more than 250,000 people.</cell></row><row><cell>The</cell><cell></cell><cell></cell></row><row><cell cols="2">treatment</cell><cell>GPT-2</cell></row><row><cell cols="2">of COVID-19</cell><cell></cell></row><row><cell>The</cell><cell>drug</cell><cell></cell></row><row><cell cols="2">that can treat</cell><cell></cell></row><row><cell cols="2">COVID-19 is</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 .</head><label>9</label><figDesc>Results on KD-DTI with different target formats MedlinePlus8 . Notice that GPT-2 is pre-trained on the corpus before COVID-19 while BioGPT is pre-trained on the corpus before 2021 that contains COVID-19 information, therefore it is not surprising that BioGPT performs much better than GPT-2 on COIVD-19 related key words in Table8. However, in the last example in Table8, both models do not have any knowledge of the Omicron variants of SARS-CoV-2 which appear in the late 2021, while BioGPT still generates more fluent and relevant text compared to GPT-2.Overall, we can see that BioGPT pre-trained on in-domain biomedical literature from scratch performs better than general domain GPT-2 across various biomedical NLP tasks, and performs better than most previous methods on respective tasks, achieving state-of-the-art on four out of six tasks.</figDesc><table><row><cell>Target format</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>&lt;head&gt; head entity &lt;tail&gt; tail entity &lt;relation&gt; relation</cell><cell>38.21</cell><cell cols="2">40.21 37.32</cell></row><row><cell>svo (head entity relation tail entity)</cell><cell>37.95</cell><cell cols="2">37.77 36.57</cell></row><row><cell>is-of (head entity is the relation of tail entity)</cell><cell>39.37</cell><cell cols="2">39.11 37.77</cell></row><row><cell>rel-is (the relation between head entity and tail entity is relation)</cell><cell>38.93</cell><cell cols="2">40.70 38.38</cell></row><row><cell>or file name rather the entities we care about while BioGPT can</cell><cell></cell><cell></cell></row><row><cell>generate clear descriptions. More interestingly, when prompting</cell><cell></cell><cell></cell></row><row><cell>"The drug that can treat COVID-19 is", BioGPT is able to</cell><cell></cell><cell></cell></row><row><cell>answer it with the drug "hydroxychloroquine" which is indeed</cell><cell></cell><cell></cell></row><row><cell>noticed at</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 .</head><label>10</label><figDesc>Results on KD-DTI with different prompts</figDesc><table><row><cell>Prompts</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>we have that</cell><cell>38.55</cell><cell cols="2">38.37 36.95</cell></row><row><cell>in conclusion,</cell><cell>39.03</cell><cell cols="2">39.45 37.76</cell></row><row><cell>we can conclude that</cell><cell>39.56</cell><cell cols="2">39.88 38.16</cell></row><row><cell>continuous embeddings (length=1)</cell><cell>39.50</cell><cell cols="2">39.71 38.06</cell></row><row><cell>continuous embeddings (length=5)</cell><cell>39.57</cell><cell cols="2">39.63 38.09</cell></row><row><cell>continuous embeddings (length=9)</cell><cell>38.93</cell><cell cols="2">40.70 38.38</cell></row><row><cell>continuous embeddings (length=13)</cell><cell>39.48</cell><cell cols="2">39.17 38.60</cell></row><row><cell>continuous embeddings (length=17)</cell><cell>39.82</cell><cell cols="2">39.60 38.28</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>https://pubmed.ncbi.nlm.nih.gov</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>https://ftp.ncbi.nlm.nih.gov/pubmed/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>https://github.com/glample/fastBPE</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>https://huggingface.co/gpt2-medium</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">GLUE: A multitask benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transfer learning in biomedical natural language processing: An evaluation of BERT and ELMo on ten benchmarking datasets</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shankai</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th BioNLP Workshop and Shared Task</title>
		<meeting>the 18th BioNLP Workshop and Shared Task<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-08">August 2019</date>
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain-specific language model pretraining for biomedical natural language processing</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoto</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computing for Healthcare (HEALTH)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">So</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2019-09">09 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Gpt-3 models are poor few-shot learners in the biomedical domain</title>
		<author>
			<persName><forename type="first">Milad</forename><surname>Moradi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathrin</forename><surname>Blagec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Haberl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Samwald</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.02555</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Thinking about gpt-3 in-context learning for biomedical ie? think again</title>
		<author>
			<persName><forename type="first">Jim?nez</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolas</forename><surname>Guti?rrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clay</forename><surname>Mcneal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">You</forename><surname>Washington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.08410</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BioCreative V CDR task corpus: a resource for chemical disease relation extraction</title>
		<author>
			<persName><forename type="first">Jiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueping</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Sciaky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Hsuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><forename type="middle">Peter</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolyn</forename><forename type="middle">J</forename><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">C</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Database</title>
		<imprint>
			<biblScope unit="page" from="5" to="2016" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Yutai</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shufang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.13187</idno>
		<title level="m">Discovering drug-target interaction knowledge from biomedical literature</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The ddi corpus: An annotated corpus with pharmacological substances and drug-drug interactions</title>
		<author>
			<persName><forename type="first">Mar?a</forename><surname>Herrero-Zazo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Segura-Bedmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paloma</forename><surname>Mart?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Declerck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="914" to="920" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pubmedqa: A dataset for biomedical research question answering</title>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghua</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2567" to="2577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic semantic classification of scientific literature according to the hallmarks of cancer</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilona</forename><surname>Silins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imran</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>H?gberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulla</forename><surname>Stenius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="432" to="440" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SciBERT: A pretrained language model for scientific text</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11">November 2019</date>
			<biblScope unit="page" from="3615" to="3620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Wei H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengling</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">G</forename><surname>Anthony Celi</surname></persName>
		</author>
		<author>
			<persName><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mimic-iii, a freely accessible critical care database. Scientific data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Electramed: a new pre-trained language representation model for biomedical nlp</title>
		<author>
			<persName><forename type="first">Giacomo</forename><surname>Miolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giulio</forename><surname>Mantoan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlotta</forename><surname>Orsenigo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09585</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Dare: Data augmented relation extraction with gpt-2</title>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Papanikolaou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Pierleoni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13845</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Large language models are zero-shot clinical information extractors</title>
		<author>
			<persName><forename type="first">Monica</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Hegselmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hunter</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.12689</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Global-to-local neural networks for document-level relation extraction</title>
		<author>
			<persName><forename type="first">Difeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ermei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.10359</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rebel: Relation extraction by end-to-end language generation</title>
		<author>
			<persName><forename type="first">Pere-Llu?s</forename><surname>Huguet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cabot</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2370" to="2381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A sequence-tosequence approach for document-level relation extraction</title>
		<author>
			<persName><forename type="first">John</forename><surname>Giorgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><forename type="middle">D</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.01098</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Qanet: Combining local convolution with global self-attention for reading comprehension</title>
		<author>
			<persName><forename type="first">Adams</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09541</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName><surname>Luke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01057</idno>
		<title level="m">deep contextualized entity representations with entity-aware self-attention</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">BioELECTRA:pretrained biomedical text encoder using discriminators</title>
		<author>
			<persName><forename type="first">Bhuvana</forename><surname>Kamal Raj Kanakarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malaikannan</forename><surname>Kundumani</surname></persName>
		</author>
		<author>
			<persName><surname>Sankarasubbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Workshop on Biomedical Language Processing</title>
		<meeting>the 20th Workshop on Biomedical Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-06">June 2021</date>
			<biblScope unit="page" from="143" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Linkbert: Pretraining language models with document links</title>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8003" to="8016" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An overview of the bioasq large-scale biomedical semantic indexing and question answering competition</title>
		<author>
			<persName><forename type="first">George</forename><surname>Tsatsaronis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Balikas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prodromos</forename><surname>Malakasiotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Partalas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Zschunke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Michael R Alvers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergios</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName><surname>Polychronopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Results of the seventh edition of the bioasq challenge</title>
		<author>
			<persName><forename type="first">Anastasios</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Bougiatiotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Paliouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="553" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07180</idno>
		<title level="m">Specter: Document-level representation learning using citation-informed transformers</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th international conference on computational linguistics: technical papers</title>
		<meeting>COLING 2014, the 25th international conference on computational linguistics: technical papers</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention-based bidirectional long short-term memory networks for relation classification</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-08">August 2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="212" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint type inference on entities and relations via graph convolutional networks</title>
		<author>
			<persName><forename type="first">Changzhi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiliang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1361" to="1370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A relation-specific attention network for joint entity and relation extraction</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiannan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeliang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="4054" to="4060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention as relation: learning supervised multi-head self-attention for relation extraction</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaowei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingquan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Na</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3787" to="3793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A novel cascade binary tagging framework for relational triple extraction</title>
		<author>
			<persName><forename type="first">Zhepei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1476" to="1488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graphrel: Modeling text as relational graphs for joint entity and relation extraction</title>
		<author>
			<persName><forename type="first">Tsu-Jui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng-Hsuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Yun</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1409" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Single-stage joint extraction of entities and relations through token pair linking</title>
		<author>
			<persName><forename type="first">Yucheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Tplinker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12">December 2020</date>
			<biblScope unit="page" from="1572" to="1582" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A partition filter network for joint entity and relation extraction</title>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">November 2021</date>
			<biblScope unit="page" from="185" to="197" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Extracting relational facts by an end-toend neural model with copy mechanism</title>
		<author>
			<persName><forename type="first">Xiangrong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="506" to="514" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Minimize exposure bias of seq2seq models in joint entity and relation extraction</title>
		<author>
			<persName><forename type="first">Qianying</forename><surname>Ranran Haoran Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aysa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Xuemo Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisuke</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadao</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="236" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Joint entity and relation extraction with set prediction networks</title>
		<author>
			<persName><forename type="first">Dianbo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangrong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengping</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.01675</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Reinforced mnemonic reader for machine reading comprehension</title>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02798</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.13586</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08">August 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4582" to="4597" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<publisher>Demonstrations</publisher>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-10">October 2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">T</forename><surname>Long N Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Anibal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaurya</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><surname>Chanana</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03598</idno>
		<title level="m">Erol Bahadroglu, Alec Peltekian, and Gr?goire Altan-Bonnet. Scifive: a text-to-text transformer model for biomedical literature</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
