<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Metric Index: An efficient and scalable solution for precise and approximate similarity search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2010-10-28">28 October 2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">David</forename><surname>Novak</surname></persName>
							<email>david.novak@fi.muni.czd.novak</email>
							<affiliation key="aff0">
								<orgName type="institution">Masaryk University</orgName>
								<address>
									<settlement>Brno</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michal</forename><surname>Batko</surname></persName>
							<email>batko@fi.muni.cz</email>
							<affiliation key="aff0">
								<orgName type="institution">Masaryk University</orgName>
								<address>
									<settlement>Brno</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pavel</forename><surname>Zezula</surname></persName>
							<email>zezula@fi.muni.cz</email>
							<affiliation key="aff0">
								<orgName type="institution">Masaryk University</orgName>
								<address>
									<settlement>Brno</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Metric Index: An efficient and scalable solution for precise and approximate similarity search</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2010-10-28">28 October 2010</date>
						</imprint>
					</monogr>
					<idno type="MD5">A41214250CFBC01510305738218F9108</idno>
					<idno type="DOI">10.1016/j.is.2010.10.002</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Metric space Similarity search Data structure Approximation Scalability</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Metric space is a universal and versatile model of similarity that can be applied in various areas of information retrieval. However, a general, efficient, and scalable solution for metric data management is still a resisting research challenge. We introduce a novel indexing and searching mechanism called Metric Index (M-Index) that employs practically all known principles of metric space partitioning, pruning, and filtering, thus reaching high search performance while having constant building costs per object. The heart of the M-Index is a general mapping mechanism that enables to actually store the data in established structures such as the B + -tree or even in a distributed storage. We implemented the M-Index with the B + -tree and performed experiments on two datasets-the first is an artificial set of vectors and the other is a real-life dataset composed of a combination of five MPEG-7 visual descriptors extracted from a database of up to several million digital images. The experiments put several M-Index variants under test and compare them with established techniques for both precise and approximate similarity search. The trials show that the M-Index outperforms the others in terms of efficiency of search-space pruning, I/O costs, and response times for precise similarity queries. Further, the M-Index demonstrates excellent ability to keep similar data close in the index which makes its approximation algorithm very efficient-maintaining practically constant response times while preserving a very high recall as the dataset grows and even beating approaches designed purely for approximate search.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>There are many indexing techniques that focus on processing textual or vector data. These techniques are not always sufficient for current digital data types or their efficiency is significantly reduced, for example because of the phenomenon referred to as the curse of dimensionality <ref type="bibr" target="#b0">[1]</ref>. Metric space, as a very general data abstraction, allows to grasp a wider variety of these data types. However, after more than a decade of research, efficiency and scalability of metric access methods is still an issue.</p><p>In this paper, we introduce a novel index structure for metric data which builds upon a long-term research in this area. Metric Index (M-Index) defines a universal mapping schema from a generic metric space to a numeric domain. This schema has the ability to preserve proximity of data, i.e. it maps similar metric objects to close numbers in the numeric domain. The M-Index indexing and searching mechanisms use a set of reference objects and synergically exploit practically all known metric-based principles of data partitioning, pruning and filtering. At the same time, having a fixed set of reference objects, M-Index has fixed building costs in terms of number of metric-function evaluations during the insertion of a data object.</p><p>The mapping nature of the M-Index separates its principles from the specific storage structure and thus enables to use well-established techniques such as the B + -tree to efficiently manage and search the actual data. Moreover, there is a straightforward way to build a distributed version of the index by storing the data in a convenient distributed structure and evaluate queries in parallel. Besides precise similarity search algorithm, we also propose an approximate search strategy to tune the query response time versus the answer quality.</p><p>We implemented the M-Index with the B + -tree data storage and used it to prove our concept by numerous experiments, mainly on a complex real-life dataset-a collection of several million images taken from the photo-sharing site Flickr. These images are compared by a combination of five MPEG-7 visual descriptors <ref type="bibr" target="#b1">[2]</ref> extracted from each image-this collection of descriptors is part of the CoPhIR dataset <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. The results indicate that the M-Index outperforms current best metric access methods for precise similarity search. Experiments with M-Index approximate search show that it saves a significant portion of the search costs while preserving almost precise recall. Moreover, this strategy has nearly constant scalability in terms of costs/recall tradeoff and it outperforms techniques designed purely for approximate search.</p><p>This work is an extension of a previously published conference paper <ref type="bibr" target="#b5">[6]</ref>-the main enhancements are: deeper description of the M-Index structure implementation and all its search algorithms, experiments on a different dataset, and additional comparative evaluation of the M-Index approximate search. Section 2 of this paper introduces the basics of metric-based searching. In Section 3, we thoroughly describe the M-Index-its core ideas, structure, and search algorithms. Section 4 is dedicated to experiments that test the efficiency and scalability of various M-Index settings and variants. Our technique is compared to related work in Section 5 and the paper concludes in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries: metric searching</head><p>Metric space M is a pair M ¼ ðD,dÞ, where D is a domain of objects and d is a total distance function d : D Â DÀ!R satisfying these postulates for all o,p,q 2 D: dðo,pÞ Z0 ðnon À negativityÞ, dðo,pÞ ¼ 0 iff o ¼ p ðidentityÞ, dðo,pÞ ¼ dðp,oÞ ðsymmetryÞ, dðo,qÞ r dðo,pÞþdðp,qÞ ðtriangle inequalityÞ: Throughout this paper, we assume that the distance function d is normalized: d : D Â DÀ!½0,1Þ. This can be achieved by dividing the original distances by a constant greater than the maximal value of d expected in the domain. Such constant can always be determined in practice; the transformed space retains properties of the original one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Similarity queries</head><p>The metric space, as a model of similarity, is typically searched according to the query-by-example paradigm-the query is formed by an object q 2 D and some constraint on the data to be retrieved from the indexed dataset X DD. We focus on two basic types of these queries: <ref type="bibr" target="#b0">(1)</ref> the range query R(q, r), which retrieves all objects o 2 X within the range r from q (i.e. fo 2 Xj dðq,oÞ r rg) and (2) the nearest-neighbors query kNN(q,k), which returns k objects from X with the smallest distances to q (ties are broken arbitrarily).</p><p>Evaluating similarity queries in a precise manner may be costly in terms of both I/O and CPU costs. The approximate strategy with a certain level of inaccuracy may be tolerable for many applications. Quality of approximation can be measured by the recall, i.e. the percentage of the precise query result that is returned by the approximate search. See recent monographs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> for more details about metric indexing and searching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Metric Index</head><p>In this section, we introduce principles, architecture and search algorithms of the M-Index. We uncover the concepts in three steps that follow its natural development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">M-Index level one</head><p>The initial concept of the M-Index was inspired by iDistance <ref type="bibr" target="#b8">[9]</ref>, which is an index method for similarity search in vector spaces. Having a sample set S D D, the iDistance partitions S into n clusters and establishes a reference point p i for each cluster C i , i 2 f0, . . . ,nÀ1g. Every object o 2 X is then assigned a numeric key according to the distance from its cluster's reference object. Having a largeenough constant c to separate individual clusters, the iDistance key for an object o 2 C i is</p><formula xml:id="formula_0">iDistðoÞ ¼ dðp i ,oÞþi Á c:</formula><p>This formula maps all objects in any cluster C i to interval ½i Á c,ðiþ 1Þ Á cÞFsee Fig. <ref type="figure" target="#fig_1">1a</ref>. Data objects are then stored in a B + -tree according to their iDist keys. During a search, the iDistance data space is pruned by the principle illustrated in Fig. <ref type="figure" target="#fig_1">1b</ref>-for a range query R(q,r), we can determine several intervals of the iDist keys which need to be accessed in order to process the query. Further details can be found in relevant literature <ref type="bibr" target="#b8">[9]</ref>.</p><p>The M-Index generalizes the idea of iDistance so that it can be used for general metric spaces (not restricting it to vector spaces). This is done by selecting a set of n pivots p 0 , p 1 ,y,p nÀ 1 a priori (from a sample dataset S DX) and then applying the Voronoi-like partitioning to divide the space into n partitions; we will continue using the term clusters. Fig. <ref type="figure">2</ref> (left) shows an example of this partitioning in 2D for four pivots (n=4). The mapping schema follows the idea of iDistance; since we assume normalized metric space (see Section 2), separation constant c is not necessary (c=1)-see Fig. <ref type="figure">2</ref> (right). The figure also demonstrates the M-Index search space pruning principle which is improved in comparison with the iDistance. In the case of the iDistance, the range query in the figure would access a significant part of cluster C 2 , but in the M-Index, we can skip this cluster due to the Voronoi partitioning and application of the double-pivot distance constraint <ref type="bibr" target="#b6">[7]</ref>. Details on the M-Index search algorithm are provided further in Section 3.4.</p><formula xml:id="formula_1">q r 0 c p 2 p 1 2*c 3*c C 2 C 1 p 0 C 0 2*c 3*c 0 c C 0 C 2 C 1 p 2 p 1 p 0 (iDistance)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-level M-Index</head><p>In order to make the M-Index scalable for growing volumes of data, we would like to apply further partitioning of the clusters. Before describing the principles of the multi-level M-Index, let us introduce a preliminary definition. Having a fixed set of n pivots {p 0 , p 1 ,y,p nÀ 1 } and an object o 2 D, let ðÁÞ o : f0,1, . . . ,nÀ1gÀ!f0,1, . . . ,nÀ1g be permutation such that 8i,j 2 f0, . . In other words, sequence p ð0Þ o ,p ð1Þ o , . . . ,p ðnÀ1Þ o is ordered with respect to distances between the pivots and object o (this is also referred to as distance permutation <ref type="bibr" target="#b9">[10]</ref> or pivot permutation <ref type="bibr" target="#b10">[11]</ref>).</p><p>The M-Index with l levels, where l is an integer 1 rl rn, partitions space D into n Á ðnÀ1Þ Á Á Á Á Á ðnÀl þ1Þ partitions (clusters) using the following recursive Voronoi partitioning: On the first level, each object o 2 D is assigned to its closest pivot p i -clusters C i are formed in this way (in other words, ð0Þ o ¼ i for any object o 2 C i ). On the second level, each cluster C i is partitioned into n À 1 clusters by the same procedure using set of n À 1 pivots fp 1 , . . . ,p iÀ1 ,p i þ 1 , . . . ,p n g creating clusters C i,j , where j is the index of the second closest pivot to objects o 2 C i,j , i.e. ð1Þ o ¼ j. This partitioning process is repeated l-times. Fig. <ref type="figure" target="#fig_2">3</ref> (left) shows an example of the M-Index partitioning for l=2.</p><p>Following the pattern sketched in the previous section, the M-Index defines a mapping key l : DÀ!R. Let o 2 D belong to cluster C i 0 ,...,i lÀ1 . The integral part of key l (o) identifies the cluster: it is equal to a number ''i 0 i 1 ,y,i l À 1 '' written in numeral system with base n. The fractional part of key l (o) is the distance between object o and its closest pivot dðp ð0Þ o ,oÞ. The following formula formalizes the overall M-Index mapping: <ref type="figure">2</ref>. Principles of M-Index level one: partitioning (left) and mapping (right).  Fig. <ref type="figure" target="#fig_2">3</ref> (right) shows the mapping in a two-level M-Index (l=2) for clusters C 1,* . The size of the numeric domain is 4 2 =16 and objects from cluster C i,j are mapped to interval ½i Á n þ j,i Á n þ j þ 1Þ. For instance, having structure with four pivots (n=4), object o 2 C 1,2 with d(p 1 , o) = 0.2 is assigned key key 2 ðoÞ ¼ 0:</p><formula xml:id="formula_2">key l ðoÞ ¼ dðp ð0Þ o ,oÞþ X lÀ1 i ¼ 0 ðiÞ o Á n ðlÀ1ÀiÞ :<label>ð1Þ</label></formula><formula xml:id="formula_3">p 3 p 0 p 1 p 2 p 3 C 3 C 2 C 1 C 0 C 3 C 2 C 1 C 0 p 0 p 2 0 4 3 1 p 1 q r 2 Fig.</formula><formula xml:id="formula_4">p 3 p 2 p 0 p 1 C 3.2 C 2.1 C 1.2 C 0.3 C 0.1 C 3.0 C 1.3 p 1 C 1.2 C 1.0 C 1.3 C 1.0 C 2.3 C 3.</formula><formula xml:id="formula_5">2þð0Þ o Á 4 1 þð1Þ o Á 4 0 ¼ 0:2 þ1 Á 4 þ 2 Á 1 ¼ 6:2.</formula><p>Note that not all partitions exist for a particular space and a set of pivots-in Fig. <ref type="figure" target="#fig_2">3</ref> example, clusters C 0,2 and C 2,0 do not exist. Corresponding intervals of the numeric domain are empty (as well as intervals for clusters C i,i,y ). The question of both theoretical and actual number of existing partitions, or distinct distance permutations, was studied recently <ref type="bibr" target="#b9">[10]</ref> and the numbers mainly depend on the size of the pivot set and the complexity of the space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">M-Index with dynamic level</head><p>The multi-level M-Index improves the clustering ability over the single-level M-Index by gathering close objects to smaller partitions. As will be shown later, this can improve efficiency of both precise and approximate searching strategies. Theoretically speaking, the more levels the M-Index has, the more efficient the searching is because the search space is pruned better. On the other hand, large numbers of small or empty clusters lead to a higher fragmentation of the data accessed by a query and to a more demanding management during a query processing. In practice, this can exceed the profit gained from having more levels.</p><p>Let us enhance the concept of the M-Index by introducing the dynamic number of levels which enables (1) to dynamically increase the number of levels (depth) only for large clusters, while (2) the increase requires only local modifications of the M-Index keys. This improvement requires the following modifications of the M-Index concept: a ''maximal M-Index level'' 1r l max r n is chosen (maximal key of the M-Index domain is n lmax Þ, the key l Formula (1) for level l, 1rl r l max is modified to</p><formula xml:id="formula_6">key l ðoÞ ¼ dðp ð0Þ o ,oÞþ X lÀ1 i ¼ 0 ðiÞ o Á n ðlmaxÀ1ÀiÞ ,<label>ð2Þ</label></formula><p>a dynamic cluster tree is created to keep track of actual depth for individual M-Index clusters.</p><p>The cluster tree for l max = 3 is sketched in Fig. <ref type="figure" target="#fig_3">4</ref>. Objects are assigned keys according to the lowest levels at the specific parts of the tree. There is a limit set on the data volume stored in a cluster and the cluster level is increased when this limit is exceeded (up to the level l max ). Partitioning of a cluster by one level means local reordering of the data belonging to the cluster. If we align the number of pivots n to the power of two ðn ¼ 2 m ,m 2 NÞ then each level of the key l , 1r l r l max defines m bits of the integral part of key lmax . This key-assignment approach then becomes similar to extensible hashing <ref type="bibr" target="#b11">[12]</ref> and partitioning of a cluster only means considering more bits of the key lmax .</p><p>Let us denote C i,* all clusters having pivot p i on the first level. Every key in these clusters carries information about distance between pivot p i and the corresponding object. Further, the leaf nodes of the cluster tree store minimal and maximal keys in the corresponding leaf cluster. These bounds represent the minimal/maximal object-pivot distances and are used for space pruning, as discussed below.</p><p>Specifically, internal node entries of the cluster tree have the following structure:</p><formula xml:id="formula_7">/l,ði 0 , . . . ,i lÀ1 Þ,ðptr l þ 1 0 , . . . ,ptr l þ 1 nÀ1</formula><p>ÞS, where l is the node level, (i 0 ,y,i l À 1 ) is the pivot permutation prefix of length l, and ptr j l + 1 are pointers to nodes on level l+ 1. Note that not all clusters on level l + 1 must exist-in such case, the respective pointers ptr i l + 1 are undefined (null). Leaf node entries are of this structure:</p><p>/l,ði 0 , . . . ,i lÀ1 Þ,½key min ,key max S,</p><p>where key min and key max are minimal and maximal keys of objects o 2 C i 0 ,...,i lÀ1 . The root of the cluster tree is always: /0,ðÞ,ðptr 1 0 , . . . ,ptr 1 nÀ1 ÞS. For better orientation in the notation, Table <ref type="table" target="#tab_4">1</ref> summarizes the symbols used throughout this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">M-Index search principles</head><p>In general, indexing and search-space pruning in metric spaces can be done only on the basis of mutual distances of the objects and some designated objects (pivots), respecting the metric distance postulates. There are several theoretical principles of space pruning and data filtering <ref type="bibr" target="#b6">[7]</ref> and the M-Index tries to exploit them. Let us explain these principles on the M-Index algorithm for range query as a fundamental similarity query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1.">Range search algorithm</head><p>Algorithm 1 summarizes the search procedure for range query R(q,r) and its individual steps are commented in detail in the following. First, the procedure calculates distances d(p i , q), i =0,y,n À1 and sorts them in order to find the pivot permutation ðÁÞ q (lines 2-4 of Algorithm 1). The algorithm then traverses the cluster tree from root to leaves in the breadth-first manner using a queue Q of tree nodes (initialization on lines 5-6). First, the algorithm tries to skip accessing leaf clusters or prune whole branches (skip internal nodes) due to the repetitive application of Voronoi partitioning. According to double-pivot distance constraint <ref type="bibr" target="#b6">[7]</ref>, cluster C i can be skipped if</p><formula xml:id="formula_8">C 0 C 1 C 0.2 l = 2 l = 3 l = 1 C 0.1 key l C 0, 2.3 C 0, 2.1 C 0.2, n C n, 0.1 C n, 0.2 C n, 0, n-1 ... ... ... ... ... 0 n3 C 0, n C n, 0 C n C n,1 C n, n-1</formula><formula xml:id="formula_9">dðp i ,qÞÀdðp j ,qÞ 4 2 Á r,<label>ð3Þ</label></formula><p>where p j can be any pivot j 2 f0, . . . ,nÀ1g. To maximize the distance difference on the left side of Eq. ( <ref type="formula" target="#formula_9">3</ref>), we set p j ¼ p ð0Þ q . As the Voronoi partitioning is repeated l-times for cluster C i 0 ,...,i lÀ1 , this rule can be applied l-times in order to skip this cluster-once for each pivot p i 0 , . . . ,p lÀ1 . If pivot p ð0Þ q is among pivots p i 0 , . . . ,p i lÀ2 (for l Z2), it means that it was not considered for Voronoi partitioning on level l (see Section 3.2). Therefore, this pivot cannot be used as pivot p j in the pruning equation (3)-p j is identified as the pivot with the smallest distance d(p j ,q) that is not among p i 0 , . . . ,p i lÀ2 (see lines 9-11 of Algorithm 1). A similar pruning mechanism is used in spatial approximation tree <ref type="bibr" target="#b12">[13]</ref>, but the M-Index uses the same set of pivots repeatedly, which reduces the number of distance computations between pivots and the processed object and avoids the costs of dynamic pivot selection during the building phase of the index.</p><p>Algorithm 1. M-Index range search algorithm.</p><p>Input: Query object q, query radius r Output:</p><formula xml:id="formula_10">Set A ¼ fo 2 Xjdðq,oÞr rg 1 A'|; 2 for i'0 to n À 1do 3 calculate d(p i ,q); 4</formula><p>sort pivots to find p ð0Þ q , . . . ,p ðnÀ1Þ q ; 5 Q 'empty queue; 6 Q.enqueue(root);</p><p>// root node: /0,ðÞ,ðptr 1 0 , . . . ,ptr  The descendants of non-pruned internal nodes need to be further explored (lines <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>. As mentioned in Section 3.3, the leaf nodes maintain minimal and maximal keys of objects stored in that cluster (key min and key max ). For a cluster C i,* , the fractional parts of these keys match the minimal and maximal objects-pivot distances for objects in the cluster, i.e. minfdðp i ,oÞjo 2 C i,Ã g and maxfdðp i ,oÞ jo 2 C i,Ã g. Let us denote these values r min = frac(key min ) and r max =frac(key max ), respectively. Knowing r min and r max (line 16), we can apply range-pivot distance constraint <ref type="bibr" target="#b6">[7]</ref> and skip accessing a cluster C i,* if dðp i ,qÞþr o r min or dðp i ,qÞÀr 4 r max (see lines <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>. Similar approach is used in GNAT <ref type="bibr" target="#b13">[14]</ref> for both leaf and internal nodes of its Voronoi-like tree.</p><formula xml:id="formula_11">, . . . ,i lÀ1 Þ,ðptr l þ 1 0 , . . . ,ptr l þ 1 nÀ1 ÞS 9 j' smallest j Z 0 s.t. ðjÞ q = 2fi 0 , . . . ,i lÀ2 g; 10 if l 404dðp ilÀ1 ,qÞÀdðp<label>ðjÞ</label></formula><p>When none of the above-mentioned mechanisms filters out cluster C i 0 ,...,i lÀ1 on leaf level then the algorithm determines an interval of the M-Index key domain to be searched within that cluster:</p><formula xml:id="formula_12">½dðp i 0 ,qÞÀr,dðp i 0 ,qÞþr,</formula><p>where both limits are shifted by the integral part of keys belonging to this cluster (on line 19 of the algorithm, the integral part is taken as bkey min c). This mechanism is adopted from the iDistance and it is a direct application of object-pivot distance constraint <ref type="bibr" target="#b6">[7]</ref>.</p><p>In many metric spaces, computation of the distance function d is time-consuming and an important objective of majority of Metric Access Methods is to avoid distance computations even at the price of storing some additional metadata with the objects. For each object o 2 X, the M-Index can store distances dðp 0 ,oÞ, . . . ,dðp nÀ1 ,oÞ (computed during insertion) and use them at query time to avoid computation of d(q,o) if max i2f0,...,nÀ1g jdðp i ,qÞÀdðp i ,oÞj 4r:</p><p>This mechanism is referred to as pivot filtering <ref type="bibr" target="#b6">[7]</ref> and is applied in Algorithm 1 on lines 21-22. Finally, the distance d(q,o) is computed for all non-filtered objects o 2 X and it is checked against the range-query condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2.">Nearest-neighbors search algorithms</head><p>We have considered three algorithms for kNN(q,k) queries. The first follows the schema applied in the iDistance <ref type="bibr" target="#b8">[9]</ref>: Initially, the procedure accesses all intervals of the key domain (as on line 19 of Algorithm 1) that Mapping function, key l : DÀ!n l correspond to a range query with a given small initial radius r init (this value is to be specified separately for a particular data space). This radius is increased in each iteration skipping the data that was already accessed. Only the k nearest objects are always kept and the current distance between q and the k-th answer object ðR k Þ is an upper bound on the distance to the actual k-th nearest neighbor of q. The process ends when the examined radius r exceeds R k (see iDistance <ref type="bibr" target="#b8">[9]</ref> for details). Once at least k objects are explored and, thus, R k is known, this radius could be used to prune the cluster tree by the mechanisms mentioned above. As the clusters are visited in a given order unrelated to a particular q, the actual value of R k can be large during the algorithm execution, which reduces efficiency of the pruning. This problem is improved in the second algorithm which first determines an upper bound R k by means of a heuristic and then evaluates range query Rðq,R k Þ. The heuristic accesses cluster C ð0Þ q ,...,ðlÀ1Þ q , in which object q would be stored as ''the most promising cluster''. The actual query radius R k shrinks as more objects are explored. In real-life datasets, the R k radius shrinks faster (depending on the number of accessed objects) than in the previous algorithm, which makes it more efficient. In addition, this algorithm is simpler to implement and avoids specification of the space-dependent initial radius r init . The third algorithm extends the idea of the previous one and exploits principles of the approximate algorithm that is introduced in the following section. Basically, it runs the range algorithm as described above, only the tree-traversal queue Q is ordered by a heuristic designed to place clusters that are ''close to q'' ahead in Q. Note that in Algorithm 1, the clusters in queue Q are ordered randomly with respect to a particular query object q. The query-specific ordering speeds up the shrinkage of the query radius R k and that improves the pruning efficiency of the search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Approximate search with M-Index</head><p>Let us introduce the M-Index approximate search strategy for kNN(q,k) queries. The method follows the schema from Algorithm 1 for range query Rðq,R k Þ, where radius R k continuously shrinks. The cluster queue Q becomes a priority queue ordered according to a heuristic designed to prioritize clusters that are more likely to contain objects from the precise kNN(q,k) answer. The algorithm is in the category of early-termination approximation strategies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15]</ref>-it stops when given condition is fulfilled. The algorithm monitors the volume of data accessed in the visited clusters and a typical stop-condition is defined by a maximal number of object accesses or disk block reads.</p><p>The cluster-ordering heuristic is based purely on an analysis of the dðp 0 ,qÞ, . . . ,dðp nÀ1 ,qÞ distances. Cluster C ð0Þ q ,...,ðlÀ1Þ q , in which object q would be stored (let us denote it C q ), gets the highest priority-it is assigned the lowest penalty: penalty(C q ) =0. Cluster C q has the smallest sum of distances between q and the cluster's pivots: dðp ð0Þ q ,qÞþ Á Á Á þdðp ðlÀ1Þ q ,qÞ and this sum grows for other clusters. This is reflected by the penalty in order to express the ''proximity'' of the cluster to q. Specifically:</p><formula xml:id="formula_13">penaltyðC i 0 ,...,i lÀ1 Þ ¼ 1 l Á X lÀ1 j ¼ 0 maxfdðp i j ,qÞÀdðp<label>ðjÞ</label></formula><p>q ,qÞ,0g:</p><p>The penalty is normalized by 1/l in order to make its value comparable for clusters on different levels l. Fig. <ref type="figure" target="#fig_4">5</ref> provides an example in which C q =C 0,3 and the query-pivots distances are d(p 0 ,q)= 0.2, d(p 3 ,q)= 0.25, d(p 1 ,q) =0.3, and d(p 2 ,q)= 0.5. We know that penalty(C 0,3 ) = 0 and penalties for other ''close'' clusters are, for instance:</p><formula xml:id="formula_14">penaltyðC 0,1 Þ ¼ 1=2 Á ðdðp 0 ,qÞÀdðp 0 ,qÞ þðdðp 1 ,qÞÀdðp 3 ,qÞÞÞ ¼ 0:025, penaltyðC 3,0 Þ ¼ 1=2 Á ðð0:25À0:2Þ þ maxf0:2À0:25,0gÞ ¼ 0:025, penaltyðC 1,0 Þ ¼ 1=2 Á ðð0:3À0:2Þ</formula><p>þ maxf0:2À0:25,0gÞ ¼ 0:5: Note that after accessing first k objects, the approximate algorithm uses current query radius R k for pruning and filtering the data space as applied in Algorithm 1. If there is no stop-condition specified, then the algorithm behaves like a precise kNN(q,k) evaluation strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Architecture of the index</head><p>The M-Index principles and algorithms introduced in the previous sections can be employed to design and build various concrete data structures. Data objects can be actually stored in any structure that would index them according to their M-Index keys, ideally with efficient evaluation of interval queries. This data structure should also be efficient for non-uniform distribution of the M-Index keys in the numeric domain ½0,ðl max Þ n . B + -tree is an ideal candidate for such data management and it also ensures efficient memory and disk utilization.</p><p>Implementation of the cluster tree, as described in Section 3.3, is straightforward. This tree is actually a dynamic hashing directory and is expected to be kept in main memory-in our implementation, the tree for 100,000 objects occupied about 60 kB of memory (l max = 5, n =20, leaf-cluster capacity was 500 objects from the CoPhIR dataset-see Section 4.1 for details). </p><formula xml:id="formula_15">p 3 p 2 p 0 p 1 C 3,2 C 2,1 C 1,2 C 0,3 C 0,1 q C 3,0 C 1,3 C 1,0 C 2,3 C 3,1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.1.">Distributed M-Index</head><p>As the hash principles of the M-Index are kept separated from the actual data storage, we can directly design a distributed variant of the M-Index. The data can be stored in a distributed structure that efficiently evaluates interval queries on a simple key domain. A suitable solution would be, for instance, structured peer-to-peer network Skip Graphs <ref type="bibr" target="#b15">[16]</ref>-every node (peer) of this structure manages data with keys from a given interval and queries are efficiently forwarded to peers that overlap with the given query interval.</p><p>The cluster tree, necessary for the dynamic variant of M-Index, can be either (1) centralized (or kept as synchronized replicas on the peers) or (2) fully distributed over the structure nodes. In the first case, the data is always inserted and queries issued through the centralized directory, that is always kept up-to-date. The distributed range search algorithm practically does not differ from Algorithm 1; the kNN(q,k) queries are evaluated by the second strategy proposed in Section 3.4.2-first an upper bound radius R k is identified and then Rðq,R k Þ is evaluated. Design of the fully distributed cluster tree and query navigation algorithms goes beyond the scope of this paper. Note that a single-level distributed M-Index restricted to the iDistance pruning corresponds to the structure called M-Chord <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental evaluation</head><p>In this section, we present experiments conducted on the M-Index with B + -tree storage. The structure was implemented in Java using the MESSIF <ref type="bibr" target="#b17">[18]</ref> framework that provides extensive support for creating prototypes of metric-based indexing techniques. We evaluated the performance of the M-Index with various settings and compared it with two other approaches for precise similarity search-the iDistance <ref type="bibr" target="#b8">[9]</ref> generalized for metric spaces (see Section 3.1) and the PM-Tree <ref type="bibr" target="#b18">[19]</ref>. The PM-Tree extends a popular structure M-Tree <ref type="bibr" target="#b19">[20]</ref> by adding space pruning and filtering using a static set of preselected pivots. We also compare the approximate strategy of the M-Index with several recent approaches that also utilize permutations of sets of reference objects. In order to make the results comparable, all tested structures always use the same set of pivots (PM-Tree uses additional dynamically selected pivots <ref type="bibr" target="#b19">[20]</ref>). Also, all the structures were implemented using the MESSIF framework, ran in the same operating environment, and used the same datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Settings</head><p>The experiments were realized on two datasets with different characteristics. The first one consists of visual descriptors from images-five MPEG-7 features were extracted from every image: scalable color, color structure, color layout, edge histogram, and homogeneous texture <ref type="bibr" target="#b1">[2]</ref>. The images with the descriptors were taken from the CoPhIR database <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. Each of these descriptors is compared using a metric function and we use a weighted sum of these distances to aggregate them into a single metric space <ref type="bibr" target="#b3">[4]</ref>. In total, this representation can be viewed as a 280-dimensional vector (occupying about 750 B on the disk) together with a complex aggregation distance function-evaluation of one distance takes approximately 0.01 ms on a standard hardware. The intrinsic dimensionality <ref type="bibr" target="#b20">[21]</ref> of this dataset is 12.9 which makes it rather difficult to index. We used sets of 20,000-1,000,000 images-the actual size is specified in each experiment. We refer to this dataset as CoPhIR.</p><p>The second dataset used in the experiments is artificial and consists of 1,000,000 randomly uniformly generated vectors from ½0,1 3 . The vectors are compared by Euclidean distance (L 2 ), the dataset was created using the SISAP random generator <ref type="bibr" target="#b21">[22]</ref>, and we will refer to it as vectors. The intrinsic dimensionality of this dataset is 3.5, which makes it significantly better indexable than the CoPhIR dataset.</p><p>All similarity queries in this section were processed for 50 query objects and we always present results averaged over these 50 queries.</p><p>Search efficiency of the structures is measured by I/O costs, computational costs, and response times. Specifically, the I/O costs express the number of 4 kB-block reads realized during the search. The computational costs are measured as the number of evaluations of the distance function d (distance computations). This number is influenced by the pivot filtering (see Section 3.4) applied by all the structures under test. When we want to eliminate this influence, we report on the number of data objects accessed during the search. For the approximate evaluation strategies, we measure the answer quality by the recall (see <ref type="bibr">Section 2)</ref>.</p><p>The experiments were realized on a standard PC with 4 GB memory and a disk with 7200 rpm. The response times of the searches are influenced by caching, both on the disk and the operating system level. These results are mutually comparable, as all the experiments were conducted on equal terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Construction costs</head><p>Due to the mapping nature of the M-Index, the computational construction costs can be directly derived from the size of the dataset and the number of pivots used. Table <ref type="table" target="#tab_5">2</ref> compares construction costs of the M-Index and the PM-Tree for the CoPhIR dataset when both use the same set of 20 pivots.</p><p>The PM-Tree node-splits were realized using Slim-Tree algorithm <ref type="bibr" target="#b22">[23]</ref>. We can see that the PM-Tree costs are 3-4 Â higher than for the M-Index. The I/O costs of the M-Index depend on the specific data-indexing structure. Our prototype uses the B + -tree, which has a very efficient disk management, thus the M-Index required about 80% I/O operations compared to the building costs of the PM-Tree. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Precise similarity search</head><p>In the first set of the experiments, we test efficiency of the data-space partitioning and subsequent pruning applied by the precise search strategies. We alter (1) the size of the set of pivots n and (2) the size of the indexed dataset X. We put under test the static M-Indexes with various levels and the dynamic M-Index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Number of pivots</head><p>Fig. <ref type="figure" target="#fig_5">6</ref> reports on the influence of the number of pivots n on M-Index search efficiency for the CoPhIR dataset. We processed a set of kNN(q,50) queries on three static indexes (l =1,2,3) and on a dynamic M-Index with l max =5. All structures managed 100,000 CoPhIR objects and the graph shows the number of accessed objects while n varied from 1 to 50 (with the exception that n Z l).</p><p>We can observe the expected positive influence of growing n on the pruning efficiency and the trend is similar for all tested M-Index variants. These sets of pivots were selected from a sample set by a variant of the standard incremental pivot-selection algorithm <ref type="bibr" target="#b23">[24]</ref>, which tries to maximize efficiency of pivot filtering. We have tested other pivot-selection approaches as well, including random selection-they had similar performance and their trends were practically identical. In the rest of the trials, we will fixate the set of pivots for the CoPhIR dataset to 20 (used in this experiment).</p><p>Since the M-Index stores precomputed object-pivot distances together with the objects, the number of pivots increases the size of the data in memory or on disk. This influence is negligible for the CoPhIR dataset where each object occupies around 750 B but it increases the I/O costs significantly for the three-dimensional vectors. Fig. <ref type="figure" target="#fig_6">7</ref> shows the number of disk-block reads for R(q,0.1) on 500,000 vector dataset for n= 4, 8, 16, and 32 (the values are interpolated in the graphs). This query retrieves around 2000 objects from this dataset.</p><p>We can see that, in general, the I/O costs grow for higher numbers of pivots because the improved pruning effect cannot exceed the negative effect caused by the data size growth. Looking at the line for the dynamic M-Index, we can see that n = 8 is the optimum value in terms of I/O costs. Table <ref type="table" target="#tab_6">3</ref> shows the actual I/O cost values for the dynamic M-Index together with the corresponding numbers of objects accessed and distances evaluated by the search algorithm. For instance, M-Index with n = 16 prunes 1/3 more objects than that with n= 8. The stored precomputed distances filter the number of distance computations down to 1/7 of the number of accessed objects for n =8 and to 1/8 for n = 16. It seems that optimal setting for this dataset would be to use more pivots for space partitioning and space pruning, and use only a part of the precomputed distances for the pivot filtering. This option is not available in the M-Index as described above and, therefore, we use n = 16 for the vectors dataset in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Dataset size</head><p>Fig. <ref type="figure" target="#fig_7">8</ref> compares the efficiency of the kNN query processing on the CoPhIR dataset for several M-Index variants, the iDistance, and the PM-Tree. The three graphs show the number of data objects accessed (space pruning efficiency), I/O costs, and response times, respectively, while increasing the volume of the indexed data. In general, all these graphs have linear trends typical for most centralized metric structures with precise search strategies.</p><p>Looking at graph (a), we can see that the M-Index with l = 1 is slightly more efficient than the iDistance and that the multi-level M-Indexes prune the search space even better. The dynamic M-Index (l max = 5) exhibits best pruning effect-it accesses 25% fewer objects than the PM-Tree. For some of these accessed objects, evaluation of the distance function is skipped due to the pivot filtering. In these experiments, the number of distance computations were about 2/3 of the number of accessed objects, but the differences between individual structures were small   because all the structures use the same set of pivots for filtering.</p><p>The I/O costs, depicted in graph (b), show the accessed objects in terms of actual number of pages read during the query processing. All the M-Index settings have similar results, which confirms the expectation that the spacepruning improvement achieved by recursive partitioning is paid by higher data fragmentation. In particular, the average disk block utilization of M-Index with l =1 is 0.85, while for l = 3 this value is 0.78. For the PM-Tree, we have measured the average disk block utilization 0.89. In general, the M-Index has about 15% lower I/O costs than the PM-Tree.</p><p>As mentioned in Section 3.3, static M-Index with higher l has the disadvantage of creating many small clusters for smaller datasets. Table <ref type="table" target="#tab_7">4</ref> shows the number of clusters for different M-Index variants and various dataset sizes. As expected, static M-Indexes have rather stable number of clusters regardless of the dataset size while their number increases practically linearly for the dynamic M-Index (the maximal cluster size for dynamic M-Index was set to 512 kB). Higher fragmentation and management of many small clusters have negative effect on the overall efficiency of the search, which can be observed in Fig. <ref type="figure" target="#fig_7">8(c)</ref> showing kNN-query response times. The M-Index response times do not differ significantly, and the dynamic M-Index outperforms the others-it is about 20% faster than the PM-Tree for the 200,000-objects dataset.</p><p>Fig. <ref type="figure" target="#fig_8">9</ref> shows results of analogous experiment on the vectors dataset. Graph (a) shows the number of objects accessed during the query processing. We can observe the same trends as for the CoPhIR dataset, only the differences between individual M-Index settings and iDistance are more significant. For this dataset, the dynamic M-Index seems to prune the search space slightly better than the PM-Tree for smaller datasets and slightly worse for larger ones. This is confirmed by the second graph that shows the I/O costs. The PM-Tree uses dynamically selected pivots and they seem to be useful especially for larger datasets. This can be further improved for the M-Index by choosing larger l max (the actual setting is l max = 5 and maximal capacity of leaf cluster is 16 kB). The third graph (c) shows the number of distances evaluated during the query processing. Since all the structures (including the PM-Tree) use the same set of pivots for filtering, the values are almost identical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Observations</head><p>Generally, we can say that the M-Index fulfilled our expectations-its precise search strategies prune the   search space well, competes with current high-end metric index structures, and outperforms them on a complex reallife dataset. The number of pivots and levels has the expected influence and the dynamic M-Index seems to be the best in respect of both I/O costs and response times.</p><formula xml:id="formula_16">PM-Tree iDistance PM-Tree iDistance M-Index level 1 M-Index level 2 M-Index level 3 dynamic M-Index M-Index level 1 M-Index level 2 M-Index level 3 dynamic M-Index M-Index level 1 M-Index level 2 M-Index</formula><p>The precomputed distances stored together with data objects may multiply the object size for smaller data types and larger pivot sets. This may increase the I/O costs while cutting the computational costs only slightly more than smaller pivot sets. This observation calls for the possibility to use less objects for filtering than for space partitioning. A similar feature is applied and studied by the PM-Tree <ref type="bibr" target="#b18">[19]</ref>.</p><p>Even though the M-Index lowers the search costs down for the CoPhIR dataset, the percentage of accessed data is still relatively high. This confirms the general fact that precise metric search is expensive-it typically has linear scalability, which is the major motivation for approximate metric searching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Approximate search-basic evaluation</head><p>Let us now focus on performance of the M-Index approximate strategy. All experiments in this section are realized on the CoPhIR dataset and we preserve all the settings introduced in Section 4.3 in order to make the precise and approximate results comparable. For the same reason, we compare the M-Index (in several variants) with the PM-Tree structure. Similarly to the M-Index, the PM-Tree algorithm for kNN(q,k) uses a priority queue of structure nodes sorted according to a heuristic <ref type="bibr" target="#b19">[20]</ref> with respect to particular query point q. The search process can be stopped in an appropriate moment and the approximate result of the search can be reported <ref type="bibr" target="#b24">[25]</ref>. In order to make the results comparable, we use the same early-stopping condition for both the M-Index and the PM-Tree (size of accessed data) and we always report corresponding results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">Maximal percentage of data</head><p>In the first experiment, we limit the percentage of data volume accessed by the approximate algorithms. Fig. <ref type="figure" target="#fig_1">10</ref> shows the recall of the approximate search while varying this percentage for the dataset of 100,000 objects.</p><p>The graph demonstrates a good clustering ability of the M-Index which grows with level l and is best for the dynamic M-Index. Limiting the percentage of accessed data to 5%, the dynamic M-Index reaches 93% recall while the PM-Tree has a recall of about 50%. The PM-Tree has to access four times more data to reach 90% recall. Note that the M-Index pruning mechanisms are applied also during the approximate search and, thus, the algorithm may reach the precise answer and stop even before reaching the accessed-data limit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">Maximal fixed volume of data</head><p>In this experiment, we fixate the maximal amount of data accessed by the approximate algorithm regardless of the size of the data being indexed. The graph in Fig. <ref type="figure" target="#fig_1">11</ref> shows the search recall when the number of accessed objects is limited to 10,000 and varying the dataset size from 20,000 to 200,000 objects.</p><p>We can see that PM-Tree recall is falling from 99% for the 20,000 dataset down to about 60% for the 200,000 dataset. As the M-Index level l grows from 1 to 3, the recall becomes more stable and the dynamic M-Index reaches more than 93% recall even for the database of 200,000 objects.</p><p>The graph in Fig. <ref type="figure" target="#fig_1">12</ref> shows that the approximate-search response times of the M-Index are stable and significantly shorter than for the PM-Tree. The PM-Tree manages a dynamic queue of tree nodes ordered according to continuously evaluated distances between query q and nodes' pivots <ref type="bibr" target="#b24">[25]</ref>. This mechanism generates relatively nontrivial additional time costs in comparison to a small fixed number of query-pivot distances computed by the M-Index algorithm. Comparing these results with the precise response times in Fig. <ref type="figure" target="#fig_7">8</ref>(c), we can conclude that, for 200,000-objects database, the dynamic M-Index is able to return over 95% of the precise answer in less than 10% of the time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Approximate search-advanced evaluation</head><p>This section provides a deeper evaluation of the M-Index approximate strategy, independently of the precise search evaluation. The results are compared with relevant techniques designed purely for approximate similarity search in metric spaces. In this section, we focus only on the dynamic M-Index (with various l max ) and we use the CoPhIR dataset with the data volume of up to 1,000,000 objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1.">Number of pivots for approximation</head><p>In Section 4.3.1, we have uncovered the correlation between the number of pivots and performance of the M-Index precise similarity search. Now, let us observe the influence of the number of pivots on the approximate search efficiency. Fig. <ref type="figure" target="#fig_2">13</ref> shows the recall of the approximate search that accesses maximally 5000 disk blocks. The M-Index stores 1,000,000 data objects using various number of randomly selected pivots.</p><p>We can see that the recall grows together with the clustering effect of the index, which is positively influenced by both the maximal M-Index level l max and by the number of pivots n. On the other hand, growing the number of pivots (and, thus, number of clusters) increases fragmentation of the data and also the objects occupy more space because every object o 2 X stores precomputed distances dðo,p i Þ,8i 2 f0, . . . ,nÀ1g. Therefore, accessing a given number of objects results in more disk reads and the recall (for a fixed number of disk reads) fluctuates and eventually begins to fall. The curves are identical for l max = 5 and 6-higher levels do not appear to be meaningful for this dataset size. For the following experiments, we fixate l max = 6 and number of pivots n= 96.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2.">Cluster order heuristic</head><p>Let us analyze the heuristic that determines the clusters' access order that is used in the M-Index approximate algorithm (see <ref type="bibr">Section 3.5)</ref>. The heuristic tries to express the ''distance of the cluster from the query object q''. Fig. <ref type="figure" target="#fig_3">14</ref> shows how the recall evolves with growing volume of data accessed according to four different cluster orders.</p><p>The highest curve corresponds to an ideal order identified by the percentage of precise answer held by individual clusters. The second curve is the M-Index cluster order curve. We can see that, for instance, accessing 0.5% of the indexed data, M-Index reaches 81% recall on average, while an ideal heuristic (established separately for each query object q) would return 98% of the precise answer. If we compare these values with Fig. <ref type="figure" target="#fig_1">10</ref>, we can see a significant improvement achieved by this M-Index configuration (l max =6, n =96).</p><p>There are various approximate techniques that also index the data according to pivot permutations. Amato and Savino <ref type="bibr" target="#b25">[26]</ref> propose a structure that indexes the data by pivot permutation prefixes using inverted files. They propose to compare the pivot permutations by Spearman Footrule distance <ref type="bibr" target="#b26">[27]</ref> modified to be applicable only to permutation prefixes (Induced Footrule Distance). Cha ´vez et al. <ref type="bibr" target="#b27">[28]</ref> propose approximation technique based on ordering the object-pivot distances and compare three measures of pivot permutations: Spearman Rho distance <ref type="bibr" target="#b28">[29]</ref> is evaluated as slightly more effective than Spearman Footrule.</p><p>We have implemented these two distances as the cluster-order heuristic and compared them with the M-Index heuristic in Fig. <ref type="figure" target="#fig_3">14</ref> (Spearman Rho was modified to work on permutation prefixes). These cluster orders appear to be slightly less efficient and, for this setting, they give practically identical results. For other M-Index settings (less efficient and not presented in this section), the M-Index heuristics always led to better results and the difference from Spearman-like distances was more significant.</p><p>PP-Index <ref type="bibr" target="#b10">[11]</ref> is a recent technique for approximate search that is based on analogous principles as the M-Index. Its keystone is a main-memory tree similar to M-Index cluster tree, only the leaf cluster always contains at least z data objects (where z is a fixed structure Influence of pivots on approx. kNN (q, 30) 200 300 400 500 Fig. <ref type="figure" target="#fig_2">13</ref>. Recall of approximate kNN(q,30) accessing maximally 5000 disk blocks for the 1,000,000 CoPhIR dataset.</p><p>Various cluster orders, kNN (q, 30) parameter). The data is physically stored in a static file ordered by a numbering schema. The approximate kNN (q,k) search always accesses the cluster where object q would be stored and possibly several other clusters identified according to a heuristic similar to the M-Index penalty <ref type="bibr" target="#b10">[11]</ref>.</p><p>We have evaluated the PP-Index structure with the same settings as the M-Index (set of pivots and l max ). Fig. <ref type="figure" target="#fig_11">15</ref> compares recall for the M-Index and PP-Index depending on the number of disk block reads. In order to make the comparison objective, we have created both indexes in two variants-with B + -tree and static file as underlying data storages. We have tested three variants of parameter z values (250, 500, and 1000) and chose the best setting for the given dataset (500, in this case). We can see that the M-Index has higher and more stable recall.</p><p>Observations: The experiments with the M-Index approximate search demonstrated that the index has a good clustering ability and that this feature is well utilized by the approximation algorithm. The efficiency of the search grows with the M-Index level and is best for the dynamic M-Index. The processing costs of the algorithm are stable and quite low and the dynamic M-Index demonstrated nearly constant approximate search efficiency as the volume of indexed data grows. The number of pivots influences the efficiency positively, up to a certain value. The M-Index cluster-order heuristic seems to be more efficient than the heuristics used in other techniques that focus on the approximate search based on pivot permutations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related work</head><p>As described in Section 3.1, the M-Index shares one of its core space pruning principles -the object-pivot distance constraint -with the iDistance technique <ref type="bibr" target="#b8">[9]</ref> for kNN search in vector spaces. However, the M-Index is defined for general metric space and it exploits several other pruning techniques which results in more efficient range and kNN similarity search in comparison with the iDistance. A Voronoi-like partitioning of the metric space is the core of the GNAT <ref type="bibr" target="#b13">[14]</ref> technique, which is a multi-way tree structure that exploits the range-pivot distance constraint similarly to the M-Index.</p><p>We have also compared the M-Index with the PM-Tree <ref type="bibr" target="#b18">[19]</ref> structure, which is an improved variant of the well-established Metric Tree (M-Tree) <ref type="bibr" target="#b19">[20]</ref>. It is a balanced disk-oriented tree structure that supports also approximate similarity search <ref type="bibr" target="#b24">[25]</ref>. Experiments showed that the M-Index was more efficient for precise search on a complex real-life dataset and comparable on an artificial vector dataset. Considering the approximate strategy, the M-Index achieves better recall with a significantly more stable trend when the dataset size increases. Also, the M-Index has fixed and predictable construction costs.</p><p>An approximation-oriented structure spatial approximation tree (SAT) <ref type="bibr" target="#b12">[13]</ref> builds a spanning tree of the stored objects based on their mutual metric distances. As well as the M-Index, this structure exploits repeatedly the doublepivot distance constraint but the M-Index uses the same set of pivots recursively, which significantly reduces the building costs. The SAT index is static in its nature and, although a dynamic extension <ref type="bibr" target="#b29">[30]</ref> exists, its approximation recall and efficiency is, in general, not better than that of the PM-Tree. Recently, several approximate techniques emerged <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b10">11]</ref> that index the data according to pivot permutations <ref type="bibr" target="#b9">[10]</ref>. We have evaluated these techniques or simulated the behavior of their approximate heuristics and compared them with the M-Index, which appeared to be more efficient. See recent survey <ref type="bibr" target="#b14">[15]</ref> for more metricbased approximate approaches and their categorization.</p><p>The D-Index <ref type="bibr" target="#b30">[31]</ref> structure uses a hashing-like approach where objects are mapped to well-separated partitions by several split-functions. Since the M-Index assigns a simple number to each indexed object, it can be also viewed as a hashing technique. Moreover, objects that are close in the original metric space are mapped to close numbers by the M-Index, so the core property of the locality sensitive hashing (LSH) <ref type="bibr" target="#b31">[32]</ref> is satisfied. Compared with LSH, which is currently defined only for certain specific data spaces (distance functions), the M-Index broadens the usability of the LSH approach to a generic metric space. Contrary to the classic LSH, the M-Index has the ability to stop the search when precise answer is gathered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and future work</head><p>We have presented the M-Index-a novel metric-based indexing and searching technique that exploits practically all known principles of metric space partitioning, pruning and filtering. The M-Index maps objects from any metric space to a simple numeric domain and then employs interval queries to implement precise and approximate search algorithms. We define a dynamic version of the mapping that shares certain principles with the extensible hashing. There are many techniques, such as the B + -tree or relational databases, that can speed up processing of the interval queries and they can be directly employed by the M-Index. As such, the M-Index is highly extensible and can be used in many application areas.</p><p>Our experiments compare the M-Index with techniques that focus on precise and approximate search in metric space. The M-Index outperforms the state-of-the-art precise-search techniques in terms of I/O, computational costs, and query response times. The M-Index efficiency is comparable with the PM-Tree on a simple artificial vector dataset. The M-Index approximate search strategy seems to be more efficient even than the recent techniques that focus only on approximate search. This search strategy reaches nearly recall as well as response times while the dataset size increases. Since the M-Index has fixed construction costs, its indexing directory grows logarithmically with the dataset size, and since the storage can use established database technologies, the M-Index is also highly scalable.</p><p>The principles introduced by the M-Index can be directly utilized in a distributed environment. We have recently applied the M-Index technique to the full CoPhIR database (over 100 million images) by employing a structured peer-to-peer network called Skip Graphs. <ref type="foot" target="#foot_0">1</ref> We believe that the distributed M-Index can provide an efficient similarity management on real-life collections of billions of complex data objects. We would like to focus our future research on this area as well as to study and develop the M-Index mapping principle as a universal technique of locality sensitive hashing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Contents lists available at ScienceDirect journal homepage: www.elsevier.com/locate/infosys Information Systems 0306-4379/$ -see front matter &amp; 2010 Elsevier B.V. All rights reserved. doi:10.1016/j.is.2010.10.002</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Principles of the iDistance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Principles of the two-level M-Index (l = 2): partitioning (left) and mapping (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Dynamic cluster tree, l max =3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Principles of M-Index approximate strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Pruning efficiency with various n (CoPhIR).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig.7. I/O costs for range query R(q,0.1) on M-Index with various n for the dataset of 500,000 vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Search efficiency for kNN(q,30) on CoPhIR: (a) search-space pruning efficiency, (b) I/O costs, (c) response times.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Efficiency of R(q,0.1) query evaluation on vectors: (a) search space pruning, (b) I/O costs, (c) computational costs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .Fig. 11 .Fig. 12 .</head><label>101112</label><figDesc>Fig.10. Recall of approximate kNN(q,30) on CoPhIR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 15 .</head><label>15</label><figDesc>Fig.<ref type="bibr" target="#b14">15</ref>. Recall for M-Index and PP-Index, varying the maximal accessed data (disk blocks) for the 1,000,000 CoPhIR dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. ,nÀ1g: ðiÞ o o ðjÞ o 3dðp ðiÞ o ,oÞ odðp ðjÞ o ,oÞ3ðdðp ðiÞ o ,oÞ ¼ dðp ðjÞ o ,oÞ4i ojÞ:</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc><ref type="bibr" target="#b0">1</ref> </figDesc><table><row><cell>...</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>...</cell><cell></cell></row><row><cell>0</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>15 else node is a leaf node 16 get r min , r max fromkey min , key max ; 17 if dðp i0 ,qÞþr o r min 3 dðp i0 ,qÞÀr 4rmax then</figDesc><table><row><cell>18</cell><cell>Continue;</cell></row><row><cell>19</cell><cell>C'storage. getDataForInterval(</cell></row><row><cell></cell><cell>bkey min cþdðp i0 ,qÞÀr,</cell></row><row><cell></cell><cell>bkey min cþdðp i0 ,qÞþr);</cell></row><row><cell>20</cell><cell>foreach object o in C do</cell></row><row><cell>21</cell><cell>if max nÀ1 i ¼ 0 jdðp i ,qÞÀdðp i ,oÞj4 r then</cell></row><row><cell>22</cell><cell>Next o;</cell></row><row><cell>23</cell><cell>if dðq,oÞ r r then</cell></row><row><cell>24</cell><cell>A.addObject(o);</cell></row></table><note><p>q ,qÞ4 2r then 11 Continue; 12 if node is internal then 13 for i'0 to n À 1 do 14 Q :enqueueðdereferenceðptr l þ 1 i ÞÞ;</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1</head><label>1</label><figDesc>Symbols used in this work. Cluster formed by objects o 2 X : ð0Þ o ¼ i 0 ,ð1Þ o ¼ i 1 , . . . ,ðlÀ1Þ o ¼ i lÀ1 key l</figDesc><table><row><cell cols="2">Symbol Meaning</cell></row><row><cell>D</cell><cell>The data domain</cell></row><row><cell>X</cell><cell>Indexed objects, X D D</cell></row><row><cell>d</cell><cell>Distance function, d : D Â DÀ!½0,1Þ</cell></row><row><cell>n</cell><cell>Number of pivots</cell></row><row><cell>p i</cell><cell>Pivots, p i 2 D,8i 2 f0, . . . ,nÀ1g</cell></row><row><cell>ðÁÞ o</cell><cell>Permutation of {0,y,n À 1} sorting non-decreasingly</cell></row><row><cell></cell><cell>distances dðp ðiÞ o ,oÞ</cell></row><row><cell>l</cell><cell>Number of M-Index levels, l 2 N</cell></row><row><cell>l max</cell><cell>Maximum level of dynamic M-Index</cell></row><row><cell>C i0 ,...,ilÀ1</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc>Numbers of distance computations during index construction for CoPhIR dataset and n= 20.</figDesc><table><row><cell>Dataset size</cell><cell>20,000</cell><cell>80,000</cell><cell>140,000</cell><cell>200,000</cell></row><row><cell>M-Index</cell><cell>400,000</cell><cell>1,600,000</cell><cell>2,800,000</cell><cell>4,000,000</cell></row><row><cell>PM-Tree</cell><cell>1,205,538</cell><cell>6,299,207</cell><cell>11,627,729</cell><cell>16,897,996</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc>Various costs of range query R(q,0.1) evaluation on dynamic M-Index for the dataset of 500,000 vectors.</figDesc><table><row><cell></cell><cell>I/O</cell><cell>Objs. reads</cell><cell>Dist. comp.</cell></row><row><cell>n= 4</cell><cell>530</cell><cell>69,714</cell><cell>12,574</cell></row><row><cell>n= 8</cell><cell>402</cell><cell>30,007</cell><cell>4191</cell></row><row><cell>n= 16</cell><cell>497</cell><cell>20,756</cell><cell>2617</cell></row><row><cell>n= 32</cell><cell>696</cell><cell>16,121</cell><cell>2198</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc>Numbers of clusters for various M-Index settings and various sizes of CoPhIR dataset.</figDesc><table><row><cell>M-Index</cell><cell>20 K</cell><cell>40 K</cell><cell>60 K</cell><cell>80 K</cell><cell>100 K</cell><cell>120 K</cell><cell>140 K</cell><cell>160 K</cell><cell>180 K</cell><cell>200 K</cell></row><row><cell>Static l = 1</cell><cell>20</cell><cell>20</cell><cell>20</cell><cell>20</cell><cell>20</cell><cell>20</cell><cell>20</cell><cell>20</cell><cell>20</cell><cell>20</cell></row><row><cell>Static l = 2</cell><cell>298</cell><cell>321</cell><cell>328</cell><cell>332</cell><cell>333</cell><cell>333</cell><cell>341</cell><cell>343</cell><cell>344</cell><cell>347</cell></row><row><cell>Static l = 3</cell><cell>1778</cell><cell>2261</cell><cell>2558</cell><cell>2770</cell><cell>2927</cell><cell>3064</cell><cell>3188</cell><cell>3275</cell><cell>3354</cell><cell>3432</cell></row><row><cell>Dynamic l max = 5</cell><cell>341</cell><cell>595</cell><cell>855</cell><cell>1134</cell><cell>1390</cell><cell>1684</cell><cell>2051</cell><cell>2414</cell><cell>2621</cell><cell>2852</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>See a demonstration at http://mufin.fi.muni.cz.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been supported by national research projects GACR 201/08/P507, GACR 201/09/0683, GACR 103/10/0886, GACR P202/10/P220, and MSMT 1M0545. The hardware infrastructure was provided by the META-Centrum under the research intent MSM6383917201.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Searching in high-dimensional spaces: index structures for improving the performance of multimedia databases</title>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Berchtold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="322" to="373" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><surname>Mpeg-7</surname></persName>
		</author>
		<idno>ISO/IEC 15938-3:2002</idno>
		<title level="m">Multimedia content description interfaces. Part 3: Visual</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">CoPhIR: a test collection for content-based image retrieval</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bolettieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Falchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lucchese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Piccioli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rabitti</surname></persName>
		</author>
		<ptr target="http://cophir.isti.cnr.itS" />
	</analytic>
	<monogr>
		<title level="m">CoRR abs/0905</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Building a Web-scale image similarity search system</title>
		<author>
			<persName><forename type="first">M</forename><surname>Batko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Falchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lucchese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rabitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sedmidubsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zezula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="599" to="629" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cophir image collection under the microscope</title>
		<author>
			<persName><forename type="first">M</forename><surname>Batko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohoutkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Novak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SISAP &apos;09: Proceedings of the 2009 Second International Workshop on Similarity Search and Applications</title>
		<meeting><address><addrLine>Washington, DC; USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Metric index: an efficient and scalable solution for similarity search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Batko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SISAP &apos;09: Proceedings of the 2009 Second International Workshop on Similarity Search and Applications</title>
		<meeting><address><addrLine>Washington, DC; USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="65" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Similarity Search: The Metric Space Approach</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zezula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Amato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dohnal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Batko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Database Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Foundations of Multidimensional and Metric Data Structures, Computer Graphics and Geometric Modeling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<pubPlace>San Francisco, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">iDistance: an adaptive B + -tree based indexing method for nearest neighbor search</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Jagadish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Database Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="364" to="397" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Counting distance permutations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Skala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Discrete Algorithms</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="61" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">PP-Index: using permutation prefixes for efficient and scalable approximate similarity search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Esuli</surname></persName>
		</author>
		<idno>LSDS-IR09</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Workshop on Large-Scale Distributed Systems for Information Retrieval</title>
		<meeting>the 7th Workshop on Large-Scale Distributed Systems for Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Extendible hashing-a fast access method for dynamic files</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fagin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nievergelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pippenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Strong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Database Systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="315" to="344" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Searching in metric spaces by spatial approximation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Navarro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="46" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Near neighbor search in large metric spaces</title>
		<author>
			<persName><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th International Conference on Very Large Data Bases (VLDB&apos;95)</title>
		<editor>
			<persName><forename type="first">U</forename><surname>Dayal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">M D</forename><surname>Gray</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Nishio</surname></persName>
		</editor>
		<meeting>the 21th International Conference on Very Large Data Bases (VLDB&apos;95)<address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">September 11-15, 1995</date>
			<biblScope unit="page" from="574" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The many facets of approximate similarity search</title>
		<author>
			<persName><forename type="first">M</forename><surname>Patella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ciaccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SISAP &apos;08</title>
		<meeting>SISAP &apos;08<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><surname>Aspnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Skip graphs, in: SODA &apos;03: Proceedings of the 14th Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting><address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="384" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Chord: a scalable distributed similarity search structure</title>
		<author>
			<persName><forename type="first">D</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zezula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Conference on Scalable Information Systems (INFOSCALE 2006)</title>
		<meeting>the 1st International Conference on Scalable Information Systems (INFOSCALE 2006)<address><addrLine>Hong Kong; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006-06-01">May 30-June 1. 2006</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">MESSIF: metric similarity search implementation framework</title>
		<author>
			<persName><forename type="first">M</forename><surname>Batko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zezula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First International DELOS Conference</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4877</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pivoting M-Tree: a metric access method for efficient similarity search</title>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Dateso 2004 Annual International Workshop on DAtabases, TExts, Specifications and Objects</title>
		<title level="s">CEUR Workshop Proceedings</title>
		<meeting>the Dateso 2004 Annual International Workshop on DAtabases, TExts, Specifications and Objects<address><addrLine>Desna, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">April 14-16, 2004. 2004</date>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="27" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An efficient access method for similarity search in metric spaces</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ciaccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Patella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zezula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M-Tree</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 23rd International Conference on Very Large Data Bases (VLDB&apos;97)</title>
		<meeting>23rd International Conference on Very Large Data Bases (VLDB&apos;97)<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">August 25-29, 1997. 1997</date>
			<biblScope unit="page" from="426" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Measuring the dimensionality of general metric spaces</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cha ´vez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Navarro</surname></persName>
		</author>
		<idno>TR/DCC-00-1</idno>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Chile</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Figueroa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cha ´vez</surname></persName>
		</author>
		<ptr target="www.sisap.org/Metric_Space_Library.htmlS" />
		<title level="m">Metric spaces library</title>
		<imprint>
			<date type="published" when="2009-12">December 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Slim-trees: high performance metric trees minimizing overlap between nodes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Traina</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J M</forename><surname>Traina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Seeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EDBT 2000</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>EDBT 2000<address><addrLine>Konstanz, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000">March 27-31, 2000. 2000</date>
			<biblScope unit="volume">1777</biblScope>
			<biblScope unit="page" from="51" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pivot selection techniques for proximity searching in metric spaces</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bustos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cha ´vez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="2357" to="2366" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Approximate similarity retrieval with M-Trees</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zezula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Savino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Amato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rabitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="275" to="293" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Approximate similarity search in metric spaces using inverted files</title>
		<author>
			<persName><forename type="first">G</forename><surname>Amato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Savino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">InfoScale &apos;08: Proceedings of the 3rd International Conference on Scalable Information Systems</title>
		<meeting><address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>ICST</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<author>
			<persName><forename type="first">P</forename><surname>Diaconis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Group Representation in Probability and Statistics</title>
		<title level="s">IMS Lecture Notes-Monograph Series</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Effective proximity retrieval by ordering permutations</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cha ´vez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Figueroa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Navarro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1647" to="1658" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Comparing top k lists</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fagin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sivakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Discrete Mathematics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="134" to="160" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dynamic spatial approximation trees</title>
		<author>
			<persName><forename type="first">G</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Reyes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Algorithmics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="68" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">D-index: distance searching index for metric data sets</title>
		<author>
			<persName><forename type="first">V</forename><surname>Dohnal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gennaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Savino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zezula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="33" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Similarity search in high dimensions via hashing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Very Large Data Bases (VLDB&apos;99)</title>
		<meeting>the 25th International Conference on Very Large Data Bases (VLDB&apos;99)</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="518" to="529" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
