<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yang</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Xiaoyan</forename><surname>Sun</surname></persName>
						</author>
						<author role="corresp">
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Yaochu</forename><surname>Jin</surname></persName>
							<email>yaochu.jin@surrey.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yaochu</forename><forename type="middle">Jin</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Communication-Efficient Federated Deep Learning With Layerwise Asynchronous Model Update and Temporally Weighted Aggregation</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Information and Control Engineering</orgName>
								<orgName type="institution">University of Mining and Technology</orgName>
								<address>
									<postCode>221116</postCode>
									<settlement>Xuzhou</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<postCode>639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">China University of Mining and Technology</orgName>
								<address>
									<postCode>221116</postCode>
									<settlement>Xuzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<postCode>GU2 7XH</postCode>
									<settlement>Guildford</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A1FA2D7CB3BF6B9D1F2FC1E7A7832B82</idno>
					<idno type="DOI">10.1109/TNNLS.2019.2953131</idno>
					<note type="submission">received March 7, 2019; revised July 7, 2019; accepted November 1, 2019.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Aggregation</term>
					<term>asynchronous learning</term>
					<term>deep neural network (DNN)</term>
					<term>federated learning</term>
					<term>temporally weighted aggregation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Federated learning obtains a central model on the server by aggregating models trained locally on clients. As a result, federated learning does not require clients to upload their data to the server, thereby preserving the data privacy of the clients. One challenge in federated learning is to reduce the client-server communication since the end devices typically have very limited communication bandwidth. This article presents an enhanced federated learning technique by proposing an asynchronous learning strategy on the clients and a temporally weighted aggregation of the local models on the server. In the asynchronous learning strategy, different layers of the deep neural networks (DNNs) are categorized into shallow and deep layers, and the parameters of the deep layers are updated less frequently than those of the shallow layers. Furthermore, a temporally weighted aggregation strategy is introduced on the server to make use of the previously trained local models, thereby enhancing the accuracy and convergence of the central model. The proposed algorithm is empirically on two data sets with different DNNs. Our results demonstrate that the proposed asynchronous federated deep learning outperforms the baseline algorithm both in terms of communication cost and model accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>and then aggregate the local models on the server. Since all local models are trained upon data that are locally stored in clients, data privacy can be preserved. The whole process of the typical federated learning is divided into communication rounds, in which the local models on the clients are trained on the local data sets. For the kth client, where k ∈ S, and S refers to the participating subset of m clients, its training samples are denoted as P k , and the trained local model is represented by the model parameter vector ω k . In each communication round, only models of the clients belonging to the subset S will download the parameters of the central model from the server and use them as the initial values of the local models. Once the local training is completed, the participating clients send the updated parameters back to the server. Consequently, the central model can be updated by aggregating the updated local models, i.e., ω = Agg(ω k ) <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>.</p><p>In this setting, the local models of each client can be any type of machine learning models, which can be chosen according to the task to be accomplished. In most existing work on federated learning <ref type="bibr" target="#b0">[1]</ref>, deep neural networks (DNNs), e.g., long short-term memory (LSTM), are employed to conduct text-word/text-character prediction tasks. In recent years, DNNs have been successfully applied to many complex problem-solving, including text classification, image classification, and speech recognition <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>. Therefore, DNNs are widely adopted as the local model in federated learning, and the stochastic gradient descent (SGD) is the most popular learning algorithm for training the local models.</p><p>As aforementioned, one communication round includes parameter download (on clients), local training (on clients), trained parameter upload (on clients), and model aggregation (on the server). Such a framework appears to be similar to a distributed machine learning algorithm <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b11">[12]</ref>. In federated learning, however, only the models' parameters are uploaded and downloaded between the clients and server, and the data of local clients are not uploaded to the server or exchanged between the clients. Accordingly, the data privacy of each client can be preserved.</p><p>Compared with other machine learning paradigms, federated learning is subject to the following challenges <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b12">[13]</ref>.</p><p>1) Unbalanced Data: The data amount on different clients may be highly imbalanced because there are light and heavy users. 2) Non-IID Data: The data on the clients may be strongly non-independent and identically distributed (IID) because of the different preferences of different users. As a result, local data sets are not able to represent the overall data distribution, and the local distributions are different from each other, too. The IID assumption in distributed learning that training data are distributed over local clients uniformly at random <ref type="bibr" target="#b13">[14]</ref> usually does not hold in federated learning. 3) Massively Distributed Data: The number of clients is large. For example, the clients may be mobile phone users <ref type="bibr" target="#b1">[2]</ref>, which can be enormous. 4) Unreliable Participating Clients: It is common that a large portion of participating clients is often offline or on unreliable connections. Again in case the clients are mobile phone users, their communication state can vary a lot and, thus, cannot ensure their participation in each round of learning <ref type="bibr" target="#b0">[1]</ref>. Apart from the above-mentioned challenges, the total communication cost is often used as an overall performance indicator of federated learning due to the limited bandwidth and battery capacity of mobile phones. Of course, like other learning algorithms, the learning accuracy, which is mainly determined by the local training and the aggregation strategy, is also of great importance. Accordingly, the motivation of our article is to reduce the communication cost and improve the accuracy of the central model, assuming that DNNs are used as the local learning models. Inspired by interesting observations in fine-tuning of DNNs <ref type="bibr" target="#b14">[15]</ref>, a layerwise asynchronous update strategy for local model updating and aggregation is proposed to improve the communication efficiency in each round. Note that in the field of evolutionary optimization, evolutionary algorithms have also adopted asynchronous strategies for enhancing computational efficiency, e.g., parallel asynchronous particle swarm optimization (PAPSO) <ref type="bibr" target="#b15">[16]</ref>. Concretely, motivations behind our layerwise asynchronous strategy and the one adopted by PAPSO, i.e., higher communication efficiency versus computational efficiency, make the essential difference. Driven by these aforementioned motivations, our proposed algorithm addresses the parameters from shallow and deeps layers in an asynchronous manner, while PAPSO updates particle positions and velocities in parallel and asynchronously.</p><p>The main contributions of this article are as follows. First, an asynchronous strategy that aggregates and updates the parameters in the shallow and deep layers of DNNs at different frequencies is proposed to reduce the number of parameters to be communicated between the server and clients. Second, a temporally weighted aggregation strategy is suggested to more efficiently integrate information of the previously trained local models in model aggregation to enhance the learning performance.</p><p>The remainder of this article is organized as follows. In Section II, related work is briefly reviewed. The detail of the proposed algorithm, especially the asynchronous strategy, the temporally weighted aggregation, and the overall framework are described in Section III. Section IV presents the experimental results and discussions. Finally, conclusions are drawn in Section V.</p><p>II. RELATED WORK Konečný et al. <ref type="bibr" target="#b1">[2]</ref> developed the first framework of federated learning and also experimentally proved that existing Algorithm 1 FedAVG <ref type="bibr" target="#b0">[1]</ref> 1: function SERVEREXECUTION Run on the server return w to server 21: end function machine learning algorithms are not suited for this setting. Konečný et al. <ref type="bibr" target="#b2">[3]</ref> proposed two ways to reduce the uplink communication costs, i.e., structured updates and sketched updates, using data compression/reconstruction techniques. A more recent version of federated learning, federated averaging (FedAVG), was reported in <ref type="bibr" target="#b0">[1]</ref>, which was developed for obtaining a central prediction model of Google's Gboard app and can be embedded in a mobile phone to protect the user's privacy. The pseudocode of FedAVG is provided in Algorithm 1 <ref type="bibr" target="#b0">[1]</ref>.</p><p>In the following, we briefly explain the main components of FedAVG. For clients that do not participate, their models remain unchanged until they are chosen to participate. In model aggregation, the parameters uploaded from clients in the current round and those in previous ones contribute equally to the central model. In FedAVG, parameters in the shallow and deep layers are always synchronously updated. The parameters uploaded from the clients not participating in the current round are the same as in the previous rounds.</p><p>Apart from reducing communication costs, other studies of federated learning have focused on protocol security. For example, to tackle differential attacks, Gayer et al. <ref type="bibr" target="#b16">[17]</ref> proposed an algorithm incorporating a preserving mechanism into federated learning for client-sided differential privacy. Bonawitz et al. <ref type="bibr" target="#b17">[18]</ref> designed an efficient and robust transfer protocol for secure aggregation of high-dimensional data.</p><p>While privacy preserving and reduction of communication costs are two important aspects to take into account in federated learning, the main concern remains to be the enhancement of learning performance of the central model on all data. The loss function of federated learning is defined as</p><formula xml:id="formula_0">F(ω) = K k=1 (n k /n) f k (ω), where f k (ω)</formula><p>is the loss function of the kth client model. Clearly, the performance of federated learning heavily depends on the model aggregation strategy.</p><p>Not much work has been reported on reducing the communication cost by reducing the number of parameters to be uploaded and downloaded between clients and the server except for some recent work reported most recently <ref type="bibr" target="#b18">[19]</ref>. In this article, we present a layerwise asynchronous model learning model that updates only part of the model parameters to reduce communication and a temporally weighted aggregation strategy that gives a higher weight on more recent models in aggregation to enhance learning performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. LAYERWISE ASYNCHRONOUS MODEL UPDATE AND TEMPORALLY WEIGHTED AGGREGATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Layerwise Asynchronous Model Update</head><p>The most intrinsic requirement for decreasing the communication cost of federated learning is to upload/download as little data as possible without deteriorating the performance of the central model. To this end, we present in this article a layerwise asynchronous model update strategy that updates only part of the local model parameters to reduce the amount of data to be uploaded or downloaded.</p><p>Our idea was primarily inspired by the following interesting observations made in fine-tuning DNNs <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b19">[20]</ref>.</p><p>1) Shallow layers in a DNN learn the general features that are applicable to different tasks and data sets, meaning that a relatively small part of the parameters in DNNs (those in the shallow layers) represent features general to different data sets. 2) By contrast, deep layers in a DNN learn ad hoc features related to specific data sets, and a large number of parameters focus on learning features in specific data. These above-mentioned observations indicate that the relatively smaller number of parameters in the shallow layers is more pivotal for the performance of the central model in federated learning. Accordingly, parameters of the shallow layers should be updated more frequently than those parameters in the deep layers. Therefore, the parameters in the shallow and deep layers in the models can be updated asynchronously, thereby reducing the amount of data to be sent between the server and clients. We term this layerwise asynchronous model update.</p><p>The DNN employed for the local models on the clients is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, it can be separated into shallow layers for learning general features and deep layers for learning specific-feature layers features, which are denoted as ω g and ω s , respectively. The sizes of ω g and ω s are denoted as S g and S s , respectively, and typically, S g S s . In the proposed asynchronous learning strategy, ω g will be updated and uploaded/downloaded more frequently than ω s . Assume that the whole federated process is divided into loops, and each loop has T rounds of model updates. In each loop, ω g will be updated and communicated in every round, while ω s will be updated and communicated in only f e rounds, where f e &lt; T . As a result, the number of reduced parameters to be exchanged between the server and clients, i.e., the reduced communication cost, will be f e * S s . This way, the communication cost can be significantly reduced, since S s is usually very large in DNNs.</p><p>An example is given in Fig. <ref type="figure" target="#fig_1">2</ref> to show the asynchronous learning strategy. The abscissa and ordinate denote the communication round and the local client, respectively. In this example, there are five local devices, i.e., {A, B, C, D, E}, and a server. Point (A, t) indicates that client A is participating in updating the global model in round t.</p><p>Fig. <ref type="figure" target="#fig_1">2</ref>(a) provides an illustration of a conventional synchronous aggregation strategy, where both ω g and ω s are uploaded/downloaded in each round. The gray rounded rectangles in the bottom represent the aggregation, in which both shallow and deep layers participate in all rounds. By contrast, Fig. <ref type="figure" target="#fig_1">2(b)</ref> shows the proposed asynchronous learning strategy. In this example, there are six computing rounds (t -5, t -4, . . . , t) in the loop, and the parameters of deep layers are exchanged in rounds t -1 and t only. As a result, the number of reduced parameters to be communicated is 2/3 * S s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Temporally Weighted Aggregation</head><p>In federated learning, the aggregation strategy (line 9 in Algorithm 1) usually weights the importance of each local  <ref type="figure" target="#fig_2">3</ref>) will be weighted by their data size only regardless the computing round in which these models are updated. In other words, local models updated in round tp are as important as those updated in round t, which might not be reasonable.</p><p>In federated learning, however, the training data on each participating client are changing in each round and, therefore, models that are more recently updated should have a larger weight in the aggregation. Accordingly, the following model aggregation method taking into account of timeliness of the  local models is proposed.</p><formula xml:id="formula_1">ω t +1 ← K k=1 n k n * (e/2) -(t -timestamp k ) * ω k (1)</formula><p>where e is the natural logarithm used to depict the time effect, t means the current round, and timestamp k is the round in which the newest ω k was updated. The proposed temporally weighted aggregation is shown in Fig. <ref type="figure" target="#fig_3">4</ref>. Similar to the setting used in Fig. <ref type="figure" target="#fig_2">3</ref>, all clients participating in the aggregation are denoted by an orange dot, while others are represented by blue diamonds. The darker the color is, the larger the weight of this local model will have in the aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Framework</head><p>The framework of the proposed federated learning with a layerwise asynchronous model update and temporally weighted aggregation is shown in Fig. <ref type="figure" target="#fig_4">5</ref>. A detailed description of the main components will be given in Section III-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Temporally Weighted Asynchronous Federated Learning</head><p>The pseudocode for the two main components of the proposed temporally weighted aggregation asynchronous (ASTW) federated learning ASTW_FedAVG, one implemented on the server, and the other on the clients is given in Algorithms 2 and 3, respectively.</p><p>The part to be implemented on the server consists of an initialization step followed by a number of communication rounds. In initialization (Algorithm 2, lines 2-6), the central model ω 0 , timestamps timestamp g , and timestamp s are initialized. Timestamps are stored and to be used to weight the timeliness of corresponding parameters in aggregation.</p><p>The training process is divided into loops. Lines 8-12 in Algorithm 2 set f lag to be true in the last 1/freq rounds in each loop. Assume there are rounds_in_loop rounds in each loop. Lines 13 and 14 randomly select a participating subset S t of the clients according to C, which is the fraction of participating clients per round. In lines 15-24, subfunction Cli entU pdate is called in parallel to get ω k /ω k g , and the corresponding timestamps are updated. f lag specified whether all layers or the shallow layers only will be updated and communicated. Then in lines 25-28, the aggregation is performed to update ω g . Note that compared with 1, a parameter a is introduced into the weighting function in line 25 or line 27 to examine the influence of different weightings in the experiments. In this article, a is set to e or e/2.</p><p>The implementation of the local model update (Algorithm 3) is controlled by three parameters, k, ω, and f lag, where k is the index of the selected client, f lag indicates whether all layers or the shallow layers will be updated. B and E denote the local mini-batch size and the local epoch, respectively. In Algorithm 3, line 2 splits data into batches, whereas lines 3-7 set all layers or shallow layers of the local model to be downloaded according to f lag. In lines 8-12, local SGD is performed. Lines 13-17 return local parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS AND ANALYSES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Design</head><p>We perform a set of experiments using two popular DNN models, i.e., convolution neural networks (CNNs) for image processing and LSTM for human activity recognition (HAR), respectively, to empirically examine the learning performance and communication cost of the proposed ASTW_FedAVG. A CNN with two stacked convolution layers and one fully connected layer <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> is applied on the Modified National Institute of Standards and Technology (MNIST) handwritten digit recognition data set, and an LSTM with two stacked LSTM layers and one fully connected layer is chosen to accomplish the HAR task <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Both the MNIST and the HAR data sets are adapted to test the performance of the proposed federated learning framework for different real-world scenarios, e.g., non-IID distribution, unbalanced amount, and massively decentralized data sets, which will be discussed in detail in Section IV-B.</p><p>FedAVG <ref type="bibr" target="#b0">[1]</ref> is selected as the baseline algorithm since it is the state-of-the-art approach. The proposed ASTW_FedAVG is also compared with two variants, namely, temporally weighted federated learning (TWFL) that adopts temporally weighted aggregation without the layerwise asynchronous model update, and AS_FedAVG that employs the layerwise asynchronous model update without using temporally weighted aggregation. Thus, four algorithms, i.e., FedAVG, ASTW_FedAVG, TW_FedAVG, and AS_FedAVG will be compared in the following experiments.</p><p>The most important parameters in the proposed algorithm are listed in Table <ref type="table">I</ref>. The parameter f req controls the frequency for updating and exchanging the parameters in the deep layers ω s between the server and the local clients in a loop. end for 25:</p><formula xml:id="formula_2">ω g,t +1 ← K k=1 n k n * f g (t, k) * ω k g † 26:</formula><p>if f lag then 27:</p><formula xml:id="formula_3">ω s,t +1 ← K k=1 n k n * f s (t, k) * ω k s ‡ 28:</formula><p>end if</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>29:</head><p>end for 30: end function § rounds_in_loop = 15 and set E S = {11, 12, 13, 14, 0} †</p><formula xml:id="formula_4">f g (t, k) = a -(t -timestamp k g ) ‡ f s (t, k) = a -(t -timestamp k s ) TABLE I PARAMETER SETTINGS</formula><p>For instance, f req 5/15 means that only in the last 5 of the 15 rounds, the parameters in the deep layers ω s will be uploaded/downloaded between the server and clients. a is a parameter for adjusting the time effect in model aggregation. K and m are environmental parameters controlling the scale or complexity of the experiments, K denotes the number of local clients, and m is the number of participating clients per round.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Settings on Data Sets</head><p>As discussed in Section I, the federated learning framework has its particular challenges, such as non-IID, unbalanced, return ω s to server 17:</p><p>end if 18: end function and massively decentralized data sets. Therefore, the data sets used in our experiments should be designed to reflect these challenges. The generation of the client data set is described in detail in Algorithm 4, which is controlled by four parameters Labels, N c , S min , and S max , where N c controls the number of classes in each local data set, S min and S max specify the range of the size of the local data, and Labels indicates the names of classes involved in the corresponding tasks. P class ← weights class sum × num 12: end for 13: P k ← P 1) Handwritten Digit Recognition Using CNN: The MNIST data set has ten different kinds of digits and one digit is a 28 × 28 pixel gray-scale image. To partition the data over local clients, we first sort them by their digit labels and divide them into ten shards, i.e., 0, 1, . . ., 9. Then, Algorithm 4 is performed to compute P k , which is the kth client's partition coefficient corresponding to these shards. In this task, Labels = {0, 1, 2, . . . , 9}; N c is randomly chosen from {2, 3}, given K = 20, S min = 1000 and S max = 1600. For the sake of easy analyses, five partitions/local data sets, namely, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE II PARAMETERS SETTINGS FOR THE CNN</head><p>1@MNIST, 2@MNIST, . . ., 5@MNIST are predefined. Their corresponding 3-D column charts are plotted in Fig. <ref type="figure">6</ref>.</p><p>The architecture of the CNN used for the MNIST task has two 5 × 5 convolution layers (the first one has 32 channels and the other has 64) and 2 × 2 max-pooling layers. Then, a fully connected layer with units and ReLu activation is followed. An output layer is a softmax unit. The parameter settings for CNN are listed in Table <ref type="table">II</ref>.</p><p>2) Human Activity Recognition Using LSTM: In the HAR data set, each data is a sequence of images with a label out of six activities. Similar operations are applied on the HAR data set to divide the data set over local clients. Here, Labels = {0, 1, 2, . . . , 5}; N c is randomly chosen from {2, 3}, given K = 20, S min = 250 and S max = 500.</p><p>The architecture of the LSTM used in this article has two 5 × 5 LSTM layers (the first one with cell_si ze = 20 and  ti me_steps = 128 and the other with cell_si ze = 10 and the same ti me_steps), a fully connected layer with 128 units and the ReLu activation, and a softmax output layer. The corresponding parameters of the LSTM is given in Table <ref type="table" target="#tab_3">III</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results and Analysis</head><p>Two sets of experiments are performed in this subsection. The first set of experiments examines the influence of the most important parameters, f req, a, K , and m, on the performance of the two strategies using the CNN for the 1@MNIST data set. The second set of experiments compares four algorithms in terms of the communication cost and learning accuracy using the LSTM on the HAR data set since the HAR is believed to be more challenging than the MNIST data set.</p><p>1) Effect of the Parameters: The experiments here are not meant for a detailed sensitivity analysis. These experiments and related discussions aim to offer a basic understanding of parameter settings that may be helpful in practice. We mainly conduct research on the parameters listed in Table <ref type="table">I</ref>.</p><p>In investigating the influence of a particular parameter, all others are set to be their default value. Experiment on f req are carried out for f req = {3/15, 5/15, 7/15}, given a = e/2, K = 20, and m = 2.</p><p>Two metrics are adopted in this article for measuring the performance of the compared algorithms. One is the best accuracy of the central model within 200 rounds, and the other is to the required rounds before the central model's accuracy reaches 95.0%. Note that the same computing rounds mean the same communication cost. All experiments are independently run for ten times, and their average (AVG) and standard deviation (STDEV) values are presented in Tables IV-VI. In Tables IV-VI, the average value is listed before the standard deviation in parenthesis. Based on the results in Tables IV-VI, the following observations can be made.</p><p>1) Analysis on freq: From the results presented in Table <ref type="table" target="#tab_4">IV</ref>, we can conclude that the lower the exchange frequency is, the fewer communication costs will be required, which is very much expected. However, a too low f req will deteriorate the accuracy of the central model. 2) Analysis on a: a is a parameter that controls the influence of time effect on the model aggregation. When a is set as e, the more recently updated local models are more heavily weighted in the aggregated model, and a takes the value e/2, the previously updated local models will have a greater impact on the central model. The results in Table <ref type="table" target="#tab_5">V</ref> indicate that e/2 is a better option for CNN on the 1@MNIST data set. When a = 1, the algorithm is reduced to AS_FedAVG, meaning that the parameters uploaded in different rounds will be of equal importance in the aggregation. Both parameters, K and m, leverage the scalability of federated learning. The AVG and STDEV values are calculated based on the recognition accuracy of the CNN on the five predefined data sets. Different combinations of K and m are separately assessed. Based on the results presented in Table VI, the following three conclusions can be made.</p><p>First, a larger number of involved clients (a larger m) leads to higher recognition accuracy. Second, ASTW_FedAVG outperforms FedAVG in most cases, as indicated in the results in the first, third, fourth, and fifth rows in Table <ref type="table" target="#tab_5">VI</ref>. Third, FedAVG is slightly better when K , i.e., the total number of clients, is smaller and C is higher, as shown by the results listed in the second row of the table. This implies that the advantage of the proposed algorithm over the traditional federated learning will become more apparent as the number of clients increases. This is encouraging since, in most real-world problems, the number of clients is typically very large. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Comparison on Accuracy and Communication Cost:</head><p>The following experiments are conducted for testing the overall performance of the algorithms under comparison using the default values of the parameters. Five local data sets are predefined for MNIST and HAR, respectively. For instance, local data set 1 of task MNIST is termed as 1@MNIST. The importance of the temporal weighting is first demonstrated by comparing the changes in the accuracy over the computing rounds. The baseline FedAVG and TW_FedAVG without the asynchronous update are compared, and the results on the MNIST and HAR data sets are shown in Figs. <ref type="figure" target="#fig_8">7</ref> and<ref type="figure" target="#fig_9">8</ref>, respectively.</p><p>The following conclusions can be reached from the results in Figs. <ref type="figure" target="#fig_8">7</ref> and<ref type="figure" target="#fig_9">8</ref>. First, the proposed temporally weighted aggregation helps the central model to converge to an acceptable accuracy. On the 1@MNIST and 2@MNIST data sets, TW_FedAVG needs about 30 communication rounds to achieve an accuracy reaches to 95.0%, while the traditional FedAVG needs about 75 rounds to achieve similar accuracy, leading to a reduction of 40% communication cost. Similar conclusions can also be drawn on data sets 3@MNIST, 4@MNIST, and 5@MNIST, although the accuracy of TW_FedAVG becomes more fluctuating. Second, on the HAR data set, a more difficult task, TW_FedAVG can mostly achieve higher accuracy than FedAVG except on 5@HAR. Notably, TW_FedAVG only needs about 75 communication rounds to achieve an accuracy of 90%, while FedAVG requires about 750 rounds, resulting in a nearly 90% reduction of the communication cost. Even on 5@HAR,  TW_FedAVG shows a much faster convergence than FedAVG in the early stage. Finally, the temporally weighted aggregation may result in some fluctuations in the learning performance, which may be attributed to the fact that the contributions of some high-quality local models are less weighted in some communication rounds.</p><p>Finally, the comparative results of the four algorithms on ten test cases generated from MNIST and HAR tasks are given in Table <ref type="table" target="#tab_6">VII</ref>. The listed metrics include the number of rounds needed, the classification accuracy (listed in parenthesis), and total communication cost (Cost. C for short). From these results, the following observations can be made.</p><p>1) Both ASTW_FedAVG and TW_FedAVG outperform FedAVG in most cases in terms of the total number of rounds, the best accuracy, and the total communication cost. 2) TW_FedAVG achieves the best performance on most tasks in terms of the total number of rounds and the best accuracy. The temporally weighted aggregation strategy accelerates the convergence of the learning and improved the learning performance. 3) ASTW_FedAVG performs slightly better than TW_FedAVG on MNIST in terms of the total communication cost, while TW_FedAVG works better than ASTW_FedAVG on the HAR data sets. The layerwise asynchronous model update strategy significantly contributes to reducing the communication cost per round. 4) AS_FedAVG performs the worst among the four compared algorithms. When comparing the performance of the AS_FedAVG only adopting the asynchronous strategy and the ASTW_FedAVG using both of them, the asynchronous one always needs the help of the other strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>This article aims to reduce the communication costs and improve the learning performance of federated learning by suggesting an asynchronous model update strategy and a temporally weighted aggregation method. Empirical studies comparing the performance and communication costs of the canonical federated learning and the proposed federated learning on the MNIST and human action recognition data sets demonstrate that the proposed asynchronous federated learning with temporally weighted aggregation outperforms the canonical one in terms of both learning performance and communication costs. IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS This article follows the assumption that all local models adopt the same neural network architecture and share the same hyperparameters, such as the learning rate of SGD. In future research, we are going to develop new federated learning algorithms allowing clients to evolve their local models to further improve the learning performance and reduce communication costs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of shallow and deep layers of a DNN.</figDesc><graphic coords="3,321.95,58.13,230.30,136.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Parameter exchange. (a) Synchronous model update strategy. (b) Asynchronous model update strategy.</figDesc><graphic coords="4,53.03,258.89,242.78,182.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Conventional aggregation strategy.</figDesc><graphic coords="4,321.47,59.09,231.86,173.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Illustration of the temporally weighted aggregation.</figDesc><graphic coords="4,323.51,254.45,227.66,170.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Federated learning with a layerwise asynchronous model update and temporally weighted aggregation.</figDesc><graphic coords="5,74.99,58.73,461.42,238.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 3 2 :for each local epoch i from 1 to E do 9 :</head><label>329</label><figDesc>Client Component of ASTW_FedAVG 1: function CLIENTUPDATE(k, w, f lag) Run on client k B ← (split P k into batches of size B) for batch b ∈ B do 10: ω ← ω -η * (w; b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 4 6 :</head><label>46</label><figDesc>Generation of Local Data Sets. Input: Labels, N C , S min , and S max Output: non-IID and unbalanced local dataset P k 1: classes ← Choices(Labels,N C ) 2: L ← Len(Labels) 3: weights ← Zeros(L) 4: P ← Zeros(L) 5: for each class ∈ classes do weights class ← Random(0,1) 7: end for 8: sum ← L class=1 weights class 9: num ← Random(S min , S max ) 10: for each class ∈ classes do 11:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 . 3 -</head><label>63</label><figDesc>Fig. 6. 3-D column charts of pregenerated data sets. (a) 1@MNIST. (b) 2@MNIST. (c) 3@MNIST. (d) 4@MNIST. (e) 5@MNIST.</figDesc><graphic coords="7,176.99,288.76,111.70,68.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Comparative studies on temporally weighted aggregation on data set MNIST using the CNN. (a) 1@MNIST. (b) 2@MNIST. (c) 3@MNIST. (d) 4@MNIST. (e) 5@MNIST.</figDesc><graphic coords="8,318.71,283.85,113.90,55.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Comparative studies on temporally weighted aggregation on data set HAR using the LSTM. (a) 1@HAR. (b) 2@HAR. (c) 3@HAR. (d) 4@HAR. (e) 5@HAR.</figDesc><graphic coords="9,57.23,524.57,112.34,55.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>1) Server Execution consists of the initialization and</head><label></label><figDesc></figDesc><table><row><cell>communication rounds.</cell><cell></cell></row><row><cell cols="2">a) Initialization: Line 2 initializes parameter ω 0 .</cell></row><row><cell cols="2">b) Communication Rounds: Line 4 obtains m, the number</cell></row><row><cell cols="2">of participating clients; K indicates the number of local</cell></row><row><cell cols="2">clients, and C corresponds to the fraction of participat-</cell></row><row><cell cols="2">ing clients per round, according to which line 5 ran-</cell></row><row><cell cols="2">domly selects participating subset S t . In lines 6-8,</cell></row><row><cell cols="2">subfunction Cli entU pdate is called in parallel to</cell></row><row><cell cols="2">get ω k t +1 . Line 9 executes aggregation to update ω t +1 .</cell></row><row><cell cols="2">2) Client Update: The subfunction gets k and ω. B and</cell></row><row><cell cols="2">E are the local mini-batch size and the number of local</cell></row><row><cell cols="2">epochs, respectively. η is the learning rate. Line 14 splits</cell></row><row><cell cols="2">data into batches; where P k corresponds to the set of</cell></row><row><cell cols="2">indices of the data points on client k, with n k = |P k |.</cell></row><row><cell cols="2">Lines 15-19 execute the local SGD on batches. Line 20</cell></row><row><cell>returns the local parameters.</cell><cell></cell></row><row><cell>The equation in line 9, ω t +1 ←</cell><cell>K k=1 (n k /n)  *  ω k t +1 ,</cell></row><row><cell cols="2">described the aggregation strategy, where n k is the sample</cell></row><row><cell cols="2">size of the kth client and n is the total number of training</cell></row><row><cell cols="2">samples. ω k t +1 comes from client k in round t + 1; however,</cell></row><row><cell cols="2">it is not always updated in the current communication round.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS</figDesc><table><row><cell cols="3">Algorithm 2 Server Component of ASTW_FedAVG</cell></row><row><cell cols="2">1: function SERVEREXECUTION</cell><cell>Run on the server</cell></row><row><cell>2:</cell><cell>initialize ω 0</cell></row><row><cell>3:</cell><cell>for each client k ∈ {1, 2, . . . , K } do</cell></row><row><cell>4:</cell><cell>ti mestamp k g ← 0</cell></row><row><cell>5:</cell><cell>ti mestamp k s ← 0</cell></row><row><cell>6:</cell><cell>end for</cell></row><row><cell>7:</cell><cell>for each round t = 1, 2, . . . do</cell></row><row><cell>8:</cell><cell cols="2">if t mod rounds_in_loop ∈ set E S then  §</cell></row><row><cell>9:</cell><cell>f lag ← True</cell></row><row><cell>10:</cell><cell>else</cell></row><row><cell>11:</cell><cell>f lag ← False</cell></row><row><cell>12:</cell><cell>end if</cell></row><row><cell>13:</cell><cell>m ← max(C  *  K , 1)</cell></row><row><cell>14:</cell><cell>S t ← (random set of m clients)</cell></row><row><cell>15:</cell><cell cols="2">for each client k ∈ S t in parallel do</cell></row><row><cell>16:</cell><cell>if f lag then</cell></row><row><cell>17:</cell><cell></cell></row><row><cell>18:</cell><cell>ti mestamp k g ← t</cell></row><row><cell>19:</cell><cell>ti mestamp k s ← t</cell></row><row><cell>20:</cell><cell>else</cell></row><row><cell>21:</cell><cell cols="2">ω k g ← ClientUpdate(k, ω g,t , f lag)</cell></row><row><cell>22:</cell><cell>ti mestamp k g ← t</cell></row><row><cell>23:</cell><cell>end if</cell></row><row><cell>24:</cell><cell></cell></row></table><note><p>ω k ← ClientUpdate(k, ω t , f lag)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III PARAMETER</head><label>III</label><figDesc>SETTINGS FOR THE LSTM</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV EXPERIMENTAL</head><label>IV</label><figDesc>RESULTS ON f req</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V EXPERIMENTAL</head><label>V</label><figDesc>RESULTS ON a TABLE VI EXPERIMENTAL RESULTS ON THE SCALABILITY OF THE ALGORITHM</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII EXPERIMENTS</head><label>VII</label><figDesc>ON PERFORMANCE</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the National Natural Science Foundation of China under Grant 61876184 and Grant 61473298.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Communication-efficient learning of deep networks from decentralized data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A Y</forename><surname>Arcas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. Artif. Intell. Statist</title>
		<imprint>
			<biblScope unit="page" from="1273" to="1282" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Federated optimization: Distributed optimization beyond the datacenter</title>
		<author>
			<persName><forename type="first">J</forename><surname>Konecný</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03575</idno>
		<ptr target="https://arxiv.org/abs/1511.03575" />
		<imprint>
			<date type="published" when="2015-11">Nov. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Federated learning: Strategies for improving communication efficiency</title>
		<author>
			<persName><forename type="first">J</forename><surname>Konecný</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bacon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-10">Oct. 2016</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for computeraided detection: Cnn architectures, dataset characteristics and transfer learning</title>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1285" to="1298" />
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">LSTM: A search space odyssey</title>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Koutnìk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2222" to="2232" />
			<date type="published" when="2017-10">Oct. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distributed optimization with arbitrary local solvers</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optim. Methods Softw</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="813" to="848" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Aide: Fast and communication efficient distributed optimization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konecný</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Póczós</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06879</idno>
		<ptr target="https://arxiv.org/abs/1608.06879" />
		<imprint>
			<date type="published" when="2016-08">Aug. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Communication-efficient distributed optimization using an approximate newton-type method</title>
		<author>
			<persName><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2014-01">Jan. 2014</date>
			<biblScope unit="page" from="1000" to="1008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Disco: Distributed optimization for selfconcordant empirical loss</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="362" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Project adam: Building an efficient and scalable deep learning training system</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chilimbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Suzue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Apacible</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kalyanaraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. OSDI</title>
		<meeting>OSDI</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="571" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large scale distributed deep networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1223" to="1231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Federated optimization: Distributed machine learning for on-device intelligence</title>
		<author>
			<persName><forename type="first">J</forename><surname>Konecný</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Richtárik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02527</idno>
		<ptr target="https://arxiv.org/abs/1610.02527" />
		<imprint>
			<date type="published" when="2016-10">Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="122" />
			<date type="published" when="2011-01">Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Parallel asynchronous particle swarm optimization</title>
		<author>
			<persName><forename type="first">B.-I</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Haftka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Fregly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Numer. Methods Eng</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="578" to="595" />
			<date type="published" when="2006-07">Jul. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Differentially private federated learning: A client level perspective</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nabi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.07557</idno>
		<ptr target="https://arxiv.org/abs/1712.07557" />
		<imprint>
			<date type="published" when="2017-12">Dec. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Practical secure aggregation for privacy-preserving machine learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bonawitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGSAC Conf</title>
		<meeting>ACM SIGSAC Conf</meeting>
		<imprint>
			<date type="published" when="2017-11">Oct./Nov. 2017</date>
			<biblScope unit="page" from="1175" to="1191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-objective evolutionary federated learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. neural Netw. Learn. Syst</title>
		<imprint/>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">One weird trick for parallelizing convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.5997</idno>
		<ptr target="https://arxiv.org/abs/1404.5997" />
		<imprint>
			<date type="published" when="2014-04">Apr. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998-11">Nov. 1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A public domain dataset for human activity recognition using smartphones</title>
		<author>
			<persName><forename type="first">D</forename><surname>Anguita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Oneto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Reyes-Ortiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ESANN</title>
		<meeting>ESANN</meeting>
		<imprint>
			<date type="published" when="2013-04">Apr. 2013</date>
			<biblScope unit="page" from="437" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to forget: Continual prediction with LSTM</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Int. Conf. Artif. Neural Netw</title>
		<meeting>9th Int. Conf. Artif. Neural Netw</meeting>
		<imprint>
			<date type="published" when="1999-09">Sep. 1999</date>
			<biblScope unit="page" from="850" to="855" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
