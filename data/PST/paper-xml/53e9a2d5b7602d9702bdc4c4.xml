<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bag-of-Features HMMs for Segmentation-free Word Spotting in Handwritten Documents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Leonard</forename><surname>Rothacker</surname></persName>
							<email>leonard.rothacker@udo.edu</email>
						</author>
						<author>
							<persName><forename type="first">Gernot</forename><forename type="middle">A</forename><surname>Fink</surname></persName>
							<email>gernot.fink@udo.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">TU Dortmund University</orgName>
								<address>
									<postCode>44221</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science Edifici O</orgName>
								<orgName type="institution">Universitat Autònoma de</orgName>
								<address>
									<postCode>08193</postCode>
									<settlement>Barcelona, Bellaterra</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">TU Dortmund University</orgName>
								<address>
									<postCode>44221</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bag-of-Features HMMs for Segmentation-free Word Spotting in Handwritten Documents</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">07E29DFBDF6E276F370AE9C5185E90BF</idno>
					<idno type="DOI">10.1109/ICDAR.2013.264</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent HMM-based approaches to handwritten word spotting require large amounts of learning samples and mostly rely on a prior segmentation of the document. We propose to use Bag-of-Features HMMs in a patch-based segmentation-free framework that are estimated by a single sample. Bag-of-Features HMMs use statistics of local image feature representatives. Therefore they can be considered as a variant of discrete HMMs allowing to model the observation of a number of features at a point in time. The discrete nature enables us to estimate a query model with only a single example of the query provided by the user. This makes our method very flexible with respect to the availability of training data. Furthermore, we are able to outperform state-of-the-art results on the George Washington dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Word spotting methods aim at providing efficient access to textual information without the need to exhaustively process and recognize the whole document contents. They are especially suitable for handwritten documents, for which text recognizers do not yet achieve satisfactory transcription results. Huge amounts of cultural information are still locked in image libraries, because they cannot be efficiently exploited. This has motivated the document analysis community to deal with the handwritten word spotting problem for more than fifteen years.</p><p>Inspired by the handwriting recognition field, many handwritten word spotting approaches take advantage of the sequential nature of handwritten text. They model word images as a sequence of column-wise extracted features that are finally matched by a sequence alignment technique. The most widely used features in the literature are word profiles <ref type="bibr" target="#b0">[1]</ref> or features combining global and local information. Very prominent representations have been proposed by Marti and Bunke <ref type="bibr" target="#b1">[2]</ref> and Vinciarelli et al. <ref type="bibr" target="#b2">[3]</ref>. Regarding the feature alignment, techniques such as dynamic time warping <ref type="bibr" target="#b0">[1]</ref>, Hidden Markov models (HMM) <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> or neural networks <ref type="bibr" target="#b5">[6]</ref> have been used for spotting purposes.</p><p>However, such techniques rely on preprocessing methods. In the layout analysis step the document is usually segmented at word or line level. In the normalization step unwanted variabilities are removed. The major problem is that failures in preprocessing regularly lead to failures in the recognition. Especially, accurate word segmentations are difficult to obtain in handwritten and degraded documents. For that reason, several segmentation-free word spotting techniques have emerged.</p><p>The most common approach <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> is to use a patchbased framework in which a window slides over the whole document. In such a framework perfect segmentations are not expected and elements from surrounding words will appear within a patch. With respect to the normalization the most common methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref> include binarization, skew and slant correction, baseline height and word width normalizations. Representations, like profile-based features, tend to be very sensitive with respect to the successful application of these preprocessing methods. Artifacts from the segmentation introduce further variabilities. Therefore, they are hardly applicable in the scenario considered here. In previous publications like <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b10">[11]</ref> it was proposed to use features from the object recognition field as a basis for representing both handwritten and typewritten words. For instance, the work of Rodríguez-Serrano and Perronnin <ref type="bibr" target="#b9">[10]</ref> was one of the first contributions that proposed to use local gradient histogram features (SIFTlike, cf. <ref type="bibr" target="#b11">[12]</ref>) for word spotting. The results obtained clearly outperformed profile-based features. A major advantage of using these gradient statistics is that they usually do not require extensive preprocessing. In <ref type="bibr" target="#b7">[8]</ref> neither normalization nor segmentation techniques have been applied.</p><p>In this paper we propose to use a Bag-of-Features representation powered with SIFT descriptors <ref type="bibr" target="#b11">[12]</ref> to feed an HMM as previously proposed in <ref type="bibr" target="#b10">[11]</ref>. The Bag-of-Features HMM is a generative finite state machine. Its distinctive characteristic is its output modeling. At each point in time it generates a Bagof-Features representation. In comparison to a discrete HMM this can be interpreted as generating multiple instead of only a single symbol with respective probabilities. These HMMs are used in a patch-based framework in order to bypass the segmentation step. The descriptors are directly computed on the gray-scale document image, omitting any prior normalization. Since HMMs encode the sequential information of the extracted features as well, we obtain better results than just applying a Bag-of-Features model as in <ref type="bibr" target="#b7">[8]</ref>.</p><p>Bag-of-Features and HMMs have not been used in an integrated methodology for word spotting before. In our approach we are able to combine the advantages and avoid the disadvantages of both techniques. In <ref type="bibr" target="#b7">[8]</ref> it was shown that Bag-of-Features representations can successfully be applied in a segmentation-free word spotting scenario. As for visual recognition in general (cf. <ref type="bibr" target="#b12">[13]</ref>), these representations tend to be very robust with respect to different variabilities. This is a consequence of the properties of the SIFT descriptor (cf. <ref type="bibr" target="#b11">[12]</ref>) and the clustering step necessary to obtain the features that a Bag-of-Features model is built upon. Furthermore, they do not encode any specific spatial information what makes them also robust against larger transformations. However, it has been shown that for visual recognition tasks the complete lack of spatial information is not helpful. Capturing loose spatial context is improving the results significantly (cf. <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b7">[8]</ref>). For the majority of word spotting methods, on the other hand, it is a standard approach to model spatial information by sequences of feature vectors. As described before, these features are usually very sensitive with respect to segmentation errors and writing styles. Often this requires complex models with a high number of free parameters that have to be estimated from training samples. Given the sequential structure and high variability of handwritten script we propose a method that deals with both aspects by combining a statistical sequence model with a statistical model of unordered local features. Our HMMs are initialized and trained with standard algorithms (Baum-Welch) although we only use a single sample. Therefore, our method will profit immediately in scenarios where more training material is available.</p><p>In summary, the main contribution of this paper is the proposal of a segmentation-free word spotting method that integrates the Bag-of-Features principle with a statistical sequence model. In addition, the statistical model is estimated only from the query itself. In that sense the method is completely unsupervised as it does not need any prior word transcription.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BAG-OF-FEATURES HMMS FOR SEGMENTATION-FREE WORD SPOTTING</head><p>In a query-by-example word spotting scenario we have to query a document collection with a sample word image. Our proposed method can be divided into three parts. First, the document images must be represented by local feature representatives (Section II-A). In the second step a query model is created by estimating a Bag-of-Features HMM. Only the provided sample is used for that purpose. The model is initialized and trained on the sequence of Bag-of-Features representations obtained from the bounding box around the query word image (Section II-B). In the last step the model is decoded on the complete document collection. In order to bypass the segmentation step, patches are densely sampled from all pages. For each patch the maximum probability of generating the respective Bag-of-Features sequence is computed (Section II-C). For the final word retrieval, we order the patches with respect to their scores and evaluate the method's performance. Figure <ref type="figure" target="#fig_0">1</ref> shows an overview of the depicted process by visualizing the respective steps on a small section from a document of the George Washington dataset (cf. <ref type="bibr" target="#b0">[1]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Document image representation</head><p>A Bag-of-Features representation is a histogram of features that are representative for the problem domain. The first step for obtaining those features is to densely extract SIFT descriptors on each page of the document collection. The descriptors are located on a regular grid of 5 × 5 pixels and have a uniform scale of 40 × 40 pixels. Please note that no scale space is involved. Given the nature of handwritten script, it is also important to set the descriptor orientation to zero (cf.</p><p>[8], <ref type="bibr" target="#b10">[11]</ref>), since we want to discriminate between horizontal and vertical pen strokes. Figure <ref type="figure" target="#fig_0">1</ref> (1, 2) shows a section of a document and the dense grid of SIFT descriptors. In the next step, we can estimate features that are representative for the document corpus. Therefore, the Generalized Lloyd <ref type="bibr" target="#b14">[15]</ref> clustering algorithm is applied on randomly sampled 5% of all the extracted descriptors in order to build the codebook. Afterwards, we quantize each descriptor in the document collection with respect to the most similar feature representative. In our experiments we choose a codebook size of 4096. Figure <ref type="figure" target="#fig_0">1</ref> (3) exemplarily visualizes the quantization result. Each dot corresponds to a descriptor location. Their colors indicate the respective representatives from the codebook. Please note the similar color patterns in visually similar parts of the document. The choice of the parameters is derived from prior experiments based on <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model estimation</head><p>A query is defined by a bounding box around a word in the document image. In Figure <ref type="figure" target="#fig_0">1</ref> (4) this is exemplarily shown for the word the. The objective is now to estimate a Bag-of-Features HMM that encodes its sequential visual appearance. The description of the visual appearance is already given by the features that are located within the query bounding box. In order to reduce noise caused by the adjacent upper and lower text lines, only features not extending farther than the upper and lower boundaries of the query are used. On the other hand, features overlapping with the left and right query boundaries are kept. This way the beginning and ending of the word can be modeled. The sequential visual appearance is then created by sliding a window over the feature grid. For each position, a Bag-of-Features histogram is computed from the features within the window, in the following referred to as term vector. Additionally, the term vector is normalized to unit length. This is important for the integration with a probabilistic model.</p><p>For estimating the HMM, only the query's term vector sequence is used. We use a linear topology which only allows transitions to the same and the successive state. The first step in the initialization process determines the number of states with respect to the length of the sequence. Then, the term vectors are linearly mapped to the states. Transition and output probabilities are defined for each state i. Let N i be the number of associated term vectors. The probabilities for self-transition and transition to the successive state j are given by</p><formula xml:id="formula_0">a ii = N i -1 N i , a ij = 1 N i (1)</formula><p>Thus, a probabilistic process depending on N i activations of state i is described. Finally, for N i associated term vectors f n the output probability vector c i for feature representatives in state i is given by</p><formula xml:id="formula_1">c i = 1 N i Ni-1 n=0 f n (2)</formula><p>The output coefficients c i are, therefore, obtained by averaging the observed term vectors. After initializing the model, it can be refined with Baum-Welch training <ref type="bibr" target="#b15">[16]</ref>. In this process the model's probability of generating the query's term vector sequence is optimized by an iterative maximum-likelihood procedure. Figure <ref type="figure" target="#fig_0">1</ref> (4) visualizes the estimated model exemplarily. The color distributions indicate the feature probabilities in each state. Please note that the described estimation process can directly be adapted to cases where multiple training samples are available. The number of states is then calculated with respect to the shortest observed term vector sequence.</p><p>After initially associating each sample's term vectors with the respective states, the transition and output probabilities are computed analogously.</p><p>In the model estimation a number of parameters have to be defined. The sliding window width is set to 5 pixels and the window shift to 2 pixels based on the feature grid resolution and prior experiments from <ref type="bibr" target="#b10">[11]</ref>. The effect of pruning boundary features in the query bounding box, the number of HMM states and the number of Baum-Welch training iterations are thoroughly studied in Section III.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Model decoding</head><p>After the model is estimated, the document collection can be queried in a patch-based approach. From each page in the document collection patches of the same size as the query are sampled. For efficiency reasons the patch shift is 50% of the patch size. Figure <ref type="figure" target="#fig_0">1</ref> (5) illustrates this for the example document section. In order to decode the query model with the Viterbi algorithm, a term vector sequence is obtained from each patch. As these vector sequences have always the same length, scores can directly be compared with each other when computing the optimal path probability. For further detail on how to obtain the output probability given an observed term vector, refer to <ref type="bibr" target="#b10">[11]</ref>. An interpolated map of probabilistic scores is shown in Figure <ref type="figure" target="#fig_0">1</ref>  <ref type="bibr" target="#b5">(6)</ref>. Given the probabilistic scores, the query results can be retrieved. For that purpose overlapping patches are eliminated by non-maximum-suppression. Finally, the remaining top 200 patches per page are returned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EVALUATION</head><p>As in <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b8">[9]</ref> we evaluate our method on the George Washington dataset <ref type="bibr" target="#b0">[1]</ref>. The document collection contains 20 handwritten pages with available annotations for 4860 words. All these words are used as queries without any further ground truth modifications, like stemming or stopword filtering. Our results are reported in terms of mean average precision (mAP) and mean recall (mR). For a single query we, therefore, order all retrieved patches from all pages with respect to their Viterbi scores. A patch is considered relevant if it overlaps with a positive patch from the ground truth by more than 20%. The overlap is measured as fraction of the intersecting area and the enclosing area spanned by the two patches. The considered overlap threshold is the only difference in the evaluation protocol compared to <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. This is due to the larger patch shifts for larger query words (cf. Section II-C). For a comparable threshold (of 50%) the ground truth is often not hit as precisely as necessary. As this can be seen as a limitation on the one hand, it emphasizes the robustness of our method on the other hand. Achieving high mean average precision scores is more difficult if relevant patches do not overlap as much with the actual word in the document image. Please note that as in <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> the query itself might be among the retrieved patches. No filtering as e.g. in <ref type="bibr" target="#b0">[1]</ref> is applied.</p><p>In our experiments we focus on the evaluation of parameters for modeling queries with HMMs. As discussed in Section II-B, features have to be selected in the bounding box of the query word image. The term vector sequence obtained with a sliding window is then modeled by a number of HMM states. Finally, the model is optimized with Baum-Welch training. In the evaluation only one parameter is varied at once. In Tables I, II and III the respective best observed reference scores and their parameters are marked in bold. In the following we will discuss the effect of different parameters on the mean average precision. Effects on the mean recall are not as evident and behave consistently over the experiments.</p><p>Table <ref type="table">I</ref> shows results for different feature selection schemes. No feature pruning refers to the scheme where all features located within the query's bounding box are used in the term vector computation. Vertical pruning refers to selecting only those features whose local image descriptors do not extend beyond the lower or upper query boundary. Horizontal and vertical feature pruning finally refers to the scheme where feature descriptors are not allowed to extend beyond any query boundary. The results clearly show the benefit of pruning features at the top and bottom. This way the noise resulting from the adjacent text lines can be be reduced. The mean average precision drops by an absolute 4.7% when additionally pruning features at the sides. Figure <ref type="figure" target="#fig_1">2a</ref> shows that this is due to the decreased performance of shorter words. These are very likely to appear within longer words. For instance, the can be found in they, them, other etc. It is, therefore, especially important to model those words' contexts in the sentence. For longer words Figure <ref type="figure" target="#fig_1">2a</ref> shows the importance of feature pruning. The more characters a query contains the more context from the upper and lower text line is encoded in the model, otherwise.</p><p>Table <ref type="table">II</ref> shows the influence of different model lengths on the retrieval performance. As described in Section II-B, the number of states in the HMM is estimated based on the number of frames, i.e. different sliding window positions, within the query's bounding box. The respective parameter is given as a percental value of the frame number. As can be seen in Figure <ref type="figure" target="#fig_1">2b</ref>, a 3% scaling factor works best for long queries and a scaling factor of 17% works best for short queries. The values have been determined in prior experiments. However, the best overall performance was achieved with a linear combination of the two variants. The considerable difference in mean average precision between a uniform and a query length dependent scaling theme clearly demonstrates the importance of this parameter. It controls how much of the query word is modeled by a single state. The smaller the modeled segments, the more accurate the HMM representation. Generally, a trade-off between specificity and generality must be found. In our given experimental scenario, we can observe (see Figure <ref type="figure" target="#fig_1">2b</ref>) that a higher scaling factor works well for short queries and that performance decreases once the queries get longer. This can be explained with our patch-based configuration (cf. Section II-C). For efficiency reasons the patch shift is proportional to the patch size. Longer words are, therefore, often not aligned with the patches. By reducing the number of states the representation does not enforce a strict sequence of term vectors. We rather model the possible occurrence of different term vectors in fewer states. This gives us the flexibility of generating high scores even though only a part of a queried word is observed. Adapting the state number with respect to the query length allows us to address both requirements. Figure <ref type="figure" target="#fig_1">2b</ref> shows how the scores obtained with the linear combination match the performances for short and long query words obtained with the respective uniform scaling approaches.</p><p>Table <ref type="table" target="#tab_1">III</ref> shows the effect of Baum-Welch training. In the training process the probability of observing the query's term vector sequence is optimized. Similarly, as for the number of model states a compromise between specificity and generality must be found. For the considered benchmark on the George Washington dataset it is found after three iterations.</p><p>Finally, Table <ref type="table" target="#tab_2">IV</ref> shows a performance overview of other state-of-the-art word spotting methods. The work of Rath and Manmatha <ref type="bibr" target="#b0">[1]</ref> is often considered as a baseline experiment. However, we cannot directly compare it to our method due to the different prerequisites and the different dataset configurations. A comparison against the other two methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b7">[8]</ref> is possible, because here these requirements are met. Regarding the evaluation, we almost use the same protocol. Relative improvements of 12.3% <ref type="bibr" target="#b8">[9]</ref> and 101.0% <ref type="bibr" target="#b7">[8]</ref> clearly show how our proposed method outperforms the state-of-the-art results. The difference is especially noticeable for shorter words. The proposed strategy shares the Bag-of-Features application with <ref type="bibr" target="#b7">[8]</ref> and the query-size dependent patch-based framework with <ref type="bibr" target="#b8">[9]</ref>. The results are therefore suited to demonstrate the benefit of using Bag-of-Features HMMs in a segmentation-free word spotting scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In this paper we proposed a novel method for segmentationfree word spotting. With our Bag-of-Features HMMs we were able to outperform state-of-the-art methods on the George The red diamond markers indicate the configuration for the overall optimal scores found in the evaluation. Please note that the 100% mean average precision for 14 characters word length can be considered an artifact. Only a single word exists in this category. Generally, the average character number is 4.4 leading to a respective influence of words in that order of length when computing the mean average precision over all queries. Best viewed in color.   <ref type="bibr" target="#b8">[9]</ref> 20 pages, 4856 queries 54.4% Rusiñol et al. <ref type="bibr" target="#b7">[8]</ref> 20 pages, 4860 queries 30.4%</p><p>Washington dataset. In comparison we could demonstrate that the retrieval performance for shorter words has been improved. This can be regarded as especially challenging. In addition, we only need a single sample for estimating our query model. Continuous HMMs, in contrast, require substantial amounts of training data. Future research could investigate to which extent word spotting performance can be improved when estimating Bag-of-Features HMMs based on a set of training samples.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Overview of the proposed segmentation-free word spotting method. In the upper part of the Figure the document image representation is visualized. In the lower part the estimation of a query model and the patch-based decoding with respect to that model are shown. Patch-based representations are evaluated at each grid point. The scores obtained are visualized by interpolating them over the document image indicating low to high responses with blue to red colors. Best viewed in color. Based on image from the George Washington dataset [1].</figDesc><graphic coords="3,390.84,252.05,160.76,146.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure2. Plots 2a and 2b show the effect of feature pruning and the number of HMM states with respect to different word lengths. Markers are used to visualize mean average precision scores for different parameterizations and discrete numbers of characters. Dashed lines are only drawn for an easier visual comparison. The red diamond markers indicate the configuration for the overall optimal scores found in the evaluation. Please note that the 100% mean average precision for 14 characters word length can be considered an artifact. Only a single word exists in this category. Generally, the average character number is 4.4 leading to a respective influence of words in that order of length when computing the mean average precision over all queries. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table III .</head><label>III</label><figDesc>INFLUENCE OF BAUM-WELCH TRAINING</figDesc><table><row><cell>Baum-Welch iterations</cell><cell>mAP</cell><cell>mR</cell></row><row><cell>0</cell><cell>5 9 .5%</cell><cell>95.1%</cell></row><row><cell>1</cell><cell>6 0 .3%</cell><cell>95.3%</cell></row><row><cell>3</cell><cell>6 1 .1%</cell><cell>95.5%</cell></row><row><cell>5</cell><cell>6 0 .5%</cell><cell>95.5%</cell></row><row><cell>7</cell><cell>5 9 .0%</cell><cell>95.3%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table IV .</head><label>IV</label><figDesc>COMPARISON OF STATE-OF-THE-ART WORD SPOTTING RESULTS FOR THE GEORGE WASHINGTON DATASET</figDesc><table><row><cell>Method</cell><cell>GW dataset configuration</cell><cell>mAP</cell></row><row><cell>Rath and Manmatha [1]</cell><cell>10 pages, 2381 queries</cell><cell>40.9%</cell></row><row><cell>Proposed</cell><cell>20 pages, 4860 queries</cell><cell>61.1%</cell></row><row><cell>Almazán et al.</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Word spotting for historical documents</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal on Document Analysis and Recognition</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page" from="139" to="152" />
			<date type="published" when="2007-04">April 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Using a statistical language model to improve the performance of an HMM-based cursive handwriting recognition system</title>
		<author>
			<persName><forename type="first">U</forename><surname>Marti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="90" />
			<date type="published" when="2001-02">February 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Offline recognition of unconstrained handwritten texts using HMMs and statistical language models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vinciarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="709" to="720" />
			<date type="published" when="2004-06">June 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Handwritten word-spotting using hidden Markov models and universal vocabularies</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rodríguez-Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2106" to="2116" />
			<date type="published" when="2009-09">September 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lexicon-free handwritten word spotting using character HMMs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Frinken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="934" to="942" />
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A novel word spotting method based on recurrent neural networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Frinken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="211" to="224" />
			<date type="published" when="2012-02">February 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Segmentation-free word spotting in historical printed documents</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pratikakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Document Analysis and Recognition</title>
		<meeting>of the Int. Conf. on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="271" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Browsing heterogeneous document collections by a segmentation-free word spotting method</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rusiñol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Aldavert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Toledo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lladós</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Document Analysis and Recognition</title>
		<meeting>of the Int. Conf. on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="63" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient exemplar word spotting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Almazán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fornés</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Valveny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the British Machine Vision Conf</title>
		<meeting>of the British Machine Vision Conf</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="67" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Local gradient histogram features for word spotting in unconstrained handwritten documents</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rodríguez-Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Frontiers in Handwriting Recognition</title>
		<meeting>of the Int. Conf. on Frontiers in Handwriting Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bag-of-features representations for offline handwriting recognition applied to Arabic script</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rothacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Frontiers in Handwriting Recognition</title>
		<meeting>of the Int. Conf. on Frontiers in Handwriting Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Introduction to the bag of features paradigm for image classification and retrieval</title>
		<author>
			<persName><forename type="first">S</forename><surname>O'hara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1101.3354v1</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Comp. Soc. Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Least squares quantization in PCM</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Information Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains</title>
		<author>
			<persName><forename type="first">L</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Petrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Soules</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Ann. of Math. Stat</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="164" to="171" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
