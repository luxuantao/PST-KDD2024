<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Approximation Algorithms for Clustering Uncertain Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Graham</forename><surname>Cormode</surname></persName>
							<email>graham@research.att.com</email>
							<affiliation key="aff0">
								<orgName type="institution">UC San Diego</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Mcgregor</surname></persName>
							<email>andrewm@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UC San Diego</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Approximation Algorithms for Clustering Uncertain Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4B68DE115D76568105E6855614407B31</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H</term>
					<term>3</term>
					<term>3 [INFORMATION STORAGE AND RETRIEVAL]: Information Search and Retrieval-Clustering Clustering, Probabilistic Data</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There is an increasing quantity of data with uncertainty arising from applications such as sensor network measurements, record linkage, and as output of mining algorithms. This uncertainty is typically formalized as probability density functions over tuple values. Beyond storing and processing such data in a DBMS, it is necessary to perform other data analysis tasks such as data mining. We study the core mining problem of clustering on uncertain data, and define appropriate natural generalizations of standard clustering optimization criteria. Two variations arise, depending on whether a point is automatically associated with its optimal center, or whether it must be assigned to a fixed cluster no matter where it is actually located.</p><p>For uncertain versions of k-means and k-median, we show reductions to their corresponding weighted versions on data with no uncertainties. These are simple in the unassigned case, but require some care for the assigned version. Our most interesting results are for uncertain k-center, which generalizes both traditional kcenter and k-median objectives. We show a variety of bicriteria approximation algorithms. One picks O(k -1 log 2 n) centers and achieves a (1 + ) approximation to the best uncertain k-centers. Another picks 2k centers and achieves a constant factor approximation. Collectively, these results are the first known guaranteed approximation algorithms for the problems of clustering uncertain data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>There is a growing awareness of the need for database systems to be able to handle and correctly process data with uncertainty. Conventional systems and query processing tools are built on the assumption of precise values being known for every attribute of every tuple. But any real dataset has missing values, data quality issues, rounded values and items which do not quite fit any of the intended options. Any real world measurements, such as those arising from sensor networks, have inherent uncertainty around the reported value. Other uncertainty can arise from combination of data values, such as record linkage across multiple data sources (e.g., how sure is it that these two addresses refer to the same location?) and from intermediate analysis of the data (e.g., how much confidence is there in a particular derived rule?). Given such motivations, research is ongoing into how to represent, manage, and process data with uncertainty.</p><p>Thus far, most focus from the database community has been on problems of understanding the impact of uncertain data within the DBMS, such as how to answer SQL-style queries over uncertain data. Because of interactions between tuples, evaluating even relatively simple queries is #P -hard, and care is needed to analyze which queries can be "safely" evaluated, avoiding such complexity <ref type="bibr" target="#b9">[9]</ref>. However, the equally relevant question of mining uncertain data has received less attention. Recent work has studied the cost of computing simple aggregates over uncertain data with limited (sublinear) resources, such as average, count distinct, and median <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b22">22]</ref>. But beyond this, there has been little principled work on the challenge of mining uncertain data, despite the fact that most data to be mined is inherently uncertain.</p><p>We focus on the core problem of clustering. Adopting the tuple level semantics, the input is a set of points, each of which is described by a compact probability distribution function (pdf). The pdf describes the possible locations of the point; thus traditional clustering can be modeled as an instance of clustering of uncertain data where each input point is at a fixed location with probability 1. The goal of the clustering is to find a set of cluster centers, which minimize the expected cost of the clustering. The cost will vary depending on which formulation of the cost function we adopt, described more formally below. The expectation is taken over all "possible-worlds", that is, all possible configurations of the point locations. The probability of a particular configuration is determined from the individual pdfs, under the usual assumption of tuple independence. Note that even if each pdf only has a constant number of discrete possible locations for a point, explicitly evaluating all possible configurations will be exponential in the number of points, and hence highly impractical.</p><p>Given a cost metric, any clustering of uncertain data has a welldefined (expected) cost. Thus, even in this probabilistic setting, there is a clear notion of the optimal cost, i.e., the minimum cost clustering attainable. Typically, finding such an optimal clustering is NP-hard, even in the non-probabilistic case. Hence we focus on finding α-approximation algorithms which promise to find a clustering of size k whose cost is at most α times the cost of the optimal clustering of size k. We also consider (α, β)-bicriteria approximation algorithms, which find a clustering of size βk whose cost is at most α times the cost of the optimal k-clustering. These give strong guarantees on the quality of the clustering relative to the desired cost-objective. It might be hoped that such approximations would follow immediately from simple generalizations of approximation algorithms for corresponding cost metrics in the non-probabilistic regime. Unfortunately, naive approaches fail, and instead more involved solutions are necessary, generating new approximation schemes for uncertain data.</p><note type="other">Objective Metric Assignment</note><p>Clustering Uncertain Data and Soft Clustering. 'Soft clustering' (sometimes also known as probabilistic clustering) is a relaxation of clustering which asks for a set of cluster centers and a fractional assignment for each point to the set of centers. The fractional assignments can be interpreted probabilistically as the probability of a point belonging to that cluster. This is especially relevant in modelbased clustering, where the output clusters are themselves distributions (e.g., multivariate Gaussians with particular means and variances): here, the assignments are implicit from the descriptions of the clusters as the ratio of the probability density of each cluster at that point. Although both soft clustering and clustering uncertain data can be thought of notions of "probabilistic clustering", they are quite different: soft clustering takes fixed points as input, and outputs appropriate distributions; our problem has probability distributions as inputs but requires fixed points as output. There is no obvious way to use solutions for one problem to solve the other. Clearly, one can define an appropriate hybrid generalization, where both input and output can be probabilistic. In this setting, methods such as expectation maximization (EM) <ref type="bibr" target="#b10">[10]</ref> can naturally be applied to generate model-based clusterings. However, for clarity, we focus on the formulation of the problem where a 'hard' assignment of clusters is required, so do not discuss soft clustering further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Our results</head><p>In traditional clustering, the three most popular clustering objectives are k-center (to find a set of k cluster centers that minimize the radius of the clusters), k-median (to minimize the sum of distances between points and their closest center) and k-means (to minimize the sum of squares of distances). Each has an uncertain counterparts, where the aim is to find a set of k cluster centers which minimize the expected cost of the clustering for k-means, k-median, or k-center objectives. In traditional clustering, the closest center for an input point is well-defined even in the event of ties. But when a single 'point' has multiple possible locations, the meaning is less clear. We consider two variations. In the assigned case, the output additionally assigns a cluster center to each discrete input point. Wherever that point happens to be located, it is always assigned to that center, and the (expected) cost of the clustering is computed accordingly. In the unassigned case, the output is solely the set of cluster centers. Given a particular possible world, each point is assigned to whichever cluster minimizes the distance from its realized location in order to evaluate the cost. Both versions are meaningful: one can imagine clustering data of distributions of consumers' locations to determine where to place k facilities. If the facilities are bank branches, then we may want to assign each customer to a particular branch (so that they can meet personally with their account manager), meaning an assigned solution is needed for branch placement. But customers can also use any ATM, so the unassigned case would apply for ATM placement. We provide results for both assigned and unassigned versions.</p><p>• k-median and k-means. Due to their linear nature, the unassigned case for k-median and k-means can be efficiently reduced to their non-probabilistic counterparts. But in the assigned version, some more work is needed in order to give required guarantees. In Section 5, we show that uncertain k-means can be directly reduced to weighted deterministic k-means, in both the assigned and unassigned case. We go on to show that uncertain k-median can also be reduced to its deterministic version, but with a constant increase in the approximation factor.</p><p>• k-center. The uncertain k-center problem is considerably more challenging. Due to the nature of the optimization function, several seemingly intuitive approaches turn out not to be valid. In Section 4, we describe a pair of bicriteria approximation algorithms, for inputs of a particular form one of which achieves a 1 + approximation with a large blowup in the number of centers, and the other which achieves a constant factor approximation with only 2k centers. These apply to general inputs in the unassigned case with a further constant increase in the approximation factor.</p><p>• We consider a variety of different models for optimizing the cluster cost in Section 6. Some of these turn out to be provably hard even to approximate, while others yield clusterings which do not seem useful; thus the formulation based on expectation is the most natural of those considered.</p><p>Our (α, β)-approximation bounds are summarized in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Clustering data is the topic of much study, and has generated many books devoted solely to the subject. Within database research, various methods have proven popular, such as DBSCAN <ref type="bibr" target="#b13">[13]</ref>, CURE <ref type="bibr" target="#b16">[16]</ref>, and BIRCH <ref type="bibr" target="#b28">[28]</ref>. Algorithms have been proposed for a variety of clustering problems, such as the k-means, k-median and k-center objectives discussed above. The term 'k-means' is often used casually to refer to Lloyd's algorithm, which provides a heuristic for the k-means objective <ref type="bibr" target="#b25">[25]</ref>. It has been shown recently that careful seeding of this heuristic provides a O(log n) approximation <ref type="bibr" target="#b2">[2]</ref>. Other recent work has resulted in algorithms for k-means which guarantee a 1 + approximation, although these are exponential in k <ref type="bibr" target="#b24">[24]</ref>. Clustering relative to the k-center or kmedian objective is known to be NP-hard <ref type="bibr" target="#b19">[19]</ref>. Some of the earliest approximation algorithms were for the k-center problem, and for points in a metric space, the achieved ratio of 2 is the best possible guarantee assuming P = N P . For k-median, the best known approximation algorithm guarantees a 3 + approximate solution <ref type="bibr" target="#b3">[3]</ref>, with time cost exponential in . Approximation algorithms for clustering and its variations continues to be an active area of research in the algorithms community <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b21">21]</ref>. See the lecture notes by Har-Peled <ref type="bibr" target="#b17">[17,</ref><ref type="bibr">Chapter 4]</ref> for a clear introduction.</p><p>Uncertain data has recently attracted much interest in the data management community due to increasing awareness of the prevalence of uncertain data. Typically, uncertainty is formalized by providing some compact representation of the possible values of each tuple, in the form of a pdf. Such tuple-level uncertainty models assume that the pdf of each tuple is independent of the others. Prior work has studied the complexity of query evaluation on such data <ref type="bibr" target="#b9">[9]</ref>, and how to explicitly track the lineage of each tuple over a query to ensure correct results <ref type="bibr" target="#b5">[5]</ref>. Query answering over uncertain data remains an active area of study, but our focus is on the complementary area of mining uncertain data, and in particular clustering uncertain data.</p><p>The problem of clustering uncertain data has been previously proposed, motivated by the uncertainty of moving objects in spatiotemporal databases <ref type="bibr">[7]</ref>. A heuristic provided there was to run Lloyd's algorithm for k-means on the data, using tuple-level probabilities to compute an expected distance from cluster centers (similar to the "fuzzy c-means" algorithm <ref type="bibr" target="#b11">[11]</ref>). Subsequent work has studied how to more rapidly compute this distance given pdfs represented by uniform uncertainty within a geometric region (e.g., bounding rectangle or other polygon) <ref type="bibr" target="#b26">[26]</ref>. We formalize this concept, and show a precise reduction to weighted k-means in Section 5.1.</p><p>Most recently, Aggarwal and Yu <ref type="bibr" target="#b1">[1]</ref> proposed an extension of their micro-clustering technique to uncertain data. This tracks the mean and variance of each dimension within each cluster (and so assumes geometric points rather than an arbitrary metric), and uses a heuristic to determine when to create new clusters and remove old ones. This heuristic method is highly dependent on the input order; moreover, this approach has no proven guarantee relative to any of our clustering objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROBLEM DEFINITIONS</head><p>In this section, we formalize the definitions of the input and the cost objectives. The input comes in the form of a set of n probability distribution functions (pdfs) describing the locations of n points in a metric space (X , d). Our results address two cases: when the points are arbitrary locations in d-dimensional Euclidean space, and when the metric is any arbitrary metric. For the latter case, we consider only discrete clusterings, i.e., when the cluster centers are restricted to be among the points identified in the input.</p><p>The pdfs specify a set of n random variables X = {X1, . . . , Xn}. The pdf for an individual point, Xi, describes the probability that the i-th point is at any given location x ∈ X , i.e., Pr[Xi = x]. We mostly consider the case of discrete pdfs, that is, Pr[Xi = x] is non-zero only for a small number of x's. We assume that</p><formula xml:id="formula_0">γ = min i∈[n],x∈X Pr[Xi = x]| Pr[Xi = x] &gt; 0</formula><p>is only polynomially small. There can be a probability that a point does not appear, which is denoted by ⊥ ∈ X , so that 0 ≤ Pr[Xi = ⊥] &lt; 1. For any point xi and its associated pdf Xi, define Pi, the probability that the point occurs, as 1 -Pr[Xi = ⊥]. All our methods apply to the cases where Pi = 1, and more generally to Pi &lt; 1. The aspect ratio, ∆, is defined as the ratio between the greatest distance and smallest distance between pairs of points identified in the input, i.e.,</p><formula xml:id="formula_1">∆ = max x,y∈X ,∃i,j∈[n]:Pr[X i =x] Pr[X j =y]&gt;0 d(x, y) min x,y∈X ,∃i,j∈[n]:Pr[X i =x] Pr[X j =y]&gt;0 d(x,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>y) .</head><p>A special case of this model is when Xi is of the form</p><formula xml:id="formula_2">Pr[Xi = xi] = pi and Pr[Xi = ⊥] = 1 -pi</formula><p>which we refer to as the point probability case.</p><p>The goal is to produce a (hard) clustering of the points. We consider two variants of clustering, assigned clustering and unassigned clustering. In both we must specify a set of k points C = {c1, . . . , c k } and in the case of assigned clustering we also specify an assignment from each Xi to a point c ∈ C: Definition 1. Assigned Clustering: Wherever the i-th point falls, it is always assigned to the same cluster. The output of the clustering is a set of k points C = {c1, . . . , c k } and a function σ : [n] → C mapping points to clusters.</p><p>Unassigned Clustering: Wherever it happens to fall, the i-th point is assigned to the closest cluster center. In this case, the assignment function τ is implicitly defined by the clusters C, as the function that maps from locations to clusters based on Voronoi cells: τ : X → C such that τ (x) = arg minc∈C d(x, c).</p><p>We consider three standard objective functions generalized appropriately to the uncertain data setting. The cost of a clustering is formalized as follows. To allow a simple statement of the costs for both the assigned and unassigned cases, define a function ρ : [n] × X → C so that ρ(i, x) = σ(i) in the assigned case, and ρ(i, x) = τ (x) in the unassigned case. These are defined based on the indicator function, I[A], which is 1 when the event A occurs, and 0 otherwise. Definition 2. The k-median cost, cost med , is the sum of the distances of points to their center:</p><formula xml:id="formula_3">cost med (X, C, ρ) = X i∈[n],x∈X I[Xi = x]d(x, ρ(i, x))</formula><p>The k-means cost, costmea, is the sum of the squares of distances:</p><formula xml:id="formula_4">costmea(X, C, ρ) = X i∈[n],x∈X I[Xi = x]d 2 (x, ρ(i, x))</formula><p>The k-center cost, costcen, is the maximum distance from any point to its associated center:</p><formula xml:id="formula_5">costcen(X, C, ρ) = max i∈[n],x∈X {I[Xi = x]d(x, ρ(i, x))}</formula><p>These costs are random variables, and so we can consider natural statistical properties such as the expectation and variance of the cost, or the probability that the cost exceeds a certain value. Here we set out to minimize the (expected) cost of the clustering. This generates corresponding optimization problems. Definition 3. The uncertain k-median problem is to find a set of centers C which minimize the expected k-median cost, i.e., min</p><formula xml:id="formula_6">C:|C|=k E[cost med (X, C, ρ)] .</formula><p>The uncertain k-means problem is to find k centers C which minimize the expected k-means cost,</p><formula xml:id="formula_7">min C:|C|=k E[costmea(X, C, ρ)] .</formula><p>The uncertain k-center problem is to find k centers C which minimize:</p><formula xml:id="formula_8">min C:|C|=k E[costcen(X, C, ρ)] .</formula><p>These costs implicitly range over all possible assignments of points to locations (possible worlds). Even in the point probability case, naively computing the cost of a particular clustering by enumerating all possible worlds would take time exponential in the input size. However, each cost can be computed efficiently given C and ρ. In the point-probability case, ρ is implicit from C, we may drop it from our notation.</p><p>A special case is when all variables X are of the form Pr[Xi = xi] = 1, i.e., there is no uncertainty, since the i-th point is always at location xi. Then we have precisely the "traditional" clustering problem on deterministic data, and the above optimization problems correspond precisely to the standard definitions of k-center, k-median and k-means. We refer to these problems as "deterministic k-center" etc., in order to clearly distinguish them from their uncertain counterparts.</p><p>In prior work on computing with uncertain data, a general technique is to sample repeatedly from possible worlds, and compute the expected value of the desired function <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b22">22]</ref>. Such approaches do not work in the case of clustering, however, since the desired output is a single set of clusters. While it is possible to sample multiple possible worlds and compute clusterings for each, it is unclear how to combine these into a single clustering with some provable guarantees, so more tailored methods are required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">UNCERTAIN K-CENTER</head><p>In this section, we give results for uncertain k-center clustering. This optimization problem turns out to be richer than its certain counterpart, since it encapsulates aspects of both deterministic kcenter and deterministic k-median.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Characteristics of Uncertain k-center</head><p>Clearly, uncertain k-center is NP-hard, since it contains deterministic k-center as a special case when Pr[Xi = xi] = 1 for all i. Further, this shows that it is hard to approximate uncertain kcenter over arbitrary metrics to better than a factor of 2. There exist simple greedy approximation algorithms for deterministic k-center which achieve this factor of 2 in the unweighted case or 3 in the weighted case <ref type="bibr" target="#b12">[12]</ref>. We show that such natural greedy heuristics from the deterministic case do not carry over to the probabilistic case for k-center.</p><p>Example 1. Consider n points distributed as follows:</p><formula xml:id="formula_9">Pr[X1 = y] = p Pr[Xi&gt;1 = x] = p/2 Pr[X1 = ⊥] = 1 -p Pr[Xi&gt;1 = ⊥] = 1 -p/2</formula><p>where the two locations x and y satisfy d(x, y) = 1. Placing 1 center at x has expected cost p. As n grows larger, placing 1 center at y has expected cost tending to 1. Greedy algorithms for unweighted deterministic k-center pick the first center as an arbitrary point from the input, and so could pick y <ref type="bibr" target="#b15">[15]</ref>. Greedy algorithms for weighted deterministic k-center consider each point individually, and pick the point which has the highest weight as the first center <ref type="bibr" target="#b19">[19]</ref>: in this case, y has the highest individual weight (p instead of p/2) and so would be picked as the center. Thus applying algorithms for the deterministic version of the problems can do arbitrarily badly: they fail to achieve an approximation ratio of 1/p for any chosen p. The reason is that the metric of expected cost is quite different from the unweighted and weighted versions of deterministic k-center, and so approximations for the latter do not translate into approximations for the former.</p><p>Our next example gives some further insight.</p><p>Example 2. Consider n points distributed as follows:</p><formula xml:id="formula_10">Pr[Xi = xi] = pi Pr[Xi = ⊥] = 1 -pi</formula><p>where xi's are some arbitrary set of locations. We can show that if all pi's are close to 1, the cost is dominated by the point with the greatest distance from its nearest center, i.e., this instance of uncertain k-center is essentially equivalent to deterministic k-center. On the other hand, if all pi are sufficiently small then the problem is almost equivalent to deterministic k-median. Both statements follow from the next lemma which applies to the point probability case:</p><formula xml:id="formula_11">LEMMA 1. For a given set of centers C, let di = d(xi, C). Assume that d1 ≥ d2 ≥ . . . ≥ dn. Then, E[costcen(X, C, ρ)] = X i pidi Y j&lt;i (1 -pj) .<label>(1)</label></formula><formula xml:id="formula_12">If P j pj ≤ then for all i, 1 ≥ Q j&lt;i (1 -pj) ≥ 1 -and hence 1 -≤ E[costcen(X, C, ρ)] E[cost med (X, C, ρ)] ≤ 1 .</formula><p>Note that from equation (1) it is also clear that if we round all probabilities up to 1 we alter the value of the optimization criterion by at most a 1/γ = 1/ min i∈[n] pi factor. But this gives precisely an instance of the deterministic k-center problem, which can be approximated up to a factor 2. So we can easily give a 2/γ approximation for probabilistic k-center.</p><p>Thus the same probabilistic problem encompasses two quite distinct deterministic problems, both of which are NP-Hard, even to approximate to better than constant factors. More strongly, the same hardness holds even in the case where Pi = 1 (so Pr[Xi = ⊥] = 0): in the above examples, replace ⊥ with some point far away from all other points, and allocate k + 1 centers instead of 1. Now any near-optimal algorithm in the unassigned case must allocate a center for this far point. This leaves k centers for the remaining problem, whose cost is the same as that in the examples with ⊥.</p><p>Lastly, we show that an intuitive divide-and-conquer approach fails. We might imagine that partitioning the input into subsets, and finding an αj approximation on each subset of points, would result in k centers which provide an overall max j∈[ ] αj guarantee. This example shows that this is not the case: Example 3. Consider the metric space over 4 locations {x1, x2, c, o} so that:</p><formula xml:id="formula_13">d(x1, c) = 4 d(x1, o) = 3 d(x2, c) = 8 d(x2, o) = 3</formula><p>The input consists of</p><formula xml:id="formula_14">Pr[X1 = x1] = 1 Pr[X2 = x1] = 1 Pr[X3 = x2] = 1 2 Pr[X3 = ⊥] = 1 2 Pr[X4 = x2] = 1 2 Pr[X4 = ⊥] = 1 2</formula><p>Suppose we partition the input into {X1, X3} and {X2, X4}. The optimal solution to the induced uncertain 1-center problem is to place a center at o, with cost 3. Our approximation algorithm may decide to place a center at c, which relative to {X1, X3} is a 2-approximation (and also for {X2, X4}). But on the whole input, placing a center (or rather, two centers) at c has cost 7, and so is no longer a 2-approximation to the optimal cost (placing a center at o still has cost 3 over the full input). Thus approximations on subsets of the input do not translate to approximations on the whole input.</p><p>Instead, one can show the following: LEMMA 2. Let X be partitioned into Y1 . . . Y . For i ∈ [ ], let Ci be a set of k points that satisfy,</p><formula xml:id="formula_15">E[costcen(Yi, Ci)] ≤ αi min C:|C|=k E[costcen(X, C)] . Then for α = P i=1 αi, E ˆcostcen(X, ∪ i∈[ ] Ci) ˜≤ α min C:|C|=k E[costcen(X, C)] .</formula><p>PROOF. Consider splitting X into just two subsets, Y1 and Y2, and finding α1 approximate centers C1 for Y1, and α2 approximate centers C2 for Y2. We can write the cost of using C1 ∪C2 as</p><formula xml:id="formula_16">E[costcen(X, C1 ∪C2)] = X j∈[t] Pr[costcen(Y1 ∪Y2, C1 ∪C2) = rj] rj ≤ X j∈[t]</formula><p>Pr[costcen(Y1, C1) = rj] rj</p><formula xml:id="formula_17">+ X j∈[t] Pr[costcen(Y2, C2) = rj] rj =E[costcen(Y1, C1)]+E[costcen(Y2, C2)] ≤(α1 + α2) min C:|C|=k E[costcen(X, C)]</formula><p>This implies the full result by induction.</p><p>Efficient Computation of costcen. Lemma 1 implies an efficient way to compute the cost of a given uncertain k-center clustering C against input X in the point probability model: using the same indexing of points, define X i = {X1 . . . Xi}, and so recursively</p><formula xml:id="formula_18">E h costcen(X i , C, ρ) i = pidi + (1 -pi)E h costcen(X i-1 , C, ρ) i and E ˆcostcen(X 0 , C, ρ) ˜= 0.</formula><p>In the more general discrete pdf case, for both the assigned and unassigned cases we can form a similar expression, although a more complex form is needed to handle the interactions between points belonging to the same pdf. We omit the straightforward details for brevity. The consequence is that the cost of any proposed k-center clustering can be found in time linear in the input size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Cost Rewriting</head><p>In subsequent sections, we present bicriteria approximation algorithms for uncertain k-center over point probability distributions, i.e., when all Xi are of the form</p><formula xml:id="formula_19">Pr[Xi = xi] = pi, Pr[Xi = ⊥] = 1 -pi.</formula><p>We assume that γ = mini min(pi, 1 -pi) is only polynomially small. In Section 4.5, we extend our results from the point probability case to arbitrary discrete probability density functions.</p><p>Our algorithms begin by rewriting the objective function. Given input X, we consider the set of distances between pairs of points d(xi, x ). Denote the set of these t = n(n -1)/2 distances as {rj} 0≤j≤t where 0 = r0 ≤ r1 ≤ . . . ≤ rt. Then the cost of the clustering with C is</p><formula xml:id="formula_20">E[costcen(X, C)] = X j∈[t] Pr[costcen(X, C) = rj] rj = X j∈[t]</formula><p>Pr[costcen(X, C) ≥ rj] (rj -rj-1)</p><p>Note that Pr[costcen(X, C) ≥ r] is non-increasing as r increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">(1 + ) Factor Approximation Algorithm</head><p>The following lemma exploits the natural connection between k-center and set-cover, a connection exploited in the optimal asymmetric k-center clustering algorithms <ref type="bibr" target="#b27">[27]</ref>. LEMMA 3. In polynomial time, for any r we can find C of size at most ck log(n) (for some constant c) such that Pr ˆcostcen(X, C ) ≥ r ˜≤ min</p><formula xml:id="formula_21">C:|C|=k Pr[costcen(X, C) ≥ r] PROOF. Let C = argmin C:|C|=k Pr[costcen(X, C) ≥ r]</formula><p>and A = {xi : d(xi, C) &lt; r}. For each xi we define a positive weight, wi = -ln(1-pi). It will be convenient to assume the each pi &lt; 1 so that these weights are not infinite. However, because following argument applies if pi ≤ 1for any &gt; 0, it can be shown that the argument holds in the limit when = 0. Note that</p><formula xml:id="formula_22">Pr[costcen(X, C) ≥ r] = 1- Y i:x i ∈A (1-pi) = 1-exp(- X i ∈A wi) .</formula><p>We will greedily construct a set C of size at most k log(nγ -1 )</p><formula xml:id="formula_23">such that B = {xi : d(xi, C ) &lt; r} satisfies, X i ∈A wi ≥ X i ∈B</formula><p>wi and therefore, as required,</p><formula xml:id="formula_24">Pr[costcen(X, C) ≥ r] ≥ Pr ˆcostcen(X, C ) ≥ r ˜.</formula><p>We construct C incrementally: at the j-th step let</p><formula xml:id="formula_25">C j = {c1, . . . , cj}, Bj = {xi : d(xi, C j ) &lt; r} ,</formula><p>and define tj = P i:x i ∈B j wi. We choose cj+1 such that tj+1 is maximized. Let w = P i∈A wi and let si = w -ti. At each step there exists a choice for cj+1 such that tj+1 -tj ≥ si/k. This follows because P i∈A\B j wi ≥ si and |C| = k. Hence si ≤ w(1 -1/k) i . Therefore, for i = k ln(w/wmin) we have si &lt; wmin and hence si ≤ 0. Note that ln(w/wmin) ≤ c ln n for some c because 1/(1 -pi) and pi are poly(n). THEOREM 4. There exists a polynomial time (1+ , c -1 log n log ∆) bi-criteria approximation algorithm for uncertain k-center where ∆ is the aspect ratio.</p><p>PROOF. We round up all distances to the nearest power of (1+ ) so that there are t = ˚lg 1+ (∆) ˇdifferent distances r1 &lt; r2 &lt; . . . &lt; rt. This will cost us the (1 + ) factor in the objective function. Then, for j ∈ [t], using Lemma 3, we may find a set Cj of O(k log n) centers such that</p><formula xml:id="formula_26">Pr[costcen(X, Cj) ≥ rj] ≤ min C:|C|=k Pr[costcen(X, C) ≥ rj] .</formula><p>Taking the union C0 ∪ . . . ∪ Ct as the centers gives the required result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Constant Factor Approximation</head><p>The following lemma is based on a result for k-center clustering with outliers by Charikar et al. <ref type="bibr" target="#b6">[6]</ref>. The outlier problem is defined as follows: given a set of n points and an integer o, find a set of centers such that for the smallest possible value of r, all but at most o points are within distance r of a center. Charikar et al. present a 3-approximation algorithm for this problem. In fact, we use their approach for a dual problem, where r is fixed and the number of outliers o is allowed to vary. The proof of the following lemma follows by considering the weighted case of the outlier problem where each xi receives weight wi = ln 1  1-p i . LEMMA 5. In polynomial time, we can find C of size k such that Pr ˆcostcen(X, C ) ≥ 3r ˜≤ min</p><formula xml:id="formula_27">C:|C|=k Pr[costcen(X, C) ≥ r] .</formula><p>The above lemma allows us to construct a bi-criteria approximation for k-center with uncertainty such that α and β are both constant. THEOREM 6. There exists a polynomial time (12 + , 2) bicriteria approximation for uncertain k-center.</p><p>PROOF. Denote the set of all t = n(n -1)/2 distances as {rj} 1≤j≤t where r1 ≤ . . . ≤ rt and let r0 = 0 and rt+1 = ∞. Let C be the optimum size k set of cluster centers. For j ∈ [t], using Lemma 5 we find a set Cj of k centers such that</p><formula xml:id="formula_28">Pr[costcen(X, Cj) ≥ rj] ≤ Pr[costcen(X, C) ≥ rj/3]</formula><p>Note that we may assume,</p><formula xml:id="formula_29">Pr[costcen(X, Cj) ≥ rj] ≥ Pr[costcen(X, Cj) ≥ rj+1] ≥ Pr[costcen(X, Cj+1) ≥ rj+1] .</formula><p>Let be the smallest j such that</p><formula xml:id="formula_30">1 -κ ≥ Pr[costcen(X, Cj) ≥ rj]</formula><p>where κ ∈ (0, 1) to be determined. Let N = {Xi : d(xi, C ) ≤ r } be the set of points "near" to C and let F = X \ N be the set of points "far" from C . We consider clustering the near and far points separately.</p><p>We first consider the near points. Note,</p><formula xml:id="formula_31">E[costcen(X, C)] ≥ X j∈[t] Pr » 1 3 rj+1 &gt; costcen(X, C) ≥ 1 3 rj - 1 3 rj = X j∈[t] Pr » costcen(X, C) ≥ 1 3 rj - ( 1 3 rj - 1 3 rj-1) ≥ 1 3 X j∈[t] Pr[costcen(X, Cj) ≥ rj] (rj -rj-1) ≥ 1 -κ 3 X j∈[ ] Pr[costcen(X, C ) ≥ rj] (rj -rj-1) = 1 -κ 3 E[costcen(N, C )]</formula><p>where the second inequality follows because</p><formula xml:id="formula_32">Pr[costcen(N, Cj) ≥ rj] Pr[costcen(N, C ) ≥ rj] ≥ 1 -κ for j ≤ .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The last inequality follows because</head><p>Pr[costcen(N, C ) ≥ rj] = 0 for j ≥ + 1.</p><p>We now consider the far points. Let C * be an (3+ )-approximation to the (weighted) k-median problem on F (i.e., each point xi has weight pi). This can be found in polynomial time using the result of Arya et al. <ref type="bibr" target="#b3">[3]</ref>. Then, by Lemma 1 (note that</p><formula xml:id="formula_33">Q x i ∈F (1 -pi) ≥ κ), E[costcen(F, C * )] ≤ (3 + )κ -1 E[costcen(F, C)] ≤ (3 + )κ -1 E[costcen(X, C)] .</formula><p>Appealing to Lemma 2, we deduce that C * ∪ C are 2k centers that achieve a 3 + κ + 3 1 -κ approximation to the objective function. Setting κ = 1/2 gives the stated result (rescaling as appropriate.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Extension to General Density Functions</head><p>So far we have been considering the point probability case for uncertain k-center, in which there is no difference between the assigned and unassigned versions. In this section, we show that we can use these solutions in the unassigned case for general pdfs, and only lose a constant factor in the objective function. The following lemma makes this concrete:</p><formula xml:id="formula_34">LEMMA 7. Let 0 ≤ pi,j ≤ 1 satisfy Pi = P j pi,j ≤ 1. Then 1- Y i,j (1-pi,j) ≤ 1- Y i (1-Pi) ≤ g(P * ) 1 - Y i,j</formula><p>(1 -pi,j)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>!</head><p>where P * = maxi Pi and g(x) = x/(1 -exp(-x)). PROOF. The first inequality follows by the union bound. To prove the next inequality we first note that 1 -pi,j ≤ exp(-pi,j) and hence Q i,j 1 -pi,j ≤ exp(-P i Pi). We now prove that</p><formula xml:id="formula_35">g(P * )(1 -exp(- X i∈[n] Pi)) ≥ (1 - Y i∈[n] (1 -Pi))<label>(2)</label></formula><p>by induction on n.</p><formula xml:id="formula_36">1 - Y i∈[n] (1 -Pi) = (1 -Pn)(1 - Y i∈[n-1] (1 -Pi)) + Pn ≤ g(P * )(1 -Pn)(1 -e -P i∈[n-1] P i ) + Pn ≤ g(P * )(e -Pn (1 -e -P i∈[n-1] P i ) + Pn/g(P * )) ≤ g(P * )(e -Pn + Pn/c(P * ) -e -P i∈[n] P i ) ≤ g(P * )(1 -e -P i∈[n] P i ) .</formula><p>This requires exp(-Pn) + Pn/g(P * ) ≤ 1 which is satisfied by the definition of g(P * ). For the base case, we derive the same requirement on g(P * ).</p><p>Hence, in the unassigned clustering case with a discrete pdf, we can treat each possible point as a separate independent point pdf, and be accurate upto a g(1) = e/(e -1) factor in the worst case. We then appeal to the results in the previous sections, where now n denotes the total size of the input (i.e. the sum of description sizes of all pdfs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">UNCERTAIN K-MEANS AND K-MEDIAN</head><p>The definitions of k-means and k-median are based on the sum of costs over all points. This linearity allows the use of linearity of expectation to efficiently compute and optimize the cost of clustering. We outline how to use reductions to appropriately formulated weighted instances of deterministic clustering. In some cases, solutions to the weighted versions are well-known, but we note that weighted clustering can be easily reduced to unweighted clustering with only a polynomial blow-up in the problem size (on the assumption that the ratio between the largest and smallest non-zero weights is polynomial) by replacing each weighted point with an appropriate number of points of unit weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Uncertain k-means</head><p>The k-means objective function is defined as the expectation of a linear combination of terms. Consequently,</p><formula xml:id="formula_37">E[costmea(X, C, ρ)] = X i∈[n],x∈X Pr[Xi = x]d 2 (x, ρ(i, x)).<label>(3)</label></formula><p>So given a clustering C and ρ, the cost of that clustering can be computed quickly, by computing the contribution of each point independently, and summing. THEOREM 8. For X = (R d , 2), there exists a randomized, polynomial time (1 + )-approximation for uncertain k-means.</p><p>PROOF. First observe that the result for the unassigned version of uncertain k-means can be immediately reduced to a weighted version of the non-probabilistic problem: by linearity of expectation the cost to be minimized is exactly the same as that of a weighted instance of k-means, where the weight on each point Xi is Pr[Xi = x]. Applying known results for k-means gives the desired accuracy and time bounds <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b14">14]</ref>.</p><p>The assigned clustering version of the problem can be reduced to the point probability case where the assigned and unassigned versions are identical. To show this we first define i.e., µi and σ 2 i are the weighted (vector) mean and the (scalar) variance of Xi, respectively. Then we can rewrite E[costmea(X, C, ρ)] using properties of Euclidean distances as:</p><formula xml:id="formula_38">E[costmea(X, C, ρ)] = X i∈[n],x∈X Pr[Xi = x]d(x, ρ(i)) 2 = X i∈[n],x∈X Pr[Xi = x] " d(x, µi) 2 + d(µi, ρ(i)) 2 + 2d(x, µi)d(µi, ρ(i)) " = X i∈[n] σ 2 i + X i∈[n],x∈X Pid 2 (µi, ρ(i)) = X i∈[n] σ 2 i + E ˆcostmea(X , C, ρ) ˜,</formula><p>where X is the input in the point probability case defined by Pr[X i = µi] = Pi. Note that P i∈[n] σ 2 i is a non-negative scalar that does not depend on C and ρ. Hence any α-approximation algorithm for the minimization of E[costmea(X , C, ρ)] gives an α-approximation for the minimization of E[costmea(X, C, ρ)]. But now, as in the non-assigned case, known results for k-means can be used to find an (1 + ) approximation for the minimization of the instance of weighted deterministic k-means given by X .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Uncertain k-median</head><p>Similarly to k-means, linearity of expectation means that the uncertain k-median cost of any proposed clustering can be quickly computed since</p><formula xml:id="formula_39">E[cost med (X, C, ρ)] = X i∈[n],x∈X Pr[Xi = x]d(x, ρ(i, x))].</formula><p>Uncertain k-median (Unassigned Clustering). Again due to the linearity of the cost metric, solutions to the weighted instance of the k-median problem can be applied to solve the uncertain version for the unassigned case.</p><p>THEOREM 9. For X = (R d , 2), there exists a randomized, polynomial time (1 + )-approximation for uncertain k-median (unassigned). For arbitrary metrics, the approximation factor is (3 + )-approximation.</p><p>The result follows by the reduction to weighted k-median and appealing to bounds for k-median for arbitrary metrics <ref type="bibr" target="#b3">[3]</ref> or in the Euclidean case <ref type="bibr" target="#b23">[23]</ref>.</p><p>Uncertain k-median (Assigned Clustering). For k-means assigned case, it was possible to find the mean of each point distribution, and then cluster these means. This succeeds due to the properties of the k-means objective function in Euclidean space. For k-median assigned case, we adopt an approach that is similar in spirit, but whose analysis is more involved. The approach has two stages: first, it finds a near-optimal 1-clustering of each point distribution, and then it clusters the resulting points. THEOREM 10. Given an algorithm for weighted k-median clustering with approximation factor α, we can find a (2α + 1) approximation for the uncertain k-median problem.</p><p>PROOF. For each Xi, find the discrete 1-median yi by simply finding the point from the (discrete) distribution Xi which minimizes the cost.</p><p>Let Pi = 1 -Pr[Xi = ⊥], as before, and let</p><formula xml:id="formula_40">T = X i∈[n],x∈X Pr[Xi = x]d(x, yi),</formula><p>be the expected cost of assigning each point to its discrete 1-median. Define OP T , the optimal cost of an assigned k-median clustering, as</p><formula xml:id="formula_41">OP T = min C,σ:|C|=k E[cost med (X, C, σ))] .</formula><p>Finally, set OP TY = min C:|C|=k E ˆPi Pid(ρ(i, x), yi) ˜, the optimal cost of weighted k-median clustering of the chosen 1-medians Y = ∪ i∈[n] {yi}. Now T ≤ OP T because it is an optimal assigned n-median clustering, whose optimal cost can be no worse than an optimal assigned k-median clustering, i.e., min</p><formula xml:id="formula_42">C,σ:|C|=n E[cost med (X, C, σ)] ≤ min C,σ:|C|=k E[cost med (X, C, σ)] .</formula><p>Let C = argmin C:|C|=k E[cost med (X, C, ρ)] be a set of k medians which achieves the optimal cost and let σ : [n] → C be the allocation of uncertain points to these centers. Then</p><formula xml:id="formula_43">T + OP T = X i∈[n],x∈X Pr[Xi = x](d(x, σ(i)) + d(x, yi)) ≥ X i∈[n] d(yi, σ(i))Pi ≥ OP TY .</formula><p>The α-approximation algorithm is used to find a set of k-medians C for Y . Define σ (i) by σ (i) = arg minc∈C d(c, yi). This bounds the cost of the clustering with centers C as</p><formula xml:id="formula_44">E[ cost med (X, C , σ )] = X i∈[n],x∈X Pr[Xi = x]d(x, σ (i)) ≤ X i∈[n],x∈X Pr[Xi = x](d(x, yi) + d(yi, σ (i)))</formula><p>≤ T + αOP TY ≤ α(T + OP T ) + T ≤ (2α + 1)OP T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXTENSIONS</head><p>We consider a variety of related formulations, and show that of these, optimizing to minize the expected cost is the most natural and feasible. Bounded probability clustering. One possible alternate formulation of uncertain clustering is to require only that the probability of the cost of the clustering exceeding a certain amount is bounded. However, one can easily show that such a formulation does not admit any approximation. THEOREM 11. Given a desired cost τ , it NP-hard to approximate to any constant factor</p><formula xml:id="formula_45">min C:|C|=k Pr[cost med (X, C, ρ) &gt; τ ] , min C:|C|=k Pr[costmea(X, C, ρ) &gt; τ ] , or min C:|C|=k Pr[costcen(X, C, ρ) &gt; τ ] .</formula><p>PROOF. The hardness follows from considering deterministic arrangements of points, i.e., each point occurs in exactly one location with certainty. If there is an optimal clustering with cost τ or less, then the probability of exceeding this is 0; otherwise, the probability that the optimal cost exceeds τ is 1. Hence, any algorithm which approximates this probability correctly decides whether there is a k clustering with cost τ . But this is known to be NP-hard for the three deterministic clustering cost functions.</p><p>Consequently, similar optimization goals such as "minimize the expectation subject to Pr[costmea(X, C, ρ) &gt; τ ] &lt; δ" are also NP-hard.</p><p>Minimizing Variance. Rather than minimizing the expectation, it may be desirable to choose a clustering which has minimal variance, i.e., is more reliable. However, we can reduce this to the original problem of minimizing expectation. For example, in the point probability case when each Xi = xi with probability pi and is absent otherwise i.e., E[costmea(X, C, ρ)] where pi is replaced by pi -p 2 i . The same idea extends to minimizing the sum of p-th powers of the distances, since these too can be rewritten using cumulant relations. Note that if for all pi ≤ then pi(1 -) ≤ pi -p 2 i ≤ pi, and so there is little difference between E[costmea] and Var (cost med ) when the probabilities are small. But a clustering that minimizes Var(costcen(X, C, ρ)) can be a "very bad" clustering in terms of its expected cost when the probabilities are higher, as shown by this example: gives a (rather conservative) lower bound on the cost of producing a single clustering. The form of this optimization problem is closer to that of previously studied problems <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b22">22]</ref>, and so may be more amenable to similar approaches, such as sampling a number of possible worlds and estimating the cost in each.</p><p>Continuous Distributions. We have, thus far, considered discrete input distributions. In some cases, it is also useful to study continuous distributions, such as a multi-variate Gaussian, or an area within which the point is uniformly likely to appear. Some of our techniques naturally and immediately handle certain continuous distributions: following the analysis of uncertain k-means (Section 5.1), we only have to know the location of the mean in the assigned case, which can typically be easily calculated or is a given parameter of the distribution. It remains open to fully extend these results to continuous distributions. In particular, it sometimes requires a complicated integral just to compute the cost of a proposed clustering C under a metric such as k-center, where we need to evaluate Z ∞ 0 Pr[costcen(C, X, ρ) ≥ r]rdr over a potentially arbitrary collection of continuous pdfs. Correctly evaluating such integrals can require careful arguments based on numerical precision and appropriate rounding.</p><p>Facility Location. We have focused on the clustering version of problems. However, it is equally feasible to study related problems, in particular formulations such as Facility Location, where instead of a fixed number of clusters, there is a facility cost associated with opening a new center, and a service cost for assigning a point to a center, and the goal is to minimize the overall cost. Formalizing this as a deterministic facility cost and expected service cost is straightforward, and means that this and other variations are open for further study.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>x</head><label></label><figDesc>Pr[Xi = x] and σ 2 i = X x∈X d(x, µi) 2 Pr[Xi = x] ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Var(cost med (X, C, ρ)) = X i (pi -p 2 i )d 2 (xi, ρ(i, xi))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Example 4 .</head><label>4</label><figDesc>Consider picking 1-center on two points such that Pr[X1 = x1] = 1 and Pr[X2 = x2] = 0.5. Then Var(costcen(X, {x2})) = 0, while Var(costcen(X, {x1})) = d 2 (x1, x2)/4. But costcen(X, {x2}) = d(x1, x2) while costcen(X, {x1}) = 1 2 d(x1, x2): the zero-variance solution costs twice as much (in expectation) as the higher-variance solution.Expected Clustering Cost. Our problem was to produce a clustering that is good over all possible worlds. An alternative is to instead ask, given uncertain data, what is the expected cost of the optimal clustering in each possible world? That is,• k-median: E ˆmin C:|C|=k cost med (X, C, ρ) • k-means: E ˆmin C:|C|=k costmea(X,C, ρ) • k-center: E ˆmin C:|C|=k costcen(X, C, ρ) This</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Our Results for (α, β)-bicriteria approximations</figDesc><table><row><cell>α</cell><cell>β</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank S. Muthukrishnan and Aaron Archer for some stimulating discussions. We also thank Chandra Chekuri, Bolin Ding, and Nitish Korula for assistance in clarifying proofs.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Framework for clustering uncertain data streams</title>
		<author>
			<persName><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Engineering</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">kmeans++: The advantages of careful seeding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM-SIAM Symposium on Discrete Algorithms</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1027" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Local search heuristics for k-median and facility location problems</title>
		<author>
			<persName><forename type="first">V</forename><surname>Arya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Khandekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Meyerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Munagala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pandit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="544" to="562" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Approximate clustering via core-sets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Badoiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Har-Peled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Theory of Computing</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="250" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Uldbs: Databases with uncertainty and lineage</title>
		<author>
			<persName><forename type="first">O</forename><surname>Benjelloun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Very Large Data Bases</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Algorithms for facility location problems with outliers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Mount</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM-SIAM Symposium on Discrete Algorithms</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="642" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Uncertain data mining: An example in clustering location data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ngai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sketching probabilistic data streams</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Garofalakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGMOD International Conference on Management of Data</title>
		<meeting>ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="281" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient query evaluation on probabilistic databases</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Suciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB J</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="523" to="544" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A fuzzy relative of the ISODATA process and its use in detecting compact well-separated clusters</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Dunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cybernetics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="32" to="57" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A simple heuristic for the p-center problem</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Frieze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="285" to="288" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Second International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page">226</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A PTAS for k-means clustering based on weak coresets</title>
		<author>
			<persName><forename type="first">D</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Monemizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sohler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Computational Geometry</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Clustering to minimize the maximum intercluster distance</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="293" to="306" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">CURE: An efficient clustering algorithm for large databases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGMOD International Conference on Management of Data</title>
		<meeting>ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="73" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Har-Peled</surname></persName>
		</author>
		<ptr target="http://valis.cs.uiuc.edu/~sariel/teach/notes/aprx/book.pdf" />
		<title level="m">Geometric approximation algorithms</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On coresets for k-means and k-median clustering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Har-Peled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mazumdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Theory of Computing</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="291" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A best possible heuristic for the k-center problem</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hochbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shmoys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="180" to="184" />
			<date type="published" when="1985-05">May 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Applications of weighted voronoi diagrams and randomization to variance-based k-clustering (extended abstract)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Inaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Katoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Imai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Computational Geometry</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="332" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Algorithms for dynamic geometric problems over data streams</title>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Theory of Computing</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Estimating statistical aggregates on probabilistic data streams</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jayram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mcgregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Principles of Database Systems</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="243" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A nearly linear-time approximation scheme for the euclidean k-median problem</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Kolliopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Symposium on Algorithms</title>
		<meeting>European Symposium on Algorithms</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A simple linear time (1+ )-approximation algorithm for k-means clustering in any dimensions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Some method for the classification and analysis of multivariate observations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Berkeley Symposium on Mathematical Structures</title>
		<meeting>the 5th Berkeley Symposium on Mathematical Structures</meeting>
		<imprint>
			<date type="published" when="1967">1967</date>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient clustering of uncertain data</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Ngai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Chui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Yip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An O(log * n) approximation algorithm for the asymmetric p-center problem</title>
		<author>
			<persName><forename type="first">R</forename><surname>Panigrahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Algorithms</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">BIRCH: an efficient data clustering method for very large databases</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Livny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGMOD International Conference on Management of Data</title>
		<meeting>ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="103" to="114" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
