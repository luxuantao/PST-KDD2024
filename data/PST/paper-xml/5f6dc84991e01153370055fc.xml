<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-09-24">24 Sep 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
							<email>keyulu@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingling</forename><surname>Li</surname></persName>
							<email>jingling@cs.umd.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mozhi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
							<email>ssdu@cs.washington.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">|| National Institute of Informatics</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-09-24">24 Sep 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2009.11848v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study how neural networks trained by gradient descent extrapolate, i.e., what they learn outside the support of training distribution. Previous works report mixed empirical results when extrapolating with neural networks: while multilayer perceptrons (MLPs) do not extrapolate well in simple tasks, Graph Neural Networks (GNNs), a structured network with MLP modules, have some success in more complex tasks. We provide a theoretical explanation and identify conditions under which MLPs and GNNs extrapolate well. We start by showing ReLU MLPs trained by gradient descent converge quickly to linear functions along any direction from the origin, which suggests ReLU MLPs cannot extrapolate well in most non-linear tasks. On the other hand, ReLU MLPs can provably converge to a linear target function when the training distribution is "diverse" enough. These observations lead to a hypothesis: GNNs can extrapolate well in dynamic programming (DP) tasks if we encode appropriate non-linearity in the architecture and input representation. We provide theoretical and empirical support for the hypothesis. Our theory explains previous extrapolation success and suggest their limitations: successful extrapolation relies on incorporating task-specific non-linearity, which often requires domain knowledge or extensive model search.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Humans extrapolate well in many tasks. For example, we can apply arithmetics to arbitrarily large numbers. One may wonder whether a neural network can do the same and generalize to examples arbitrarily far from the training data <ref type="bibr" target="#b51">[Santoro et al., 2018]</ref>. Curiously, previous works report mixed extrapolation results with neural networks. Early works demonstrate feedforward neural networks, a.k.a. multilayer perceptrons (MLPs), fail to extrapolate well when learning simple polynomial functions <ref type="bibr" target="#b28">[Haley and</ref><ref type="bibr">Soloway, 1992, Barnard and</ref><ref type="bibr" target="#b7">Wessels, 1992]</ref>. However, recent works show Graph Neural Networks (GNNs) <ref type="bibr" target="#b54">[Scarselli et al., 2009]</ref>, a class of structured networks with MLP building blocks, can be successful in more challenging tasks, such as predicting the time evolution of physical systems <ref type="bibr" target="#b8">[Battaglia et al., 2016]</ref>, solving mathematical equations <ref type="bibr" target="#b37">[Lample and Charton, 2020]</ref>, and learning graph algorithms <ref type="bibr">[Velickovic et al., 2020]</ref>. In these tasks, GNNs can generalize to graphs larger than those in the training set. These results lead to a puzzle: What makes a neural network extrapolate well or not in a task?</p><p>We formally study how neural networks trained by (stochastic) gradient descent (GD) extrapolate, i.e., what they learn outside the support of training distribution. We say a neural network extrapolates well if it predicts accurately outside the training distribution. At first glance, it may seem that neural networks can behave arbitrarily outside the training distribution, since they have high capacity <ref type="bibr" target="#b67">[Zhang et al., 2017]</ref> and are universal approximators <ref type="bibr" target="#b18">[Cybenko, 1989</ref><ref type="bibr" target="#b32">, Hornik et al., 1989</ref><ref type="bibr" target="#b23">, Funahashi, 1989</ref><ref type="bibr" target="#b35">, Kurková, 1992]</ref>. However, neural network predictions are constrained by the implicit biases of gradient descent training <ref type="bibr" target="#b30">[Hardt et al., 2016</ref><ref type="bibr" target="#b57">, Soudry et al., 2018</ref><ref type="bibr" target="#b44">, Neyshabur et al., 2017</ref><ref type="bibr">, Li et al., 2018]</ref>. We analyze extrapolation with these implicit biases via the recently discovered equivalence of the training dynamics of over-parameterized neural networks and those of kernel regression with a specific neural tangent kernel (NTK) <ref type="bibr" target="#b34">[Jacot et al., 2018]</ref>.</p><p>We begin by studying MLPs, the simplest neural network and building blocks of more complex architectures such as GNNs. First, we show that the predictions of over-parameterized MLPs with ReLU activation trained by GD converge to linear functions along any direction from the origin. We prove a convergence rate and experimentally show that convergence often happens immediately outside the boundary of training data (Figure <ref type="figure" target="#fig_0">1</ref>). This behavior suggests ReLU MLPs do not extrapolate well for most non-linear tasks unless the target function also converges to linear functions along directions from the origin. Second, we identify situations where MLPs extrapolate well. When the target function of a task is globally linear, we show that ReLU MLPs trained by GD converge can extrapolate well: they converge to the target function if the geometry of training distribution satisfies a condition (Figure <ref type="figure" target="#fig_2">3</ref>). To our knowledge, we are the first to theoretically explain why extrapolation is hard and provide conditions for correct extrapolation.</p><p>Building on our insights of MLPs, we analyze how GNNs extrapolate and explain why GNNs extrapolate well in some tasks. We observe that the tasks where prior works report successful extrapolation can all be solved by dynamic programming (DP) <ref type="bibr" target="#b10">[Bellman, 1966]</ref>, which have a similar computation structure as GNNs <ref type="bibr" target="#b66">[Xu et al., 2020]</ref>. The DP updates can be decomposed into non-linear and linear steps. Hence, we hypothesize that GNNs trained by GD can extrapolate well in a DP task, if we encode appropriate non-linearity in the architecture and input representation (Figure <ref type="figure">2</ref>). Importantly, encoding non-linearity do not always help GNNs interpolate, because the MLP modules can easily learn many non-linear functions inside the training distribution <ref type="bibr" target="#b66">[Xu et al., 2020]</ref>. In contrast, encoding non-linearity are crucial for GNNs to extrapolate correctly. We prove the hypothesis for a simplified case by analyzing the Graph Neural Tangent Kernel <ref type="bibr">[Du et al., 2019b]</ref>. Empirically, we support our hypothesis on three DP tasks: max degree, shortest paths, and n-body problem. We confirm GNNs with appropriate architecture, input representation, and training distribution extrapolate to unseen graph sizes, graph structure, edge weights, and node features. Our theory explains the empirical success in previous works and suggests their limitations: successful extrapolation relies on incorporating task-specific non-linearity, which often requires domain knowledge or extensive model search.</p><p>In summary, we study how MLPs and GNNs extrapolate. We first show that ReLU MLPs trained by </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Σv</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GNN Architectures DP Algorithm (Target Function)</head><p>MLP has to learn non-linear steps MLP learns linear steps <ref type="bibr">w(v, u)</ref> ) minv minv (a) Network architecture. Figure <ref type="figure">2</ref>: How GNNs extrapolate. Since MLPs can extrapolate well when learning linear functions, we hypothesize that GNNs can extrapolate well in dynamic programming (DP) tasks if we encode appropriate non-linearity in the architecture (left) and/or input representation (right; through domain knowledge or representation learning). The non-linearity do not always help interpolation as it can be approximated by MLP modules, but makes the target function "easier" for extrapolation. We support the hypothesis theoretically (Theorem 9) and empirically (Figure <ref type="figure">5</ref>).</p><formula xml:id="formula_0">d[k][u] = d[k − 1][v] + w(v, u) h (k) u = MLP (k) ( h (k−1) v , h (k−1) u , w(v, u) ) h (k) u = MLP (k) ( h (k−1) v , h (k−1) u ,</formula><p>GD converge to linear functions along directions from the origin and prove a convergence rate. Our results formally explain why neural networks generally do not extrapolate well. We further show when MLPs and GNNs can extrapolate well. We hypothesize that encoding appropriate non-linearity into the architecture and input representation helps extrapolation. We prove the hypothesis for a simplified case and provide experiment support for more general settings. All our claims are supported by experimental results.<ref type="foot" target="#foot_0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related work</head><p>There have been extensive theoretical studies on MLPs. Early works show example tasks where MLPs do not extrapolate well <ref type="bibr" target="#b28">[Haley and</ref><ref type="bibr">Soloway, 1992, Barnard and</ref><ref type="bibr" target="#b7">Wessels, 1992]</ref>. We instead show a general pattern of how ReLU MLPs extrapolate and identify when MLPs can extrapolate well, with theoretical support. Previous works also show that ReLU MLPs have finitely many linear regions <ref type="bibr" target="#b3">[Arora et al., 2018</ref><ref type="bibr" target="#b31">, Hein et al., 2019]</ref>, which implies that MLPs cannot extrapolate well far away from training data. We take a step further and provide a convergence rate for how fast ReLU MLPs converge to linear functions. Our results suggest that MLPs often fail even on examples close to (but outside) the training distribution.</p><p>More recent works study the implicit bias induced on MLPs by gradient descent. They study biases for both the "NTK" and "adaptive" regimes <ref type="bibr" target="#b63">[Williams et al., 2019</ref><ref type="bibr" target="#b42">, Maennel et al., 2018</ref><ref type="bibr" target="#b56">, Song et al., 2018</ref><ref type="bibr" target="#b52">, Savarese et al., 2019</ref><ref type="bibr" target="#b16">, Chizat and Bach, 2018</ref><ref type="bibr" target="#b41">, Li et al., 2019]</ref>. Closest to our work are results showing that MLP predictions converge to "simple" piecewise linear functions, e.g., with few linear regions <ref type="bibr" target="#b63">[Williams et al., 2019</ref><ref type="bibr" target="#b42">, Maennel et al., 2018</ref><ref type="bibr" target="#b52">, Savarese et al., 2019</ref><ref type="bibr" target="#b29">, Hanin and Rolnick, 2019]</ref>. Our work differs in three aspects:</p><p>(1) none of these works explicitly studies extrapolation;</p><p>(2) some of them focus on one-dimensional inputs <ref type="bibr" target="#b63">[Williams et al., 2019</ref><ref type="bibr" target="#b52">, Savarese et al., 2019]</ref>; and (3) none of them studies GNNs. Our results complement those results, but for extrapolation in multiple dimensions for MLPs and GNNs. Previous works explore extrapolation with GNNs by testing on graphs larger than those in training sets <ref type="bibr" target="#b8">[Battaglia et al., 2016</ref><ref type="bibr">, 2018</ref><ref type="bibr">, Velickovic et al., 2020</ref><ref type="bibr">, Santoro et al., 2018</ref><ref type="bibr" target="#b53">, Saxton et al., 2019]</ref>. We provide a theoretical explanation and expand the notion of extrapolation on graphs by considering unseen edge weights, node features, and graph structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>We begin by introducing our setting. Let X be the domain of interest. We focus on two types of domains: vectors and graphs. Our task is to learn an underlying function g : X → R. We consider supervised learning with a training set {(x i , y i )} n i=1 ⊂ D, where D is the support of training distribution. We assume our training set is clean; i.e., y i = g(x i ) for any i.</p><p>Previous works have extensively studied in-distribution generalization by assuming that the training and the test distributions are the same <ref type="bibr" target="#b60">[Valiant, 1984</ref><ref type="bibr" target="#b61">, Vapnik, 2013]</ref>; i.e., D = X . In our setting, the support of the training distribution D is smaller than the domain X , and we study how the model extrapolates from D to X . For example, X can be the d-dimensional vector space R d , and D can be a d-dimensional hypercube [−1, 1] d . We say a model extrapolates well in a task if it has small extrapolation error, the maximum test error outside the training support D.</p><p>Definition 1. (Extrapolation error). Suppose f : X → R is a model trained on {(x i , y i )} n i=1 ⊂ D. We define the extrapolation error of f on X as f − g ∞,X \D = sup{|f (x) − g(x)| : x ∈ X \ D}.</p><p>Alternatively, we can define extrapolation error as the expected error over a test distribution on X \ D. In Definition 1, We use infinity norm for simplicity.</p><p>This paper focuses on neural networks trained by gradient descent or its variants with mean squared loss. We study two neural network architectures: MLPs and GNNs.</p><p>Graph Neural Networks. GNNs are structured network operating on graphs with MLP modules <ref type="bibr">[Battaglia et al., 2018]</ref>. The input is a graph G = (V, E). Each node u ∈ V has a feature vector x u , and each edge (u, v) ∈ E has a feature vector w <ref type="bibr">(u,v)</ref> . GNN iteratively computes a representation for each node. Initially, the node representation h</p><formula xml:id="formula_1">(0) u of each node u is the node feature x u . At iteration k = 1..K, GNN updates the representation h (k)</formula><p>u of each node u by aggregating the representation of neighboring nodes with MLP modules <ref type="bibr" target="#b25">[Gilmer et al., 2017</ref><ref type="bibr" target="#b64">, Xu et al., 2018</ref><ref type="bibr">, 2019]</ref>. After the updates, we can compute a graph representation h G by aggregating the final representation of all nodes with a final MLP. Formally,</p><formula xml:id="formula_2">h (k) u = v∈N (u) MLP (k) h (k−1) u , h (k−1) v , w (u,v) , h G = MLP (K+1) u∈G h (K) u .<label>(1)</label></formula><p>Depending on the task, the final output can be the graph representation h G or final node representations h (K)</p><p>u . We refer to the neighbor aggregation step for h (k) u as aggregation and the pooling step in h G as readout. In the GNN literature, sum-pooling is often used for both aggregation and readout <ref type="bibr">[Battaglia et al., 2018]</ref>. We show how replacing them with non-linear operations may help extrapolation (Section 4).  Related settings. Previous works have studied some related settings. Domain adaptation aim to generalize beyond the training distribution <ref type="bibr" target="#b11">[Ben-David et al., 2010</ref><ref type="bibr" target="#b43">, Mansour et al., 2009]</ref>, often by adversarial learning using samples from the target domain <ref type="bibr" target="#b13">[Blitzer et al., 2008</ref><ref type="bibr" target="#b24">, Ganin et al., 2016</ref><ref type="bibr" target="#b69">, Zhao et al., 2018]</ref>. These methods do not extrapolate well, as they fail when the label distribution of training and test data are different <ref type="bibr" target="#b70">[Zhao et al., 2019]</ref>. Distributional robustness <ref type="bibr" target="#b26">[Goh and Sim, 2010</ref><ref type="bibr" target="#b55">, Sinha et al., 2018</ref><ref type="bibr" target="#b49">, Sagawa et al., 2020]</ref> and adversarial examples <ref type="bibr" target="#b58">[Szegedy et al., 2014</ref><ref type="bibr" target="#b27">, Goodfellow et al., 2015]</ref> consider small adversarial perturbations within a pre-specified set, which can be special cases of extrapolation. We study generalization beyond local neighborhoods. Another line of work studies out-of-distribution generalization by learning invariant features <ref type="bibr" target="#b2">[Arjovsky et al., 2019</ref><ref type="bibr" target="#b48">, Rojas-Carulla et al., 2018]</ref>, which helps when there are spurious correlations. We show why they do not guarantee correct extrapolation (Section 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">How ReLU Multilayer Perceptrons Extrapolate</head><p>MLPs are the simplest neural networks and the building blocks of more complex networks such as GNNs, so we start by studying how MLPs trained by gradient descent (GD) extrapolate. Throughout the paper, we assume that the MLP has ReLU activation functions. We present preliminary results for other activation functions in Appendix D.3,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Why Extrapolation is Hard: Linear Extrapolation Behavior of ReLU MLPs</head><p>Surprisingly, ReLU MLPs extrapolate with a simple pattern: outside the training data range, MLP predictions quickly become linear along directions from origins (Figure <ref type="figure" target="#fig_0">1</ref>). We empirically verify this pattern by running linear regressions on MLPs' predictions (details in Appendix C.2). Outside the training data range, along any directions from the origin, the coefficient of determination (R 2 ) is always greater than 0.99; i.e., MLPs "linearize" almost immediately outside the training data range. This behavior suggests that MLPs cannot extrapolate well for most non-linear target functions.</p><p>We formalize this observation by exploiting the implicit biases of neural networks trained by GD via the neural tangent kernel (NTK) theory: the optimization trajectories of overparameterized networks trained by GD are equivalent to those of kernel regression with a specific neural tangent kernel, under a set of assumptions called the "NTK regime" <ref type="bibr" target="#b34">[Jacot et al., 2018]</ref>. We give the informal definition below and refer readers to <ref type="bibr" target="#b34">Jacot et al. [2018]</ref> for the complete definition. Appendix A provides more background on NTK.</p><p>Definition 2. (NTK regime; informal) A neural network trained in the NTK regime is infinitely wide, randomly initialized with certain scaling, and trained by gradient descent with infinitesimally small learning rate using squared loss.</p><p>Previous works analyze the optimization and in-distribution generalization behavior of overparameterized neural networks via NTK <ref type="bibr" target="#b4">[Arora et al., 2019a</ref><ref type="bibr">,b, Allen-Zhu et al., 2019a</ref><ref type="bibr">,b, Li and Liang, 2018</ref><ref type="bibr" target="#b34">, Jacot et al., 2018</ref><ref type="bibr" target="#b22">, Du et al., 2019c</ref><ref type="bibr">,a, Lee et al., 2019</ref><ref type="bibr" target="#b14">, Cao and Gu, 2019]</ref>. We instead use NTK to analyze the out-of-distribution learning of overparameterized networks.</p><p>We theoretically explain our observation from Figure <ref type="figure" target="#fig_0">1</ref> in Theorem 3: outside the training data range, along any direction from the origin, the predictions of a two-layer ReLU MLP converges to a linear function with convergence rate O( 1 ). The linear coefficient β v and the constant terms in convergence rate O( 1 ) depend on the training data and direction v. We prove Theorem 3 in Appendix B.1.</p><p>Theorem 3. Suppose we train a two-layer ReLU network f : R d → R on {(x i , y i )} n i=1 in the NTK regime. For any given direction v,</p><formula xml:id="formula_3">let x 0 = tv. As t → ∞, f (x 0 + hv) − f (x 0 ) → β v • h for any h &gt; 0, where β v is a constant linear coefficient. Moreover, given &gt; 0, for t = O( 1 ), we have | f (x 0 +hv)−f (x 0 ) h − β v | &lt; .</formula><p>Experiments: Hardness of extrapolation. Theorem 3 suggests that MLPs trained by GD generally cannot extrapolate well in a task, unless the target function is linear along directions from the origin outside the training distribution. This excludes most non-linear functions. To empirically support the hardness of extrapolation, we train MLPs on tasks with various target functions and training/test distributions (Appendix C.1). Indeed, MLPs do not extrapolate well in tasks with most non-linear target functions (Figure <ref type="figure" target="#fig_3">4a</ref>), including x Ax (quadratic), d i=1 cos(2π • x (i) ) (cos), and</p><formula xml:id="formula_4">d i=1 √ x (i) (sqrt)</formula><p>, where x (i) is the i-th dimension of input vector x. On the other hand, MLPs can extrapolate well for the L1 norm function, d i=1 |x (i) |, with the right hyperparameter (Figure <ref type="figure" target="#fig_3">4a</ref>). This is consistent with Theorem 3, since L1 is linear along any direction outside the training distribution.</p><p>Comparison with previous results. Previously, <ref type="bibr" target="#b3">Arora et al. [2018]</ref> show that ReLU MLPs have finitely many linear regions, which implies that MLP cannot perfectly learn many non-linear function. In contrast, we focus on the extrapolation setting, and we provide a more specific explanation of how MLPs extrapolate. Specifically, we show that MLP predictions immediately converge to linear functions outside the training range along directions from origins, and we provide a convergence rate.</p><p>Theorem 3 formally shows that extrapolation is hard: MLPs cannot extrapolate well for most non-linear tasks, because MLP predictions quickly converge to linear functions along directions across origin. However, our experiments provide one positive result: MLPs can extrapolate well when the target function is linear (Figure <ref type="figure" target="#fig_3">4a</ref>). While learning linear functions may seem unimportant at first, we will use this insight to explain why GNNs can extrapolate well in non-linear practical tasks (Section 4). Before that, we first theoretically analyze when MLPs can extrapolate well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">When ReLU MLPs Provably Extrapolate Well</head><p>Figure <ref type="figure" target="#fig_3">4a</ref> shows that MLPs can extrapolate well when the target function is linear. However, this is not always true. In this section, we show that successful extrapolation depends on the geometry of training data. Intuitively, the training distribution must be "diverse" enough for MLPs to learn a linear target function.</p><p>We provide two conditions to show how the geometry of training data affects extrapolation. First, Lemma 4 shows that overparameterized MLPs can learn the linear target function with only 2d training examples.</p><p>Lemma 4. Assume all data satisfies y = β x for some β. Suppose the training data {x i } n i=1 contains an orthogonal basis {x i } d i=1 and their opposite vectors {−x i } d i=1 . Suppose we train a two-layer ReLU network f on {(x i , y i )} n i=1 in the NTK regime. Then the network learns</p><formula xml:id="formula_5">f (x) = β x for all x ∈ R d .</formula><p>This lemma is mainly of theoretical interest, as the 2d examples need to be carefully chosen to cover an orthogonal basis and their opposite vectors. Theorem 5 builds on Lemma 4 to study a more practical condition: if the support of training distribution D covers all directions (e.g., a hypercube or hypersphere that covers the origin), an overparameterized MLP converges to the linear target function with enough training data.</p><p>Theorem 5. Assume all data satisfies y = β x for some β. Suppose the training data {x i } n i=1 is sampled from a distribution whose support contains a connected set S, where for any non-zero w ∈ R d , there exists k &gt; 0 so that kw ∈ S. Suppose we train a two-layer ReLU network f : R d → R on {(x i , y i )} n i=1 in the NTK regime. Then as n → ∞, the network learns f (x) p −→ β x.</p><p>Experiments: geometry of training data affects extrapolation. The condition in Theorem 5 explains the intuition that training distribution must be "diverse" for successful extrapolation, as D must cover all directions. Empirically, our experiments support the importance of this "diversity" condition. Extrapolation error is indeed small when the condition of Theorem 5 is satisfied, i.e., when D covers all directions ("all" in Figure <ref type="figure" target="#fig_3">4b</ref>). This holds for different hyperparameters, including the width and depth of the network, learning rate, and batch size (Appendix C.3). In contrast, extrapolation error is much higher when the condition of Theorem 5 does not hold (Figure <ref type="figure" target="#fig_3">4b</ref>). Figure <ref type="figure" target="#fig_2">3</ref> illustrates that MLPs extrapolate differently from the target function when the training examples are restricted to some directions; e.g., if we constrain some features to be positive, negative, or constant.</p><p>Connection to spurious correlation. This condition also explains why spurious correlations hurt extrapolation: spurious correlations are equivalent to not seeing some directions, or combinations of features, in the training set; e.g., camels might only appear in deserts in an image collection. Therefore, the condition of Theorem 5 does not hold when there is spurious correlation. Our perspective is complementary to the causality argument from previous works <ref type="bibr" target="#b2">[Arjovsky et al., 2019</ref><ref type="bibr" target="#b46">, Peters et al., 2016</ref><ref type="bibr" target="#b48">, Rojas-Carulla et al., 2018]</ref>.</p><p>"Identifiability" condition. Interestingly, Theroem 5 is analogous to the identifiability condition for linear models. Specifically, we can uniquely identify a linear function if we have enough training data from an arbitrary d-dimensional subspace of R d . MLPs are more expressive, so "identifying" the linear target function requires an additional constraint: the training distribution must cover all directions.</p><p>To recap, we theoretically explain how MLPs extrapolate. Our analysis leads to two insights: (1) MLPs cannot extrapolate in most non-linear tasks, because they quickly converge to linear function along every directions from the origin; and (2) MLPs can extrapolate well when the target function is linear, provided the training distribution satisfies the assumption in Theorem 5. While learning linear functions is not always useful by itself, we use these insights to understand how more complex networks extrapolate in the next section. Specifically, we explain why GNNs can extrapolate well in challenging non-linear tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">How Graph Neural Networks Extrapolate</head><p>In the previous section, we show that extrapolation in non-linear tasks is hard (Theorem 3). Despite this limitation, GNNs can extrapolate well in some non-linear tasks, such as intuitive physics <ref type="bibr" target="#b8">[Battaglia et al., 2016</ref><ref type="bibr">, Sanchez-Gonzalez et al., 2018]</ref>, graph algorithms <ref type="bibr">[Battaglia et al., 2018</ref><ref type="bibr">, Velickovic et al., 2020]</ref>, and symbolic mathemtaics <ref type="bibr" target="#b37">[Lample and Charton, 2020]</ref>. We explain this puzzle by studying how GNNs trained by GD extrapolate, using our insights of MLPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Hypothesis</head><p>We start from an example of training GNNs to solve the shortest path problem. Prior works demonstrate that a modified GNN architecture with max or min-pooling aggregation can generalize to graphs larger than those on training set <ref type="bibr">[Battaglia et al., 2018</ref><ref type="bibr">, Velickovic et al., 2020]</ref>.</p><formula xml:id="formula_6">h (k) u = min v∈N (u) MLP (k) h (k−1) u , h (k−1) v , w (u,v) . (<label>2</label></formula><formula xml:id="formula_7">)</formula><p>Shortest path is also known to be solvable by the Bellman-Ford (BF) algorithm <ref type="bibr" target="#b10">[Bellman, 1958]</ref> with the following recursive update equation:</p><formula xml:id="formula_8">d[k][u] = min v∈N (u) d[k − 1][v] + w(v, u),<label>(3)</label></formula><p>where w(v, u) is the weight of edge (v, u), and d[k][u] represents the shortest path from the source node s to node u within k steps. Comparing the above equations, we notice that the computation structure of min-pooling GNN and Bellman-Ford algorithm are similar. Specifically, GNN can simulate BF algorithm if the MLP modules extrapolate as a linear function d[k − 1][v] + w(v, u). In the previous section, we show that MLPs trained by GD can indeed extrapolate well in linear tasks (Theorem 5). Therefore, this might explain why GNN with max/min-pooling can also extrapolate correctly in the shortest path task. Figure <ref type="figure">2a</ref> illustrates this intuition.</p><p>On the other hand, suppose we train the more commonly used GNN with sum-pooling aggregation</p><formula xml:id="formula_9">h (k) u = v∈N (u) MLP (k) h (k−1) u , h (k−1) v , w (u,v) .<label>(4)</label></formula><p>To learn BF algorithm with sum-pooling GNN, the MLP modules must learn a non-linear function. Since MLPs cannot extrapolate well in non-linear tasks (Theorem 3), GNNs with sum-pooling also do not extrapolate well in this task. We now extend the above intuition to other tasks. In general, the target tasks where GNNs extrapolate well can be solved by dynamic programming (DP) <ref type="bibr" target="#b10">[Bellman, 1966]</ref>, an algorithmic paradigm with a recursive structure similar to GNN's (equation 1) <ref type="bibr" target="#b66">[Xu et al., 2020]</ref>. Definition 6. Dynamic programming (DP) is defined by the recursive procedure.</p><formula xml:id="formula_10">Answer[k][i] = DP-Update({Answer[k − 1][j]} , j = 1...n),<label>(5)</label></formula><p>where <ref type="bibr">Answer[k][i]</ref> is the solution to a sub-problem indexed by iteration k and state i, and DP-Update is a task-specific update function that computes Answer</p><formula xml:id="formula_11">[k][i] from Answer[k − 1][j]'s.</formula><p>Based on the extrapolation behavior of MLP, we make the following hypothesis: given a DP task, if we can encode appropriate non-linearity in the model architecture and input representations so that the MLP modules only need to learn a linear step, then GNNs can extrapolate well.</p><p>Hypothesis 7. (Linear algorithmic alignment). Let f : X → R be an algorithm and N a neural network with m MLP modules. Suppose there exist m linear functions {g i } m i=1 so that by replacing N 's MLP modules with g i 's, N simulates f . Then given &gt; 0, there exists a training set {(x i , f (x i ))} n i=1 ⊂ D X so that the GNN trained on {(x i , f (x i ))} n i=1 by GD and squared loss learns f with f − f &lt; .</p><p>The "linear algorithmic alignment" assumption in Hypothesis 7 requires a GNN is able to represent the target DP algorithm by replacing its MLP modules with some linear functions. We introduce two approaches to help satisfy the assumption: encoding the appropriate non-linear operations in the architecture or input representation.</p><p>First, similar to encoding "min" into GNN aggregation in shortest path, we can encode appropriate non-linearity into the architecture. This approach applies to many DP tasks, whose solution can usually be decomposed into non-linear and linear steps.</p><p>For some tasks, the non-linear steps cannot be easily encoded into architecture. However, we may still be able to extrapolate well if we use a good input representation. (Figure <ref type="figure">2b</ref>). Sometimes we can decompose the target function f into two functions f = g • h, where g is a "simpler" target function that our model can extrapolate well. If we can identify h through domain knowledge, then we can transform the input with h, and the model only needs to learn g <ref type="bibr" target="#b37">[Lample and</ref><ref type="bibr">Charton, 2020, Zhang et al., 2019]</ref>. Alternatively, h can be obtained through representation learning using unlabeled out-of-distribution data from X \ D <ref type="bibr" target="#b19">[Devlin et al., 2019</ref><ref type="bibr">, Peters et al., 2018</ref><ref type="bibr" target="#b15">, Chen et al., 2020</ref><ref type="bibr" target="#b33">, Hu et al., 2020]</ref>. This might explain why pre-trained representations such as BERT can improve out-of-distribution robustness <ref type="bibr">[?]</ref>.</p><p>Previous works indeed use these two approaches to help extrapolation by using specialized architectures <ref type="bibr">[Velickovic et al., 2020]</ref> and input representations <ref type="bibr" target="#b37">[Lample and Charton, 2020]</ref>. Therefore, our theory explains these success stories. In addition, our theory explains why GNNs by default (with sum-pooling) often do not extrapolate well <ref type="bibr" target="#b51">[Santoro et al., 2018</ref><ref type="bibr" target="#b53">, Saxton et al., 2019]</ref>. Our theory also suggests that extrapolation is hard in general, because encoding appropriate non-linearity often requires domain expertise fo the task and/or extensive model tuning.</p><p>Closely related to our work, <ref type="bibr" target="#b66">Xu et al. [2020]</ref> studies the connection between GNNs and DP for interpolation. They show GNNs trained by GD have good in-distribution generalization for DP tasks and relate the sample complexity with an "algorithmic alignment" measure. In comparison, we show good extrapolation requires additional assumptions, the "linear algorithmic alignment" on architecture and representation.</p><p>Next, we give theoretical and empirical support for Hypothesis 7. While we focus on GNNs and graphbased reasoning tasks, our hypothesis and insights are general and may be applied to other networks in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Theoretical and Empirical Support</head><p>We study three DP tasks: max degree,<ref type="foot" target="#foot_1">2</ref> shortest path, and n-body problem. Empirically, we show GNNs extrapolate well if and only if we encode proper non-linearity in the architectures and/or input representations. Theoretically, we prove Hypothesis 7 for an one-step DP task (max degree), and show how the graph structure in training set affects extrapolation.</p><p>Theoretical analysis. Given an input graph G = (V, E), its label in the max degree task is g(G) = max u∈G d u , where d u is the degree of node u. We first show that, as a Corollary of Theorem 3, the commonly used GNN (equation 1) cannot extrapolate well in this task (proof in Appendix B.4).  Corollary 8. GNNs with sum-pooling aggregation and readout do not extrapolate well in max degree.</p><p>For comparison, we consider GNN with max-pooling readout:</p><formula xml:id="formula_12">h G = MLP (2) max u∈G v∈N (u) MLP (1) x u , x v , w (u,v) .</formula><p>This architecture satisfies Hypothesis 7 because it encodes the only non-linearity in the task, max, and thus can simulate max degree if its MLP modules correctly learns the linear function. Theorem 9 confirms our hypothesis: GNN with max-readout indeed extrapolates well in this task, if the graph structure covered in the training set is not too restricted.</p><p>Theorem 9. Assume all graphs G have uniform node features. Let</p><formula xml:id="formula_13">g(G) = max u∈G d u . Let {G i } n i=1 be the training set. If the vectors {(max u∈G i d u , min u∈G i d u , max u∈G i d u • N max i , min u∈G i d u • N min i )} n i=1 span R 4</formula><p>, where N max i and N min i are the number of nodes achieving max/min degree on G i , then an one-layer GNN f with max-pooling readout trained on {(G i , y i )} n i=1 in the NTK regime learns f = g.</p><p>Theorem 9 does not follow immediately from Theorem 5, because MLP modules in GNN only receive indirect supervision. We use Graph Neural Tangent Kernel <ref type="bibr">[Du et al., 2019b]</ref> to analyze GNNs' GD training dynamics (proof in Appendix B.5).</p><p>Interpretation of conditions. The condition in Theorem 9 is analogous to that of Theorem 5. Both theorems require diverse training data, measured by graph structure in Theorem 9 or directions in Theorem 5.</p><p>In Theorem 9, the condition is violated if all training graphs have the same max or min node degrees. For example, the condition is not satisified when training data comes from the following graph structure families: path, C-regular (regular graphs with degree C), cycle, and ladder, because they always have constant max degrees (2, C, 2, 3 respectively). On the other hand, the following graph structure families satisfy the condition: trees, complete graphs, expanders, and general graphs. We refer general graphs to Erdős-Rényi random graphs sampled with a wide range of edge probabilities, which covers diverse graph structures.</p><p>Experiments: Architectures that help extrapolation.</p><p>We validate Hypothesis 7, Corollary 8, and Theorem 9 with two DP tasks: max degree and shortest path (details in Appendix C.5 and C.6). Our results confirm how architecture and training graph structure affect extrapolation.</p><p>For the max degree task, GNNs with sum-pooling does not extrapolate well (Figure <ref type="figure">5a</ref>), confirming Corollary 8. In contrast, as predicted by Theorem 9, GNNs that encode suitable non-linearity (i.e., max) in readout has small extrapolation error (0.0 to 0.1 MAPE), if the training set satisfies the condition in Theorem 9 (Figure <ref type="figure" target="#fig_4">6a</ref>). The condition on training data is important for extrapolation: GNNs extrapolate well when trained on trees, complete graphs, expanders, and general graphs, but the extrapolation error is much higher when trained on 4-regular, cycles, or ladder graphs (6.4 to 94.5 MAPE). Finally, while Theorem 9 assumes uniform feature, we observe similar results when using non-uniform node features (Figure <ref type="figure" target="#fig_17">15</ref> in Appendix).</p><p>Experiments on shortest path further supports Hypothesis 7 with experiments on shortest path. We test GNNs on graphs with unseen sizes, structure, edge weights and node features, while previous works on shortest path only consider graph sizes <ref type="bibr">[Battaglia et al., 2018</ref><ref type="bibr">, Velickovic et al., 2020]</ref>. The results are consistent with what we hypothesized above: GNNs with sum-pooling aggregation and readout do not extrapolate well (Figure <ref type="figure">5a</ref>), while GNNs with min-pooling in aggregation and readout extrapolate well (0.0 MAPE) if the graph structure covered by training set is not restricted (Figure <ref type="figure" target="#fig_4">6b</ref>). This again confirms that encoding non-linearity in the architecture helps extrapolation.</p><p>Interestingly, the required training graph families are different for shortest path and max degree. For shortest path, extrapolation errors follow a U-shaped curve as we change the sparsity of training graphs (Figure <ref type="figure" target="#fig_4">6b</ref> and Figure <ref type="figure" target="#fig_19">17</ref> in Appendix). Intuitively, models trained on sparse or dense graphs are more likely to learn dengerative solutions, because sparse graphs often have only one or two paths between a pair of nodes, while the shortest path is often whithin one or two hops in dense graphs.</p><p>Experiments: Representations that help extrapolation. Finally, we show a good input representation also helps extrapolation. We study a well-known physics task, the n-body problem <ref type="bibr" target="#b8">[Battaglia et al., 2016</ref><ref type="bibr" target="#b62">, Watters et al., 2017]</ref> (Appendix C.7). The goal is to predict the time evolution of n objects in a gravitational system. Following previous work, we consider a complete graph where the nodes are the objects <ref type="bibr" target="#b8">[Battaglia et al., 2016]</ref>. Each node u has feature</p><formula xml:id="formula_14">h u = (m u , x (t) u , v (t)</formula><p>u ), a concatenation of the object's mass, position and velocity at time t. There is no pairwise feature, so edge features are set to zero. We train GNNs to predict the velocity of each object u at time t + 1. By physics law, it is known that the true prediction f (G; u) for object u is approximately</p><formula xml:id="formula_15">f (G; u) ≈ v t u + a t u • dt, a t u = C • v =u m v x t u − x t v 3 2 • x t v − x t u ,<label>(6)</label></formula><p>where C is some constant. To learn f , the MLP modules need to learn a non-linear function. Therefore GNNs cannot extrapolate well in this task. Experiment results confirm this: GNNs have high extrapolation error with unseen masses or distances ("original features" in Figure <ref type="figure">5b</ref>).</p><p>To extrapolate well in this task, we can use an improved representation h(G) to satisfies the condition of Hypothesis 7. At time t, for any edge (u, v), we transform its original feature 0 to w</p><formula xml:id="formula_16">(u,v) = m v • x (t) v − x (t) u / x (t) u − x (t) v 3</formula><p>2 to encode the non-linearity. The new edge feature does not provide additional information, but the new target function g(G; u)</p><formula xml:id="formula_17">≈ v t u + C • dt • v =u w (u,v</formula><p>) is easier for GNN to extrapolate correctly. Experiments confirm that GNNs trained on the improved representation achieve 1.5 and 1.1 test MAPE when extrapolating to different star masses and distances, significantly improving upon the 11.0 and 6.3 test MAPE with the original representation (Figure <ref type="figure">5b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper is an initial step towards formally understanding how neural networks trained by gradient descent extrapolate. We show extrapolation is provably hard. We further solve the puzzle that while MLPs usually do not extrapolate well in simple tasks, how GNNs could extrapolate well in more challenging tasks: Encoding appropriate non-linearity in architecture and input representation can help extrapolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Theoretical Background</head><p>In this section we introduce theoretical background on neural tangent kernel (NTK), which draws an equivalence between the training dynamics of infinitely-wide (or ultra-wide) neural networks and that of a kernel regression with respect to neural tangent kernel.</p><p>Consider a general neural network f (θ, x) : X → R where θ ∈ R m is the parameters in the network and x ∈ X is the input. Suppose we train the neural network by minimizing the mean squared loss over training data, (θ) = 1 2 n i=1 (f (θ, x i ) − y i ) 2 , by gradient descent with infinitesimally small learning rate, i.e., dθ(t)  dt = −∇ (θ(t)). Let u(t) = (f (θ(t), x i )) n i=1 be the network outputs. u(t) follows the dynamics</p><formula xml:id="formula_18">du(t) dt = −H(t)(u(t) − y),</formula><p>where H(t) is an n × n matrix whose (i, j)-th entry is</p><formula xml:id="formula_19">H(t) ij = ∂f (θ(t), x i ) ∂θ , ∂f (θ(t), x j ) ∂θ .</formula><p>A line of works show that for sufficiently wide networks, H(t) stays almost constant during training, i.e., H(t) = H(0) in the limit <ref type="bibr" target="#b4">[Arora et al., 2019a</ref><ref type="bibr">,b, Allen-Zhu et al., 2019a</ref><ref type="bibr" target="#b22">, Du et al., 2019c</ref><ref type="bibr" target="#b39">,a, Li and Liang, 2018</ref><ref type="bibr" target="#b34">, Jacot et al., 2018]</ref>. Suppose network parameters are randomly initialized, as network width goes to infinity, H(0) converges to a fixed matrix, the neural tangent kernel (NTK) <ref type="bibr" target="#b34">[Jacot et al., 2018</ref>]</p><formula xml:id="formula_20">NTK(x, x ) = E θ∼W ∂f (θ(t), x) ∂θ , ∂f (θ(t), x ) ∂θ ,<label>(7)</label></formula><p>where W is Gaussian. Therefore, the training dynamics of sufficiently wide neural networks in this regime is equivalent to that of kernel regression with respect to NTK. This implies the function learned by a neural network given a training set, denoted by f NTK (x), can be precisely characterized, and is equivalent to the following kernel regression solution</p><formula xml:id="formula_21">f NTK (x) = (NTK(x, x 1 ), ..., NTK(x, x n )) • NTK −1 train Y ,<label>(8)</label></formula><p>where NTK train is the n × n kernel for training data, NTK(x, x i ) is the kernel value between test data x and training data x i , and Y is training labels. We can in fact exactly calculate the neural tangent kernel matrix. Exact formula of NTK has been derived for multi-layer perceptron (MLP), a.k.a. fully-connected networks <ref type="bibr" target="#b34">[Jacot et al., 2018]</ref>, convolutional networks <ref type="bibr" target="#b5">[Arora et al., 2019b]</ref>, and Graph Neural Networks (GNN) <ref type="bibr">[Du et al., 2019b]</ref>.</p><p>Our theory builds upon this equivalence of network learning and kernel regression to more precisely characterize the function learned by a sufficiently-wide neural network given a training set. In particular, the difference between the learned function and true function over the domain of X determines the extrapolation error.</p><p>However, in general it is non-trivial to compute or analyze the functional form of what a neural network learns using equation 8, because the kernel regression solution using neural tangent kernel only gives pointwise evaluation. Thus, we instead analyze the function learned by a network in the NTK's induced feature space, because representations in the feature space would give a functional form.</p><p>Lemma 10 makes this connection more precise: the solution to the kernel regression using neural tangent kernel, which also equals over-parameterized network learning, is equivalent to a min-norm solution among functions in the NTK's induced feature space that fits all training data. Here the min-norm refers to the RKHS norm.</p><p>Lemma 10. Let φ(x) be a feature map induced by a neural tangent kernel, for any x ∈ R d . The solution to kernel regression equation 8 is equivalent to f NTK (x) = φ(x) β NTK , where β NTK is</p><formula xml:id="formula_22">min β β 2 s.t. φ(x i ) β = y i , for i = 1, ..., n.</formula><p>We prove Lemma 10 in Appendix B.6. To analyze the learned functions as the min-norm solution in feature space, we also need the explicit formula of an induced feature map of the corresponding neural tangent kernel.</p><p>Next, we give a NTK feature space for MLPs with ReLU activation. It follows easily from the kernel formula described in <ref type="bibr" target="#b34">Jacot et al. [2018]</ref>, <ref type="bibr" target="#b5">Arora et al. [2019b]</ref>, <ref type="bibr" target="#b12">Bietti and Mairal [2019]</ref>.</p><p>Lemma 11. An infinite-dimensional feature map φ(x) induced by the neural tangent kernel of a two-layer multi-layer perceptron with ReLU activation function is</p><formula xml:id="formula_23">φ (x) = c x • I w (k) x ≥ 0 , w (k) x • I w (k) x ≥ 0 , ... ,<label>(9)</label></formula><p>where w (k) ∼ N (0, I), with k going to infinity. c is a constant, and I is the indicator function.</p><p>We prove Lemma 11 in Appendix B.7. The feature maps for other architectures, e.g., Graph Neural Networks (GNNs) can be derived similarly. We analyze the Graph Neural Tangent Kernel (GNTK) for a simple GNN architecture in Theorem 9.</p><p>We then use Lemma 10 and 11 to characterize the properties of functions learned by an over-parameterized neural network. We precisely characterize the neural networks' learned functions in the NTK regime via solving the constrained optimization problem corresponding to the min-norm function in NTK feature space with the constraint of fitting the training data.</p><p>However, there still remains many challenges for analyzing the solution to the min-norm solution in NTK space. For example, provable extrapolation (exact or asymptotic) is often not achieved with most training data distribution. Understanding the desirable condition requires significant insights into the geometry properties of training data distribution, and how they interact with the solution learned by neural networks. Our insights and refined analysis shows in R d space, we need to consider the directions of training data. In graphs, we need to consider, in addition, the graph structure of training data. We refer readers to detailed proofs for the intuition of data conditions. Moreover, since NTK corresponds to infinitely wide neural networks, the feature space is of infinite dimension. The analysis of infinite dimensional spaces poses non-trivial technical challenges too.</p><p>Since different theorems have their respective challenges and insights/techniques, we refer the interested readers to the respective proofs for details. In Lemma 4 (proof in Appendix B. Recall from Section A that in the NTK regime, i.e., networks are infinitely wide, randomly initialized, and trained by gradient descent with infinitesimally small learning rate, the learning dynamics of the neural network is equivalent to that of a kernel regression with respect to its neural tangent kernel.</p><p>For any x ∈ R d , the network output is given by</p><formula xml:id="formula_24">f (x) = φ(x), φ(x 1 ) , ..., φ(x), φ(x n ) • NTK −1 train Y ,</formula><p>where NTK train is the n × n kernel for training data, φ(x), φ(x i ) is the kernel value between test data x and training data x i , and Y is training labels. By Lemma 10, the kernel regression solution is also equivalent to the min-norm solution in the NTK RKHS space that fits all training data</p><formula xml:id="formula_25">f (x) = φ(x) β NTK ,<label>(10)</label></formula><p>where the representation coefficient β NTK is</p><formula xml:id="formula_26">min β β 2 s.t. φ(x i ) β = y i , for i = 1, ..., n.</formula><p>The feature map φ(x) for a two-layer MLP with ReLU activation is given by Lemma 11</p><formula xml:id="formula_27">φ (x) = c x • I w (k) x ≥ 0 , w (k) x • I w (k) x ≥ 0 , ... ,<label>(11)</label></formula><p>where w (k) ∼ N (0, I), with k going to infinity. c is a constant, and I is the indicator function. Without loss of generality, we assume the bias term to be 1. For simplicity of notations, we denote each data x plus bias term by, i.e., x = [x|1] <ref type="bibr" target="#b12">[Bietti and Mairal, 2019]</ref>, and assume constant term is 1.</p><p>Given any direction v on the unit sphere, the network outputs for out-of-distribution data x 0 = tv and x = x 0 + hv = (1 + λ)x 0 , where we introduce the notation of x and λ for convenience, are given by equation 10 and equation 11</p><formula xml:id="formula_28">f ( x0 ) =β NTK x0 • I w (k) x0 ≥ 0 , w (k) x0 • I w (k) x0 ≥ 0 , ... , f (x) =β NTK x • I w (k) x ≥ 0 , w (k) x • I w (k) x ≥ 0 , ... , where we have x0 = [x 0 |1] and x = [(1 + λ)x 0 |1]. It follows that f (x) − f ( x0 ) = β NTK x • I w (k) x ≥ 0 − x0 • I w (k) x0 ≥ 0 ,<label>(12)</label></formula><formula xml:id="formula_29">w (k) x • I w (k) x ≥ 0 − w (k) x0 • I w (k) x0 ≥ 0 , ... (<label>13</label></formula><formula xml:id="formula_30">)</formula><p>By re-arranging the terms, we get the following equivalent form of the entries:</p><formula xml:id="formula_31">x • I w x ≥ 0 − x0 • I w x0 ≥ 0 (14) = x • I w x ≥ 0 − I w x0 ≥ 0 + I w x0 ≥ 0 − x0 • I w x0 ≥ 0 (15) = x • I w x ≥ 0 − I w x0 ≥ 0 + (x − x0 ) • I w x0 ≥ 0 (16) = [x|1] • I w x ≥ 0 − I w x0 ≥ 0 + [hv|0] • I w x0 ≥ 0<label>(17)</label></formula><p>Similarly, we have</p><formula xml:id="formula_32">w x • I w x ≥ 0 − w x0 • I w x0 ≥ 0 (18) = w x • I w x ≥ 0 − I w x0 ≥ 0 + I w x0 ≥ 0 − w x0 • I w x0 ≥ 0 (19) = w x • I w x ≥ 0 − I w x0 ≥ 0 + w (x − x0 ) • I w x0 ≥ 0 (20) = w [x|1] • I w x ≥ 0 − I w x0 ≥ 0 + w [hv|0] • I w x0 ≥ 0<label>(21)</label></formula><p>Again, let us denote the part of β NTK corresponding to each w by β w . Moreover, let us denote the part corresponding to equation 17 by β 1 w and the part corresponding to equation 21 by β 2 w . Then we have</p><formula xml:id="formula_33">f (x) − f ( x0 ) h (22) = β 1 w [x/h|1/h] • I w x ≥ 0 − I w x0 ≥ 0 dP(w)<label>(23)</label></formula><formula xml:id="formula_34">+ β 1 w [v|0] • I w x0 ≥ 0 dP(w)<label>(24)</label></formula><formula xml:id="formula_35">+ β 2 w • w [x/h|1/h] • I w x ≥ 0 − I w x0 ≥ 0 dP(w) (25) + β 2 w • w [v|0] • I w x0 ≥ 0 dP(w)<label>(26)</label></formula><p>Note that all β w are finite constants that depend on the training data. Next, we show that as t → ∞, each of the terms above converges in O(1/ ) to some constant coefficient β v that depend on the training data and the direction v. Let us first consider equation 24. We have</p><formula xml:id="formula_36">I w x0 ≥ 0 dP(w) = I w [x 0 |1] ≥ 0 dP(w) (27) = I w [x 0 /d|1/d] ≥ 0 dP(w)<label>(28)</label></formula><formula xml:id="formula_37">− → I w [v|0] ≥ 0 dP(w) as d → ∞<label>(29)</label></formula><p>Because β 1 w are finite constants, it follows that</p><formula xml:id="formula_38">β 1 w [v|0] • I w x0 ≥ 0 dP(w) → β 1 w [v|0] • I w [v|0] ≥ 0 dP(w),<label>(30)</label></formula><p>where the right hand side is a constant that depends on training data and direction v. Next, we show the convergence rate for equation 30. Given error &gt; 0, because β 1 w [v|0] are finite constants, we need to bound the following by C • for some constant C,</p><formula xml:id="formula_39">| I w x0 ≥ 0 − I w [v|0] ≥ 0 dP(w)| (31) = | I w [x 0 |1] ≥ 0 − I w [x 0 |0] ≥ 0 dP(w)|<label>(32)</label></formula><p>Observe that the two terms in equation 32 represent the volume of half-(balls) that are orthogonal to vectors</p><formula xml:id="formula_40">[x 0 |1] and [x 0 |0].</formula><p>Hence, equation 32 is the volume of the non-overlapping part of the two (half)balls, which is created by rotating an angle θ along the last coordinate. By symmetry, equation 32 is linear in θ. Moreover, the angle θ = arctan(C/t) for some constant C. Hence, it follows that</p><formula xml:id="formula_41">| I w [x 0 |1] ≥ 0 − I w [x 0 |0] ≥ 0 dP(w)| = C 1 • arctan(C 2 /t) (33) ≤ C 1 • C 2 /t (34) = O(1/t)<label>(35)</label></formula><p>In the last inequality, we used the fact that arctan x &lt; x for x &gt; 0. Hence, O(1/t) &lt; implies t = O(1/ ) as desired. Next, we consider equation 23.</p><formula xml:id="formula_42">β 1 w [x/h|1/h] • I w x ≥ 0 − I w x0 ≥ 0 dP(w)<label>(36)</label></formula><p>Let us first analyze the convergence of the following:</p><formula xml:id="formula_43">| I w x ≥ 0 − I w x0 ≥ 0 dP(w)| (37) = | I w [(1 + λ)x 0 |1] ≥ 0 − I w [x 0 |1] ≥ 0 dP(w)dP(w)| (38) = | I w [x 0 | 1 1 + λ ] ≥ 0 − I w [x 0 |1] ≥ 0 dP(w)dP(w)| → 0<label>(39)</label></formula><p>The convergence to 0 follows from equation 33. Now we consider the convergence rate. The angle θ is at most 1 − 1 1+λ times of that in equation 33. Hence, the rate is as follows</p><formula xml:id="formula_44">1 − 1 1 + λ • O 1 t = λ 1 + λ • O 1 t = h/t 1 + h/t • O 1 t = O h (h + t)t<label>(40)</label></formula><p>Now we get back to equation 23, which simplifies as the following.</p><formula xml:id="formula_45">β 1 w v + tv h | 1 h • I w x ≥ 0 − I w x0 ≥ 0 dP(w)<label>(41)</label></formula><p>We compare the rate of growth of left hand side and the rate of decrease of right hand side (indicators). Recall from Section A that in the NTK regime, where networks are infinitely wide, randomly initialized, and trained by gradient descent with infinitesimally small learning rate, the learning dynamics of a neural network is equivalent to that of a kernel regression with respect to its neural tangent kernel. Moreover, Lemma 10 tells us that this kernel regression solution can be expressed in the functional form in the neural tangent kernel's feature space. That is, the function learned by the neural network (in the ntk regime) can be precisely characterized as</p><formula xml:id="formula_46">t h • h (h + t)t = 1 h + t → 0 as t → ∞ (42) 1 h • h (h + t)t = 1 (h + t)t → 0 as t → ∞<label>(</label></formula><formula xml:id="formula_47">f (x) = φ(x) β NTK ,</formula><p>where the representation coefficient β NTK is</p><formula xml:id="formula_48">min β β 2 (44) s.t. φ(x i ) β = y i , for i = 1, ..., n.<label>(45)</label></formula><p>An infinite-dimensional feature map φ(x) for a two-layer ReLU network is described in Lemma 11</p><formula xml:id="formula_49">φ (x) = c x • I w (k) x ≥ 0 , w (k) x • I w (k) x ≥ 0 , ... ,</formula><p>where w (k) ∼ N (0, I), with k going to infinity. c is a constant, and I is the indicator function. That is, there are infinitely many directions w with Gaussian density, and each direction comes with two features. Without loss of generality, we can assume the scaling constant to be 1. Constrained optimization in NTK feature space. The representation or weight of the neural network's learned function in the neural tangent kernel feature space, β NTK , consists of weight vectors for each x • I w (k) x ≥ 0 ∈ R d and w (k) x • I w (k) x ≥ 0 ∈ R. For simplicity of notation, we will use w to refer to a particular w, without considering the index (k), which does not matter for our purposes. For any w ∈ R d , we denote by βw = ( β(1) w , ..., β(d) w ) ∈ R d the weight vectors corresponding to x • I w x ≥ 0 , and denote by β w ∈ R d the weight for w x • I w x ≥ 0 . Observe that for any w ∼ N (0, I) ∈ R d , any other vectors in the same direction will activate the same set of</p><formula xml:id="formula_50">x i ∈ R d . That is, if w x i ≥ 0 for any w ∈ R d , then (k • w) x i ≥ 0 for any k &gt; 0.</formula><p>Hence, we can reload our notation to combine the effect of weights for w's in the same direction. This enables simpler notations and allows us to change the distribution of w in NTK features from Gaussian distribution to uniform distribution on the unit sphere.</p><p>More precisely, we reload our notation by using β w and β w to denote the combined effect of all weights ( β(1) kw , ..., β(d) kw ) ∈ R d and β kw ∈ R for all kw with k &gt; 0 in the same direction of w. That is, for each w ∼ Uni(unit sphere) ∈ R d , we define β (j) w as the total effect of weights in the same direction</p><formula xml:id="formula_51">β (j) w = β(j) u I w u w • u = 1 dP(u), for j = [d]<label>(46)</label></formula><p>where u ∼ N (0, I). Note that to ensure the β w is a well-defined number, here we can work with the polar representation and integrate with respect to an angle. Then β w is well-defined. But for simplicity of exposition, we use the plain notation of integral. Similarly, we define β w as reloading the notation of</p><formula xml:id="formula_52">β w = βu I w u w • u = 1 • u w dP(u)<label>(47)</label></formula><p>Here, in equation 47 we have an extra term of u w compared to equation 46 because the NTK features that equation 47 corresponds to, w x • I w x ≥ 0 , has an extra w term. So we need to take into account the scaling. This abstraction enables us to make claims on the high-level parameters β w and β w only, which we will show to be sufficient to determine the learned function.</p><p>Then we can formulate the constrained optimization problem whose solution gives a functional form of the neural network's learned function. We rewrite the min-norm solution in equation 44 as</p><formula xml:id="formula_53">min β β (1) w 2 + β (2) w 2 + ... + β (d) w 2 + β w 2 dP(w)<label>(48)</label></formula><p>s.t.</p><formula xml:id="formula_54">w x i ≥0 β w x i + β w • w x i dP(w) = β g x i ∀i ∈ [n],<label>(49)</label></formula><p>where the density of w is now uniform on the unit sphere of R d . Observe that since w is from a uniform distribution, the probability density function P(w) is a constant. This means every x i is activated by half of the w on the unit sphere, which implies we can now write the right hand side of equation 49 in the form of left hand side, i.e., integral form. This allows us to further simplify equation 49 as</p><formula xml:id="formula_55">w x i ≥0 β w + β w • w − 2 • β g x i dP(w) = 0 ∀i ∈ [n],<label>(50)</label></formula><p>where equation 50 follows from the following steps of simplification</p><formula xml:id="formula_56">w x i ≥0 β (1) w x (1) i + ..β (d) w x (d) i + β w • w x i dP(w) = β (1) g x (1) i + ...β (d) g x (d) i ∀i ∈ [n], ⇐⇒ w x i ≥0 β (1) w x (1) i + ... + β (d) w x (d) i + β w • w x i dP(w) = 1 w x i ≥0 dP(w) • w x i ≥0 dP(w) • β (1) g x (1) i + ... + β (d) g x (d) i ∀i ∈ [n], ⇐⇒ w x i ≥0 β (1) w x (1) i + ... + β (d) w x (d) i + β w • w x i dP(w) = 2 • w x i ≥0 β (1) g x (1) i + ... + β (d) g x (d) i dP(w) ∀i ∈ [n], ⇐⇒ w x i ≥0 β w + β w • w − 2 • β g x i dP(w) = 0 ∀i ∈ [n].</formula><p>Claim 12. Without loss of generality, assume the scaling factor c in NTK feature map φ(x) is 1. Then the global optimum to the constraint optimization problem equation 48 subject to equation 50, i.e.,</p><formula xml:id="formula_57">min β β (1) w 2 + β (2) w 2 + ... + β (d) w 2 + β w 2 dP(w)<label>(51)</label></formula><p>s.t.</p><formula xml:id="formula_58">w x i ≥0 β w + β w • w − 2 • β g x i dP(w) = 0 ∀i ∈ [n].<label>(52)</label></formula><p>satisfies β w + β w • w = 2β g for all w.</p><p>This claim implies the exact extrapolation we want to prove, i.e., f NTK (x) = g(x). This is because, if our claim holds, then for any</p><formula xml:id="formula_59">x ∈ R d f NTK (x) = w x≥0 β w x + β w • w x dP(w) = w x≥0 2 • β g x dP(w) = w x≥0 dP(w) • 2β g x = 1 2 • 2β g x = g(x)</formula><p>Thus, it remains to prove Claim 12. To compute the optimum to the constrained optimization problem equation 51, we consider the Lagrange multipliers. It is clear that the objective equation 51 is convex. Moreover, the constraint equation 52 is affine. Hence, by KKT, solution that satisfies the Lagrange condition will be the global optimum. We compute the Lagrange multiplier as</p><formula xml:id="formula_60">L(β, λ) = β (1) w 2 + β (2) w 2 + ... + β (d) w 2 + β w 2 dP(w) (53) − n i=1 λ i •    w x i ≥0 β w + β w • w − 2 • β g x i dP(w)   <label>(54)</label></formula><p>Setting the partial derivative of L(β, λ) with respect to each variable to zero gives</p><formula xml:id="formula_61">∂L ∂β (k) w = 2β (k) w P(w) + n i=1 λ i • x (k) i • I w x i ≥ 0 = 0 (55) ∂L β w = 2β w P(w) + n i=1 λ i • w x i • I w x i ≥ 0 = 0 (56) ∂L ∂λ i = w x i ≥0 β w + β w • w − 2 • β g x i dP(w) = 0 (57)</formula><p>It is clear that the solution in Claim 12 immediately satisfies equation 57. Hence, it remains to show there exist a set of λ i for i ∈ [n] that satisfies equation 55 and equation 56. We can simplify equation 55 as</p><formula xml:id="formula_62">β (k) w = c • n i=1 λ i • x (k) i • I w x i ≥ 0 , (<label>58</label></formula><formula xml:id="formula_63">)</formula><p>where c is a constant. Similarly, we can simplify equation 56 as</p><formula xml:id="formula_64">β w = c • n i=1 λ i • w x i • I w x i ≥ 0 (59)</formula><p>Observe that combining equation 58 and equation 59 implies that the constraint equation 59 can be further simplified as</p><formula xml:id="formula_65">β w = w β w<label>(60)</label></formula><p>It remains to show that given the condition on training data, there exists a set of λ i so that equation 58 and equation 60 are satisfied.</p><p>Global optimum via the geometry of training data. Recall that we assume our training data {(x i , y i )} n i=1 satisfies for any w ∈ R d , there exist d linearly independent {x w i } d i=1 ⊂ X, where X = {x i } n i=1 , so that w x w i ≥ 0 and −x w i ∈ X for i = 1..d, e.g., an orthogonal basis of R d and their opposite vectors. We will show that under this data regime, we have (a) for any particular w, there indeed exist a set of λ i that can satisfy the constraints equation 58 and equation 60 for this particular w.</p><p>(b) For any w 1 and w 2 that activate the exact same set of {x i }, the same set of λ i can satisfy the constraints equation 58 and equation 60 of both w 1 and w 2 .</p><p>(c) Whenever we rotate a w 1 to a w 2 so that the set of x i being activated changed, we can still find λ i that satisfy constraint of both w 1 and w 2 .</p><p>Combining (a), (b) and (c) implies there exists a set of λ that satisfy the constraints for all w. Hence, it remains to show these three claims.</p><p>We first prove Claim (a). For each w, we must find a set of λ i so that the following hold.</p><formula xml:id="formula_66">β (k) w = c • n i=1 λ i • x (k) i • I w x i ≥ 0 , β w = w β w β w + β w • w = 2β g</formula><p>Here, β g and w are fixed, and w is a vector on the unit sphere. It is easy to see that β w is then determined by β g and w, and there indeed exists a solution (solving a consistent linear system). Hence we are left with a linear system with d linear equations</p><formula xml:id="formula_67">β (k) w = c • n i=1 λ i • x (k) i • I w x i ≥ 0 ∀k ∈ [d]</formula><p>to solve with free variables being λ i so that w activates x i , i.e., w x i ≥ 0. Because the training data {(x i , y i )} n i=1 satisfies for any w, there exist at least d linearly independent x i that activate w. This guarantees for any w we must have at least d free variables. It follows that there must exist solutions λ i to the linear system. This proves Claim (a).</p><p>Next, we show that (b) for any w 1 and w 2 that activate the exact same set of {x i }, the same set of λ i can satisfy the constraints equation 58 and equation 60 of both w 1 and w 2 . Because w 1 and w 2 are activated by the same set of x i , this implies</p><formula xml:id="formula_68">β w 1 = c • n i=1 λ i • x i • I w 1 x i ≥ 0 = c • n i=1 λ i • x i • I w 2 x i ≥ 0 = β w 2</formula><p>Since λ i already satisfy constraint equation 58 for w 1 , they also satisfy that for w 2 . Thus, it remains to show that β w</p><formula xml:id="formula_69">1 + β w 1 • w 1 = β w 2 + β w 2 • w 1 assuming β w 1 = β w 2 , β w 1 = w 1 β w 1 ,</formula><p>and β w 2 = w 2 β w 2 . This indeed holds because</p><formula xml:id="formula_70">β w 1 + β w 1 • w 1 = β w 2 + β w 2 • w 2 ⇐⇒ β w 1 • w 1 = β w 2 • w 2 ⇐⇒ w 1 β w 1 w 1 = w 2 β w 2 w 2 ⇐⇒ w 1 w 1 β w 1 = w 2 w 2 β w 2 ⇐⇒ 1 • β w 1 = 1 • β w 2 ⇐⇒ β w 1 = β w 1</formula><p>Here, we used the fact that w 1 and w 2 are vectors on the unit sphere. This proves Claim (b).</p><p>Finally, we show (c) that Whenever we rotate a w 1 to a w 2 so that the set of x i being activated changed, we can still find λ i that satisfy constraint of both w 1 and w 2 . Suppose we rotate w 1 to w 2 so that w 2 lost activation with x 1 , x 2 , ..., x p which in the set of linearly independent x i 's being activated by w 1 and their opposite vectors −x i are also in the training set (without loss of generality). Then w 2 must now also get activated by −x 1 , −x 2 , ..., −x p . This is because if w 2 x i &lt; 0, we must have w 2 (−x i ) &gt; 0.</p><p>Recall that in the proof of Claim (a), we only needed the λ i from linearly independent x i that we used to solve the linear systems, and their opposite as the free variables to solve the linear system of d equations. Hence, we can set λ to 0 for the other x i while still satisfying the linear system. Then, suppose there exists λ i that satisfy</p><formula xml:id="formula_71">β (k) w 1 = c • d i=1 λ i • x (k) i</formula><p>where the x i are the linearly independent vectors that activate w 1 with opposite vectors in the training set, which we have proved in (a). Then we can satisfy the constraint for β w 2 below</p><formula xml:id="formula_72">β (k) w 2 = c • p i=1 λi • (−x i ) (k) + d i=p+1 λ i • x (k) i</formula><p>by setting λi = −λ i for i = 1...p. Indeed, this gives</p><formula xml:id="formula_73">β (k) w 2 = c • p i=1 (−λ i ) • (−x i ) (k) + d i=p+1 λ i • x (k) i = c • d i=1 λ i • x (k) i</formula><p>Thus, we can also find λ i that satisfy the constraint for β w 2 . Here, we do not consider the case where w 2 is parallel with an x i because such w 2 has measure zero. Note that we can apply this argument iteratively because the flipping the sign always works and will not create any inconsistency.</p><p>Moreover, we can show that the constraint for β w2 is satisfied by a similar argument as in proof of Claim (b). This follows from the fact that our construction makes β w 1 = β w 2 . Then we can follow the same argument as in (b) to show that β w 1 + β w 1 • w 1 = β w 2 + β w 2 • w 1 . This completes the proof of Claim (c).</p><p>In summary, combining Claim (a), (b) and (c) gives that Claim 12 holds. That is, given our training data, the global optimum to the constrained optimization problem of finding the min-norm solution among functions that fit the training data satisfies β w + β w • w = 2β g . We also showed that this claim implies exact extrapolation, i.e., the network's learned function f (x) is equal to the true underlying function g(x) for all x ∈ R d . This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Proof of Theorem 5</head><p>Proof of the asymptotic convergence to extrapolation builds upon our proof of exact extrapolation, i.e., Lemma 4. The proof idea is that if the training data distribution has support at all directions, when the number of samples n → ∞, asymptotically the training set will converge to some imaginary training set that satisfies the condition for exact extrapolation. Since if training data are close the neural tangent kernels are also close, the predictions or learned function will converge to a function that achieves perfect extrapolation, that is, the true underlying function.</p><p>Asymptotic convergence of data sets. We first show the training data converge to a data set that satisfies the exact extrapolation condition in Lemma 4. Suppose training data {x i } n i=1 are sampled from a distribution whose support contains a connected set S that intersects all directions, i.e., for any non-zero w ∈ R d , there exists k &gt; 0 so that kw ∈ S.</p><p>Let us denote by S the set of datasets that satisfy the exact condition in Lemma 4. Given a general dataset X and a dataset S ∈ S of the same size n, let σ(X, S) denote a matching of their data points, i.e., σ outputs a sequence of pairs</p><formula xml:id="formula_74">σ(X, S) i = (x i , s i ) for i ∈ [n] s.t. X = {x i } n i=1 S = {s i } n i=1</formula><p>Let : R d × R d → R be the l2 distance that takes in a pair of points. We then define the distance between the datasets d(X, S) as the minimum sum of l2 distances of their data points over all possible matching.</p><formula xml:id="formula_75">d(X, S) =    min σ n i=1 (σ (X, S) i ) |X| = |S| = n ∞ |X| = |S|</formula><p>We can then define a "closest distance to perfect dataset" function D * : X → R which maps a dataset X to the minimum distance of X to any dataset in</p><formula xml:id="formula_76">S D * (X) = min S∈S d (X, S)</formula><p>It is easy to see that for any dataset X = {x i } n i=1 , D * (X) can be bounded by the minimum of the closest distance to perfect dataset D * of sub-datasets of X of size 2d.</p><formula xml:id="formula_77">D * ({x i } n i=1 ) ≤ n/2d min k=1 D * {x j } k * 2d j=(k−1) * 2d+1<label>(61)</label></formula><p>This is because for any S ∈ S, and any S ⊆ S , we must have S ∈ S because a dataset satisfies exact extrapolation condition as long as it contains some key points. Thus, adding more data will not hurt, i.e., for any X 1 ⊆ X 2 , we always have</p><formula xml:id="formula_78">D * (X 1 ) ≤ D * (X 2 )</formula><p>Now let us denote by X n a random dataset of size n where each x i ∈ X n is sampled from the training distribution. Recall that our training data {x i } n i=1 are sampled from a distribution whose support contains a connected set S * that intersects all directions, i.e., for any non-zero w ∈ R d , there exists k &gt; 0 so that kw ∈ S * . It follows that for a random dataset X 2d of size 2d, the probability that D * (X 2d ) &gt; happens is less than 1 for any &gt; 0.</p><p>First there must exist S 0 = {s i } 2d i=1 ∈ S of size 2d, e.g., orthogonal basis and their opposite vectors. Observe that if we scale any s i by k &gt; 0, the resulting dataset is still in S by the definition of S. We denote the set of datasets where we are allowed to scale elements of S 0 by S 0 . It follows that</p><formula xml:id="formula_79">P (D * (X 2d ) &gt; ) = P min S∈S d (X 2d , S) &gt; ≤ P min S∈S 0 d (X 2d , S) &gt; = P min S∈S 0 min σ n i=1 (σ (X 2d , S) i ) &gt; = 1 − P min S∈S 0 min σ n i=1 (σ (X 2d , S) i ) ≤ ≤ 1 − P min S∈S 0 min σ n max i=1 (σ (X 2d , S) i ) ≤ ≤ δ &lt; 1</formula><p>where we denote the bound of P (D * (X 2d ) &gt; ) by δ &lt; 1, and the last step follows from</p><formula xml:id="formula_80">P min S∈S 0 min σ n max i=1 (σ (X 2d , S) i ) ≤ &gt; 0</formula><p>which further follows from the fact that for any s i ∈ S 0 , by the assumption on training distribution, we can always find k &gt; 0 so that ks i ∈ S * , a connected set in the support of training distribution. By the connectivity of support S * , ks i cannot be an isolated point in S * , so for any &gt; 0, we must have</p><formula xml:id="formula_81">x−ks i ≤ ,x∈S * f X (x)dx &gt; 0</formula><p>Hence, we can now apply equation 61 to bound D * (X n ). Given any &gt; 0, we have</p><formula xml:id="formula_82">P (D * (X n ) &gt; ) = 1 − P (D * (X n ) ≤ ) ≤ 1 − P n/2d min k=1 D * {x j } k * 2d j=(k−1) * 2d+1 ≤ ≤ 1 −   1 − n/2d k=1 P D * {x j } k * 2d j=(k−1) * 2d+1 &gt;   = n/2d k=1 P D * {x j } k * 2d j=(k−1) * 2d+1 &gt; ≤ δ n/2d</formula><p>Here δ &lt; 1. This implies D * (X n ) p −→ 0, i.e.,</p><formula xml:id="formula_83">lim n→∞ P (D * (X n ) &gt; ) = 0 ∀ &gt; 0 (62)</formula><p>equation 62 says as the number of training samples n → ∞, our training set will converge in probability to a dataset that satisfies the requirement for exact extrapolation. Asymptotic convergence of predictions. Let NTK(x, x ) : R d × R d → R denote the neural tangent kernel for a two-layer ReLU MLP. It is easy to see that if x → x * , then NTK(x, •) → NTK(x * , •) <ref type="bibr" target="#b5">(Arora et al. [2019b]</ref>). Let NTK train denote the n × n kernel matrix for training data.</p><p>We have shown that our training set converges to a perfect data set that satisfies conditions of exact extrapolation. Moreover, note that our training set will only have a finite number of (not increase with n) x i that are not precisely the same as those in a perfect dataset. This is because a perfect data only contains a finite number of key points and the other points can be replaced by any other points while still being a perfect data set. Thus, we have NTK train → N * , where N * is the n × n NTK matrix for some perfect data set.</p><p>Because neural tangent kernel is positive definite, we have NTK −1 train → N * −1 . Recall that for any x ∈ R d , the prediction of NTK is </p><formula xml:id="formula_84">f NTK (x) = (NTK(x, x 1 ), ..., NTK(x, x n )) • NTK −1 train Y ,</formula><formula xml:id="formula_85">→ N * −1 gives f NTK p −→ f * NTK = g,</formula><p>where f NTK is the function learned using our training set, and f * NTK is that learned using a perfect data set, which is equal to the true underlying function g. This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Proof of Corollary 8</head><p>In order for GNN with linear aggregations</p><formula xml:id="formula_86">h (k) u = v∈N (u) MLP (k) h (k) u , h (k) v , x (u,v) , h G = MLP (K+1) u∈G h (K) u ,</formula><p>to extrapolate in the maximum degree task, it must be able to simulate the underlying function</p><formula xml:id="formula_87">h G = max u∈G v∈N (u)<label>1</label></formula><p>Because the max function cannot be decomposed as the composition of piece-wise linear functions, the MLP (K+1) module in GNN must learn a function that is not piece-wise linear over domains outside the training data range. Since Theorem 3 proves for two-layer overparameterized MLPs, here we also assume MLP (K+1) is a two-layer overparameterized MLP, although the result can be extended to more layers. It then follows from Theorem 3 that for any input and label (and thus gradient), MLP (K+1) will converge to linear functions along directions from the origin. Hence, there are always domains where the GNN cannot learn a correct target function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Proof of Theorem 9</head><p>Our proof applies the similar proof techniques for Lemma 4 and 5 to Graph Neural Networks (GNNs). This is essentially an analysis of Graph Neural Tangent Kernel (GNTK), i.e., neural tangent kernel of GNNs. We first define the simple GNN architecture we will be analyzing, and then present the GNTK for this architecture. Suppose G = (V, E) is an input graph without edge feature, and x u ∈ R d is the node feature of any node u ∈ V . Let us consider the simple one-layer GNN whose input is G and output is h</p><formula xml:id="formula_88">G h G = W (2) max u∈G v∈N (u) W (1) x v (63)</formula><p>Note that our analysis can be extended to other variants of GNNs, e.g., with non-empty edge features, ReLU activation, different neighbor aggregation and graph-level pooling architectures. We analyze this GNN for simplicity of exposition.</p><p>Next, let us calculate the feature map of the neural tangent kernel for this GNN. Recall from Section A that consider a graph neural network f (θ, G) : G → R where θ ∈ R m is the parameters in the network and G ∈ G is the input graph. Then the neural tangent kernel is</p><formula xml:id="formula_89">H ij = ∂f (θ, G i ) ∂θ , ∂f (θ, G j ) ∂θ ,</formula><p>where θ are the infinite-dimensional parameters. Hence, the gradients with respect to all parameters give a natural feature map. Let us denote, for any node u, the degree of u by</p><formula xml:id="formula_90">h u = v∈N (u) x v<label>(64)</label></formula><p>It then follows from simple computation of derivative that the following is a feature map of the GNTK for equation 63</p><formula xml:id="formula_91">φ(G) = c • max u∈G w (k) h u , u∈G I u = arg max v∈G w (k) h v • h u , ... ,<label>(65)</label></formula><p>where w (k) ∼ N (0, I), with k going to infinity. c is a constant, and I is the indicator function.</p><p>Next, given training data {(G i , y i } n i=1 , let us analyze the function learned by GNN through the min-norm solution in the GNTK feature space. The same proof technique is also used in Lemma 4 and 5.</p><p>Recall the assumption that all graphs have uniform node feature, i.e., the learning task only considers graph structure, but not node feature. We assume x v = 1 without loss of generality. Observe that in this case, there are two directions, positive or negative, for one-dimensional Gaussian distribution. Hence, we can simplify our analysis by combining the effect of linear coefficients for w in the same direction as in Lemma 4 and 5.</p><p>Similarly, for any w, let us define βw ∈ R as the linear coefficient corresponding to Recall that the underlying reasoning function, maximum degree, is</p><formula xml:id="formula_92">g(G) = max u∈G h u .</formula><p>We formulate the constrained optimization problem, i.e., min-norm solution in GNTK feature space that fits all training data, as</p><formula xml:id="formula_93">min β, β β2 w + β 2</formula><p>w dP(w) s.t.</p><formula xml:id="formula_94">u∈G i I u = arg max v∈G w • h v • βw • h u + max u∈G i (w • h u ) • β w dP(w) = max u∈G i h u ∀i ∈ [n],</formula><p>where G i is the i-th training graph and w ∼ N (0, 1). By combining the effect of β, and taking the derivative of the Lagrange for the constrained optimization problem and setting to zero, we get the global optimum solution satisfy the following constraints.</p><formula xml:id="formula_95">β + = c • n i=1 λ i • u∈G i h u • I u = arg max v∈G i h v<label>(66)</label></formula><formula xml:id="formula_96">β − = c • n i=1 λ i • u∈G i h u • I u = arg min v∈G i h v<label>(67)</label></formula><formula xml:id="formula_97">β + = c • n i=1 λ i • max u∈G i h u<label>(68)</label></formula><formula xml:id="formula_98">β − = c • n i=1 λ i • min u∈G i h u<label>(69)</label></formula><formula xml:id="formula_99">max u∈G i h u = β + • u∈G i I u = arg max v∈G i h v • h u + β + • max u∈G i h u<label>(70)</label></formula><formula xml:id="formula_100">+ β − • u∈G i I u = arg min v∈G i h v • h u + β − • min u∈G i h u ∀i ∈ [n]<label>(71)</label></formula><p>where c is some constant, λ i are the Lagrange parameters. Note that here we used the fact that there are two directions +1 and −1. This enables the simplification of Lagrange derivative. For a similar step-by-step derivation of Lagrange, refer to the proof of Lemma 4. Let us consider the solution β + = 1 and β + = β − = β − = 0. It is clear that this solution can fit the training data, and thus satisfies equation 70. Moreover, this solution is equivalent to the underlying reasoning function, maximum degree, g(G) = max u∈G h u .</p><p>Hence, it remains to show that, given our training data, there exist λ i so that the remaining four constraints are satisfies for this solution. Let us rewrite these constraints as a linear systems where the variables are</p><formula xml:id="formula_101">λ i     β + β − β + β −     = c • n i=1 λ i •           u∈G i h u • I u = arg max v∈G i h v u∈G i h u • I u = arg min v∈G i h v max u∈G i h u min u∈G i h u           (72)</formula><p>By standard theory of linear systems, there exist λ i to solve equation 72 if there are at least four training data G i whose following vectors linear independent</p><formula xml:id="formula_102">          u∈G i h u • I u = arg max v∈G i h v u∈G i h u • I u = arg min v∈G i h v max u∈G i h u min u∈G i h u           =        max u∈G i h u • N max i min u∈G i h u • N min i max u∈G i h u min u∈G i h u        (73)</formula><p>Here, N max i denotes the number of nodes that achieve the maximum degree in the graph G i , and N min i denotes the number of nodes that achieve the min degree in the graph G i . By the assumption of our training data that there are at least four G i ∼ G with linearly independent equation 73. Hence, our simple GNN learns the underlying function as desired.</p><p>This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 Proof of Lemma 10</head><p>Let W denote the span of the feature maps of training data x i , i.e.</p><formula xml:id="formula_103">W = span (φ (x 1 ) , φ (x 2 ) , ..., φ (x n )) .</formula><p>Then we can decompose the coordinates of f NTK in the RKHS space, β NTK , into a vector β 0 for the component of f NTK in the span of training data features W , and a vector β 1 for the component in the orthogonal complement W , i.e.,</p><formula xml:id="formula_104">β NTK = β 0 + β 1 .</formula><p>First, note that since f NTK must be able to fit the training data (NTK is a universal kernel as we will discuss next), i.e., φ(x i ) β NTK = y i .</p><p>Thus, we have φ(x i ) β 0 = y i . Then, β 0 is uniquely determined by the kernel regression solution with respect to the neural tangent kernel</p><formula xml:id="formula_105">f NTK (x) = φ(x), φ(x 1 ) , ..., φ(x), φ(x n ) • NTK −1 train Y ,</formula><p>where NTK train is the n × n kernel for training data, φ(x), φ(x i ) is the kernel between test data x and training data x i , and Y is training labels. The kernel regression solution f NTK is uniquely determined because the neural tangent kernel NTK train is positive definite assuming no two training data are parallel, which can be enforced with a bias term <ref type="bibr" target="#b22">[Du et al., 2019c]</ref>. In any case, the solution is a min-norm by pseudo-inverse.</p><p>Moreover, a unique kernel regression solution f NTK that spans the training data features corresponds to a unique representation in the RKHS space β 0 .</p><p>Since β 0 and β 1 are orthogonal, we also have the following</p><formula xml:id="formula_106">β NTK 2 2 = β 0 + β 1 2 2 = β 0 2 2 + β 1 2 2 .</formula><p>This implies the norm of β NTK is at least as large as the norm of any β such that φ(x i ) β NTK = y i . Moreover, observe that the solution to kernel regression equation 8 is in the feature span of training data, given the kernel matrix for training data is full rank.</p><formula xml:id="formula_107">f NTK (x) = φ(x), φ(x 1 ) , ..., φ(x), φ(x n ) • NTK −1 train Y .</formula><p>Since β 1 is for the component of f NTK in the orthogonal complement of training data feature span, we must have β 1 = 0. It follows that β NTK is equivalent to</p><formula xml:id="formula_108">min β β 2 s.t. φ(x i ) β = y i , for i = 1, ..., n.</formula><p>as desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.7 Proof of Lemma 11</head><p>We first compute the neural tangent kernel NTK(x, x ) for a two-layer multi-layer perceptron (MLP) with ReLU activation function, and then show that it can be induced by the feature space φ(x) specified in the lemma so that NTK(x, x ) = φ(x), φ(x ) .</p><p>Recall that <ref type="bibr" target="#b34">Jacot et al. [2018]</ref> have derived the general framework for computing the neural tangent kernel of a neural network with general architecture and activation function. This framework is also described in <ref type="bibr" target="#b5">Arora et al. [2019b]</ref>, <ref type="bibr">Du et al. [2019b]</ref>, which, in addition, compute the exact kernel formula for convolutional networks and Graph Neural Networks, respectively. Following the framework in <ref type="bibr" target="#b34">Jacot et al. [2018]</ref> and substituting the general activation function σ with ReLU gives the kernel formula for a two-layer MLP with ReLU activation. This has also been described in several previous works <ref type="bibr" target="#b22">[Du et al., 2019c</ref><ref type="bibr" target="#b17">, Chizat et al., 2019</ref><ref type="bibr" target="#b12">, Bietti and Mairal, 2019]</ref>.</p><p>Below we describe the general framework in <ref type="bibr" target="#b34">Jacot et al. [2018]</ref> and <ref type="bibr" target="#b5">Arora et al. [2019b]</ref>. Let σ denote the activation function. The neural tangent kernel for an h-layer multi-layer perceptron can be recursively defined via a dynamic programming process. Here, Σ (i) : R d × R d → R for i = 0...h is the covariance for the i-th layer.</p><formula xml:id="formula_109">Σ (0) (x, x ) = x x , ∧ (i) (x, x ) = Σ (i−1) (x, x) Σ (i−1) (x, x ) Σ (i−1) (x , x) Σ (i−1) (x , x ) , Σ (i) (x, x ) = c • E u,v∼N (0,∧ (i) ) [σ(u)σ(v)] .</formula><p>The derivative covariance is defined similarly:</p><formula xml:id="formula_110">Σ(i) (x, x ) = c • E u,v∼N (0,∧ (i) ) [ σ(u) σ(v)] .</formula><p>Then the neural tangent kernel for an h-layer network is defined as</p><formula xml:id="formula_111">NTK (h−1) (x, x ) = h i=1 Σ (i−1) (x, x ) • h k=i Σ(k) (x, x ) ,</formula><p>where we let Σ(h) (x, x ) = 1 for the convenience of notations. We compute the explict NTK formula for a two-layer MLP with ReLU activation function by following this framework and substituting the general activation function with ReLU, i.e. σ(a) = max(0, a) = a•I(a ≥ 0) and σ(a) = I(a ≥ 0).</p><formula xml:id="formula_112">NTK (1) (x, x ) = 2 i=1 Σ (i−1) (x, x ) • h k=i Σ(k) (x, x ) = Σ (0) (x, x ) • Σ(1) (x, x ) + Σ (1) (x, x )</formula><p>So we can get the NTK via Σ (1) (x, x ) and Σ(1) (x, x ), Σ (0) (x, x ). Precisely,</p><formula xml:id="formula_113">Σ (0) (x, x ) = x x , ∧ (1) (x, x ) = x x x x x x x x = x x • x x , Σ (1) (x, x ) = c • E u,v∼N (0,∧ (1) ) [u • I(u ≥ 0) • v • I(v ≥ 0)] .</formula><p>To sample from N (0, ∧ (1) ), we let L be a decomposition of ∧ (1) , such that ∧ (1) = LL . Here, we can see that L = (x, x ) . Thus, sampling from N (0, ∧ (1) ) is equivalent to first sampling w ∼ N (0, I), and output</p><formula xml:id="formula_114">Lw = w (x, x ).</formula><p>Then we have the equivalent sampling (u, v) = (w x, w x ). It follows that</p><formula xml:id="formula_115">Σ (1) (x, x ) = c • E w∼N (0,I) w x • I w x ≥ 0 • w x • I w x ≥ 0</formula><p>It follows from the same reasoning that</p><formula xml:id="formula_116">Σ(1) (x, x ) = c • E w∼N (0,I) I w x ≥ 0 • I w x ≥ 0 .</formula><p>The neural tangent kernel for a two-layer MLP with ReLU activation is then</p><formula xml:id="formula_117">NTK (1) (x, x ) = Σ (0) (x, x ) • Σ(1) (x, x ) + Σ (1) (x, x ) = c • E w∼N (0,I) x x • I w x ≥ 0 • I w x ≥ 0 + c • E w∼N (0,I) w x • I w x ≥ 0 • w x • I w x ≥ 0 .</formula><p>Next, we use the kernel formula to compute a feature map for a two-layer MLP with ReLU activation function.</p><p>Recall that by definition a valid feature map must satisfy the following condition</p><formula xml:id="formula_118">NTK (1) (x, x ) = φ(x), φ(x )</formula><p>It is easy to see that the way we represent our NTK formula makes it easy to find such a decomposition. The following infinite-dimensional feature map would satisfy the requirement because the inner product of φ(x) and φ(x ) for any x, x would be equivalent to the expected value in NTK, after we integrate with respect to the density function of w.</p><formula xml:id="formula_119">φ (x) = c x • I w (k) x ≥ 0 , w (k) x • I w (k) x ≥ 0 , ... ,</formula><p>where w (k) ∼ N (0, I), with k going to infinity. c is a constant, and I is the indicator function. Note that here the density of features of φ(x) is determined by the density of w, i.e. Gaussian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experimental Details</head><p>In this section, we describe the model, data and training details for reproducing our experiments. Our experiments support all of our theoretical claims and insights.</p><p>Overview. We classify our experiments into the following major categories, each of which includes several ablation studies:</p><p>1) Learning tasks where the target functions are simple non-linear functions in various dimensions and training/test distributions: quadratic, cosine, square root, and l1 norm functions, with MLPs with a wide range of hyper-parameters.</p><p>This validates our implications on MLPs generally cannot extrapolate in tasks with non-linear target functions, unless the non-linear function is directionally linear out-of-distribution. In the latter case, the extrapolation error is more sensitive to the hyper-parameters.</p><p>2) Computation of the R-Squared of MLP's learned functions along (thousands of) randomly sampled directions in out-of-distribution domain.</p><p>This validates Theorem 3 and shows the convergence rate is very high in practice, and often happens immediately out of training range.</p><p>3) Learning tasks where the target functions are linear functions with MLPs. Test graphs are all sampled from the "general graphs" family with a diverse range of structure.</p><p>Regarding the type of training graph structure, we consider two schemes. Both schemes show a U-shape curve of extrapolation error with respect to the sparsity of training graphs.</p><p>a) Specific graph structure: path, cycle, tree, expander, ladder, complete graphs, general graphs, 4-regular graphs.</p><p>b) Random graphs with a range of probability p of an edge between any two nodes. Smaller p samples sparse graphs and large p samples dense graphs.</p><p>7) Dynamic programming: Physical reasoning of the n-Body problem in the orbit setting with Graph Neural Networks. We show that GNNs on the original features from previous works fail to extrapolate to unseen masses and distances. On the other hand, we show extrapolation can be achieved via an improved representation of the input edge features. We consider the following extrapolation regimes.</p><p>a) Extrapolation on the masses of the objects.</p><p>b) Extrapolation on the distances between objects.</p><p>We consider the following two input representation schemes to compare the effects of how representation helps extrapolation.</p><p>a) Original features. Following previous works on solving n-body problem with GNNs, the edge features are simply set to 0.</p><p>b) Improved features. We show although our edge features do not bring in new information, it helps extrapolation.</p><p>C.1 Learning Simple Non-Linear Functions Dataset details. We consider four tasks where the underlying functions are simple non-linear functions g : R d → R. Given an input x ∈ R d , the label is computed by y = g(x) for all x. We consider the following four families of simple functions g. ii) Training and validation data are uniformly sampled from a sphere, where every point has L2 distance r from the origin. We sample r from r ∈ {0.5, 1.0}. Then, we sample a random Gaussian vector q in R d . We obtain the training or validation data x = q/ q 2 • r. This corresponds to uniform sampling from the sphere.</p><p>Test data are sampled (non-uniformly) from a hyper-ball. We first sample r uniformly from [0.0, 2.0], [0.0, 5.0], and [0.0, 10.0]. Then, we sample a random Gaussian vector q in R d . We obtain the test data x = q/ q 2 •r. Model and hyperparameter settings. We consider the multi-layer perceptron (MLP) architecture.</p><formula xml:id="formula_120">MLP(x) = W (d) • σ W (d−1) σ ...σ W (1) x</formula><p>We search the following hyper-parameters for MLPs i) The default initialization in PyTorch.</p><p>ii) The initialization scheme in neural tangent kernel theory, i.e., we sample entries of W k from N (0, 1) and scale the output after each W (k) by We perform kernel regression on these training sets using a two-layer neural tangent kernel (NTK). Our code for exact computation of NTK is adapted from <ref type="bibr" target="#b6">Arora et al. [2020]</ref>, <ref type="bibr" target="#b45">Novak et al. [2020]</ref>.</p><p>We verify that the test losses are all precisely 0, up to machine precision. This empirically confirms Lemma 4.</p><p>Although we also trained MLPs in the regular regimes on the same datasets, the test losses are not 0. This reveals some difference between neural tangent kernel and finite neural networks. We conjecture the difference is partially explained by the more linear nature of NTK. In the large data regime in Theorem 5, both finite MLPs and NTK can achieve low test loss, i.e., asymptotic extrapolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 MLPs with sin, quadratic, and tanh Activation</head><p>This section describes the experimental settings for results in Appendix D.3. sin activation. Formally, the activation functino is σ(x) = sin(x). We consider two tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Max Degree</head><p>Dataset details. We consider the task of finding the maximum degree on a graph. Given any input graph G = (V, E), the label is computed by the underlying function y = g(G) = max u∈G v∈N (u) 1. For each dataset, we sample the graphs and node features with the following parameters a) Graph structure for training and validation sets. For each dataset, we consider one of the following graph structure: path graphs, cycles, ladder graphs, 4-regular random graphs, complete graphs, random trees, expanders (random graphs with p = 0.8), and general graphs (random graphs with p = 0.1 to 0.9 with equal probability). We use the networkx library for sampling graphs.</p><p>b) Graph structure for test set. We consider the general graphs (random graphs with p = 0.1 to 0.9 with equal probability). ii) Spurious (continuous) features. Node features in training and validation sets are sampled uniformly from [−5.0, 5.0] 3 , i.e., a three-dimensional vector where each dimension is sampled from [−5.0, 5.0]. There are two schemes for test sets, in the first case we do not extrapolate node features, so we sample node features uniformly from [−5.0, 5.0] 3 . In the second case we extrapolate node features, we sample node features uniformly from [−10.0, 10.0] 3 . e) We sample 5, 000 graphs for training, 1, 000 graphs for validation, and 2, 500 graphs for testing.</p><p>Model and hyperparameter settings. We consider the following Graph Neural Network (GNN) architecture. Given an input graph G, GNN learns the output h G by first iteratively aggregating and transforming the neighbors of all node vectors h (k) u (vector for node u in layer k), and perform a max or sum-pooling over all node features h u to obtain h G . Formally, we have</p><formula xml:id="formula_121">h (k) u = v∈N (u) MLP (k) h (k−1) v , h (k−1) u , h G = MLP (K+1) graph-pooling{h (K) u : u ∈ G} . (75)</formula><p>Here, N (u) denotes the neighbors of u, K is the number of GNN iterations, and graph-pooling is a hyperparameter with choices as max or sum. h d) The number of layers for MLP (k) with k = 1..K are set to 2. The number of layers for MLP (K+1) is set to 1.</p><p>We train the GNNs with the mean squared error (MSE) loss and Adam optimizer with learning rate schedule 0.5 for every 50 epochs. We search the following hyper-parameters for training a) Initial learning rate is set to 0.01. b) Batch size is set to 64. c) Weight decay is set to 1e − 5. d) Number of epochs is set to 300 for graphs with continuous node features, and 100 for graphs with uniform node features.</p><p>Model and hyperparameter settings. We consider the following Graph Neural Network (GNN) architecture. Given an input graph G, GNN learns the output h G by first iteratively aggregating and transforming the neighbors of all node vectors h (k) u (vector for node u in layer k), and perform a max or sum-pooling over all node features h u to obtain h G . Formally, we have</p><formula xml:id="formula_122">h (k) u = min v∈N (u) MLP (k) h (k−1) v , h (k−1) u , w (u,v) , h G = MLP (K+1) min u∈G h u .<label>(76)</label></formula><p>Here, N (u) denotes the neighbors of u, K is the number of GNN iterations, and for neighbor aggregation we run both min and sum. h e) The number of layers for MLP (k) with k = 1..K are set to 2. The number of layers for MLP (K+1) is set to 1.</p><p>We train the GNNs with the mean squared error (MSE) loss and Adam optimizer with learning rate schedule 0.5 for every 50 epochs. We search the following hyper-parameters for training a) Initial learning rate is set to 0.01. We perform the same model selection and validation as in Section C.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.7 N-Body Problem</head><p>Task description. The n-body problem asks a neural network to predict how n stars in a physical system evolves according to physics laws. That is, we train neural networks to predict properties of future states of each star in terms of next frames, e.g., 0.001 seconds.</p><p>Mathematically, in an n-body system S = {X i } n i=1 , such as solar systems, all n stars {X i } n i=1 exert distance and mass-dependent gravitational forces on each other, so there were n(n − 1) relations or forces in the system. Suppose X i at time t is at position x t i and has velocity v t i . The overall forces a star X i receives from other stars is determined by physics laws as the following</p><formula xml:id="formula_123">F t i = G • j =i m i × m j x t i − x t j 3 2 • x t j − x t i ,<label>(77)</label></formula><p>where G is the gravitational constant, and m i is the mass of star X i . Then acceralation a t i is determined by the net force F t i and the mass of star m i</p><formula xml:id="formula_124">a t i = F t i /m i<label>(78)</label></formula><p>Suppose the velocity of star X i at time t is v t i . Then assuming the time steps dt, i.e., difference between time frames, are sufficiently small, the velocity at the next time frame t + 1 can be approximated by</p><formula xml:id="formula_125">v t+1 i = v t i + a t i • dt.<label>(79)</label></formula><p>Given m i , x t i , and v t i , our task asks the neural network to predict v t+1 i for all stars X i . In our task, we consider two extrapolation schemes a) The distances between stars x t i − x t j 2 are out-of-distribution for test set, i.e., different sampling ranges from the training set.</p><p>b) The masses of stars m i are out-of-distribution for test set, i.e., different sampling ranges from the training set.</p><p>Here, we use a physics engine that we code in Python to simulate and sample the inputs and labels. We describe the dataset details next. Dataset details. We first describe the simulation and sampling of our training set. We sample 100 videos of n-body system evolution, each with 500 rollout, i.e., time steps. We consider the orbit situation: there exists a huge center star and several other stars. We sample the initial states, i.e., position, velocity, masses, acceleration etc according to the following parameters.</p><p>a) The mass of the center star is 100kg.</p><p>b) The masses of other stars are sampled from [0.02, 9.0]kg.</p><p>c) The number of stars is 3.</p><p>d) The initial position of the center star is (0.0, 0.0).</p><p>d) The initial positions x t i of other objects are randomly sampled from all angles, with a distance in [10.0, 100.0]m.</p><p>e) The velocity of the center star is 0.</p><p>f) The velocities of other stars are perpendicular to the gravitational force between the center star and itself. The scale is precisely determined by physics laws to ensure the initial state is an orbit system.</p><p>For each video, after we get the initial states, we continue to rollout the next frames according the physics engine described above. We perform rejection sampling of the frames to ensure that all pairwise distances of stars in a frame are at least 30m. We guarantee that there are 10, 000 data points in the training set.</p><p>The validation set has the same sampling and simultation parameters as the training set. We have 2, 500 data points in the validation set.</p><p>For test set, we consider two datasets, where we respectively have OOD distances and masses. We have 5, 000 data points for each dataset. iii) The distances are in-distribution, i.e., same sampling process as training set.</p><p>Model and hyperparameter settings. We consider the following one-iteration Graph Neural Network (GNN) architecture, a.k.a. Interaction Networks. Given a collection of stars S = {X i } n i=1 , our GNN runs on a complete graph with nodes being the stars X i . GNN learns the star (node) representations by aggregating and transforming the interactions (forces) of all other node vectors</p><formula xml:id="formula_126">o u = MLP (2)   v∈S\{u} MLP (1) h v , h u , w (u,v)   .<label>(80)</label></formula><p>Here, h v is the input feature of node v, including mass, position and velocity</p><formula xml:id="formula_127">h v = (m v , x v , v v )</formula><p>w (u,v) is the input edge feature of edge (u, v). The loss is computed and backpropagated via the MSE loss of c) The number of layers for MLP (1) is set to 4. The number of layers for MLP (2) is set to 2.</p><p>d) We consider two representations of edge/relations w (i,j) .</p><p>i) The first one is simply 0.</p><p>ii) The better representation, which makes the underlying target function more linear, is </p><formula xml:id="formula_128">w (i,j) = m j x t i − x t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Visualization and Additional Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Visualization Results</head><p>In this section, we show additional visualization results of the MLP's learned function out of training distribution (in black color) v.s. the underlying true function (in grey color). We color the predictions in training distribution in blue color.</p><p>In general, MLP's learned functions agree with the underlying true functions in training range (blue). This is explained by in-distribution generalization arguments. When out of distribution, the MLP's learned functions become linear along directions from the origin. We explain this OOD directional linearity behavior in Theorem 3.</p><p>Finally, we show additional experimental results for graph-based reasoning tasks.      The underlying functions are linear, but we train MLPs on different distributions, whose support potentially miss some directions. The training support for "all" are hyper-cubes that intersect all directions. In "fix1", we set the first dimension of training data to a fixed number. In "posX", we restrict the first X dimensions of training data to be positive. We can see that MLPs trained on "all" extrapolate the underlying linear functions, but MLPs trained on datasets with missing directions, i.e., "fix1" and "posX", often cannot extrapolate well.  Here, W is a square matrix and a is a vector with random entries. The results suggest that using tanh activation function can help encode appropriate non-linearity and help extrapolation, but is sometimes sensitive to hyper-parameters. The detailed experimental settings (e.g., hyper-parameters) can be found in Appendix C.4. The results suggest that using quadratic activation function can help encode appropriate non-linearity, but the extrapolation error is sometimes sensitive to hyper-parameters. Moreover, as expected, MLPs with quadratic activation functions fail to extrapolate L1 target functions. The detailed experimental settings (e.g., hyper-parameters) can be found in Appendix C.4.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: How ReLU MLPs extrapolate. We train MLPs to learn non-linear 2D functions (grey) and plot their predictions both within (blue) and outisde (black) the training distribution. MLPs converge quickly to linear functions outside the training data range along directions from the origin (Theorem 3). Hence, MLPs do not extrapolate well in most non-linear tasks. But, under mild conditions MLPs can extrapolate well in tasks with globally linear target functions (Theorem 5).</figDesc><graphic url="image-1.png" coords="4,75.91,57.64,159.12,119.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Conditions for MLPs to extrapolate well when learning linear target functions. We train MLPs to learn linear 2D functions (grey) with different training distributions (blue) and plot their out-ofdistribution predictions (black). Following Theorem 5, MLPs extrapolate well when the support of the training distribution (blue) covers all directions (first panel). MLPs do not extrapolate well when we restrict the directions of the training examples: In the second and third panel, we constrain one or two dimensions of training data to be positive (red arrows). In the last panel, we fix one dimension of training data to a constant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Distribution of mean absolute percentage error (MAPE) for extrapolation. We train ReLU MLPs to learn different target functions and compute MAPE on test examples outside the training distribution (Appendix C). We plot distributions of test errors from many trials with different training/test distributions and hyperparameters. Figure (a) compares extrapolation in tasks with different target functions. Figure (b) compares different training distributions for tasks with linear target functions: "all" covers all directions; "fix1" has one dimension fixed to a constant; and "negd" has d dimensions constrained to negative values. Results align with our theory. MLPs generally do not extrapolate well, unless the target function is almost-linear along each direction (Figure (a)). For linear target functions, MLPs extrapolate well if only if the support of training distribution covers all directions (Figure (b)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Importance of training graph structure. Each row indicates the graph structure covered by training set and the extrapolation error (MAPE). (a) GNNs with max-pooling readout extrapolate well in max degree, if the max/min degrees of training graphs are not fixed (Theorem 9). (b) In shortest path, the extrapolation errors of min-pooling GNNs follow a U-shape with respect to the sparsity of training graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>2), Theorem 5 (proof in Appendix B.3), and Theorem 3 (proof in Appendix B.1) we analyze over-parameterized MLPs. The proof of Corollary 8 is in Appendix B.4. In Theorem 9 we analyze Graph Neural Networks (GNNs) (proof in Appendix B.5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>43)Hence, the indicators decrease faster, and it follows that equation 23 converges to 0 with rate O( 1 ). Moreover, we can bound w with standard concentration techniques. Then the proofs for equation 25 and equation 26 follow similarly. This completes the proof.B.2 Proof of Lemma 4Overview of proof. To prove exact extrapolation given the conditions on training data, we analyze the function learned by the neural network in a functional form. The network's learned function can be precisely characterized by a solution in the network's neural tangent kernel feature space which has a minimum RKHS norm among functions that can fit all training data, i.e., it corresponds to the optimum of a constrained optimization problem. We show that the global optimum of this constrained optimization problem, given the conditions on training data, is precisely the same function as the underlying true function.Setup and preparation. Let X = {x 1 , ..., x n } and Y = {y 1 , ..., y n } denote the training set input features and their labels. Let β g ∈ R d denote the true parameters/weights for the underlying linear function g, i.e., g(x) = β g x for all x ∈ R d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>h u in RKHS space, and denote by β w ∈ R the weight for max u∈G w h u . Similarly, we can combine the effect of all β in the same direction as in Lemma 4 and 5. We define the combined effect with β w and β w . This allows us to reason about w with two directions, + and −.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>a)</head><label></label><figDesc>Quadratic functions g(x) = x Ax. In each dataset, we randomly sample A. In the simplest case whereA = I, g(x) = d i=1 x 2 i . a) Cosine functions g(x) = d i=1 cos (2π • x i ). c) Square root functions g(x) = d i=1 √ x i .Here, the domain X of x is restricted to the space in R d with non-negative value in each dimension.d) L1 norm functions g(x) = |x| 1 = d i=1 |x i |.We sample each dataset of a task by considering the following parameters a) The shape and support of training, validation, and test data distributions. i) Training, validation, and test data are uniformly sampled from a hyper-cube. Training and validation data are sampled from [−a, a] d with a ∈ {0.5, 1.0}, i.e., each dimension of x ∈ R d is uniformly sampled from [−a, a]. Test data are sampled from [−a, a] d with a ∈ {2.0, 5.0, 10.0}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>a) Number of layers d from {2, 4}. b) Width of each W (k) from {64, 128, 512}. c) Initialization schemes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>2</head><label></label><figDesc>d k , where d k is the output dimension of W (k) . d) Activation function σ is set to ReLU. We train the MLP with the mean squared error (MSE) loss and Adam optimizer with learning rate schedule 0.5 for every 50 epochs. We search the following hyper-parameters for training a) Initial learning rate from {5e − 2, 1e − 2, 5e − 3, 1e − 3}. b) Batch size from {32, 64, 128}. c) Weight decay is set to 1e − 5. d) Number of epochs is set to 250. the QR decomposition. Our training samples are QX, i.e., multiply each point in X by Q. This gives 100 training sets with 2d data points satisfying the condition in Theorem 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>a) Underlying function is y = a sin(W x). Here, W is a square matrix and a is a vector, whose entries are randomly sampled from [−5, 5]. The input dimension d = [1, 2, 4, 8]. The training and test data are sampled uniformly from the hyper-cube, with training range [−20π, 20π] d and test range [−40π, 40π] d . We have 40, 000 training data, 1, 000 validation data, and 80, 000 test data. b) Underlying function is y = W x, whose entries are randomly sampled from [−5, 5]. The input dimension d = [2]. The training and test data are sampled uniformly from the hyper-cube or hypersphere, with training range [−5, 5] d and test range [−20, 20] d . We have 10, 000 training data, 1, 000 validation data, and 20, 000 test data. quadratic activation. Formally, the activation functino is σ(x) = x 2 . We consider two tasks. a) Underlying function is y = a(W x) 2 . Here, W is a square matrix and a is a vector, whose entries are randomly sampled from [−5, 5]. The input dimension d = [1, 32]. The training and test data are sampled uniformly from the hyper-cube, with training range [−4, 4] d and test range [−20, 20] d . We have 20, 000 training data, 1, 000 validation data, and 80, 000 test data. b) Underlying function is y = d i=1 |x i |. The input dimension d = [1, 2, 8]. The training and test data are sampled uniformly from the hyper-cube or hyper-sphere, with training range [−1, 1] d and test range [−5, 5] d . We have 20, 000 training data, 1, 000 validation data, and 20, 000 test data. tanh activation. Formally, the activation functino is σ(x) = tanh(x). Underlying function is y = a tanh(W x). Here, W is a square matrix and a is a vector, whose entries are randomly sampled from [−5, 5]. The input dimension d = [1, 32]. The training and test data are sampled uniformly from the hyper-cube, with training range [−4, 4] d and test range [−20, 20] d . We have 20, 000 training data, 1, 000 validation data, and 80, 000 test data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>c) The number of vertices of graphs |V | for training and validation sets are sampled uniformly from [20...30]. The number of vertices of graphs |V | for test set is sampled uniformly from [50..100]. d) We consider two schemes for node features. i) Identical features. All nodes in training, validation and set sets have uniform feature 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>u</head><label></label><figDesc>is the input node feature of node u. We search the following hyper-parameters for GNNs a) Number of GNN iterations K is 1. b) Graph pooling is from max or sum. c) Width of all MLPs are set to 256.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>u</head><label></label><figDesc>is the input node feature of node u. w (u,v) is the input edge feature of edge (u, v). We search the following hyper-parameters for GNNs a) Number of GNN iterations K is set to 3. b) Graph pooling is set to min. c) Neighobr aggregation is selected from min and sum. d) Width of all MLPs are set to 256.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>b) Batch size is set to 64. c) Weight decay is set to 1e − 5. d) Number of epochs is set to 250.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>a) We sample the distance OOD test set to ensure all pairwise distances of stars in a frame are from [1..20]m, but have in-distribution masses. b) We sample the mass OOD test set as follows i) The mass of the center star is 200kg, i.e., twice of that in the training set. ii) The masses of other stars are sampled from [0.04, 18.0]kg, compared to [0.02, 9.0]kg in the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>[o 1</head><label>1</label><figDesc>, ..., o n ] − [ans 1 , .., ans n ] 2 ,where o i denotes the output of GNN for node i, and ans i denotes the true label for node i in the next frame.We search the following hyper-parameters for GNNs a) Number of GNN iterations is set to 1.b) Width of all MLPs are set to 128.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>i</head><label></label><figDesc>We train the GNN with the mean squared error (MSE) loss and Adam optimizer with learning rate schedule 0.5 for every 50 epochs. We search the following hyper-parameters for training a) Initial learning rate is set to 0.005. b) Batch size is set to 32. c) Weight decay is set to 1e − 5. d) Number of epochs is set to 2, 000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: (Quadratic function). Both panels show the learned v.s. true y = x 2 1 + x 2 2 . In each figure, we color OOD predictions by MLPs in black, underlying function in grey, and in-distribution predictions in blue. The support of training distribution is a square (cube) for the top panel, and is a circle (sphere) for the bottom panel.</figDesc><graphic url="image-9.png" coords="47,118.80,353.81,374.40,280.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: (Cos function). Both panels show the learned v.s. true y = cos(2π • x 1 ) + cos(2π • x 2 ). In each figure, we color OOD predictions by MLPs in black, underlying function in grey, and in-distribution predictions in blue. The support of training distribution is a square (cube) for both top and bottom panels, but with different ranges.</figDesc><graphic url="image-11.png" coords="48,118.80,364.42,374.42,280.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: (Cos function). Top panel shows the learned v.s. true y = cos(2π • x 1 ) + cos(2π • x 2 ) where the support of training distribution is a circle (sphere). Bottom panel shows results for cosine in 1D, i.e. y = cos(2π • x). In each figure, we color OOD predictions by MLPs in black, underlying function in grey, and in-distribution predictions in blue.</figDesc><graphic url="image-13.png" coords="49,165.60,399.52,280.80,210.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: (Sqrt function). Top panel shows the learned v.s. true y = √ x 1 + √ x 2 where the support of training distribution is a square (cube). Bottom panel shows the results for the square root function in 1D, i.e. y = √ x. In each figure, we color OOD predictions by MLPs in black, underlying function in grey, and in-distribution predictions in blue.</figDesc><graphic url="image-15.png" coords="50,165.60,399.52,280.80,210.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: (L1 function). Both panels show the learned v.s. true y = |x|. In the top panel, the MLP successfully learns to extrapolate the absolute function. In the bottom panel, an MLP with different hyperparameters fails to extrapolate. In each figure, we color OOD predictions by MLPs in black, underlying function in grey, and in-distribution predictions in blue.</figDesc><graphic url="image-17.png" coords="51,165.60,283.59,280.80,210.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 13 :Figure 14 :</head><label>1314</label><figDesc>Figure 13: (Linear function). Both panels show the learned v.s. true y = x 1 + x 2 , with the support of training distributions being square (cube) for top panel, and circle (sphere) for bottom panel. MLPs successfully extrapolate the linear function with both training distributions. This is explained by Theorem 5: both sphere and cube intersect all directions. In each figure, we color OOD predictions by MLPs in black, underlying function in grey, and in-distribution predictions in blue.</figDesc><graphic url="image-21.png" coords="53,118.80,357.65,374.40,280.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 16 :Figure 17 :Figure 19 :</head><label>161719</label><figDesc>Figure 16: Maximum degree: max-pooling v.s. sum-pooling. In each sub-figure, left column shows test errors for GNNs with graph-level max-pooling; right column shows test errors for GNNs with graph-level sum-pooling. x-axis shows the graph structure covered in training set. GNNs with sum-pooling fail to extrapolate, validating Corollary 8. GNNs with max-pooling encodes appropriate non-linear operations, and thus extrapolates under appropriate training sets (Theorem 9).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 20 :</head><label>20</label><figDesc>Figure20: Density plot of mean average percentage error (extrapolation) with MLPs with quadratic activation function. Formally, activation is σ(x) = x 2 . The x-axis stands for the target function class, and the y-axis stands for the MAPE. On the x-axis, the underlying functions for "quad" are y = a(W x) 2 . Here, W is a square matrix and a is a vector with random entries. L1 stands for target function y = i=1 d |x i |. The results suggest that using quadratic activation function can help encode appropriate non-linearity, but the extrapolation error is sometimes sensitive to hyper-parameters. Moreover, as expected, MLPs with quadratic activation functions fail to extrapolate L1 target functions. The detailed experimental settings (e.g., hyper-parameters) can be found in Appendix C.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-18.png" coords="52,118.80,82.62,374.40,280.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-19.png" coords="52,118.80,364.42,374.40,280.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Extrapolation for DP tasks. Each column is the task and mean average percentage error (MAPE). Encoding appropriate non-linearity in the architecture or representation significantly improves extrapolation, and is less helpful for interpolation. Left: In max degree and shortest path, GNNs that appropriately encode max/min extrapolate well, but GNNs with sum-pooling do not extrapolate well. Right: With an improved input representation, GNNs extrapolate better in n-body problem.</figDesc><table><row><cell></cell><cell cols="2">sum pooling</cell><cell cols="2">max/min pooling</cell><cell></cell><cell cols="3">original features</cell><cell cols="2">improved features</cell></row><row><cell>70.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>11.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>43.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6.3</cell><cell></cell></row><row><cell>0.0</cell><cell>6.5</cell><cell>0.0</cell><cell>0.0</cell><cell>6.1</cell><cell>0.0</cell><cell>1.5</cell><cell></cell><cell>1.1</cell><cell></cell><cell>1.2</cell><cell>0.7</cell></row><row><cell cols="3">extrapolate interpolate max degree</cell><cell cols="3">extrapolate interpolate shortest path</cell><cell cols="4">extrapolate dist extrapolate mass n-body problem</cell><cell>interpolate</cell></row><row><cell cols="5">(a) Importance of architecture.</cell><cell></cell><cell cols="5">(b) Importance of representation.</cell></row><row><cell>Figure 5: path 4regular ladder cycle expander complete tree general</cell><cell>0.1 0.1 0.0 0.0</cell><cell>6.4</cell><cell>12.5 11.0</cell><cell></cell><cell>94.5</cell><cell>complete expander general 4regular ladder cycle tree path</cell><cell>2.4 0.0 0.4</cell><cell>8.9 11.6 13.8</cell><cell>19.3</cell><cell>33.6</cell></row></table><note>(a) Max degree with max-pooling GNN.(b) Shortest path with min-pooling GNN.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>i is a perfect data set that our training set converges to. Combining this with NTK −1 train</figDesc><table /><note>where NTK train is the n × n kernel for training data, NTK(x, x i ) is the kernel value between test data x and training data x i , and Y is training labels.Similarly, we have (NTK(x, x 1 ), ..., NTK(x, x n )) → (NTK(x, x * 1 ), ..., NTK(x, x * n )), where x *</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>These validate Theorem 5 and 4, i.e., MLPs can extrapolate if the underlying function is linear under conditions on training distribution. This section includes four ablation studies: a) Training distribution satisfy the conditions in Theorem 5 and cover all directions, and hence, MLPs extrapolate. b) Training data distribution is restricted in some directions, e.g., restricted to be positive/negative/constant in some feature dimensions. This shows when training distribution is restrictive in directions, MLPs may fail to extrapolate. Node features are spurious and continuous. This also requires extrapolation on OOD node features. GNNs with graph-level max-pooling with appropriate training sets also extrapolate to OOD spurious node features. 6) Dynamic programming: learning the length of the shortest path between given source and target nodes, with Graph Neural Networks. Extrapolation on graph structure, number of nodes, and edge weights. We study the following regimes. a) Continuous features. Edge and node features are real values. This regime requires extrapolating to graphs with edge weights out of training range.</figDesc><table><row><cell>c) Exact extrapolation with infinitely-wide neural networks, i.e., exact computation with neu-</cell></row><row><cell>ral tangent kernel (NTK) on the data regime in Theorem 4. This is mainly for theoretical</cell></row><row><cell>understanding.</cell></row><row><cell>4) MLPs with sin, quadratic, and tanh activation functions.</cell></row><row><cell>5) Summary statistics: learning maximum degree of graphs with Graph Neural Networks. Extrapo-</cell></row><row><cell>lation on graph structure, number of nodes, and node features. To show the role of architecture for</cell></row><row><cell>extrapolation, we study the following GNN architecture regimes.</cell></row><row><cell>a) GNN with graph-level max-pooling and neighbor-level sum-pooling. By Theorem 9, this GNN</cell></row><row><cell>architecture extrapolates in max degree with appropriate training data.</cell></row><row><cell>b) GNN with graph-level and neighbor-level sum-pooling. By Corollary 8, this default GNN</cell></row><row><cell>architecture cannot extrapolate in max degree.</cell></row><row><cell>To show the importance of training distribution, i.e., graph structure in training set, we study the</cell></row><row><cell>following training data regimes.</cell></row><row><cell>a) Node features are identical, e.g., 1. In such regimes, our learning tasks only consider graph</cell></row><row><cell>structure. We consider training sets sampled from various graph structure, and find only those</cell></row><row><cell>satisfy conditions in Theorem 9 enables GNNs with graph-level max-pooling to extrapolate.</cell></row><row><cell>b)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>This corresponds to (non-uniform) sampling from a hyper-ball in R d . b) We sample 20, 000 training data, 1, 000 validation data, and 20, 000 test data. c) We sample input dimension d from {1, 2, 8}. d) For quadratic functions, we sample the entries of A uniformly from [−1, 1].</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Figure15: Maximum degree: spurious (real-valued) node features. Here, each node has a spurious node feature in R 3 that shall not contribute to the answer of maximum degree. GNNs with graph-level max-pooling extrapolate to graphs with OOD node features and graph structure, graph sizes, if trained on graphs that satisfy the condition in Theorem 9.</figDesc><table><row><cell></cell><cell>75.7</cell><cell>86.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>94.2</cell><cell></cell><cell></cell><cell>94.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell>0.1</cell><cell>0.3</cell><cell>0.1</cell></row><row><cell>path</cell><cell>4-regular</cell><cell>ladder</cell><cell>cycle</cell><cell cols="2">expander complete</cell><cell>tree</cell><cell>general</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">All experimental setups and details are in Appendix C.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We consider max degree as a special case of DP: it only requires one round of DP-update.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Ruosong Wang, Tianle Cai, Toru Lin, Han Zhao, Yuichi Yoshida, Takuya Konishi, Weihua Hu, Matt J. Staib, and Leo Yu for insightful discussions. This research was supported by NSF CAREER award 1553284, NSF III 1900933, and a Chevron-MIT Energy Fellowship. This research was also supported by JST ERATO JPMJER1201 and JSPS Kakenhi JP18H05291. MZ was supported by ODNI, IARPA, via the BETTER Program contract 2019-19051600005. The views, opinions, and/or findings contained in this article are those of the author and should not be interpreted as representing the official views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency, the Department of Defense, ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proofs of All Theorems and Lemmas</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Proof of Theorem 3</head><p>To show neural network outputs f (x) converge to a linear function along all directions v, we will analyze the function learned by a neural network on the training set {(x i , y i )} n i=1 , by studying the functional representation in the network's neural tangent kernel RKHS space.</p><p>Test error and model selection. For each dataset and architecture, training hyper-parameter setting, we perform model selection via validation set, i.e., we report the test error by selecting the epoch where the model achieves the best validation error. Note that our validation sets always have the same distribution as the training sets.</p><p>We train our models with the MSE loss according to the neural tangent kernel (NTK) theory. But because we sample test data from different ranges, the mean absolute percentage error (MAPE) loss, which scales the error by the actual value, better measures the extrapolation performance</p><p>where A i is the actual value and F i is the predicted value. Hence, in our experiments, we also report the MAPE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 R-squared for Out-of-distribution Directions</head><p>We perform linear regression to fit the predictions of MLPs along randomly sampled directions in out-ofdistribution regions, and compute the R-squared (or R 2 ) for these directions. This experiment is to validate Theorem 3 and show that the convergence rate (to a linear function) is very high in practice.</p><p>Definition. R-squared, also known as coefficient of determination, assesses how strong the linear relationship is between input and output variables. The closer R-squared is to 1, the stronger the linear relationship is, with 1 being perfectly linear.</p><p>Datasets and models. We perform the R-squared computation on over 2, 000 combinations of datasets, test/train distributions, and hyper-parameters, e.g., learning rate, batch size, MLP layer, width, initialization. These are described in Appendix C.1.</p><p>Computation. For each combination of dataset and model hyper-parameters as described in Section C.1, we save the trained MLP model f : R d → R. For each dataset and model combination, we then randomly sample 5, 000 directions via Gaussian vectors N (0, I). For each of these directions w, we compute the intersection point x w of direction w and the training data distribution support (specified by a hyper-sphere or hyper-cube; see Section C.1 for details).</p><p>We then collect 100 predictions of the trained MLP f along direction w (assume w is normalized) with</p><p>where r is the range of training data distribution support (see Section C.1). We perform linear regression on these predictions in equation 74, and obtain the R-squared.</p><p>Results. We obtain the R-squared for each combination of dataset, model and training setting, and randomly sampled direction. For the tasks of learning the simple non-linear functions, we confirm that more than 96% of the R-squared results are above 0.99. This empirically confirms Theorem 3 and shows that the convergence rate is in fact fast in practice. Along most directions, MLP's learned function becomes linear immediately out of the training data support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Learning Linear Functions</head><p>Dataset details. We consider the tasks where the underlying functions are linear g : R d → R. Given an input x ∈ R d , the label is computed by y = g(x) = Ax for all x. For each dataset, we sample the following parameters ii) Training and validation data are uniformly sampled from a sphere, where every point has L2 distance r from the origin. We sample r from r ∈ {5.0, 10.0}. Then, we sample a random Gaussian vector q in R d . We obtain the training or validation data x = q/ q 2 • r. This corresponds to uniform sampling from the sphere.</p><p>Test data are sampled (non-uniformly) from a hyper-ball. We first sample r uniformly from [0.0, 20.0] and [0.0, 50.0],. Then, we sample a random Gaussian vector q in R d . We obtain the test data x = q/ q 2 • r. This corresponds to (non-uniform) sampling from a hyper-ball in R d .</p><p>e) We perform ablation study on how the training distribution support misses directions. The test distributions remain the same as in d).</p><p>i) We restrict the first dimension of any training data x i to a fixed number 0.1, and randomly sample the remaining dimensions according to d).</p><p>ii) We restrict the first k dimensions of any training data x i to be positive. We train our models with the MSE loss according to the neural tangent kernel (NTK) theory. But because we sample test data from different ranges, the mean absolute percentage error (MAPE) loss, which scales the error by the actual value, and better measures the extrapolation performance</p><p>where A i is the actual value and F i is the predicted value. Hence, we also report the MAPE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 Shortest Path</head><p>Dataset details. We consider the task of finding the length of the shortest path on a graph, from a given source to target nodes. Given any graph G = (V, E), the node features, besides regular node features, encode whether a node is source s, and whether a node is target t. The edge features are a scalar representing the edge weight. For unweighted graphs, all edge weights are 1. Then the label y = g(G) is the length of the shortest path from s to t on G.</p><p>For each dataset, we sample the graphs and node, edge features with the following parameters a) Graph structure for training and validation sets. For each dataset, we consider one of the following graph structure: path graphs, cycles, ladder graphs, 4-regular random graphs, complete graphs, random trees, expanders (random graphs with p = 0.6), and general graphs (random graphs with p = 0.1 to 0.9 with equal probability). We use the networkx library for sampling graphs.</p><p>b) Graph structure for test set. We consider the general graphs (random graphs with p = 0.1 to 0.9 with equal probability). e) After sampling a graph and edge weights, we sample source s and t by randomly sampling s, t and selecting the first pair s, s whose shortest path involves at most 3 hops. This enables us to solve the task using GNNs with 3 iterations.</p><p>f) We sample 10, 000 graphs for training, 1, 000 graphs for validation, and 2, 500 graphs for testing.</p><p>We also consider the ablation study of training on random graphs with different p. We consider p = 0.05..1.0 and report the test error curve. The other parameters are the same as described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Sin, quadratic and tanh Activation</head><p>This section shows preliminary experimental results for MLPs with sin, quadratic, and tanh activation functions trained by gradient descent. The detailed experimental settings (e.g., hyper-parameters) can be found in Appendix C.4. Formally, activation is σ(x) = sin(x). The x-axis stands for the target function class, and the y-axis stands for the MAPE. On the x-axis, the underlying functions for 1, 2, 4, 8 are y = a sin(W x) with dimension of input feature x being 1, 2, 4, 8 respectively. Here, W is a square matrix and a is a vector with random entries. Linear stands for linear target function y = W x. The results suggest that using sin activation function can help encode appropriate non-linearity, but the extrapolation is not always successful, in particular when the input dimension is high. An explanation is that for high dimensions, we need larger training domains for "identifiability". Moreover, as expected, MLPs with sin activation functions fail to extrapolate linear target functions. The detailed experimental settings (e.g., hyper-parameters) can be found in Appendix C.4.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning and generalization in overparameterized neural networks, going beyond two layers</title>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="page" from="6155" to="6166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A convergence theory for deep learning via overparameterization</title>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="242" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<title level="m">Invariant risk minimization</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding deep neural networks with rectified linear units</title>
		<author>
			<persName><forename type="first">Raman</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amitabh</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Poorya</forename><surname>Mianjy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirbit</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruosong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="page" from="322" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On exact computation with an infinitely wide neural net</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruosong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="8139" to="8148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Harnessing the power of infinitely wide deep nets on small-data tasks</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingli</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Extrapolation and interpolation in neural network classifiers</title>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName><surname>Wessels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Control Systems Magazine</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="50" to="53" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4502" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Peter W Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On a routing problem</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Bellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly of applied mathematics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="37" />
			<date type="published" when="1958">1958. 1966</date>
		</imprint>
	</monogr>
	<note>Science</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the inductive bias of neural tangent kernels</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Bietti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="12873" to="12884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning bounds for domain adaptation</title>
		<author>
			<persName><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Wortman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generalization bounds of stochastic gradient descent for wide and deep neural networks</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10835" to="10845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A note on lazy training in supervised differentiable programming</title>
		<author>
			<persName><forename type="first">Lenaic</forename><surname>Chizat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07956</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On lazy training in differentiable programming</title>
		<author>
			<persName><forename type="first">Lenaic</forename><surname>Chizat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Oyallon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2933" to="2943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematics of control, signals and systems</title>
				<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="303" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gradient descent finds global minima of deep neural networks</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haochuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyu</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="page" from="1675" to="1685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph neural tangent kernel: Fusing graph neural networks with graph kernels</title>
		<author>
			<persName><forename type="first">Kangcheng</forename><surname>Simon S Du</surname></persName>
		</author>
		<author>
			<persName><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabas</forename><surname>Russ R Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruosong</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="5724" to="5734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gradient descent provably optimizes overparameterized neural networks</title>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyu</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019c</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the approximate realization of continuous mappings by neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Funahashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="183" to="192" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1273" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributionally robust optimization and its tractable approximations</title>
		<author>
			<persName><forename type="first">Joel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvyn</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations research</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">4-part-1</biblScope>
			<biblScope unit="page" from="902" to="917" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Extrapolation limitations of multilayer feedforward neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pamela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Haley</surname></persName>
		</author>
		<author>
			<persName><surname>Soloway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Complexity of linear regions in deep networks</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Hanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davd</forename><surname>Rolnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Train faster, generalize better: Stability of stochastic gradient descent</title>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Why relu networks yield high-confidence predictions far away from the training data and how to mitigate the problem</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksym</forename><surname>Andriushchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Bitterwolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="41" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Halbert</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural tangent kernel: Convergence and generalization in neural networks</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clément</forename><surname>Hongler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8571" to="8580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Kolmogorov&apos;s theorem and multilayer neural networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kurková</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="501" to="506" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Building machines that learn and think like people</title>
		<author>
			<persName><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tomer D Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and brain sciences</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep learning for symbolic mathematics</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franãgois</forename><surname>Charton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Wide neural networks of any depth evolve as linear models under gradient descent</title>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lechao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8570" to="8581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning overparameterized neural networks via stochastic gradient descent on structured data</title>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8157" to="8166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations</title>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research</title>
				<meeting>Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="1" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Towards explaining the regularization effect of initial large learning rate in training neural networks</title>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11669" to="11680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Hartmut</forename><surname>Maennel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08367</idno>
		<title level="m">Gradient Descent Quantizes ReLU Network Features. arXiv e-prints, art</title>
				<imprint>
			<date type="published" when="2018-03">March 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Domain adaptation: Learning bounds and algorithms</title>
		<author>
			<persName><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Exploring generalization in deep learning</title>
		<author>
			<persName><forename type="first">Srinadh</forename><surname>Behnam Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nati</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5947" to="5956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Neural tangents: Fast and easy infinite neural networks in python</title>
		<author>
			<persName><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lechao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiri</forename><surname>Hron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Causal inference by using invariant prediction: identification and confidence intervals</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bühlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolai</forename><surname>Meinshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="947" to="1012" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Invariant models for causal transfer learning</title>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Rojas-Carulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1309" to="1342" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Distributionally robust neural networks</title>
		<author>
			<persName><forename type="first">Shiori</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Graph networks as learnable physics engines for inference and control</title>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4467" to="4476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Measuring abstract reasoning in neural networks</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4477" to="4486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">How do infinite width bounded norm networks look in function space?</title>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Evron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory (COLT)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Analysing mathematical reasoning abilities of neural models</title>
		<author>
			<persName><forename type="first">David</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Certifying some distributional robustness with principled adversarial training</title>
		<author>
			<persName><forename type="first">Aman</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A mean field view of the landscape of two-layers neural networks</title>
		<author>
			<persName><forename type="first">Mei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="E7665" to="E7671" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The implicit bias of gradient descent on separable data</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mor</forename><surname>Shpigel Nacson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suriya</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2822" to="2878" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Neural arithmetic logic units</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Trask</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8035" to="8044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A theory of the learnable</title>
		<author>
			<persName><forename type="first">G</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixteenth annual ACM symposium on Theory of computing</title>
				<meeting>the sixteenth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1984">1984</date>
			<biblScope unit="page" from="436" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Petar Velickovic, Rex Ying, Matilde Padovano, Raia Hadsell, and Charles Blundell. Neural execution of graph algorithms</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2013">2013. 2020</date>
		</imprint>
	</monogr>
	<note>The nature of statistical learning theory</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Visual interaction networks: Learning a physics simulator from video</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theophane</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Tacchetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4539" to="4547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Gradient dynamics of shallow univariate relu networks</title>
		<author>
			<persName><forename type="first">Francis</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Trager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Panozzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Zorin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8376" to="8385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">What can neural networks reason</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Ichi Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJxbJeHFPS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Are girls neko or shōjo? cross-lingual alignment of non-isomorphic embeddings with iterative normalization</title>
		<author>
			<persName><forename type="first">Mozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3180" to="3189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Adversarial multiple source domain adaptation</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanhang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><forename type="middle">P</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8559" to="8570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">On learning invariant representations for domain adaptation</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Tachet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Des</forename><surname>Combes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7523" to="7532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Both panels show the learned v.s. true y = |x 1 | + |x 2 |. In the top panel, the MLP successfully learns to extrapolate the l1 norm function. In the bottom panel, an MLP with different hyper-parameters fails to extrapolate. In each figure, we color OOD predictions by MLPs in black, underlying function in grey</title>
	</analytic>
	<monogr>
		<title level="m">L1 function)</title>
				<imprint>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
	<note>and in-distribution predictions in blue</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
