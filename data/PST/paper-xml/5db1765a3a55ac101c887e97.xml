<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-10-24">24 Oct 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
							<email>craffel@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
						</author>
						<title level="a" type="main">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-10-24">24 Oct 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1910.10683v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code. 1 * Equal contribution. A description of each author's contribution is available in Appendix A.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Training a machine learning model to perform natural language processing (NLP) tasks often requires that the model can process text in a way that is amenable to downstream learning. This can be loosely viewed as developing general-purpose knowledge that allows the model to "understand" text. This knowledge can range from low-level (e.g. the spelling or meaning of words) to high-level (e.g. that a tuba is too large to fit in most backpacks). In modern machine learning practice, providing this knowledge is rarely done explicitly; instead, it is often "learned" as part of an auxiliary task. For example, a historically common approach is to use "word vectors" <ref type="bibr">[Mikolov et al., 2013b,a;</ref><ref type="bibr" target="#b75">Pennington et al., 2014]</ref> to map word identities to a continuous representation where, ideally, similar words map to similar vectors. These vectors are often learned through an objective that, for example, encourages co-occurring words to be positioned nearby in the continuous space <ref type="bibr">[Mikolov et al., 2013b]</ref>.</p><p>Recently, it has become increasingly common to pre-train the entire model on a data-rich task. Ideally, this pre-training causes the model to develop general-purpose abilities and knowledge that can then be "transferred" to downstream tasks. In applications of transfer learning to computer vision <ref type="bibr" target="#b72">[Oquab et al., 2014;</ref><ref type="bibr" target="#b41">Jia et al., 2014;</ref><ref type="bibr" target="#b39">Huh et al., 2016;</ref><ref type="bibr" target="#b118">Yosinski et al., 2014]</ref>, pre-training is typically done via supervised learning on a large labeled dataset like ImageNet <ref type="bibr" target="#b89">[Russakovsky et al., 2015;</ref><ref type="bibr" target="#b22">Deng et al., 2009]</ref>. In contrast, modern techniques for transfer learning in NLP often pre-train using unsupervised learning on unlabeled data. This approach has recently been used to obtain state-of-the-art results in many of the most common NLP benchmarks <ref type="bibr" target="#b23">[Devlin et al., 2018;</ref><ref type="bibr" target="#b64">Yang et al., 2019;</ref><ref type="bibr" target="#b25">Dong et al., 2019;</ref><ref type="bibr" target="#b65">Liu et al., 2019c;</ref><ref type="bibr" target="#b57">Lan et al., 2019]</ref>. Beyond its empirical strength, "translate English to German: That is good." "cola sentence: The course is jumping well." "summarize: state authorities dispatched emergency crews tuesday to survey the damage after an onslaught of severe weather in mississippi?"</p><p>"stsb sentence1: The rhino grazed on the grass. sentence2: A rhino is grazing in a field."</p><p>T5 "Das ist gut."</p><p>"not acceptable"</p><p>"six people hospitalized after a storm in attala county."</p><p>"3.8"</p><p>Figure <ref type="figure">1</ref>: A diagram of our text-to-text framework. Every task we consider -including translation, question answering, and classification -is cast as feeding our model text as input and training it to generate some target text. This allows us to use the same model, loss function, hyperparameters, etc. across our diverse set of tasks. It also provides a standard testbed for the methods included in our empirical survey. "T5" refers to our model, which we dub the "Text-to-Text Transfer Transformer".</p><p>unsupervised pre-training for NLP is particularly attractive because unlabeled text data is available en masse thanks to the Internet -for example, the Common Crawl project<ref type="foot" target="#foot_0">2</ref> produces about 20TB of text data extracted from web pages each month. This is a natural fit for neural networks, which have been shown to exhibit remarkable scalability, i.e. it is often possible to achieve better performance simply by training a larger model on a larger dataset <ref type="bibr" target="#b32">[Hestness et al., 2017;</ref><ref type="bibr" target="#b96">Shazeer et al., 2017;</ref><ref type="bibr" target="#b45">Jozefowicz et al., 2016;</ref><ref type="bibr" target="#b67">Mahajan et al., 2018;</ref><ref type="bibr" target="#b82">Radford et al., 2019;</ref><ref type="bibr">Shazeer et al., 2018</ref><ref type="bibr" target="#b96">Shazeer et al., , 2017;;</ref><ref type="bibr" target="#b38">Huang et al., 2018b]</ref>. This synergy has resulted in a great deal of recent work developing transfer learning methodology for NLP, which has produced a wide landscape of pre-training objectives <ref type="bibr" target="#b36">[Howard and Ruder, 2018;</ref><ref type="bibr" target="#b23">Devlin et al., 2018;</ref><ref type="bibr" target="#b64">Yang et al., 2019;</ref><ref type="bibr" target="#b25">Dong et al., 2019]</ref>, unlabeled datasets <ref type="bibr" target="#b64">[Yang et al., 2019;</ref><ref type="bibr" target="#b65">Liu et al., 2019c;</ref><ref type="bibr" target="#b120">Zellers et al., 2019]</ref>, benchmarks <ref type="bibr" target="#b111">[Wang et al., 2019b</ref><ref type="bibr" target="#b109">[Wang et al., , 2018;;</ref><ref type="bibr" target="#b17">Conneau and Kiela, 2018]</ref>, fine-tuning methods <ref type="bibr" target="#b36">[Howard and Ruder, 2018;</ref><ref type="bibr" target="#b35">Houlsby et al., 2019;</ref><ref type="bibr" target="#b76">Peters et al., 2019]</ref>, and more. The rapid rate of progress and diversity of techniques in this burgeoning field can make it difficult to compare different algorithms, tease apart the effects of new contributions, and understand the space of existing methods for transfer learning. Motivated by a need for more rigorous understanding, we present a unified approach to transfer learning that allows us to systematically study different approaches and push the current limits of the field.</p><p>The basic idea underlying our work is to treat every NLP problem as a "text-to-text" problem, i.e. taking text as input and producing new text as output. Similar approaches were used as part of the Natural Language Decathlon <ref type="bibr" target="#b68">[McCann et al., 2018]</ref> and to test the zero-shot learning capabilities of language models <ref type="bibr" target="#b82">[Radford et al., 2019]</ref>. Crucially, our text-to-text framework allows us to directly apply the same model, objective, training procedure, and decoding process to every task we consider. We leverage this flexibility by evaluating performance on a wide variety of English-based NLP problems, including question answering, document summarization, and sentiment classification, to name a few. With this unified approach, we can compare the effectiveness of different transfer learning objectives, unlabeled datasets, and other factors, while exploring the limits of transfer learning for NLP by scaling up models and datasets beyond what has previously been considered. By combining our insights and scale, we obtain state-of-the-art results in many of the tasks we consider.</p><p>We emphasize that our main goal is not to propose new methods but instead to provide a comprehensive perspective on where the field stands. As such, the bulk of our work comprises of a survey, exploration, and empirical comparison of existing techniques. Our approach of casting every problem as a text-to-text task constitutes an additional major contribution. This unifying framework differs from current practice and boasts both simplicity and strong performance. Finally, we also push the field forward by training larger models than have been previously considered (up to 11 billion parameters). In order to perform experiments at this scale, we introduce the "Colossal Clean Crawled Corpus" (C4), a dataset consisting of hundreds of gigabytes of clean English text scraped from the web. Recognizing that the main utility of transfer learning is the possibility of leveraging pre-trained models in data-scarce settings, we also contribute our code, datasets, and pre-trained models. 1  The remainder of the paper is structured as follows: In the following section, we discuss our base model and its implementation, our procedure for formulating every problem as a text-to-text task, and the suite of tasks we consider. In Section 3, we present a large set of experiments that explore the field of transfer learning for NLP. At the end of the section (Section 3.7), we combine insights from our systematic study to obtain state-of-the-art results on a wide variety of benchmarks. Finally, we provide a summary of our results and wrap up with a look towards the future in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Setup</head><p>Before presenting the results from our large-scale empirical study, we review the necessary background topics required to understand our results, including the Transformer model architecture and the downstream tasks we evaluate on. We also introduce our approach for treating every problem as a text-to-text task and describe our "Colossal Clean Crawled Corpus" (C4), the Common Crawl-based dataset we created as a source of unlabeled text data. We refer to our model and framework as the "Text-to-Text Transfer Transformer" (T5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model</head><p>Early results on transfer learning for NLP leveraged recurrent neural networks <ref type="bibr" target="#b77">[Peters et al., 2018;</ref><ref type="bibr" target="#b36">Howard and Ruder, 2018]</ref>, but it has recently become more common to use models based on the "Transformer" architecture <ref type="bibr" target="#b108">[Vaswani et al., 2017]</ref>. The Transformer was initially shown to be effective for machine translation, but it has subsequently been used in a wide variety of NLP settings <ref type="bibr" target="#b81">[Radford et al., 2018;</ref><ref type="bibr" target="#b23">Devlin et al., 2018;</ref><ref type="bibr" target="#b68">McCann et al., 2018;</ref><ref type="bibr" target="#b119">Yu et al., 2018]</ref>. Due to its improved performance and increasing ubiquity, all of the models we study are based on the Transformer architecture. Apart from the details mentioned below and the variants we explore in Section 3.2, we do not deviate significantly from this architecture as originally proposed. Instead of providing a comprehensive definition of this model, we refer the interested reader to the original paper <ref type="bibr" target="#b108">[Vaswani et al., 2017]</ref> or follow-up tutorials 3,4 for a more detailed introduction.</p><p>The primary building block of the Transformer is self-attention <ref type="bibr" target="#b15">[Cheng et al., 2016]</ref>. Self-attention is a variant of attention <ref type="bibr" target="#b27">[Graves, 2013;</ref><ref type="bibr" target="#b6">Bahdanau et al., 2015]</ref> that processes a sequence by replacing each element by a weighted average of the rest of the sequence. The original Transformer consisted of an encoder-decoder architecture and was intended for sequence-to-sequence <ref type="bibr" target="#b103">[Sutskever et al., 2014;</ref><ref type="bibr" target="#b46">Kalchbrenner et al., 2014]</ref> tasks. It has recently also become common to use models consisting of a single Transformer layer stack, with varying forms of self-attention used to produce architectures appropriate for language modeling <ref type="bibr" target="#b81">[Radford et al., 2018;</ref><ref type="bibr" target="#b0">Al-Rfou et al., 2019]</ref> or classification and span prediction tasks <ref type="bibr" target="#b23">[Devlin et al., 2018;</ref><ref type="bibr" target="#b64">Yang et al., 2019]</ref>. We empirically explore these architectural variants in Section 3.2.</p><p>Overall, our encoder-decoder Transformer implementation closely follows its originally-proposed form <ref type="bibr" target="#b108">[Vaswani et al., 2017]</ref>. First, an input sequence of tokens is mapped to a sequence of embeddings, which is then passed into the encoder. The encoder consists of a stack of "blocks", each of which comprises two subcomponents: a self-attention layer followed by a small feed-forward network. Layer normalization <ref type="bibr" target="#b4">[Ba et al., 2016]</ref> is applied to the input of each subcomponent and a residual skip connection <ref type="bibr" target="#b28">[He et al., 2016]</ref> adds each subcomponent's input to its output. Dropout <ref type="bibr" target="#b101">[Srivastava et al., 2014]</ref> is applied within the feed-forward network, on the skip connection, on the attention weights, and at the input and output of the entire stack. The decoder is similar in structure to the encoder except that it includes a standard attention mechanism after each self-attention layer that attends to the output of the encoder. The self-attention mechanism in the decoder also uses a form of autoregressive or causal self-attention, which only allows the model to attend to past outputs. The output of the final decoder block is fed into a dense layer with a softmax output, whose weights are shared with the input embedding matrix. All attention mechanisms in the Transformer are split up into independent "heads" whose outputs are concatenated before being further processed.</p><p>Since self-attention is order-independent (i.e. it is an operation on sets), it is common to provide an explicit position signal to the Transformer. While the original Transformer used a sinusoidal position signal or learned position embeddings, it has recently become more common to use relative position embeddings <ref type="bibr" target="#b94">[Shaw et al., 2018;</ref><ref type="bibr" target="#b37">Huang et al., 2018a]</ref>. Instead of using a fixed embedding for each position, relative position embeddings produce a different learned embedding according to the offset between the "key" and "query" being compared in the self-attention mechanism. We use a simplified form of position embeddings where each "embedding" is simply a scalar that is added to the corresponding logit used for computing the attention weights. For efficiency, we also share the position embedding parameters across all layers in our model, though within a given layer each attention head uses a different learned position embedding. Typically, a fixed number of embeddings are learned, each corresponding to a range of possible key-query offsets. In this work, we use 32 embeddings for all of our models with ranges that increase in size logarithmically up to an offset of 128 beyond which we assign all relative positions to the same embedding. Note that a given layer is insensitive to relative position beyond 128 tokens, but subsequent layers can build a sensitivity to larger offsets by combining local information from previous layers.</p><p>As part of our study, we experiment with the scalability of these models, i.e. how their performance changes as they are made to have more parameters or layers. Training large models can be non-trivial since they might not fit on a single machine and require a great deal of computation. As a result, we use a combination of model and data parallelism and train models on "slices" of Cloud TPU Pods.<ref type="foot" target="#foot_3">5</ref> TPU pods are are multi-rack ML supercomputers that contain 1,024 TPU v3 chips connected via a high-speed 2D mesh interconnect with supporting CPU host machines. We leverage the Mesh TensorFlow library <ref type="bibr">[Shazeer et al., 2018]</ref> for ease of implementation of both model parallelism and data parallelism <ref type="bibr" target="#b53">[Krizhevsky, 2014]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Colossal Clean Crawled Corpus</head><p>Much of the previous work on transfer learning for NLP makes use of large unlabeled datasets for unsupervised learning. In this paper, we are interested in measuring the effect of the quality, characteristics, and size of this unlabeled data. To generate datasets that satisfy our needs, we leverage Common Crawl as a source of text scraped from the web. Common Crawl has previously been used as a source of text data for NLP, for example to train an n-gram language model <ref type="bibr" target="#b12">[Buck et al., 2014]</ref>, for commonsense reasoning <ref type="bibr" target="#b106">[Trinh and Le, 2018]</ref>, for mining parallel texts for machine translation <ref type="bibr" target="#b98">[Smith et al., 2013]</ref>, as a pre-training dataset <ref type="bibr" target="#b5">[Baevski et al., 2019;</ref><ref type="bibr" target="#b120">Zellers et al., 2019;</ref><ref type="bibr" target="#b65">Liu et al., 2019c]</ref>, and even simply as a giant text corpus for testing optimizers <ref type="bibr" target="#b1">[Anil et al., 2019]</ref>.</p><p>Common Crawl is a publicly-available web archive that provides "web extracted text" by removing markup and other non-text content from the scraped HTML files. This process produces around 20TB of scraped text data each month. Unfortunately, the majority of the resulting text is not natural language. Instead, it largely comprises gibberish or boiler-plate text like menus, error messages, or duplicate text. Furthermore, a good deal of the scraped text contains content that is unlikely to be helpful for any of the tasks we consider (offensive language, placeholder text, source code, etc.). To address these issues, we used the following heuristics for cleaning up Common Crawl's web extracted text:</p><p>? We only retained lines that ended in a terminal punctuation mark (i.e. a period, exclamation mark, question mark, or end quotation mark).</p><p>? We removed any page that contained any word on the "List of Dirty, Naughty, Obscene or Otherwise Bad Words". ? Many of the scraped pages contained warnings stating that Javascript should be enabled so we removed any line with the word Javascript.</p><p>? Some pages had placeholder "lorem ipsum" text; we removed any page where the phrase "lorem ipsum" appeared.</p><p>? Some pages inadvertently contained code. Since the curly bracket "{" appears in many programming languages (such as Javascript, widely used on the web) but not in natural text, we removed any pages that contained a curly bracket.</p><p>? To deduplicate the dataset, we discarded all but one of any three-sentence span occurring more than once in the dataset.</p><p>Additionally, since most of our downstream tasks are focused on English-language text, we used langdetect<ref type="foot" target="#foot_5">7</ref> to filter out any pages that were not classified as English with a probability of at least 0.99. To assemble our base dataset, we downloaded the web extracted text from April 2019 and applied the aforementioned filtering. This produces a collection of text that is not only orders of magnitude larger than most datasets used for pre-training (about 750 GB) but also comprises reasonably clean and natural English text. We dub this dataset the "Colossal Clean Crawled Corpus" (or C4 for short) and release it as part of TensorFlow Datasets. <ref type="foot" target="#foot_6">8</ref> We consider the impact of using various alternative versions of this dataset in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Downstream tasks</head><p>Our goal in this paper is to measure general language learning abilities. As such, we study downstream performance on a diverse set of benchmarks, including machine translation, question answering, abstractive summarization, and text classification. Specifically, we measure performance on the GLUE and SuperGLUE text classification meta-benchmarks; CNN/Daily Mail abstractive summarization; SQuAD question answering; and WMT English to German, French, and Romanian translation. All data was sourced from TensorFlow Datasets.<ref type="foot" target="#foot_7">9</ref> </p><p>GLUE <ref type="bibr" target="#b109">[Wang et al., 2018]</ref> and SuperGLUE <ref type="bibr" target="#b111">[Wang et al., 2019b]</ref> each comprise a collection of text classification tasks meant to test general language understanding abilities:</p><p>? Sentence acceptability judgment (CoLA <ref type="bibr" target="#b113">[Warstadt et al., 2018]</ref>)</p><p>? Sentiment analysis (SST-2 <ref type="bibr" target="#b99">[Socher et al., 2013]</ref>)</p><p>? Paraphrasing/sentence similarity (MRPC <ref type="bibr" target="#b24">[Dolan and Brockett, 2005]</ref>, STS-B <ref type="bibr" target="#b14">[Cer et al., 2017]</ref>,</p><p>QQP <ref type="bibr" target="#b40">[Iyer et al., 2017]</ref>)</p><p>? Natural language inference (MNLI <ref type="bibr" target="#b114">[Williams et al., 2017]</ref>, QNLI <ref type="bibr" target="#b84">[Rajpurkar et al., 2016]</ref>, RTE <ref type="bibr" target="#b19">[Dagan et al., 2005]</ref>, CB <ref type="bibr" target="#b21">[De Marneff et al., 2019]</ref>)</p><p>? Coreference resolution (WNLI and WSC <ref type="bibr" target="#b58">[Levesque et al., 2012]</ref>)</p><p>? Sentence completion (COPA <ref type="bibr" target="#b86">[Roemmele et al., 2011]</ref>)</p><p>? Word sense disambiguation <ref type="bibr">(WIC [Pilehvar and Camacho-Collados, 2018]</ref>)</p><p>? Question answering (MultiRC <ref type="bibr" target="#b47">[Khashabi et al., 2018]</ref>, ReCoRD <ref type="bibr" target="#b121">[Zhang et al., 2018]</ref>, BoolQ <ref type="bibr" target="#b16">[Clark et al., 2019]</ref>)</p><p>We use the datasets as distributed by the GLUE and SuperGLUE benchmarks. For simplicity, when fine-tuning we treat all of the tasks in the GLUE benchmark (and similarly for SuperGLUE) as a single task by concatenating all of the constituent datasets. As suggested by <ref type="bibr" target="#b49">Kocijan et al. [2019]</ref> we also include the Definite Pronoun Resolution (DPR) dataset <ref type="bibr" target="#b83">[Rahman and Ng, 2012]</ref> in the combined SuperGLUE task.</p><p>The <ref type="bibr">CNN/Daily Mail [Hermann et al., 2015]</ref> dataset was introduced as a question-answering task but was adapted for text summarization by <ref type="bibr" target="#b71">Nallapati et al. [2016]</ref>; we use the non-anonymized version from <ref type="bibr" target="#b91">See et al. [2017]</ref> as an abstractive summarization task. SQuAD <ref type="bibr" target="#b84">[Rajpurkar et al., 2016]</ref> is a common question-answering benchmark. In our experiments, the model is fed the question and its context and asked to generate the answer token-by-token. For WMT English to German, we use the same training data as <ref type="bibr" target="#b108">[Vaswani et al., 2017]</ref> (i.e. News Commentary v13, Common Crawl, Europarl v7) and newstest2013 as a validation set <ref type="bibr" target="#b8">[Bojar et al., 2014]</ref>. For English to French, we use the standard training data from 2015 and newstest2014 as a validation set <ref type="bibr" target="#b9">[Bojar et al., 2015]</ref>. For English to Romanian, which is a standard lower-resource machine translation benchmark, we use the train and validation sets from WMT 2016 <ref type="bibr" target="#b10">[Bojar et al., 2016]</ref>. Note that we only pre-train on English data, so in order to learn to translate a given model will need to learn to generate text in a new language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Input and output format</head><p>In order to train a single model on the diverse set of tasks described above, we need a consistent input and output format across all tasks. As recently noted by <ref type="bibr" target="#b68">[McCann et al., 2018;</ref><ref type="bibr" target="#b82">Radford et al., 2019]</ref>, it is possible to formulate most NLP tasks in a "text-to-text" format -that is, a task where the model is fed some text for context or conditioning and is then asked to produce some output text. This framework provides a consistent training objective both for pre-training and fine-tuning. Specifically, the model is trained with a maximum likelihood objective (using "teacher forcing" <ref type="bibr" target="#b115">[Williams and Zipser, 1989]</ref>) regardless of the task. To specify which task the model should perform, we add a task-specific (text) prefix to the original input sequence before feeding it to the model. As an example, to ask the model to translate the sentence "That is good." from English to German, the model would be fed the sequence "translate English to German: That is good." and would be trained to output "Das ist gut." For text classification tasks, the model simply predicts a single word corresponding to the target label. For example, on the MNLI benchmark <ref type="bibr" target="#b114">[Williams et al., 2017]</ref> the goal is to predict whether a premise implies ("entailment"), contradicts ("contradiction"), or neither ("neutral") a hypothesis. With our preprocessing, the input sequence becomes "mnli premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity." with the corresponding target word "entailment". Note that an issue arises if our model outputs text on a text classification task that does not correspond to any of the possible labels (for example if the model outputs "hamburger" when the only possible labels for a task were "entailment", "neutral", or "contradiction"). In this case, we always count the model's output as wrong, though we never observed this behavior in any of our trained models. A diagram of our text-to-text framework with a few input/output examples is shown in Figure <ref type="figure">1</ref>.</p><p>Following this approach allows us to straightforwardly use a text-to-text format for every task except STS-B, which is a regression task where the goal is to predict a similarity score between 1 and 5. We found that most of these scores were annotated in increments of 0.2, so we simply rounded any score to the nearest increment of 0.2 and converted the result to a literal string representation of the number (e.g. the floating-point value 2.57 would be mapped to the string "2.6"). At test time, if the model outputs a string corresponding to a number between 1 and 5, we convert it to a floating-point value; otherwise, we treat the model's prediction as incorrect. This effectively recasts the STS-B regression problem as a 21-class classification problem.</p><p>Separately, we also convert the Winograd tasks (WNLI from GLUE, WSC from SuperGLUE, and the DPR dataset we add to SuperGLUE) into a simpler format that is more amenable to the text-to-text framework. Examples from the Winograd tasks consist of a text passage containing an ambiguous pronoun that could refer to more than one of the noun phrases in the passage. For example, the passage might be "The city councilmen refused the demonstrators a permit because they feared violence.", which contains the ambiguous pronoun "they" that could refer to "city councilmen" or "demonstrators". We cast the WNLI, WSC, and DPR tasks as text-to-text problems by highlighting the ambiguous pronoun in the text passage and asking the model to predict the noun that it refers to. The example mentioned above would be transformed to the input "The city councilmen refused the demonstrators a permit because *they* feared violence." and the model would be trained to predict the target text "The city councilmen".</p><p>For WSC, examples contain the passage, the ambiguous pronoun, a candidate noun, and a True/False label reflecting whether the candidate matches the pronoun (ignoring any articles). We only train on examples with a "True" label since we do not know the correct noun targets for examples with a "False" label. For evaluation, we assign a "True" label if the words in the model's output are a subset of the words in the candidate noun phrase (or vice versa) and assign a "False" label otherwise. This removes roughly half of the WSC training set, but the DPR dataset adds about 1,000 pronoun resolution examples. Examples from DPR are annotated with the correct referent noun, making it easy to use this dataset in the format listed above.</p><p>The WNLI training and validation sets have a significant overlap with the WSC training set. To avoid leaking validation examples into our training data (a particular issue in the multi-task experiments of Section 3.5.2), we therefore never train on WNLI and never report results on the WNLI validation set. Omitting results on the WNLI validation set is standard practice <ref type="bibr" target="#b23">[Devlin et al., 2018]</ref> due to the fact that it is "adversarial" with respect to the training set, i.e. validation examples are all slightly-perturbed versions of training examples with the opposite label. As such, we do not include WNLI in the average GLUE score whenever we report on the validation set (all sections except Section 3.7 where results are presented on the test sets). Converting examples from WNLI to the "referent noun prediction" variant described above is a little more involved; we describe this process in Appendix B.</p><p>We provide full examples of preprocessed inputs for every task we studied in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Recent advances in transfer learning for NLP have come from a wide variety of developments, such as new pre-training objectives, model architectures, unlabeled datasets, and more. In this section, we carry out an empirical survey of these techniques in hopes of teasing apart their contribution and significance. We then combine the insights gained to attain state-of-the-art in many of the tasks we consider. Since transfer learning for NLP is a rapidly growing area of research, it is not feasible for us to cover every possible technique or idea in our empirical study. For a broader literature review, we recommend a recent survey by <ref type="bibr" target="#b88">Ruder et al. [2019]</ref>.</p><p>We systematically study these contributions by taking a reasonable baseline (described in Section 3.1) and altering one aspect of the setup at a time. For example, in Section 3.3 we measure the performance of different unsupervised objectives while keeping the rest of our experimental pipeline fixed. This "coordinate descent" approach might miss second-order effects (for example, some particular unsupervised objective may work best on a model larger than our baseline setting), but performing a combinatorial exploration of all of the factors in our study would be prohibitively expensive. In future work, we expect it could be fruitful to more thoroughly consider combinations of the approaches we study.</p><p>Our goal is to compare a variety of different approaches on a diverse set of tasks while keeping as many factors fixed as possible. In order to satisfy this aim, in some cases we do not exactly replicate existing approaches. For example, "encoder-only" models like BERT <ref type="bibr" target="#b23">[Devlin et al., 2018]</ref> are designed to produce a single prediction per input token or a single prediction for an entire input sequence. This makes them applicable for classification or span prediction tasks but not for generative tasks like translation or abstractive summarization. As such, none of the model architectures we consider are identical to BERT or consist of an encoder-only structure. Instead, we test approaches that are similar in spirit -for example, we consider an analogous objective to BERT's "masked language modeling" objective in Section 3.3 and we consider a model architecture that behaves similarly to BERT on text classification tasks in Section 3.2.</p><p>After outlining our baseline experimental setup in the following subsection, we undertake an empirical comparison of model architectures (Section 3.2), unsupervised objectives (Section 3.3), pre-training datasets (Section 3.4), transfer approaches (Section 3.5), and scaling (Section 3.6). At the culmination of this section, we combine insights from our study with scale to obtain state-of-the-art results in many tasks we consider (Section 3.7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline</head><p>Our goal for our baseline is to reflect typical, modern practice. We pre-train a standard Transformer (described in Section 2.1) using a simple denoising objective and then separately fine-tune on each of our downstream tasks. We describe the details of this experimental setup in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Model</head><p>For our model, we use a standard encoder-decoder Transformer as proposed by <ref type="bibr" target="#b108">Vaswani et al. [2017]</ref>. While many modern approaches to transfer learning for NLP use a Transformer architecture consisting of only a single "stack" (e.g. for language modeling <ref type="bibr" target="#b81">[Radford et al., 2018;</ref><ref type="bibr" target="#b25">Dong et al., 2019]</ref> or classification and span prediction <ref type="bibr" target="#b23">[Devlin et al., 2018;</ref><ref type="bibr" target="#b64">Yang et al., 2019]</ref>), we found that using a standard encoder-decoder structure achieved good results on both generative and classification tasks. We explore the performance of different model architectures in Section 3.2.</p><p>Our baseline model is designed so that the encoder and decoder are each similar in size and configuration to a "BERT BASE " <ref type="bibr" target="#b23">[Devlin et al., 2018]</ref> stack. Specifically, both the encoder and decoder consist of 12 blocks (each block comprising self-attention, optional encoder-decoder attention, and a feed-forward network). The feed-forward networks in each block consist of a dense layer with an output dimensionality of d ff = 3072 followed by a ReLU nonlinearity and another dense layer. The "key" and "value" matrices of all attention mechanisms have an inner dimensionality of d kv = 64 and all attention mechanisms have 12 heads. All other sub-layers and embeddings have a dimensionality of d model = 768. In total, this results in a model with about 220 million parameters. This is roughly twice the number of parameters of BERT BASE since our baseline model contains two layer stacks instead of one. For regularization, we use a dropout probability of 0.1 everywhere dropout is applied in the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Training</head><p>As described in Section 2.4, all tasks are formulated as text-to-text tasks. As a result, we always train using standard maximum likelihood, i.e. using teacher forcing <ref type="bibr" target="#b115">[Williams and Zipser, 1989</ref>] and a cross-entropy loss. For optimization, we use AdaFactor <ref type="bibr" target="#b95">[Shazeer and Stern, 2018]</ref>. At test time, we use greedy decoding (i.e. choosing the highest-probability logit at every timestep).</p><p>We pre-train each model for 2 19 = 524,288 steps on C4 before fine-tuning. We use a maximum sequence length of 512 and a batch size of 128 sequences. Whenever possible, we "pack" multiple sequences into each entry of the batch<ref type="foot" target="#foot_8">10</ref> so that our batches contain roughly 2 16 = 65,536 tokens. In total, this batch size and number of steps corresponds to pre-training on 2 35 ? 34B tokens. This is considerably less than BERT <ref type="bibr" target="#b23">[Devlin et al., 2018]</ref>, which used roughly 137B tokens, or RoBERTa <ref type="bibr" target="#b65">[Liu et al., 2019c]</ref>, which used roughly 2.2T tokens. Using only 2 35 tokens results in a reasonable computational budget while still providing a sufficient amount of pre-training for acceptable performance. We consider the effect of pre-training for more steps in Sections 3.6 and 3.7. Note that 2 35 tokens only covers a fraction of the entire C4 dataset, so we never repeat any data during pre-training.</p><p>During pre-training, we use an "inverse square root" learning rate schedule: 1 max(n, k) where n is the current training iteration and k is the number of warm-up steps (set to 10 4 in all of our experiments). This sets a constant learning rate of 0.01 for the first 10 4 steps, then exponentially decays the learning rate until pre-training is over. We also experimented with using a triangular learning rate <ref type="bibr" target="#b36">[Howard and Ruder, 2018]</ref>, which produced slightly better results but requires knowing the total number of training steps ahead of time. Since we will be varying the number of training steps in some of our experiments, we opt for the more generic inverse square root schedule.</p><p>Our models are fine-tuned for 2 18 = 262,144 steps on all tasks. This value was chosen as a trade-off between the high-resource tasks (i.e. those with large datasets), which benefit from additional fine-tuning, and low-resource tasks (smaller datasets), which overfit quickly. During fine-tuning, we continue using batches with 128 length-512 sequences (i.e. 2 16 tokens per batch). We use a constant learning rate of 0.001 when fine-tuning. We save a checkpoint every 5,000 steps and report results on the model checkpoint corresponding to the highest validation performance. For models fine-tuned on multiple tasks, we choose the best checkpoint for each task independently. For all of the experiments except those in Section 3.7, we report results in the validation set to avoid performing model selection on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Vocabulary</head><p>We use SentencePiece <ref type="bibr" target="#b55">[Kudo and Richardson, 2018]</ref> to encode text as WordPiece tokens <ref type="bibr" target="#b92">[Sennrich et al., 2015;</ref><ref type="bibr" target="#b54">Kudo, 2018]</ref>. For all experiments, we use a vocabulary of 32,000 wordpieces. Since we ultimately fine-tune our model on English to German, French, and Romanian translation, we also require that our vocabulary covers these non-English languages. To address this, we classified pages from the Common Crawl scrape used in C4 as German, French, and Romanian. Then, we trained our SentencePiece model on a mixture of 10 parts of English C4 data with 1 part each of data classified as German, French or Romanian. This vocabulary was shared across both the input and output of our model. Note that our vocabulary makes it so that our model can only process a predetermined, fixed set of languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Unsupervised objective</head><p>Leveraging unlabeled data to pre-train our model necessitates an objective that does not require labels but (loosely speaking) teaches the model generalizable knowledge that will be useful in downstream tasks. Early work on transfer learning for NLP used a language modeling objective <ref type="bibr">[Peters et al.</ref>, In this example, we process the sentence "Thank you for inviting me to your party last week." The words "for", "inviting" and "last" (marked with an ?) are randomly chosen for corruption. Each consecutive span of corrupted tokens is replaced by a sentinel token (shown as &lt;X&gt; and &lt;Y&gt;) that is unique over the example. Since "for" and "inviting" occur consecutively, they are replaced by a single sentinel &lt;X&gt;. The output sequence then consists of the dropped-out spans, delimited by the sentinel tokens used to replace them in the input plus a final sentinel token &lt;Z&gt;.</p><p>2018; <ref type="bibr" target="#b81">Radford et al., 2018;</ref><ref type="bibr" target="#b36">Howard and Ruder, 2018]</ref>. However, it has recently been shown that "denoising" objectives <ref type="bibr" target="#b23">[Devlin et al., 2018;</ref><ref type="bibr" target="#b64">Yang et al., 2019;</ref><ref type="bibr" target="#b105">Taylor, 1953]</ref> produce better performance, and as a result they have quickly become standard. In a denoising objective, the model is trained to predict missing or otherwise corrupted tokens in the input. Inspired by BERT's "masked language modeling" objective and the "word dropout" regularization technique <ref type="bibr" target="#b11">[Bowman et al., 2015]</ref>, we design an objective that randomly samples and then drops out 15% of tokens in the input sequence.</p><p>All consecutive spans of dropped-out tokens are replaced by a single sentinel token. Each sentinel token is assigned a token ID that is unique to the sequence. The target then corresponds to all of the dropped-out spans of tokens, delimited by the same sentinel tokens used in the input sequence plus a final sentinel token to mark the end of the target sequence. An example of the transformation resulting from applying this objective is shown in Figure <ref type="figure" target="#fig_0">2</ref>. We empirically compare this objective to many other variants in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5">Baseline performance</head><p>In this section, we present results using the baseline experimental procedure described above to get a sense of what kind of performance to expect on our suite of downstream tasks. Ideally, we would repeat every experiment in our study multiple times to get a confidence interval on our results. Unfortunately, this would be prohibitively expensive due to the large number of experiments we run. As a cheaper alternative, we train our baseline model 10 times from scratch (i.e. with different random initializations and dataset shuffling) and assume that the variance over these runs of the base model also applies to each experimental variant. We don't expect most of the changes we make to have a dramatic effect on the inter-run variance, so this should provide a reasonable indication of the significance of different changes. Separately, we also measure the performance of training our model for 2 18 steps (the same number we use for fine-tuning) on all downstream tasks without pre-training. This gives us an idea of how much pre-training benefits our model in the baseline setting.</p><p>When reporting results in the main text, we only report a subset of the scores across all the benchmarks to conserve space and ease interpretation. For GLUE and SuperGLUE, we report the average score across all subtasks (as stipulated by the official benchmarks) under the headings "GLUE" and "SGLUE". For all translation tasks, we report the BLEU score <ref type="bibr" target="#b73">[Papineni et al., 2002]</ref> as provided by SacreBLEU v1.3.0 <ref type="bibr" target="#b80">[Post, 2018]</ref> with "exp" smoothing and "intl" tokenization. We refer to scores for WMT English to German, English to French, and English to Romanian as EnDe, EnFr, and EnRo, respectively. For CNN/Daily Mail, we find the performance of models on the ROUGE-1-F, ROUGE-2-F, and ROUGE-L-F metrics <ref type="bibr" target="#b59">[Lin, 2004]</ref> to be highly correlated so we report the ROUGE-2-F score alone under the heading "CNNDM". Similarly, for SQuAD we find the performance of the "exact match" and "F1" scores to be highly correlated so we report the "exact match" score alone. We provide every score achieved on every task for all experiments in Table <ref type="table" target="#tab_6">15</ref>, Appendix E.</p><p>Our results tables are all formatted so that each row corresponds to a particular experimental configuration with columns giving the scores for each benchmark. We will include the mean performance of the baseline configuration in most tables. Wherever a baseline configuration appears, we will mark it with a (as in the first row of Table <ref type="table">1</ref>). We also will boldface any score that is within two standard deviations of the maximum (best) in a given experiment.</p><p>Our baseline results are shown in Table <ref type="table">1</ref>. Overall, our results are comparable to existing models of similar size. For example, BERT BASE achieved an exact match score of 80.8 on SQuAD and an accuracy of 84.4 on MNLI-matched, whereas we achieve 80.88 and 84.24, respectively (see Table <ref type="table" target="#tab_6">15</ref>). Note that we cannot directly compare our baseline to BERT BASE because ours is an encoder-decoder model and was pre-trained for roughly 1 ?4 as many steps. Unsurprisingly, we find that pre-training provides significant gains across almost all benchmarks. The only exception is WMT English to French, which is a large enough dataset that gains from pre-training tend to be marginal. We include this task in our experiments to test the behavior of transfer learning in the high-resource regime.</p><p>As for inter-run variance, we find that for most tasks the standard deviation across runs is smaller than 1% of the task's baseline score. Exceptions to this rule include CoLA, CB, and COPA, which are all low-resource tasks from the GLUE and SuperGLUE benchmarks. For example, on CB our baseline model had an average F1 score of 91.22 with a standard deviation of 3.237 (see Table <ref type="table" target="#tab_6">15</ref>), which may be partly due to the fact that CB's validation set contains only 56 examples. Note that the GLUE and SuperGLUE scores are computed as the average of scores across the tasks comprising each benchmark. As a result, we caution that the high inter-run variance of CoLA, CB, and COPA can make it harder to compare models using the GLUE and SuperGLUE scores alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Architectures</head><p>While the Transformer was originally introduced with an encoder-decoder architecture, much modern work on transfer learning for NLP uses alternative architectures. In this section, we review and compare these architectural variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Model structures</head><p>Attention masks A major distinguishing factor for different architectures is the "mask" used by different attention mechanisms in the model. Recall that the self-attention operation in a Transformer takes a sequence as input and outputs a new sequence of the same length. Each entry of the output sequence is produced by computing a weighted average of entries of the input sequence. Specifically, let y i refer to the ith element of the input sequence and x j refer to the jth entry of the input sequence. y i is computed as j w i,j x j , where w i,j is the scalar weight produced by the self-attention mechanism The input and output of the self-attention mechanism are denoted x and y respectively. A dark cell at row i and column j indicates that the self-attention mechanism is allowed to attend to input element j at output timestep i. A light cell indicates that the self-attention mechanism is not allowed to attend to the corresponding i and j combination. Left: A fully-visible mask allows the self-attention mechanism to attend to the full input at every output timestep. Middle: A causal mask prevents the ith output element from depending on any input elements from "the future". Right: Causal masking with a prefix allows the self-attention mechanism to use fully-visible masking on a portion of the input sequence. as a function of x i and x j . The attention mask is then used to zero out certain weights in order to constrain which entries of the input can be attended to at a given output timestep. Diagrams of the masks we will consider are shown in Figure <ref type="figure" target="#fig_1">3</ref>. For example, the causal mask (Figure <ref type="figure" target="#fig_1">3</ref>, middle) sets any w i,j to zero if j &gt; i.</p><p>Encoder-decoder An encoder-decoder Transformer consists of two layer stacks: The encoder, which is fed an input sequence, and the decoder, which produces a new output sequence. A schematic of this architectural variant is shown in the left panel of Figure <ref type="figure" target="#fig_2">4</ref>.</p><p>The encoder uses a "fully-visible" attention mask. Fully-visible masking allows a self-attention mechanism to attend to any entry of the input when producing each entry of its output. We visualize this masking pattern in Figure <ref type="figure" target="#fig_1">3</ref>, left. This form of masking is appropriate when attending over a "prefix", i.e. some context provided to the model that is later used when making predictions. BERT <ref type="bibr" target="#b23">[Devlin et al., 2018</ref>] also uses a fully-visible masking pattern and appends a special "classification" token to the input. BERT's output at the timestep corresponding to the classification token is then used to make a prediction for classifying the input sequence.</p><p>The self-attention operations in the Transformer's decoder use a "causal" masking pattern. When producing the ith entry of the output sequence, causal masking prevents the model from attending to the jth entry of the input sequence for j &gt; i. This is used during training so that the model can't "see into the future" as it produces its output. An attention matrix for this masking pattern is shown in Figure <ref type="figure" target="#fig_1">3</ref>, middle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language model</head><p>The decoder in a Transformer is used to autoregressively produce an output sequence. That is, at each output timestep, a token is sampled from the model's predicted distribution and the sample is fed back into the model to produce a prediction for the next output timestep, and so on. As such, a Transformer decoder (without an encoder) can be used as a language model (LM), i.e. a model trained solely for next-step prediction <ref type="bibr">[Liu et al., 2018;</ref><ref type="bibr" target="#b81">Radford et al., 2018;</ref><ref type="bibr" target="#b0">Al-Rfou et al., 2019]</ref>. A schematic of this architecture is shown in Figure <ref type="figure" target="#fig_2">4</ref>, middle. In fact, early work on transfer learning for NLP used this architecture with a language modeling objective as a pre-training method <ref type="bibr" target="#b81">[Radford et al., 2018]</ref>.</p><p>x 1 x 2 x 3 x 4 y 1 y 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder Decoder</head><p>x 1 x 2 x 3 y 1 y 2</p><p>x 2 x 3 y 1 y 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language model</head><p>x 1 x 2 x 3 y 1 y 2</p><p>x 2 x 3 y 1 y 2 . Dark grey lines correspond to fully-visible masking and light grey lines correspond to causal masking. We use "." to denote a special end-of-sequence token that represents the end of a prediction. The input and output sequences are represented as x and y respectively. Left: A standard encoder-decoder architecture uses fully-visible masking in the encoder and the encoder-decoder attention, with causal masking in the decoder. Middle: A language model consists of a single Transformer layer stack and is fed the concatenation of the input and target, using a causal mask throughout. Right: Adding a prefix to a language model corresponds to allowing fully-visible masking over the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prefix LM</head><p>Language models are typically used for compression or sequence generation <ref type="bibr" target="#b27">[Graves, 2013]</ref>. However, they can also be used in the text-to-text framework simply by concatenating the inputs and targets. As an example, consider the case of English to German translation: If we have a training datapoint with input sentence "That is good." and target "Das ist gut.", we would simply train the model on next-step prediction over the concatenated input sequence "translate English to German: That is good. target: Das ist gut." If we wanted to obtain the model's prediction for this example, the model would be fed the prefix "translate English to German: That is good. target:" and would be asked to generate the remainder of the sequence autoregressively. In this way, the model can predict an output sequence given an input, which satisfies the needs of text-to-text tasks. This approach was recently used to show that language models can learn to perform some text-to-text tasks without supervision <ref type="bibr" target="#b82">[Radford et al., 2019]</ref>.</p><p>Prefix LM A fundamental and frequently cited drawback of using a language model in the textto-text setting is that causal masking forces the model's representation of the ith entry of the input sequence to only depend on the entries up until i. To see why this is potentially disadvantageous, consider the text-to-text framework where the model is provided with a prefix/context before being asked to make predictions (e.g., the prefix is an English sentence and the model is asked to predict the German translation). With fully causal masking, the model's representation of a prefix state can only depend on prior entries of the prefix. So, when predicting an entry of the output, the model will attend to a representation of the prefix that is unnecessarily limited. Similar arguments have been made against using a unidirectional recurrent neural network encoder in sequence-to-sequence models <ref type="bibr" target="#b6">[Bahdanau et al., 2015]</ref>.</p><p>This issue can be avoided in a Transformer-based language model simply by changing the masking pattern. Instead of using a causal mask, we use fully-visible masking during the prefix portion of the sequence. This masking pattern and a schematic of the resulting "prefix LM" are illustrated in the rightmost panels of Figures <ref type="figure" target="#fig_1">3</ref> and<ref type="figure" target="#fig_2">4</ref>, respectively. In the English to German translation example mentioned above, fully-visible masking would be applied to the prefix "translate English to German: That is good. target:" and causal masking would be used during training for predicting the target "Das ist gut." Using a prefix LM in the text-to-text framework was originally proposed by <ref type="bibr">Liu et al. [2018]</ref>. More recently, <ref type="bibr" target="#b25">Dong et al. [2019]</ref> showed that this architecture is effective on a wide variety of text-to-text tasks.</p><p>We note that when following our text-to-text framework, the prefix LM architecture closely resembles BERT <ref type="bibr" target="#b23">[Devlin et al., 2018]</ref> for classification tasks. To see why, consider an example from the MNLI benchmark where the premise is "I hate pigeons.", the hypothesis is "My feelings towards pigeons are filled with animosity." and the correct label is "entailment". To feed this example into a language model, we would transform it into the sequence "mnli premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity. target: entailment". In this case, the fully-visible prefix would correspond to the entire input sequence up to the word "target:", which can be seen as being analogous to the "classification" token used in BERT. So, our model would have full visibility over the entire input, and then would be tasked with making a classification by outputting the word "entailment". It is easy for the model to learn to output one of the valid class labels given the task prefix ("mnli" in this case). As such, the main difference between a prefix LM and the BERT architecture is that the classifier is simply integrated into the output layer of the Transformer decoder in the prefix LM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Comparing different model structures</head><p>In the interest of experimentally comparing these architectural variants, we would like each model we consider to be equivalent in some meaningful way. We might say that two models are equivalent if they either have the same number of parameters or they require roughly the same amount of computation to process a given (input-sequence, target-sequence) pair. Unfortunately, it is not possible to compare an encoder-decoder model to a language model architecture (comprising a single Transformer stack) according to both of these criteria at the same time. To see why, first note an encoder-decoder model with L layers in the encoder and L layers in the decoder has approximately the same number of parameters as a language model with 2L layers. However, the same L + L encoder-decoder model will have approximately the same computational cost as a language model with only L layers. This is a consequence of the fact that the L layers in the language model must be applied to both the input and output sequence, while the encoder is only applied to the input sequence and the decoder is only applied to the output sequence. Note that these equivalences are approximate -there are some extra parameters in the decoder due to the attention over the encoder and there are also some computational costs in the attention layers that are quadratic in the sequence lengths. In practice, however, we observed nearly identical step times for L-layer language models versus L + L-layer encoder-decoder models, suggesting a roughly equivalent computational cost.</p><p>To provide a reasonable means of comparison, we consider multiple configurations for our encoderdecoder model. We will refer to the number of layers and parameters in a BERT BASE -sized layer stack as L and P , respectively. We will use M to refer to the number of FLOPs required for an L + L-layer encoder-decoder model or L-layer decoder-only model to process a given input-target pair. In total, we will compare:</p><p>? An encoder-decoder model with L layers in the encoder and L layers in the decoder. This model has 2P parameters and a computation cost of M FLOPs.</p><p>? An equivalent model, but with parameters shared across the encoder and decoder, resulting in P parameters and an M -FLOP computational cost.</p><p>? An encoder-decoder model with L/2 layers each in the encoder and decoder, giving P parameters and an M/2-FLOP cost. Table <ref type="table">2</ref>: Performance of the different architectural variants described in Section 3.2.2. We use P to refer to the number of parameters in a 12-layer base Transformer layer stack and M to refer to the FLOPs required to process a sequence using the encoder-decoder model. We evaluate each architectural variant using a denoising objective (described in Section 3.1.4) and an autoregressive objective (as is commonly used to train language models).</p><p>? A decoder-only language model with L layers and P parameters and a resulting computational cost of M FLOPs.</p><p>? A decoder-only prefix LM with the same architecture (and thus the same number of parameters and computational cost), but with fully-visible self-attention over the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Objectives</head><p>As an unsupervised objective, we will consider both a basic language modeling objective as well as our baseline denoising objective described in Section 3.1.4. We include the language modeling objective due to its historic use as a pre-training objective <ref type="bibr" target="#b20">[Dai and Le, 2015;</ref><ref type="bibr" target="#b85">Ramachandran et al., 2016;</ref><ref type="bibr" target="#b36">Howard and Ruder, 2018;</ref><ref type="bibr" target="#b81">Radford et al., 2018;</ref><ref type="bibr" target="#b77">Peters et al., 2018]</ref> as well as its natural fit for the language model architectures we consider. For models that ingest a prefix before making predictions (the encoder-decoder model and prefix LM), we sample a span of text from our unlabeled dataset and choose a random point to split it into prefix and target portions. For the standard language model, we train the model to predict the entire span from beginning to end. Our unsupervised denoising objective is designed for text-to-text models; to adapt it for use with a language model we concatenate the inputs and targets as described in Section 3.2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Results</head><p>The scores achieved by each of the architectures we compare are shown in Table <ref type="table">2</ref>. For all tasks, the encoder-decoder architecture with the denoising objective performed best. This variant has the highest parameter count (2P ) but the same computational cost as the P -parameter decoder-only models. Surprisingly, we found that sharing parameters across the encoder and decoder performed nearly as well. In contrast, halving the number of layers in the encoder and decoder stacks significantly hurt performance. Concurrent work <ref type="bibr" target="#b57">[Lan et al., 2019</ref>] also found that sharing parameters across Transformer blocks can be an effective means of lowering the total parameter count without sacrificing much performance. XLNet also bears some resemblance to the shared encoder-decoder approach with a denoising objective <ref type="bibr" target="#b64">[Yang et al., 2019]</ref>. We also note that the shared parameter encoder-decoder outperforms the decoder-only prefix LM, suggesting that the addition of an explicit encoder-decoder attention is beneficial. Finally, we confirm the widely-held conception that using a denoising objective always results in better downstream task performance compared to a language modeling objective. We undertake a more detailed exploration of unsupervised objectives in the following section.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Unsupervised objectives</head><p>The choice of unsupervised objective is of central importance as it provides the mechanism through which the model gains general-purpose knowledge to apply to downstream tasks. This has led to the development of a wide variety of pre-training objectives <ref type="bibr" target="#b20">[Dai and Le, 2015;</ref><ref type="bibr" target="#b85">Ramachandran et al., 2016;</ref><ref type="bibr" target="#b81">Radford et al., 2018;</ref><ref type="bibr" target="#b23">Devlin et al., 2018;</ref><ref type="bibr" target="#b64">Yang et al., 2019;</ref><ref type="bibr" target="#b63">Liu et al., 2019b;</ref><ref type="bibr" target="#b110">Wang et al., 2019a;</ref><ref type="bibr" target="#b100">Song et al., 2019;</ref><ref type="bibr" target="#b25">Dong et al., 2019;</ref><ref type="bibr" target="#b44">Joshi et al., 2019]</ref>. In this section, we perform a procedural exploration of the space of unsupervised objectives. In many cases, we will not replicate an existing objective exactly -some will be modified to fit our text-to-text encoder-decoder framework and, in other cases, we will use objectives that combine concepts from multiple common approaches. An analogous exploration was performed by <ref type="bibr" target="#b110">Wang et al. [2019a]</ref>.</p><p>Overall, all of our objectives ingest a sequence of token IDs corresponding to a tokenized span of text from our unlabeled text dataset. The token sequence is processed to produce a (corrupted) input sequence and a corresponding target. Then, the model is trained as usual with maximum likelihood to predict the target sequence. We provide illustrative examples of many of the objectives we consider in Table <ref type="table" target="#tab_4">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Disparate high-level approaches</head><p>To begin with, we compare three techniques that are inspired by commonly-used objectives but differ significantly in their approach. First, we include a basic "prefix language modeling" objective as was used in Section 3.2.3. This technique splits a span of text into two components, one to use as inputs to the encoder and the other to use as a target sequence to be predicted by the decoder. Second, we consider an objective inspired by the "masked language modeling" (MLM) objective used in BERT <ref type="bibr" target="#b23">[Devlin et al., 2018]</ref>. MLM takes a span of text and corrupts 15% of the tokens. 90% of the corrupted tokens are replaced with a special mask token and 10% are replaced with a random token. Since BERT is an encoder-only model, its goal is to reconstruct the original sequence at the output of the encoder. In the encoder-decoder case, we simply use the entire uncorrupted sequence as the target. Note that this differs from our baseline objective, which uses only the corrupted tokens as targets; we compare these two approaches in Section 3.3.2. Finally, we also consider a basic deshuffling objective as used e.g. in <ref type="bibr" target="#b61">[Liu et al., 2019a]</ref> where it was applied to a denoising sequential autoencoder. This approach takes a sequence of tokens, shuffles it, and then uses the original deshuffled sequence as a target. We provide examples of the inputs and targets for these three methods in the first three rows of Table <ref type="table" target="#tab_4">3</ref>.  The performance of these three objectives is shown in Table <ref type="table" target="#tab_5">4</ref>. Overall, we find that the BERT-style objective performs best, though the prefix language modeling objective attains similar performance on the translation tasks. Indeed, the motivation for the BERT objective was to outperform language model-based pre-training. The deshuffling objective performs considerably worse than both prefix language modeling and the BERT-style objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Simplifying the BERT objective</head><p>Based on the results in the prior section, we will now focus on exploring modifications to the BERT-style denoising objective. This objective was originally proposed as a pre-training technique for an encoder-only model trained for classification and span prediction. As such, it may be possible to modify it so that it performs better or is more efficient in our encoder-decoder text-to-text setup.</p><p>First, we consider a simple variant of the BERT-style objective where we don't include the random token swapping step. The resulting objective simply replaces 15% of the tokens in the input with a mask token and the model is trained to reconstruct the original uncorrupted sequence. A similar masking objective was used by <ref type="bibr" target="#b100">Song et al. [2019]</ref> where it was referred to as "MASS", so we call this variant the "MASS-style" objective. Second, we were interested to see if it was possible to avoid predicting the entire uncorrupted text span since this requires self-attention over long sequences in the decoder. We consider two strategies to achieve this: First, instead of replacing each corrupted token with a mask token, we replace the entirety of each consecutive span of corrupted tokens with a unique mask token. Then, the target sequence becomes the concatenation of the "corrupted" spans, each prefixed by the mask token used to replace it in the input. This is the pre-training objective we use in our baseline, described in Section 3.1.4. Second, we also consider a variant where we simply drop the corrupted tokens from the input sequence completely and task the model with reconstructing the dropped tokens in order. Examples of these approaches are shown in the fifth and sixth rows of Table <ref type="table" target="#tab_4">3</ref>.</p><p>An empirical comparison of the original BERT-style objective to these three alternatives is shown in Table <ref type="table" target="#tab_6">5</ref>. We find that in our setting, all of these variants perform similarly. The only exception was that dropping corrupted tokens completely produced a small improvement in the GLUE score thanks to a significantly higher score on CoLA (60.04, compared to our baseline average of 53.84, see Table <ref type="table" target="#tab_6">15</ref>). This may be due to the fact that CoLA involves classifying whether a given sentence is grammatically and syntactically acceptable, and being able to determine when tokens are missing is closely related to detecting acceptability. However, dropping tokens completely performed worse than replacing them with sentinel tokens on SuperGLUE. The two variants that do not require predicting the full original sequence ("replace corrupted spans" and "drop corrupted spans") are both potentially attractive since they make the target sequences shorter and consequently make training faster. Going forward, we will explore variants where we replace corrupted spans with sentinel tokens and only predict the corrupted tokens (as in our baseline objective).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Varying the corruption rate</head><p>So far, we have been corrupting 15% of the tokens, the value used in BERT <ref type="bibr" target="#b23">[Devlin et al., 2018]</ref>. Again, since our text-to-text framework differs from BERT's, we are interested to see if a different corruption rate works better for us. We compare corruption rates of 10%, 15%, 25%, and 50% in Table <ref type="table" target="#tab_7">6</ref>. Overall, we find that the corruption rate had a limited effect on the model's performance.</p><p>The only exception is that the largest corruption rate we consider (50%) results in a significant degradation of performance on GLUE and SQuAD. Using a larger corruption rate also results in longer targets, which can potentially slow down training. Based on these results and the historical precedent set by BERT, we will use a corruption rate of 15% going forward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Corrupting spans</head><p>We now turn towards the goal of speeding up training by predicting shorter targets. The approach we have used so far makes an i.i.d. decision for each input token as to whether to corrupt it or not. When multiple consecutive tokens have been corrupted, they are treated as a "span" and a single unique mask token is used to replace the entire span. Replacing entire spans with a single token results in unlabeled text data being processed into shorter sequences. Since we are using an i.i.d. corruption strategy, it is not always the case that a significant number of corrupted tokens appear consecutively.</p><p>As a result, we might obtain additional speedup by specifically corrupting spans of tokens rather than corrupting individual tokens in an i.i.d. manner. Corrupting spans was also previously considered as a pre-training objective for BERT, where it was found to improve performance <ref type="bibr" target="#b44">[Joshi et al., 2019]</ref>.</p><p>To test this idea, we consider an objective that specifically corrupts contiguous, randomly-spaced spans of tokens. This objective can be parametrized by the proportion of tokens to be corrupted and the total number of corrupted spans. The span lengths are then chosen randomly to satisfy these specified parameters. For example, if we are processing a sequence of 500 tokens and we have specified that 15% of tokens should be corrupted and that there should be 25 total spans, then the total number of corrupted tokens would be 500 ? 0.15 = 75 and the average span length would be 75/25 = 3. Note that given the original sequence length and corruption rate, we can equivalently parametrize this objective either by the average span length or the total number of spans.</p><p>We compare the span-corruption objective to the i.i.d-corruption objective in Table <ref type="table">7</ref>. We use a corruption rate of 15% in all cases and compare using average span lengths of 2, 3, 5 and 10. Again, we find a limited difference between these objectives, though the version with an average span length of 10 slightly underperforms the other values in some cases. We also find in particular that using an average span length of 3 slightly (but significantly) outperforms the i.i.d. objective on most non-translation benchmarks. Fortunately, the span-corruption objective also provides some speedup during training compared to the i.i.d. noise approach because span corruption produces shorter sequences on average. Table <ref type="table">7</ref>: Performance of the span-corruption objective (inspired by <ref type="bibr" target="#b44">Joshi et al. [2019]</ref>) for different average span lengths. In all cases, we corrupt 15% of the original text sequence.</p><p>Figure <ref type="figure">5</ref>: A flow chart of our exploration of unsupervised objectives. We first consider a few disparate approaches in Section 3.3.1 and find that a BERT-style denoising objective performs best. Then, we consider various methods for simplifying the BERT objective so that it produces shorter target sequences in Section 3.3.2. Given that replacing dropped-out spans with sentinel tokens performs well and results in short target sequences, in Section 3.3.3 we experiment with different corruption rates. Finally, we evaluate an objective that intentionally corrupts contiguous spans of tokens in Section 3.3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.5">Discussion</head><p>Figure <ref type="figure">5</ref> shows a flow chart of the choices made during our exploration of unsupervised objectives.</p><p>Overall, the most significant difference in performance we observed was that denoising objectives outperformed language modeling and deshuffling for pre-training. We did not observe a remarkable difference across the many variants of the denoising objectives we explored. However, different objectives (or parameterizations of objectives) can lead to different sequence lengths and thus different training speeds. This implies that choosing among the denoising objectives we considered here should mainly be done according to their computational cost. Our results also suggest that additional exploration of objectives similar to the ones we consider here may not lead to significant gains for the tasks and model we consider. Instead, it may be fortuitous to explore entirely different ways of leveraging unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Pre-training dataset</head><p>Like the unsupervised objective, the pre-training dataset itself is a crucial component of the transfer learning pipeline. However, unlike objectives and benchmarks, new pre-training datasets are usually not treated as significant contributions on their own and are often not released alongside pre-trained models and code. Instead, they are typically introduced in the course of presenting a new method or model. As a result, there has been relatively little comparison of different pre-training datasets as well as a lack of a "standard" dataset used for pre-training. Some recent notable exceptions <ref type="bibr" target="#b5">[Baevski et al., 2019;</ref><ref type="bibr" target="#b65">Liu et al., 2019c;</ref><ref type="bibr" target="#b64">Yang et al., 2019]</ref> have compared pre-training on a new large (often Common Crawl-sourced) dataset to using a smaller preexisting dataset (often Wikipedia). To probe more deeply into the impact of the pre-training dataset on performance, in this section we compare variants of our C4 dataset and other potential sources of pre-training data. We release all of the C4 dataset variants we consider as part of TensorFlow Datasets. 11</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Unlabeled datasets</head><p>In creating C4, we developed various heuristics to filter the web-extracted text from Common Crawl (see Section 2.2 for a description). We are interested in measuring whether this filtering results in improved performance on downstream tasks, in addition to comparing it to other filtering approaches and common pre-training datasets. Towards this end, we compare the performance of our baseline model after pre-training on the following datasets:</p><p>C4 As a baseline, we first consider pre-training on our proposed unlabeled dataset as described in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unfiltered C4</head><p>To measure the effect of the heuristic filtering we used in creating C4 (deduplication, removing bad words, only retaining sentences, etc.), we also generate an alternate version of C4 that forgoes this filtering. Note that we still use langdetect to extract English text. As a result, our "unfiltered" variant still includes some filtering because langdetect sometimes assigns a low probability to non-natural English text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RealNews-like</head><p>Recent work has used text data extracted from news websites <ref type="bibr" target="#b120">[Zellers et al., 2019;</ref><ref type="bibr" target="#b5">Baevski et al., 2019]</ref>. To compare to this approach, we generate another unlabeled dataset by additionally filtering C4 to only include content from one of the domains used in the "RealNews" dataset <ref type="bibr" target="#b120">[Zellers et al., 2019]</ref>. Note that for ease of comparison, we retain the heuristic filtering methods used in C4; the only difference is that we have ostensibly omitted any non-news content.</p><p>WebText-like Similarly, the WebText dataset <ref type="bibr" target="#b82">[Radford et al., 2019]</ref> only uses content from webpages that were submitted to the content aggregation website Reddit and received a "score" of at least 3. The score for a webpage submitted to Reddit is computed based on the proportion of users who endorse (upvote) or oppose (downvote) the webpage. The idea behind using the Reddit score as a quality signal is that users of the site would only upvote high-quality text content. To generate a comparable dataset, we first tried removing all content from C4 that did not originate from a URL that appeared in the list prepared by the OpenWebText effort. 12 However, this resulted in comparatively little content -only about 2 GB -because most pages never appear on Reddit. Recall that C4 was created based on a single month of Common Crawl data. To avoid using a prohibitively small dataset, we therefore downloaded 12 months of data from Common Crawl from August 2018 to July 2019, applied our heuristic filtering for C4, then applied the Reddit filter. This produced a 17 GB WebText-like dataset, which is of comparable size to the original 40GB WebText dataset <ref type="bibr" target="#b82">[Radford et al., 2019]</ref>. Wikipedia + Toronto Books Corpus A drawback of using pre-training data from Wikipedia is that it represents only one possible domain of natural text (encyclopedia articles). To mitigate this, BERT <ref type="bibr" target="#b23">[Devlin et al., 2018]</ref> combined data from Wikipedia with the Toronto Books Corpus (TBC) <ref type="bibr" target="#b123">[Zhu et al., 2015]</ref>. TBC contains text extracted from eBooks, which represents a different domain of natural language. BERT's popularity has led to the Wikipedia + TBC combination being used in many subsequent works.</p><p>The results achieved after pre-training on each of these datasets is shown in Table <ref type="table" target="#tab_9">8</ref>. A first obvious takeaway is that removing the heuristic filtering from C4 uniformly degrades performance and makes the unfiltered variant perform the worst in every task. Beyond this, we found that in some cases a pre-training dataset with a more constrained domain outperformed the diverse C4 dataset. For example, using the Wikipedia + TBC corpus produced a SuperGLUE score of 73.24, beating our baseline's score (using C4) of 71.36. This is almost entirely attributable to a boost in performance from 25.78 (baseline, C4) to 50.93 (Wikipedia + TBC) on the Exact Match score for MultiRC (see Table <ref type="table" target="#tab_6">15</ref>). MultiRC is a reading comprehension dataset whose largest source of data comes from fiction books, which is exactly the domain covered by TBC. Similarly, using the RealNews-like dataset for pre-training conferred an increase from 68.16 to 73.72 on the Exact Match score for ReCoRD, a dataset that measures reading comprehension on news articles. As a final example, using data from Wikipedia produced significant (but less dramatic) gains on SQuAD, which is a question-answering dataset with passages sourced from Wikipedia. The main lesson behind these findings is that pre-training on in-domain unlabeled data can improve performance on downstream tasks. This is unsurprising but also unsatisfying if our goal is to pre-train a model that can rapidly adapt to language tasks from arbitrary domains.</p><p>A drawback to only pre-training on a single domain is that the resulting datasets are often substantially smaller. Similarly, while the WebText-like variant performed as well or better than the C4 dataset in our baseline setting, the Reddit-based filtering produced a dataset that was about 40? smaller than C4 despite being based on 12? more data from Common Crawl. We investigate whether using smaller pre-training datasets can pose an issue in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Pre-training dataset size</head><p>The pipeline we use to create C4 was designed to be able to create extremely large pre-training datasets. The access to so much data allows us to pre-train our models without repeating examples. It is not clear whether repeating examples during pre-training would be helpful or harmful to downstream performance because our pre-training objective is itself stochastic and can help prevent the model from seeing the same exact data multiple times.</p><p>To test the effect of limited unlabeled dataset sizes, we pre-trained our baseline model on artificially truncated versions of C4. Recall that we pre-train our baseline model on 2 35 ? 34B tokens (a small fraction of the total size of C4). We consider training on truncated variants of C4 consisting of 2 29 , 2 27 , 2 25 and 2 23 tokens. These sizes correspond to repeating the dataset 64, 256, 1,024, and 4,096  <ref type="figure" target="#fig_3">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7XIT 8VEMRMRKPSWW</head><p>(EXEWIXWM^I *YPPHEXEWIX times respectively over the course of pre-training.</p><p>The resulting downstream performance is shown in Table <ref type="table" target="#tab_10">9</ref>. As expected, performance degrades as the dataset size shrinks. We suspect this may be due to the fact that the model begins to memorize the pre-training dataset. To measure if this is true, we plot the training loss for each of these dataset sizes in Figure <ref type="figure" target="#fig_3">6</ref>. Indeed, the model attains significantly smaller training losses as the size of the pre-training dataset shrinks, suggesting possible memorization.</p><p>We note that these effects are limited when the pre-training dataset is repeated only 64 times. This suggests that some amount of repetition of pre-training data might not be harmful. However, given that additional pre-training can be beneficial (as we will show in Section 3.6) and that obtaining additional unlabeled data is cheap and easy, we suggest using large pre-training datasets whenever possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training strategy</head><p>So far we have considered the setting where all parameters of a model are pre-trained on an unsupervised task before being fine-tuned on individual supervised tasks. While this approach is straightforward, various alternative methods for training the model on downstream/supervised tasks have been proposed. In this section, we compare different schemes for fine-tuning the model in addition to the approach of training the model simultaneously on multiple tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Fine-tuning methods</head><p>It has been argued that fine-tuning all of the model's parameters can lead to suboptimal results, particularly on low-resource tasks <ref type="bibr" target="#b76">[Peters et al., 2019]</ref>. Early results on transfer learning for text classification tasks advocated fine-tuning only the parameters of a small classifier that was fed sentence embeddings produced by a fixed pre-trained model <ref type="bibr" target="#b102">[Subramanian et al., 2018;</ref><ref type="bibr" target="#b48">Kiros et al., 2015;</ref><ref type="bibr" target="#b66">Logeswaran and Lee, 2018;</ref><ref type="bibr" target="#b33">Hill et al., 2016;</ref><ref type="bibr" target="#b18">Conneau et al., 2017]</ref>. This approach is less applicable to our encoder-decoder model because the entire decoder must be trained to output the target sequences for a given task. Instead, we focus on two alternative fine-tuning approaches that update only a subset of the parameters of our encoder-decoder model.</p><p>The first, "adapter layers" <ref type="bibr" target="#b35">[Houlsby et al., 2019;</ref><ref type="bibr" target="#b7">Bapna et al., 2019]</ref>, is motivated by the goal of keeping most of the original model fixed while fine-tuning. Adapter layers are additional dense-ReLUdense blocks that are added after each of the preexisting feed-forward networks in each block of the Transformer. These new feed-forward networks are designed so that their output dimensionality matches their input. This allows them to be inserted into the network with no additional changes to the structure or parameters. When fine-tuning, only the adapter layer and layer normalization parameters are updated. The main hyperparameter of this approach is the inner dimensionality d of the feed-forward network, which changes the number of new parameters added to the model. We experiment with various values for d.</p><p>The second alternative fine-tuning method we consider is "gradual unfreezing" <ref type="bibr" target="#b36">[Howard and Ruder, 2018]</ref>. In gradual unfreezing, more and more of the model's parameters are fine-tuned over time. Gradual unfreezing was originally applied to a language model architecture consisting of a single stack of layers. In this setting, at the start of fine-tuning only the parameters of the final layer are updated, then after training for a certain number of updates the parameters of the second-to-last layer are also included, and so on until the entire network's parameters are being fine-tuned. To adapt this approach to our encoder-decoder model, we gradually unfreeze layers in the encoder and decoder in parallel, starting from the top in both cases. Since the parameters of our input embedding matrix and output classification matrix are shared, we update them throughout fine-tuning. Recall that our baseline model consists of 12 layers each in the encoder and decoder and is fine-tuned for 2 18 steps. As such, we subdivide the fine-tuning process into 12 episodes of 2 18 /12 steps each and train from layers 12 -n to 12 in the nth episode. We note that <ref type="bibr" target="#b36">Howard and Ruder [2018]</ref> suggested fine-tuning an additional layer after each epoch of training. However, since our supervised datasets vary so much in size and since some of our downstream tasks are actually mixtures of many tasks (GLUE and SuperGLUE), we instead adopt the simpler strategy of fine-tuning an additional layer after every 2 18 /12 steps.</p><p>A comparison of the performance of these fine-tuning approaches is shown in Table <ref type="table" target="#tab_11">10</ref>. For adapter layers, we report the performance using an inner dimensionality d of 32, 128, 512, 2048. Pursuant with past results <ref type="bibr" target="#b35">[Houlsby et al., 2019;</ref><ref type="bibr" target="#b7">Bapna et al., 2019]</ref> we find that lower-resource tasks like SQuAD work well with a small value of d whereas higher resource tasks require a large dimensionality to achieve reasonable performance. This suggests that adapter layers could be a promising technique for fine-tuning on fewer parameters as long as the dimensionality is scaled appropriately to the task size. Note that in our case we treat GLUE and SuperGLUE each as a single "task" by concatenating their constituent datasets, so although they comprise some low-resource datasets the combined dataset is large enough that it necessitates a large value of d. We found that gradual unfreezing caused a minor degradation in performance across all tasks, though it did provide some speedup during fine-tuning. Better results may be attainable by more carefully tuning the unfreezing schedule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Multi-task learning</head><p>So far, we have been pre-training our model on a single unsupervised learning task before fine-tuning it individually on each downstream task. An alternative approach, called "multi-task learning" <ref type="bibr">[Ruder, Fine-</ref> 2017; <ref type="bibr">Caruana, 1997]</ref>, is to train the model on multiple tasks at a time. This approach typically has the goal of training a single model that can simultaneously perform many tasks at once, i.e. the model and most of its parameters are shared across all tasks. We relax this goal somewhat and instead investigate methods for training on multiple tasks at once in order to eventually produce separate parameter settings that perform well on each individual task. For example, we might train a single model on many tasks, but when reporting performance we are allowed to select a different checkpoint for each task. This loosens the multi-task learning framework and puts it on more even footing compared to the pre-train-then-fine-tune approach we have considered so far. We also note that in our unified text-to-text framework, "multi-task learning" simply corresponds to mixing datasets together. In contrast, most applications of multi-task learning to NLP add task-specific classification networks or use different loss functions for each task <ref type="bibr" target="#b63">[Liu et al., 2019b]</ref>.</p><p>As pointed out by <ref type="bibr" target="#b3">Arivazhagan et al. [2019]</ref>, an extremely important factor in multi-task learning is how much data from each task the model should be trained on. Our goal is to not under-or over-train the model -that is, we want the model to see enough data from a given task that it can perform the task well, but not to see so much data that it memorizes the training set. How exactly to set the proportion of data coming from each task can depend on various factors including dataset sizes, the "difficulty" of learning the task (i.e. how much data the model must see before being able to perform the task effectively), regularization, etc. An additional issue is the potential for "task interference" or "negative transfer", where achieving good performance on one task can hinder performance on another. Given these concerns, we begin by exploring various strategies for setting the proportion of data coming from each task. A similar exploration was performed by <ref type="bibr" target="#b110">Wang et al. [2019a]</ref>.</p><p>Examples-proportional mixing A major factor in how quickly a model will overfit to a given task is the task's dataset size. As such, a natural way to set the mixing proportions is to sample in proportion to the size of each task's dataset. This is equivalent to concatenating the datasets for all tasks and randomly sampling examples from the combined dataset. Note, however, that we are including our unsupervised denoising task, which uses a dataset that is orders of magnitude larger than every other task's. It follows that if we simply sample in proportion to each dataset's size, the vast majority of the data the model sees will be unlabeled, and it will undertrain on all of the supervised tasks. Even without the unsupervised task, some tasks (e.g. WMT English to French) are so large that they would similarly crowd out most of the batches. To get around this issue, we set an artificial "limit" on the dataset sizes before computing the proportions. Specifically, if the number of examples in each of our N task's datasets is e n , n ? {1, . . . , N } then we set probability of sampling an example from the mth task during training to r m = min(e m , K)/ min(e n , K) where K is the artificial dataset size limit.</p><p>Temperature-scaled mixing An alternative way of mitigating the huge disparity between dataset sizes is to adjust the "temperature" of the mixing rates. This approach was used by multilingual BERT to ensure that the model was sufficiently trained on low-resource languages. <ref type="foot" target="#foot_10">14</ref> To implement temperature scaling with temperature T , we raise each task's mixing rate r m to the power of 1 ?T and renormalize the rates so that they sum to 1. When T = 1, this approach is equivalent to examples-proportional mixing and as T increases the proportions become closer to equal mixing. We retain the dataset size limit K (applied to obtain r m before temperature scaling) but set it to a large value of K = 2 21 . We use a large value of K because increasing the temperature will decrease the mixing rate of the largest datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Equal mixing</head><p>In this case, we sample examples from each task with equal probability. Specifically, each example in each batch is sampled uniformly at random from one of the datasets we train on. This is most likely a suboptimal strategy, as the model will overfit quickly on low-resource tasks and underfit on high-resource tasks. We mainly include it as a point of reference of what might go wrong when the proportions are set suboptimally.</p><p>To compare these mixing strategies on equal footing with our baseline pre-train-then-fine-tune results, we train multi-task models for the same total number of steps: 2 19 + 2 18 = 786,432. The results are shown in Table <ref type="table" target="#tab_12">11</ref>.</p><p>In general, we find that multi-task training underperforms pre-training followed by fine-tuning on most tasks. The "equal" mixing strategy in particular results in dramatically degraded performance, which may be because the low-resource tasks have overfit, the high-resource tasks have not seen enough data, or the model has not seen enough unlabeled data to learn general-purpose language capabilities. For examples-proportional mixing, we find that for most tasks there is a "sweet spot" for K where the model obtains its the best performance, and larger or smaller values of K tend to result in worse performance. The exception (for the range of K values we considered) was WMT English to French translation, which is such a high-resource task that it always benefits from a higher mixing proportion. Finally, we note that temperature-scaled mixing also provides a means of obtaining reasonable performance from most tasks, with T = 2 performing the best in most cases. In the following section, we explore ways to close the gap between multi-task training and the pre-train-then-fine-tune approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3">Combining multi-task learning with fine-tuning</head><p>Recall that we are studying a relaxed version of multi-task learning where we train a single model on a mixture of tasks but are allowed to evaluate performance using different parameter settings (checkpoints) for the model. We can extend this approach by considering the case where the model is pre-trained on all tasks at once but is then fine-tuned on the individual supervised tasks. This is the method used by the "MT-DNN" <ref type="bibr" target="#b62">[Liu et al., 2015</ref><ref type="bibr" target="#b63">[Liu et al., , 2019b]]</ref>, which achieved state-of-the-art performance on GLUE and other benchmarks when it was introduced. We consider three variants of this approach: In the first, we simply pre-train the model on an examples-proportional mixture with an artificial dataset size limit of K = 2 19 before fine-tuning it on each individual downstream task. This helps us measure whether including the supervised tasks alongside the unsupervised objective during pre-training gives the model some beneficial early exposure to the downstream tasks.</p><p>We might also hope that mixing in many sources of supervision could help the pre-trained model obtain a more general set of "skills" (loosely speaking) before it is adapted to an individual task.</p><p>To measure this directly, we consider a second variant where we pre-train the model on the same examples-proportional mixture (with K = 2 19 ) except that we omit one of the downstream tasks from this pre-training mixture. Then, we fine-tune the model on the task that was left out during pre-training. We repeat this for each of the downstream tasks we consider. We call this approach "leave-one-out" multi-task training. This simulates the real-world setting where a pre-trained model is fine-tuned on a task it had not seen during pre-training. Note that multi-task pre-training provides a diverse mixture of supervised tasks. Since other fields (e.g. computer vision <ref type="bibr" target="#b72">[Oquab et al., 2014;</ref><ref type="bibr" target="#b41">Jia et al., 2014;</ref><ref type="bibr" target="#b39">Huh et al., 2016;</ref><ref type="bibr" target="#b118">Yosinski et al., 2014]</ref>) use a supervised dataset for pre-training, we were interested to see whether omitting the unsupervised task from the multi-task pre-training mixture still produced good results. For our third variant we therefore pre-train on an examples-proportional mixture of all of the supervised tasks we consider with K = 2 19 . In all of these variants, we follow our standard procedure of pre-training for 2 19 steps before fine-tuning for 2 18 steps. We compare the results of these approaches in Table <ref type="table" target="#tab_13">12</ref>. For comparison, we also include results for our baseline (pre-train then fine-tune) and for standard multi-task learning (without fine-tuning) on an examples-proportional mixture with K = 2 19 . We find that fine-tuning after multi-task pre-training results in comparable performance to our baseline. This suggests that using fine-tuning after multi-task learning can help mitigate some of the trade-offs between different mixing rates described in Section 3.5.2. Interestingly, the performance of "leave-one-out" training was only slightly worse, suggesting that a model that was trained on a variety of tasks can still adapt to new tasks (i.e. multi-task pre-training might not result in a dramatic task interference). Finally, supervised multitask pre-training performed significantly worse in every case except for the translation tasks. This could suggest that the translation tasks benefit less from (English) pre-training, whereas unsupervised pre-training is an important factor in the other tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Scaling</head><p>The "bitter lesson" of machine learning research argues that general methods that can leverage additional computation ultimately win out against methods that rely on human expertise <ref type="bibr" target="#b104">[Sutton, 2019;</ref><ref type="bibr" target="#b32">Hestness et al., 2017;</ref><ref type="bibr" target="#b96">Shazeer et al., 2017;</ref><ref type="bibr" target="#b45">Jozefowicz et al., 2016;</ref><ref type="bibr" target="#b67">Mahajan et al., 2018;</ref><ref type="bibr">Shazeer et al., 2018</ref><ref type="bibr" target="#b96">Shazeer et al., , 2017;;</ref><ref type="bibr" target="#b38">Huang et al., 2018b]</ref>. Recent results suggest that this may hold true for transfer learning in NLP <ref type="bibr" target="#b65">[Liu et al., 2019c;</ref><ref type="bibr" target="#b82">Radford et al., 2019;</ref><ref type="bibr" target="#b64">Yang et al., 2019;</ref><ref type="bibr" target="#b57">Lan et al., 2019]</ref>, i.e. it has repeatedly been shown that scaling up produces improved performance. However, there are a variety of possible ways to scale, including using a bigger model, training the model for more steps, and ensembling. In this section, we compare these different approaches by addressing the following premise: "You were just given 4? more compute. How should you use it?"</p><p>We start with our baseline model, which has 220M parameters and is pre-trained and fine-tuned for 2 19 and 2 18 steps respectively. The encoder and decoder are both sized similarly to "BERT BASE ".</p><p>To experiment with increased model size, we follow the guidelines of "BERT LARGE " <ref type="bibr" target="#b23">Devlin et al. [2018]</ref> and use d ff = 4096, d model = 1024, d kv = 64 and 16-head attention mechanisms. We then generate two variants, one with 16 layers and one with 32, producing models with 2? and 4? as many parameters as our original model. These two variants also have a roughly 2? and 4? the computational cost. Using our baseline and these two larger models, we consider three ways of using 4? as much computation: Training for 4? as many steps, training for 2? as many steps with the 2? bigger model, and training the 4? bigger model for the "baseline" number of training steps. When we increase the training steps, we scale both the pre-train and fine-tune steps for simplicity. Note that when increasing the number of pre-training steps, we are effectively including more pre-training data as C4 is so large that we do not complete one pass over the data even when training for 2 23 steps.</p><p>An alternative way for the model to see 4? as much data is to increase the batch size by a factor of 4. This can potentially result in faster training due to more efficient parallelization. However, training with a 4? larger batch size can yield a different outcome than training for 4? as many steps <ref type="bibr" target="#b93">[Shallue et al., 2018]</ref>. We include an additional experiment where we train our baseline model with a 4? larger batch size to compare these two cases.</p><p>It is common practice on many of the benchmarks we consider to eke out additional performance by training and evaluating using an ensemble of models. This provides an orthogonal way of utilizing additional computation. To compare other scaling methods to ensembling, we also measure the performance of an ensemble of 4 separately pre-trained and fine-tuned models. We average the logits across the ensemble before feeding them into the output softmax nonlinearity to obtain an aggregate prediction. Instead of pre-training 4 separate models, a cheaper alternative is to take a single pre-trained model and produce 4 separate fine-tuned versions. While this does not use our entire 4? computational budget, we also include this method to see if it produces competitive performance to the other scaling methods.</p><p>The performance achieved after applying these various scaling methods is shown in Table <ref type="table" target="#tab_14">13</ref>. Unsurprisingly, increasing the training time and/or model size consistently improves the baseline. There was no clear winner between training for 4? as many steps or using a 4? larger batch size, though both were beneficial. In general, increasing the model size resulted in an additional bump in performance compared to solely increasing the training time or batch size. We did not observe a large difference between training a 2? bigger model for 2? as long and training a 4? bigger model on any of the tasks we studied. This suggests that increasing the training time and increasing the model size can be complementary means of improving performance. Our results also suggest that ensembling provides an orthogonal and effective means of improving performance through scale. In some tasks (CNN/DM, WMT English to German, and WMT English to Romanian), ensembling 4 completely separately trained models significantly outperformed every other scaling approach. Ensembling models that were pre-trained together but fine-tuned separately also gave a substantial performance increase over the baseline, which suggests a cheaper means of improving performance. The only exception was SuperGLUE, where neither ensembling approach significantly improved over the baseline.</p><p>We note that different scaling methods have different trade-offs that are separate from their performance. For example, using a larger model can make downstream fine-tuning and inference more expensive. In contrast, the cost of pre-training a small model for longer is effectively amortized if it is applied to many downstream tasks. Separately, we note that ensembling N separate models has a similar cost to using a model that has an N ? higher computational cost. As a result, some consideration for the eventual use of the model is important when choosing between scaling methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Putting it all together</head><p>We now leverage the insights from our systematic study to determine how far we can push performance on popular NLP benchmarks. We are also interested in exploring the current limits of transfer learning for NLP by training larger models on large amounts of data. We start with our baseline training approach and make the following changes: Objective We swap out the i.i.d. denoising objective in our baseline for the span-corruption objective described in Section 3.3.4, which was loosely inspired by SpanBERT <ref type="bibr" target="#b44">[Joshi et al., 2019]</ref>. Specifically, we use a mean span length of 3 and corrupt 15% of the original sequence. We found that this objective produced marginally better performance (Table <ref type="table">7</ref>) while being slightly more computationally efficient due to shorter target sequence lengths.</p><p>Longer training Our baseline model uses a relatively small amount of pre-training ( 1 ?4 as much as BERT <ref type="bibr" target="#b23">[Devlin et al., 2018]</ref>, 1 ?16 as much as XLNet <ref type="bibr" target="#b64">[Yang et al., 2019]</ref>, 1 ?64 as much as RoBERTa <ref type="bibr" target="#b65">[Liu et al., 2019c]</ref>, etc.). Fortunately, C4 is big enough that we can train for substantially longer without repeating data (which can be detrimental, as shown in Section 3.4.2). We found in Section 3.6 that additional pre-training can indeed be helpful, and that both increasing the batch size and increasing the number of training steps can confer this benefit. We therefore pre-train our models for 1 million steps on a batch size of 2 11 sequences of length 512, corresponding to a total of about 1 trillion pre-training tokens (about 32? as many as our baseline). In Section 3.4.1, we showed that pre-training on the RealNews-like, WebText-like, and Wikipedia + TBC datasets outperformed pre-training on C4 on a few downstream tasks. However, these dataset variants are sufficiently small that they would be repeated hundreds of times over the course of pre-training on 1 trillion tokens. Since we showed in Section 3.4.2 that this repetition could be harmful, we opted instead to continue using the C4 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model sizes</head><p>In Section 3.6 we also showed how scaling up the baseline model size improved performance. However, using smaller models can be helpful in settings where limited computational resources are available for fine-tuning or inference. Based on these factors, we train models with a wide range of sizes:</p><p>? Base. This is our baseline model, whose hyperparameters are described in Section 3.1.1. It has roughly 220 million parameters.</p><p>? Small. We consider a smaller model, which scales the baseline down by using d model = 512, d ff = 2,048, 8-headed attention, and only 6 layers each in the encoder and decoder. This variant has about 60 million parameters.</p><p>? Large. Multi-task pre-training In Section 3.5.3, we showed that pre-training on a multi-task mixture of unsupervised and supervised tasks before fine-tuning worked as well as pre-training on the unsupervised task alone. This is the approach advocated by the "MT-DNN" <ref type="bibr" target="#b62">[Liu et al., 2015</ref><ref type="bibr" target="#b63">[Liu et al., , 2019b]]</ref>. It also has the practical benefit of being able to monitor "downstream" performance for the entire duration of training, rather than just during fine-tuning. We therefore used multi-task pre-training in our final set of experiments. We hypothesize that larger models trained for longer might benefit from a larger proportion of unlabeled data because they are more likely to overfit to smaller training datasets. However, we also note that the results of Section 3.5.3 suggest that fine-tuning after multi-task pre-training can mitigate some of the issues that might arise from choosing a suboptimal proportion of unlabeled data. Based on these ideas, we substitute the following artificial dataset sizes for our unlabeled data before using standard example-proportional mixing (described in Section 3.5.2): 710,000 for Small, 2,620,000 for Base, 8,660,000 for Large, 33,500,000 for 3B, and 133,000,000 for 11B. For all model variants, we also capped the effective dataset size of the WMT English to French and WMT English to German datasets to 1M examples during pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-tuning on individual GLUE and SuperGLUE tasks</head><p>So far, when fine-tuning on GLUE and SuperGLUE, we have concatenated all of the datasets in each benchmark so that we only finetune models once for GLUE and once for SuperGLUE. This approach makes our study logistically simpler, but we found that this sacrifices a small amount of performance on some tasks compared to fine-tuning on the task separately. A potential issue with fine-tuning on individual tasks, which would otherwise be mitigated by training on all tasks at once, is that we might overfit quickly to low-resource tasks. For example, our large batch size of 2 11 length-512 sequences would result in the entire dataset appearing multiple times in each batch for many of the low-resource GLUE and SuperGLUE tasks. We therefore use a smaller batch size of 8 length-512 sequences during fine-tuning for each GLUE and SuperGLUE task. We also save checkpoints every 1,000 steps rather than every 5,000 steps to ensure we have access to the model's parameters before it overfits. We fine-tuned models both on the GLUE/SuperGLUE mixtures as well as on each task individually. We then choose the best checkpoints from mixture fine-tuning or individual-task fine-tuning based on the validation set performance for each task. Specifically, we use models fine-tuned on the GLUE or SuperGLUE mixtures for STS-B, QQP, RTE, BoolQ, COPA, and MultiRC and use individually fine-tuned models for all other tasks.</p><p>Beam search All of our previous results were reported using greedy decoding. For tasks with long output sequences, we found improved performance from using beam search <ref type="bibr" target="#b103">[Sutskever et al., 2014]</ref>. Specifically, we use a beam width of 4 and a length penalty of ? = 0.6 <ref type="bibr" target="#b116">[Wu et al., 2016]</ref> for the WMT translation and CNN/DM summarization tasks.</p><p>Test set Since this is our final set of experiments, we report results on the test set rather than the validation set. For CNN/Daily Mail, we use the standard test set distributed with the dataset.</p><p>For the WMT tasks, this corresponds to using newstest2014 for English-German, newstest2015 for English-French, and newstest2016 for English-Romanian. For GLUE and SuperGLUE, we used the benchmark evaluation servers to compute official test set scores. 15,16 For SQuAD, evaluating on the test set requires running inference on a benchmark server. Unfortunately, the computational resources on this server are insufficient for obtaining predictions from our largest models. As a result, we instead continue to report performance on the SQuAD validation set. Fortunately, the model with the highest performance on the SQuAD test set also reported results on the validation set, so we can still compare to what is ostensibly the state-of-the-art.</p><p>Apart from those changes mentioned above, we use the same training procedure and hyperparameters as our baseline (AdaFactor optimizer, inverse square root learning rate schedule for pre-training, constant learning rate for fine-tuning, dropout regularization, vocabulary, etc.). For reference, these details are described in Section 2. The results of this final set of experiments are shown in Table <ref type="table" target="#tab_5">14</ref>. Overall, we achieved stateof-the-art performance on 17 out of the 24 tasks we consider. As expected, our largest (11 billion parameter) model performed best among our model size variants across all tasks. Our T5-3B model variant did beat the previous state of the art in a few tasks, but scaling the model size to 11 billion parameters was the most important ingredient for achieving our best performance. We now analyze the results for each individual benchmark.</p><p>We achieved a state-of-the-art average GLUE score of 89.7. Interestingly, our performance was substantially better than the previous state-of-the-art for some tasks (CoLA, RTE, and WNLI) and substantially worse for others (QNLI and MRPC). RTE and WNLI are two of the tasks where machine performance has historically lagged behind human performance, which is 93.6 and 95.9 respectively <ref type="bibr" target="#b109">[Wang et al., 2018]</ref>. Our inferior performance on QNLI is likely due to the fact that most of the best models on this task use a special pairwise ranking formulation that integrates information from multiple examples when making a prediction. We nevertheless outperformed models that do not use this approach on QNLI. In terms of parameter count, our 11B model variant is the largest model that has been submitted to the GLUE benchmark. However, most of the best-scoring submissions use a large amount of ensembling and computation to produce predictions. For example, the variant of ALBERT <ref type="bibr" target="#b57">[Lan et al., 2019]</ref> that achieved the previous state-of-the-art uses a model similar in size and architecture to our 3B variant (though it has dramatically fewer parameters due to clever parameter sharing). To produce its impressive performance on GLUE, the ALBERT authors ensembled "from 6 to 17" models depending on the task. This likely results in it being more computationally expensive to produce predictions with the ALBERT ensemble than it is with our 11B variant.</p><p>For SQuAD, we outperformed the previous state-of-the-art (XLNet <ref type="bibr" target="#b64">[Yang et al., 2019]</ref>) by about one point on both the Exact Match and F1 metrics. SQuAD is a long-standing benchmark that was created over three years ago, and most recent improvements have only increased the state-of-the-art Table <ref type="table" target="#tab_5">14</ref>: Performance of our T5 variants on every task we study. Small, Base, Large, 3B, and 11B refer to model configurations with 60 million, 220 million, 770 million, 3 billion, and 11 billion parameters, respectively. In the first row of each table, we report the state-of-the-art for the task, with the superscript denoting its source with references listed at the end of this caption. All results are reported on the test set except for SQuAD where we use the validation set. a <ref type="bibr" target="#b57">[Lan et al., 2019]</ref> b <ref type="bibr" target="#b112">[Wang et al., 2019c]</ref> c <ref type="bibr" target="#b122">[Zhu et al., 2019]</ref> d <ref type="bibr" target="#b64">[Yang et al., 2019]</ref> e <ref type="bibr" target="#b65">[Liu et al., 2019c]</ref> f <ref type="bibr" target="#b26">[Edunov et al., 2018]</ref> g <ref type="bibr" target="#b56">[Lample and Conneau, 2019]</ref> h <ref type="bibr" target="#b25">[Dong et al., 2019]</ref> by a fraction of a percentage point. We note that when results are reported on the test set, they are typically based on an ensemble of models and/or leverage external datasets (e.g. TriviaQA <ref type="bibr">[Joshi et al., 2017]</ref> or NewsQA <ref type="bibr" target="#b107">[Trischler et al., 2016]</ref>) to augment the small SQuAD training set. Human performance on SQuAD is estimated at 82.30 and 91.22 for the Exact Match and F1 metric respectively <ref type="bibr" target="#b84">[Rajpurkar et al., 2016]</ref>, so it is not clear if further improvements on this benchmark are meaningful.</p><p>For SuperGLUE, we improved upon the state-of-the-art by a large margin (from an average score of 84.6 <ref type="bibr" target="#b65">[Liu et al., 2019c]</ref> to 88.9). SuperGLUE was designed to comprise of tasks that were "beyond the scope of current state-of-the-art systems, but solvable by most college-educated English speakers" <ref type="bibr" target="#b111">[Wang et al., 2019b]</ref>. We nearly match the human performance of 89.8 <ref type="bibr" target="#b111">[Wang et al., 2019b]</ref>. Interestingly, on the reading comprehension tasks (MultiRC and ReCoRD) we exceed human performance by a large margin, suggesting the evaluation metrics used for these tasks may be biased towards machine-made predictions. On the other hand, humans achieve 100% accuracy on both COPA and WSC, which is significantly better than our model's performance. This suggests that there remain linguistic tasks that are hard for our model to perfect, particularly in the low-resource setting.</p><p>We did not achieve state-of-the-art performance on any of the WMT translation tasks. This may be in part due to our use of an English-only unlabeled dataset. We also note that most of the best results on these tasks use backtranslation <ref type="bibr" target="#b26">[Edunov et al., 2018;</ref><ref type="bibr" target="#b56">Lample and Conneau, 2019]</ref>, which is a sophisticated data augmentation scheme. The state of the art on the low-resource English to Romanian benchmark also uses additional forms of cross-lingual unsupervised training <ref type="bibr" target="#b56">[Lample and Conneau, 2019]</ref>. Our results suggest that scale and English-language pre-training may be insufficient to match the performance of these more sophisticated methods. On a more specific note, the best results on English to German newstest2014 set use the much larger training set from WMT 2018 <ref type="bibr" target="#b26">[Edunov et al., 2018]</ref>, making direct comparison to our results difficult.</p><p>Finally, on CNN/Daily Mail we attain state-of-the-art performance, though only by a significant amount on the ROUGE-2-F score. It has been shown that improvements to the ROUGE score do not necessarily correspond to more coherent summaries <ref type="bibr">[Paulus et al., 2017]</ref>. Furthermore, while CNN/Daily Mail is posed as an abstractive summarization benchmark, purely extractive approaches have been shown to work well <ref type="bibr" target="#b64">[Liu, 2019]</ref>. It has also been argued that generative models trained with maximum likelihood are prone to producing repetitive summaries <ref type="bibr" target="#b91">[See et al., 2017]</ref>. Despite these potential issues, we find that our models do generate coherent and largely correct summaries. We provide some non-cherry-picked validation set examples in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Reflection</head><p>Having completed our systematic study, we wrap up by first recapping some of our most significant findings. Our results provide some high-level perspective on which avenues of research might be more or less promising. To conclude, we outline some topics we think might provide effective approaches for further progressing the field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Takeaways</head><p>Text-to-text Our text-to-text framework provides a simple way to train a single model on a wide variety of text tasks using the same loss function and decoding procedure. We showed how this approach can be successfully applied to generative tasks like abstractive summarization, classification tasks like natural language inference, and even regression tasks like STS-B. In spite of its simplicity, we found the text-to-text framework obtained comparable performance to task-specific architectures and ultimately produced state-of-the-art results when combined with scale.</p><p>Architectures While some work on transfer learning for NLP has considered architectural variants of the Transformer, we found the original encoder-decoder form worked best in our text-to-text framework. Though an encoder-decoder model uses twice as many parameters as "encoder-only" (e.g. BERT) or "decoder-only" (language model) architectures, it has a similar computational cost. We also showed that sharing the parameters in the encoder and decoder did not result in a substantial performance drop while halving the total parameter count.</p><p>Unsupervised objectives Overall, we found that most "denoising" objectives, which train the model to reconstruct randomly corrupted text, performed similarly in the text-to-text setup. As a result, we suggest using objectives that produce short target sequences so that unsupervised pre-training is more computationally efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>We introduced the "Colossal Clean Crawled Corpus" (C4), which comprises heuristicallycleaned text from the Common Crawl web dump. When comparing C4 to datasets that use additional filtering, we found that training on in-domain unlabeled data could boost performance in a few downstream tasks. However, constraining to a single domain typically results in a smaller dataset. We separately showed that performance can degrade when an unlabeled dataset is small enough that it is repeated many times over the course of pre-training. This motivates the use of a large and diverse dataset like C4 for generic language understanding tasks.</p><p>Training strategies We found that the basic approach of updating all of a pre-trained model's parameters during fine-tuning outperformed methods that are designed to update fewer parameters, although updating all parameters is most expensive. We also experimented with various approaches for training the model on multiple tasks at once, which in our text-to-text setting simply corresponds to mixing examples from different datasets when constructing batches. The primary concern in multi-task learning is setting the proportion of each task to train on. We ultimately did not find a strategy for setting mixing proportions that matched the performance of the basic approach of unsupervised pre-training followed by supervised fine-tuning. However, we found that fine-tuning after pre-training on a mixture of tasks produced comparable performance to unsupervised pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scaling</head><p>We compared various strategies for taking advantage of additional compute, including training the model on more data, training a larger model, and using an ensemble of models. We found each approach conferred a significant boost in performance, though training a smaller model on more data was often outperformed by training a larger model for fewer steps. We also showed an ensemble of models can provide substantially better results than a single model, which provides an orthogonal means of leveraging additional computation. Ensembling models that were fine-tuned from the same base pre-trained model performed worse than pre-training and fine-tuning all models completely separately, though fine-tune-only ensembling still substantially outperformed a single model.</p><p>Pushing the limits We combined our above insights and trained substantially larger models (up to 11 billion parameters) to achieve state-of-the-art results across many of the benchmarks we considered. For unsupervised training, we extracted text from our C4 dataset and applied a denoising objective that corrupts contiguous spans of tokens. We pre-trained on a multi-task mixture before fine-tuning on individual tasks. Overall, our models were trained on over 1 trillion tokens. In the interest of facilitating the replication, extension, and application of our results, we release our code, the C4 dataset, and pre-trained model weights for each T5 variant. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Outlook</head><p>The inconvenience of large models An unsurprising but important result from our study is that larger models tend to perform better. The fact that the hardware used for running these models is continually getting cheaper and more powerful suggests that scaling up may continue to be a promising way to achieve better performance <ref type="bibr" target="#b104">[Sutton, 2019]</ref>. However, it will always be the case that there are applications and scenarios where using a smaller or less expensive model is helpful, for example when performing client-side inference or federated learning <ref type="bibr" target="#b50">[Kone?n? et al., 2015</ref><ref type="bibr" target="#b51">[Kone?n? et al., , 2016]]</ref>. Relatedly, one beneficial use of transfer learning is the possibility of attaining good performance on low-resource tasks. Low-resource tasks often occur (by definition) in settings where one lacks the assets to label more data. It follows that low-resource applications often also have limited access to computational resources which can incur additional costs. As a result, we advocate for research on methods that achieve stronger performance with cheaper models so that transfer learning can be applied where it will have the most impact. Some current work along these lines include distillation <ref type="bibr" target="#b34">[Hinton et al., 2015;</ref><ref type="bibr" target="#b90">Sanh et al., 2019;</ref><ref type="bibr" target="#b42">Jiao et al., 2019]</ref>, parameter sharing <ref type="bibr" target="#b57">[Lan et al., 2019]</ref>, and conditional computation <ref type="bibr" target="#b96">[Shazeer et al., 2017]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>More efficient knowledge extraction</head><p>Recall that one of the goals of pre-training is (loosely speaking) to provide the model with general-purpose "knowledge" that improves its performance on downstream tasks. The method we use in this work, which is currently common practice, is to train the model to denoise corrupted spans of text. We suspect that this simplistic technique may not be a very efficient way to teach the model general-purpose knowledge. More concretely, it would be useful to be able to attain good fine-tuning performance without needing to train our models on 1 trillion tokens of text first. Some concurrent work along these lines improves efficiency by pre-training a model to distinguish between real and machine-generated text <ref type="bibr" target="#b2">[Anonymous, 2019]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formalizing the similarity between tasks</head><p>We observed that pre-training on unlabeled indomain data can improve performance on downstream tasks (Section 3.4). This finding mostly relies on basic observations like the fact that SQuAD was created using data from Wikipedia. It would be useful to formulate a more rigorous notion of the "similarity" between the pre-training and downstream tasks, so that we could make more principled choices about what source of unlabeled data to use. There is some early empirical work along these lines in the field of computer vision <ref type="bibr" target="#b39">[Huh et al., 2016;</ref><ref type="bibr" target="#b52">Kornblith et al., 2018;</ref><ref type="bibr" target="#b29">He et al., 2018]</ref>. A better notion of the relatedness of tasks could also help choose supervised pre-training tasks, which has been shown to be helpful for the GLUE benchmark <ref type="bibr" target="#b78">[Phang et al., 2018]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language-agnostic models</head><p>We were disappointed to find that English-only pre-training did not achieve state-of-the-art results on the translation tasks we studied. We also are interested in avoiding the logistical difficulty of needing to specify which languages a vocabulary can encode ahead of time. To address these issues, we are interested in further investigating language-agnostic models, i.e. models that can perform a given NLP task with good performance regardless of the text's language. This is an especially pertinent issue given that English is not the native language for the majority of the world's population.</p><p>The motivation for this paper was the flurry of recent work on transfer learning for NLP. Before we began this work, these advances had already enabled breakthroughs in settings where learning-based methods had not yet been shown to be effective. We are happy to be able to continue this trend, for example by nearly matching human-level performance on the SuperGLUE benchmark, a task specifically designed to be difficult for modern transfer-learning pipelines. Our results stem from the combination of a straightforward and unified text-to-text framework, our new C4 dataset, and insights from our systematic study. Additionally, we provided an empirical overview of the field and a perspective on where it stands. We are excited to see continued work using transfer learning towards the goal of general language understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Contributions</head><p>Colin designed the scope of this project and wrote this paper, ran all the experiments in Sections 3.1 to 3.6, and contributed a large portion of our codebase. Noam contributed many of the ideas, including the text-to-text framework, unsupervised objectives, and dataset mixing strategies; implemented our base Transformer model and its architectural variants; and ran the experiments in Section 3.7. Adam oversaw all engineering aspects for this project, created the C4 dataset, implemented our dataset pipeline, and added various benchmark datasets. Katherine coordinated experiments, wrote and updated documentation, ran experiments to help design our baseline, and contributed to many parts of our codebase. Sharan contributed some of the required datasets and preprocessors, and ran assorted preliminary experiments, in addition to co-leading the open-sourcing of our codebase. Michael owned all aspects of the Winograd datasets, ingested many of the datasets we used, contributed various improvements and fixes to our infrastructure, and ran some preliminary experiments. Yanqi ran experiments and implemented methods to help settle on a reasonable baseline and helped with the final fine-tuning of the models in Section 3.7. Wei also helped with final fine-tuning and improved some of our preprocessors. Peter prototyped an early version of the pre-training dataset and resolved issues pertaining to the SQuAD and CNN/DM tasks. All authors helped set the scope and research direction we followed in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Converting WNLI to our text-to-text format</head><p>Note that as discussed in Section 2.4, we do not train on any of the data from WNLI. Instead, when evaluating on the WNLI test set (for the results in Section 3.7), we convert the WNLI test set to the "referent noun prediction" text-to-text format so that we can evaluate using a model trained on WSC and DPR. Our WNLI preprocessor is inspired by the one proposed by <ref type="bibr" target="#b30">He et al. [2019]</ref>. Recall that examples from WNLI consist of a premise, a hypothesis, and a label that indicates whether the hypothesis is True or False. Using the example from Section 2.4, the hypothesis would be "The city councilmen refused the demonstrators a permit because they feared violence." with the premise "The demonstrators feared violence." and the label False. We first find the location of all pronouns in the premise ("they" in our example). Then, we find the maximum number of words that precede or follow each pronoun that are a substring in the hypothesis ("feared violence" in our example), ignoring case and punctuation. When the premise contains multiple candidate pronouns, we choose the pronoun that is preceded or followed by the largest substring of the hypothesis. We then highlight the pronoun in the premise by surrounding it with asterisks. For the candidate noun (which is compared to our model's prediction to obtain a True or False label), we remove the matching substring from the hypothesis and optionally make it non-possessive (resulting in "the demonstrators").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Example predictions on CNN/Daily Mail</head><p>To show that our model is generating fluent summaries, we include a few example decodes from our best model (T5-11B) on the validation set along with the ground-truth summaries. These examples selected at random and were not cherry-picked.</p><p>1. Ground-truth: leopard gave up after spiky creature refused to back down in fight in kruger national park, south africa . wildlife enthusiast lisl moolman, 41, caught the bizarre battle while out on the road and armed with her camera . the leopard and porcupine tumbled out of the bushes and began to fight by roadside -watched by ms moolman . Prediction: leopard tried to make lunch out of a plucky porcupine in kruger national park, south africa . but the predator was put firmly in its place after the spiky</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RTE Original input:</head><p>Sentence 1: A smaller proportion of Yugoslavia's Italians were settled in Slovenia (at the 1991 national census, some 3000 inhabitants of Slovenia declared themselves as ethnic Italians). Sentence 2: Slovenia has 3,000 inhabitants.</p><p>Processed input: rte sentence1: A smaller proportion of Yugoslavia's Italians were settled in Slovenia (at the 1991 national census, some 3000 inhabitants of Slovenia declared themselves as ethnic Italians). sentence2: Slovenia has 3,000 inhabitants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original target: 1</head><p>Processed target: not_entailment</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MNLI</head><p>Original input:</p><p>Hypothesis: The St. Louis Cardinals have always won.</p><p>Premise: yeah well losing is i mean i'm i'm originally from Saint Louis and Saint Louis Cardinals when they were there were uh a mostly a losing team but Processed input: mnli hypothesis: The St. Louis Cardinals have always won. premise: yeah well losing is i mean i'm i'm originally from Saint Louis and Saint Louis Cardinals when they were there were uh a mostly a losing team but</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original target: 2</head><p>Processed target: contradiction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MRPC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original input:</head><p>Sentence 1: We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , " Rumsfeld said .</p><p>Sentence 2: Rather , the US acted because the administration saw " existing evidence in a new light , through the prism of our experience on September 11 " .</p><p>Processed input: mrpc sentence1: We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , " Rumsfeld said . sentence2: Rather , the US acted because the administration saw " existing evidence in a new light , through the prism of our experience on September 11 " . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MultiRC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original input:</head><p>Answer: There was only pie to eat, rather than traditional breakfast foods Paragraph: &lt;b&gt;Sent 1: &lt;/b&gt;Once upon a time, there was a squirrel named Joey.&lt;br&gt;&lt;b&gt;Sent 2: &lt;/b&gt;Joey loved to go outside and play with his cousin Jimmy.&lt;br&gt;&lt;b&gt;Sent 3: &lt;/b&gt;Joey and Jimmy played silly games together, and were always laughing.&lt;br&gt;&lt;b&gt;Sent 4: &lt;/b&gt;One day, Joey and Jimmy went swimming together at their Aunt Julie's pond.&lt;br&gt;&lt;b&gt;Sent 5: &lt;/b&gt;Joey woke up early in the morning to eat some food before they left.&lt;br&gt;&lt;b&gt;Sent 6: &lt;/b&gt;He couldn't find anything to eat except for pie!&lt;br&gt;&lt;b&gt;Sent 7: &lt;/b&gt;Usually, Joey would eat cereal, fruit (a pear), or oatmeal for breakfast.&lt;br&gt;&lt;b&gt;Sent 8: &lt;/b&gt;After he ate, he and Jimmy went to the pond.&lt;br&gt;&lt;b&gt;Sent 9: &lt;/b&gt;On their way there they saw their friend Jack Rabbit.&lt;br&gt;&lt;b&gt;Sent 10: &lt;/b&gt;They dove into the water and swam for several hours.&lt;br&gt;&lt;b&gt;Sent 11: &lt;/b&gt;The sun was out, but the breeze was cold.&lt;br&gt;&lt;b&gt;Sent 12: &lt;/b&gt;Joey and Jimmy got out of the water and started walking home.&lt;br&gt;&lt;b&gt;Sent 13: &lt;/b&gt;Their fur was wet, and the breeze chilled them.&lt;br&gt;&lt;b&gt;Sent 14: &lt;/b&gt;When they got home, they dried off, and Jimmy put on his favorite purple shirt.&lt;br&gt;&lt;b&gt;Sent 15: &lt;/b&gt;Joey put on a blue shirt with red and green dots.&lt;br&gt;&lt;b&gt;Sent 16: &lt;/b&gt;The two squirrels ate some food that Joey's mom, Jasmine, made and went off to bed.&lt;br&gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original target: 1</head><p>Processed target: stable</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN/Daily Mail</head><p>Original input: marouane fellaini and adnan januzaj continue to show the world they are not just teammates but also best mates. the manchester united and belgium duo both posted pictures of themselves out at a restaurant on monday night ahead of their game against newcastle on wednesday . januzaj poses in the middle of fellaini and a friend looking like somebody who failed to receive the memo about it being a jackson 5 themed night. premier league duo adnan januzaj and marouane fellaini pose with a friend on the dance floor . manchester united and belgium duo fellaini and januzaj are good friends both on and off the pitch . manchester united ace fellaini runs over to the bench to celebrate his goal against qpr with friend januzaj . the disco effect in the background adds to the theory, but januzaj doesn't seem to mind as they later pose on the dance floor with other friends. united haven't had too many reasons to have a song and dance this season so it seems they may be hitting the discotheques as another form of release. however, victory against newcastle on wednesday would leave manager louis van gaal at least tapping his toes as they continue to fight for a champions league spot this season. januzaj and robin van persie join fellaini in celebrating in front of the manchester united fans at west brom . januzaj receives some words of wisdom from manchester united's dutch manager louis van gaal . januzaj and fellaini are joined by some friends as they take to the dance floor ahead of the newcastle game .</p><p>Processed input: summarize: marouane fellaini and adnan januzaj continue to show the world they are not just teammates but also best mates. the manchester united and belgium duo both posted pictures of themselves out at a restaurant on monday night ahead of their game against newcastle on wednesday . januzaj poses in the middle of fellaini and a friend looking like somebody who failed to receive the memo about it being a jackson 5 themed night. premier league duo adnan januzaj and marouane fellaini pose with a friend on the dance floor . manchester united and belgium duo fellaini and januzaj are good friends both on and off the pitch . manchester united ace fellaini runs over to the bench to celebrate his goal against qpr with friend januzaj . the disco effect in the background adds to the theory, but januzaj doesn't seem to mind as they later pose on the dance floor with other friends. united haven't had too many reasons to have a song and dance this season so it seems they may be hitting the discotheques as another form of release. however, victory against newcastle on wednesday would leave manager louis van gaal at least tapping his toes as they continue to fight for a champions league spot this season. januzaj and robin van persie join fellaini in celebrating in front of the manchester united fans at west brom . januzaj receives some words of wisdom from manchester united's dutch manager louis van gaal . januzaj and fellaini are joined by some friends as they take to the dance floor ahead of the newcastle game .</p><p>Original target: the belgian duo took to the dance floor on monday night with some friends . manchester united face newcastle in the premier league on wednesday . red devils will be looking for just their second league away win in seven . louis van gaal's side currently sit two points clear of liverpool in fourth .</p><p>Processed target: the belgian duo took to the dance floor on monday night with some friends . manchester united face newcastle in the premier league on wednesday . red devils will be looking for just their second league away win in seven . louis van gaal's side currently sit two points clear of liverpool in fourth .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SQuAD</head><p>Original input:</p><p>Question: What does increased oxygen concentrations in the patient's lungs displace?</p><p>Context: Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O 2 around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the 'bends') are sometimes treated using these devices. Increased O 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O 2 as soon as possible is part of the treatment.</p><p>Processed input: question: What does increased oxygen concentrations in the patient's lungs displace? context: Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O 2 around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the 'bends') are sometimes treated using these devices. Increased O 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin.</p><p>Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O 2 as soon as possible is part of the treatment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original target: carbon monoxide</head><p>Processed target: carbon monoxide</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WMT English to German</head><p>Original input: "Luigi often said to me that he never wanted the brothers to end up in court," she wrote.</p><p>Processed input: translate English to German: "Luigi often said to me that he never wanted the brothers to end up in court," she wrote.</p><p>Original target: "Luigi sagte oft zu mir, dass er nie wollte, dass die Br?der vor Gericht landen", schrieb sie.</p><p>Processed target: "Luigi sagte oft zu mir, dass er nie wollte, dass die Br?der vor Gericht landen", schrieb sie.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WMT English to French</head><p>Original input: This image section from an infrared recording by the Spitzer telescope shows a "family portrait" of countless generations of stars: the oldest stars are seen as blue dots, while more difficult to identify are the pink-coloured "new-borns" in the star delivery room.</p><p>Processed input: translate English to French: This image section from an infrared recording by the Spitzer telescope shows a "family portrait" of countless generations of stars: the oldest stars are seen as blue dots, while more difficult to identify are the pink-coloured "new-borns" in the star delivery room. Table <ref type="table" target="#tab_6">15</ref>: Score achieved on every task we consider for all of the experiments in this paper. In the first column, we list the table where the condensed results were presented for a given experiment. As in the main text, a row marked with denotes our baseline model (described in Section 3.1).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Schematic of the objective we use in our baseline model. In this example, we process the sentence "Thank you for inviting me to your party last week." The words "for", "inviting" and "last" (marked with an ?) are randomly chosen for corruption. Each consecutive span of corrupted tokens is replaced by a sentinel token (shown as &lt;X&gt; and &lt;Y&gt;) that is unique over the example. Since "for" and "inviting" occur consecutively, they are replaced by a single sentinel &lt;X&gt;. The output sequence then consists of the dropped-out spans, delimited by the sentinel tokens used to replace them in the input plus a final sentinel token &lt;Z&gt;.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Matrices representing different attention mask patterns. The input and output of the self-attention mechanism are denoted x and y respectively. A dark cell at row i and column j indicates that the self-attention mechanism is allowed to attend to input element j at output timestep i. A light cell indicates that the self-attention mechanism is not allowed to attend to the corresponding i and j combination. Left: A fully-visible mask allows the self-attention mechanism to attend to the full input at every output timestep. Middle: A causal mask prevents the ith output element from depending on any input elements from "the future". Right: Causal masking with a prefix allows the self-attention mechanism to use fully-visible masking on a portion of the input sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Schematics of the Transformer architecture variants we consider. In this diagram, blocks represent elements of a sequence and lines represent attention visibility. Different colored groups of blocks indicate different Transformer layer stacks.Dark grey lines correspond to fully-visible masking and light grey lines correspond to causal masking. We use "." to denote a special end-of-sequence token that represents the end of a prediction. The input and output sequences are represented as x and y respectively. Left: A standard encoder-decoder architecture uses fully-visible masking in the encoder and the encoder-decoder attention, with causal masking in the decoder. Middle: A language model consists of a single Transformer layer stack and is fed the concatenation of the input and target, using a causal mask throughout. Right: Adding a prefix to a language model corresponds to allowing fully-visible masking over the input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Pre-training loss for our original C4 dataset as well as 4 artificially truncated versions. The sizes listed refer to the number of tokens in each dataset. The four sizes considered correspond to repeating the dataset between 64 and 4,096 times over the course of pre-training. Using a smaller dataset size results in smaller training loss values, which may suggest some memorization of the unlabeled dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Choice 1 :Choice 2 :</head><label>12</label><figDesc>Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? Processed input: cb hypothesis: Valence was helping premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? Political violence broke out in the nation. Many citizens relocated to the capitol. Many citizens took refuge in other territories. Processed input: copa choice1: Many citizens relocated to the capitol. choice2: Many citizens took refuge in other territories. premise: Political violence broke out in the nation. question: effect Original target: 1 Processed target: True</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>.28 19.24 80.88 71.36 26.98 39.82 27.65</head><label></label><figDesc></figDesc><table><row><cell></cell><cell cols="4">GLUE CNNDM SQuAD SGLUE</cell><cell>EnDe</cell><cell>EnFr</cell><cell>EnRo</cell></row><row><cell cols="2">Baseline average 83Baseline standard deviation 0.235</cell><cell>0.065</cell><cell>0.343</cell><cell>0.416</cell><cell>0.112</cell><cell>0.090</cell><cell>0.108</cell></row><row><cell>No pre-training</cell><cell>66.22</cell><cell>17.60</cell><cell>50.31</cell><cell>53.04</cell><cell>25.86</cell><cell>39.77</cell><cell>24.04</cell></row><row><cell cols="8">Table 1: Average and standard deviation of scores achieved by our baseline model and training</cell></row><row><cell cols="8">procedure. For comparison, we also report performance when training on each task from</cell></row><row><cell cols="8">scratch (i.e. without any pre-training) for the same number of steps used to fine-tune the</cell></row><row><cell cols="8">baseline model. All scores in this table (and every table in our paper except Table 14) are</cell></row><row><cell cols="3">reported on the validation sets of each dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Examples of inputs and targets produced by some of the unsupervised objectives we consider applied to the input text "Thank you for inviting me to your party last week ." Note that all of our objectives process tokenized text. For this particular sentence, all words were mapped to a single token by our vocabulary. We write (original text) as a target to denote that the model is tasked with reconstructing the entire input text. &lt;M&gt; denotes a shared mask token and &lt;X&gt;, &lt;Y&gt;, and &lt;Z&gt; denote sentinel tokens that are assigned unique token IDs. The BERT-style objective (second row) includes a corruption where some tokens are replaced by a random token ID; we show this via the greyed-out word apple.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance of the three disparate pre-training objectives described in Section 3.3.1.</figDesc><table><row><cell>Objective</cell><cell cols="4">GLUE CNNDM SQuAD SGLUE</cell><cell>EnDe</cell><cell>EnFr</cell><cell>EnRo</cell></row><row><cell>Prefix language modeling</cell><cell>80.69</cell><cell>18.94</cell><cell>77.99</cell><cell>65.27</cell><cell>26.86</cell><cell>39.73</cell><cell>27.49</cell></row><row><cell>BERT-style [Devlin et al., 2018]</cell><cell>82.96</cell><cell>19.17</cell><cell>80.65</cell><cell>69.85</cell><cell cols="3">26.78 40.03 27.41</cell></row><row><cell>Deshuffling</cell><cell>73.17</cell><cell>18.59</cell><cell>67.61</cell><cell>58.47</cell><cell>26.11</cell><cell>39.30</cell><cell>25.62</cell></row><row><cell>Objective</cell><cell cols="4">GLUE CNNDM SQuAD SGLUE</cell><cell>EnDe</cell><cell>EnFr</cell><cell>EnRo</cell></row><row><cell>BERT-style [Devlin et al., 2018]</cell><cell>82.96</cell><cell>19.17</cell><cell>80.65</cell><cell>69.85</cell><cell>26.78</cell><cell>40.03</cell><cell>27.41</cell></row><row><cell>MASS-style [Song et al., 2019]</cell><cell>82.32</cell><cell>19.16</cell><cell>80.10</cell><cell>69.28</cell><cell>26.79</cell><cell>39.89</cell><cell>27.55</cell></row><row><cell>Replace corrupted spans</cell><cell>83.28</cell><cell>19.24</cell><cell>80.88</cell><cell>71.36</cell><cell>26.98</cell><cell>39.82</cell><cell>27.65</cell></row><row><cell>Drop corrupted tokens</cell><cell>84.44</cell><cell>19.31</cell><cell>80.52</cell><cell>68.67</cell><cell>27.07</cell><cell>39.76</cell><cell>27.82</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison of variants of the BERT-style pre-training objective. In the first two variants, the model is trained to reconstruct the original uncorrupted text segment. In the latter two, the model only predicts the sequence of corrupted tokens.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Performance of the i.i.d. corruption objective with different corruption rates.</figDesc><table><row><cell cols="5">Corruption rate GLUE CNNDM SQuAD SGLUE</cell><cell>EnDe</cell><cell>EnFr</cell><cell>EnRo</cell></row><row><cell>10%</cell><cell>82.82</cell><cell>19.00</cell><cell>80.38</cell><cell>69.55</cell><cell>26.87</cell><cell>39.28</cell><cell>27.44</cell></row><row><cell>15%</cell><cell>83.28</cell><cell>19.24</cell><cell>80.88</cell><cell>71.36</cell><cell cols="3">26.98 39.82 27.65</cell></row><row><cell>25%</cell><cell>83.00</cell><cell>19.54</cell><cell>80.96</cell><cell>70.48</cell><cell cols="3">27.04 39.83 27.47</cell></row><row><cell>50%</cell><cell>81.27</cell><cell>19.32</cell><cell>79.80</cell><cell>70.33</cell><cell cols="3">27.01 39.90 27.49</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Performance resulting from pre-training on different datasets. The first four variants are based on our new C4 dataset.</figDesc><table><row><cell>Dataset</cell><cell>Size</cell><cell cols="4">GLUE CNNDM SQuAD SGLUE</cell><cell>EnDe</cell><cell>EnFr</cell><cell>EnRo</cell></row><row><cell>C4</cell><cell>745GB</cell><cell>83.28</cell><cell>19.24</cell><cell>80.88</cell><cell>71.36</cell><cell cols="3">26.98 39.82 27.65</cell></row><row><cell>C4, unfiltered</cell><cell>6.1TB</cell><cell>81.46</cell><cell>19.14</cell><cell>78.78</cell><cell>68.04</cell><cell>26.55</cell><cell>39.34</cell><cell>27.21</cell></row><row><cell>RealNews-like</cell><cell>35GB</cell><cell>83.83</cell><cell>19.23</cell><cell>80.39</cell><cell>72.38</cell><cell cols="3">26.75 39.90 27.48</cell></row><row><cell>WebText-like</cell><cell>17GB</cell><cell>84.03</cell><cell>19.31</cell><cell>81.42</cell><cell>71.40</cell><cell cols="3">26.80 39.74 27.59</cell></row><row><cell>Wikipedia</cell><cell>16GB</cell><cell>81.85</cell><cell>19.31</cell><cell>81.29</cell><cell>68.01</cell><cell>26.94</cell><cell>39.69</cell><cell>27.67</cell></row><row><cell>Wikipedia + TBC</cell><cell>20GB</cell><cell>83.65</cell><cell>19.28</cell><cell>82.08</cell><cell>73.24</cell><cell>26.77</cell><cell>39.63</cell><cell>27.57</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Measuring the effect of artificially shrinking our C4 dataset. This results in the dataset being repeated over the course of pre-training, which may result in memorization (see Figure</figDesc><table><row><cell cols="6">Number of tokens Repeats GLUE CNNDM SQuAD SGLUE</cell><cell>EnDe</cell><cell>EnFr</cell><cell>EnRo</cell></row><row><cell>Full dataset</cell><cell>0</cell><cell>83.28</cell><cell>19.24</cell><cell>80.88</cell><cell>71.36</cell><cell cols="3">26.98 39.82 27.65</cell></row><row><cell>2 29</cell><cell>64</cell><cell>82.87</cell><cell>19.19</cell><cell>80.97</cell><cell>72.03</cell><cell cols="3">26.83 39.74 27.63</cell></row><row><cell>2 27</cell><cell>256</cell><cell>82.62</cell><cell>19.20</cell><cell>79.78</cell><cell>69.97</cell><cell cols="2">27.02 39.71</cell><cell>27.33</cell></row><row><cell>2 25</cell><cell>1,024</cell><cell>79.55</cell><cell>18.57</cell><cell>76.27</cell><cell>64.76</cell><cell>26.38</cell><cell>39.56</cell><cell>26.80</cell></row><row><cell>2 23</cell><cell>4,096</cell><cell>76.34</cell><cell>18.33</cell><cell>70.92</cell><cell>59.29</cell><cell>26.37</cell><cell>38.84</cell><cell>25.81</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Comparison of different alternative fine-tuning methods that only update a subset of the model's parameters. For adapter layers, d refers to the inner dimensionality of the adapters.</figDesc><table><row><cell>tuning method</cell><cell cols="4">GLUE CNNDM SQuAD SGLUE</cell><cell>EnDe</cell><cell>EnFr</cell><cell>EnRo</cell></row><row><cell>All parameters</cell><cell>83.28</cell><cell>19.24</cell><cell>80.88</cell><cell>71.36</cell><cell cols="3">26.98 39.82 27.65</cell></row><row><cell>Adapter layers, d = 32</cell><cell>80.52</cell><cell>15.08</cell><cell>79.32</cell><cell>60.40</cell><cell>13.84</cell><cell>17.88</cell><cell>15.54</cell></row><row><cell>Adapter layers, d = 128</cell><cell>81.51</cell><cell>16.62</cell><cell>79.47</cell><cell>63.03</cell><cell>19.83</cell><cell>27.50</cell><cell>22.63</cell></row><row><cell>Adapter layers, d = 512</cell><cell>81.54</cell><cell>17.78</cell><cell>79.18</cell><cell>64.30</cell><cell>23.45</cell><cell>33.98</cell><cell>25.81</cell></row><row><cell>Adapter layers, d = 2048</cell><cell>81.51</cell><cell>16.62</cell><cell>79.47</cell><cell>63.03</cell><cell>19.83</cell><cell>27.50</cell><cell>22.63</cell></row><row><cell>Gradual unfreezing</cell><cell>82.50</cell><cell>18.95</cell><cell>79.17</cell><cell>70.79</cell><cell>26.71</cell><cell>39.02</cell><cell>26.93</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Comparison of multi-task training using different mixing strategies. Examplesproportional mixing refers to sampling examples from each dataset according to the total size of each dataset, with an artificial limit (K) on the maximum dataset size. Temperature-scaled mixing re-scales the sampling rates by a temperature T . For temperature-scaled mixing, we use an artificial dataset size limit of K = 2 21 .</figDesc><table><row><cell>Mixing strategy</cell><cell cols="4">GLUE CNNDM SQuAD SGLUE</cell><cell>EnDe</cell><cell>EnFr</cell><cell>EnRo</cell></row><row><cell>Baseline (pre-train/fine-tine)</cell><cell>83.28</cell><cell>19.24</cell><cell>80.88</cell><cell>71.36</cell><cell cols="3">26.98 39.82 27.65</cell></row><row><cell>Equal</cell><cell>76.13</cell><cell>19.02</cell><cell>76.51</cell><cell>63.37</cell><cell>23.89</cell><cell>34.31</cell><cell>26.78</cell></row><row><cell>Examples-proportional, K = 2 16</cell><cell>80.45</cell><cell>19.04</cell><cell>77.25</cell><cell>69.95</cell><cell>24.35</cell><cell>34.99</cell><cell>27.10</cell></row><row><cell>Examples-proportional, K = 2 17</cell><cell>81.56</cell><cell>19.12</cell><cell>77.00</cell><cell>67.91</cell><cell>24.36</cell><cell>35.00</cell><cell>27.25</cell></row><row><cell>Examples-proportional, K = 2 18</cell><cell>81.67</cell><cell>19.07</cell><cell>78.17</cell><cell>67.94</cell><cell>24.57</cell><cell>35.19</cell><cell>27.39</cell></row><row><cell>Examples-proportional, K = 2 19</cell><cell>81.42</cell><cell>19.24</cell><cell>79.78</cell><cell>67.30</cell><cell>25.21</cell><cell>36.30</cell><cell>27.76</cell></row><row><cell>Examples-proportional, K = 2 20</cell><cell>80.80</cell><cell>19.24</cell><cell>80.36</cell><cell>67.38</cell><cell>25.66</cell><cell>36.93</cell><cell>27.68</cell></row><row><cell>Examples-proportional, K = 2 21</cell><cell>79.83</cell><cell>18.79</cell><cell>79.50</cell><cell>65.10</cell><cell>25.82</cell><cell>37.22</cell><cell>27.13</cell></row><row><cell>Temperature-scaled, T = 2</cell><cell>81.90</cell><cell>19.28</cell><cell>79.42</cell><cell>69.92</cell><cell>25.42</cell><cell>36.72</cell><cell>27.20</cell></row><row><cell>Temperature-scaled, T = 4</cell><cell>80.56</cell><cell>19.22</cell><cell>77.99</cell><cell>69.54</cell><cell>25.04</cell><cell>35.82</cell><cell>27.45</cell></row><row><cell>Temperature-scaled, T = 8</cell><cell>77.21</cell><cell>19.10</cell><cell>77.14</cell><cell>66.07</cell><cell>24.55</cell><cell>35.35</cell><cell>27.17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>Comparison of unsupervised pre-training, multi-task learning, and various forms of multi-task pre-training.</figDesc><table><row><cell>Training strategy</cell><cell cols="4">GLUE CNNDM SQuAD SGLUE</cell><cell>EnDe</cell><cell>EnFr</cell><cell>EnRo</cell></row><row><cell>Unsupervised pre-training + fine-tuning</cell><cell>83.28</cell><cell>19.24</cell><cell>80.88</cell><cell>71.36</cell><cell>26.98</cell><cell>39.82</cell><cell>27.65</cell></row><row><cell>Multi-task training</cell><cell>81.42</cell><cell>19.24</cell><cell>79.78</cell><cell>67.30</cell><cell>25.21</cell><cell>36.30</cell><cell>27.76</cell></row><row><cell>Multi-task pre-training + fine-tuning</cell><cell>83.11</cell><cell>19.12</cell><cell>80.26</cell><cell>71.03</cell><cell>27.08</cell><cell>39.80</cell><cell>28.07</cell></row><row><cell>Leave-one-out multi-task training</cell><cell>81.98</cell><cell>19.05</cell><cell>79.97</cell><cell>71.68</cell><cell>26.93</cell><cell>39.79</cell><cell>27.87</cell></row><row><cell>Supervised multi-task pre-training</cell><cell>79.93</cell><cell>18.96</cell><cell>77.38</cell><cell>65.36</cell><cell>26.81</cell><cell cols="2">40.13 28.04</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 13 :</head><label>13</label><figDesc>Comparison of different methods of scaling up our baseline model. All methods except ensembling fine-tuned models use 4? the computation as the baseline. "Size" refers to the number of parameters in the model and "training time" refers to the number of steps used for both pre-training and fine-tuning.</figDesc><table><row><cell>Scaling strategy</cell><cell cols="4">GLUE CNNDM SQuAD SGLUE</cell><cell>EnDe</cell><cell>EnFr</cell><cell>EnRo</cell></row><row><cell>Baseline</cell><cell>83.28</cell><cell>19.24</cell><cell>80.88</cell><cell>71.36</cell><cell>26.98</cell><cell>39.82</cell><cell>27.65</cell></row><row><cell>1? size, 4? training steps</cell><cell>85.33</cell><cell>19.33</cell><cell>82.45</cell><cell>74.72</cell><cell>27.08</cell><cell>40.66</cell><cell>27.93</cell></row><row><cell>1? size, 4? batch size</cell><cell>84.60</cell><cell>19.42</cell><cell>82.52</cell><cell>74.64</cell><cell>27.07</cell><cell>40.60</cell><cell>27.84</cell></row><row><cell>2? size, 2? training steps</cell><cell>86.18</cell><cell>19.66</cell><cell>84.18</cell><cell>77.18</cell><cell>27.52</cell><cell>41.03</cell><cell>28.19</cell></row><row><cell>4? size, 1? training steps</cell><cell>85.91</cell><cell>19.73</cell><cell>83.86</cell><cell>78.04</cell><cell>27.47</cell><cell>40.71</cell><cell>28.10</cell></row><row><cell>4? ensembled</cell><cell>84.77</cell><cell>20.10</cell><cell>83.09</cell><cell>71.74</cell><cell>28.05</cell><cell>40.53</cell><cell>28.57</cell></row><row><cell>4? ensembled, fine-tune only</cell><cell>84.05</cell><cell>19.57</cell><cell>82.36</cell><cell>71.55</cell><cell>27.55</cell><cell>40.22</cell><cell>28.09</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Since our baseline uses a BERT BASE -sized encoder and decoder, we also consider a variant where the encoder and decoder are both similar in size and structure to BERT LARGE . Specifically, this variant uses d model = 1,024, d ff = 4,096, d kv = 64, 16-headed attention, and 12 layers each in the encoder and decoder, resulting in around 770 million parameters. ? 3B and 11B. To further explore what kind of performance is possible when using larger models, we consider two additional variants. In both cases, we use d model = 1024, a 24 layer encoder and decoder, and d kv = 128. For the "3B" variant, we use d ff = 16,384 with 32-headed attention, which results in around 2.8 billion parameters; for "11B" we use d ff = 65,536 with 128-headed attention producing a model with about 11 billion parameters. We chose to scale up d ff specifically because modern accelerators (such as the TPUs we train our models on) are most efficient for large dense matrix multiplications like those in the Transformer's feed-forward networks.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>Ce d?tail d'une photographie infrarouge prise par le t?lescope Spitzer montre un "portrait de famille" des innombrables g?n?rations d'?toiles: les plus vieilles ?toiles sont en bleu et les points roses, plus difficiles ? identifier, sont les "nouveau-n?s" dans la salle d'accouchement de l'univers. Taco Bell said it plans to add 2,000 locations in the US by 2022.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GLUE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SuperGLUE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>WMT</cell></row><row><cell></cell><cell></cell><cell>Score</cell><cell cols="6">CoLA SST-2 MRPC MRPC STSB STSB</cell><cell>QQP</cell><cell>QQP</cell><cell>MNLIm</cell><cell>MNLImm</cell><cell>QNLI</cell><cell>RTE</cell><cell></cell><cell>CNN/DM</cell><cell></cell><cell cols="2">SQuAD</cell><cell>Score</cell><cell>BoolQ</cell><cell>CB</cell><cell>CB</cell><cell cols="5">COPA MultiRC MultiRC ReCoRD ReCoRD</cell><cell>RTE</cell><cell>WiC</cell><cell>WSC</cell><cell>EnDe</cell><cell>EnFr</cell><cell>EnRo</cell></row><row><cell cols="2">Table Experiment</cell><cell>Average</cell><cell>MCC</cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>PCC</cell><cell>SCC</cell><cell>F1</cell><cell>Acc</cell><cell>Acc</cell><cell>Acc</cell><cell>Acc</cell><cell>Acc</cell><cell cols="3">R-1-F R-2-F R-L-F</cell><cell>EM</cell><cell>F1</cell><cell>Average</cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>Acc</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>Acc</cell><cell>Acc</cell><cell>Acc</cell><cell>BLEU BLEU BLEU</cell></row><row><cell cols="7">1 Original target: Ce d?tail d'une photographie infrarouge prise par le t?lescope Spitzer Baseline average 83.28 53.84 92.68 92.07 88.92 1 Baseline standard deviation 0.235 1.111 0.569 0.729 1.019 montre un "portrait de famille" des innombrables g?n?rations d'?toiles: les plus 1 No pre-training 66.22 12.29 80.62 81.42 73.04</cell><cell>88.02 0.374 72.58</cell><cell>87.94 0.418 72.97</cell><cell cols="2">88.67 91.56 0.108 0.070 81.94 86.62</cell><cell>84.24 0.291 68.02</cell><cell>84.57 0.231 67.98</cell><cell>90.48 0.361 75.69</cell><cell>76.28 1.393 58.84</cell><cell>41.33 0.065 39.19</cell><cell>19.24 0.065 17.60</cell><cell>38.77 0.058 36.69</cell><cell cols="2">80.88 88.81 0.343 0.226 50.31 61.97</cell><cell>71.36 0.416 53.04</cell><cell>76.62 0.365 65.38</cell><cell cols="2">91.22 91.96 3.237 2.560 71.61 76.79</cell><cell>66.20 2.741 62.00</cell><cell>66.13 0.716 59.10</cell><cell>25.78 1.011 0.84</cell><cell>69.05 0.370 20.33</cell><cell>68.16 0.379 17.95</cell><cell cols="2">75.34 68.04 1.228 0.850 54.15 54.08</cell><cell>78.56 2.029 65.38</cell><cell>26.98 0.112 25.86</cell><cell>39.82 0.090 39.77</cell><cell>27.65 0.108 24.04</cell></row><row><cell cols="8">vieilles ?toiles sont en bleu et les points roses, plus difficiles ? identifier, sont 2 Enc/dec, denoising 83.28 53.84 92.68 92.07 88.92 88.02 les "nouveau-n?s" dans la salle d'accouchement de l'univers. 2 Enc/dec, shared, denoising 82.81 55.24 91.86 91.58 88.24 87.43</cell><cell>87.94 87.58</cell><cell cols="2">88.67 91.56 88.69 91.60</cell><cell>84.24 83.88</cell><cell>84.57 84.01</cell><cell>90.48 90.23</cell><cell>76.28 73.65</cell><cell>41.33 41.11</cell><cell>19.24 18.78</cell><cell>38.77 38.48</cell><cell cols="2">80.88 88.81 80.63 88.49</cell><cell>71.36 70.73</cell><cell>76.62 77.13</cell><cell cols="2">91.22 91.96 95.04 96.43</cell><cell>66.20 65.00</cell><cell>66.13 66.16</cell><cell>25.78 22.98</cell><cell>69.05 68.95</cell><cell>68.16 68.09</cell><cell cols="2">75.34 68.04 70.76 68.18</cell><cell>78.56 75.96</cell><cell>26.98 26.72</cell><cell>39.82 39.03</cell><cell>27.65 27.46</cell></row><row><cell cols="2">2 2 2 2 2 2 2 Processed target: WMT English to Romanian Enc/dec, 6 layers, denoising Language model, denoising Prefix LM, denoising Enc/dec, LM Enc/dec, shared, LM Enc/dec, 6 layers, LM Language model, LM 2 Prefix LM, LM</cell><cell>80.88 74.70 81.82 79.56 79.60 78.67 73.78 79.68</cell><cell>46.26 24.50 49.99 42.03 44.83 38.72 28.53 41.26</cell><cell>92.09 90.60 92.43 91.86 92.09 91.40 89.79 92.09</cell><cell>91.51 86.08 91.43 91.64 90.20 90.40 85.23 90.11</cell><cell>87.99 78.92 88.24 88.24 85.78 86.52 78.68 86.27</cell><cell>87.01 85.22 87.20 87.13 86.03 86.82 84.22 86.82</cell><cell>86.76 85.42 86.98 87.00 85.87 86.49 84.00 86.32</cell><cell cols="2">87.93 90.97 85.40 88.99 88.41 91.39 88.21 91.15 87.77 91.02 87.87 91.03 84.88 88.70 88.35 91.35</cell><cell>82.20 76.72 82.32 81.68 81.74 80.99 74.94 81.71</cell><cell>82.41 77.05 82.93 81.66 82.29 80.92 75.77 82.02</cell><cell>88.83 86.02 88.71 88.54 89.16 88.05 84.84 89.04</cell><cell>71.48 64.62 74.01 65.70 65.34 65.70 58.84 68.59</cell><cell>40.83 39.49 40.46 40.67 40.16 40.29 38.97 39.66</cell><cell>18.97 17.93 18.61 18.59 18.13 18.26 17.54 17.84</cell><cell>38.31 36.91 37.90 38.13 37.59 37.70 36.37 37.13</cell><cell cols="2">77.59 86.07 61.14 71.37 78.94 87.31 76.02 84.85 76.35 84.86 75.32 84.06 53.81 64.55 76.87 85.39</cell><cell>68.42 55.02 68.11 64.29 63.50 64.06 56.51 64.86</cell><cell>73.79 65.47 75.50 72.23 70.49 71.38 64.22 71.47</cell><cell cols="2">91.70 92.86 60.08 71.43 93.37 91.07 85.74 89.29 91.41 87.50 85.25 89.29 59.92 71.43 93.37 91.07</cell><cell>67.00 58.00 60.00 57.00 55.00 60.00 64.00 57.00</cell><cell>61.02 43.03 63.43 60.53 60.21 57.56 53.04 58.67</cell><cell>19.62 2.94 21.20 16.26 16.89 16.79 1.05 16.89</cell><cell>61.26 53.35 65.03 59.28 57.83 55.22 46.81 59.25</cell><cell>60.33 52.31 64.11 58.30 56.73 54.30 45.78 58.16</cell><cell cols="2">72.20 65.99 53.07 58.62 71.48 65.67 65.34 64.89 63.54 63.48 66.79 63.95 58.84 56.74 64.26 66.30</cell><cell>75.00 63.46 73.08 70.19 70.19 71.15 69.23 71.15</cell><cell>26.38 25.09 26.43 26.27 26.62 26.13 25.23 26.28</cell><cell>38.40 35.28 37.98 39.17 39.17 38.42 34.31 37.51</cell><cell>26.95 25.86 27.39 26.86 27.05 26.89 25.38 26.76</cell></row><row><cell cols="7">4 4 4 Original input: Processed input: translate English to Romanian: Taco Bell said it plans to add 2,000 Language modeling with prefix 80.69 44.22 93.00 91.68 88.48 BERT-style [Devlin et al., 2018] 82.96 52.49 92.55 92.79 89.95 Deshuffling 73.17 22.82 87.16 86.88 81.13 5 BERT-style [Devlin et al., 2018] 82.96 52.49 92.55 92.79 89.95 locations in the US by 2022. 5 MASS-style [Song et al., 2019] 82.32 47.01 91.63 92.53 89.71</cell><cell>87.20 87.68 84.03 87.68 88.21</cell><cell>87.18 87.66 83.82 87.66 88.18</cell><cell cols="2">88.39 91.41 88.47 91.44 86.38 89.90 88.47 91.44 88.58 91.44</cell><cell>82.66 83.60 76.30 83.60 82.96</cell><cell>83.09 84.05 76.34 84.05 83.67</cell><cell>89.29 90.33 84.18 90.33 90.02</cell><cell>68.95 75.45 58.84 75.45 77.26</cell><cell>40.71 41.27 40.75 41.27 41.16</cell><cell>18.94 19.17 18.59 19.17 19.16</cell><cell>38.15 38.72 38.10 38.72 38.55</cell><cell cols="2">77.99 86.43 80.65 88.24 67.61 76.76 80.65 88.24 80.10 88.07</cell><cell>65.27 69.85 58.47 69.85 69.28</cell><cell>73.55 76.48 69.17 76.48 75.08</cell><cell cols="2">83.95 87.50 94.37 94.64 63.70 78.57 94.37 94.64 84.98 89.29</cell><cell>55.00 61.00 56.00 61.00 63.00</cell><cell>59.65 63.29 59.85 63.29 64.46</cell><cell>18.89 25.08 12.70 25.08 23.50</cell><cell>61.76 66.76 45.52 66.76 66.71</cell><cell>60.76 65.85 44.36 65.85 65.91</cell><cell cols="2">68.59 65.67 72.20 69.12 57.04 64.89 72.20 69.12 72.20 67.71</cell><cell>73.08 75.00 68.27 75.00 78.85</cell><cell>26.86 26.78 26.11 26.78 26.79</cell><cell>39.73 40.03 39.30 40.03 39.89</cell><cell>27.49 27.41 25.62 27.41 27.55</cell></row><row><cell>5</cell><cell>Replace corrupted spans</cell><cell>83.28</cell><cell>53.84</cell><cell>92.68</cell><cell>92.07</cell><cell>88.92</cell><cell>88.02</cell><cell>87.94</cell><cell cols="2">88.67 91.56</cell><cell>84.24</cell><cell>84.57</cell><cell>90.48</cell><cell>76.28</cell><cell>41.33</cell><cell>19.24</cell><cell>38.77</cell><cell cols="2">80.88 88.81</cell><cell>71.36</cell><cell>76.62</cell><cell cols="2">91.22 91.96</cell><cell>66.20</cell><cell>66.13</cell><cell>25.78</cell><cell>69.05</cell><cell>68.16</cell><cell cols="2">75.34 68.04</cell><cell>78.56</cell><cell>26.98</cell><cell>39.82</cell><cell>27.65</cell></row><row><cell>5</cell><cell>Drop corrupted tokens</cell><cell>84.44</cell><cell>60.04</cell><cell>92.89</cell><cell>92.79</cell><cell>89.95</cell><cell>87.28</cell><cell>86.85</cell><cell cols="2">88.56 91.54</cell><cell>83.94</cell><cell>83.92</cell><cell>90.74</cell><cell>79.42</cell><cell>41.27</cell><cell>19.31</cell><cell>38.70</cell><cell cols="2">80.52 88.28</cell><cell>68.67</cell><cell>75.90</cell><cell cols="2">96.02 94.64</cell><cell>56.00</cell><cell>65.06</cell><cell>23.92</cell><cell>65.54</cell><cell>64.60</cell><cell cols="2">71.12 67.40</cell><cell>74.04</cell><cell>27.07</cell><cell>39.76</cell><cell>27.82</cell></row><row><cell>6</cell><cell>Corruption rate = 10%</cell><cell>82.82</cell><cell>52.71</cell><cell>92.09</cell><cell>91.55</cell><cell>88.24</cell><cell>88.19</cell><cell>88.15</cell><cell cols="2">88.47 91.40</cell><cell>83.50</cell><cell>84.51</cell><cell>90.33</cell><cell>75.45</cell><cell>41.05</cell><cell>19.00</cell><cell>38.53</cell><cell cols="2">80.38 88.36</cell><cell>69.55</cell><cell>74.98</cell><cell cols="2">92.37 92.86</cell><cell>62.00</cell><cell>66.04</cell><cell>24.66</cell><cell>67.93</cell><cell>67.09</cell><cell cols="2">70.76 67.24</cell><cell>75.96</cell><cell>26.87</cell><cell>39.28</cell><cell>27.44</cell></row><row><cell>6</cell><cell>Corruption rate = 15%</cell><cell>83.28</cell><cell>53.84</cell><cell>92.68</cell><cell>92.07</cell><cell>88.92</cell><cell>88.02</cell><cell>87.94</cell><cell cols="2">88.67 91.56</cell><cell>84.24</cell><cell>84.57</cell><cell>90.48</cell><cell>76.28</cell><cell>41.33</cell><cell>19.24</cell><cell>38.77</cell><cell cols="2">80.88 88.81</cell><cell>71.36</cell><cell>76.62</cell><cell cols="2">91.22 91.96</cell><cell>66.20</cell><cell>66.13</cell><cell>25.78</cell><cell>69.05</cell><cell>68.16</cell><cell cols="2">75.34 68.04</cell><cell>78.56</cell><cell>26.98</cell><cell>39.82</cell><cell>27.65</cell></row><row><cell>6</cell><cell>Corruption rate = 25%</cell><cell>83.00</cell><cell>53.47</cell><cell>93.00</cell><cell>92.44</cell><cell>89.46</cell><cell>87.36</cell><cell>87.36</cell><cell cols="2">88.68 91.53</cell><cell>84.44</cell><cell>84.15</cell><cell>90.77</cell><cell>74.01</cell><cell>41.69</cell><cell>19.54</cell><cell>39.14</cell><cell cols="2">80.96 88.61</cell><cell>70.48</cell><cell>76.39</cell><cell cols="2">93.02 92.86</cell><cell>68.00</cell><cell>65.46</cell><cell>24.66</cell><cell>68.20</cell><cell>67.39</cell><cell cols="2">73.65 67.87</cell><cell>72.12</cell><cell>27.04</cell><cell>39.83</cell><cell>27.47</cell></row><row><cell>6</cell><cell>Corruption rate = 50%</cell><cell>81.27</cell><cell>46.26</cell><cell>91.63</cell><cell>91.11</cell><cell>87.99</cell><cell>87.87</cell><cell>87.64</cell><cell cols="2">88.70 91.57</cell><cell>83.64</cell><cell>84.10</cell><cell>90.24</cell><cell>70.76</cell><cell>41.51</cell><cell>19.32</cell><cell>38.89</cell><cell cols="2">79.80 87.76</cell><cell>70.33</cell><cell>75.02</cell><cell cols="2">93.05 92.86</cell><cell>68.00</cell><cell>62.97</cell><cell>24.13</cell><cell>64.94</cell><cell>64.13</cell><cell cols="2">72.20 68.50</cell><cell>77.88</cell><cell>27.01</cell><cell>39.90</cell><cell>27.49</cell></row><row><cell>7</cell><cell>Baseline (i.i.d.)</cell><cell>83.28</cell><cell>53.84</cell><cell>92.68</cell><cell>92.07</cell><cell>88.92</cell><cell>88.02</cell><cell>87.94</cell><cell cols="2">88.67 91.56</cell><cell>84.24</cell><cell>84.57</cell><cell>90.48</cell><cell>76.28</cell><cell>41.33</cell><cell>19.24</cell><cell>38.77</cell><cell cols="2">80.88 88.81</cell><cell>71.36</cell><cell>76.62</cell><cell cols="2">91.22 91.96</cell><cell>66.20</cell><cell>66.13</cell><cell>25.78</cell><cell>69.05</cell><cell>68.16</cell><cell cols="2">75.34 68.04</cell><cell>78.56</cell><cell>26.98</cell><cell>39.82</cell><cell>27.65</cell></row><row><cell>7</cell><cell>Average span length = 2</cell><cell>83.54</cell><cell>53.82</cell><cell>92.20</cell><cell>93.05</cell><cell>90.44</cell><cell>87.85</cell><cell>87.71</cell><cell cols="2">88.42 91.40</cell><cell>84.28</cell><cell>84.46</cell><cell>90.88</cell><cell>77.62</cell><cell>41.23</cell><cell>19.39</cell><cell>38.69</cell><cell cols="2">82.09 89.69</cell><cell>72.20</cell><cell>77.06</cell><cell cols="2">90.43 91.07</cell><cell>70.00</cell><cell>66.28</cell><cell>26.13</cell><cell>71.34</cell><cell>70.61</cell><cell cols="2">75.45 68.34</cell><cell>78.85</cell><cell>26.76</cell><cell>39.99</cell><cell>27.63</cell></row><row><cell>7</cell><cell>Average span length = 3</cell><cell>83.49</cell><cell>53.90</cell><cell>92.43</cell><cell>92.25</cell><cell>89.46</cell><cell>87.49</cell><cell>87.53</cell><cell cols="2">88.72 91.51</cell><cell>84.85</cell><cell>84.84</cell><cell>90.99</cell><cell>77.26</cell><cell>41.50</cell><cell>19.62</cell><cell>38.94</cell><cell cols="2">81.84 89.66</cell><cell>72.53</cell><cell>76.85</cell><cell cols="2">94.37 94.64</cell><cell>70.00</cell><cell>67.64</cell><cell>28.75</cell><cell>70.84</cell><cell>69.90</cell><cell cols="2">74.73 67.71</cell><cell>77.88</cell><cell>26.86</cell><cell>39.65</cell><cell>27.62</cell></row><row><cell>7</cell><cell>Average span length = 5</cell><cell>83.40</cell><cell>52.12</cell><cell>93.12</cell><cell>92.63</cell><cell>89.71</cell><cell>88.70</cell><cell>88.47</cell><cell cols="2">88.84 91.64</cell><cell>84.32</cell><cell>84.29</cell><cell>90.79</cell><cell>76.90</cell><cell>41.39</cell><cell>19.24</cell><cell>38.82</cell><cell cols="2">82.05 89.79</cell><cell>72.23</cell><cell>77.06</cell><cell cols="2">83.06 89.29</cell><cell>69.00</cell><cell>68.16</cell><cell>30.12</cell><cell>71.36</cell><cell>70.53</cell><cell cols="2">75.81 69.91</cell><cell>79.81</cell><cell>26.88</cell><cell>39.40</cell><cell>27.53</cell></row><row><cell>7</cell><cell>Average span length = 10</cell><cell>82.85</cell><cell>50.11</cell><cell>92.09</cell><cell>91.95</cell><cell>88.97</cell><cell>88.45</cell><cell>88.22</cell><cell cols="2">88.86 91.63</cell><cell>84.34</cell><cell>84.28</cell><cell>91.07</cell><cell>76.17</cell><cell>41.38</cell><cell>19.33</cell><cell>38.80</cell><cell cols="2">81.84 89.39</cell><cell>70.44</cell><cell>76.45</cell><cell cols="2">87.40 89.29</cell><cell>65.00</cell><cell>66.87</cell><cell>29.59</cell><cell>69.82</cell><cell>68.94</cell><cell cols="2">72.56 67.55</cell><cell>75.96</cell><cell>26.79</cell><cell>39.49</cell><cell>27.69</cell></row><row><cell>8</cell><cell>C4</cell><cell>83.28</cell><cell>53.84</cell><cell>92.68</cell><cell>92.07</cell><cell>88.92</cell><cell>88.02</cell><cell>87.94</cell><cell cols="2">88.67 91.56</cell><cell>84.24</cell><cell>84.57</cell><cell>90.48</cell><cell>76.28</cell><cell>41.33</cell><cell>19.24</cell><cell>38.77</cell><cell cols="2">80.88 88.81</cell><cell>71.36</cell><cell>76.62</cell><cell cols="2">91.22 91.96</cell><cell>66.20</cell><cell>66.13</cell><cell>25.78</cell><cell>69.05</cell><cell>68.16</cell><cell cols="2">75.34 68.04</cell><cell>78.56</cell><cell>26.98</cell><cell>39.82</cell><cell>27.65</cell></row><row><cell>8</cell><cell>C4, unfiltered</cell><cell>81.46</cell><cell>48.01</cell><cell>91.63</cell><cell>92.72</cell><cell>89.95</cell><cell>87.79</cell><cell>87.60</cell><cell cols="2">88.31 91.27</cell><cell>82.30</cell><cell>82.34</cell><cell>88.71</cell><cell>72.20</cell><cell>41.09</cell><cell>19.14</cell><cell>38.54</cell><cell cols="2">78.78 87.04</cell><cell>68.04</cell><cell>75.75</cell><cell cols="2">89.17 91.07</cell><cell>62.00</cell><cell>65.52</cell><cell>25.60</cell><cell>62.42</cell><cell>61.58</cell><cell cols="2">69.68 67.08</cell><cell>72.12</cell><cell>26.55</cell><cell>39.34</cell><cell>27.21</cell></row><row><cell>8</cell><cell>RealNews-like</cell><cell>83.83</cell><cell>56.55</cell><cell>92.66</cell><cell>92.06</cell><cell>88.97</cell><cell>87.71</cell><cell>87.37</cell><cell cols="2">88.51 91.49</cell><cell>84.35</cell><cell>84.46</cell><cell>90.61</cell><cell>78.34</cell><cell>41.38</cell><cell>19.23</cell><cell>38.84</cell><cell cols="2">80.39 88.50</cell><cell>72.38</cell><cell>77.00</cell><cell cols="2">93.09 94.64</cell><cell>66.00</cell><cell>65.92</cell><cell>23.82</cell><cell>74.56</cell><cell>73.72</cell><cell cols="2">75.81 66.61</cell><cell>80.77</cell><cell>26.75</cell><cell>39.90</cell><cell>27.48</cell></row><row><cell>8</cell><cell>WebText-like</cell><cell>84.03</cell><cell>56.38</cell><cell>93.12</cell><cell>92.31</cell><cell>89.22</cell><cell>88.69</cell><cell>88.68</cell><cell cols="2">88.65 91.56</cell><cell>84.70</cell><cell>84.84</cell><cell>90.83</cell><cell>77.62</cell><cell>41.23</cell><cell>19.31</cell><cell>38.70</cell><cell cols="2">81.42 89.15</cell><cell>71.40</cell><cell>76.88</cell><cell cols="2">83.08 89.29</cell><cell>66.00</cell><cell>64.10</cell><cell>24.24</cell><cell>72.24</cell><cell>71.36</cell><cell cols="2">75.45 68.03</cell><cell>82.69</cell><cell>26.80</cell><cell>39.74</cell><cell>27.59</cell></row><row><cell>8</cell><cell>Wikipedia</cell><cell>81.85</cell><cell>45.53</cell><cell>92.32</cell><cell>91.67</cell><cell>88.24</cell><cell>85.62</cell><cell>86.40</cell><cell cols="2">88.37 91.34</cell><cell>82.61</cell><cell>83.25</cell><cell>90.96</cell><cell>77.26</cell><cell>41.39</cell><cell>19.31</cell><cell>38.81</cell><cell cols="2">81.29 89.18</cell><cell>68.01</cell><cell>76.12</cell><cell cols="2">56.03 80.36</cell><cell>67.00</cell><cell>65.01</cell><cell>25.92</cell><cell>69.03</cell><cell>68.06</cell><cell cols="2">74.73 67.08</cell><cell>76.92</cell><cell>26.94</cell><cell>39.69</cell><cell>27.67</cell></row><row><cell>8</cell><cell>Wikipedia + TBC</cell><cell>83.65</cell><cell>55.53</cell><cell>92.78</cell><cell>92.41</cell><cell>89.22</cell><cell>86.67</cell><cell>86.27</cell><cell cols="2">89.47 92.29</cell><cell>84.38</cell><cell>83.45</cell><cell>91.94</cell><cell>76.90</cell><cell>41.22</cell><cell>19.28</cell><cell>38.67</cell><cell cols="2">82.08 89.70</cell><cell>73.24</cell><cell>76.22</cell><cell cols="2">95.40 92.86</cell><cell>69.00</cell><cell>51.59</cell><cell>50.93</cell><cell>69.53</cell><cell>68.51</cell><cell cols="2">77.62 66.93</cell><cell>81.73</cell><cell>26.77</cell><cell>39.63</cell><cell>27.57</cell></row><row><cell>9</cell><cell>Full dataset</cell><cell>83.28</cell><cell>53.84</cell><cell>92.68</cell><cell>92.07</cell><cell>88.92</cell><cell>88.02</cell><cell>87.94</cell><cell cols="2">88.67 91.56</cell><cell>84.24</cell><cell>84.57</cell><cell>90.48</cell><cell>76.28</cell><cell>41.33</cell><cell>19.24</cell><cell>38.77</cell><cell cols="2">80.88 88.81</cell><cell>71.36</cell><cell>76.62</cell><cell cols="2">91.22 91.96</cell><cell>66.20</cell><cell>66.13</cell><cell>25.78</cell><cell>69.05</cell><cell>68.16</cell><cell cols="2">75.34 68.04</cell><cell>78.56</cell><cell>26.98</cell><cell>39.82</cell><cell>27.65</cell></row><row><cell>9</cell><cell>2 29 (64 repeats)</cell><cell>82.87</cell><cell>53.82</cell><cell>92.78</cell><cell>91.79</cell><cell>88.73</cell><cell>87.56</cell><cell>87.58</cell><cell cols="2">88.73 91.54</cell><cell>84.07</cell><cell>84.21</cell><cell>90.59</cell><cell>73.65</cell><cell>41.18</cell><cell>19.19</cell><cell>38.67</cell><cell cols="2">80.97 88.90</cell><cell>72.03</cell><cell>76.76</cell><cell cols="2">92.96 92.86</cell><cell>66.00</cell><cell>65.11</cell><cell>26.76</cell><cell>69.35</cell><cell>68.49</cell><cell cols="2">75.81 67.24</cell><cell>82.69</cell><cell>26.83</cell><cell>39.74</cell><cell>27.63</cell></row><row><cell>9</cell><cell>2 27 (256 repeats)</cell><cell>82.62</cell><cell>50.60</cell><cell>92.32</cell><cell>92.07</cell><cell>88.73</cell><cell>87.83</cell><cell>87.60</cell><cell cols="2">88.65 91.54</cell><cell>83.43</cell><cell>84.37</cell><cell>90.12</cell><cell>75.81</cell><cell>41.24</cell><cell>19.20</cell><cell>38.70</cell><cell cols="2">79.78 87.63</cell><cell>69.97</cell><cell>75.29</cell><cell cols="2">93.42 91.07</cell><cell>63.00</cell><cell>61.82</cell><cell>23.61</cell><cell>66.27</cell><cell>65.39</cell><cell cols="2">73.65 66.30</cell><cell>80.77</cell><cell>27.02</cell><cell>39.71</cell><cell>27.33</cell></row><row><cell>9</cell><cell>2 25 (1,024 repeats)</cell><cell>79.55</cell><cell>43.84</cell><cell>91.28</cell><cell>89.32</cell><cell>85.05</cell><cell>85.92</cell><cell>85.74</cell><cell cols="2">88.05 91.09</cell><cell>81.29</cell><cell>81.72</cell><cell>87.90</cell><cell>69.31</cell><cell>40.66</cell><cell>18.57</cell><cell>38.13</cell><cell cols="2">76.27 84.58</cell><cell>64.76</cell><cell>72.63</cell><cell cols="2">83.97 82.14</cell><cell>64.00</cell><cell>59.39</cell><cell>17.94</cell><cell>56.94</cell><cell>56.04</cell><cell cols="2">64.98 65.20</cell><cell>73.08</cell><cell>26.38</cell><cell>39.56</cell><cell>26.80</cell></row><row><cell>9</cell><cell>2 23 (4,096 repeats)</cell><cell>76.34</cell><cell>32.68</cell><cell>89.45</cell><cell>89.84</cell><cell>86.03</cell><cell>83.49</cell><cell>83.42</cell><cell cols="2">87.18 90.61</cell><cell>77.80</cell><cell>78.69</cell><cell>85.47</cell><cell>64.62</cell><cell>40.16</cell><cell>18.33</cell><cell>37.66</cell><cell cols="2">70.92 80.20</cell><cell>59.29</cell><cell>69.85</cell><cell cols="2">73.48 73.21</cell><cell>56.00</cell><cell>57.66</cell><cell>14.38</cell><cell>46.69</cell><cell>45.79</cell><cell cols="2">59.57 65.05</cell><cell>68.27</cell><cell>26.37</cell><cell>38.84</cell><cell>25.81</cell></row><row><cell>10</cell><cell>All parameters</cell><cell>83.28</cell><cell>53.84</cell><cell>92.68</cell><cell>92.07</cell><cell>88.92</cell><cell>88.02</cell><cell>87.94</cell><cell cols="2">88.67 91.56</cell><cell>84.24</cell><cell>84.57</cell><cell>90.48</cell><cell>76.28</cell><cell>41.33</cell><cell>19.24</cell><cell>38.77</cell><cell cols="2">80.88 88.81</cell><cell>71.36</cell><cell>76.62</cell><cell cols="2">91.22 91.96</cell><cell>66.20</cell><cell>66.13</cell><cell>25.78</cell><cell>69.05</cell><cell>68.16</cell><cell cols="2">75.34 68.04</cell><cell>78.56</cell><cell>26.98</cell><cell>39.82</cell><cell>27.65</cell></row><row><cell>10</cell><cell>Adapter layers, d = 32</cell><cell>80.52</cell><cell>45.33</cell><cell>91.63</cell><cell>90.59</cell><cell>86.76</cell><cell>88.38</cell><cell>88.06</cell><cell cols="2">86.99 90.26</cell><cell>83.63</cell><cell>83.94</cell><cell>90.72</cell><cell>67.15</cell><cell>34.50</cell><cell>15.08</cell><cell>32.15</cell><cell cols="2">79.32 87.70</cell><cell>60.40</cell><cell>65.32</cell><cell cols="2">50.87 73.21</cell><cell>52.00</cell><cell>58.61</cell><cell>19.41</cell><cell>65.50</cell><cell>64.58</cell><cell cols="2">62.09 64.58</cell><cell>73.08</cell><cell>13.84</cell><cell>17.88</cell><cell>15.54</cell></row><row><cell>10</cell><cell>Adapter layers, d = 128</cell><cell>81.51</cell><cell>45.35</cell><cell>92.89</cell><cell>91.49</cell><cell>88.24</cell><cell>87.73</cell><cell>87.65</cell><cell cols="2">87.73 90.93</cell><cell>83.64</cell><cell>84.09</cell><cell>90.52</cell><cell>72.56</cell><cell>36.71</cell><cell>16.62</cell><cell>34.37</cell><cell cols="2">79.47 87.61</cell><cell>63.03</cell><cell>69.20</cell><cell cols="2">52.21 75.00</cell><cell>56.00</cell><cell>61.08</cell><cell>18.05</cell><cell>67.94</cell><cell>66.97</cell><cell cols="2">68.59 66.77</cell><cell>73.08</cell><cell>19.83</cell><cell>27.50</cell><cell>22.63</cell></row><row><cell>10</cell><cell>Adapter layers, d = 512</cell><cell>81.54</cell><cell>44.25</cell><cell>93.35</cell><cell>91.00</cell><cell>87.25</cell><cell>88.74</cell><cell>88.44</cell><cell cols="2">88.02 91.15</cell><cell>83.08</cell><cell>83.80</cell><cell>89.62</cell><cell>74.37</cell><cell>38.63</cell><cell>17.78</cell><cell>36.25</cell><cell cols="2">79.18 87.32</cell><cell>64.30</cell><cell>73.18</cell><cell cols="2">59.86 71.43</cell><cell>56.00</cell><cell>62.94</cell><cell>18.57</cell><cell>66.56</cell><cell>65.74</cell><cell cols="2">70.76 67.87</cell><cell>74.04</cell><cell>23.45</cell><cell>33.98</cell><cell>25.81</cell></row><row><cell>10</cell><cell>Adapter layers, d = 2048</cell><cell>82.62</cell><cell>49.86</cell><cell>92.55</cell><cell>91.30</cell><cell>87.99</cell><cell>88.46</cell><cell>88.35</cell><cell cols="2">88.36 91.40</cell><cell>83.63</cell><cell>83.18</cell><cell>90.66</cell><cell>76.53</cell><cell>39.44</cell><cell>18.30</cell><cell>37.06</cell><cell cols="2">79.40 87.36</cell><cell>68.61</cell><cell>74.53</cell><cell cols="2">88.00 91.07</cell><cell>58.00</cell><cell>61.10</cell><cell>18.89</cell><cell>66.73</cell><cell>66.06</cell><cell cols="2">73.29 71.16</cell><cell>75.96</cell><cell>25.64</cell><cell>36.92</cell><cell>26.93</cell></row><row><cell>10</cell><cell>Gradual Unfreezing</cell><cell>82.50</cell><cell>51.74</cell><cell>91.97</cell><cell>92.61</cell><cell>89.71</cell><cell>87.27</cell><cell>86.90</cell><cell cols="2">88.26 91.35</cell><cell>83.42</cell><cell>83.49</cell><cell>89.71</cell><cell>75.09</cell><cell>40.88</cell><cell>18.95</cell><cell>38.40</cell><cell cols="2">79.17 87.30</cell><cell>70.79</cell><cell>75.51</cell><cell cols="2">93.09 94.64</cell><cell>70.00</cell><cell>62.03</cell><cell>21.51</cell><cell>65.69</cell><cell>64.79</cell><cell cols="2">72.92 69.12</cell><cell>77.89</cell><cell>26.71</cell><cell>39.02</cell><cell>26.93</cell></row><row><cell>11</cell><cell>Baseline (pre-train/fine-tune)</cell><cell>83.28</cell><cell>53.84</cell><cell>92.68</cell><cell>92.07</cell><cell>88.92</cell><cell>88.02</cell><cell>87.94</cell><cell cols="2">88.67 91.56</cell><cell>84.24</cell><cell>84.57</cell><cell>90.48</cell><cell>76.28</cell><cell>41.33</cell><cell>19.24</cell><cell>38.77</cell><cell cols="2">80.88 88.81</cell><cell>71.36</cell><cell>76.62</cell><cell cols="2">91.22 91.96</cell><cell>66.20</cell><cell>66.13</cell><cell>25.78</cell><cell>69.05</cell><cell>68.16</cell><cell cols="2">75.34 68.04</cell><cell>78.56</cell><cell>26.98</cell><cell>39.82</cell><cell>27.65</cell></row><row><cell>11</cell><cell>Equal</cell><cell>76.13</cell><cell>39.47</cell><cell>90.94</cell><cell>82.90</cell><cell>75.74</cell><cell>78.83</cell><cell>78.44</cell><cell cols="2">86.45 89.71</cell><cell>82.08</cell><cell>82.92</cell><cell>90.13</cell><cell>59.93</cell><cell>40.95</cell><cell>19.02</cell><cell>38.39</cell><cell cols="2">76.51 85.61</cell><cell>63.37</cell><cell>73.06</cell><cell cols="2">82.37 83.93</cell><cell>65.00</cell><cell>60.89</cell><cell>17.52</cell><cell>60.51</cell><cell>59.70</cell><cell cols="2">61.01 60.03</cell><cell>65.38</cell><cell>23.89</cell><cell>34.31</cell><cell>26.78</cell></row><row><cell>11</cell><cell>Examples-proportional, K = 2 16</cell><cell>80.45</cell><cell>42.07</cell><cell>91.97</cell><cell>90.97</cell><cell>87.50</cell><cell>85.41</cell><cell>85.04</cell><cell cols="2">86.89 90.10</cell><cell>83.01</cell><cell>83.66</cell><cell>90.74</cell><cell>72.56</cell><cell>41.16</cell><cell>19.04</cell><cell>38.59</cell><cell cols="2">77.25 85.72</cell><cell>69.95</cell><cell>76.67</cell><cell cols="2">86.38 89.29</cell><cell>70.00</cell><cell>65.93</cell><cell>27.91</cell><cell>62.78</cell><cell>61.95</cell><cell cols="2">76.90 65.83</cell><cell>73.08</cell><cell>24.35</cell><cell>34.99</cell><cell>27.10</cell></row><row><cell>11</cell><cell>Examples-proportional, K = 2 17</cell><cell>81.56</cell><cell>47.35</cell><cell>91.40</cell><cell>91.55</cell><cell>88.24</cell><cell>86.15</cell><cell>85.93</cell><cell cols="2">86.94 90.06</cell><cell>82.76</cell><cell>84.12</cell><cell>90.79</cell><cell>75.09</cell><cell>41.06</cell><cell>19.12</cell><cell>38.47</cell><cell cols="2">77.00 85.87</cell><cell>67.91</cell><cell>77.89</cell><cell cols="2">77.54 85.71</cell><cell>57.00</cell><cell>67.78</cell><cell>27.07</cell><cell>61.51</cell><cell>60.54</cell><cell cols="2">79.06 65.20</cell><cell>74.04</cell><cell>24.36</cell><cell>35.00</cell><cell>27.25</cell></row><row><cell>11</cell><cell>Examples-proportional, K = 2 18</cell><cell>81.67</cell><cell>46.85</cell><cell>91.63</cell><cell>91.99</cell><cell>88.73</cell><cell>87.68</cell><cell>87.20</cell><cell cols="2">86.93 90.35</cell><cell>83.30</cell><cell>84.01</cell><cell>91.47</cell><cell>73.29</cell><cell>40.96</cell><cell>19.07</cell><cell>38.43</cell><cell cols="2">78.17 86.74</cell><cell>67.94</cell><cell>76.57</cell><cell cols="2">78.88 87.50</cell><cell>62.00</cell><cell>67.70</cell><cell>30.85</cell><cell>63.43</cell><cell>62.54</cell><cell cols="2">76.53 65.67</cell><cell>67.31</cell><cell>24.57</cell><cell>35.19</cell><cell>27.39</cell></row><row><cell>11</cell><cell>Examples-proportional, K = 2 19</cell><cell>81.42</cell><cell>45.94</cell><cell>91.63</cell><cell>92.20</cell><cell>89.22</cell><cell>88.44</cell><cell>88.32</cell><cell cols="2">86.84 90.10</cell><cell>83.73</cell><cell>84.29</cell><cell>91.84</cell><cell>70.40</cell><cell>41.26</cell><cell>19.24</cell><cell>38.71</cell><cell cols="2">79.78 88.15</cell><cell>67.30</cell><cell>75.66</cell><cell cols="2">75.59 87.50</cell><cell>59.00</cell><cell>68.22</cell><cell>30.64</cell><cell>65.32</cell><cell>64.29</cell><cell cols="2">73.65 65.05</cell><cell>69.23</cell><cell>25.21</cell><cell>36.30</cell><cell>27.76</cell></row><row><cell>11</cell><cell>Examples-proportional, K = 2 20</cell><cell>80.80</cell><cell>42.55</cell><cell>92.78</cell><cell>91.27</cell><cell>87.99</cell><cell>88.36</cell><cell>88.10</cell><cell cols="2">86.10 89.62</cell><cell>84.15</cell><cell>84.26</cell><cell>92.20</cell><cell>68.95</cell><cell>41.05</cell><cell>19.24</cell><cell>38.46</cell><cell cols="2">80.36 88.27</cell><cell>67.38</cell><cell>73.21</cell><cell cols="2">76.18 83.93</cell><cell>62.00</cell><cell>67.57</cell><cell>26.86</cell><cell>66.12</cell><cell>65.22</cell><cell cols="2">76.90 64.73</cell><cell>69.23</cell><cell>25.66</cell><cell>36.93</cell><cell>27.68</cell></row><row><cell>11</cell><cell>Examples-proportional, K = 2 21</cell><cell>79.83</cell><cell>44.45</cell><cell>91.28</cell><cell>89.00</cell><cell>84.31</cell><cell>87.54</cell><cell>87.40</cell><cell cols="2">84.93 88.53</cell><cell>82.54</cell><cell>84.16</cell><cell>90.85</cell><cell>67.87</cell><cell>40.51</cell><cell>18.79</cell><cell>37.92</cell><cell cols="2">79.50 87.48</cell><cell>65.10</cell><cell>71.16</cell><cell cols="2">68.88 85.71</cell><cell>57.00</cell><cell>62.75</cell><cell>23.40</cell><cell>64.50</cell><cell>63.65</cell><cell cols="2">72.92 64.11</cell><cell>71.15</cell><cell>25.82</cell><cell>37.22</cell><cell>27.13</cell></row><row><cell>11</cell><cell>Temperature-scaled, T = 2</cell><cell>81.90</cell><cell>54.00</cell><cell>91.74</cell><cell>90.56</cell><cell>86.76</cell><cell>85.11</cell><cell>84.60</cell><cell cols="2">86.40 89.74</cell><cell>83.47</cell><cell>84.15</cell><cell>91.51</cell><cell>72.56</cell><cell>41.09</cell><cell>19.28</cell><cell>38.54</cell><cell cols="2">79.42 87.77</cell><cell>69.92</cell><cell>76.73</cell><cell cols="2">92.37 92.86</cell><cell>57.00</cell><cell>69.80</cell><cell>31.90</cell><cell>66.65</cell><cell>65.74</cell><cell cols="2">72.92 67.08</cell><cell>75.96</cell><cell>25.42</cell><cell>36.72</cell><cell>27.20</cell></row><row><cell>11</cell><cell>Temperature-scaled, T = 4</cell><cell>80.56</cell><cell>45.38</cell><cell>91.97</cell><cell>89.68</cell><cell>85.78</cell><cell>83.13</cell><cell>82.76</cell><cell cols="2">86.39 90.00</cell><cell>82.78</cell><cell>84.19</cell><cell>91.16</cell><cell>73.65</cell><cell>41.09</cell><cell>19.22</cell><cell>38.51</cell><cell cols="2">77.99 86.81</cell><cell>69.54</cell><cell>76.76</cell><cell cols="2">97.36 96.43</cell><cell>59.00</cell><cell>68.10</cell><cell>31.48</cell><cell>64.26</cell><cell>63.27</cell><cell cols="2">74.73 64.26</cell><cell>71.15</cell><cell>25.04</cell><cell>35.82</cell><cell>27.45</cell></row><row><cell>11</cell><cell>Temperature-scaled, T = 8</cell><cell>77.21</cell><cell>40.07</cell><cell>91.06</cell><cell>88.11</cell><cell>83.33</cell><cell>79.20</cell><cell>79.06</cell><cell cols="2">86.60 89.90</cell><cell>83.05</cell><cell>83.56</cell><cell>90.21</cell><cell>59.93</cell><cell>41.01</cell><cell>19.10</cell><cell>38.40</cell><cell cols="2">77.14 85.99</cell><cell>66.07</cell><cell>73.94</cell><cell cols="2">93.70 94.64</cell><cell>60.00</cell><cell>66.36</cell><cell>26.86</cell><cell>63.46</cell><cell>62.60</cell><cell cols="2">62.09 63.32</cell><cell>65.38</cell><cell>24.55</cell><cell>35.35</cell><cell>27.17</cell></row><row><cell>12</cell><cell>Unsupervised pre-training + fine-tuning</cell><cell>83.28</cell><cell>53.84</cell><cell>92.68</cell><cell>92.07</cell><cell>88.92</cell><cell>88.02</cell><cell>87.94</cell><cell cols="2">88.67 91.56</cell><cell>84.24</cell><cell>84.57</cell><cell>90.48</cell><cell>76.28</cell><cell>41.33</cell><cell>19.24</cell><cell>38.77</cell><cell cols="2">80.88 88.81</cell><cell>71.36</cell><cell>76.62</cell><cell cols="2">91.22 91.96</cell><cell>66.20</cell><cell>66.13</cell><cell>25.78</cell><cell>69.05</cell><cell>68.16</cell><cell cols="2">75.34 68.04</cell><cell>78.56</cell><cell>26.98</cell><cell>39.82</cell><cell>27.65</cell></row><row><cell>12</cell><cell>Multi-task training</cell><cell>81.42</cell><cell>45.94</cell><cell>91.63</cell><cell>92.20</cell><cell>89.22</cell><cell>88.44</cell><cell>88.32</cell><cell cols="2">86.84 90.10</cell><cell>83.73</cell><cell>84.29</cell><cell>91.84</cell><cell>70.40</cell><cell>41.26</cell><cell>19.24</cell><cell>38.71</cell><cell cols="2">79.78 88.15</cell><cell>67.30</cell><cell>75.66</cell><cell cols="2">75.59 87.50</cell><cell>59.00</cell><cell>68.22</cell><cell>30.64</cell><cell>65.32</cell><cell>64.29</cell><cell cols="2">73.65 65.05</cell><cell>69.23</cell><cell>25.21</cell><cell>36.30</cell><cell>27.76</cell></row><row><cell>12</cell><cell>Multi-task pre-training + fine-tuning</cell><cell>83.11</cell><cell>51.42</cell><cell>92.66</cell><cell>91.73</cell><cell>88.73</cell><cell>88.06</cell><cell>87.70</cell><cell cols="2">88.61 91.61</cell><cell>84.09</cell><cell>84.31</cell><cell>91.85</cell><cell>76.53</cell><cell>41.15</cell><cell>19.12</cell><cell>38.59</cell><cell cols="2">80.26 88.50</cell><cell>71.03</cell><cell>79.54</cell><cell cols="2">81.69 87.50</cell><cell>65.00</cell><cell>70.72</cell><cell>31.48</cell><cell>65.94</cell><cell>65.03</cell><cell cols="2">81.23 68.18</cell><cell>73.08</cell><cell>27.08</cell><cell>39.80</cell><cell>28.07</cell></row><row><cell>12</cell><cell>Leave-one-out multi-task training</cell><cell>81.98</cell><cell>48.00</cell><cell>93.23</cell><cell>91.72</cell><cell>88.24</cell><cell>87.76</cell><cell>87.32</cell><cell cols="2">88.61 91.44</cell><cell>84.00</cell><cell>84.11</cell><cell>90.79</cell><cell>72.20</cell><cell>41.34</cell><cell>19.05</cell><cell>38.77</cell><cell cols="2">79.97 88.10</cell><cell>71.68</cell><cell>78.35</cell><cell cols="2">86.76 89.29</cell><cell>66.00</cell><cell>68.09</cell><cell>29.49</cell><cell>66.23</cell><cell>65.27</cell><cell cols="2">79.06 68.65</cell><cell>78.85</cell><cell>26.93</cell><cell>39.79</cell><cell>27.87</cell></row><row><cell>12</cell><cell>Supervised multi-task pre-training</cell><cell>79.93</cell><cell>36.60</cell><cell>92.43</cell><cell>91.58</cell><cell>88.24</cell><cell>87.03</cell><cell>86.78</cell><cell cols="2">88.15 91.20</cell><cell>82.87</cell><cell>83.16</cell><cell>90.13</cell><cell>70.76</cell><cell>41.12</cell><cell>18.96</cell><cell>38.49</cell><cell cols="2">77.38 85.65</cell><cell>65.36</cell><cell>75.66</cell><cell cols="2">68.87 83.93</cell><cell>58.00</cell><cell>64.81</cell><cell>21.93</cell><cell>55.37</cell><cell>54.61</cell><cell cols="2">71.12 67.40</cell><cell>75.96</cell><cell>26.81</cell><cell>40.13</cell><cell>28.04</cell></row><row><cell>13</cell><cell>Baseline</cell><cell>83.28</cell><cell>53.84</cell><cell>92.68</cell><cell>92.07</cell><cell>88.92</cell><cell>88.02</cell><cell>87.94</cell><cell cols="2">88.67 91.56</cell><cell>84.24</cell><cell>84.57</cell><cell>90.48</cell><cell>76.28</cell><cell>41.33</cell><cell>19.24</cell><cell>38.77</cell><cell cols="2">80.88 88.81</cell><cell>71.36</cell><cell>76.62</cell><cell cols="2">91.22 91.96</cell><cell>66.20</cell><cell>66.13</cell><cell>25.78</cell><cell>69.05</cell><cell>68.16</cell><cell cols="2">75.34 68.04</cell><cell>78.56</cell><cell>26.98</cell><cell>39.82</cell><cell>27.65</cell></row><row><cell>13</cell><cell>1? size, 4? training steps</cell><cell>85.33</cell><cell>60.29</cell><cell>93.81</cell><cell>94.06</cell><cell>91.67</cell><cell>89.42</cell><cell>89.25</cell><cell cols="2">89.15 91.87</cell><cell>86.01</cell><cell>85.70</cell><cell>91.63</cell><cell>78.34</cell><cell>41.52</cell><cell>19.33</cell><cell>38.96</cell><cell cols="2">82.45 90.19</cell><cell>74.72</cell><cell>79.17</cell><cell cols="2">94.75 92.86</cell><cell>71.00</cell><cell>67.34</cell><cell>29.70</cell><cell>72.63</cell><cell>71.59</cell><cell cols="2">78.34 72.10</cell><cell>82.69</cell><cell>27.08</cell><cell>40.66</cell><cell>27.93</cell></row><row><cell>13</cell><cell>1? size, 4? batch size</cell><cell>84.60</cell><cell>56.08</cell><cell>93.12</cell><cell>92.31</cell><cell>89.22</cell><cell>88.85</cell><cell>88.84</cell><cell cols="2">89.35 92.07</cell><cell>85.98</cell><cell>86.13</cell><cell>91.07</cell><cell>80.14</cell><cell>41.70</cell><cell>19.42</cell><cell>39.08</cell><cell cols="2">82.52 90.21</cell><cell>74.64</cell><cell>78.78</cell><cell cols="2">93.69 94.64</cell><cell>72.00</cell><cell>68.09</cell><cell>30.95</cell><cell>74.73</cell><cell>73.90</cell><cell cols="2">76.53 70.06</cell><cell>81.73</cell><cell>27.07</cell><cell>40.60</cell><cell>27.84</cell></row><row><cell>13</cell><cell>2? size, 2? training steps</cell><cell>86.18</cell><cell>62.04</cell><cell>93.69</cell><cell>93.36</cell><cell>90.69</cell><cell>89.18</cell><cell>89.23</cell><cell cols="2">89.35 92.05</cell><cell>87.23</cell><cell>87.05</cell><cell>92.68</cell><cell>81.95</cell><cell>41.74</cell><cell>19.66</cell><cell>39.14</cell><cell cols="2">84.18 91.29</cell><cell>77.18</cell><cell>80.98</cell><cell cols="2">97.36 96.43</cell><cell>74.00</cell><cell>71.34</cell><cell>35.68</cell><cell>77.11</cell><cell>76.34</cell><cell cols="2">80.51 69.28</cell><cell>85.58</cell><cell>27.52</cell><cell>41.03</cell><cell>28.19</cell></row><row><cell>13</cell><cell>4? size, 1? training steps</cell><cell>85.91</cell><cell>57.58</cell><cell>94.38</cell><cell>92.67</cell><cell>89.95</cell><cell>89.60</cell><cell>89.60</cell><cell cols="2">89.44 92.14</cell><cell>87.05</cell><cell>87.12</cell><cell>93.12</cell><cell>83.39</cell><cell>41.60</cell><cell>19.73</cell><cell>39.08</cell><cell cols="2">83.86 91.32</cell><cell>78.04</cell><cell>81.38</cell><cell cols="2">89.09 94.64</cell><cell>73.00</cell><cell>73.74</cell><cell>40.40</cell><cell>78.25</cell><cell>77.40</cell><cell cols="2">81.59 70.22</cell><cell>91.35</cell><cell>27.47</cell><cell>40.71</cell><cell>28.10</cell></row><row><cell>13</cell><cell>4? ensembled</cell><cell>84.77</cell><cell>56.14</cell><cell>93.46</cell><cell>93.31</cell><cell>90.67</cell><cell>89.71</cell><cell>89.60</cell><cell cols="2">89.62 92.24</cell><cell>86.22</cell><cell>86.53</cell><cell>91.60</cell><cell>77.98</cell><cell>42.10</cell><cell>20.10</cell><cell>39.56</cell><cell cols="2">83.09 90.40</cell><cell>71.74</cell><cell>77.58</cell><cell cols="2">89.85 91.07</cell><cell>66.00</cell><cell>69.32</cell><cell>29.49</cell><cell>72.67</cell><cell>71.94</cell><cell cols="2">76.90 69.12</cell><cell>72.12</cell><cell>28.05</cell><cell>40.53</cell><cell>28.09</cell></row><row><cell>13</cell><cell>4? ensembled, fine-tune only</cell><cell>84.05</cell><cell>54.78</cell><cell>92.78</cell><cell>93.146</cell><cell>90.44</cell><cell>88.34</cell><cell>88.12</cell><cell cols="2">89.27 91.97</cell><cell>85.33</cell><cell>85.88</cell><cell>90.98</cell><cell>77.62</cell><cell>41.66</cell><cell>19.57</cell><cell>39.12</cell><cell cols="2">82.36 89.86</cell><cell>71.56</cell><cell>77.43</cell><cell cols="2">90.07 92.86</cell><cell>69.00</cell><cell>67.31</cell><cell>26.34</cell><cell>70.47</cell><cell>69.64</cell><cell cols="2">75.45 68.18</cell><cell>74.04</cell><cell>27.55</cell><cell>40.22</cell><cell>28.09</cell></row></table><note><p><p>Original target: Taco Bell a afirmat c?, p?n? ?n 2022, intent , ioneaz? s? deschid? 2000 de restaurante ?n SUA.</p>Processed target: Taco Bell a afirmat c?, p?n? ?n 2022, intent , ioneaz? s? deschid? 2000 de restaurante ?n SUA.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>http://commoncrawl.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>http://nlp.seas.harvard.edu/2018/04/03/attention.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>http://jalammar.github.io/illustrated-transformer/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://cloud.google.com/tpu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>https://pypi.org/project/langdetect/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>https://www.tensorflow.org/datasets/catalog/c4</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>https://www.tensorflow.org/datasets</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8"><p>https://www.pydoc.io/pypi/tensor2tensor-1.5.7/autoapi/data_generators/generator_utils/index.html# data_generators.generator_utils.pack_examples</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_9"><p>WikipediaThe website Wikipedia consists of millions of encyclopedia articles written collaboratively. The content on the site is subject to strict quality guidelines and therefore has been used as a reliable source of clean and natural text. We use the English Wikipedia text data from TensorFlow Datasets, 13 which omits any markup or reference sections from the articles.11 https://www.tensorflow.org/datasets/catalog/c4 12 https://github.com/jcpeterson/openwebtext 13 https://www.tensorflow.org/datasets/catalog/wikipedia</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_10"><p>https://github.com/google-research/bert/blob/master/multilingual.md</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_11"><p>http://gluebenchmark.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_12"><p>http://super.gluebenchmark.com</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank <rs type="person">Grady Simon</rs>, <rs type="person">Noah Fiedel</rs>, <rs type="person">Samuel R. Bowman</rs>, <rs type="person">Augustus Odena</rs>, <rs type="person">Daphne Ippolito</rs>, <rs type="person">Noah Constant</rs>, <rs type="person">Orhan Firat</rs>, <rs type="person">Ankur Bapna</rs>, and <rs type="person">Sebastian Ruder</rs> for their comments on this manuscript; <rs type="person">Zak Stone</rs> and the <rs type="institution">TFRC team</rs> for their support; <rs type="person">Austin Tarango</rs> for his guidance on dataset creation; <rs type="person">Melvin Johnson</rs>, <rs type="person">Dima Lepikhin</rs>, <rs type="person">Katrin Tomanek</rs>, <rs type="person">Jeff Klingner</rs>, and <rs type="person">Naveen Arivazhagan</rs> for insight into multi-task machine translation; <rs type="person">Neil Houlsby</rs> for comments on adapter layers; <rs type="person">Olga Wichowska</rs>, <rs type="person">Ola Spyra</rs>, <rs type="person">Michael Banfield</rs>, <rs type="person">Yi Lin</rs>, and <rs type="person">Frank Chen</rs> for assistance with infrastructure; <rs type="person">Etienne Pot</rs>, <rs type="person">Ryan Sepassi</rs>, and <rs type="person">Pierre Ruyssen</rs> for collaboration on TensorFlow Datasets; <rs type="person">Rohan Anil</rs> for help with our download pipeline for Common Crawl; <rs type="person">Robby Neale</rs> and <rs type="person">Taku Kudo</rs> for their work on SentencePiece; and many other members of the <rs type="institution">Google Brain team</rs> for their discussion and insight.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>creature refused to back down during a fight . wildlife enthusiast lisl moolman, 41, caught the bizarre battle while out on the road and armed with her camera .</p><p>2. Ground-truth: researchers say homes are a hotbed of undiscovered species . study of 40 north carolina homes found 8,000 bacterial and archaeal taxa . 11 houses in california revealed a great variety of fungus among us . Prediction: researchers estimate that the indoor biome makes up about 0.5 percent of ice-free land, or about 247,000 square miles, almost the size of texas . a study of 40 houses in north carolina used genomic technologies to document more than 8,000 bacterial and archaeal taxa . another study of 50 houses in the same state described more than 750 types of arthropods, including insects and spiders . an examination of 11 houses in california found a great variety of fungus among us .</p><p>3. Ground-truth: executions have been temporarily halted in the state after the drug used for lethal injections appeared faulty on monday night . kelly gissendaner had her death delayed for a second time when it was found that the lethal pentobarbital dose appeared cloudy . state officials dithered over the decision , changing their mind three times before deciding not to take the risk that it wouldn't work . it is 'essential that executions are carried out in a constitutional manner,' said georgia attorney general sam olens . Prediction: the execution of kelly gissendaner, the only woman on georgia's death row, was delayed for a second time on monday . the execution team found that the lethal pentobarbital dose appeared cloudy . the cloudy drug bolstered death penalty opponents, who have been vocal in their opposition after three botched executions in other parts of the country .</p><p>4. Ground-truth: dani alves was not selected for the brazil squad to face france and chile . barcelona defender was filmed serving up hot dogs to people on saturday . this week alves released a charity single with former team-mat jose pinto . alves looks set to leave barcelona on a free transfer this summer . reports in spanish press claim he will sign for paris saint-germain . Prediction: dani alves was not selected for brazil's friendlies against france and chile . the barcelona right back has released a charity single with jose pinto . alves was filmed serving up snacks from behind the counter of a hot dog bar on saturday . alves is yet to be offered a new deal at the nou camp . click here for all the latest barcelona news .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Preprocessed examples</head><p>In this section, we provide examples of our preprocessing for each of the datasets we consider.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CoLA</head><p>Original input:</p><p>Sentence: John made Bill master of himself. Sentence: it confirms fincher 's status as a film maker who artfully bends technical know-how to the service of psychological insight .</p><p>Processed input: sst2 sentence: it confirms fincher 's status as a film maker who artfully bends technical know-how to the service of psychological insight .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original target: 1</head><p>Processed target: positive</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STSB Original input:</head><p>Sentence 1: Representatives for Puretunes could not immediately be reached for comment Wednesday. Sentence 2: Puretunes representatives could not be located Thursday to comment on the suit.</p><p>Processed input: stsb sentence1: Representatives for Puretunes could not immediately be reached for comment Wednesday. sentence2: Puretunes representatives could not be located Thursday to comment on the suit.</p><p>Original target: 3.25</p><p>Processed target: 3.2</p><p>Question: Why was Joey surprised the morning he woke up for breakfast?</p><p>Processed input: multirc question: Why was Joey surprised the morning he woke up for breakfast? answer: There was only pie to eat, rather than traditional breakfast foods paragraph: &lt;b&gt;Sent 1: &lt;/b&gt;Once upon a time, there was a squirrel named Joey.&lt;br&gt;&lt;b&gt;Sent 2: &lt;/b&gt;Joey loved to go outside and play with his cousin Jimmy.&lt;br&gt;&lt;b&gt;Sent 3: &lt;/b&gt;Joey and Jimmy played silly games together, and were always laughing.&lt;br&gt;&lt;b&gt;Sent 4: &lt;/b&gt;One day, Joey and Jimmy went swimming together at their Aunt Julie's pond.&lt;br&gt;&lt;b&gt;Sent 5: &lt;/b&gt;Joey woke up early in the morning to eat some food before they left.&lt;br&gt;&lt;b&gt;Sent 6: &lt;/b&gt;He couldn't find anything to eat except for pie!&lt;br&gt;&lt;b&gt;Sent 7: &lt;/b&gt;Usually, Joey would eat cereal, fruit (a pear), or oatmeal for breakfast.&lt;br&gt;&lt;b&gt;Sent 8: &lt;/b&gt;After he ate, he and Jimmy went to the pond.&lt;br&gt;&lt;b&gt;Sent 9: &lt;/b&gt;On their way there they saw their friend Jack Rabbit.&lt;br&gt;&lt;b&gt;Sent 10: &lt;/b&gt;They dove into the water and swam for several hours.&lt;br&gt;&lt;b&gt;Sent 11: &lt;/b&gt;The sun was out, but the breeze was cold.&lt;br&gt;&lt;b&gt;Sent 12: &lt;/b&gt;Joey and Jimmy got out of the water and started walking home.&lt;br&gt;&lt;b&gt;Sent 13: &lt;/b&gt;Their fur was wet, and the breeze chilled them.&lt;br&gt;&lt;b&gt;Sent 14: &lt;/b&gt;When they got home, they dried off, and Jimmy put on his favorite purple shirt.&lt;br&gt;&lt;b&gt;Sent 15: &lt;/b&gt;Joey put on a blue shirt with red and green dots.&lt;br&gt;&lt;b&gt;Sent 16: &lt;/b&gt;The two squirrels ate some food that Joey's mom, Jasmine, made and went off to bed.&lt;br&gt; Text: The stable was very roomy, with four good stalls; a large swinging window opened into the yard , which made it pleasant and airy.</p><p>Processed input: wsc: The stable was very roomy, with four good stalls; a large swinging window opened into the yard , which made *it* pleasant and airy.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Character-level language modeling with deeper self-attention</title>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dokook</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandy</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Memory-efficient adaptive optimization for large-scale learning</title>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11150</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ELECTRA: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName><surname>Anonymous</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1xMH1BtvB.34" />
	</analytic>
	<monogr>
		<title level="m">Submitted to the 8th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Xu Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05019</idno>
		<title level="m">Massively multilingual neural machine translation in the wild: Findings and challenges</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Cloze-driven pretraining of self-attention networks</title>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.07785</idno>
		<imprint>
			<date type="published" when="1920">2019. 4, 19, 20</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Simple, scalable adaptation for neural machine translation</title>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08478</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Findings of the 2014 workshop on statistical machine translation</title>
		<author>
			<persName><forename type="first">Ond?ej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herve</forename><surname>Saint-Amand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Findings of the 2015 workshop on statistical machine translation</title>
		<author>
			<persName><forename type="first">Ond?ej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hokamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Findings of the 2016 conference on machine translation</title>
		<author>
			<persName><forename type="first">Ond?ej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Jimeno Yepes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06349</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">N-gram counts and language models from the common crawl</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bas</forename><surname>Van Ooyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<idno>1997. 24</idno>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00055</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Long short-term memory-networks for machine reading</title>
		<author>
			<persName><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06733</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Boolq</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10044</idno>
		<title level="m">Exploring the surprising difficulty of natural yes/no questions</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05449</idno>
		<title level="m">SentEval: An evaluation toolkit for universal sentence representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02364</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The PASCAL recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges Workshop</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The CommitmentBank: Investigating projection in naturally occurring discourse</title>
		<author>
			<persName><forename type="first">Marie-Catherine De</forename><surname>Marneff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandy</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judith</forename><surname>Tonhauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sinn und Bedeutung</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018. 1, 2, 3, 7, 8, 9, 10, 12, 14, 16, 17, 18, 21, 27</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">53</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Paraphrasing (IWP2005)</title>
		<meeting>the Third International Workshop on Paraphrasing (IWP2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03197</idno>
		<imprint>
			<date type="published" when="2019">2019. 1, 2, 8, 14</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Understanding back-translation at scale</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09381</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08883</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Rethinking ImageNet pre-training. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11983</idno>
		<title level="m">A hybrid neural network model for commonsense reasoning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">44</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Newsha</forename><surname>Ardalani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Kianinejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Mostofa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00409</idno>
		<title level="m">Deep learning scaling is predictable, empirically</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning distributed representations of sentences from unlabelled data</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.03483</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00751</idno>
		<title level="m">Parameter-efficient transfer learning for NLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06146</idno>
		<imprint>
			<date type="published" when="2018">2018. 2, 3, 9, 10, 15</date>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Music transformer: Generating music with long-term structure</title>
		<author>
			<persName><forename type="first">Cheng-Zhi Anna</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><surname>Dinculescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018-04">2018a. 4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Gpipe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06965</idno>
		<title level="m">Efficient training of giant neural networks using pipeline parallelism</title>
		<imprint>
			<date type="published" when="2018-02-27">2018b. 2, 27</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Minyoung</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08614</idno>
		<title level="m">What makes ImageNet good for transfer learning? arXiv preprint</title>
		<imprint>
			<date type="published" when="2016">2016. 1, 26</date>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">First Quora dataset release: Question pairs</title>
		<author>
			<persName><forename type="first">Shankar</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Dandekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kornel</forename><surname>Csernai</surname></persName>
		</author>
		<ptr target="https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Xiaoqi</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Tinybert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.10351</idno>
		<title level="m">Distilling BERT for natural language understanding</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03551,2017.32</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10529</idno>
		<title level="m">SpanBERT: Improving pre-training by representing and predicting spans</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Looking beyond the surface: A challenge set for reading comprehension over multiple sentences</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of North American Chapter</title>
		<meeting>North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">A surprisingly robust trick for Winograd schema challenge</title>
		<author>
			<persName><forename type="first">Ana-Maria</forename><surname>Vid Kocijan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oana-Maria</forename><surname>Cretu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yordan</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Yordanov</surname></persName>
		</author>
		<author>
			<persName><surname>Lukasiewicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06290</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Kone?n?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03575</idno>
		<title level="m">Federated optimization: Distributed optimization beyond the datacenter</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Federated learning: Strategies for improving communication efficiency</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Brendan</forename><surname>Jakub Kone?n?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">X</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananda</forename><surname>Richt?rik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Theertha Suresh</surname></persName>
		</author>
		<author>
			<persName><surname>Bacon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.05492</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08974</idno>
		<title level="m">Do better ImageNet models transfer better? arXiv preprint</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">One weird trick for parallelizing convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.5997</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10959</idno>
		<title level="m">Subword regularization: Improving neural network translation models with multiple subword candidates</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06226</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Cross-lingual language model pretraining</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07291</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">ALBERT: A lite BERT for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019">2019. 1, 15, 27, 30</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The Winograd schema challenge</title>
		<author>
			<persName><forename type="first">Hector</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Pot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10198</idno>
		<title level="m">Generating Wikipedia by summarizing long sequences</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-An</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00998</idno>
		<title level="m">SummAE: Zero-shot abstractive text summarization using length-agnostic auto-encoders</title>
		<imprint>
			<date type="published" when="2016">2019a. 16</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Representation learning using multi-task deep neural networks for semantic classification and information retrieval</title>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye-Yi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter</title>
		<meeting>the 2015 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Multi-task deep neural networks for natural language understanding</title>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11504</idno>
		<imprint>
			<date type="published" when="2016">2019b. 16</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Fine-tune BERT for extractive summarization</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10318</idno>
		<imprint>
			<date type="published" when="2019">2019. 32</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019c. 1, 2, 4, 9, 19, 27, 28</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">An efficient framework for learning sentence representations</title>
		<author>
			<persName><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02893</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">The natural language decathlon: Multitask learning as question answering</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08730</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013-01">2013a. 1</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Abstractive text summarization using sequence-to-sequence RNNs and beyond</title>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.06023</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Cicero Nogueira dos santos, Caglar Gulcehre, and Bing Xiang</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<author>
			<persName><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04304,2017.32</idno>
		<title level="m">A deep reinforced model for abstractive summarization</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.05987</idno>
		<title level="m">To tune or not to tune? adapting pretrained representations to diverse tasks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Sentence encoders on STILTs: Supplementary training on intermediate labeled-data tasks</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>F?vry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01088</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">WIC: 10,000 example pairs for evaluating context-sensitive representations</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pilehvar</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09121</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.08771</idno>
		<title level="m">A call for clarity in reporting BLEU scores</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 3, 8, 10, 12</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2, 6, 13</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Resolving complex cases of definite pronouns: the Winograd schema challenge</title>
		<author>
			<persName><forename type="first">Altaf</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Unsupervised pretraining for sequence to sequence learning</title>
		<author>
			<persName><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02683</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Choice of plausible alternatives: An evaluation of commonsense causal reasoning</title>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Roemmele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cosmin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Bejan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">S</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 AAAI Spring Symposium Series</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">An overview of multi-task learning in deep neural networks</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05098</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Transfer learning in natural language processing</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Tutorials</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04368</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Christopher J Shallue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Antognini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03600</idno>
		<title level="m">Measuring the effects of data parallelism on neural network training</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<author>
			<persName><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02155</idno>
		<title level="m">Self-attention with relative position representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Adafactor: Adaptive learning rates with sublinear memory cost</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04235</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06538</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Mesh-tensorflow: Deep learning for supercomputers</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penporn</forename><surname>Koanantakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Dirt cheap web-scale parallel text from the common crawl</title>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herve</forename><surname>Saint-Amand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Magdalena</forename><surname>Plamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">MASS: Masked sequence to sequence pre-training for language generation</title>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02450</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">53</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Learning general purpose distributed sentence representations via large scale multi-task learning</title>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00079</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<ptr target="http://www.incompleteideas.net/IncIdeas/BitterLesson.html" />
		<title level="m">The bitter lesson</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Cloze procedure&quot;: A new tool for measuring readability</title>
		<author>
			<persName><forename type="first">Wilson</forename><forename type="middle">L</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journalism Bulletin</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02847</idno>
		<title level="m">A simple method for commonsense reasoning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09830</idno>
		<title level="m">NewsQA: A machine comprehension dataset</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008">2017. 3, 4, 6, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amapreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Can you tell me how to get past sesame street? sentence-level pretraining beyond language modeling</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghavendra</forename><surname>Pappagari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Thomas</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roma</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Najoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherin</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016">2019a. 16</date>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">SuperGLUE: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00537</idno>
		<imprint>
			<date type="published" when="2005">2019b. 2, 5</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">StructBERT: Incorporating language structures into pre-training for deep language understanding</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuyi</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.04577</idno>
		<imprint>
			<date type="published" when="2019">2019c. 31</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12471</idno>
		<title level="m">Neural network acceptability judgments</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05426</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Xlnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">Generalized autoregressive pretraining for language understanding</title>
		<imprint>
			<date type="published" when="2019">2019. 1, 2, 3, 8, 10, 15, 16, 19, 27, 28</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<title level="m" type="main">Combining local convolution with global self-attention for reading comprehension</title>
		<author>
			<persName><forename type="first">Adams</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Qanet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09541</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franziska</forename><surname>Roesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12616</idno>
		<title level="m">Defending against neural fake news</title>
		<imprint>
			<date type="published" when="1920">2019. 2, 4, 20</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12885</idno>
		<title level="m">ReCoRD: Bridging the gap between human and machine commonsense reading comprehension</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11764</idno>
		<title level="m">Freelb: Enhanced adversarial training for language understanding</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
