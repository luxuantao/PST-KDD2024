<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NeST: A Neural Network Synthesis Tool Based on a Grow-and-Prune Paradigm</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
							<email>xdai@princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Princeton University</orgName>
								<address>
									<postCode>08544</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Hongxu</forename><surname>Yin</surname></persName>
							<email>hongxuy@princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Princeton University</orgName>
								<address>
									<postCode>08544</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Niraj</forename><forename type="middle">K</forename><surname>Jha</surname></persName>
							<email>jha@princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Princeton University</orgName>
								<address>
									<postCode>08544</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NeST: A Neural Network Synthesis Tool Based on a Grow-and-Prune Paradigm</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4414BCC681FFFD41F106DC737AD8EFA2</idno>
					<idno type="DOI">10.1109/TC.2019.2914438</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2019.2914438, IEEE Transactions on Computers</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Architecture synthesis</term>
					<term>grow-and-prune paradigm</term>
					<term>network parameters</term>
					<term>neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks (DNNs) have begun to have a pervasive impact on various applications of machine learning. However, the problem of finding an optimal DNN architecture for large applications is challenging. Common approaches go for deeper and larger DNN architectures but may incur substantial redundancy. To address these problems, we introduce a network growth algorithm that complements network pruning to learn both weights and compact DNN architectures during training. We propose a DNN synthesis tool (NeST) that combines both methods to automate the generation of compact and accurate DNNs. NeST starts with a randomly initialized sparse network called the seed architecture. It iteratively tunes the architecture with gradient-based growth and magnitude-based pruning of neurons and connections. Our experimental results show that NeST yields accurate, yet very compact DNNs, with a wide range of seed architecture selection. For the LeNet-300-100 (LeNet-5) architecture, we reduce network parameters by 70.2× (74.3×) and floating-point operations (FLOPs) by 79.4× (43.7×). For the AlexNet, VGG-16, and ResNet-50 architectures, we reduce network parameters (FLOPs) by 15.7× (4.6×), 33.2× (8.9×), and 4.1× (2.1×) respectively. NeST's grow-and-prune paradigm delivers significant additional parameter and FLOPs reduction relative to pruning-only methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Over the last decade, deep neural networks (DNNs) have begun to revolutionize myriad research domains, such as computer vision, speech recognition, and robotic control. Their ability to distill intelligence from a dataset through multi-level abstraction even leads to super-human performance. Thus, DNNs are emerging as a new cornerstone of modern artificial intelligence.</p><p>The DNN architecture derived from a given dataset has a huge impact on its final performance. To illustrate this, in Table 1, we compare several well-known DNNs from the Ima-geNet Large Scale Visual Recognition Challenge (ILSVRC) 2012-2016 <ref type="bibr" target="#b0">[1]</ref>. We describe the architecture in terms of network depth, number of network parameters, number of connections, and performance in terms of top-5 error rate on the ImageNet dataset. All these contest-winning DNNs were obtained using the same back-propagation (BP) algorithm <ref type="bibr" target="#b1">[2]</ref> for training weights. Thus, the network architecture becomes their distinguishing characteristic. This architectural diversity leads to substantially different final performance, as can be seen from Table <ref type="table" target="#tab_0">1</ref>.</p><p>Though critically important, how to efficiently derive an appropriate DNN architecture from large datasets has remained an open problem. Researchers have traditionally derived the DNN architecture by sweeping through its architectural parameters and training the corresponding architecture until the point of diminishing returns in its performance. This suffers from three major problems:</p><p>1) Fixed architecture: Most BP-based methods train weights, not architectures. They only utilize the gradient information in the DNN weight space, but </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network</head><p>Depth #Param #Connections Top-5 error AlexNet <ref type="bibr" target="#b2">[3]</ref> 7 61M 0.7B 18.2% VGG-16 <ref type="bibr" target="#b3">[4]</ref> 16 138M 15.5B 7.3% GoogLeNet <ref type="bibr" target="#b4">[5]</ref> 22 7M 1.6B 6.7% ResNet-152 <ref type="bibr" target="#b5">[6]</ref> 152 60M 11.3B 3.6%</p><p>keep the DNN architecture fixed throughout the entire training process. Thus, such an approach does not lead to a better DNN architecture.</p><p>2) Lengthy upgrade: Searching for an appropriate DNN architecture through trial-and-error is inefficient. This problem is exacerbated when DNNs get larger and contain tens of millions of parameters. Each trial can easily consume tens of hours on even the fastest graphical processing units (GPUs). Even with all the available computational power and expended researcher efforts, it still takes years to unveil superior architectures for a given application, such as image classification (as can be seen from the evolution of AlexNet to VGG, GoogLeNet, and ResNet).</p><p>3) Vast redundancy: Most DNNs are significantly over-parameterized. Even the best-known humandefined DNN architectures for image classification (e.g., LeNets <ref type="bibr" target="#b6">[7]</ref>, AlexNet <ref type="bibr" target="#b2">[3]</ref>, VGG <ref type="bibr" target="#b3">[4]</ref>) suffer from substantial storage and computation redundancy <ref type="bibr" target="#b7">[8]</ref>. For example, <ref type="bibr">Han et al.</ref> showed that the number of parameters and floating-point operations (FLOPs) in AlexNet can be reduced by 9× and 3×, respectively, with no loss of accuracy <ref type="bibr" target="#b8">[9]</ref>.</p><p>To address these problems, we propose a novel DNN synthesis tool (NeST) that trains both DNN weights and architectures. Inspired by the learning mechanism of the human brain, NeST starts DNN synthesis from a seed DNN architecture (birth point). It allows the DNN to grow connections and neurons based on gradient information (baby brain) so that the DNN can easily adapt to the problem at hand. Then, it prunes away insignificant connections and neurons based on magnitude information (adult brain) to avoid redundancy. This enables NeST to generate compact yet accurate DNNs. We used NeST to synthesize various compact DNNs for the MNIST <ref type="bibr" target="#b6">[7]</ref> and ImageNet <ref type="bibr" target="#b0">[1]</ref> datasets. As we show later, NeST leads to drastic reductions in the number of parameters and FLOPs relative to the corresponding state-of-the-art DNN baselines while maintaining classification accuracy, hence dramatically cutting memory cost, inference run-time, and energy consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Next, we discuss various synthesis approaches for DNNs.</p><p>An evolutionary algorithm provides a promising solution to DNN architecture selection through evolution of network architectures. Its search mechanism involves iterations over mutation, recombination, and most importantly, evaluation and selection of network architectures <ref type="bibr" target="#b9">[10]</ref>. Additional performance enhancement techniques include better encoding methods <ref type="bibr" target="#b10">[11]</ref> and algorithmic redesign for DNNs <ref type="bibr" target="#b11">[12]</ref>. All these assist with more efficient search in the wide DNN architecture space.</p><p>Reinforcement learning (RL) has emerged as a new powerful tool to solve this problem <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Zoph et al. <ref type="bibr" target="#b14">[15]</ref> use a recurrent neural network controller to iteratively generate groups of candidate networks, whose performance is then used as a reward for enhancing the controller. Baker et al. <ref type="bibr" target="#b12">[13]</ref> propose a Q-learning based RL approach that enables convolutional architecture search. A recent work <ref type="bibr" target="#b13">[14]</ref> proposes the NASNet architecture that uses RL to search for architectural building blocks and achieves better performance than human-invented architectures.</p><p>The structure adaptation (SA) approach exploits network clues (e.g., distribution of weights) to incorporate architecture selection into the training process. Existing SA methods can be further divided into two categories: constructive and destructive. A constructive approach starts with a small network and iteratively adds connections/neurons <ref type="bibr" target="#b15">[16]</ref>. A destructive approach, on the other hand, starts with a large network and iteratively removes connections/neurons. This can effectively reduce model redundancy. For example, recent pruning methods, such as network pruning <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, layer-wise surgeon <ref type="bibr" target="#b19">[20]</ref>, sparsity learning <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, and dynamic network surgery <ref type="bibr" target="#b22">[23]</ref>, can offer significant compactness for existing DNNs with no or little accuracy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SYNTHESIS METHODOLOGY</head><p>In this section, we propose NeST that leverages both constructive and destructive SA approaches through a growand-prune paradigm. This leads to significantly reduced synthesis cost compared to RL-based approaches, as we will show later. We first give a high-level overview of NeST, after which we zoom into specific growth and pruning algorithms. Unless otherwise stated, we adopt the notations given in Table <ref type="table" target="#tab_1">2</ref> to represent various variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Neural Network Synthesis Tool</head><p>We illustrate the NeST approach with Fig. <ref type="figure" target="#fig_0">1</ref>. Synthesis begins with an initial seed architecture, typically initialized as a sparse and partially connected DNN, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>(a). We also ensure that all neurons are connected in the seed architecture. Then, NeST utilizes two sequential phases to synthesize the DNN: (i) gradient-based growth phase, and (ii) magnitude-based pruning phase. In the growth phase, the gradient information in the architecture space is used to gradually grow new connections, neurons, and feature maps to achieve the desired accuracy. In the pruning phase, the DNN inherits the synthesized architecture and weights from the growth phase and iteratively removes redundant connections and neurons, based on their magnitudes. In the synthesis flow, we perform growth and pruning periodically during the training process, and the architecture updates (i.e., growth and pruning) and parameter training are interleaved during the synthesis process. Finally, NeST comes to rest at a lightweight DNN model that incurs no accuracy degradation relative to a fully connected model. We show the major components of the NeST algorithm in Fig. <ref type="figure" target="#fig_1">2</ref>. We zoom into specific growth and pruning algorithms and provide details of these components as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Gradient-based Growth</head><p>In this section, we explain how we grow connections, neurons, and feature maps based on gradient information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Connection Growth</head><p>The connection growth algorithm greedily activates useful, but currently 'dormant', connections. We incorporate it in the following learning policy: Policy 1: Add a connection w if and only if it can quickly reduce the value of loss function L.</p><p>The initial sparse DNN seed contains only a small fraction of active connections that can propagate gradient information. To locate the 'dormant' connections that can reduce L effectively, we evaluate ∂L/∂w for all the 'dormant' connections w (either computed using the whole training set or a large batch). We activate the connections with large gradient magnitudes |∂L/∂w|, and initialize their weights as η × ∂L/∂w, where η is the learning rate. Note that if we introduce learning rate decay to network training, then η is  the learning rate right before the growth. Otherwise η is a pre-defined constant throughout the training process.</p><p>Policy 1 activates 'dormant' connections if and only if they are the most efficient at reducing L. This can also assist with avoiding local minima and achieving higher accuracy <ref type="bibr" target="#b23">[24]</ref>. To illustrate this policy, we count the number of grown connections between each pixel of the input image and the first hidden layer of LeNet-300-100 <ref type="bibr" target="#b6">[7]</ref> (for the MNIST dataset with image size 28×28), and plot the number of connections at each pixel in Fig. <ref type="figure" target="#fig_2">3</ref>. The image center has a much higher grown density than the margins, consistent with the fact that the MNIST digits are centered.</p><p>From a neuroscience perspective, our connection growth algorithm coincides with the Hebbian theory: "Neurons that fire together wire together <ref type="bibr" target="#b24">[25]</ref>." We define the stimulation magnitude of the m th presynaptic neuron in the (l + 1) th layer and the n th postsynaptic neuron in the l th layer as ∂L/∂u l+1 m and x l n , respectively. The connections activated based on Hebbian theory would have a strong correlation between presynaptic and postsynaptic cells, thus a large value of (∂L/∂u l+1 m )x l n . This is also the magnitude of the gradient of L with respect to w (w is the weight that connects u l+1 m and x l n ):</p><formula xml:id="formula_0">|∂L/∂w| = (∂L/∂u l+1 m )x l n (1)</formula><p>Thus, this is mathematically equivalent to Policy 1. Furthermore, the sign of the initialized weight η × ∂L/∂w = η(∂L/∂u l+1 m )x l n is identical to the correlation direction between presynaptic and postsynaptic cells. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Neuron Growth</head><p>Our neuron growth algorithm consists of two major steps: (i) connection establishment and (ii) weight initialization.</p><p>The neuron growth policy is as follows:</p><p>Policy 2: In the l th layer, add a new neuron as a shared intermediate node between existing neuron pairs that have high postsynaptic (x) and presynaptic (∂L/∂u) neuron correlations [each pair contains one neuron from the (l -1) th layer and the other from the (l + 1) th layer].</p><p>Initialize weights based on batch gradients to reduce the value of L.</p><p>Policy 2 also greedily reduces L. Since a newly added neuron connects neurons in the previous and subsequent layers, we only target neuron pairs in these two layers that have strong presynaptic and postsynaptic correlations.</p><p>Algorithm 1 incorporates Policy 2 and illustrates the neuron growth iterations in detail. Before adding a neuron to the l th layer, we evaluate the bridging gradient between the neurons at the previous (l-1) th and subsequent (l+1) th layers. We connect the top β × 100% (β is the growth ratio, typically 30%-40% in our experiment) correlated neuron pairs through a new neuron in the l th layer. We initialize the weights based on the bridging gradient to enable gradient descent, thus decreasing the value of L.</p><p>We implement a square root rule for weight initialization in order to imitate a BP update on the bridging connection Algorithm 1 Neuron growth in the l th layer Input: α: birth strength, β: growth ratio Denote: M : number of neurons in the (l + 1) th layer, N : number of neurons in the (l -1) th layer, G ∈ R M ×N : bridging gradient matrix, avg: extracts mean value of nonzero elements Add a neuron in the l th layer, initialize</p><formula xml:id="formula_1">w out = 0 ∈ R M , w in = 0 ∈ R N Begin for 1 ≤ m ≤ M, 1 ≤ n ≤ N do G m,n = ∂L ∂u l+1 m × x l-1 n end for thres = (βM N ) th largest element in abs(G) for 1 ≤ m ≤ M, 1 ≤ n ≤ N do if |G m,n | &gt; thres then δw = |G m,n | ×rand{1, -1} w out m ← w out m + δw w in n ← w in n + δw × sgn(G m,n ) end if w out ← w out × α avg(abs(W l+1 )) avg(abs(w out )) w in ← w in × α avg(abs(W l )) avg(abs(w in ))</formula><p>end for Concatenate network weights W with w in , w out End w b , which connects x l-1 n [output value of the n th neuron in the (l -1) th layer] and u l+1 m [input value of the m th neuron in the (l + 1) th layer]. The BP update leads to a change in u l+1 m :</p><formula xml:id="formula_2">|∆u l+1 m | b.p. = |x l-1 n × δw b | = η|x l-1 n × ∂L/∂w b | = η|x l-1 n × G m,n |<label>(2)</label></formula><p>where η is the learning rate. In Algorithm 1, when we connect the newly added neuron (in the l th layer) with x l-1 n and u l+1 m , we initialize their weights to the square root of the magnitude of the bridging gradient:</p><formula xml:id="formula_3">|δw in n | = |δw out m | = |G m,n |<label>(3)</label></formula><p>where δw in n (δw out m ) is the initialized value of the weight that connects the newly added neuron with x l-1 n (u l+1 m ). The weight initialization rule leads to a change in u l+1 m :</p><formula xml:id="formula_4">|∆u l+1 m | = |f (x l-1 n × δw in n ) × δw out m | (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where f is the neuron activation function. Suppose tanh is the activation function. Then,</p><formula xml:id="formula_6">f (x) = tanh(x) ≈ x, if x 1<label>(5)</label></formula><p>Since δw in n and δw out m are typically very small, the approximation in Eq. ( <ref type="formula" target="#formula_6">5</ref>) leads to Eq. <ref type="bibr" target="#b5">(6)</ref>. This is linearly proportional to the effect of a BP update. Thus, our weight initialization mathematically imitates a BP update. Though we illustrated the algorithm with the tanh activation function, the weight initialization rule works equally well with other activation functions, such as rectified linear unit (ReLU) and leaky rectified linear unit (Leaky ReLU).</p><formula xml:id="formula_7">|∆u l+1 m | ≈ |x l-1 n × δw in n × δw out m | = |x l-1 n × G m,n | = 1 η |∆u l+1 m | b.p.<label>(6)</label></formula><p>Our gradient-based weight initialization method easily outperforms the naïve approach that just assigns random values to the new weights. To compare the impact of the proposed gradient-based growth approach and the naïve approach on the value of the loss function (L), we obtain multiple checkpoints in a normal training process without growth, execute both our proposed method and the naïve approach, and extract the corresponding L reductions. We show the percentage reduction in L for gradient-based growth versus the naïve approach when applied to LeNet-5 in Fig. <ref type="figure" target="#fig_3">4</ref>, where the value of L (green curve) is derived from the normal training process without growth. Note that as the value of L decreases, it becomes difficult to reduce L with a stochastic approach. Thus, neither method continues to show further improvements.</p><p>We use a birth strength factor α to strengthen the connections in and out of a newly grown neuron. This mechanism prevents these connections from becoming too weak to survive the pruning phase. Specifically, after square root rule based weight initialization, we scale up the newly added weights by</p><formula xml:id="formula_8">w out ← w out × α avg(abs(W l+1 )) avg(abs(w out )) w in ← w in × α avg(abs(W l )) avg(abs(w in ))<label>(7)</label></formula><p>where avg is an operation that extracts the mean value of all non-zero elements. This strengthens new weights. In practice, we find α &gt; 0.3 to be an appropriate range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Growth in the Convolutional Layers</head><p>Convolutional layers share the connection growth methodology of Policy 1. However, we use a unique feature map growth algorithm for convolutional layers, which differs from the neuron growth algorithm (Algorithm 1).</p><p>In a convolutional layer, we convolve input images with kernels to generate feature maps. Thus, to add a feature map, we need to initialize the corresponding set of kernels.</p><p>We summarize the feature map growth policy as follows:</p><p>Policy 3: To add a new feature map to the convolutional layers, randomly generate sets of kernels, and pick the set of kernels that reduces L the most.</p><p>In our experiment, we observe that the percentage reduction in L for Policy 3 is approximately twice as in the case of the naïve approach that initializes the new kernels with random values, as shown in Fig. <ref type="figure" target="#fig_4">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Magnitude-based Pruning</head><p>We prune away insignificant connections and neurons based on the magnitude of weights and outputs, as stated in the following policy: Policy 4: Remove a connection (neuron) if and only if the magnitude of the weight (neuron output) is smaller than a pre-defined threshold.</p><p>With this policy, a connection/neuron removal only has a minor adverse impact on L, which one can recover from through retraining. Policy 4 has two variants: (i) pruning of insignificant weights, and (ii) partial-area convolution. Pruning of insignificant weights is targeted at reducing both memory and computation power requirements. Partial-area convolution reduces run-time FLOPs for the DNN. We explain these two variants in detail next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Pruning of Insignificant Weights</head><p>Han et al. <ref type="bibr" target="#b8">[9]</ref> show that magnitude-based pruning can successfully cut down the memory and computational costs. We extend this approach to incorporate the batch normalization technique. Such a technique can reduce the internal covariate shift by normalizing layer inputs. It significantly improves the training speed and behavior and, hence, has been widely applied to large DNNs <ref type="bibr" target="#b25">[26]</ref>. Consider the l th layer <ref type="bibr" target="#b25">[26]</ref> with batch normalization:</p><formula xml:id="formula_9">u l = f ([(W l x l-1 + b l ) -E] V) = f (W l * x + b l * ) (8)</formula><p>where E and V are batch normalization terms, depicts the Hadamard (element-wise) division operator, and f is the activation function. We define effective weights W l * and effective biases b l * as:</p><formula xml:id="formula_10">W l * = W l V, b l * = (b l -E) V<label>(9)</label></formula><p>We treat connections with small effective weights as insignificant. Pruning of insignificant weights is an iterative process. In each iteration, we only prune the most insignificant weights (e.g., top 1%) in each layer, and then retrain the whole DNN to recover its performance.</p><p>After each iteration of insignificant weight pruning, we perform a neuron pruning procedure to remove all the redundant neurons. We define a neuron to be redundant if and only if one of the following two conditions is satisfied:</p><p>1) All the input connections are pruned.</p><p>2) All the output connections are pruned.</p><p>If either condition is satisfied, removing the neuron has no impact on the network. Then, we prune away the neuron as well as all the connections to or from it. The neuron pruning policy removes redundant neurons effectively in practice. For example, it prunes away 44.3% of all neurons in the first hidden layer of LeNet-300-100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Partial-area Convolution</head><p>In common convolutional neural networks (CNNs), the convolutional layers typically only consume ∼ 5% of the parameters (memory), but contribute to ∼ 90%-95% of the total FLOPs (computational load) at inference time <ref type="bibr" target="#b26">[27]</ref>. In a convolutional layer, kernels shift and convolve with the entire input image to generate feature maps. This process incurs significant redundancy, since not the whole input image is of interest to a particular kernel. Anwar et al. <ref type="bibr" target="#b27">[28]</ref> presented a method to prune all connections from a notof-interest input image to a particular kernel. This method reduces FLOPs but incurs performance degradation <ref type="bibr" target="#b27">[28]</ref>.</p><p>Instead of discarding an entire image, our proposed partial-area convolution algorithm allows kernels to convolve with only the image areas that are of interest. We refer to such an area as an area-of-interest. We prune away connections to other image areas to avoid incurring unnecessary FLOPs. We illustrate this process in Fig. <ref type="figure">6</ref>. The green area depicts area-of-interest in the image, whereas the red area depicts parts that are not of interest. Thus, green connections (solid green lines) are kept, whereas red ones (dashed red lines) are pruned away.</p><p>Partial-area convolution pruning is also an iterative process. We depict one iteration in Algorithm 2. We first convolve M input images with M × N convolution kernels and generate M × N feature maps, which are stored in the form of a four-dimensional feature map matrix C. We set the pruning threshold thres to the (100γ) th percentile of all elements in abs(C), where γ is the pruning ratio (we choose γ = 1% in our experiment). We mark the elements whose values are below this threshold as insignificant, and then prune away their input connections. We retrain the whole DNN after each pruning iteration. In our current implementation, we utilize a mask Msk to disregard the pruned convolution area. </p><formula xml:id="formula_11">≤ m ≤ M, 1 ≤ n ≤ N do C m,n = convolve(I m , K m,n ) end for thres = (γM N P Q) th largest element in abs(C) for 1 ≤ m ≤ M, 1 ≤ n ≤ N, 1 ≤ p ≤ P, 1 ≤ q ≤ Q do if |C m,n,p,q | &lt; thres then M sk m,n,p,q = 0 end if end for C ← C ⊗ Msk F ← Σ M m=1 C m Return F, Msk End TABLE 3</formula><p>FLOPs reduction with partial-area convolution on LeNet-5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pruning method</head><p>Error Parameters FLOPs w/o partial-area convolution 0.77% 5.8K 219K w/ partial-area convolution 0.77% 5.8K 105K (2.09×) Partial-area convolution enables substantial FLOPs reduction without any performance degradation. For example, we can reduce FLOPs in LeNet-5 by 2.09× when applied to MNIST, as shown in Table <ref type="table">3</ref>. Compared to the conventional CNNs that intrinsically force a fixed squareshaped area-of-interest on all kernels, we allow each kernel to self-explore the preferred shape of its area-of-interest. This exploration cuts down on redundancy and unveils interesting shapes that correspond to different kernels. For example, Fig. <ref type="figure" target="#fig_6">7</ref> shows the area-of-interest found by the layer-1 kernels in LeNet-5 when applied to MNIST. We observe significant overlaps in the image central area, which most kernels are interested in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>We implement NeST using Tensorflow <ref type="bibr" target="#b28">[29]</ref> and PyTorch <ref type="bibr" target="#b29">[30]</ref> on Nvidia GTX 1060 and Tesla P100 GPUs. We use NeST to synthesize compact DNNs for the MNIST and ImageNet Fig. <ref type="figure">6</ref>. Pruned connections (dashed red lines) and remaining connections (solid green lines) in partial-area convolution. datasets. We select DNN seed architectures based on clues (e.g., depth, kernel size, etc.) from the existing LeNets, AlexNet, VGG-16, and ResNet-50 architectures, respectively. NeST exhibits two major advantages:</p><p>1) Wide seed range: NeST yields high-performance DNNs with a wide range of seed architectures. Its greedy nature enables gradient descent in the architecture space. Its ability to start from a wide range of seed architectures alleviates reliance on humandefined architectures, and offers more freedom to DNN designers.</p><p>2) Drastic redundancy removal: NeST-generated DNNs are more compact than the state-of-the-art DNNs. Compared to the DNN architectures generated with pruning-only methods, DNNs generated through our grow-and-prune paradigm have much fewer parameters and require much fewer FLOPs.</p><p>Next, we present our experimental results for a wide spectrum of architectures based on the MNIST and Ima-geNet datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">LeNets on MNIST</head><p>We derive the seed architectures from the original LeNet-300-100 and LeNet-5 networks <ref type="bibr" target="#b6">[7]</ref>. LeNet-300-100 is a multilayer perceptron with two hidden layers. It has 300 and 100 neurons in these two hidden layers. LeNet-5 is a CNN with two convolutional layers and three fully connected layers. It contains 6 and 16 feature maps in the two convolutional layers and 120, 84, and 10 neurons in the three fully connected layers. We use the affine-distorted MNIST dataset <ref type="bibr" target="#b6">[7]</ref>, on which LeNet-300-100 (LeNet-5) can achieve the lowest error rate of 1.3% (0.8%). We separately discuss our results for the growth and pruning phases and their combination next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Growth Phase</head><p>First, we derive nine (four) seed architectures and refer to them as LeNet-300-100 (LeNet-5) 'birth points'. These seeds contain fewer neurons and connections per layer than the original LeNets. The number of neurons in each layer is the product of a ratio r and the corresponding number in the original LeNets (e.g., the seed architecture for LeNet-300-100 becomes LeNet-120-40 if r = 0.4). We randomly initialize only 10% of all the possible connections in the seed architecture (initial filling percentage is 10%). Also, we ensure that all neurons in the network are connected.</p><p>We first sweep r for LeNet-300-100 (LeNet-5) from 0.2 (0.5) to 1.0 (1.0) with a step-size of 0.1 (0.17), and then grow the DNN architectures from these seeds. We study the impact of these seeds on the GPU time for growth and post-growth DNN sizes under the same target accuracy (this accuracy is typically the reference value for the architecture). We summarize the results for LeNet-300-100 and LeNet-5 in Fig. <ref type="figure" target="#fig_7">8</ref> and Fig. <ref type="figure" target="#fig_8">9</ref>, respectively. All the models in each figure share the same target accuracy (1.3% for LeNet-300-100 and 0.8% for LeNet-5). We have two interesting findings for the growth phase:</p><p>• Observation 1: There is a trade-off between growth time and post-growth DNN size. Smaller seed architectures often lead to smaller post-growth DNN sizes, but at the expense of a higher growth time. We will later show that smaller seeds and thus smaller post-growth DNN sizes are better, since they also lead to smaller final NN sizes.</p><p>• Observation 2: When the post-growth DNN size saturates due to the full exploitation of the synthesis freedom for a target accuracy, a smaller seed is no longer beneficial. Hence, there is a minimum postgrowth DNN size associated with a given target accuracy constraint, as evident from the flat left end of the dashed curves in Fig. <ref type="figure" target="#fig_7">8</ref> and Fig. <ref type="figure" target="#fig_8">9</ref>. Beyond this point, using a smaller seed architecture only increases growth time, but does not further reduce the post-growth DNN size.</p><p>In our experiment, the initial filling percentage also has an impact on the growth time and post-growth DNN size. A smaller initial filling percentage typically leads to a higher growth time but a smaller post-growth DNN size, and vice versa (similar to growth phase Observation 1), as shown in Fig. <ref type="figure" target="#fig_9">10</ref>. Thus, to avoid excessively high growth time cost for architectures on the ImageNet dataset, we use a higher initial filling percentage (30%) for ImageNet-based DNNs, as we will show later.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Pruning Phase</head><p>Next, we prune the post-growth LeNet DNNs to remove their redundant neurons/connections. We show the postpruning DNN sizes and compression ratios for LeNet-300-100 (LeNet-5) for the different seeds in Fig. <ref type="figure" target="#fig_0">11</ref> (Fig. <ref type="figure" target="#fig_5">12</ref>), where the compression ratio is the quotient of pre-pruning DNN size divided by the post-pruning DNN size. Again, all the models in each figure have the same target accuracy. We have two major observations for the pruning phase as follows:</p><p>• Observation 1: Larger the pre-pruning DNN, larger is its compression ratio. This is because larger prepruning DNNs have a larger number of weights and thus also higher redundancy.</p><p>• Observation 2: Larger the pre-pruning DNN, larger is its post-pruning DNN. In most cases, DNNs with a larger number of weights are still larger after pruning. This is a fundamental limitation of the pruningonly approach. Thus, to synthesize a more compact DNN, one should choose a smaller seed architecture (growth phase Observation 1) within an appropriate range (growth phase Observation 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Results and Discussions</head><p>Tables <ref type="table" target="#tab_2">4</ref> and<ref type="table">5</ref> show the smallest NN models we could synthesize for LeNet-300-100 and LeNet-5, respectively. In these tables, Conv% refers to the percentage the area-ofinterest occupies relative to the full image for partial-area convolution, and Act% refers to the non-zero activation percentage (the average percentage of neurons with nonzero output values per inference). We compare our results against related results from the literature in Table <ref type="table" target="#tab_3">6</ref>. Our results outperform other reference models from various design perspectives. Without any loss of accuracy, we were able to reduce the number of connections and FLOPs of LeNet-300-100 (LeNet-5) by 70.2× (74.3×) and 79.4× (43.7×), respectively, relative to the baseline Caffe model with the same accuracy <ref type="bibr" target="#b31">[32]</ref>. Our results also substantially outperform other reference models from various design perspectives. Note that the LeNet-5 Caffe model is a variant of the original LeNet-5 proposed in <ref type="bibr" target="#b6">[7]</ref>. The LeNet-5 Caffe model has 20 and 50 feature maps in the two convolutional layers and 400 and 10 neurons in the two fully connected layers. It contains one fewer layer, 7.3× more parameters, and 6.3× more connections than the original LeNet-5 <ref type="bibr" target="#b6">[7]</ref>.</p><p>In our implementation, we incorporate an activation function shift mechanism to improve accuracy and reduce FLOPs. In the growth phase, we use Leaky ReLU as the activation function to improve accuracy. This is because Leaky ReLU alleviates the 'dying ReLU' problem (i.e., when an inactive ReLU neuron is stuck at a perpetually inactive state since no gradients flow backward through it) and leads to better performance <ref type="bibr" target="#b32">[33]</ref>. Then, we keep the network weights, change all the activation functions to ReLU, and retrain the NN. Finally, we prune the network with the ReLU activation function, where we take advantage of ReLU's zero outputs to reduce FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">AlexNet, VGG-16, and ResNet-50 on ImageNet</head><p>To demonstrate NeST's applicability to different architectures on different datasets, we use NeST to synthesize DNNs for the ILSVRC 2012 image classification dataset <ref type="bibr" target="#b0">[1]</ref>. We initialize a slim and sparse seed architecture based on the AlexNet <ref type="bibr" target="#b26">[27]</ref>, VGG-16 <ref type="bibr" target="#b3">[4]</ref>, and ResNet-50 <ref type="bibr" target="#b5">[6]</ref> architectures. Our seed architecture for AlexNet contains only 60, 140, 240, 210, and 160 feature maps in the five convolutional layers, and 3200, 1600, and 1000 neurons in the fully connected layers. The seed architecture for VGG-16 uses r = 0.75 for the first 13 convolutional layers, and has 3200, 1600, and 1000 neurons in the fully connected layers. The seed architecture for ResNet-50 simply adopts r = 0.75 for the convolutional layers (there are no fully connected layers in ResNet-50). We randomly activate 30% of all the possible connections for both seed architectures. Note that we use a higher initial filling percentage compared to LeNets to reduce the synthesis time, as explained earlier in Section 4.1.1. We ensure that all neurons are connected in the seed architecture.</p><p>We illustrate the details of our final compact inference model based on the AlexNet and VGG-16 architectures in Tables <ref type="table">7</ref> and<ref type="table">8</ref>, respectively. The final models for AlexNet and VGG-16 only contain 3.9M and 4.2M parameters, which are 15.7× and 33.2× smaller than the original AlexNet and VGG-16, respectively. For ResNet-50, all 50 convolutional layers have the same sparsity level of 56%.</p><p>Table <ref type="table" target="#tab_5">9</ref> compares the model synthesized by NeST with various AlexNet, VGG-16, and ResNet-50 based inference models derived for the ImageNet dataset. Our baselines are the AlexNet Caffe model (42.78% top-1 and 19.73% top-5 error rate) <ref type="bibr" target="#b8">[9]</ref>, the VGG-16 PyTorch model (28.41% top-1 and 9.62% top-5 error rate), and the ResNet-50 PyTorch model (23.85% top-1 and 7.13% top-5 error rate). Our grow-andprune synthesis paradigm outperforms the pruning-only methods listed in Table <ref type="table" target="#tab_5">9</ref>. This may be explained by the observation that pruning methods potentially inherit a certain  <ref type="bibr" target="#b5">[6]</ref>, using masks to grow and prune connections/neurons/feature maps may not be economical due to this temporal training memory overhead. One possible solution is to utilize low-bit (e.g., 1-bit) instead of fullprecision representation for masks, which can significantly cut down the memory overhead. We plan to address this aspect in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSIONS</head><p>In this section, we discuss inspirations from the human brain behind our synthesis methodology.</p><p>The human brain has continuously provided serendipitous inspirations for modern AI. Examples include the fundamental idea of a neuron, layered NN structure, and even convolution kernels. Our synthesis methodology incorporates three inspirations from the human brain.</p><p>First, the number of synaptic connections in the brain varies at different human ages. It rapidly increases upon the baby's birth, peaks after a few months, and decreases steadily thereafter. A DNN experiences a very similar learning process in NeST. The initial seed NN is simple and sparse, akin to a baby's brain at birth point. In the growth phase, it rapidly grows its connections and neurons based on outside information, thus reacting to this information in a manner similar to how a baby brain reacts. In the pruning phase, it reduces the number of synaptic connections to rid itself of the vast redundancy, which is akin to how a baby brain matures into an adult brain. To have a clearer view of this trend, we show the plot of the number of connections in LeNet-300-100 along its synthesis lifespan in Fig. <ref type="figure" target="#fig_12">13</ref>. It can be seen that this curve shares a very similar pattern to the number of synapses in the human brain as it evolves <ref type="bibr" target="#b39">[40]</ref>.</p><p>Second, most learning processes in our brain result from rewiring of synapses between neurons. Our brain grows and prunes away a large amount (up to 40%) of synaptic connections every day. NeST wakes up new connections, thus effectively rewiring more neurons pairs in the learning process. Thus, it mimics the 'learning through rewiring' mechanism of a human brain.</p><p>Third, only a small fraction of neurons are active at any given time in a human brain. This is known as the sparse neuron response phenomena. This mechanism enables the human brain to operate at an ultra-low power (20 Watts). However, fully connected DNNs contain a substantial amount of insignificant neuron responses per inference.    To address this problem, we include a magnitude-based neuron/connection pruning algorithm in NeST to remove the redundancy, thus achieving sparsity and compactness. This leads to huge storage and computation reductions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>In this paper, we proposed a synthesis tool, NeST, to synthesize compact yet accurate DNNs. NeST starts from a sparse seed architecture, adaptively adjusts the architecture through gradient-based growth and magnitude-based pruning, and finally arrives at a compact DNN with high accuracy. For LeNet-300-100 (LeNet-5) derived for the MNIST dataset, we reduced the number of network parameters by 70.2× (74.3×) and FLOPs by 79.4× (43.7×), without accuracy loss. For AlexNet, VGG-16, and ResNet-50 derived for the ImageNet dataset, we reduced the network parameters (FLOPs) by 15.7× (4.6×), 33.2× (8.9×), and 4.1× (2.1×), with no or minimal impact on accuracy, respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An illustration of the architecture synthesis flow in NeST.</figDesc><graphic coords="3,79.23,43.70,453.54,142.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Major components of the DNN architecture synthesis algorithm in NeST.</figDesc><graphic coords="3,49.28,235.62,249.45,92.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Grown connections from the input layer to the first layer of LeNet-300-100.</figDesc><graphic coords="3,374.80,251.58,107.15,103.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Percentage reduction in L for Policy 2 and naïve weight initialization when applied to LeNet-5.</figDesc><graphic coords="4,330.28,43.70,215.42,141.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Percentage reduction in L for Policy 3 and naïve kernel initialization when applied to LeNet-5.</figDesc><graphic coords="5,66.28,43.70,215.43,134.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 2</head><label>2</label><figDesc>Partial-area convolution Input: I: M input images, K: kernel matrix, Msk: feature map mask, γ: pruning ratio Output: Msk, F: N feature maps Denote: C ∈ R M ×N ×P ×Q : Depthwise feature map, ⊗: Hadamard (element-wise) multiplication Begin for 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Area-of-interest for five different kernels in the first layer of LeNet-5.</figDesc><graphic coords="6,324.61,43.70,226.77,155.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Growth time vs. post-growth DNN size trade-off for various seed architectures for LeNet-300-100 to reach the target 1.3% error rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Growth time vs. post-growth DNN size trade-off for various seed architectures for LeNet-5 to reach the target 0.8% error rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Growth time vs. post-growth DNN size trade-off for various initial filling percentages for LeNet-300-100 to reach the same target 1.3% error rate.</figDesc><graphic coords="7,330.28,230.72,215.43,122.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .Fig. 12 .</head><label>1112</label><figDesc>Fig. 11. Compression ratio and final DNN size for different LeNet-300-100 seed architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Number of connections vs. synthesis iteration for LeNet-300-100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Architecture and performance comparison for ILSVRC DNNs</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 Notations</head><label>2</label><figDesc>output value of the n th neuron in l th layer u l m input value of the m th neuron in l th layer W l weights between (l -1) th and l th layer b l biases in the l th layer R d 0 ×d 1 ×d 2 d 0 by d 1 by d 2 matrix with real elements</figDesc><table><row><cell>Label</cell><cell>Description</cell></row><row><cell>L</cell><cell>DNN loss function</cell></row><row><cell>x l n</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="6">Synthesized LeNet-300-100 (error rate 1.29%)</cell></row><row><cell cols="6">Layer #Param Act% FLOPs</cell></row><row><cell>fc1</cell><cell></cell><cell>7032</cell><cell>46%</cell><cell cols="2">14.1K</cell></row><row><cell>fc2</cell><cell></cell><cell>718</cell><cell>71%</cell><cell></cell><cell>0.7K</cell></row><row><cell>fc3</cell><cell></cell><cell>94</cell><cell>100%</cell><cell></cell><cell>0.1K</cell></row><row><cell cols="2">Total</cell><cell>7844</cell><cell>N/A</cell><cell cols="2">14.9K</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE 5</cell><cell></cell></row><row><cell cols="6">Synthesized LeNet-5 (error rate 0.77%)</cell></row><row><cell>Layer</cell><cell cols="5">#Param Conv% Act% FLOPs</cell></row><row><cell>conv1</cell><cell>74</cell><cell></cell><cell>39%</cell><cell>89%</cell><cell>45.2K</cell></row><row><cell>conv2</cell><cell>749</cell><cell></cell><cell>41%</cell><cell>57%</cell><cell>54.4K</cell></row><row><cell>fc1</cell><cell>4151</cell><cell></cell><cell>N/A</cell><cell>79%</cell><cell>4.7K</cell></row><row><cell>fc2</cell><cell>632</cell><cell></cell><cell>N/A</cell><cell>58%</cell><cell>1.0K</cell></row><row><cell>fc3</cell><cell>166</cell><cell></cell><cell>N/A</cell><cell>100%</cell><cell>0.2K</cell></row><row><cell>Total</cell><cell>5772</cell><cell></cell><cell>N/A</cell><cell>N/A</cell><cell>105K</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 6</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Different inference models for MNIST</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Model</cell><cell></cell><cell></cell><cell>Method</cell><cell>Error #Param FLOPs</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Linear classifier [7]</cell><cell>-</cell><cell>8.40%</cell><cell>4K</cell><cell>8K</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">RBF network [7]</cell><cell></cell><cell>-</cell><cell>3.60%</cell><cell>794K</cell><cell>1588K</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Polynomial classifier [7]</cell><cell>-</cell><cell>3.30%</cell><cell>40K</cell><cell>78K</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">K-nearest neighbors [7]</cell><cell>-</cell><cell>3.09%</cell><cell>47M</cell><cell>94M</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">SVMs (reduced set) [31]</cell><cell>-</cell><cell>1.10%</cell><cell>650K</cell><cell>1300K</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Caffe model (LeNet-300-100) [32]</cell><cell>-</cell><cell>1.60%</cell><cell>266K</cell><cell>532K</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">LWS (LeNet-300-100) [20]</cell><cell>Prune</cell><cell>1.96%</cell><cell>4K</cell><cell>8K</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Net pruning (LeNet-300-100) [9]</cell><cell>Prune</cell><cell>1.59%</cell><cell>22K</cell><cell>43K</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Our LeNet-300-100: compact</cell><cell>Grow+Prune 1.58%</cell><cell>3.8K</cell><cell>6.7K</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Our LeNet-300-100: accurate</cell><cell>Grow+Prune 1.29%</cell><cell>7.8K</cell><cell>14.9K</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Caffe model (LeNet-5) [32]</cell><cell>-</cell><cell>0.80%</cell><cell>431K</cell><cell>4586K</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">LWS (LeNet-5) [20]</cell><cell>Prune</cell><cell>1.66%</cell><cell>4K</cell><cell>199K</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Net pruning (LeNet-5) [9]</cell><cell>Prune</cell><cell>0.77%</cell><cell>35K</cell><cell>734K</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Our LeNet-5</cell><cell></cell><cell>Grow+Prune 0.77%</cell><cell>5.8K</cell><cell>105K</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TABLE 7</cell><cell></cell><cell></cell></row><row><cell cols="6">Synthesized AlexNet (error rate 42.76%)</cell></row><row><cell cols="6">Layers #Param Conv% Act% FLOPs</cell></row><row><cell cols="2">conv1</cell><cell>17K</cell><cell>92%</cell><cell>87%</cell><cell>97M</cell></row><row><cell cols="2">conv2</cell><cell>107K</cell><cell>91%</cell><cell>82%</cell><cell>124M</cell></row><row><cell cols="2">conv3</cell><cell>164K</cell><cell>88%</cell><cell>49%</cell><cell>40M</cell></row><row><cell cols="2">conv4</cell><cell>253K</cell><cell>86%</cell><cell>48%</cell><cell>36M</cell></row><row><cell cols="2">conv5</cell><cell>180K</cell><cell>87%</cell><cell>56%</cell><cell>25M</cell></row><row><cell>fc1</cell><cell></cell><cell>1.8M</cell><cell>N/A</cell><cell>49%</cell><cell>2.0M</cell></row><row><cell>fc2</cell><cell></cell><cell>0.8M</cell><cell>N/A</cell><cell>47%</cell><cell>0.8M</cell></row><row><cell>fc3</cell><cell></cell><cell>0.5M</cell><cell>N/A</cell><cell>100%</cell><cell>0.5M</cell></row><row><cell>Total</cell><cell></cell><cell>3.9M</cell><cell>N/A</cell><cell>N/A</cell><cell>325M</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TABLE 8</cell><cell></cell><cell></cell></row><row><cell cols="6">Synthesized VGG-16 (error rate 30.72%)</cell></row><row><cell>Layer</cell><cell cols="6">#Param FLOPs #Param Act% FLOPs</cell></row><row><cell></cell><cell></cell><cell cols="2">Original VGG-16</cell><cell cols="3">Synthesized VGG-16</cell></row><row><cell>conv1 1</cell><cell></cell><cell>2K</cell><cell>0.2B</cell><cell>1K</cell><cell>64%</cell><cell>0.1B</cell></row><row><cell>conv1 2</cell><cell></cell><cell>37K</cell><cell>3.7B</cell><cell>9K</cell><cell>76%</cell><cell>0.6B</cell></row><row><cell>conv2 1</cell><cell></cell><cell>74K</cell><cell>1.8B</cell><cell>19K</cell><cell>73%</cell><cell>0.4B</cell></row><row><cell>conv2 2</cell><cell></cell><cell>148K</cell><cell>3.7B</cell><cell>36K</cell><cell>76%</cell><cell>0.7B</cell></row><row><cell>conv3 1</cell><cell></cell><cell>295K</cell><cell>1.8B</cell><cell>72K</cell><cell>53%</cell><cell>0.3B</cell></row><row><cell>conv3 2</cell><cell></cell><cell>590K</cell><cell>3.7B</cell><cell>94K</cell><cell>57%</cell><cell>0.3B</cell></row><row><cell>conv3 3</cell><cell></cell><cell>590K</cell><cell>3.7B</cell><cell>100K</cell><cell>56%</cell><cell>0.4B</cell></row><row><cell>conv4 1</cell><cell></cell><cell>1M</cell><cell>1.8B</cell><cell>187K</cell><cell>37%</cell><cell>0.2B</cell></row><row><cell>conv4 2</cell><cell></cell><cell>2M</cell><cell>3.7B</cell><cell>306K</cell><cell>37%</cell><cell>0.2B</cell></row><row><cell>conv4 3</cell><cell></cell><cell>2M</cell><cell>3.7B</cell><cell>313K</cell><cell>35%</cell><cell>0.2B</cell></row><row><cell>conv5 1</cell><cell></cell><cell>2M</cell><cell>925M</cell><cell>319K</cell><cell>33%</cell><cell>47M</cell></row><row><cell>conv5 2</cell><cell></cell><cell>2M</cell><cell>925M</cell><cell>303K</cell><cell>32%</cell><cell>42M</cell></row><row><cell>conv5 3</cell><cell></cell><cell>2M</cell><cell>925M</cell><cell>302K</cell><cell>24%</cell><cell>40M</cell></row><row><cell>fc1</cell><cell></cell><cell>103M</cell><cell>206M</cell><cell>1.5M</cell><cell>38%</cell><cell>0.8M</cell></row><row><cell>fc2</cell><cell></cell><cell>17M</cell><cell>34M</cell><cell>218K</cell><cell>41%</cell><cell>0.2M</cell></row><row><cell>fc3</cell><cell></cell><cell>4M</cell><cell>8M</cell><cell>387K</cell><cell>100%</cell><cell>0.4M</cell></row><row><cell>Total</cell><cell></cell><cell>138M</cell><cell>30.9B</cell><cell>4.2M</cell><cell>N/A</cell><cell>3.5B</cell></row><row><cell cols="7">amount of redundancy associated with the original large</cell></row><row><cell cols="7">DNNs. Network growth can alleviate this phenomenon. We</cell></row><row><cell cols="7">further compare the synthesis cost of NeST against RL-</cell></row><row><cell cols="5">based neural architecture search in</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 10 .</head><label>10</label><figDesc>When applied to ResNet-50, NeST yields similar network compactness and accuracy to NASNet<ref type="bibr" target="#b13">[14]</ref> at a much lower synthesis cost.Note that our current mask-based implementation of growth and pruning incurs a temporal memory overhead during training. If the model becomes deeper and larger, as in the case of ResNet-152</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 9 Different</head><label>9</label><figDesc>AlexNet, VGG-16, and ResNet-50 based inference models for ImageNet * Currently without partial-area convolution due to GPU memory limits.</figDesc><table><row><cell>Model</cell><cell>Method</cell><cell cols="2">∆Top-1 err. ∆Top-5 err.</cell><cell>#Param (M)</cell><cell>FLOPs (B)</cell></row><row><cell>Baseline AlexNet [3]</cell><cell>-</cell><cell>0.0%</cell><cell>0.0%</cell><cell>61 (1.0×)</cell><cell>1.5 (1.0×)</cell></row><row><cell>Data-free pruning [34]</cell><cell>Prune</cell><cell>+1.62%</cell><cell>-</cell><cell>39.6 (1.5×)</cell><cell>1.0 (1.5×)</cell></row><row><cell>Fastfood-16-AD [35]</cell><cell>-</cell><cell>+0.12%</cell><cell>-</cell><cell>16.4 (3.7×)</cell><cell>1.4 (1.1×)</cell></row><row><cell>Memory-bounded [36]</cell><cell>-</cell><cell>+1.62%</cell><cell>-</cell><cell>15.2 (4.0×)</cell><cell>-</cell></row><row><cell>SVD [37]</cell><cell>-</cell><cell>+1.24%</cell><cell>+0.83%</cell><cell>11.9 (5.1×)</cell><cell>-</cell></row><row><cell>LWS (AlexNet) [20]</cell><cell>Prune</cell><cell>+0.33%</cell><cell>+0.28%</cell><cell>6.7 (9.1×)</cell><cell>0.5 (3.0×)</cell></row><row><cell>Net pruning (AlexNet) [9]</cell><cell>Prune</cell><cell>-0.01%</cell><cell>-0.06%</cell><cell>6.7 (9.1×)</cell><cell>0.5 (3.0×)</cell></row><row><cell>Our AlexNet</cell><cell>Grow+Prune</cell><cell>-0.02%</cell><cell>-0.06%</cell><cell>3.9 (15.7×)</cell><cell>0.33 (4.6×)</cell></row><row><cell>Baseline VGG-16 [4]</cell><cell>-</cell><cell>0.0%</cell><cell>0.0%</cell><cell cols="2">138 (1.0×) 30.9 (1.0×)</cell></row><row><cell>LWS (VGG-16) [20]</cell><cell>Prune</cell><cell>+3.61%</cell><cell cols="2">+1.35% 10.3 (13.3×)</cell><cell>6.5 (4.8×)</cell></row><row><cell>Net pruning (VGG-16) [9]</cell><cell>Prune</cell><cell>+2.93%</cell><cell cols="2">+1.26% 10.3 (13.3×)</cell><cell>6.5 (4.8×)</cell></row><row><cell>Our VGG-16: accurate Our VGG-16: compact</cell><cell>Grow+Prune Grow+Prune</cell><cell>-0.35% +2.31%</cell><cell>-0.31% +1.05%</cell><cell>9.9 (13.9×) 4.2 (33.2×)</cell><cell>6.3 (4.9×) 3.5 (8.9×)  *</cell></row><row><cell>Baseline ResNet-50 [6]</cell><cell>-</cell><cell>0.0%</cell><cell>0.0%</cell><cell>25.6 (1.0×)</cell><cell>7.6 (1.0×)</cell></row><row><cell>Filter pruning (ResNet-50) [38]</cell><cell>Prune</cell><cell>+4.11%</cell><cell>+2.20%</cell><cell>16.9 (1.5×)</cell><cell>4.9 (1.6×)</cell></row><row><cell>LWS (ResNet-50) [20]</cell><cell>Prune</cell><cell>-</cell><cell>+2.87%</cell><cell>12.8 (2.0×)</cell><cell>-</cell></row><row><cell>Net pruning (ResNet-50) [39]</cell><cell>Prune</cell><cell>-</cell><cell>+0.57%</cell><cell>10.2 (2.5×)</cell><cell>-</cell></row><row><cell>Our ResNet-50</cell><cell>Grow+Prune</cell><cell>+0.93%</cell><cell>+0.57%</cell><cell>6.3 (4.1×)</cell><cell>3.8 (2.1×)</cell></row></table><note><p>*     </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 10</head><label>10</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Synthesis cost comparison</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Method</cell><cell cols="2">Synthesis #Params cost  *</cell><cell>Top-5 error (%)</cell></row><row><cell>NASNet-A [14]</cell><cell>RL</cell><cell>2,000</cell><cell>5.3M</cell><cell>8.4</cell></row><row><cell>Our ResNet-50</cell><cell>Grow+Prune</cell><cell>60</cell><cell>6.3M</cell><cell>7.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>* Synthesis cost in GPU-days.</p></note></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by NSF Grant No. CNS-1617640. Xiaoliang Dai, Hongxu Yin, and Niraj K. Jha are with the</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Modeling</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">EIE: Efficient inference engine on compressed deep neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pedram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symp. on Computer Architecture</title>
		<meeting>Int. Symp. on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="243" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01041</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deeparchitect: Automatically designing and training deep architectures</title>
		<author>
			<persName><forename type="first">R</forename><surname>Negrinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gordon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08792</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Evolving deep neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Meyerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Francon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shahrzad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Navruzyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duffy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00548</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Designing neural network architectures using reinforcement learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02167</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07012</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic node creation in backpropagation networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="365" to="375" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Compressing deep convolutional networks using vector quantization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6115</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Pruning filters for efficient convnets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08710</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Network trimming: A data-driven neuron pruning approach towards efficient deep architectures</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.03250</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning to prune deep neural networks via layer-wise optimal brain surgeon</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07565</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning structured sparsity in deep neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2074" to="2082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sparse convolutional neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="806" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic network surgery for efficient DNNs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1379" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">DSD: Regularizing deep neural networks with dense-sparse-dense training flow</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04381</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Selection of intrinsic horizontal connections in the visual cortex by correlated neuronal activity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lowel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">255</biblScope>
			<biblScope unit="issue">5041</biblScope>
			<biblScope unit="page">209</biblScope>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Machine Learning</title>
		<meeting>Int. Conf. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">One weird trick for parallelizing convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.5997</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Compact deep convolutional neural networks with coarse pruning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09639</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving the accuracy and speed of support vector machines</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="375" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00853</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Data-free parameter pruning for deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.06149</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep fried ConvNets</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moczulski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision</title>
		<meeting>IEEE Int. Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1476" to="1483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Memory bounded deep convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1442</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Exploiting linear structure within convolutional networks for efficient evaluation</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1269" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">ThiNet: A filter level pruning method for deep neural network compression</title>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06342</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Exploring the regularity of sparse structure in convolutional neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08922</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The neurological development of the child with the educational enrichment in mind</title>
		<author>
			<persName><forename type="first">G</forename><surname>Leisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mualem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Mughrabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psicología Educativa</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="79" to="96" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
