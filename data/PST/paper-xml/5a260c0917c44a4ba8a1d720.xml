<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mining Twitter Feeds for Software User Requirements</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Grant</forename><surname>Williams</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Division of Computer Science and Engineering</orgName>
								<orgName type="institution">Louisiana Sate University Baton Rouge</orgName>
								<address>
									<postCode>70803</postCode>
									<region>LA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anas</forename><surname>Mahmoud</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Division of Computer Science and Engineering</orgName>
								<orgName type="institution">Louisiana Sate University Baton Rouge</orgName>
								<address>
									<postCode>70803</postCode>
									<region>LA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mining Twitter Feeds for Software User Requirements</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A7DB89DEDB9D90C820381A0582AA5668</idno>
					<idno type="DOI">10.1109/RE.2017.14</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Twitter enables large populations of end-users of software to publicly share their experiences and concerns about software systems in the form of micro-blogs. Such data can be collected and classified to help software developers infer users' needs, detect bugs in their code, and plan for future releases of their systems. However, automatically capturing, classifying, and presenting useful tweets is not a trivial task. Challenges stem from the scale of the data available, its unique format, diverse nature, and high percentage of irrelevant information and spam. Motivated by these challenges, this paper reports on a three-fold study that is aimed at leveraging Twitter as a main source of software user requirements. The main objective is to enable a responsive, interactive, and adaptive data-driven requirements engineering process. Our analysis is conducted using 4,000 tweets collected from the Twitter feeds of 10 software systems sampled from a broad range of application domains. The results reveal that around 50% of collected tweets contain useful technical information. The results also show that text classifiers such as Support Vector Machines and Naive Bayes can be very effective in capturing and categorizing technically informative tweets. Additionally, the paper describes and evaluates multiple summarization strategies for generating meaningful summaries of informative software-relevant tweets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In recent years, Twitter has become one of the most popular micro-blogging social media platforms, providing an outlet for millions of users around the world to share their daily activities through real-time status updates. As of the fourth quarter of 2015, Twitter has averaged around 305 million monthly active users. The sheer volume of real-time and highly-diverse data provided by Twitter has revolutionized research in many data science areas. Twitter data has been leveraged to predict the daily ups and downs of the stock market <ref type="bibr" target="#b0">[1]</ref>, predict the political affiliation of the masses <ref type="bibr" target="#b1">[2]</ref>, and uncover and explain temporal variations in social happiness <ref type="bibr" target="#b2">[3]</ref>.</p><p>From a software engineering perspective, Twitter has created an unprecedented opportunity for software providers to monitor the opinions of large populations of end-users of their systems <ref type="bibr" target="#b3">[4]</ref>. Using Twitter, end-users of software can publicly express their needs and concerns in the form of micro-blogs. In fact, it has become a social media tradition that, with the release of each new mobile app, operating system, or web service, people resort to Twitter to describe their experiences and problems and recommend software to their friends, causing these systems to be trending worldwide. Such data can be leveraged to extract rich and timely information about newly-released systems, enabling developers to get instant technical and social feedback about their software.</p><p>Motivated by these observations, this paper reports on a three-fold study that is aimed at leveraging Twitter as a main source of useful software user feedback. In particular, we evaluate the performance of multiple data classification and summarization techniques in automatically detecting and summarizing technical user concerns raised in software-relevant tweets. Automation is necessary to deal with the massive scale of Twitter data available, its unique format, and diverse nature <ref type="bibr" target="#b1">[2]</ref>. Generated feedback can be used as an input for a wellinformed release-planning process by pointing out critical bugs and helping developers prioritize the most desired features that need to be addressed in forthcoming releases <ref type="bibr" target="#b4">[5]</ref>. This presents an advantage over classical user feedback collection methods that rely on face-to-face, user reviews, bug tracking, or survey communication. Ultimately, our main objective is to support an adaptive data-driven requirements engineering process that can detect users' needs in an effective and timely manner. In particular, in this paper, we document the following contributions:</p><p>• We collect and manually classify 4,000 tweets sampled from the Twitter feeds of 10 different software systems. These systems extends over a broad range of application domains. Our objective is to qualitatively assess the technical value of software-relevant tweets. • We employ two text classification techniques to accurately capture and categorize the various types of actionable software maintenance requests present in software systems' Twitter feeds. • We investigate the performance of various text summarization techniques in generating compact summaries of the common concerns raised in technically informative tweets.</p><p>The remainder of this paper is organized as follows. Section II motivates our work and describes our research questions. Section III describes our data collection and qualitative analysis process. Section IV and V evaluate the performance of various text classification and summarization strategies for capturing and summarizing technically informative tweets. Section VI discusses the threats to the study's validity. Section VII reviews related work. Finally, Section VIII concludes the paper and discusses prospects for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MOTIVATION AND RESEARCH QUESTIONS</head><p>The explosive growth of the computational capabilities of computing devices has led to a drastic increase in the demand for software <ref type="bibr" target="#b5">[6]</ref>. This in turn has led to a huge spike in the number of software systems released on a daily basis. For instance, as of March 2015, the Apple App Store alone has reported around 2.25 million active apps, growing by over 1000 apps per day. Software systems, such as operating systems, video games, and social networks have expanded in use to reach vastly broad and diverse populations of users.</p><p>This unprecedented level of competition has encouraged software providers to look beyond traditional software engineering practices into methods that enable them to connect with their end-users in a more effective and instant way. An underlying tenet is that user involvement in the software process is a major contributing factor to software success <ref type="bibr" target="#b6">[7]</ref>. User feedback contains important information that helps developers to understand user requirements and expectations and identify missing features <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Considerable work in this domain has been focused on mining user reviews in mobile app stores (e.g., Apple App Store, Google Play, and Nokia Ovi) <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Recent analysis of large datasets of app store user reviews has revealed that almost one third of these reviews were technically informative to app developers <ref type="bibr" target="#b8">[9]</ref>.</p><p>Motivated by these observations, in this paper we exploit the online micro-blogging service Twitter as a more open, more widespread, and more instant source of technicallyinformative software information. Unlike user reviews in mobile application stores, Twitter feedback is not limited to mobile applications. Rather, it extends to any software system with a sizable user base. Prior research on leveraging microblogging services in software engineering has been focused on the developer side, or how communities of software engineers use such services to support their daily development activities (Sec. VII). Analysis of sample software-relevant tweets has revealed that developers frequently make use of social media as a means to facilitate team coordination, learn about new technologies, and stay in touch with the interests and opinions of all stakeholders <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>.</p><p>In our analysis, we examine the value of Twitter as a source of user feedback that can be translated into actionable software engineering requests. The main objective is to help software developers to instantly and directly connect with their users, and ultimately, survive in a highly-competitive and volatile market. Based on these assumptions, we formulate the following research questions:</p><p>• RQ 1 : How informative is Twitter data for software engineers? Twitter is a public service. Millions of tweets are generated every day by millions of users all over the world about a vast spectrum of subjects. The assumption that all these tweets carry useful technical information is unrealistic. For instance, users might tweet about software to share their experience with others, ask people to follow them on social media platforms, or recommend a video game to their friends. Therefore, the first objective of our analysis is to determine how technically informative, or useful, software users' tweets are. Technically informative tweets can be described as any user concern that can be translated into an actionable software maintenance request, such as a bug report or a user requirement.</p><p>Uninformative tweets, on the other hand, can be simply spam or messages that provide no immediate technical feedback to the developer. This emphasizes the need for automated methods to summarize informative tweets in such a way that enables a more effective data exploration process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DATA COLLECTION AND QUALITATIVE ANALYSIS</head><p>In this section, we answer our first research question regarding the potential value of tweets for software developers. In particular, we describe our data collection process along with the main findings of our manual qualitative analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Collection</head><p>In our analysis, we used Twitter's Search API to collect our dataset <ref type="bibr" target="#b15">[16]</ref>. This API can be customized to search for a specific word or hashtag (#) in Twitter feeds. Twitter has integrated hashtags into the core architecture of the service, allowing users to search for these terms explicitly to retrieve a list of recent tweets about a specific topic. Searching through hashtags can be effective when Twitter is mined at a massive scale to infer the opinions of the masses towards a certain topic, such as learning peoples' views of a certain public figure (search for #obama) or certain recent events (#election) <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b16">[17]</ref>. However, one of the main drawbacks of hashtag, or word, search is the very high noise-to-signal ratio. More specifically, such queries can return millions of tweets. For example, a search for #snapchat returns millions of tweets of the nature "please follow me on #snapchat". While such vast amounts of data can be very useful for inferring public trends, classifying such data manually can be a tedious task. To overcome these limitations, in our analysis, we limit our data collection process to tweets addressed directly to the Twitter account of a given software product (e.g., tweets beginning with @Windows10). This strategy ensures that only tweets that are meant to be a direct interaction with the software provider are included.  We select 10 software products from a broad range of application domains to conduct our analysis (Table <ref type="table">I</ref>). The data collection process was repeated on a daily basis from April 1 st to May 27 th of 2016. The resulting dataset contained 188, 737 unique tweets. Fig. <ref type="figure">1</ref> shows the number of tweets collected per day over the course of our data collection process. Random sampling is used to prepare our dataset. In particular, a Ruby script is used to randomly select 400 tweets from the set of tweets collected for each of our software systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. What's in a tweet?</head><p>To get a sense of the information value of our data (i.e., to answer RQ 1 ), the sampled data is manually analyzed. To conduct our analysis, we adopt the standard categorization typically used to classify user reviews in app store mining research <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b10">[11]</ref>. More specifically, we assume that tweets that are addressed to the account of a software system can be classified into two main categories, including technically informative and uninformative messages.</p><p>Our manual classification process was performed by three independent industry professional experts with an average of 5 years of experience in software development. Each expert examined each of the sampled tweets in our dataset. A majority vote was taken in cases of a conflict. Conflicts arise in cases that carry a double meaning. For example, the tweet "@android run app and it dies, any way to know why it dies rather than looking through logs?" could be classified as a bug report (i.e., it dies) or a user requirement (i.e., any way to). In total, 121 conflicts were detected in our data (≈ 3%). Table <ref type="table">I</ref> summarizes our findings <ref type="foot" target="#foot_0">1</ref> . The outcome of our manual classification process can be described as follows:</p><p>• Bug reports: These tweets report a potential problem with the software. For example, "@googlechrome I have never ever seen the auto-update function of chrome work on any of all my computers." and "@Photoshop When will the biggest Photoshop issue of: lag, freezing, unresponsive Marquee Tool be fixed?". • User requirements: These tweets mainly include requests for new features, or alternatively express that a recently added feature is undesirable. For example "@Snapchat pls make it to where I can see an individual score with someone so I know how many snaps we've sent back &amp; forth !". Some requests tend to be less obvious, especially requests for a removed feature to be added back, for example "@googlechrome any chance I could get my bookmark folders back now please?". Such requests can play a crucial rule in release planning as they help developers to decide what features to include/omit in the new release. • Miscellaneous and spam: A considerable part of software-relevant tweets do not provide any useful technical information to the developer. Such tweets might include praise (e.g. "@VisualStudio is quickly becoming my #goto #dev environment"), insults (e.g. "mediocre as usual #meh"), general information or news (e.g. "@WhatsApp announced it's started full end-to-end #encryption across its messaging app."), and spam. Spammers take advantage of the openness and popularity of Twitter to spread unsolicited messages to legitimate users <ref type="bibr" target="#b17">[18]</ref>. For instance, spammers tend to post tweets containing typical words of trending topics along with URLs that lead users to completely unrelated websites (e.g. "#imgur #fix #problem bit.ly/1xTYs"). In summary, our manual analysis shows that, out of the 4000 tweets examined, 51% of these tweets were technically informative (27% bug reports and 24% user requirements), while the other 49% were basically spam and miscellaneous information. These findings answer RQ 1 and motivate RQ 2 and RQ 3 . In particular, given that around half of our data contain potentially useful information, how can that information be automatically identified and effectively summarized? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Classifiers</head><p>To answer the first part of RQ 2 , we investigate the performance of two text classification algorithms, including Naive Bayes (NB) and Support Vector Machines (SVM). SVM and NB have been found to work well with short text. Shorttext is a relatively recent Natural Language Processing (NLP) type of text that has been motivated by the explosive growth of micro-blogs on social media (e.g., Tweets and YouTube and Facebook comments) and the urgent need for effective methods to analyze such large amounts of lexically and semantically limited textual data <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. For instance, Twitter posts are limited to 140 character long messages (tweets) and typically contain colloquial terms (e.g., LOL, smh, idk), hyperlinks, Twitter-specific characters such as hashtags (#) and mentions (@), along with phonetic spellings and other neologisms <ref type="bibr" target="#b19">[20]</ref>. In detail, NB and SVM can be described as follows:</p><p>• Naive Bayes (NB): NB is an efficient linear probabilistic classifier that is based on Bayes' theorem <ref type="bibr" target="#b20">[21]</ref>. NB assumes the conditional independence of the attributes of the data. In other words, classification features are independent of each other given the class. In the context of text classification, the features of the model can be defined as the individual words of the text. Under this approach, known as the Bag-of-Words (BOW), the data is typically represented by a 2-dimensional word x document matrix. In the Bernoulli NB model, an entry in the matrix is a binary value that indicates whether the document contains a word or not (i.e., {0,1}). The Multinomial NB, on the other hand, uses normalized frequencies of the words in the text to construct the word x document matrix <ref type="bibr" target="#b21">[22]</ref>. • Support Vector Machines (SVM): SVM is a supervised machine learning algorithm that is used for classification and regression analysis in multidimensional data spaces <ref type="bibr" target="#b22">[23]</ref>. SVM attempts to find optimal hyperplanes for linearly separable patterns in the data and then maximizes the margins around these hyperplanes. Technically, support vectors are the critical instances of the training set that would change the position of the dividing hyperplane if removed. SVM classifies the data by mapping input vectors into an N-dimensional space, and deciding on which side of the defined hyperplane the data instance lies. SVMs have been shown to be effective in domains where the data is sparse and highly dimensional <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Classification Features</head><p>To deal with its unique limited form of text, researchers typically use combinations of Twitter features to help the classifier make more accurate decisions <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. These features include:</p><p>• Textual Content (BOW): Our main classification feature is the words of the tweet. The phrase Bag-of-Words (BOW), stems from the fact that the text is simply represented as an un-ordered collection of words. Given that Twitter limits messages to 140 characters, a tweet typically has 14 words on average. • Text processing: This set of features includes text reduction strategies such as stemming (ST) and stopword (SW) removal. Stemming reduces words to their morphological roots. This leads to a reduction in the number of features (words) as only one base form of the word is considered. Stop-word removal, on the other hand, is concerned with removing English words that are considered too generic (e.g., the, in, will). We further remove words that appear in one data instance (tweet) since they are highly unlikely to carry any generalizable information <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b26">[27]</ref>. • Sentiment Analysis (SA): Sentiment analysis is concerned with determining whether a text conveys positive or negative feelings. Sentiment analysis has been found to play a paramount role in Twitter data analytics <ref type="bibr" target="#b27">[28]</ref>.</p><p>For instance, specific Twitter moods, or sentiments, were found to correlate with public opinion regarding subjects such as consumer confidence and political affiliation, even predicting the movement of the stock market <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b16">[17]</ref>. In our analysis, we assume that a negative sentiment might be associated with a bad experience, such as a system failure or a bad feature. A positive sentiment, on the other hand, might indicate a positive experience <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation</head><p>To implement NB and SVM, we use Weka <ref type="bibr" target="#b28">[29]</ref>, a data mining suite that implements a wide variety of machine learning and classification techniques. We also use Weka's built-in stemmer (IteratedLovinsStemmer <ref type="bibr" target="#b29">[30]</ref>) and stop-word list to preprocess the tweets in our dataset. In our analysis, we use Multinomial NB, which uses the normalized frequency (TF) of words in their documents <ref type="bibr" target="#b21">[22]</ref>. Multinomial Naive Bayes is known to be a robust text classifier, consistently outperforming the binary feature model (Multi-variate Bernoulli) in highly diverse, real-world corpora <ref type="bibr" target="#b21">[22]</ref>. SVM is invoked through Weka's SMO, which implements John Platt's sequential minimal optimization algorithm for training a support vector classifier <ref type="bibr" target="#b30">[31]</ref>. In our analysis, the best results were obtained using the Pearson VII function-based universal kernel (Puk) with kernel parameters σ = 8 and ω = 1 <ref type="bibr" target="#b31">[32]</ref>. Universal Kernels are known to be effective for a large class of classification problems, especially for noisy data <ref type="bibr" target="#b32">[33]</ref>.</p><p>Sentiment analysis is performed using Sentistrength [34]. Sentistrength analyzes a document and assigns it two values: a positive sentiment strength and a negative sentiment strength <ref type="bibr" target="#b33">[35]</ref>, <ref type="bibr" target="#b34">[36]</ref>. In particular, the input text is rated by the sentiment content of each word. Most words are neutral, but some words, such as "loved" or "hated" increase the respective positive or negative sentiment score for their sentence. In our analysis, a text is basically a tweet. For example, the tweet "@googlechrome really is the best option for someone who enjoys platform agnosticism. All my stuff in @Windows and #OSX" receives a positive score of 3 and a negative score of 1. The positive score originates from the words best (1 point) and enjoys (2 points) and the negative score of 1 point is the default score for texts with no negative terminology.</p><p>To train our classifiers, we use 10-fold cross validation. This method creates 10 partitions of the dataset such that each partition has 90% of the instances as a training set and 10% as an evaluation set. The benefit of this technique is that it uses all the data for building the model, and the results often exhibit significantly less variance than those of simpler techniques such as the holdout method (e.g., 70% training set, 30% testing set).</p><p>Recall, precision, and F-measure are used to evaluate the performance of the different classification techniques used in our analysis. Recall is a measure of coverage. It represents the ratio of correctly classified instances under a specific label to the number of instances in the data space that actually belong to that label. Precision, on the other hand, is a measure of accuracy. It represents the ratio of correctly classified instances under a specific label to the total number of classified instances under that label. Formally, if A is the set of data instances in the data space that belong to the label λ, and B is the set of data instances that were assigned by the classifier to that label, then recall (R) can be calculated as R λ = |A ∩ B|/|A| and precision (P) can be calculated as P λ = |A ∩ B|/|B|. We also use F = 2P R/(P + R) to measure the harmonic mean of recall and precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results and Discussion</head><p>The results of our classification process are shown in Table <ref type="table">II</ref>. In terms of classifiers' accuracy, on average, both SVM and NB were able to achieve competitive results. These results can be explained based on the characteristics of our data space. More specifically, even though Twitter messages are limited in size, the feature space (number of words) is typically very large <ref type="bibr" target="#b23">[24]</ref>. This can be attributed to the fact that people use informal language (slang, acronyms, abbreviations) in their tweets. This drastically increases the number of features the classifier needs to process, and also leads the vector representation (BOW) of Twitter messages to be very sparse. While machine learning algorithms tend to over-learn when the dimensionality is high, SVM has an over-fitting avoidance tendency-an inherent behavior of margin maximization which does not depend on the number of features <ref type="bibr" target="#b35">[37]</ref>. Therefore, it has the potential to scale up to high-dimensional data spaces with sparse instances. NB tends to be more robust to noise, which seems to work in Twitter data classification, despite the unrealistic conditional independence assumption among classification features <ref type="bibr" target="#b25">[26]</ref>.</p><p>In terms of classification features, the results also show that sentiment scores had almost no impact on performance. This can be attributed to the fact that, unlike political tweets which tend to be very polarized, and typically carry intense emotions <ref type="bibr" target="#b1">[2]</ref>, software-relevant tweets tend to be neutral. To gain more insight into these results, the boxplots in Fig. <ref type="figure">2</ref> show the average combined sentiment score of the different classes of tweets averaged over all our software systems. The figure shows that while bugs tend to be slightly negative (- 1), and requirements slightly positive (+1), the difference was not enough to affect the classification accuracy. Furthermore, miscellaneous tweets, while they might carry some extreme emotions, also tend to average out to a neutral state (-1, +1). Our results show that text processing features, such as stemming (ST) and stop-word removal (SW), produced mixed results for different classifiers. On the one hand, SVM's performance was slightly enhanced when stemming was applied, while NB performance slightly dropped. On the other hand, removing English stop-words seems to have a more noticeable negative impact on the results. In general, stop words seem to add important information value to the classifier. For instance, some of these words (e.g., would, should, will, don't, please) actually represent distinctive features of user requirements and bug reports (e.g., "@Snapchat would you please bring the dog filter back?"). Therefore, removing such words leads to a decline in the classification accuracy.</p><p>In summary, to answer RQ 2 , our results show that in the context of software relevant tweets, the textual content of Twitter messages is the only contributing factor to the classification accuracy. Other features that are often used as supplemental attributes to enhance Twitter data are irrelevant. The results also confirm previous findings regarding the suitability of SVM and NB as robust classifiers for Twitter data <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. SUMMARIZATION</head><p>The third phase of our analysis is focused on generating succinct summaries of the technically useful software tweets. A summary can be described as a short and compact description that encompasses the main theme of a collection of tweets related to a similar topic <ref type="bibr" target="#b36">[38]</ref>, <ref type="bibr" target="#b37">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Tweets Summarization: A Pilot Study</head><p>The summarization task in our analysis can be described as a multi-document summarization problem, where each tweet is considered as a separate document. In general, multidocument summarization techniques can be either extractive or abstractive. Extractive methods select specific documents, or keywords, already present within the data as representatives of the entire collection. Abstractive methods, on the other hand, attempt to generate a summary with a proper English narrative from the documents. Generating abstractive summaries can be a very challenging task. It involves heavy reasoning and lexical parsing to paraphrase novel sentences around information extracted from the corpus <ref type="bibr" target="#b38">[40]</ref>. This problem becomes more challenging when dealing with the lexically and semantically limited Twitter messages. Therefore, extractive summarization techniques are typically employed to summarize microblogging data <ref type="bibr" target="#b39">[41]</ref>.</p><p>To get a sense of how developers would identify the main topics in a list of software-relevant tweets, we conducted a pilot study using two expert programmers with more than 10 years of programming experience each. Each expert was assigned 4 sets of tweets, including 2 sets of bug reporting tweets and 2 sets of user requirement tweets from 4 different systems, including: Snapchat, Chrome, Whatsapp, and Win-dows10. Their task was to go through each set and identify ten tweets that they thought captured the main topics raised in the set. No time constraint was enforced.</p><p>Our experts were interviewed after the experiment. Both of them implied that they initially identified the main topics in the tweets after going through the set once or twice. Once these topics were identified (stood out due to their frequent appearance), they selected tweets that included requests (terms and phrases) related to the main topics identified. Our experts were then provided with a word cloud for each set of tweets they were asked to summarize. Each cloud includes the most frequent 30 terms from each set, where more frequent words are drawn in larger font. Our experts were then asked if they thought these clouds were sufficient to convey the main concerns raised in the set. Both experts implied that they preferred to see full tweet summaries over keyword summaries. In general, word-clouds lack context and structure. In contrast, full tweets have the advantage of being full sentences, and thus, can carry more meaningful information <ref type="bibr" target="#b40">[42]</ref>, <ref type="bibr" target="#b37">[39]</ref>. For example, examining the set of user requirement tweets addressed to Snapchat shows that they revolve around three main concerns, including users asking for new filters to be added, users complaining about the auto-play feature of Snapchat stories, and a few tweets raising portability concerns (requesting Snapchat to work on other devices and platforms). Table <ref type="table">III</ref> shows examples of the tweets related to the automatic play feature of Snapchat stories and the filter feature. Extracting these full tweets gives developers a better idea of what the common user concerns actually are. However, only displaying tags, such as in Fig. <ref type="figure">3</ref>, might be not as informative.</p><p>Based on our observations during our pilot study, in our analysis we examine the performance of frequency-based extractive summarization techniques in summarizing software relevant tweets. These techniques rely on the frequencies of words as an indication of their perceived importance <ref type="bibr" target="#b41">[43]</ref>. In other words, the likelihood of words appearing in a humangenerated summary is positively correlated with their frequency <ref type="bibr" target="#b42">[44]</ref>. Formally, a full-tweet extractive summarization process can be described as follows: given a topic keyword or phrase M and the desired length for the summary K, generate a set of representative tweets T with a cardinality of K such that ∀t i ∈ T, M ∈ t i and ∀t i , ∀t j ∈ T, t i t j . The condition t i t j is enforced to ensure that selected tweets provide sufficiently different information (i.e., are not redundant) <ref type="bibr" target="#b42">[44]</ref>.</p><p>In our analysis, we investigate the performance of a number of extractive summarization techniques that have been shown to work well in the context of mirco-blogging data on social media <ref type="bibr" target="#b42">[44]</ref>, <ref type="bibr" target="#b41">[43]</ref>, <ref type="bibr" target="#b43">[45]</ref>, <ref type="bibr" target="#b37">[39]</ref>. These techniques include:</p><p>• Hybrid Term Frequency (TF): Hybrid TF is the most basic method for determining the importance of a tweet. Formally, a word's w i value to the summary is computed as the frequency of the word in the entire collection of tweets f (w i ) divided by the number of unique words in the collection (N ). This hybrid modification over classical single-document TF is necessary to capture concerns that are frequent over the entire collection <ref type="bibr" target="#b42">[44]</ref>.</p><p>The probability of a tweet of length n words to appear in the summary is calculated as the average of the weights of its individual words: 1 n n i=1 f (w i )/N . • Hybrid TF.IDF: Introduced by Inouye and Kalita <ref type="bibr" target="#b42">[44]</ref>, the hybrid TF.IDF approach is a frequency-based summarization technique that is designed to summarize social media data. Hybrid TF.IDF accounts for a word's scarcity across all the tweets by using the inverse document frequency (IDF) of the word. IDF penalizes words that are too frequent in the text. Formally, TF.IDF can be computed as:</p><formula xml:id="formula_0">T F.IDF = T F (w i ) × log |D| |d j : w i ∈ d j ∧ d j ∈ D| (1)</formula><p>where T F (w i ) is the term frequency of the word w i in the entire collection, |D| is the total number of tweets in the collection, and |d j : w i ∈ d j ∧ d j ∈ D| is the number of tweets in D that contain the word w i . The importance of a tweet can then be calculated as the average TF.IDF score of its individual words.</p><p>To control for redundancy, or the chances of two very similar tweets getting selected, before adding a top tweet to the summary, the algorithm makes sure that the tweet does not have a textual similarity above a certain threshold with the tweets already in the summary. Similarity is calculated using the cosine between the vector representations of tweets. • SumBasic:</p><p>Introduced by Nenkova and Vanderwende <ref type="bibr" target="#b41">[43]</ref>, SumBasic uses the average term frequency (TF) of tweets' words to determine their value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE III EXAMPLE OF REQUIREMENT TWEETS RELATED TO SIMILAR FEATURES FROM SNAPCHAT'S TWITTER FEED</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tweets related to the feature ''filter''</head><p>Tweets related to the feature ''automatic play'' "where is my dog filter" "change the way you view stories back to the original way?" "can you bring back the bunny face filter pls" "I do not want to automatically watch people's stories!!" "more arty filters like the one today" "hate the stories autoplay feature" Fig. <ref type="figure">3</ref>. A word cloud summary of Snapchat's user requirement tweets However, the weight of individual words is updated after the selection of a tweet to minimize redundancy. This approach can be described as follows:</p><p>1) The probability of a word w i in the input corpus of size N words is calculated as ρ(w i ) = f (w i )/N , where f (w i ) is the frequency of the word in the entire corpus.</p><p>2) The weight of a tweet of length n words is calculated as the average probability of its words, given by:</p><formula xml:id="formula_1">1 n |n| i=1 ρ(w i ) 3)</formula><p>The best scoring tweet is selected. For each word in the selected tweet, its probability is reduced by</p><formula xml:id="formula_2">ρ(w i ) new = ρ(w i ) × ρ(w i ).</formula><p>This step is necessary to control for redundancy, or minimize the chances of selecting tweets describing the same topic with high frequency words. 4) Repeat from 2 until the required length of the summary is met.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation</head><p>We recruited 10 programmers (experts) to participate in our experiment, including 3 graduate students in computer science and 7 industry professionals. Our experts have reported an average of 6 years of programming experience. 5 systems from Table I were randomly selected to conduct our experiment. These systems include: Chrome, Minecraft, SnapChat, Whatsapp, and Windows 10. Each of our experts was assigned 2 different systems to summarize, such that, each system is summarized by exactly 4 different experts. For each system we provided two sets of tweets, including the set of bug reporting tweets and the set of user requirement tweets. The main task of the expert was to go through each set and identify 10 tweets that they believed captured the common concerns raised in the set. The tweets in each set were randomized ahead of time to avoid any ranking bias (e.g., an expert would always favor tweets from the top of the list). No time constraint was enforced. However, most of our participants responded within a one week period.</p><p>The various summarization techniques proposed earlier were then used to generate the automated summaries for the 5 systems included in our experiment. To enhance the quality of the generated summaries, English stop-words were excluded from our frequency analysis. Stemming was also applied to minimize the redundancy imposed by the usage of different variations of words (e.g., show, showing, shown, and shows).</p><p>To assess the quality of these summaries, for each system, we calculate the average term overlap between our experts' selected lists of tweets (reference summaries) and the various automatically generated summaries. Formally, a recall of a summarization technique t is calculated as:</p><formula xml:id="formula_3">Recall t = 1 |S| |S| i=1 match(t, s i ) count(s i ) (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>where S is the number of reference summaries, match(t, s i ) is the number of terms that appear in the reference summary s i and the automated summary generated by t, and count(s i ) is the number of unique terms in the reference summary s i . An automated summary that contains (recalled) a greater number of terms from the reference summary is considered more effective <ref type="bibr" target="#b44">[46]</ref>, <ref type="bibr" target="#b42">[44]</ref>, <ref type="bibr" target="#b41">[43]</ref>. In our analysis, recall is measured over different length summaries <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15</ref>, and 20 tweets included in the summary).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results and Discussion</head><p>The recall of the different summarization techniques is shown in Fig. <ref type="figure">4</ref> and Fig. <ref type="figure">5</ref>. Randomly generated summaries (tweets were selected randomly from each set using the .NET Random class) were used to compare the performance of our proposed methods. Our results show that all methods outperformed the random baseline. On average, SumBasic was more successful than hybrid TF.IDF and TF in summarizing the common concerns found in software-relevant tweets. TF achieved the poorest performance, suggesting that redundancy control is important in order to achieve comprehensive summaries. The results also show that TF was only slightly outperformed by hybrid TF.IDF. Due to the limited nature of the documents in our corpus (i.e., individual tweets), the IDF part seems to have a limited impact on the results as it is typically dominated by TF.</p><p>The better performance of SumBasic in comparison to hybrid TF.IDF can be explained based on their underlying redundancy control mechanisms. SumBasic tends to be more forgiving for redundant terms as it avoids excessive redundancy while also allowing common words to occasionally repeat in the summary. This can be useful in cases where there is relatively high redundancy in the data. Hybrid TF.IDF, by enforcing a similarity threshold on the entire tweet rather than individual words, can exclude important concerns from the summary. More specifically, changing the redundancy threshold can lead to extreme changes in the summaries (excluding a large number of tweets or hardly any). Finding an optimal threshold that works for all cases can be an exhaustive task especially that different datasets might require different thresholds. For instance, the best hybrid TF.IDF recall in our analysis was observed at similarity thresholds of 0.65 for bug reports and 0.50 for user requirements.</p><p>It is important to point out that more computationally expensive techniques such as Latent Dirichlet Allocation (LDA) <ref type="bibr" target="#b45">[47]</ref> and cluster-based summarization have been employed in the literature to summarize Twitter data <ref type="bibr" target="#b37">[39]</ref>. However, due to the lack of semantic structure in Twitter posts, such complex relational models were reported to be ineffective in capturing topical information. Furthermore, such techniques typically require heavy calibration of multiple parameters in order to generate decent results. This limits the practicality of such techniques and their ability to produce meaningful summaries <ref type="bibr" target="#b40">[42]</ref>, <ref type="bibr" target="#b36">[38]</ref>. Frequency-based techniques, on the other hand, are computationally inexpensive and relatively easier to implement and calibrate. This aspect can be crucial to achieve an easy transfer of our research findings to practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. THREATS TO VALIDITY</head><p>The study presented in this paper has several limitations that might affect the validity of the results <ref type="bibr" target="#b46">[48]</ref>. A potential threat to the proposed study's internal validity is the fact that human judgment is used to classify and summarize our sample tweets and prepare our ground-truth dataset. This might result in an experimental bias as humans tend to be subjective in their judgment. However, it is not uncommon in text classification to use humans to manually classify the data, especially in social media classification <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Similarly, evaluating machinegenerated against human-generated summaries is a standard evaluation procedure. Therefore, while the subjectivity and bias threats that stem from using humans are inevitable, they can be partially mitigated by using multiple judges at different levels of expertise.</p><p>In our experiment, there were minimal threats to construct validity as the standard performance measures (Recall, Pre- cision), which are extensively used in related research, were used to assess the performance of different methods. Threats to external validity impact the generalizability of results <ref type="bibr" target="#b46">[48]</ref>. A potential threat to our external validity stems from the fact that our dataset is limited in size and was generated from a limited number of software systems and tweets. To mitigate this threat, we ensured that our dataset was compiled from a wide variety of application domains. Furthermore, we used randomization to sample 400 tweets from the set of tweets collected for each system. While considering all the data instances in the analysis might enhance the validity of our results, manually analyzing such large amounts of data can be a tedious and error-prone task, and as a result, research on Twitter data analytics is often conducted using partial datasets. Other threats might stem from the tools we used in our analysis. For instance, we used Weka as our machine learning and classification platform and Sentistrength was used to tag our tweets' sentiment. Nonetheless, such tools have been extensively used in the literature and have been shown to generate robust results across a plethora of applications. Furthermore, using such publicly available benchmark tools enables other researchers to replicate our results.</p><p>Finally, it is unclear whether our approach will be as successful for systems that are less widely used. More specifically, there is no guarantee that such systems will have enough tweets that offer meaningful data for developers. In such cases, other sources of user feedback, such as app store reviews and online surveys, can be used to paint a full picture. Another concern stems from the fact that Twitter is often used for advertisement purposes by software vendors. Many tweets can be simply marketing messages from the vendor or competitors, rather than feedback from users. Therefore, there is no guarantee that the technically informative tweets captured by our approach are indeed from the right audience and not just containing publicity which accidentally describes as a requirement or a bug report.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RELATED WORK</head><p>In software engineering, the research on mining microblogging services has focused on the way developers use such platforms to share and exchange software development information. For instance, Bougie et al. <ref type="bibr" target="#b13">[14]</ref> manually analyzed 600 tweets from three different software engineering communities. The results showed that developers' tweets tend to include more conversation and information sharing in comparison to non-technical populations of Twitter users.</p><p>Tian et al. <ref type="bibr" target="#b14">[15]</ref> manually analyzed a sample of 300 developer tweets in various software engineering communities. The results showed that such tweets commonly contain job openings, news, questions and answers, or links to download new tools and code. In a follow-up study, Prasetyo et al. <ref type="bibr" target="#b47">[49]</ref> investigated the feasibility of automatically classifying tweets as relevant and irrelevant to software developers in engineering software systems. The authors used SVM to classify the data in <ref type="bibr" target="#b14">[15]</ref>. The results showed that 47% of the classified tweets were found to be relevant to developers.</p><p>Singer et al. <ref type="bibr" target="#b12">[13]</ref> surveyed 271 and interviewed 27 active GitHub developers about using Twitter in their development and interaction activities. The authors reported that developers use Twitter mainly to stay aware of industry changes, for learning, and for building relationships. Sharma et al. <ref type="bibr" target="#b11">[12]</ref> proposed an approach to help developers to identify software relevant tweets. Individual tweets were assigned a relevance probability based on their similarity to a language model generated from a subset of posts from StackOverflow. Evaluating the proposed approach over a random sample of 200 tweets showed improvement over previous models that use classification and keyword dictionaries.</p><p>Initial exploratory work on the value of Twitter data for requirements engineers was proposed by Guzman et al. <ref type="bibr" target="#b3">[4]</ref>. The authors manually analyzed and classified a sample of 1,000 tweets to determine the usage characteristics and content of software-relevant tweets. The results showed that software tweets contained useful information for different groups of technical and non-technical stakeholders. While our work builds upon this work, in our analysis we only focused on feedback dedicated to the technical stakeholders of the system (developers) by limiting data collection to tweets directly addressed to the Twitter accounts of our sample systems. This constraint enabled us to obtain higher accuracy levels. For instance, qualitative analysis of our 4,000 tweets showed that around 50% of our data contained useful technical feedback, in comparison to only 19% in <ref type="bibr" target="#b3">[4]</ref>. We were also able to achieve an average classification F 1 of 72% using SVM, in comparison to an F 1 of 48% achieved in <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSIONS AND FUTURE WORK</head><p>This paper presents a three-fold procedure aimed at leveraging Twitter as a main source of technical software information. These phases include data collection, classification, and presentation. Our analysis is conduced using 4,000 tweets sampled from tweets addressed to 10 software systems from a broad range of application domains. A manual qualitative analysis was conducted to determine the information value of sampled tweets. Our results showed that around 50% of these tweets contained useful technical data that can be translated into actionable bug reports and user requirements (RQ 1 ). Our analysis also showed that SVM and NB can be effective in capturing and categorizing technically useful tweets. In terms of classification features, our results showed that in the context of software-relevant tweets, sentiment analysis seems to have no impact on performance, while text reduction strategies had a conflicting impact on the classification accuracy (RQ 2 ).</p><p>Technically informative tweets were then summarized using multiple automated summarization algorithms. These algorithms, including hybrid TF, hybrid TF.IDF and SumBasic, are known for their simplicity (implementation, calibration, and computation overhead) and decent performance in the context of social media data. A human experiment using 10 programmers was conducted to assess the performance of the different summarization techniques. The results showed that the summarization algorithm SumBasic was the most successful in recalling majority of the common concerns raised in software-relevant tweets (RQ 3 ).</p><p>The line of work in this paper will be expanded along several directions as follows:</p><p>• Data collection: A main part of our future effort will be devoted to collecting larger datasets from a more diverse set of software systems. More data will enable us to conduct in depth analysis of software users' tweeting patterns, and thus draw more robust conclusions. • Analysis: Our future work will include experimenting with more advanced text classification and summarization techniques to achieve higher levels of accuracy. • Tool support: A working prototype that implements our findings in this paper will be developed. This prototype will enable developers to classify and summarize tweets related to their systems in an effective and accurate manner.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>IV. AUTOMATIC CLASSIFICATIONThe second phase of our analysis is concerned with automatically classifying our ground-truth dataset into the different categories of tweets identified earlier. Our research question under this phase (RQ 2 ) can be broken down into two subquestions, first, what classifiers are most effective in the context of Twitter data, and second, what classification features generate the most accurate results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig.<ref type="bibr" target="#b3">4</ref>. Performance of different summarization techniques over bug reporting tweets measured at different length summaries<ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20)</ref> </figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Data is publicly available at http://seel.cse.lsu.edu/data/re17.zip</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work was supported by the Louisiana Board of Regents Research Competitiveness Subprogram (LA BoR-RCS), contract number: LEQSF(2015-18)-RD-A-07.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Twitter mood predicts the stock market</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bollen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Political polarization on twitter</title>
		<author>
			<persName><forename type="first">M</forename><surname>Conover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Goncalves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ratkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Flammini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mencze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Weblogs and Social Media</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Temporal patterns of happiness and information in a global social network: Hedonometrics and Twitter</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kloumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bliss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Danforth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A needle in a haystack: What do twitter users say about software</title>
		<author>
			<persName><forename type="first">E</forename><surname>Guzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alkadhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Seyff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Requirements Engineering Conference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="96" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Release planning of mobile apps based on user reviews</title>
		<author>
			<persName><forename type="first">L</forename><surname>Villarroel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bavota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Oliveto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Di Penta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Software Engineering</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="14" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Value transformation in the mobile service ecosystem: A study of app store emergence and growth</title>
		<author>
			<persName><forename type="first">R</forename><surname>Basole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Karla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Service Science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="41" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A systematic review on the relationship between user involvement and system success</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zowghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Software Technology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="148" to="169" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">User feedback in the appstore: An empirical study</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pagano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Maalej</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Requirements Engineering Conference</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="125" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ar-miner: mining informative reviews for developers from mobile app marketplace</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Software Engineering</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="767" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Analysis of user comments: An approach for software requirements evolution</title>
		<author>
			<persName><forename type="first">G</forename><surname>Carreńo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Winbladh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Software Engineering</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="343" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bug report, feature request, or simply praise? On automatically classifying app reviews</title>
		<author>
			<persName><forename type="first">W</forename><surname>Maalej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nabil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Requirements Engineering Conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="116" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nirmal: Automatic identification of software relevant tweets leveraging language model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Software Analysis, Evolution, and Reengineering</title>
		<imprint>
			<biblScope unit="page">2015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Software engineering at the speed of light: How developers stay current using Twitter</title>
		<author>
			<persName><forename type="first">L</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Filho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Storey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Software Engineering</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="211" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards understanding Twitter use in software engineering: Preliminary findings, ongoing challenges and future questions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bougie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Starke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Storey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>German</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Web 2.0 for Software Engineering</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="31" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">What does software engineering community microblog about</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Achananuparp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lubis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Conference on Mining Software Repositories</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="247" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Twitter developer API</title>
		<ptr target="https://dev.twitter.com/" />
		<imprint>
			<date>August, 15th</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">From tweets to polls: Linking text sentiment to public opinion time series</title>
		<author>
			<persName><forename type="first">B</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Routledge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Weblogs and Social Media</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="122" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Spam detection on Twitter using traditional classifiers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mccord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chuah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="175" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Don&apos;t follow me: Spam detection in Twitter</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Security and Cryptography</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Enregistering internet language</title>
		<author>
			<persName><forename type="first">L</forename><surname>Squires</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language in Society</title>
		<imprint>
			<biblScope unit="issue">39</biblScope>
			<biblScope unit="page" from="457" to="492" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An analysis of Bayesian classifiers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Iba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page" from="223" to="228" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A comparison of event models for Naive Bayes text classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI -Learning for Text Categorization</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A tutorial on support vector machines for pattern recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="167" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Text categorization with suport vector machines: Learning with many relevant features</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="137" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Short text classification in Twitter to improve information filtering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fuhry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Demir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ferhatosmanoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Demirbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="841" to="842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="90" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Text as data: The promise and pitfalls of automatic content analysis methods for political texts</title>
		<author>
			<persName><forename type="first">J</forename><surname>Grimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="267" to="297" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Twitter as a corpus for sentiment analysis and opinion mining</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Paroubek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language Resources and Evaluation Conference</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1320" to="1326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Weka 3: Data mining software in java</title>
		<ptr target="http://www.cs.waikato.ac.nz/ml/weka/" />
		<imprint>
			<date>August, 15th</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Development of a stemming algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lovins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mechanical Translation and Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="22" to="31" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast training of support vector machines using sequential minimal optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods -Support Vector Learning</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Schoelkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Facilitating the application of support vector regression by using a universal pearson vii function based kernel</title>
		<author>
			<persName><forename type="first">B</forename><surname>Üstün</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Melssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Buydens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemometrics and Intelligent Laboratory Systems</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="29" to="40" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the influence of the kernel on the consistency of support vector machines</title>
		<author>
			<persName><forename type="first">I</forename><surname>Steinwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="93" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The megaphone of the people? Spanish SentiStrength for real-time analysis of political tweets</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vilares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thelwall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alonso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="799" to="813" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A new ANEW: Evaluation of a word list for sentiment analysis in microblogs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWC2011 Workshop on &apos;Making Sense of Microposts&apos;: Big things come in small packages</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The Adaptive Web: Methods and Strategies of Web Personalization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Brusilovsky</surname></persName>
		</author>
		<editor>A. Kobsa, and W. Nejdl</editor>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Summarizing newspaper comments</title>
		<author>
			<persName><forename type="first">C</forename><surname>Llewellyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oberlander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Weblogs and Social Media</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="599" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Summarizing user-contributed comments</title>
		<author>
			<persName><forename type="first">E</forename><surname>Khabiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caverlee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Weblogs and Social Media</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="534" to="537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The challenges of automatic summarization</title>
		<author>
			<persName><forename type="first">U</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="29" to="36" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Summarizing sporting events using twitter</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mahmud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Drews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Intelligent User Interfaces</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">What&apos;s the issue here?: Task-based evaluation of reader comment summarization systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paramita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Funk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kurtic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hepple</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Language Resources and Evaluation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="23" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">The impact of frequency on summarization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vanderwende</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Microsoft Research</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Comparing Twitter summarization algorithms for multiple post summaries</title>
		<author>
			<persName><forename type="first">D</forename><surname>Inouye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kalita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Social Computing (SocialCom) and International Conference on Privacy, Security</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="298" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Properties, prediction, and prevalence of useful user-generated comments for descriptive annotation of social media objects</title>
		<author>
			<persName><forename type="first">E</forename><surname>Momeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Weblogs and Social Media</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="390" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Text Summarization Branches Out</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Design and Analysis of Experiments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Voss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Automatic classification of software related microblogs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Prasetyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Achananuparp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Software Maintenance</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="596" to="599" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
