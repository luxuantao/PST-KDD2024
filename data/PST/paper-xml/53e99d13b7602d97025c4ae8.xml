<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weka-A Machine Learning Workbench for Data Mining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Eibe</forename><surname>Frank</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Waikato</orgName>
								<address>
									<settlement>Hamilton</settlement>
									<country key="NZ">New Zealand</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><surname>Hall</surname></persName>
							<email>mhall@cs.waikato.ac.nz</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Waikato</orgName>
								<address>
									<settlement>Hamilton</settlement>
									<country key="NZ">New Zealand</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Geoffrey</forename><surname>Holmes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Waikato</orgName>
								<address>
									<settlement>Hamilton</settlement>
									<country key="NZ">New Zealand</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Richard</forename><surname>Kirkby</surname></persName>
							<email>rkirkby@cs.waikato.ac.nz</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Waikato</orgName>
								<address>
									<settlement>Hamilton</settlement>
									<country key="NZ">New Zealand</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
							<email>bernhard@cs.waikato.ac.nz</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Waikato</orgName>
								<address>
									<settlement>Hamilton</settlement>
									<country key="NZ">New Zealand</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Waikato</orgName>
								<address>
									<settlement>Hamilton</settlement>
									<country key="NZ">New Zealand</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Len</forename><surname>Trigg</surname></persName>
							<affiliation key="aff1">
								<address>
									<postBox>P O Box 1538</postBox>
									<settlement>Hamilton</settlement>
									<country key="NZ">New Zealand</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Weka-A Machine Learning Workbench for Data Mining</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1007/978-0-387-09823-4_66</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>machine learning software</term>
					<term>Data Mining</term>
					<term>data preprocessing</term>
					<term>data visualization</term>
					<term>extensible workbench</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Weka workbench is an organized collection of state-of-the-art machine learning algorithms and data preprocessing tools. The basic way of interacting with these methods is by invoking them from the command line. However, convenient interactive graphical user interfaces are provided for data exploration, for setting up large-scale experiments on distributed computing platforms, and for designing configurations for streamed data processing. These interfaces constitute an advanced environment for experimental data mining. The system is written in Java and distributed under the terms of the GNU General Public License.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="66.1">Introduction</head><p>Experience shows that no single machine learning method is appropriate for all possible learning problems. The universal learner is an idealistic fantasy. Real datasets vary, and to obtain accurate models the bias of the learning algorithm must match the structure of the domain.</p><p>The Weka workbench is a collection of state-of-the-art machine learning algorithms and data preprocessing tools. It is designed so that users can quickly try out existing machine learning methods on new datasets in very flexible ways. It provides extensive support for the whole process of experimental Data Mining, including preparing the input data, evaluating learning schemes statistically, and visualizing both the input data and the result of learning. This has been accomplished by including a wide variety of algorithms for learning different types of concepts, as well as a wide range of preprocessing methods. This diverse and comprehensive set of tools can be invoked through a common interface, making it possible for users O. Maimon, L. Rokach (eds.), Data Mining and Knowledge Discovery Handbook, 2nd ed., DOI 10.1007/978-0-387-09823-4_66, Â© Springer Science+Business Media, LLC 2010 to compare different methods and identify those that are most appropriate for the problem at hand.</p><p>The workbench includes methods for all the standard Data Mining problems: regression, classification, clustering, association rule mining, and attribute selection. Getting to know the data is is a very important part of Data Mining, and many data visualization facilities and data preprocessing tools are provided. All algorithms and methods take their input in the form of a single relational table, which can be read from a file or generated by a database query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exploring the Data</head><p>The main graphical user interface, the "Explorer," is shown in Figure <ref type="figure" target="#fig_0">66</ref>.1. It has six different panels, accessed by the tabs at the top, that correspond to the various Data Mining tasks supported. In the "Preprocess" panel shown in Figure <ref type="figure" target="#fig_0">66</ref>.1, data can be loaded from a file or extracted from a database using an SQL query. The file can be in CSV format, or in the system's native ARFF file format. Database access is provided through Java Database Connectivity, which allows SQL queries to be posed to any database for which a suitable driver exists. Once a dataset has been read, various data preprocessing tools, called "filters," can be applied-for example, numeric data can be discretized. In Figure <ref type="figure" target="#fig_0">66</ref>.1 the user has loaded a data file and is focusing on a particular attribute, normalized-losses, examining its statistics and a histogram.</p><p>Through the Explorer's second panel, called "Classify," classification and regression algorithms can be applied to the preprocessed data. This panel also enables users to evaluate the resulting models, both numerically through statistical estimation and graphically through visualization of the data and examination of the model (if the model structure is amenable to visualization). Users can also load and save models.  The third panel, "Cluster," enables users to apply clustering algorithms to the dataset. Again the outcome can be visualized, and, if the clusters represent density estimates, evaluated based on the statistical likelihood of the data. Clustering is one of two methodologies for analyzing data without an explicit target attribute that must be predicted. The other one comprises association rules, which enable users to perform a market-basket type analysis of the data. The fourth panel, "Associate," provides access to algorithms for learning association rules.</p><p>Attribute selection, another important Data Mining task, is supported by the next panel. This provides access to various methods for measuring the utility of attributes, and for finding attribute subsets that are predictive of the data. Users who like to analyze the data visually are supported by the final panel, "Visualize." This presents a color-coded scatter plot matrix, and users can then select and enlarge individual plots. It is also possible to zoom in on portions of the data, to retrieve the exact record underlying a particular data point, and so on.</p><p>The Explorer interface does not allow for incremental learning, because the Preprocess panel loads the dataset into main memory in its entirety. That means that it can only be used for small to medium sized problems. However, some incremental algorithms are implemented that can be used to process very large datasets. One way to apply these is through the command-line interface, which gives access to all features of the system. An alternative, more convenient, approach is to use the second major graphical user interface, called "Knowledge Flow." Illustrated in Figure <ref type="figure" target="#fig_0">66</ref>.2, this enables users to specify a data stream by graphically connecting components representing data sources, preprocessing tools, learning algorithms, evaluation methods, and visualization tools. Using it, data can be processed in batches as in the Explorer, or loaded and processed incrementally by those filters and learning algorithms that are capable of incremental learning.</p><p>An important practical question when applying classification and regression techniques is to determine which methods work best for a given problem. There is usually no way to answer this question a priori, and one of the main motivations for the development of the workbench was to provide an environment that enables users to try a variety of learning techniques on a particular problem. This can be done interactively in the Explorer. However, to automate the process Weka includes a third interface, the "Experimenter," shown in Figure <ref type="figure" target="#fig_0">66</ref>.3. This makes it easy to run the classification and regression algorithms with different parameter settings on a corpus of datasets, collect performance statistics, and perform significance tests on the results. Advanced users can also use the Experimenter to distribute the computing load across multiple machines using Java Remote Method Invocation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods and Algorithms</head><p>Weka contains a comprehensive set of useful algorithms for a panoply of Data Mining tasks. These include tools for data engineering (called "filters"), algorithms for attribute selection, clustering, association rule learning, classification and regression. In the following subsections we list the most important algorithms in each category. Most well-known algorithms are included, along with a few less common ones that naturally reflect the interests of our research group.</p><p>An important aspect of the architecture is its modularity. This allows algorithms to be combined in many different ways. For example, one can combine bagging! boosting, decision tree learning and arbitrary filters directly from the graphical user interface, without having to write a single line of code. Most algorithms have one or more options that can be specified. Explanations of these options and their legal values are available as built-in help in the graphical user interfaces. They can also be listed from the command line. Additional information and pointers to research publications describing particular algorithms may be found in the internal Javadoc documentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification</head><p>Implementations of almost all main-stream classification algorithms are included. Bayesian methods include naive Bayes, complement naive Bayes, multinomial naive Bayes, Bayesian networks, and AODE. There are many decision tree learners: decision stumps, ID3, a C4.5 clone called "J48," trees generated by reduced error pruning, alternating decision trees, and random trees and forests thereof. Rule learners include OneR, an implementation of Ripper called "JRip," PART, decision tables, single conjunctive rules, and Prism. There are several separating hyperplane approaches like support vector machines with a variety of kernels, logistic regression, voted perceptrons, Winnow and a multi-layer perceptron. There are many lazy learning methods like IB1, IBk, lazy Bayesian rules, KStar, and locally-weighted learning.</p><p>As well as the basic classification learning methods, so-called "meta-learning" schemes enable users to combine instances of one or more of the basic algorithms in various ways: bagging! boosting (including the variants AdaboostM1 and Logit-Boost), and stacking. A method called "FilteredClassifier" allows a filter to be paired up with a classifier. Classification can be made cost-sensitive, or multi-class, or ordinal-class. Parameter values can be selected using cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regression</head><p>There are implementations of many regression schemes. They include simple and multiple linear regression, pace regression, a multi-layer perceptron, support vector regression, locallyweighted learning, decision stumps, regression and model trees (M5) and rules (M5rules). The standard instance-based learning schemes IB1 and IBk can be applied to regression problems (as well as classification problems). Moreover, there are additional meta-learning schemes that apply to regression problems, such as additive regression and regression by discretization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clustering</head><p>At present, only a few standard clustering algorithms are included: KMeans, EM for naive Bayes models, farthest-first clustering, and Cobweb. This list is likely to grow in the near future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Association rule learning</head><p>The standard algorithm for association rule induction is Apriori, which is implemented in the workbench. Two other algorithms implemented in Weka are Tertius, which can extract first-order rules, and Predictive Apriori, which combines the standard confidence and support statistics into a single measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attribute selection</head><p>Both wrapper and filter approaches to attribute selection are supported. A wide range of filtering criteria are implemented, including correlation-based feature selection, the chi-square statistic, gain ratio, information gain, symmetric uncertainty, and a support vector machinebased criterion. There are also a variety of search methods: forward and backward selection, best-first search, genetic search, and random search. Additionally, principal components analysis can be used to reduce the dimensionality of a problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Filters</head><p>Processes that transform instances and sets of instances are called "filters," and they are classified according to whether they make sense only in a prediction context (called "supervised") or in any context (called "unsupervised"). We further split them into "attribute filters," which work on one or more attributes of an instance, and "instance filters," which manipulate sets of instances.</p><p>Unsupervised attribute filters include adding a new attribute, adding a cluster indicator, adding noise, copying an attribute, discretizing a numeric attribute, normalizing or standardizing a numeric attribute, making indicators, merging attribute values, transforming nominal to binary values, obfuscating values, swapping values, removing attributes, replacing missing values, turning string attributes into nominal ones or word vectors, computing random projections, and processing time series data. Unsupervised instance filters transform sparse instances into non-sparse instances and vice versa, randomize and resample sets of instances, and remove instances according to certain criteria.</p><p>Supervised attribute filters include support for attribute selection, discretization, nominal to binary transformation, and re-ordering the class values. Finally, supervised instance filters resample and subsample sets of instances to generate different class distributions-stratified, uniform, and arbitrary user-specified spreads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System Architecture</head><p>In order to make its operation as flexible as possible, the workbench was designed with a modular, object-oriented architecture that allows new classifiers, filters, clustering algorithms and so on to be added easily. A set of abstract Java classes, one for each major type of component, were designed and placed in a corresponding top-level package.</p><p>All classifiers reside in subpackages of the top level "classifiers" package and extend a common base class called "Classifier." The Classifier class prescribes a public interface for classifiers and a set of conventions by which they should abide. Subpackages group components according to functionality or purpose. For example, filters are separated into those that are supervised or unsupervised, and then further by whether they operate on an attribute or instance basis. Classifiers are organized according to the general type of learning algorithm, so there are subpackages for Bayesian methods, tree inducers, rule learners, etc.</p><p>All components rely to a greater or lesser extent on supporting classes that reside in a top level package called "core." This package provides classes and data structures that read data sets, represent instances and attributes, and provide various common utility methods. The core package also contains additional interfaces that components may implement in order to indicate that they support various extra functionality. For example, a classifier can implement the "WeightedInstancesHandler" interface to indicate that it can take advantage of instance weights.</p><p>A major part of the appeal of the system for end users lies in its graphical user interfaces. In order to maintain flexibility it was necessary to engineer the interfaces to make it as painless as possible for developers to add new components into the workbench. To this end, the user interfaces capitalize upon Java's introspection mechanisms to provide the ability to configure each component's options dynamically at runtime. This frees the developer from having to consider user interface issues when developing a new component. For example, to enable a new classifier to be used with the Explorer (or either of the other two graphical user interfaces), all a developer need do is follow the Java Bean convention of supplying "get" and "set" methods for each of the classifier's public options.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Applications</head><p>Weka was originally developed for the purpose of processing agricultural data, motivated by the importance of this application area in New Zealand. However, the machine learning methods and data engineering capability it embodies have grown so quickly, and so radically, that the workbench is now commonly used in all forms of Data Mining applications-from bioinformatics to competition datasets issued by major conferences such as Knowledge Discovery in Databases.</p><p>New Zealand has several research centres dedicated to agriculture and horticulture, which provided the original impetus for our work, and many of our early applications. For example, we worked on predicting the internal bruising sustained by different varieties of apple as they make their way through a packing-house on a conveyor belt <ref type="bibr" target="#b3">(Holmes et al., 1998)</ref>; predicting, in real time, the quality of a mushroom from a photograph in order to provide automatic grading <ref type="bibr" target="#b6">(Kusabs et al., 1998)</ref>; and classifying kiwifruit vines into twelve classes, based on visible-NIR spectra, in order to determine which of twelve pre-harvest fruit management treatments has been applied to the vines <ref type="bibr" target="#b4">(Holmes and Hall, 2002)</ref>. The applicability of the workbench in agricultural domains was the subject of user studies <ref type="bibr" target="#b8">(McQueen et al., 1998)</ref> that demonstrated a high level of satisfaction with the tool and gave some advice on improvements.</p><p>There are countless other applications, actual and potential. As just one example, Weka has been used extensively in the field of bioinformatics. Published studies include automated protein annotation <ref type="bibr" target="#b0">(Bazzan et al., 2002)</ref>, probe selection for gene expression arrays <ref type="bibr" target="#b13">(Tobler et al., 2002)</ref>, plant genotype discrimination <ref type="bibr" target="#b12">(Taylor et al., 2002)</ref>, and classifying gene expression profiles and extracting rules from them <ref type="bibr" target="#b7">(Li et al., 2003)</ref>. Text mining is another major field of application, and the workbench has been used to automatically extract key phrases from text <ref type="bibr" target="#b2">(Frank et al., 1999)</ref>, and for document categorization <ref type="bibr" target="#b11">(Sauban and Pfahringer, 2003)</ref> and word sense disambiguation <ref type="bibr" target="#b10">(Pedersen, 2002)</ref>.</p><p>The workbench makes it very easy to perform interactive experiments, so it is not surprising that most work has been done with small to medium sized datasets. However, larger datasets have been successfully processed. Very large datasets are typically split into several training sets, and a votingcommittee structure is used for prediction. The recent development of the knowledge flow interface should see larger scale application development, including online learning from streamed data.</p><p>Many future applications will be developed in an online setting. Recent work on data streams <ref type="bibr" target="#b5">(Holmes et al., 2003)</ref> has enabled machine learning algorithms to be used in situations where a potentially infinite source of data is available. These are common in manufacturing industries with 24/7 processing. The challenge is to develop models that constantly monitor data in order to detect changes from the steady state. Such changes may indicate failure in the process, providing operators with warning signals that equipment needs re-calibrating or replacing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 66</head><label>66</label><figDesc>Fig. 66.1. The Explorer Interface.</figDesc><graphic url="image-1.png" coords="2,75.92,57.95,288.07,216.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Eibe</head><label></label><figDesc>Frank et al.   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 66</head><label>66</label><figDesc>Fig. 66.2. The Knowledge Flow Interface.</figDesc><graphic url="image-2.png" coords="3,75.92,57.95,288.07,216.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 66</head><label>66</label><figDesc>Fig. 66.3. The Experimenter Interface.</figDesc><graphic url="image-3.png" coords="4,75.92,57.95,288.07,216.07" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Many thanks to past and present members of the Waikato machine learning group and the many external contributors for all the work they have put into Weka.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summing up the Workbench</head><p>Weka has three principal advantages over most other Data Mining software. First, it is open source, which not only means that it can be obtained free, but-more importantly-it is maintainable, and modifiable, without depending on the commitment, health, or longevity of any particular institution or company. Second, it provides a wealth of state-of-the-art machine learning algorithms that can be deployed on any given problem. Third, it is fully implemented in Java and runs on almost any platform-even a Personal Digital Assistant.</p><p>The main disadvantage is that most of the functionality is only applicable if all data is held in main memory. A few algorithms are included that are able to process data incrementally or in batches <ref type="bibr" target="#b1">(Frank et al., 2002)</ref>. However, for most of the methods the amount of available memory imposes a limit on the data size, which restricts application to small or mediumsized datasets. If larger datasets are to be processed, some form of subsampling is generally required. A second disadvantage is the flip side of portability: a Java implementation may be somewhat slower than an equivalent in C/C++.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automated annotation of keywords for proteins related to mycoplasmataceae using machine learning techniques</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Bazzan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="35S" to="43S" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Racing committees for large datasets</title>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kirkby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Discovery Science</title>
				<meeting>the International Conference on Discovery Science</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="153" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain-specific keyphrase extraction</title>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Paynter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gutwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Nevill-Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Joint Conference on Artificial Intelligence</title>
				<meeting>the 16th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="668" to="673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predicting apple bruising using machine learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Rue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bollen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Hort</title>
		<imprint>
			<biblScope unit="volume">476</biblScope>
			<biblScope unit="page" from="289" to="296" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A development environment for predictive modelling in foods</title>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Food Microbiology</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="351" to="362" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mining data streams using option trees</title>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kirkby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<idno>08/03</idno>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Waikato</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Objective measurement of mushroom quality</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kusabs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bollen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Trigg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Inglis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc New Zealand Institute of Agricultural Science and the New Zealand Society for Horticultural Science Annual Convention</title>
				<meeting>New Zealand Institute of Agricultural Science and the New Zealand Society for Horticultural Science Annual Convention</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page">51</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simple rules underlying gene expression profiles of more than six subtypes of acute lymphoblastic leukemia (all) patients</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Downing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Yeoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.-J</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="71" to="78" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">User satisfaction with machine learning as a data analysis method in agricultural research</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mcqueen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Zealand Journal of Agricultural Research</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="577" to="584" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">66 Weka-A Machine Learning Workbench for Data Mining</title>
		<author>
			<persName><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Evaluating the effectiveness of ensembles of decision trees in disambiguating Senseval lexical samples</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 Workshop on Word Sense Disambiguation: Recent Successes and Future Directions</title>
				<meeting>the ACL-02 Workshop on Word Sense Disambiguation: Recent Successes and Future Directions</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Text categorisation using document profiling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sauban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th European Conference on Principles and Practice of Knowledge Discovery in Databases</title>
				<meeting>the 7th European Conference on Principles and Practice of Knowledge Discovery in Databases</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="411" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Application of metabolomics to plant genotype discrimination using statistics and machine learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Altmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fiehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="241S" to="248S" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Evaluating machine learning approaches for aiding probe selection for gene-expression arrays</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tobler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Molla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nuwaysir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shavlik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="164S" to="171S" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
