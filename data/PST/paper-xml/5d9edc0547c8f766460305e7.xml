<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural networks for topology optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ivan</forename><surname>Sosnovik</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Ivan</forename><surname>Oseledets</surname></persName>
							<email>i.oseledets@skoltech.ru</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<addrLine>The Netherlands</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Skolkovo Institute of Science and Technology</orgName>
								<address>
									<postCode>121205</postCode>
									<settlement>Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Marchuk Insti-tute of Numerical Mathematics</orgName>
								<orgName type="institution">Skolkovo Institute of Science and Technology</orgName>
								<address>
									<postCode>121205, 119333</postCode>
									<settlement>Moscow, Moscow</settlement>
									<country>Russia, Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Göteborg University -University of Gothenburg</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural networks for topology optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BB13F6A51227479E09639DEC5D89633F</idno>
					<idno type="DOI">10.1515/rnam-2019-0018</idno>
					<note type="submission">Received February 11, 2019; accepted May 21, 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep learning</term>
					<term>topology optimization</term>
					<term>image segmentation MSC 2010: 49M99</term>
					<term>78M50</term>
					<term>65N99</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this research, we propose a deep learning based approach for speeding up the topology optimization methods. The problem we seek to solve is the layout problem. The main novelty of this work is to state the problem as an image segmentation task. We leverage the power of deep learning methods as the efficient pixel-wise image labeling technique to perform the topology optimization. We introduce convolutional encoder-decoder architecture and the overall approach of solving the above-described problem with high performance. The conducted experiments demonstrate the significant acceleration of the optimization process. The proposed approach has excellent generalization properties. We demonstrate the ability of the application of the proposed model to other problems. The successful results, as well as the drawbacks of the current method, are discussed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Topology optimization solves the layout problem with the following formulation: how to distribute the material inside a design domain such that the obtained structure has optimal properties and satisfies the prescribed constraints? The most challenging formulation of the problem requires the solution to be binary, i.e., it should state whether there is a material or a void for each of the parts of the design domain. One of the common examples of such an optimization is the minimization of elastic strain energy of a body for a given total weight and boundary conditions. Initiated by the demands of automotive and aerospace industry in the 20 th century, topology optimization has spread its application to a wide range of other disciplines: e.g. fluids, acoustics, electromagnetics, optics and combinations thereof <ref type="bibr" target="#b4">[5]</ref>.</p><p>All modern approaches for topology optimization used in commercial and academic software are based on finite element methods. SIMP (Simplified Isotropic Material with Penalization), which was introduced in 1989 <ref type="bibr" target="#b2">[3]</ref>, is currently a widely spread simple and efficient technique. It proposes to use penalization of the intermediate values of density of the material, which improves the convergence of the solution to binary. Topology optimization problem could be solved by using BESO (Bi-directional evolutionary structural optimization) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25]</ref> as an alternative. The key idea of this method is to remove the material where the stress is the lowest and add material where the stress is higher. The more detailed review is given in Section 1.</p><p>For all of the above-described methods, the process of optimization could be roughly divided into two stages: general redistribution of the material and the refinement. During the first one, the material layout varies a lot from iteration to iteration. While during the second stage the material distribution converges to the final result. The global structure remains unchanged and only local alteration could be observed.</p><p>In this paper, we propose a deep learning based approach to speeding up the most time-consuming part of a traditional topology optimization methods. The main novelty of this work is to state the problem as an image segmentation task. We leverage the power of deep learning methods as an efficient pixel-wise image labeling technique to accelerate modern topology optimization solvers. The key features of our approach are the following: -acceleration of optimization process; -excellent generalization properties; -absolutely scalable techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Topology optimization problem</head><p>Current research is devoted to topology optimization of mechanical structures. Consider a design domain Ω : {ω j } N j=1 , filled with a linear isotropic elastic material and discretized with square finite elements. The material distribution is described by the binary density variable x j that represents either absence (0) or presence <ref type="bibr" target="#b0">(1)</ref> of the material at each point of the design domain. Therefore, the problem, that we seek to solve, can be written in mathematical form as:</p><formula xml:id="formula_0">{ { { { { { { { { { { { { { { min x c(u(x), x) = ∑ N j=1 E j (x j )u T j k 0 u j s.t. V(x)/V 0 = f 0 KU = F x j ∈ {0; 1}, j = 1, . . . , N (1.1)</formula><p>where c is a compliance, u j is the element displacement vector, k 0 is the element stiffness matrix for an element with unit Young's modulus, U and F are the global displacement and force vectors, respectively, and K is the global stiffness matrix; V(x) and V 0 are the material volume and design domain volume, respectively; f 0 is the prescribed volume fraction.</p><p>The discrete nature of the problem makes it difficult to solve. Therefore, the last constraint in (1.1) is replaced with the following one: x j ∈ [0; 1], j = 1, . . . , N. The most common method for topology optimization problem with continuous design variables is so-called SIMP or power-law approach <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16]</ref>. This is a gradientbased iterative method with the penalization of non-binary solutions, which is achieved by choosing Young's modulus of a simple but very efficient form:</p><formula xml:id="formula_1">E j (x j ) = E min + x p j (E 0 -E min ).</formula><p>(</p><p>The exact implementation of SIMP algorithm is out of the scope of the current paper. The updating schemes, as well as different heuristics, can be found in excellent papers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22]</ref>. The topology optimization code in Matlab is described in detail in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20]</ref> and the Python implementation of SIMP algorithm is represented in <ref type="bibr" target="#b10">[11]</ref>.</p><p>Standard half MBB-Beam problem is used to illustrate the process of topology optimization. The design domain, constraints, and loads are represented in Fig. <ref type="figure">1</ref>. The optimization of this problem is demonstrated in Fig. <ref type="figure">2</ref>. During the initial iterations, the general redistribution of the material inside of the design domain is performed. The rest of the optimization process includes the filtering of the pixels: the densities with intermediate values converge to binary values and the silhouette of the obtained structure remains almost unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Learning topology optimization</head><p>As it was illustrated in Section 1, it is enough for the solver to perform a few number N 0 of iterations to obtain the preliminary view of a structure. The fraction of non-binary densities could be close to 1, however, the global layout pattern is close to the final one. The obtained image I could be interpreted as a blurred image of a final structure, or an image distorted by other factors. The thing is that there are just two types of objects on this image: the material and the void. The image I * , obtained as a result of topology optimization does not contain intermediate values and, therefore, could be interpreted as the mask of image I. According to this notation, starting from iteration N 0 the process of optimization I → I * mimics the process of image segmentation for two classes or foreground-background segmentation.</p><p>We propose the following pipeline for topology optimization: use SIMP method to perform the initial iterations and get the distribution with non-binary densities; use the neural network to perform the segmentation of the obtained image and converge the distribution to {0, 1} solution.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Architecture</head><p>Here we introduce the Neural Network for Topology Optimization -deep fully-convolutional neural network aimed to perform the convergence of densities during the topology optimization process. The input of the model is two grayscale images (or a two-channel image). The first one is the density distribution X n inside of the design domain which was obtained after the last performed iteration of topology optimization solver. The second input is the last performed update (gradient) of the densities δX = X n -X n-1 , i.e., the difference between the densities after the nth iteration and (n -1)th iteration. The output of the proposed model is a grayscale image of the same resolution as an input, which represents the predicted final structure. The architecture of our model mimics the common for the image segmentation hourglass shape. The proposed model has an encoder network and a corresponding decoder network, followed by a final pixel-wise classification layer. The architecture is illustrated in Fig. <ref type="figure" target="#fig_1">3</ref>. The encoder network consists of 6 convolutional layers. Each layer has kernels of size 3×3 and is followed by ReLU nonlinearity. The first two layers have 16 convolutional kernels. This block is followed by the pooling of the maximal element from the window of size 2×2. The next two layers have 32 kernels and are also followed by MaxPooling layer. The last block consists of 2 layers with 64 kernels each.</p><p>The decoder network copies the architecture of the encoder part and reverses it. The MaxPooling layers are replaced with Upsampling layers followed by the concatenation with features from the corresponding lowlevel layer as it is performed in U-Net <ref type="bibr" target="#b17">[18]</ref>. The pooling operation introduces the invariance of the subsequent network to small translations of the input. The concatenation of features from different layers allows one to benefit from the use of both the raw low-level representation and significantly encoded parameterization from the higher levels. The decoder is followed by the Convolutional layer with 1 kernel and sigmoid activation function. We included 2 Dropout layers <ref type="bibr" target="#b11">[12]</ref> as the regularization for our network. The width and height of the input could however, they must be divisible by 4 in order to guarantee the coherence of the shapes of tensors in the computational graph. The proposed neural network has just 192,113 parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dataset</head><p>To train the above-described model, we need example solutions to system (1.1). To collect a large dataset from the real-life examples is difficult or even impossible. Therefore, we use synthetic data generated by using ToPy <ref type="bibr" target="#b10">[11]</ref> -an open source solver for 2D and 3D topology optimization, based on SIMP approach.</p><p>To generate the dataset we sampled the pseudo-random problem formulations and performed 100 iterations of standard SIMP method. Each problem is defined by the constraints and the loads. The strategy of sampling is the following: -The number of nodes with fixed x and y translations and the number of loads are sampled from the Poisson distribution:</p><formula xml:id="formula_3">N x ∼ P(λ = 2)</formula><p>N y , N L ∼ P(λ = 1).</p><p>-The nodes for each of the above-described constraints are sampled from the distribution defined on the grid. The probability of choosing the boundary node is 100 times higher than that for an inner node. -The load values are chosen as -1.</p><p>-The volume fraction is sampled from the Normal distribution f 0 ∼ N(µ = 0.5, σ = 0.1).</p><p>The dataset and the related code are available at https://github.com/ISosnovik/top. The obtained dataset has 10,000 objects. Each object is a tensor of shape 100 × 40 × 40: 100 iterations of the optimization process for the problem defined on a regular 40 × 40 grid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training</head><p>We used dataset, described in Section 2.2, to train our model. During the training process we 'stopped' SIMP solver after k iterations and used the obtained design variables as an input for our model. The input images were augmented with transformations from group D4: horizontal and vertical flips and rotation by 90 degrees. k was sampled from some certain distribution F. Poisson distribution P(λ) and discrete uniform distribution </p><formula xml:id="formula_4">L = L conf (X true , X pred ) + βL vol (X true , X pred ) (2.1)</formula><p>where the confidence loss is a binary cross-entropy:</p><formula xml:id="formula_5">L conf (X , pred ) = - 1 NM N ∑ i=1 M ∑ j=1 [X ij true log(X ij pred ) + (1 -X ij true ) log(1 -X ij pred )] (2.2)</formula><p>where N × M is the resolution of the image. The second summand in (2.1) represents the volume fraction constraint:</p><formula xml:id="formula_6">L vol (X true , X pred ) = ( Xpred -Xtrue ) 2 . (2.3)</formula><p>We used ADAM <ref type="bibr" target="#b12">[13]</ref> optimizer with default parameters. We halved the learning rate once during the training process. The implementation in Python is available at https://github.com/ISosnovik/nn4topopt. For neural networks, we used Keras <ref type="bibr" target="#b7">[8]</ref> with TensorFlow <ref type="bibr" target="#b0">[1]</ref> backend. NVIDIA Tesla K80 was used for deep learning computations. The training of a neural network from scratch took about 80-90 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>The goal of our experiments is to demonstrate that the proposed model and the overall pipeline are useful for solving topology optimization problems. We compare the performance of our approach with standard SIMP solver <ref type="bibr" target="#b10">[11]</ref> in terms of the accuracy of the obtained structure and the average time consumption.</p><p>We report two metrics from common image segmentation evaluation: Binary Accuracy and Intersection over Union (IoU). Let n l , l = 0, 1, be the total number of pixels of class l. The ω tp , t, p = 0, 1, is a total number of pixels of class t predicted to belong to class p. Therefore:</p><formula xml:id="formula_7">Bin. Acc. = ω 00 + ω 11 n 0 + n 1 , IoU = 1 2 [ ω 00 n 0 + ω 10 + ω 11 n 1 + ω 01 ]. (3.1)</formula><p>We examine 4 neural networks with the same architecture but trained with different policies. The number of iterations after which we 'stopped' SIMP algorithm was sampled from different distributions. We trained one neural network by choosing discrete uniform distribution U <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">100]</ref> and another three models were trained with Poisson distribution P(λ) with λ = 5, 10, 30.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Accuracy and performance</head><p>We conducted several experiments to illustrate the results of the application of the proposed pipeline and the exact model to mechanical problems. Figure <ref type="figure" target="#fig_3">4</ref> demonstrates that our neural network restores the final structure while being used even after 5 iterations. The output of the model is close to that of SIMP algorithm. The overall topology of the structure is the same. Furthermore, the time consumption of the proposed method, in this case, is almost 20 times smaller.</p><p>Neural networks trained with different policies produce close results: models preserve the final structure up to some rare pixel-wise changes. However, the accuracy of these models depends on the number of the initial iterations performed by SIMP algorithm. Tables <ref type="table">1</ref> and<ref type="table">2</ref> summarize the results obtained in the series of experiments. The trained models demonstrate the sufficiently more accurate results comparing to the thresholding applied after the same number of iterations of SIMP method. Some models benefit when they are applied after 5-10 iterations, while others demonstrate better result in the middle or at the end of the process. The proposed pipeline could significantly accelerate the overall algorithm with minimal reduction in accuracy, especially when CNN is used at the beginning of the optimization process.  The neural network which used discrete uniform distribution during the training process does not demonstrate the highest binary accuracy and IoU comparing to other models till the latest iterations. However, this model allows one to outperform the SIMP algorithm with thresholding throughout the optimization process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Transferability</head><p>This research deals with the application of neural networks to the topology optimization of minimal compliance problems. Nevertheless, the proposed model does not rely on any prior knowledge of the nature of the problem. Despite the fact that we used mechanical dataset during the training, other types of problems from topology optimization framework could be solved by using the proposed pipeline. To examine the generaliza- tion properties of our model, we generated the small dataset of heat conduction problems defined on 40 × 40 regular grid. The exact solution and the intermediate densities for the problems were obtained in absolutely the same way as it was described in Section 2.</p><p>The conducted experiments are summarized in Tables <ref type="table">3</ref> and<ref type="table">4</ref>. During the initial part of the optimization process, the results of the pre-trained CNNs are more accurate than this of thresholding. Our model approximates the mapping to the final structure precisely when the training dataset and the validation dataset are from the same distribution. However, it mimics updates of SIMP method during the initial iterations even when CNN is applied to another dataset. Therefore, this pipeline could be useful for the fast prediction of the rough structure in various topology optimization problems.</p><p>The neural network described in Section 2 is fully-convolutional, i.e., it consists of Convolutional, Pooling, Upsampling, and Dropout layers. The architecture itself does not have any constraints on the size of the input data. In this experiment, we checked the scalable properties of our method. The model we examined has been trained on the original dataset with square images of size 40×40. Figure <ref type="figure" target="#fig_4">5</ref> visualizes the result of the application of CNN to the problems defined on grids with different resolution. Here we can see that changes in the aspect ratio and reasonable changes in the resolution of the input data do not affect the accuracy of the model. Pre-trained neural network successfully reconstructs the final structure for a given problem. Significant changes of the size of the input data require additional training of the model because the typical size of a common patterns changes with the increase of the resolution of an image. Nevertheless, demonstrated cases did not require one to tune neural network and allowed to transfer model from one resolution to another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related work</head><p>The current research is supposed to be the first one which utilizes deep learning approach for the topology optimization problem. It is inspired by the recent successful application of deep learning to problems in computational physics. Greff et al. <ref type="bibr" target="#b8">[9]</ref> used the fully-connected neural network as a mapping function from the nano-material configuration and the input voltage to the output current. The adaptation of restricted Boltzmann machine for solving the Quantum Many-Body Problem was demonstrated in <ref type="bibr" target="#b6">[7]</ref>. Mills et al. <ref type="bibr" target="#b14">[15]</ref> used the machinery of deep learning to learn the mapping between potential and energy, bypassing the need to numerically solve the Schrödinger equation and the need for computing wave functions. Tompson et al. <ref type="bibr" target="#b22">[23]</ref> and Kiwon et al. <ref type="bibr" target="#b23">[24]</ref> accelerated the process of modelling of liquids by the application of neural networks. The paper <ref type="bibr" target="#b20">[21]</ref> demonstrates how a deep neural network trained on quantum mechanical density functional theory calculations can learn an accurate and transferable potential for organic molecules. The cutting-edge research <ref type="bibr" target="#b16">[17]</ref> shows how generative adversarial networks could be used for simulating 3D high-energy particle showers in multi-layer electromagnetic calorimeters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed a neural network as an effective tool for the acceleration of topology optimization process. Out model learned the mapping from the intermediate result of the iterative method to the final structure of the design domain. It allowed us to stop SIMP method earlier and significantly decrease the total time consumption.</p><p>We demonstrated that the model trained on the dataset of minimal compliance problems could produce the rough approximation of the solution for other types of topology optimization problems. Various experiments have shown that the proposed neural network transfers successfully from the dataset with a small resolution to the problems defined on the grids with better resolution.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :Fig. 2 :</head><label>12</label><figDesc>Fig. 1: The design domain, boundary conditions, and external load for the optimization of a half MBB beam.</figDesc><graphic coords="3,317.75,214.39,169.53,56.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: The architecture of the proposed neural network for topology optimization. All kernels are of size 3 × 3. The number of kernels is represented by the number at the bottom of the layer. Blue arrows and opaque boxes represent the concatenation of the features from different layers.</figDesc><graphic coords="4,104.88,65.20,439.34,137.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Brought to you by | Göteborg University -University of Gothenburg Authenticated Download Date | 10/14/19 10:17 AM U[1, 100] are of interest to us. For training the network we used the objective function of the following form:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Top: SIMP is stopped after 8 iterations, binary accuracy 0.96, mean IoU 0.92; Bottom: solver is stopped after 5 iterations, binary accuracy 0.98, mean IoU 0.95.</figDesc><graphic coords="6,104.88,65.20,395.43,208.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Results of the application of the proposed CNN to the problems defined on grids with resolution and aspect ratio different from that of the training dataset.</figDesc><graphic coords="8,104.88,65.20,439.37,231.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Intersection over Union (IoU) of the proposed method and the standard one on the mechanical dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Iteration</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell></cell><cell>60</cell><cell></cell></row><row><cell>Thresholding</cell><cell>92.9</cell><cell>95.4</cell><cell>96.5</cell><cell>97.1</cell><cell>97.7</cell><cell>98.1</cell><cell>98.4</cell><cell>98.6</cell><cell>98.9</cell></row><row><cell>CNN P(5)</cell><cell>95.8</cell><cell>97.3</cell><cell>97.7</cell><cell>97.9</cell><cell>98.2</cell><cell>98.4</cell><cell>98.5</cell><cell>98.6</cell><cell>98.7</cell></row><row><cell>CNN P(10)</cell><cell>95.4</cell><cell>97.6</cell><cell>98.1</cell><cell>98.4</cell><cell>98.7</cell><cell>98.9</cell><cell>99.0</cell><cell>99.0</cell><cell>99.0</cell></row><row><cell>CNN P(30)</cell><cell>92.7</cell><cell>96.3</cell><cell>97.8</cell><cell>98.5</cell><cell>99.0</cell><cell>99.2</cell><cell>99.4</cell><cell>99.5</cell><cell>99.6</cell></row><row><cell>CNN U[1, 100]</cell><cell>94.7</cell><cell>96.8</cell><cell>97.7</cell><cell>98.2</cell><cell>98.7</cell><cell>99.0</cell><cell>99.3</cell><cell>99.4</cell><cell>99.6</cell></row><row><cell cols="6">Tab. 2: Iteration</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>60</cell><cell>80</cell></row><row><cell>Thresholding</cell><cell>86.8</cell><cell>91.2</cell><cell>93.3</cell><cell>94.3</cell><cell>95.6</cell><cell>96.3</cell><cell>96.8</cell><cell>97.3</cell><cell>97.9</cell></row><row><cell>CNN P(5)</cell><cell>92.0</cell><cell>94.7</cell><cell>95.4</cell><cell>96.0</cell><cell>96.5</cell><cell>96.9</cell><cell>97.1</cell><cell>97.3</cell><cell>97.4</cell></row><row><cell>CNN P(10)</cell><cell>91.1</cell><cell>95.3</cell><cell>96.4</cell><cell>96.9</cell><cell>97.4</cell><cell>97.8</cell><cell>98.0</cell><cell>98.0</cell><cell>98.1</cell></row><row><cell>CNN P(30)</cell><cell>86.4</cell><cell>92.9</cell><cell>95.7</cell><cell>97.0</cell><cell>98.1</cell><cell>98.5</cell><cell>98.8</cell><cell>99.0</cell><cell>99.2</cell></row><row><cell>CNN U[1, 100]</cell><cell>90.0</cell><cell>93.9</cell><cell>95.5</cell><cell>96.4</cell><cell>97.5</cell><cell>98.1</cell><cell>98.6</cell><cell>98.8</cell><cell>99.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Brought to you by | Göteborg University -University of Gothenburg Authenticated Download Date | 10/14/19 10:17 AM Tab. 3: Binary Accuracy of the proposed method and the standard one on heat conduction dataset. Models were trained on the minimal compliance dataset. IoU of the proposed method and the standard one on heat conduction dataset. Models were trained on the minimal compliance dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Iteration</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>60</cell><cell>80</cell></row><row><cell>Thresholding</cell><cell>97.5</cell><cell>98.4</cell><cell>98.8</cell><cell>99.1</cell><cell>99.4</cell><cell>99.6</cell><cell>99.7</cell><cell>99.8</cell><cell>99.9</cell></row><row><cell>CNN P(5)</cell><cell>98.1</cell><cell>98.7</cell><cell>99.0</cell><cell>99.2</cell><cell>99.4</cell><cell>99.5</cell><cell>99.6</cell><cell>99.7</cell><cell>99.7</cell></row><row><cell>CNN P(10)</cell><cell>98.1</cell><cell>98.8</cell><cell>99.0</cell><cell>99.2</cell><cell>99.4</cell><cell>99.5</cell><cell>99.6</cell><cell>99.7</cell><cell>99.8</cell></row><row><cell>CNN P(30)</cell><cell>97.3</cell><cell>99.0</cell><cell>99.2</cell><cell>99.4</cell><cell>99.5</cell><cell>99.6</cell><cell>99.7</cell><cell>99.7</cell><cell>99.8</cell></row><row><cell>CNN U[1, 100]</cell><cell>97.8</cell><cell>98.8</cell><cell>99.1</cell><cell>99.3</cell><cell>99.5</cell><cell>99.6</cell><cell>99.7</cell><cell>99.7</cell><cell>99.8</cell></row><row><cell cols="6">Tab. 4: Iteration</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>60</cell><cell>80</cell></row><row><cell>Thresholding</cell><cell>95.1</cell><cell>96.8</cell><cell>97.6</cell><cell>98.1</cell><cell>98.8</cell><cell>99.2</cell><cell>99.4</cell><cell>99.6</cell><cell>99.9</cell></row><row><cell>CNN P(5)</cell><cell>96.2</cell><cell>97.5</cell><cell>98.0</cell><cell>98.4</cell><cell>98.8</cell><cell>99.0</cell><cell>99.2</cell><cell>99.3</cell><cell>99.5</cell></row><row><cell>CNN P(10)</cell><cell>96.3</cell><cell>97.6</cell><cell>98.1</cell><cell>98.4</cell><cell>98.9</cell><cell>99.1</cell><cell>99.3</cell><cell>99.4</cell><cell>99.5</cell></row><row><cell>CNN P(30)</cell><cell>94.8</cell><cell>98.0</cell><cell>98.5</cell><cell>98.7</cell><cell>99.0</cell><cell>99.2</cell><cell>99.3</cell><cell>99.4</cell><cell>99.5</cell></row><row><cell>CNN U[1, 100]</cell><cell>95.7</cell><cell>97.7</cell><cell>98.2</cell><cell>98.6</cell><cell>98.9</cell><cell>99.2</cell><cell>99.3</cell><cell>99.4</cell><cell>99.6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Brought to you by | Göteborg University -University of Gothenburg Authenticated Download Date | 10/14/19 10:17 AM</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Funding: This study was supported by the Ministry of Education and Science of the Russian Federation (grant 14.756.31.0001).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<idno>Date | 10/14/19 10:17 AM</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>Göteborg University -University of Gothenburg Authenticated Download</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Brought to you by</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Eflcient topology optimization in MATLAB using 88 lines of code</title>
		<author>
			<persName><forename type="first">E</forename><surname>Andreassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schevenels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Lazarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sigmund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Struct. Multidiscip. Optimiz</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optimal shape design as a material distribution problem</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Bendsøe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Struct. Multidiscip. Optimiz</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Bendsøe</surname></persName>
		</author>
		<title level="m">Optimization of Structural Topology, Shape, and Material</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">414</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Topology optimization-broadening the areas of application</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Bendsøe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Olhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sigmund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Control Cybern</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7" to="35" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Filters in topology optimization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bourdin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Numer. Meth. Engrg</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2143" to="2158" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Solving the quantum many-body problem with artificial neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Carleo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Troyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">355</biblScope>
			<biblScope unit="issue">6325</biblScope>
			<biblScope unit="page" from="602" to="606" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
	</analytic>
	<monogr>
		<title level="j">Keras. GitHub</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using neural networks to predict the functionality of reconfigurable nano-material networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M J</forename><surname>Van Damme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Koutnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Broersma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mikhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Van Der Wiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Advances in Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="339" to="351" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A simple heuristic for gray-scale suppression in optimality criterion-based topology optimization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Groenwold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F P</forename><surname>Etman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Struct. Multidiscip. Optimiz</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="217" to="225" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Hunter</surname></persName>
		</author>
		<ptr target="https://github.com/williamhunter/topy" />
		<title level="m">ToPy-Topology optimization with Python</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A new method of structural shape optimization based on biological growth</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mattheck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Burkhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Fatigue</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="185" to="190" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Spanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tamblyn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01361</idno>
		<title level="m">Deep learning and the Schrödinger equation</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Some aspects of the genesis of structures</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Mlejnek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Struct. Multidiscip. Optimiz</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="64" to="69" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">CaloGAN: simulating 3D high energy particle showers in multi-layer electromagnetic calorimeters with generative adversarial networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Paganini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nachman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02355</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting><address><addrLine>Munich</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the design of compliant mechanisms using topology optimization</title>
		<author>
			<persName><forename type="first">O</forename><surname>Sigmund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Structural Mechanics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="493" to="524" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<author>
			<persName><forename type="first">O</forename><surname>Sigmund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A 99 line topology optimization code written in Matlab</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ANI-1: an extensible neural network potential with DFT accuracy at force field computational cost</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Isayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Roitberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical Science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3192" to="3203" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Density filters for topology optimization based on the Pythagorean means</title>
		<author>
			<persName><forename type="first">K</forename><surname>Svanberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Svärd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Struct. Multidiscip. Optimiz</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="859" to="875" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Accelerating Eulerian fluid simulation with convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schlachter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Perlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.03597</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Um</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thuerey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04456</idno>
		<title level="m">Liquid splash modeling with neural networks</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A simple evolutionary procedure for structural optimization</title>
		<author>
			<persName><forename type="first">Yi</forename><forename type="middle">M</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Steven</surname></persName>
		</author>
		<idno>Date | 10/14/19 10:17 AM</idno>
	</analytic>
	<monogr>
		<title level="j">Computers&amp;Structures</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="885" to="896" />
			<date type="published" when="1993">1993</date>
		</imprint>
		<respStmt>
			<orgName>Brought to you by | Göteborg University -University of Gothenburg Authenticated Download</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
