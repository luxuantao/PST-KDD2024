<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Curriculum-Based Self-Training Makes Better Few-Shot Learners for Data-to-Text Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pei</forename><surname>Ke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DCST</orgName>
								<orgName type="institution" key="instit1">CoAI Group</orgName>
								<orgName type="institution" key="instit2">IAI</orgName>
								<orgName type="institution" key="instit3">BNRIST</orgName>
								<orgName type="institution" key="instit4">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haozhe</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DCST</orgName>
								<orgName type="institution" key="instit1">CoAI Group</orgName>
								<orgName type="institution" key="instit2">IAI</orgName>
								<orgName type="institution" key="instit3">BNRIST</orgName>
								<orgName type="institution" key="instit4">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenyu</forename><surname>Yang</surname></persName>
							<email>yangzhenyu@oppo.com</email>
							<affiliation key="aff1">
								<orgName type="institution">OPPO Mobile Telecommunications Corp</orgName>
								<address>
									<settlement>Ltd</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Huang</surname></persName>
							<email>huangyi@chinamobile.com</email>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">JIUTIAN Team</orgName>
								<orgName type="institution" key="instit2">China Mobile Research Institute</orgName>
								<address>
									<postCode>100053</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Tsinghua University-China Mobile Communications Group Co., Ltd. Joint Institute</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junlan</forename><surname>Feng</surname></persName>
							<email>fengjunlan@chinamobile.com</email>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">JIUTIAN Team</orgName>
								<orgName type="institution" key="instit2">China Mobile Research Institute</orgName>
								<address>
									<postCode>100053</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Tsinghua University-China Mobile Communications Group Co., Ltd. Joint Institute</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DCST</orgName>
								<orgName type="institution" key="instit1">CoAI Group</orgName>
								<orgName type="institution" key="instit2">IAI</orgName>
								<orgName type="institution" key="instit3">BNRIST</orgName>
								<orgName type="institution" key="instit4">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
							<email>aihuang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">DCST</orgName>
								<orgName type="institution" key="instit1">CoAI Group</orgName>
								<orgName type="institution" key="instit2">IAI</orgName>
								<orgName type="institution" key="instit3">BNRIST</orgName>
								<orgName type="institution" key="instit4">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Tsinghua University-China Mobile Communications Group Co., Ltd. Joint Institute</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Curriculum-Based Self-Training Makes Better Few-Shot Learners for Data-to-Text Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the success of text-to-text pre-trained models in various natural language generation (NLG) tasks, the generation performance is largely restricted by the number of labeled data in downstream tasks, particularly in data-to-text generation tasks. Existing works mostly utilize abundant unlabeled structured data to conduct unsupervised pre-training for task adaption, which fail to model the complex relationship between source structured data and target texts. Thus, we introduce self-training as a better few-shot learner than task-adaptive pre-training, which explicitly captures this relationship via pseudo-labeled data generated by the pre-trained model. To alleviate the side-effect of low-quality pseudo-labeled data during self-training, we propose a novel method called Curriculum-Based Self-Training (CBST) to effectively leverage unlabeled data in a rearranged order determined by the difficulty of text generation. Experimental results show that our method can outperform fine-tuning and task-adaptive pre-training methods, and achieve state-of-the-art performance in the few-shot setting of data-to-text generation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, text-to-text pre-trained models like <ref type="bibr">BART [Lewis et al., 2020]</ref> and <ref type="bibr">T5 [Raffel et al., 2020]</ref> have emerged in the field of natural language generation (NLG). Their main idea is to capture the relationship among texts by reconstructing original sentences with the input of corrupted sentences. These models can achieve state-of-the-art performance in various NLG tasks via fine-tuning on downstream datasets.</p><p>Despite the success of text-to-text pre-trained models, it's challenging to directly apply them to few-shot NLG tasks because their performance can be largely restricted by the number of labeled data <ref type="bibr" target="#b2">[Chen et al., 2020a;</ref><ref type="bibr" target="#b3">Chen et al., 2020b]</ref>. Especially in the task of few-shot data-to-text generation, it's ĜU / X denotes the corrupted structured data / texts. hard to learn the complex relationship between source structured data and target texts via limited labeled data <ref type="bibr" target="#b4">[Gong et al., 2020]</ref>. Existing works commonly continue pre-training on large amounts of structured data without corresponding texts for task adaption, aiming to enhance the model performance <ref type="bibr" target="#b5">[Gururangan et al., 2020]</ref>. As shown in Figure <ref type="figure" target="#fig_0">1</ref>(a), this task-adaptive pre-training strategy appears between general pre-training and fine-tuning, whose purpose is to adapt general pre-trained models (such as BART) to specific tasks (such as data-to-text generation) using unlabeled task data <ref type="bibr">[Xing and Wan, 2021]</ref>. We argue that task-adaptive pretraining only captures the relationship among structured data but fails to explicitly model the alignment between structured data and texts, thereby restricting the model performance on the few-shot data-to-text generation tasks.</p><p>In this paper, we introduce self-training <ref type="bibr" target="#b10">[Scudder, 1965]</ref> as a better few-shot learner for data-to-text generation. As shown in Figure <ref type="figure" target="#fig_0">1</ref>(b), self-training is a teacher-student framework where the teacher model creates synthetic labels for the unlabeled data, and the student model is trained on the pseudo-labeled data constructed by the teacher model. Compared with task-adaptive pre-training, self-training explicitly models the relationship between structured data and texts by training the student model on the pseudo-labeled data generated by the teacher model, instead of solely conducting unsupervised pre-training on the unlabeled structured data. We argue that the main challenge falls into the quality of pseudolabeled data. Even if the teacher model is initialized with pre-trained models, it may generate low-quality texts especially when dealing with the structured data with complex topologies, which hurt the performance of the student model.</p><p>Thus, we present a novel method called Curriculum-Based Self-Training (CBST) to address the challenge. This method utilizes curriculum learning <ref type="bibr" target="#b2">[Bengio et al., 2009]</ref> to construct pseudo-labeled data from easy cases to hard ones, and leverages such data into the training process at different iterations. Specifically, we divide the unlabeled dataset into different subsets based on some difficulty metric such as the number of input triples <ref type="bibr" target="#b9">[Ribeiro et al., 2020b]</ref>. At each iteration, we first collect the unlabeled data that satisfy the difficulty metric of the current curriculum. Then, we generate synthetic texts for these data using the teacher model, and select the pseudo-labeled data based on the coverage and generation probability. Finally, we train the student model on the labeled data and the selected pseudo-labeled data, and make it act as the teacher model at the next iteration. This method is expected to gradually increase the difficulty of generating texts for unlabeled structured data and improve the quality of pseudo-labeled data constructed by the teacher model.</p><p>Our contributions are mainly as follows<ref type="foot" target="#foot_1">1</ref> :</p><p>• We introduce self-training to improve text-to-text pretraining for few-shot data-to-text generation. This method explicitly captures the relationship between structured data and texts by generating texts for unlabeled structured data with the teacher model and training the student model on the pseudo-labeled data.</p><p>• We propose a novel method called CBST to alleviate the side-effect of low-quality pseudo-labeled data, which introduces curriculum learning to effectively leverage the unlabeled structured data into the training process in the order determined by the difficulty metric.</p><p>• We conduct extensive experiments in the few-shot setting of WebNLG and WikiBio datasets. Results show that our method can outperform fine-tuning and taskadaptive pre-training methods, and achieve state-of-theart performance on these two benchmarks.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data-to-Text Generation</head><p>Early studies on data-to-text generation mainly focus on how to encode the structural information of input data better. Thus, researchers devise complex encoder structures based on sequential neural networks <ref type="bibr" target="#b10">[Trisedya et al., 2018]</ref> and graph neural networks <ref type="bibr" target="#b9">[Ribeiro et al., 2020b]</ref> to achieve this goal. Recently, since text-to-text pre-trained models have shown promising performance in various NLG tasks, some works directly fine-tune these pre-trained models including <ref type="bibr">BART [Lewis et al., 2020]</ref> and <ref type="bibr">T5 [Raffel et al., 2020]</ref> on data-to-text datasets and report impressive results <ref type="bibr" target="#b8">[Ribeiro et al., 2020a;</ref><ref type="bibr">Kale and Rastogi, 2020]</ref>. Other works adapt pre-trained models to data-to-text generation tasks by designing specific pre-training tasks such as reconstructing structured data and texts, which further improve the model performance in both supervised and few-shot settings of downstream datasets <ref type="bibr" target="#b2">[Chen et al., 2020a;</ref><ref type="bibr" target="#b7">Ke et al., 2021;</ref><ref type="bibr">Xing and Wan, 2021]</ref>.</p><p>Compared with existing works, we introduce self-training to explicitly capture the relationship between structured data and texts, instead of solely relying on unsupervised taskadaptive pre-training. Our method is expected to utilize unlabeled data more effectively and further improve text-to-text pre-trained models in few-shot data-to-text generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Self-Training</head><p>Self-training <ref type="bibr" target="#b10">[Scudder, 1965]</ref> is a teacher-student framework to leverage unlabeled data through semi-supervised learning. Self-training has been applied to the tasks of text classification <ref type="bibr" target="#b4">[Du et al., 2021]</ref> and generation <ref type="bibr" target="#b5">[He et al., 2020]</ref>. With the development of pre-trained models, recent works also show that self-training is complementary to pre-training for various classification tasks <ref type="bibr" target="#b4">[Du et al., 2021]</ref>.</p><p>For comparison, our work is the first attempt to utilize selftraining to improve text-to-text pre-trained models for fewshot data-to-text generation. Moreover, we consider the difficulty of generating texts for different structured data and introduce curriculum learning <ref type="bibr" target="#b2">[Bengio et al., 2009]</ref> to incorporate unlabeled data into the self-training process at different iterations to improve the quality of pseudo-labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Definition and Model Overview</head><p>Given the text-to-text pre-trained model</p><formula xml:id="formula_0">f θ T 2T , the down- stream labeled dataset D L = (G L i , X L i ) m i=1</formula><p>, and the un-</p><formula xml:id="formula_1">labeled dataset D U = {G U i } M i=1</formula><p>where G L i / G U i denotes the structured data and X L i indicates the annotated texts (M ≫ m), our goal is to obtain a data-to-text pre-trained model which can perform well in the few-shot setting of downstream datasets.</p><p>The overview of our proposed method is shown in Figure <ref type="figure">2</ref>. We follow the existing works to linearize the structured data as a sequence of triples containing subjects, predicates, and objects <ref type="bibr" target="#b8">[Ribeiro et al., 2020a;</ref><ref type="bibr" target="#b2">Chen et al., 2020a]</ref> as shown in Figure <ref type="figure">3</ref>. We first initialize the teacher model f θ T with the pre-trained model f θ T 2T and train it on the labeled dataset D L . Then, we train the teacher model and the student model iteratively. At each iteration, the teacher model f θ T generates synthetic texts for the unlabeled dataset D ′ U which contains the data satisfying the difficulty metric of the current curriculum. Then we select the pseudo-labeled dataset D P L based on the coverage and generation probability of the teacher model. The student model f θ S which is also initialized with the pre-trained model f θ T 2T is trained on the pseudo-labeled dataset D P L and the labeled dataset D L . Finally, the student model f θ S acts as the teacher model f θ T in the next iteration.</p><formula xml:id="formula_2">Teacher (𝑓 𝜃 𝑇 ) Text-to-Text PLM (𝑓 𝜃 𝑇2𝑇 ) Labeled Dataset (𝐷 𝐿 ) Unlabeled Dataset (𝐷 𝑈 ) Pseudo-Labeled Dataset (𝐷 𝑃𝐿 ) Student (𝑓 𝜃 𝑆 ) Train Initialization Unlabeled Dataset ( 𝐷 𝑈 = 𝐷 𝑈 (1) ∪ ⋯ ∪ 𝐷 𝑈 (𝑀 𝑐 ) ) Curriculum Segmentation Unlabeled Dataset (𝐷 𝑈 ′ = ‫ڂ‬ 𝑖=1 𝑡 𝐷 𝑈 (𝑖) ) Select Train Initialization Generate Text-to-Text PLM (𝑓 𝜃 𝑇2𝑇 ) Labeled Dataset (𝐷 𝐿 ) Train</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Curriculum-Based Self-Training</head><p>Our proposed method is shown in Algorithm 1, which consists of three main steps: curriculum segmentation, pseudolabeled data construction, and student model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Curriculum Segmentation</head><p>Curriculum segmentation is designed based on the difficulty of generating texts for structured data. In this paper, we choose the number of triples as the difficulty metric to segment the unlabeled dataset D U into M C subsets. This metric reflects the complexity of structures in the input data and has a significant impact on the quality of the generated texts <ref type="bibr" target="#b9">[Ribeiro et al., 2020b]</ref>. We also try other metrics in §4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pseudo-Labeled Data Construction</head><p>This module aims to generate pseudo-labeled data with the teacher model at each iteration. Prior to the first iteration, the teacher model f θ T is initialized with the pre-trained model f θ T 2T and trained on the labeled dataset D L with the following loss function:</p><formula xml:id="formula_3">L init = 1 m m i=1 l(X L i , G L i ; f θ T ) (1)</formula><p>where l(X, G; f θ ) indicates the negative log-likelihood loss and is commonly used in the sequence generation tasks:</p><formula xml:id="formula_4">l(X, G; f θ ) = − |X| t=1 log P θ (X t |G, X &lt;t )<label>(2)</label></formula><p>Note that except for the first iteration, the teacher model is initialized by the student model from the previous iteration.</p><p>Algorithm 1 Curriculum-Based Self-Training (CBST)</p><p>Require:</p><p>Labeled dataset:</p><formula xml:id="formula_5">D L = {(G L i , X L i )} m i=1</formula><p>Unlabeled dataset:</p><formula xml:id="formula_6">D U = {G U i } M i=1</formula><p>Curriculum segmentation criterion: C Text-to-text pre-trained model:</p><formula xml:id="formula_7">f θ T 2T Ensure:</formula><p>Data-to-text pre-trained model:</p><formula xml:id="formula_8">f θ D2T 1: Use C to split D U into D (1) U , D (2) U , ..., D (M C ) U 2: Initialize the teacher model f θ T with f θ T 2T and train f θ T on D L via Eq.(1) 3: for t = 1, 2, • • • , M C do 4: D ′ U ← t i=1 D (i) U 5:</formula><p>Apply f θ T to the unlabeled dataset D ′ U to acquire the generated texts via Eq.( <ref type="formula" target="#formula_10">3</ref>) 6:</p><p>Select the pseudo-labeled data based on coverage and generation probability to construct the dataset D P L</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Initialize the student model f θ S with f θ T 2T and train f θ S on D P L ∪ D L with input noise via Eq.(4) 8:</p><formula xml:id="formula_9">f θ T ← f θ S 9: end for 10: f θ D2T ← f θ S 11: return f θ D2T</formula><p>At each iteration, we first utilize the teacher model to generate the synthetic texts for the unlabeled dataset D ′ U ⊂ D U which includes the unlabeled structured data that satisfy the difficulty metric of the current curriculum:</p><formula xml:id="formula_10">X gen i = f θ T (G U i ), i = 1, 2, • • • , |D ′ U |<label>(3)</label></formula><p>Then, we select the generated texts based on coverage and generation probability which reflect the generation quality <ref type="bibr" target="#b4">[Gehrmann et al., 2018]</ref> to construct the pseudo-labeled dataset D P L with M ′ samples. Specifically, we choose the pseudo-labeled data where the proportion of subjects and objects appearing in the generated text is larger than ϵ cov and the generation probability of the text ranks top-ϵ gen to construct the dataset for the student model training. Here, ϵ cov and ϵ gen are both hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Student Model Training</head><p>The goal of this module is to train the student model on the pseudo-labeled dataset and the labeled dataset. Initialized with the text-to-text pre-trained model f θ T 2T , the student model f θ S is trained with the following loss function:</p><formula xml:id="formula_11">L S = 1 M ′ M ′ i=1 l(X gen i , NF(G U i ); f θ S ) + 1 m m i=1 l(X L i , G L i ; f θ S )<label>(4)</label></formula><p>where NF(G) denotes the noise function perturbing the structured data G.  <ref type="table" target="#tab_0">1</ref>, including word substitution at the semantic level and triple reordering at the structural level. Specifically, we substitute each word by its synonym with the probability p word and shuffle the order of triples in each sample with the probability p triple , where p word and p triple are hyper-parameters. These functions are expected to perturb the input structured data while largely keeping the semantic and structural information.</p><p>In practice, we adopt a separate training strategy which shows better performance than joint training <ref type="bibr" target="#b5">[He et al., 2020]</ref>. This strategy first trains the student model on the pseudolabeled dataset D P L with the first term of L S , and then trains it on the labeled dataset D L with the second term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>WebNLG. This dataset aims to generate textual descriptions for RDF triples <ref type="bibr" target="#b10">[Shimorina and Gardent, 2018]</ref>. The number of instances in training / validation / test set is 34,352 / 4,316 / 4,224, respectively. We followed the existing works <ref type="bibr" target="#b2">[Chen et al., 2020a]</ref> to pre-process this dataset and use 0.5%, 1%, 5%, 10% of the training instances as the labeled datasets in the few-shot setting. WikiBio. This dataset aims to generate the first sentence of biography descriptions for Wikipedia tables <ref type="bibr" target="#b7">[Lebret et al., 2016]</ref>. The original split of the training / validation / test set is 582,658 / 2,000 / 72,831. We followed the existing works <ref type="bibr" target="#b2">[Chen et al., 2020a;</ref><ref type="bibr" target="#b3">Chen et al., 2020b]</ref> to pre-process this dataset and use 50, 100, 200, and 500 samples from the training dataset as the labeled datasets in the few-shot setting.</p><p>We further constructed the unlabeled dataset for each benchmark dataset based on GenWiki <ref type="bibr" target="#b6">[Jin et al., 2020]</ref>. This dataset consists of general-domain unpaired structured data and texts sharing the same content distribution. We directly removed the texts in this dataset, and filtered out the structured data that do not have the subjects and objects appearing in the corresponding labeled dataset. Thus, we obtained two unlabeled datasets for WebNLG and WikiBio, respectively, which contain 375,408 / 163,804 samples of structured data without annotated texts<ref type="foot" target="#foot_2">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Detail</head><p>In our self-training algorithm, we set the number of curriculum M C to be 3. For both WebNLG and WikiBio datasets, we split the corresponding unlabeled dataset into 3 subsets which contain the structured data with (≤2) / 3-4 / (≥5) triples. For the hyper-parameters to select pseudo-labeled data, we set ϵ cov = 1.0, ϵ gen = 50%. The probabilities of word substitution and triple reordering were set to p word = p triple = 0.4.</p><p>As for the model structure, we used <ref type="bibr">BART [Lewis et al., 2020]</ref> as the text-to-text pre-trained model in our experiments. The base version of BART was adopted because of the limited computational resources. We followed BART to use Byte-Pair Encoding vocabulary with the size of 50,265.</p><p>The training epoch at each iteration was set to be 20. The learning rate was 0.00003. The batch size was 32 / 24 for WebNLG / WikiBio, respectively. The maximum length of linearized structured data was 256 / 384 for WebNLG / Wik-iBio, respectively, while the length of text sequences was 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baseline</head><p>Direct Fine-Tuning. This category of baselines directly fine-tunes the state-of-the-art pre-trained models including <ref type="bibr">KGPT [Chen et al., 2020a]</ref>, Switch- <ref type="bibr">GPT-2 [Chen et al., 2020b]</ref> and <ref type="bibr">BART [Lewis et al., 2020]</ref> on the labeled data without the use of unlabeled data. We denoted these baselines as FT-KGPT, FT-Switch-GPT-2 and FT-BART, respectively.</p><p>Task-Adaptive Pre-Training. This category of baselines designs task-adaptive pre-training methods on unlabeled data before fine-tuning. We chose two representative pretraining tasks: 1) Sequence-level reconstruction (SeqRecon) from <ref type="bibr">BART [Lewis et al., 2020]</ref> which decodes complete linearized structured data when encoding corrupted structured data; 2) Graph-level reconstruction (GraphRecon) from JointGT <ref type="bibr" target="#b7">[Ke et al., 2021]</ref> which predicts the masked tokens at the output layer of the encoder with the input of corrupted structured data. We denoted them as TAPT-SeqRecon and TAPT-GraphRecon, respectively.</p><p>For a fair comparison, we also used <ref type="bibr">BART [Lewis et al., 2020]</ref> as the text-to-text pre-trained model for task-adaptive pre-training baselines. We presented the results of our model CBST with two ablation models, i.e., CBST w/o CL and CBST w/o CL &amp; Noise. The former ablation model removes </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Automatic Result</head><p>We followed the existing works <ref type="bibr" target="#b10">[Shimorina and Gardent, 2018;</ref><ref type="bibr" target="#b2">Chen et al., 2020a]</ref> to adopt <ref type="bibr">BLEU-4 [Papineni et al., 2002]</ref>, METEOR <ref type="bibr" target="#b2">[Banerjee and Lavie, 2005]</ref>, and ROUGE-L <ref type="bibr" target="#b8">[Lin, 2004]</ref> to evaluate the generated results on WebNLG, and use BLEU-4 as the metric on WikiBio.</p><p>The main results on WebNLG and WikiBio are shown in Table <ref type="table" target="#tab_3">2 and 3</ref>. We can observe that CBST significantly outperforms fine-tuning and task-adaptive pre-training methods in all the settings, which shows that our method can explicitly learn the relationship between structured data and texts via self-training and improve the model performance. The comparison between CBST and the ablation models indicates that curriculum learning can alleviate the problem of lowquality pseudo-labeled data and further improve the performance. The noise functions also contribute to the final performance by increasing the smoothness of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Human Evaluation</head><p>To further evaluate the quality of generated results, we conducted human evaluation in the 1% setting of WebNLG. We followed the existing works <ref type="bibr" target="#b9">[Ribeiro et al., 2020b]</ref> to use two criteria: fluency (whether a sentence is grammatically fluent) and adequacy (whether a sentence clearly describes the structured data). We randomly sampled 100 structured data from the test set, and collected the generated results from CBST and other baselines. We adopted pairwise comparison <ref type="bibr" target="#b7">[Ke et al., 2021]</ref> between CBST and other baselines. For each pair of generated texts (one from CBST and the other from the corresponding baseline, given the same input structured data), three annotators were hired to determine which text is better (i.e., win, lose or tie) in terms of the above metrics.</p><p>Results in Table <ref type="table" target="#tab_4">4</ref> show that CBST can significantly outperform the baselines based on direct fine-tuning and taskadaptive pre-training in both fluency and adequacy. In addition, the improvement of CBST over two ablation models shows the effectiveness of our curriculum learning module and noise functions to generate fluent texts which describe structured data more clearly. We also calculated Fleiss' Kappa <ref type="bibr" target="#b4">[Fleiss, 1971]</ref> for each pairwise comparison to measure the agreement among different annotators, where the results in Table <ref type="table" target="#tab_4">4</ref> show moderate agreement (0.4 ≤ κ ≤ 0.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation Study</head><p>We conducted a detailed ablation test to study the effects of different noise functions, selection criteria, and difficulty metrics. We removed each module respectively and presented the results on WebNLG (1%) in Table <ref type="table">5</ref>. Note that w/ DiffLen denotes that we used the length of linearized structured data as the difficulty metric. Results in Table <ref type="table">5</ref> show that all the modules contribute to the final performance. As for two noise functions, the performance of CBST degrades more in the setting of removing word substitution, which perturbs structured data more flexibly. In terms of the selection criterion, both coverage and generation probability improve the quality of pseudo-labeled data and contribute to the final performance. We can also observe the performance drop when CBST used the length of linearized structured data as the difficulty metric, indicating that the number of input triples is a more proper metric to reflect the difficulty of text generation from structured data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Analysis on Pseudo-Labeled Data</head><p>To study whether our method can improve the quality of pseudo-labeled data via curriculum learning, we evaluated the quality of data generated by the teacher model at the last iteration before selection. We resorted to human evaluation since there is no annotated text for unlabeled structured data. We randomly sampled 100 unlabeled structured data and collected the generated results of the teacher models from CBST and the ablation models at the last iteration. Three annotators were hired to judge each generated text from the following fine-grained aspects: 1) Hallucination: whether the generated text includes non-existing facts; 2) Missing fact: whether the generated text misses input facts; 3) Fluency: the fluency score of generated texts (score 1-5 where 5 indicates fully fluent sentences).</p><p>We presented the fluency score and the proportions of generated results that belong to hallucination / missing fact in Table <ref type="table" target="#tab_5">6</ref>. Results show that curriculum learning in our method plays an important role in generating fluent and adequate texts to describe unlabeled structured data, resulting in better model performance. The relatively limited improvement on missing facts may be because our base model BART already has the strong ability to generate texts that appear in the input via its pre-training tasks based on text reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Analysis on Curriculum Learning</head><p>To further analyze how curriculum learning helps selftraining in few-shot data-to-text generation, we first demonstrated how the number of curriculum (M C ) affects the fi-  nal performance. The results in Table <ref type="table">7</ref> show that the best performance is reached at M C = 3. When M C is smaller, the teacher model needs to generate texts for unlabeled structured data with multiple triples at early iterations, which may degrade the quality of pseudo-labeled data and the final performance. In contrast, when M C is larger, the student model is trained on easy unlabeled data for many iterations and cannot utilize the hard cases until the late iterations, which may also affect the model performance.</p><p>Furthermore, we set M C = 3 and visualized the performance of the student model at each iteration in Figure <ref type="figure" target="#fig_2">4</ref>. Note that the values when the number of iterations equals 0 indicate the performance of direct fine-tuning. At early iterations, the two ablation models perform better because they directly incorporate the whole unlabeled dataset into self-training and acquire a larger pseudo-labeled dataset. However, the improvement of their performance is extremely limited at the second and third iterations since the low-quality texts generated by the teacher model at early iterations may restrict the model performance. For comparison, CBST only utilizes the unlabeled data that satisfy the difficulty metric at each iteration to avoid low-quality pseudo-labeled data. Despite the worse performance at early iterations due to the smaller number of unlabeled data included in the self-training process, CBST still obtains the best performance at the last iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduce self-training to improve text-to-text pre-trained models on few-shot data-to-text generation tasks, which can utilize unlabeled structured data to explicitly model the relationship between structured data and texts. To alleviate the problem of low-quality pseudo-labeled data during self-training, we further propose a novel method called Curriculum-Based Self-Training to rearrange the order of unlabeled data incorporated into self-training based on the difficulty metric. Experimental results show that CBST outperforms fine-tuning and task-adaptive pre-training methods, and achieves state-of-the-art performance in the few-shot setting of WebNLG and WikiBio datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison between task-adaptive pre-training and selftraining in few-shot data-to-text generation. Task-adaptive pretraining adopts unsupervised pre-training methods on the unlabeled task data G U (such as structured data reconstruction), while selftraining instead generates texts X gen for G U with the teacher model and trains the student model on the generated pseudo-labeled data. ĜU / X denotes the corrupted structured data / texts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Overview of CBST. Dotted lines indicate that the teacher is initialized with the text-to-text pre-trained model and trained on the labeled dataset only before the first iteration. Afterward, the teacher model is initialized by the student model from the previous iteration. MC denotes the number of curriculum, and D ′ U contains the unlabeled data included in the current curriculum at each iteration t.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: BLEU-4 of CBST and two ablation models at different iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Word Substitution: Replace words with their synonyms Example Input: &lt;S&gt; Bakewell pudding &lt;P&gt; served &lt;O&gt; Warm or cold Example Output: &lt;S&gt; Bakewell dessert &lt;P&gt; acted &lt;O&gt; Warm or cold Triple Reordering: Shuffle the order of triples Example Input: &lt;S&gt; Alan Shepard &lt;P&gt; status &lt;O&gt; Deceased &lt;S&gt; Alan Shepard &lt;P&gt; occupation &lt;O&gt; Test pilot Example Output: &lt;S&gt; Alan Shepard &lt;P&gt; occupation &lt;O&gt; Test pilot &lt;S&gt; Alan Shepard &lt;P&gt; status &lt;O&gt; Deceased Noise functions. &lt;S&gt; / &lt;P&gt; / &lt;O&gt; is the special token indicating the subject / predicate / object of the input triples, respectively. Bold texts indicate the words for substitution.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>, METEOR(M) and ROUGE-L(R-L) in the different settings of WebNLG. The results of FT-KGPT are re-printed from the original paper of KGPT. -means that the results are not reported in the corresponding reference. ** indicates that our model significantly outperforms the best baseline in the corresponding setting (t-test, p &lt; 0.01).</figDesc><table><row><cell># of Training Data</cell><cell>50</cell><cell>100</cell><cell>200</cell><cell>500</cell></row><row><cell></cell><cell cols="2">Direct Fine-Tuning</cell><cell></cell><cell></cell></row><row><cell>FT-Switch-GPT-2</cell><cell cols="3">17.20 23.80 25.40</cell><cell>28.60</cell></row><row><cell>FT-KGPT</cell><cell cols="3">24.20 27.60 29.10</cell><cell>30.00</cell></row><row><cell>FT-BART</cell><cell cols="3">25.83 29.38 32.53</cell><cell>34.80</cell></row><row><cell cols="4">Task-Adaptive Pre-Training</cell><cell></cell></row><row><cell>TAPT-SeqRecon</cell><cell cols="3">27.93 30.81 32.59</cell><cell>34.90</cell></row><row><cell cols="4">TAPT-GraphRecon 28.86 30.40 32.49</cell><cell>35.24</cell></row><row><cell></cell><cell cols="2">Self-Training</cell><cell></cell><cell></cell></row><row><cell>CBST (Ours)</cell><cell cols="4">29.63* 31.23* 33.21* 36.27**</cell></row><row><cell>w/o CL</cell><cell cols="3">29.05 30.70 33.13</cell><cell>35.03</cell></row><row><cell cols="4">w/o CL &amp; Noise 27.96 30.54 32.65</cell><cell>34.87</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc> in the different settings of WikiBio. The results of FT-Switch-GPT-2 and FT-KGPT are re-printed from the original paper of KGPT. * indicates that our model significantly outperforms the best baseline in the corresponding setting (t-test, p &lt; 0.05), while ** means p &lt; 0.01.</figDesc><table><row><cell>curriculum learning from CBST, which is similar to noisy</cell></row><row><cell>self-training [He et al., 2020]. The latter one simultaneously</cell></row><row><cell>removes curriculum learning and input noise, which acts as</cell></row><row><cell>vanilla self-training. All the results were presented with the</cell></row><row><cell>average values over 3 runs.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Human evaluation on different methods in the 1% setting of WebNLG. The scores indicate the percentages of win, lose and tie when CBST is compared with other baselines. κ is Fleiss' Kappa (all indicate moderate agreement). The scores marked with * mean p &lt; 0.05 while ** means p &lt; 0.01 in sign test.</figDesc><table><row><cell>Criterion Model</cell><cell cols="3">Fluency Win (%) Lose (%) Tie (%)</cell><cell>κ</cell></row><row><cell>CBST vs. FT-BART</cell><cell>41.3*</cell><cell>31.7</cell><cell cols="2">27.0 0.411</cell></row><row><cell>CBST vs. TAPT-SeqRecon</cell><cell>42.0**</cell><cell>29.3</cell><cell cols="2">28.7 0.410</cell></row><row><cell>CBST vs. TAPT-GraphRecon</cell><cell>41.0*</cell><cell>32.0</cell><cell cols="2">27.0 0.492</cell></row><row><cell>CBST vs. CBST w/o CL</cell><cell>40.3**</cell><cell>28.0</cell><cell cols="2">31.7 0.438</cell></row><row><cell cols="2">CBST vs. CBST w/o CL &amp; Noise 44.7**</cell><cell>27.0</cell><cell cols="2">28.3 0.403</cell></row><row><cell>Criterion Model</cell><cell cols="3">Adequacy Win (%) Lose (%) Tie (%)</cell><cell>κ</cell></row><row><cell>CBST vs. FT-BART</cell><cell>58.7**</cell><cell>25.0</cell><cell cols="2">16.3 0.424</cell></row><row><cell>CBST vs. TAPT-SeqRecon</cell><cell>48.0**</cell><cell>26.3</cell><cell cols="2">25.7 0.427</cell></row><row><cell>CBST vs. TAPT-GraphRecon</cell><cell>44.0*</cell><cell>32.7</cell><cell cols="2">23.3 0.401</cell></row><row><cell>CBST vs. CBST w/o CL</cell><cell>40.0*</cell><cell>31.3</cell><cell cols="2">28.7 0.495</cell></row><row><cell cols="2">CBST vs. CBST w/o CL &amp; Noise 51.0**</cell><cell>28.3</cell><cell cols="2">20.7 0.449</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Human evaluation on the pseudo-labeled data generated by different methods at the last iteration.</figDesc><table><row><cell>Model</cell><cell></cell><cell>B-4</cell><cell>M</cell><cell>R-L</cell></row><row><cell>CBST</cell><cell></cell><cell cols="3">44.40 37.37 63.62</cell></row><row><cell cols="5">w/o NoiseTriple 43.87 37.24 62.92</cell></row><row><cell cols="5">w/o NoiseWord 43.53 36.93 62.94</cell></row><row><cell cols="5">w/o SelectCov 43.74 37.08 63.37</cell></row><row><cell cols="5">w/o SelectProb 43.53 37.19 62.44</cell></row><row><cell cols="2">w/ DiffLen</cell><cell cols="3">43.93 36.75 63.27</cell></row><row><cell cols="5">Table 5: Ablation test. NoiseTriple / NoiseWord / SelectCov /</cell></row><row><cell cols="5">SelectProb / DiffLen denotes triple reordering / word substitution</cell></row><row><cell cols="5">/ coverage-based selection / probability-based selection / length-</cell></row><row><cell cols="3">based difficulty metric, respectively.</cell><cell></cell></row><row><cell>Model</cell><cell cols="4">Hallucination Missing Fact Fluency</cell></row><row><cell>CBST</cell><cell></cell><cell>11.3%</cell><cell cols="2">19.0%</cell><cell>4.45</cell></row><row><cell>w/o CL</cell><cell></cell><cell>17.7%</cell><cell cols="2">19.3%</cell><cell>4.02</cell></row><row><cell>w/o CL &amp; Noise</cell><cell></cell><cell>18.0%</cell><cell cols="2">19.7%</cell><cell>3.97</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">The codes are available at https://github.com/kepei1106/CBST.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">We have conducted triple-level matching on the filtered Gen-Wiki and the test set of WebNLG / WikiBio. The results show that there is no overlap between them.Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the National Science Foundation for Distinguished Young Scholars (with No. 62125604) and the NSFC projects (Key project with No. 61936010 and regular project with No. 61876096). This work was supported by the Guoqiang Institute of Tsinghua University, with Grant No. 2019GQG1 and 2020GQG0005. This work was also supported by OPPO Research Fund. This work was sponsored by Tsinghua-Toyota Joint Research Fund.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m">5% 1% 5% 10% Model B-4 M R-L B-4 M R-L B-4 M R-L B-4 M R-L Direct Fine-Tuning FT-KGPT 22</title>
				<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="20" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">**</forename></persName>
		</author>
		<idno>34.24** 60.28** 44.40** 37.37** 63.62** 54.98** 42.18** 69.44** 58.78** 43.94** 71.30**</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">METEOR: an automatic metric for MT evaluation with improved correlation with human judgments</title>
		<author>
			<persName><forename type="first">Lavie</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Summarization@ACL 2005</title>
				<editor>
			<persName><forename type="first">Jérôme</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ronan</forename><surname>Louradour</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jason</forename><surname>Collobert</surname></persName>
		</editor>
		<editor>
			<persName><surname>Weston</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2005">2005. 2005. 2009. 2009. 2020a. 2020</date>
			<biblScope unit="volume">382</biblScope>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Few-shot NLG with pre-trained language model</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2020">2020b. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tablegpt: Few-shot table-to-text generation with structure reconstruction and content matching</title>
		<author>
			<persName><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="1971">2021. 2021. 1971. 1971. 2018. 2018. 2020</date>
			<biblScope unit="page" from="1978" to="1988" />
		</imprint>
	</monogr>
	<note>COL-ING</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<author>
			<persName><surname>Gururangan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020. 2020. 2020</date>
			<biblScope unit="page" from="8342" to="8360" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Genwiki: A dataset of 1.3 million contentsharing text and graphs for unsupervised graph-to-text generation</title>
		<author>
			<persName><surname>Hoyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Zhijing Jin, Qipeng Guo, Xipeng Qiu, and Zheng Zhang</title>
				<imprint>
			<publisher>Mihir Kale and Abhinav Rastogi</publisher>
			<date type="published" when="2019">2021. 2021. 2019. 2019. 2020. 2020. 2020. 2020</date>
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
	<note>COLING. Text-to-text pre-training for data-to-text tasks. In INLG</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><surname>Ke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of ACL-IJCNLP</title>
				<imprint>
			<date type="published" when="2016">2021. 2021. 2016. 2016. 2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><surname>Papineni</surname></persName>
		</author>
		<idno>140:1-140:67</idno>
	</analytic>
	<monogr>
		<title level="m">Hinrich Schütze, and Iryna Gurevych</title>
				<imprint>
			<date type="published" when="2002">2004. 2004. 2002. 2002. 2020. 2020. 2020a. 2020</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
	<note>Text summarization branches out. Investigating pretrained language models for graph-to-text generation. CoRR, abs/2007.08426</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modeling global and local node contexts for text generation from knowledge graphs</title>
		<author>
			<persName><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="589" to="604" />
			<date type="published" when="2020">2020b. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Probability of error of some adaptive pattern-recognition machines</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Scudder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Scudder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Shimorina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName><surname>Trisedya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Xinyu Xing and Xiaojun Wan</title>
				<imprint>
			<date type="published" when="1965">1965. 1965. 2018. 2018. 2018. 2018. 2021</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2273" to="2278" />
		</imprint>
	</monogr>
	<note>Findings of ACL-IJCNLP</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
