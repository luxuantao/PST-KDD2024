<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Drizzle: Fast and Adaptable Stream Processing at Scale</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Shivaram</forename><surname>Venkataraman</surname></persName>
							<email>shivaram@cs.berkeley.edu</email>
						</author>
						<author>
							<persName><forename type="first">U</forename><forename type="middle">C</forename><surname>Berkeley</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kay</forename><surname>Ousterhout</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Armbrust</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Databricks</forename><surname>Ali</surname></persName>
						</author>
						<author>
							<persName><roleName>UC</roleName><forename type="first">Ghodsi</forename><surname>Databricks</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Berkeley</forename><surname>Michael</surname></persName>
						</author>
						<author>
							<persName><forename type="first">J</forename><surname>Franklin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aurojit</forename><surname>Panda</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ali</forename><surname>Ghodsi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">UC</orgName>
								<address>
									<settlement>Berkeley Aurojit Panda</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">UC Berkeley Ion Stoica Databricks</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<region>UC</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Drizzle: Fast and Adaptable Stream Processing at Scale</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">43C4A8442F8F3A698ACDC07A29A355B2</idno>
					<idno type="DOI">10.1145/3132747.3132750</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Stream Processing</term>
					<term>Reliability</term>
					<term>Performance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large scale streaming systems aim to provide high throughput and low latency. They are often used to run mission-critical applications, and must be available 24x7. Thus such systems need to adapt to failures and inherent changes in workloads, with minimal impact on latency and throughput. Unfortunately, existing solutions require operators to choose between achieving low latency during normal operation and incurring minimal impact during adaptation. Continuous operator streaming systems, such as Naiad and Flink, provide low latency during normal execution but incur high overheads during adaptation (e.g., recovery), while micro-batch systems, such as Spark Streaming and FlumeJava, adapt rapidly at the cost of high latency during normal operations.</p><p>Our key observation is that while streaming workloads require millisecond-level processing, workload and cluster properties change less frequently. Based on this, we develop Drizzle, a system that decouples the processing interval from the coordination interval used for fault tolerance and adaptability. Our experiments on a 128 node EC2 cluster show that on the Yahoo Streaming Benchmark, Drizzle can achieve endto-end record processing latencies of less than 100ms and can get 2-3x lower latency than Spark. Drizzle also exhibits better adaptability, and can recover from failures 4x faster than Flink while having up to 13x lower latency during recovery.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent trends <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b51">52]</ref> in data analytics indicate the widespread adoption of stream processing workloads <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b58">59]</ref> that require low-latency and high-throughput execution. Examples of such workloads include real time object recognition <ref type="bibr" target="#b67">[68]</ref> and internet quality of experience prediction <ref type="bibr" target="#b36">[37]</ref>. Systems designed for stream processing <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b66">67]</ref> often process millions of events per second per machine and aim to provide sub-second processing latencies.</p><p>Stream processing systems are deployed to process data 24x7 <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b49">50]</ref>, and therefore in addition to meeting performance requirements, these systems must also be able to handle changes in the cluster, workload or incoming data. This requires that they gracefully react to software, machine or disk failures, that can happen once every few hours in large clusters <ref type="bibr" target="#b26">[27]</ref>, straggler tasks, which can slow down jobs by 6-8x <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref> and varying workload patterns <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b49">50]</ref>, which can result in more than 10x difference in load between peak and non-peak durations. To handle such changes data processing, systems have to adapt, i.e, dynamically change nodes on which operators are executed, and update the execution plan while ensuring consistent results. These adaptions occur frequently, and as a result systems need to handle them during normal execution, without sacrificing throughput or latency.</p><p>Unfortunately existing solutions for stream processing have either focused on providing low-latency during normal operation or on ensuring that adaptation does not affect latency. Systems like Naiad <ref type="bibr" target="#b47">[48]</ref> and Apache Flink <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b53">54]</ref> use a continuous operator streaming model that provides low latency during normal execution. However recovering from failures (or adapting to changes) in such systems is expensive, as even in cases where a single machine fails, the state for all operators must be reset to the last checkpoint, and computation must resume from that point. In contrast, micro-batch based systems like Spark Streaming <ref type="bibr" target="#b66">[67]</ref> and FlumeJava <ref type="bibr" target="#b15">[16]</ref> process data in batches using the bulk-synchronous processing (BSP) model. This design makes fault recovery (and other adaptations) more efficient as it is possible to reuse partial results and perform parallel recovery <ref type="bibr" target="#b66">[67]</ref>. However such systems typically impose a barrier across all nodes after every batch, resulting in high latency <ref type="bibr" target="#b62">[63]</ref>.</p><p>Furthermore, aspects of both solutions are required to meet throughput and latency goals. While continuous operator systems can achieve lower latency by immediately processing an input record, processing too few records at a time does not allow the use of techniques such as vectorizations <ref type="bibr" target="#b9">[10]</ref> and query optimization <ref type="bibr" target="#b54">[55]</ref> which require batching records. On the other hand, while batching in BSP systems naturally enables the use of these techniques, the coordination required at barriers induces additional processing time. As a result, reducing batch size to lower latency often results in unacceptable overhead.</p><p>In this paper we propose an alternate design based on the observation that while streaming workloads require millisecond-level processing latency, workload and cluster properties change at a much slower rate (several seconds or minutes) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b44">45]</ref>. As a result, it is possible to decouple the processing interval from the coordination interval used for fault tolerance, adaptability. This means that we can process tuples every few milliseconds, while coordinating to respond to workload and cluster changes every few seconds.</p><p>We implement this model in Drizzle. Our design makes use of a micro-batch processing model with a centralized scheduler and introduces a number of techniques to improve performance. To avoid the centralized scheduling bottleneck, we introduce group scheduling ( §3. <ref type="bibr" target="#b0">1)</ref>, where multiple batches (or a group) are scheduled at once. This decouples the granularity of data processing from scheduling decisions and amortizes the costs of task serialization and launch. One key challenge is in launching tasks before their input dependencies have been computed. We solve this using pre-scheduling ( §3.2), where we proactively queue tasks to be run on worker machines, and rely on workers to trigger tasks when their input dependencies are met. To achieve better throughput, we utilize query optimization techniques <ref type="bibr" target="#b9">[10]</ref> while processing small batches of inputs. In combination these techniques help Drizzle to achieve the latency and throughput requirements while remaining adaptable.</p><p>Choosing an appropriate group size is important to ensure that Drizzle achieves the desired properties. To simplify selecting a group size, we implement an automatic group-size tuning mechanism that adjusts the granularity of scheduling given a performance target.</p><p>We build Drizzle on Apache Spark and integrate Spark Streaming <ref type="bibr" target="#b66">[67]</ref> with Drizzle. Using micro-benchmarks on a 128 node EC2 cluster we show that group scheduling and pre-scheduling are effective at reducing coordination overheads by up to 5.5x compared to Apache Spark. With these improvements we show that on Yahoo's stream processing benchmark <ref type="bibr" target="#b62">[63]</ref>, Drizzle can achieve end-to-end record processing latencies of less than 100ms and is up to 3.5x faster when compared with Spark. Furthermore, as a result of the optimizations enabled by Drizzle, we achieve up to 3x better latency compared to Flink when throughput is held constant, and between 2-3x better throughput at fixed latency. Finally, in terms of fault tolerance, our experiments show that Drizzle recovers around 4x faster from failures than Flink while having up to 13x lower latency during recovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>We begin by providing some background about properties that are required for large scale stream processing, following which we describe and compare two computation models used by existing stream processing systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Desirable Properties of Streaming</head><p>To illustrate the requirements of production stream jobs, we briefly discuss the design of a prediction service at a video analytics company.</p><p>Case Study: Video Quality Prediction. The prediction service is run as a streaming application that computes a number of aggregates based on heartbeats from clients and then queries a machine learning model <ref type="bibr" target="#b36">[37]</ref> to determine the optimal parameters for bitrate, CDN location, etc. The output from the streaming job needs to be computed approximately every 100ms to reflect the most recent state of the network to the video frontends. Further the number of heartbeats can range in thousands to millions of updates per second. If the system cannot meet the 100ms deadline (either due to failures or other changes), the use of stale prediction results can lead to a user perceivable degradation in service due to say a wrong choice of bitrate. As a result the prediction service must ensure that it can rapidly recover from failures, and minimize violations of the target latency. Finally, due to the diurnal pattern, the number of viewers and heartbeats varies over the course of a day with specific events (e.g., Superbowl) leading to large increase in load. Thus the system also needs to adapt the number of machines used based on demand. Based on this example, we next discuss the main properties <ref type="bibr" target="#b55">[56]</ref> required by large scale streaming systems.</p><p>High Throughput. There has been a rapid increase in the amount of data being generated and collected for analysis, and high-throughput processing is essential to keeping up with data streams. For example, in a recent blog post <ref type="bibr" target="#b22">[23]</ref> LinkedIn reported generating over 1 trillion messages per day using Kafka, similarly Twitter <ref type="bibr" target="#b57">[58]</ref> reports that their timeseries databases need to handle 2.8 billion writes per minute. Keeping up with these ingest rates requires using distributed stream processing systems whose throughput matches or exceeds the incoming data rate.</p><p>Low Latency. We define the latency of a stream processing system as the time that elapses between receiving a new record and producing output that accounts for this record. For example, in an anomaly detection workload which predicts trends <ref type="bibr" target="#b1">[2]</ref> using a 1-second tumbling window, the processing latency is the time taken to process all events in a one second window and produce the necessary output. Processing latency impacts the system's responsiveness and high latency can both limit the applications that can be supported by a system and user perceived responsiveness. As a result, distributed streaming systems need to provide low latency, and should be designed to ensure that latencies remain low even when the system is scaled across machines.</p><p>Adaptability. Unlike traditional analytic queries, streaming jobs process live data and are long lived. As a result the system needs to be able to adapt to changes in both workload and cluster properties. For example, recent papers from Twitter <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b40">41]</ref> observe that failing hardware, load changes or misbehaving user code occur frequently in production clusters. Additionally it is important to ensure that adaptation does not lead to correctness or prolonged latency spikes <ref type="bibr" target="#b45">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consistency. A central challenge in distributed stream</head><p>processing is ensuring consistency for results produced over time. Consistency could be defined in terms of application-level semantics or lower-level message delivery semantics. For example if we have two counters, one tracking the number of requests and another tracking the number of responses, it is important to ensure that requests are accounted for before responses. This can be achieved by guaranteeing prefix integrity, where every output produced is equivalent to processing a well-specified prefix of the input stream. Additionally, lower-level message delivery semantics like exactly once delivery are useful for programmers implementing fault tolerant streaming applications. In this work we aim to maintain the consistency semantics offered by existing systems and focus on performance and adaptability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Computation Models for Streaming</head><p>BSP for Streaming Systems. The bulk-synchronous parallel (BSP) model has influenced many data processing frameworks. In this model, the computation consists of a phase whereby all parallel nodes in the system perform some local computation, followed by a blocking barrier that enables all nodes to communicate with each other, after which the process repeats itself. The MapReduce <ref type="bibr" target="#b23">[24]</ref> paradigm adheres to this model, whereby a map phase can do arbitrary local computations, followed by a barrier in the form of an all-toall shuffle, after which the reduce phase can proceed with each reducer reading the output of relevant mappers (often all of them). Systems such as Dryad <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b63">64]</ref>, Spark <ref type="bibr" target="#b65">[66]</ref>, and FlumeJava <ref type="bibr" target="#b15">[16]</ref>   <ref type="table">1</ref>: Comparison of the micro-batch and continuous operator models for different properties useful for streaming systems and also include specialized operators, e.g. filter, sum, groupby, join. Thus, the computation is a directed acyclic graph (DAG) of operators and is partitioned into different stages with a barrier between each of them. Within each stage, many map functions can be fused together as shown in Figure <ref type="figure">1</ref>. Further, many operators (e.g., sum, reduce) can be efficiently implemented <ref type="bibr" target="#b7">[8]</ref> by pre-combining data in the map stage and thus reducing the amount of data transferred.</p><p>Streaming systems, such as Spark Streaming <ref type="bibr" target="#b66">[67]</ref>, Google Dataflow <ref type="bibr" target="#b2">[3]</ref> with FlumeJava, adopt the aforementioned BSP model. They implement streaming by creating a micro-batch of duration T seconds. During the micro-batch, data is collected from the streaming source, processed through the entire DAG of operators and is followed by a barrier that outputs all the data to the streaming sink, e.g. Kafka. Thus, there is a barrier at the end of each micro-batch, as well as within the micro-batch if the DAG consists of multiple stages, e.g. if it has a group-by operator.</p><p>In the micro-batch model, the duration T constitutes a lower bound for record processing latency. Unfortunately, T cannot be set to adequately small values due to how barriers are implemented in all these systems. Consider a simple job consisting of a map phase followed by a reduce phase (Figure <ref type="figure">1</ref>). A centralized driver schedules all the map tasks to take turns running on free resources in the cluster. Each map task then outputs records for each reducer based on some partition strategy, such as hashing or sorting. Each task then informs the centralized driver of the allocation of output records to the different reducers. The driver can then schedule the reduce tasks on available cluster resources, and pass this metadata to each reduce task, which then fetches the relevant records from all the different map outputs. Thus, each barrier in a micro-batch requires communicating back and forth with the driver. Hence, setting T too low will result in a substantial communication overhead, whereby the communication with the driver eventually dominates the processing time. In most systems, T is limited to 0.5 seconds or more <ref type="bibr" target="#b62">[63]</ref>.</p><p>The use of barriers greatly simplifies fault-tolerance and scaling in BSP systems. First, the scheduler is notified at the end of each stage, and can reschedule tasks as necessary. This allows the scheduler to change the degree of parallelism provided to the job at the end of each stage, and effectively make use of any additional resources available to the job. Furthermore, fault tolerance in these systems is typically implemented by taking a snapshot when at a barrier. This snapshot can either be physical, i.e., record the output from each task in a stage; or logical, i.e., record the computational dependencies for some data. Task failures can be trivially recovered from using these snapshots since the scheduler can reschedule the task and have it read (or reconstruct) inputs from the previous stage's snapshot. Further since every task in the micro-batch model has deterministic inputs, fault recovery can be accelerated by running tasks from different micro-batches or different operators in parallel <ref type="bibr" target="#b66">[67]</ref>. For example, consider a job with one map and one reduce stage in each microbatch as shown in Figure <ref type="figure">1</ref>. If a machine fails, we might lose some map outputs from each timestep. With parallel recovery, map tasks from different timesteps can be run in parallel during recovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Continuous Operator Streaming.</head><p>An alternate computation model that is used in systems specialized for low latency workloads is the dataflow <ref type="bibr" target="#b37">[38]</ref> computation model with long running or continuous operators. Dataflow models have been used to build database systems <ref type="bibr" target="#b28">[29]</ref>, streaming databases <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17]</ref> and have been extended to support distributed execution in systems like Naiad <ref type="bibr" target="#b47">[48]</ref>, Stream-Scope <ref type="bibr" target="#b41">[42]</ref> and Flink <ref type="bibr" target="#b53">[54]</ref>. In such systems, user programs are similarly converted to a DAG of operators, and each operator is placed on a processor as a long running task. As data is processed, operators update local state and messages are directly transferred from between operators. Barriers are inserted only when required by specific operators. Thus, unlike BSP-based systems, there is no scheduling or communication overhead with a centralized driver. Unlike BSP-based systems, which require a barrier at the end of a micro-batch, continuous operator systems do not impose any such barriers.</p><p>To handle machine failures, dataflow systems typically use distributed checkpointing algorithms <ref type="bibr" target="#b17">[18]</ref> to create consistent snapshots periodically. The execution model is flexible and can accommodate either asynchronous <ref type="bibr" target="#b12">[13]</ref> checkpoints (in systems like Flink) or synchronous checkpoints (in systems like Naiad). Recent work on Falkirk Wheel <ref type="bibr" target="#b32">[33]</ref> provides a more detailed description comparing these two approaches and also describes how the amount of state that is checkpointed in timely dataflow can be minimized by checkpointing at the end of a processing epoch. However checkpoint replay during recovery can be more expensive in this model. In both synchronous and asynchronous approaches, whenever a node fails, all the nodes are rolled back to the last consistent checkpoint and records are then replayed from this point. As the continuous operators cannot be easily split into smaller components this precludes parallelizing recovery across timesteps (as in the BSP model) and each continuous operator is recovered serially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Comparing Computation Models</head><p>Table <ref type="table">1</ref> summarizes the difference between the models. As mentioned, BSP-based systems suffer from poor latency due to scheduling and communication overheads which lowerbound the micro-batch length. If the micro-batch duration T is set lower than that, the system will fall behind and become unstable. Continuous operator systems do not have this disadvantage, as no barrier-related scheduling and communication overhead is necessary.</p><p>On the other hand, BSP-based systems can naturally adapt at barrier boundaries to recover from failures or add/remove nodes. Continuous operator systems would have to roll-back to a checkpoint and replay from that point on. Both models can guarantee exactly-one semantics.</p><p>Finally the execution model in BSP-systems also makes it easy to achieve high throughput by applying optimized execution techniques <ref type="bibr" target="#b9">[10]</ref> within each micro-batch. This is especially advantageous for aggregation-based workloads where combining updates across records in a batch can significantly reduce the amount of data transferred over the network. Supporting such optimizations in continuous operator systems and requires explicit buffering or batching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DESIGN</head><p>Next we detail the design of Drizzle. Drizzle builds on existing BSP-based streaming systems, and we begin by showing how the BSP model can be changed to dramatically reduce average scheduling and communication overheads. In designing Drizzle, we chose to extend the BSP model since it allows us to inherit existing support for parallel recovery and optimizations for high-throughput batch processing. We believe one could go in the other direction, that is start with a continuous operator system, modify it to add support for tasks with periodic centralized coordination and get similar benefits.</p><p>Our high level approach to removing the overheads in the BSP-based streaming model is to decouple the size of the micro-batch being processed from the interval at which coordination takes place. This decoupling will allow us to reduce the size of a micro-batch to achieve sub-second processing</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head><p>Control Message Driver  latency, while ensuring that coordination, which helps the system adapt to failures and cluster membership changes, takes place every few seconds. We focus on the two sources of coordination that exists in BSP systems (Figure <ref type="figure">1</ref>). First we look at the centralized coordination that exists between microbatches and how we can remove this with group scheduling. Following that, we discuss how we can remove the barrier within a micro-batch using pre-scheduling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Group Scheduling</head><p>BSP frameworks like Spark, FlumeJava <ref type="bibr" target="#b15">[16]</ref> or Scope <ref type="bibr" target="#b14">[15]</ref> use centralized schedulers that implement many complex scheduling techniques; these include: algorithms to account for locality <ref type="bibr" target="#b64">[65]</ref>, straggler mitigation <ref type="bibr" target="#b5">[6]</ref>, fair sharing <ref type="bibr" target="#b34">[35]</ref> etc. Scheduling a single stage in these systems proceeds as follows: first, a centralized scheduler computes which worker each task should be assigned to, taking in the account locality and other constraints. Following this tasks are serialized and sent to workers using an RPC. The complexity of the scheduling algorithms used coupled with computational and network limitations of the single machine running the scheduler imposes a fundamental limit on how fast tasks can be scheduled.</p><p>The limitation a single centralized scheduler has been identified before in efforts like Sparrow <ref type="bibr" target="#b51">[52]</ref>, Apollo <ref type="bibr" target="#b10">[11]</ref>. However, these systems restrict themselves to cases where scheduled tasks are independent of each other, e.g., Sparrow forwards each scheduling request to one of many distributed schedulers which do not coordinate among themselves and hence cannot account for dependencies between requests. In Drizzle, we focus on micro-batch streaming jobs where there are dependencies between batches and thus coordination is required when scheduling parts of a single streaming job.</p><p>To alleviate centralized scheduling overheads, we study the execution DAG of streaming jobs. We observe that in stream processing jobs, the computation DAG used to process microbatches is largely static, and changes infrequently. Based on this observation, we propose reusing scheduling decisions across micro-batches. Reusing scheduling decisions means that we can schedule tasks for multiple micro-batches (or a group) at once (Figure <ref type="figure" target="#fig_2">2</ref>) and thus amortize the centralized scheduling overheads. To see how this can help, consider a streaming job used to compute moving averages. Assuming the data sources remain the same, the locality preferences computed for every micro-batch will be same. If the cluster configuration also remains static, the same worker to task mapping will be computed for every micro-batch. Thus we can run scheduling algorithms once and reuse its decisions. Similarly, we can reduce network overhead of RPCs by combining tasks from multiple micro-batches into a single message.</p><p>When using group scheduling, one needs to be careful in choosing how many micro-batches are scheduled at a time. We discuss how the group size affects the performance and adaptability properties in §3.3 and present techniques for automatically choosing this size in §3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pre-Scheduling Shuffles</head><p>While the previous section described how we can eliminate the barrier between micro-batches, as described in Section 2 (Figure <ref type="figure">1</ref>), existing BSP systems also impose a barrier within a micro-batch to coordinate data transfer for shuffle operations. We next discuss how we can eliminate these barriers as well and thus eliminate all coordination within a group.</p><p>In a shuffle operation we have a set of upstream tasks (or map tasks) that produce output and a set of downstream tasks (or reduce tasks) that receive the outputs and run the reduction function. In existing BSP systems like Spark or Hadoop, upstream tasks typically write their output to local disk and notify the centralized driver of the allocation of output records to the different reducers. The driver then applies task placement techniques <ref type="bibr" target="#b18">[19]</ref> to minimize network overheads and creates downstream tasks that pull data from the upstream tasks. Thus in this case the metadata is communicated through the centralized driver and then following a barrier, the data transfer happens using a pull based mechanism.</p><p>To remove this barrier, we pre-schedule downstream tasks before the upstream tasks (Figure <ref type="figure" target="#fig_3">3</ref>) in Drizzle. We perform scheduling so downstream tasks are launched first; upstream tasks are then scheduled with metadata that tells them which machines running the downstream tasks need to be notified on completion. Thus, data is directly transferred between workers without any centralized coordination. This approach has two benefits. First, it scales better with the number of workers as it avoids centralized metadata management. Second, it removes the barrier, where succeeding stages are launched only when all the tasks in the preceding stage complete.</p><p>We implement pre-scheduling by adding a local scheduler on each worker machine that manages pre-scheduled tasks.</p><p>When pre-scheduled tasks are first launched, these tasks are marked as inactive and do not use any resources. The local scheduler on the worker machine tracks the data dependencies that need to be satisfied. When an upstream task finishes, it materializes the output on local disk, notifies the corresponding downstream workers and asynchronously notifies the centralized scheduler. The local scheduler at the downstream task then updates the list of outstanding dependencies. When all the data dependencies for an inactive task have been met, the local scheduler makes the task active and runs it. When the task is run, it fetches the files materialized by the upstream tasks and continues processing. Thus we implement a push-metadata, pull-based data approach that minimizes the time to trigger tasks while allowing the downstream tasks to control when the data is transferred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adaptability in Drizzle</head><p>Group scheduling and shuffle pre-scheduling eliminate barriers both within and across micro-batches and ensure that barriers occur only once every group. However in doing so, we incur overheads when adapting to changes and we discuss how this affects fault tolerance, elasticity and workload changes below. This overhead largely does not affect record processing latency, which continues to happen within a group.</p><p>Fault tolerance. Similar to existing BSP systems we create synchronous checkpoints at regular intervals in Drizzle. The checkpoints can be taken at the end of any micro-batch and the end of a group of micro-batches presents one natural boundary. We use heartbeats from the workers to the centralized scheduler to detect machine failures. Upon noticing a failure, the scheduler resubmits tasks that were running on the failed machines. By default these recovery tasks begin execution from the latest checkpoint available. As the computation for each micro-batch is deterministic we further speed up the recovery process with two techniques. First, recovery tasks are executed in parallel <ref type="bibr" target="#b66">[67]</ref> across many machines. Second, we also reuse any intermediate data that was created by map stages run in earlier micro-batches. This is implemented with lineage tracking, a feature that is already present in existing BSP systems.</p><p>Using pre-scheduling means that there are some additional cases we need to handle during fault recovery in Drizzle. For reduce tasks that are run on a new machine, the centralized scheduler pre-populates the list of data dependencies that have been completed before. This list is maintained based on the asynchronous updates from upstream tasks. Similarly the scheduler also updates the active upstream (map) tasks to send outputs for succeeding micro-batches to the new machines. In both cases, if the tasks encounter a failure in either sending or fetching outputs they forward the failure to the centralized  scheduler. Thus we find having a centralized scheduler simplifies design and helps ensure that there is a single source that workers can rely on to make progress.</p><p>Elasticity. In addition to handling nodes being removed, we can also handle nodes being added to improve performance. To do this we integrate with existing cluster managers such as YARN <ref type="bibr" target="#b6">[7]</ref> and Mesos <ref type="bibr" target="#b31">[32]</ref> and the application layer can choose policies <ref type="bibr" target="#b19">[20]</ref> on when to request or relinquish resources. At the end of a group boundary, Drizzle updates the list of available resources and adjusts the tasks to be scheduled for the next group. Thus in this case, using a larger group size could lead to larger delays in responding to cluster changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Automatically selecting group size</head><p>Intuitively, the group size controls the performance to coordination trade-off in Drizzle. The total runtime of the job can be split between time spent in coordination and time spent in data-processing. In the absence of failures, the job's running time is minimized by avoiding coordination , while more frequent coordination enables better adaptability. These two objectives are thus at odds with each other. In Drizzle, we explore the tradeoff between these objectives by bounding coordination overheads while maximizing adaptability. Thus, we aim to choose a group size that is the smallest possible while having a fixed bound on coordination overheads.</p><p>We implement an adaptive group-size tuning algorithm that is inspired by TCP congestion control <ref type="bibr" target="#b11">[12]</ref>. During the execution of a group we use counters to track the amount of time spent in various parts of the system. Using these counters we are then able to determine what fraction of the end-to-end execution time was spent in scheduling and other coordination vs. time spent on the workers executing tasks. The ratio of time spent in scheduling to the overall execution gives us the scheduling overhead and we aim to maintain this overhead within user specified lower and upper bounds.</p><p>When the overhead goes above the upper bound, we multiplicatively increase the group size to ensure that the overhead decreases rapidly. Once the overhead goes below the lower bound, we additively decrease the group size to improve adaptivity. This is analogous to applying AIMD policy to determine the coordination frequency for a job. AIMD is widely used in TCP, and has been shown to provably converge in general <ref type="bibr" target="#b35">[36]</ref>. We use exponentially averaged scheduling overhead measurements when tuning group size. This ensures that we are stable despite transient latency spikes from garbage collection and other similar events.</p><p>The adaptive group-size tuning algorithms presents a simple approach that handles the most common scenarios we find in large scale deployments. However the scheme requires users to provide upper and lower bounds for the coordination overhead and these bounds could be hard to determine in a new execution environment. In the future we plan to study techniques that can measure various aspects of the environment to automatically determine the scheduling efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Data-plane Optimizations</head><p>The previous sections describe the design of the control-plane used in Drizzle, next we describe data plane optimizations enabled by Drizzle. To show the practical importance of batching in the data plane, we start by analyzing the query workload for a popular cloud hosted data analytics provider. We use this analysis to motivate the need for efficient support for aggregations and describe how batching can provide better throughput and latency for such workloads. Workload analysis. We analyze over 900,000 SQL queries and streaming queries executed on a popular cloud-based data analytics platform. These queries were executed by different users on a variety of data sets. We parsed these queries to determine the set of frequently used operators: we found that around 25% of queries used one or more aggregation functions. While our dataset did not explicitly distinguish between streaming and ad-hoc SQL queries, we believe that streaming queries which update dashboards naturally require aggregations across time. This platform supports two kinds of aggregation functions: aggregations that can use partial merge operations (e.g., sum) where the merge operation can be distributed across workers and complete aggregations (e.g., median) which require data to be collected and processed on a single machine. We found that over 95% of aggregation queries only made use of aggregates supporting partial merges. A complete breakdown is shown in Table <ref type="table" target="#tab_2">2</ref>. In summary, our analysis shows that supporting efficient computation of partial aggregates like count, sum is important for achieving good performance.</p><p>Optimization within a batch. In order to support aggregates efficiently, batching the computation of aggregates can provide significant performance benefits. These benefits come from two sources: using vectorized operations on CPUs <ref type="bibr" target="#b9">[10]</ref> and by minimizing network traffic from partial merge operations <ref type="bibr" target="#b7">[8]</ref>. For example to compute a count of how many ad-clicks were generated for each publisher, we can aggregate the counts per publisher on each machine during a map and combine them during the reduce phase. We incorporate optimizations within a batch in Drizzle and measure the benefits from this in Section §5.4.</p><p>Optimization across batches and queries. Using Drizzle's architecture also enables optimizations across batches. This is typically useful in case the query plan needs to be changed due to changes in the data distribution. To enable such optimizations, during every micro-batch, a number of metrics about the execution are collected. These metrics are aggregated at the end of a group and passed on to a query optimizer <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b46">47]</ref> to determine if an alternate query plan would perform better. Finally, a micro-batch based architecture also enables reuse of intermediate results across streaming queries. This could be useful in scenarios where a number of streaming queries are run on the same dataset and we plan to investigate the benefits from this in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Discussion</head><p>We next discuss other design approaches to group scheduling and extensions to pre-scheduling that can further improve performance. Other design approaches. An alternative design approach we considered was to treat the existing scheduler in BSP systems as a black-box and pipeline scheduling of one microbatch with task execution of the previous micro-batch. That is, while the first micro-batch executes, the centralized driver schedules one or more of the succeeding micro-batches. With pipelined scheduling, if the execution time for a micro-batch is t exec and scheduling overhead is t sched , then the overall running time for b micro-batches is b ⇥ maxt exec ,t sched . The baseline would take b ⇥ t exec + t sched . We found that this approach is insufficient for larger cluster sizes, where the value of t sched can be greater than t exec .</p><p>Another design approach we considered was to model task scheduling as leases <ref type="bibr" target="#b29">[30]</ref> that could be revoked if the centralized scheduler wished to make any changes to the execution plan. By adjusting the lease duration we can similarly control the coordination overheads. However using this approach would require reimplementing the task-based execution model used by BSP-style systems and we found that group scheduling offered similar behavior while providing easier integration with existing systems.</p><p>Improving Pre-Scheduling. While using pre-scheduling in Drizzle, the reduce tasks usually need to wait for a notification from all upstream map tasks before starting execution. We can reduce the number of inputs a task waits for if we have sufficient semantic information to determine the communication structure for a stage. For example, if we are aggregating information using binary tree reduction, each reduce task only requires the output from two map tasks run in the previous stage. In general inferring the communication structure of the DAG that is going to be generated is a hard problem because of user-defined map and hash partitioning functions. For some high-level operators like treeReduce or broadcast the DAG structure is predictable. We have implemented support for using the DAG structure for treeReduce in Drizzle and plan to investigate other operations in the future.</p><p>Quality of Scheduling. Using group and pre-scheduling requires some minor modifications to the scheduling algorithm used by the underlying systems. The main effect this introduces for scheduling quality is that the scheduler's decision algorithm is only executed once for each group of tasks. This coarser scheduling granularity can impact algorithms like fair sharing, but this impact is bounded by the size of a group, which in our experience is limited to a few seconds at most. Further, while using pre-scheduling, the scheduler is unaware of the size of the data produced from the upstream tasks and thus techniques to optimize data transfer <ref type="bibr" target="#b18">[19]</ref> cannot be applied. Similarly database-style optimizations that perform dynamic rebalancing <ref type="bibr" target="#b38">[39]</ref> of tasks usually depend on data statistics and cannot be used within a group. However for streaming applications we see that the data characteristics change over the course of seconds to minutes and thus we can still apply these techniques using previously collected data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IMPLEMENTATION</head><p>We have implemented Drizzle by extending Apache Spark 2.0.0. Our implementation required around 4000 lines of Scala code changes to the Spark scheduler and execution engine. We next describe some of the additional performance improvements we made in our implementation and also describe how we integrated Drizzle with Spark Streaming. Spark Improvements. The existing Spark scheduler implementation uses two threads: one to compute the stage dependencies and locality preferences, and the other to manage task queuing, serializing, and launching tasks on executors. We observed that when several stages are pre-scheduled together, task serialization and launch is often a bottleneck. In our implementation we separated serializing and launching tasks to a dedicated thread and optionally use multiple threads if there are many stages that can be scheduled in parallel. We also optimized locality lookup for pre-scheduled tasks and these optimizations help reduce the overheads when scheduling many stages in advance. However there are other sources of performance improvements we have not yet implemented in Drizzle. For example, while iterative jobs often share the same closure across iterations we do not amortize the closure serialization across iterations. This requires analysis of the Scala byte code that is part of the closure and we plan to explore this in the future. Spark Streaming. The Spark Streaming architecture consists of a JobGenerator that creates a Spark RDD and a closure that operates on the RDD when processing a microbatch. Every micro-batch in Spark Streaming is associated with an execution timestamp and therefore each generated RDD has an associated timestamp. In Drizzle, we extend the JobGenerator to submit a number of RDDs at the same time, where each generated RDD has its appropriate timestamp. For example, when Drizzle is configured to use group size of 3, and the starting timestamp is t, we would generate RDDs with timestamps t, t + 1 and t + 2. One of the challenges in this case lies in how to handle input sources like Kafka <ref type="bibr" target="#b39">[40]</ref>, HDFS etc. In the existing Spark architecture, the metadata management of which keys or files to process in a micro-batch is done by the centralized driver. To handle this without centralized coordination, we modified the input sources to compute the metadata on the workers as a part of the tasks that read input. Finally, we note these changes are restricted to the Spark Streaming implementation and user applications do not need modification to work with Drizzle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head><p>We next evaluate the performance of Drizzle. First, we use a series of microbenchmarks to measure the scheduling performance of Drizzle and breakdown the time taken at each step in the scheduler. Next we measure the impact of using Drizzle with an industrial streaming benchmark <ref type="bibr" target="#b62">[63]</ref>. We compare Drizzle to Apache Spark, a BSP style-system and Apache Flink, a continuous operator stream processing system. Finally, we evaluate the adaptability of Drizzle to a number of scenarios including failures, elastic scaling and also evaluate the efficacy of our group size tuning algorithm.</p><p>Our evaluation shows that</p><p>• On microbenchmarks, we find that group scheduling and pre-scheduling are effective at reducing coordination overheads by up to 5.5x. • Using optimizations within a micro-batch, Drizzle is able to achieve less than 100ms latency on the Yahoo Streaming benchmark and is up to 3x faster than Flink and Spark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Drizzle recovers from failures around 4x faster than</head><p>Flink and has up to 13x lower latency during recovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>We ran our experiments on 128 r3.xlarge instances in Amazon EC2. Each machine has 4 cores, 30.5 GB of memory and 80 GB of SSD storage. We configured Drizzle to use 4 slots for executing tasks on each machine. For all our experiments we warm up the JVM before taking measurements. We use Apache Spark v2.0.0 and Apache Flink v1.1.1 as baselines for our experiments. All three systems are run on the JVM and in our evaluation we use the same JVM heap size and garbage collection flags across all of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Micro benchmarks</head><p>In this section we present micro-benchmarks that evaluate the benefits of group scheduling and pre-scheduling. We run ten trials for each of our micro-benchmarks and report the median, 5th and 95th percentiles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Group Scheduling.</head><p>We first evaluate the benefits of group scheduling in Drizzle, and its impact in reducing scheduling overheads with growing cluster size. We use a weak scaling experiment where the amount of computation per task is fixed but the size of the cluster (and hence number of tasks) grow. For this experiment, we set the number of tasks to be equal to the number of cores in the cluster. In an ideal parallel system the total time taken should remain constant. We use a simple workload where each task computes the sum of random numbers and the computation time for each micro-batch is less than 1ms. Note that there are no shuffle operations in this benchmark. We measure the average time taken per micro-batch while running 100 micro-batches and we scale the cluster size from 4-128 machines.</p><p>As discussed in §2, we see that (Figure <ref type="figure" target="#fig_5">4</ref>(a)) in Spark task scheduling overheads grow as we increase cluster and are around 200ms when using 128 machines. Drizzle is able to  Machines (a) Time taken for a single stage job with 100x more data while running 100 micro-batches and varying the group size. We see that with group size greater than 25, the benefits diminish.  amortize these overheads leading to a 7 46⇥ speedup across cluster sizes. With a group size of 100 and a cluster of 128 machines, Drizzle's scheduler overheads are less than 5ms compared to around 195ms for Spark.</p><p>We provide a breakdown of the source of these improvements in Figure <ref type="figure" target="#fig_5">4(b)</ref>. We do so by analyzing the average time taken by each task for scheduling, task transfer (including serialization, deserialization, and network transfer of the task) and computation. We find that when this benchmark is run on Spark, scheduling and task transfer times dominate, while Drizzle is able to amortize both of these operations using group scheduling.</p><p>However the benefits for a workload also depends on the amount of computation being performed in each iteration. To measure this effect we increased the amount of data in each partition by 100x and results are shown in Figure <ref type="figure" target="#fig_7">5</ref>(a). In this case, using a group size of 25 captures most of the benefits and as the running time is dominated by computation we see no additional benefits from larger group sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Pre-Scheduling Shuffles.</head><p>To measure benefits from pre-scheduling we use the same setup as in the previous subsection but add a shuffle stage to every micro-batch with 16 reduce tasks. We compare the time taken per microbatch while running 100 micro-batches in Figure <ref type="figure" target="#fig_7">5(b)</ref>. Drizzle achieves between 2.7x to 5.5x speedup over Spark as we vary cluster size.</p><p>Figure <ref type="figure" target="#fig_7">5</ref>(b) also shows the benefits of just using prescheduling (i.e., group size = 1). In this case, we still have barriers between the micro-batches and only eliminate barriers within a single micro-batch. We see that the benefits from just pre-scheduling are limited to only 20ms when using 128 machines. However for group scheduling to be effective we need to pre-schedule all of the tasks in the DAG and thus pre-scheduling is essential.  Finally, we see that the time per micro-batch of the twostage job (45ms for 128 machines) is significantly higher than the time per micro-batch of the one-stage job in the previous section (5 ms). While part of this overhead is from messages used to trigger the reduce tasks, we also observe that the time to fetch and process the shuffle data in the reduce task grows as the number of map tasks increase. To reduce this overhead, we plan to investigate techniques that reduce connection initialization time and improve data serialization/deserialization <ref type="bibr" target="#b43">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Streaming workloads</head><p>We demonstrate Drizzle's benefits for streaming applications using the Yahoo! streaming benchmark <ref type="bibr" target="#b62">[63]</ref> which mimics running analytics on a stream of ad-impressions. A producer inserts JSON records into a stream. The benchmark then groups events into 10-second windows per ad-campaign and measures how long it takes for all events in the window to be processed after the window has ended. For example if a window ended at time a and the last event from the window was processed at time b, the processing latency for this window is said to be b a.</p><p>When implemented using the micro-batch model in Spark and Drizzle, this workload consists of two stages per microbatch: a map-stage that reads the input JSONs and buckets events into a window and a reduce stage that aggregates events in the same window. For the Flink implementation we use the optimized version <ref type="bibr" target="#b20">[21]</ref> which creates windows in Flink using the event timestamp that is present in the JSON. Similar to the micro-batch model we have two operators here, a map operator that parses events and a window operator that collects events from the same window and triggers an update every 10 seconds.</p><p>For our evaluation, we created an input stream that inserts JSON events and measure the event processing latency. We use the first five minutes to warm up the system and report results from the next five minutes of execution. We tuned each system to minimize latency while meeting throughput requirements. In Spark this required tuning the micro-batch size, while in Flink we tuned the buffer flush duration.</p><p>Latency. The CDF of processing latencies for 20M events/second on 128 machines is shown in Figure <ref type="figure">6</ref>(a). We see Drizzle is able to achieve a median latency of around 350ms and matches the latency achieved by Flink, a continuous operator streaming system. We also verified that the latency we get from Flink match previously reported numbers <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b62">63]</ref> on the same benchmark. We also see that by reducing scheduling overheads, Drizzle gets around 3.6x lower median latency than Spark.</p><p>We next analyze how latency improvements change across workloads. To do this we run a video streaming analytics benchmark which processes heartbeats sent by video streaming clients. The heartbeats are structured as JSON events and contain metadata about the clients, and the type of event being recorded. The heartbeats are grouped together by their session identifier and, and the application uses all the heartbeats for a session, to updates the session summary for each session. This session summary is used by other applications to power dashboards and CDN predictions as described in Section 2.</p><p>We show a comparison of Drizzle on both the Yahoo benchmark and the video streaming benchmark in Figure <ref type="figure">9</ref>. From the figure we see that Drizzle has a similar median latency of around 400ms for this workload but that the 95th percentile latency goes to around 780ms from 480ms. The main reason  for this is that the heartbeats used in video analytics are bigger in size when compared to the ad-campaign events in the Yahoo benchmark and hence the workload involves more data being shuffled. Further the workload also has some inherent skew where some sessions have more events when compared to others. This workload skew results in the 95th percentile latency being much higher than the Yahoo benchmark.</p><p>Throughput. We next compare the maximum throughput that can be achieved given a latency target. We use the Yahoo Streaming benchmark for this and for Spark and Drizzle we set the latency target by adjusting the micro-batch size and measure the maximum throughput that can be sustained in a stable fashion. For continuous operator systems like Flink there is no direct way to configure a latency target. Thus, we measure the latency achieved as we increase the throughput and report the maximum throughput achieved within our latency bound. The results from this experiment are shown in Figure <ref type="figure">6(b)</ref>. From the figure we can see that while Spark crashes at very low latency target of 250ms, Drizzle and Flink both get around 20M events/s. At higher latency targets we find that Drizzle gets 1.5-3x more throughput than Spark and that this number reduces as the latency target grows. This is because the overheads from scheduling become less important at higher latency targets and thus the benefits from Drizzle become less relevant.</p><p>Fault Tolerance. We use the same Yahoo streaming benchmark as above and benchmark the fault tolerance properties of all three systems. In this experiment we kill one machine in the cluster after 240 seconds and measure the event processing latency as before. Figure <ref type="figure">7</ref> shows the latency measured for each window across time. We plot the mean and standard deviation from five runs for each system. We see that using the micro-batch model in Spark has good relative adaptivity where the latency during failure is around 3x normal processing latency and that only 1 window is affected. Drizzle has similar behavior where the latency during failure increases from around 350ms to around 1s. Similar to Spark, Drizzle's latency also returns to normal after one window.</p><p>On the other hand Flink experiences severe slowdown during failure and the latency spikes to around 18s. Most of this slow down is due to the additional coordination required to stop and restart all the operators in the topology and to restore execution from the latest checkpoint. We also see that having such a huge delay means that the system is unable to catch up with the input stream and that it takes around 40s (or 4 windows) before latency returns to normal range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Micro-batch Optimizations</head><p>As discussed in §3.5, one of the advantages of using the micro-batch model for streaming is that it enables additional optimizations to be used within a batch. In the case of the Yahoo streaming benchmark, as the output only requires the number of events in a window we can reduce the amount of data shuffled by aggregating counters for each window on the map side. We implemented this optimization in Drizzle and Spark by using the reduceBy operator instead of the groupBy operator and study the latency, throughput improvements bought about by this change.</p><p>Latency. Figure <ref type="figure">8</ref>(a) shows the CDF of the processing latency when the optimization is enabled. Since Flink creates windows after the keys are partitioned, we could not directly apply the same optimization. In this case we see that using  optimization leads to Drizzle getting around 94ms median latency and is 2x faster than Spark and 3x faster than Flink.</p><p>Throughput. Similarly we again measure the maximum throughput that can be achieved given a latency target in Figure <ref type="figure">8(b)</ref>. We see that using optimization within a batch can lead to significant wins in throughput as well. Drizzle is able to sustain around 35M events/second with a 100ms latency target, a target that both Spark and Flink are unable to meet for different reasons: Spark due to scheduling overheads and Flink due to lack of batch optimizations. Similar to the previous comparison we see that the benefits from Drizzle become less pronounced at larger latency targets and that given a 1s target both Spark and Drizzle achieve similar throughput of 100M events/second. We use the optimized version for Drizzle and Spark in the following sections of the evaluation.</p><p>In summary, we find that by combining the batch-oriented data processing with the coarse grained scheduling in Drizzle we are able to achieve better performance than existing BSP systems like Spark and continuous operator systems like Flink. We also see that Drizzle also recovers faster from failures when compared to Flink and maintains low latency during recovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Adaptivity in Drizzle</head><p>We next evaluate the importance of group size in Drizzle and specifically how it affects adapativity in terms of fault tolerance and elasticity. Following that we show how our auto-tuning algorithm can find the optimal group size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Fault tolerance with Group Scheduling.</head><p>To measure the importance of group size for fault recovery in Drizzle, we use the same Yahoo workload as the previous section and we vary the group size. In this experiment we create checkpoints at the end of every group. We measure processing latency across windows and the median processing latency from five runs is shown in Figure <ref type="figure" target="#fig_13">10(a)</ref>.</p><p>We can see that using a larger group size can lead to higher latencies and longer recovery times. This is primarily because of two reasons. First, since we only create checkpoints at the end of every group having a larger group size means that more records would need to be replayed. Further, when machines fail pre-scheduled tasks need to be updated in order to reflect the new task locations and this process takes longer when there are larger number of tasks. In the future we plan to investigate if checkpoint intervals can be decoupled from group and better treatment of failures in pre-scheduling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Handling Elasticity.</head><p>To measure how Drizzle enables elasticity we consider a scenario where we start with the Yahoo Streaming benchmark but only use 64 machines in the cluster. We add 64 machines to the cluster after 4 minutes and measure how long it takes for the system to react and use the new machines. To measure elasticity we monitor the average latency to execute a micro-batch and results from varying the group size are shown in Figure <ref type="figure" target="#fig_13">10(b)</ref>.</p><p>We see that using a larger group size can delay the time taken to adapt to cluster changes. In this case, using a group size of 50 the latency drops from 150ms to 100ms within 10 seconds. On the other hand, using group size of 600 takes 100 seconds to react. Finally, as seen in the figure, elasticity can also lead to some performance degradation when the new machines are first used. This is because some of the input data needs to be moved from machines that were being used to the new machines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.3">Group Size</head><p>Tuning. To evaluate our group size tuning algorithm, we use the same Yahoo streaming benchmark but change the micro-batch size. Intuitively the scheduling overheads are inversely proportional to the micro-batch size, as small micro-batches imply there are more tasks to schedule. We run the experiment with the scheduling overhead target of 5% to 10% and start with a group size of 2. The progress of the tuning algorithm is shown in Figure <ref type="figure" target="#fig_15">11</ref> for micro-batch size of 100ms and 250ms.</p><p>We see that for a smaller micro-batch the overheads are high initially with the small group size and hence the algorithm increases the group size to 64. Following that as the overhead goes below 5% the group size is additively decreased to 49. In the case of the 250ms micro-batch we see  that a group size of 8 is good enough to maintain a low scheduling overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Stream Processing. Stream processing on a single machine has been studied by database systems including Aurora <ref type="bibr" target="#b0">[1]</ref> and TelegraphCQ <ref type="bibr" target="#b16">[17]</ref>. As the input event rate for many data streams is high, recent work has looked at large-scale, distributed stream processing frameworks, e.g., Spark Streaming <ref type="bibr" target="#b66">[67]</ref>, Storm <ref type="bibr" target="#b56">[57]</ref>, Flink <ref type="bibr" target="#b53">[54]</ref>, Google Dataflow <ref type="bibr" target="#b2">[3]</ref>, etc., to scale stream processing across a cluster. In addition to distributed checkpointing, recent work on StreamScope <ref type="bibr" target="#b41">[42]</ref> has proposed using reliable vertices and reliable channels for fault tolerance. In Drizzle, we focus on re-using existing fault tolerance semantics from BSP systems and improving performance to achieve low latency.</p><p>BSP Performance. Recent work including Nimbus <ref type="bibr" target="#b42">[43]</ref> and Thrill <ref type="bibr" target="#b8">[9]</ref> has focused on implementing high-performance BSP systems. Both systems claim that the choice of runtime (i.e., JVM) has a major effect on performance, and choose to implement their execution engines in C++. Furthermore, Nimbus similar to our work finds that the scheduler is a bottleneck for iterative jobs and uses scheduling templates. However, during execution Nimbus uses mutable state and focuses on HPC applications while we focus on improving adaptivity by using deterministic micro-batches for streaming jobs in Drizzle. On the other hand Thrill focuses on query optimization in the data plane; our work is therefore orthogonal to Thrill.</p><p>Cluster Scheduling. Systems like Borg <ref type="bibr" target="#b60">[61]</ref>, YARN <ref type="bibr" target="#b6">[7]</ref> and Mesos <ref type="bibr" target="#b31">[32]</ref> schedule jobs from different frameworks and implement resource sharing policies <ref type="bibr" target="#b27">[28]</ref>. Prior work <ref type="bibr" target="#b50">[51]</ref> has also identified the benefits of shorter task durations and this has led to the development of distributed job schedulers such as Sparrow <ref type="bibr" target="#b51">[52]</ref>, Apollo <ref type="bibr" target="#b10">[11]</ref>, etc. These frameworks assume that the jobs are independent of each other and hence perform distributed scheduling across jobs. In Drizzle, we target microbatch streaming jobs where there are dependencies between batches and thus a single streaming job cannot be easily split across distributed schedulers.</p><p>To improve performance within a job, techniques for improving data locality <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b65">66]</ref>, mitigating stragglers <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b61">62]</ref>, re-optimizing queries <ref type="bibr" target="#b38">[39]</ref> and accelerating network transfers <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b30">31]</ref> have been proposed. In Drizzle we study how we can use these techniques for streaming jobs without incurring significant overheads. Prior work <ref type="bibr" target="#b59">[60]</ref> has also looked at the benefits of removing the barrier across shuffle stages to improve performance. In addition to removing the barrier, pre-scheduling in Drizzle also helps remove the centralized co-ordination for data transfers. Finally, Ciel <ref type="bibr" target="#b48">[49]</ref> proposed a data flow framework that can distribute execution based on data-dependent decisions. In contrast, we exploit the predictable structure of streaming queries in Drizzle to reduce scheduling overheads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND FUTURE WORK</head><p>In this paper we presented techniques to optimize scheduling and other control plane operations for stream processing workloads, and showed that these techniques enable low latency, high throughput and adaptability. In the future we plan to investigate additional techniques that can be used to improve the performance of the data plane including the use of compute accelerators or specialized network hardware <ref type="bibr" target="#b24">[25]</ref> for low latency RPCs. We also plan to extend Drizzle and integrate it with other execution engines (e.g., Impala, Greenplum) and thus develop an architecture for general purpose low latency scheduling.</p><p>While we have focused on stream processing in this paper, other workloads including iterative machine learning algorithms <ref type="bibr" target="#b52">[53]</ref> can also also benefit from the control plane optimizations in Drizzle. In the future we plan to study how we can support low latency iterative algorithms and how parameter servers can be effectively integrated.</p><p>Big data stream processing systems have positioned themselves as either BSP or continuous operators. In Drizzle, we present a new design that decouples the data processing from the fault tolerance, adaptability and show that we can develop a streaming system that combines the best features from both models. This allows Drizzle to achieve high throughput with very low latency not only during normal operation, but also during adaptation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( 3 )Figure 1 :</head><label>31</label><figDesc>Figure 1: Execution of a streaming job when using the batch processing model. We show two micro-batches of execution here. The left-hand side shows the various steps used to coordinate execution. The query being executed in shown on the right hand side</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Group scheduling amortizes the scheduling overheads across multiple micro-batches of a streaming job.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Using pre-scheduling, execution of a micro-batch that has two stages: the first with 4 tasks; the next with 2 tasks. The driver launches all stages at the beginning (with information about where output data should be sent to) so that executors can exchange data without contacting the driver.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Time taken for a single stage job with 100 micro-batches while varying the group size. We see that with group size of 100, Drizzle takes less than 5ms per micro-batch. Breakdown of time taken by a task in the single stage microbenchmark when using 128 machines. We see that Drizzle can lower the scheduling overheads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Micro-benchmarks for performance improvements from group scheduling</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Time taken per micro-batch for a streaming job with shuffles. We break down the gains between pre-scheduling and group scheduling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Micro-benchmarks for performance improvements from group scheduling and pre-scheduling</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>CDF of event processing latencies when using groupBy operations in the micro-batch model. Drizzle matches the latencies of Flink and is around 3.6x faster than Spark.(b) Maximum throughput achievable at a given latency target by Drizzle, Spark and Flink. Spark is unable to sustain latency of 250ms while Drizzle and Flink achieve similar throughput.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Latency and throughput comparison of Drizzle with Spark and Flink on the Yahoo Streaming benchmark.</figDesc><graphic coords="11,314.58,96.11,237.07,85.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>CDF of event processing latencies when using micro-batch optimizations with the Yahoo Streaming Benchmark. Drizzle is 2x faster than Spark and 3x faster than Flink. (b) Maximum throughput achievable at a given latency target by Drizzle, Spark and Flink when using micro-batch optimizations. Spark and Flink fail to meet the 100ms latency target and Drizzle's throughput increases by 2-3x by optimizing execution within a micro-batch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Figure 8: Effect of micro-batch optimization in Drizzle in terms of latency and throughput.</figDesc><graphic coords="12,312.09,92.01,237.07,89.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Event Latency for the Yahoo Streaming Benchmark when handling failures. At 240 seconds, we kill one machine in the cluster and we see that having a smaller group allows us to react faster to this. Average time taken per micro-batch as we change the number of machines used. At 240 seconds, we increase the number of machines from 64 to 128. Having a smaller group allows us to react faster to this.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Effect of varying group size in Drizzle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Behavior of group size auto tuning with the YahooStreaming benchmark when using two different micro-batch sizes. The optimal group size is lower for the larger microbatch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>extend the MapReduce model to allow combining many phases of map and reduce after each other,</figDesc><table><row><cell>Property</cell><cell>Micro-batch Model</cell><cell>Continuous Operator Model</cell></row><row><cell>Latency</cell><cell>seconds</cell><cell>milliseconds</cell></row><row><cell>Consistency</cell><cell>exactly-once, prefix integrity [22]</cell><cell>exactly once [13]</cell></row><row><cell cols="3">Fault Recovery sync checkpoints, parallel recovery sync / async checkpoints, replay</cell></row><row><cell>Adaptability</cell><cell>at micro-batch boundaries</cell><cell>checkpoint and restart</cell></row><row><cell>Table</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Breakdown of aggregations used in a workload containing over 900,000 SQL and streaming queries.</figDesc><table><row><cell>Aggregate</cell><cell>Percentage of Queries</cell></row><row><cell>Count</cell><cell>60.55</cell></row><row><cell>First/Last</cell><cell>25.9</cell></row><row><cell>Sum/Min/Max</cell><cell>8.640</cell></row><row><cell>User Defined Function</cell><cell>0.002</cell></row><row><cell>Other</cell><cell>4.908</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: We would like to thank the anonymous reviewers and our shepherd, Marco Serafini, for their insightful comments that improved this paper. We would also like to thank Ganesh Ananthanarayanan, Evan Sparks and Matei Zaharia for their feedback on earlier drafts of this paper. We also thank Jibin Zhan and Yan Li for discussions on video analytics applications. This research is supported in part by DHS Award HSHQDC-16-3-00083, NSF CISE Expeditions Award CCF-1139158, and gifts from Ant Financial, Amazon Web Services, CapitalOne, Ericsson, GE, Google, Huawei, Intel, IBM, Microsoft and VMware. BR is generously supported by ONR award N00014-17-1-2401, NSF award CCF-1359814, ONR awards N00014-14-1-0024 and N00014-17-1-2191, a Sloan Research Fellowship, a Google Faculty Award, and research grants from Amazon.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Aurora: a new model and architecture for data stream management</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Carney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Çetintemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cherniack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Convey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stonebraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tatbul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zdonik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fault-tolerant stream processing at internet scale</title>
		<author>
			<persName><forename type="first">T</forename><surname>Akidau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Balikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bekiroglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chernyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Haberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mcveety</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nordstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Whittle</surname></persName>
		</author>
		<author>
			<persName><surname>Millwheel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="734" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Akidau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bradshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chernyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Fernã Ą N D E Z -Moctezuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mcveety</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Whittle</surname></persName>
		</author>
		<title level="m">The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1792" to="1803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Effective straggler mitigation: Attack of the clones</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sto-Ica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pacman: Coordinated memory caching for parallel jobs</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borthakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reining in the outliers in Map-Reduce clusters using Mantri</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sto-Ica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Apache</forename><surname>Hadoop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nextgen</forename><surname>Mapreduce</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spark SQL: Relational data processing in Spark</title>
		<author>
			<persName><forename type="first">M</forename><surname>Armbrust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kaftan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Thrill: High-performance algorithmic distributed batch data processing with c++</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bingmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Axtmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jöbstl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Noe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schlag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stumpp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sanders</surname></persName>
		</author>
		<idno>CoRR abs/1608.05634</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hyperpipelining query execution</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Boncz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zukowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nes</surname></persName>
		</author>
		<author>
			<persName><surname>Monetdb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="225" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Apollo: scalable and coordinated scheduling for cloud-scale computing</title>
		<author>
			<persName><forename type="first">E</forename><surname>Boutin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ekanayake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End to End congestion avoidance on a global Internet</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Brakmo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><surname>Vegas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Selected Areas in Communications</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1465" to="1480" />
			<date type="published" when="1995-10">Oct. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Lightweight asynchronous snapshots for distributed dataflows</title>
		<author>
			<persName><forename type="first">P</forename><surname>Carbone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fóra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ewen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Haridi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tzoumas</surname></persName>
		</author>
		<idno>CoRR abs/1506.08603</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Apache Flink: Stream and Batch Processing in a Single Engine</title>
		<author>
			<persName><forename type="first">P</forename><surname>Carbone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Katsifodimos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ewen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Markl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Haridi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tzoumas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Engineering Bulletin</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SCOPE: easy and efficient parallel processing of massive data sets</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chaiken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Å</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shakib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB</title>
		<imprint>
			<biblScope unit="page" from="1265" to="1276" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">FlumeJava: Easy, Efficient Data-Parallel Pipelines</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raniwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bradshaw</surname></persName>
		</author>
		<author>
			<persName><surname>Nathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PLDI</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">TelegraphCQ: continuous dataflow processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In SIGMOD</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed snapshots: determining global states of distributed systems</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Chandy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lamport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems (TOCS)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="63" to="75" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Managing data transfers in computer clusters with orchestra</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adaptive stream processing using dynamic batch sizing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOCC</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<ptr target="http://data-artisans.com/extending-the-yahoo-streaming-benchmark" />
		<title level="m">Extending the Yahoo! Streaming Benchmark</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<ptr target="https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html" />
		<title level="m">Structured Streaming In Apache Spark: A new high-level API for streaming</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Kafka Tops 1 Trillion Messages Per Day at LinkedIn</title>
		<author>
			<persName><surname>Datanami</surname></persName>
		</author>
		<ptr target="https://goo.gl/cY7VOz" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Simplified Data Processing on Large Clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><surname>Mapreduce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">No Compromises: Distributed Transactions with Consistency, Availability, and Performance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dragojevi Ć</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Nightingale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ren-Zelmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Badam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Castro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dhalion: self-regulating stream processing in heron</title>
		<author>
			<persName><forename type="first">A</forename><surname>Floratou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ra-Masamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1825" to="1836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Availability in globally distributed storage systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Labelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">I</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stokely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V.-A</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="61" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dominant resource fairness: Fair allocation of multiple resource types</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hindman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Konwinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Encapsulation of parallelism in the volcano query processing system</title>
		<author>
			<persName><forename type="first">G</forename><surname>Graefe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="102" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Leases: An efficient fault-tolerant mechanism for distributed file cache consistency</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cheriton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="202" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Queues don&apos;t matter when you can jump them! In NSDI</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Grosvenor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schwarzkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N M</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Crowcroft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mesos: A platform for fine-grained resource sharing in the data center</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hindman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Konwinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Falkirk wheel: Rollback recovery for dataflow systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.08877</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dryad: distributed data-parallel programs from sequential building blocks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Budiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Birrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fetterly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurosys</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fair scheduling for distributed computing clusters</title>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Currey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Wieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><surname>Quincy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Congestion avoidance and control</title>
		<author>
			<persName><forename type="first">V</forename><surname>Jacobson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="314" to="329" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">CFA: A Practical Prediction System for Video QoE Optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Milner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shepherd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="137" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Advances in dataflow programming languages</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Millar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Optimus: a dynamic rewriting framework for data-parallel execution plans</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurosys</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="15" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Kafka: A distributed messaging system for log processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kreps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Narkhede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Twitter heron: Stream processing at scale</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kedigehalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kellogg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ramasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Taneja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="239" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Streamscope: continuous reliable distributed processing of big data streams</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="439" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">O</forename><surname>Mashayekhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Levis</surname></persName>
		</author>
		<author>
			<persName><surname>Scalable</surname></persName>
		</author>
		<idno>CoRR abs/1606.01972</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>fast cloud computing with execution templates</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scalability! but at what cost?</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th Workshop on Hot Topics in Operating Systems (HotOS XV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Power management of online data-intensive services</title>
		<author>
			<persName><forename type="first">D</forename><surname>Meisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Sadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-D</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Stream</forename><surname>Sla</surname></persName>
		</author>
		<author>
			<persName><surname>Analytics</surname></persName>
		</author>
		<ptr target="https://azure.microsoft.com/en-us/support/legal/sla/stream-analytics/v1_0/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Query processing, resource management, and approximation in a data stream management system</title>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Widom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Babcock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Manku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rosenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDR</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Naiad: a timely dataflow system</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Isaacs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="439" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Ciel: a universal execution engine for distributed data-flow computing</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schwarzkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Smowton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madhavapeddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="113" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<ptr target="http://techblog.netflix.com/2016/03/stream-processing-with-mantis.html" />
		<title level="m">Stream-processing with Mantis</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The case for tiny tasks in compute clusters</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HotOS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Sparrow: distributed, low latency scheduling</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wendell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">HOGWILD!: A lockfree approach to parallelizing stochastic gradient descent</title>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="693" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">All roads lead to rome: optimistic recovery for distributed iterative data processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schelter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ewen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tzoumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Markl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Access path selection in a relational database management system</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Selinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Astrahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Chamberlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Lorie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="1979">1979</date>
			<biblScope unit="page" from="23" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The 8 requirements of real-time stream processing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stonebraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Çetintemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zdonik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGMOD Record</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="42" to="47" />
			<date type="published" when="2005-12">Dec. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Taneja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ramasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donham</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Storm at twitter. In SIGMOD</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<ptr target="https://goo.gl/wAHi2I" />
		<title level="m">Twitter: technical overview</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Apache</forename><surname>Spark</surname></persName>
		</author>
		<ptr target="http://goo.gl/FqEh94" />
		<title level="m">Preparing for the Next Wave of Reactive Big Data</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Breaking the mapreduce stage barrier</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="191" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Large-scale cluster management at google with borg</title>
		<author>
			<persName><forename type="first">A</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pedrosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Korupolu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oppenheimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wilkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurosys</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Wrangler: Predictable and faster jobs using fewer resources</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Yadwadkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOCC</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title/>
		<ptr target="https://yahooeng.tumblr.com/post/135321837876" />
	</analytic>
	<monogr>
		<title level="j">Benchmarking Streaming Computation Engines at Yahoo!</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Dryadlinq: A system for general-purpose distributed data-parallel computing using a high-level language</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fetterly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Budiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ú</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gunda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Currey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Delay scheduling: A Simple Technique for Achieving Locality and Fairness in Cluster Scheduling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borthakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Elmeleegy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurosys</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mc-Cauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Discretized streams: Fault-tolerant streaming computation at scale</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The design and implementation of a wireless video surveillance system</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Bahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual International Conference on Mobile Computing and Networking</title>
		<meeting>the 21st Annual International Conference on Mobile Computing and Networking</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="426" to="438" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
