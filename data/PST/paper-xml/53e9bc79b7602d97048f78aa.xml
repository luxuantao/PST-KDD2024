<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Camera Models and Fundamental Concepts Used in Geometric Computer Vision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Peter</forename><surname>Sturm</surname></persName>
							<email>peter.sturm@inrialpes.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">INRIA Grenoble -Rhône-Alpes and Laboratoire Jean Kuntzmann</orgName>
								<address>
									<settlement>Grenoble, Montbonnot</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Srikumar</forename><surname>Ramalingam</surname></persName>
							<email>ramalingam@merl.com</email>
							<affiliation key="aff1">
								<orgName type="institution">MERL</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jean-Philippe</forename><surname>Tardif</surname></persName>
							<email>tardifj@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">NREC -Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Simone</forename><surname>Gasparini</surname></persName>
							<email>simone.gasparini@inrialpes.fr</email>
							<affiliation key="aff3">
								<orgName type="institution">INRIA Grenoble -Rhône-Alpes and Laboratoire Jean Kuntzmann</orgName>
								<address>
									<settlement>Grenoble, Montbonnot</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">João</forename><surname>Barreto</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Coimbra University</orgName>
								<address>
									<settlement>Coimbra</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Camera Models and Fundamental Concepts Used in Geometric Computer Vision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C7CD05E02D5FF32F5D78D416859E2EC3</idno>
					<idno type="DOI">10.1561/0600000023</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This survey is mainly motivated by the increased availability and use of panoramic image acquisition devices, in computer vision and various of its applications. Different technologies and different computational models thereof exist and algorithms and theoretical studies for geometric computer vision ("structure-from-motion") are often re-developed without highlighting common underlying principles. One of the goals of this survey is to give an overview of image acquisition methods used in computer vision and especially, of the vast number of camera models that have been proposed and investigated over the years, where we try to point out similarities between different models. Results on epipolar and multi-view geometry for different camera models are reviewed as well as various calibration and self-calibration approaches, with an emphasis on non-perspective cameras. We finally describe what we consider are fundamental building blocks for geometric computer vision or structure-from-motion: epipolar geometry, pose and motion estimation, 3D scene modeling, and bundle adjustment. The main goal here is to highlight the main principles of these, which are independent of specific camera models.</p><p>1 It may be possible to achieve a central catadioptric system using a non-central camera looking at a mirror which is not necessarily conic-based; all that matters is that the rays that are back-projected from the non-central camera converge to a single point after reflection in the mirror. For any non-central camera and a desired effective viewpoint, one may be able to design an appropriate mirror.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Background Material 1.Introduction</head><p>Many different image acquisition technologies have been investigated in computer vision and other areas, many of them aiming at providing a wide field of view. The main technologies consist of catadioptric and fisheye cameras as well as acquisition systems with moving parts, e.g., moving cameras or optical elements. In this monograph, we try to give an overview of the vast literature on these technologies and on computational models for cameras. Whenever possible, we try to point out links between different models. Simply put, a computational model for a camera, at least for its geometric part, tells how to project 3D entities (points, lines, etc.) onto the image, and vice versa, how to back-project from the image to 3D. Camera models may be classified according to different criteria, for example the assumption or not of a single viewpoint or their algebraic nature and complexity. Also, recently several approaches for calibrating and using "non-parametric" camera models have been proposed by various researchers, as opposed to classical, parametric models.</p><p>In this survey, we propose a different nomenclature as our main criterion for grouping camera models. The main reason is that even so-called non-parametrics models do have parameters, e.g., the coordinates of camera rays. We thus prefer to speak of three categories: (i) A global camera model is defined by a set of parameters such that changing the value of any parameter affects the projection function all across the field of view. This is the case for example with the classical pinhole model and with most models proposed for fisheye or catadioptric cameras. (ii) A local camera model is defined by a set of parameters, each of which influences the projection function only over a subset of the field of view. A hypothetical example, just for illustration, would be a model that is "piecewise-pinhole", defined over a tessellation of the image area or the field of view. Other examples are described in this monograph. (iii) A discrete camera model has sets of parameters for individual image points or pixels. To work with such a model, one usually needs some interpolation scheme since such parameter sets can only be considered for finitely many image points. Strictly speaking, discrete models plus an interpolation scheme are thus not different from the above local camera models, since model parameters effectively influence the projection function over regions as opposed to individual points. We nevertheless preserve the distinction between discrete and local models, since in the case of discrete models, the considered regions are extremely small and since the underlying philosophies are somewhat different for the two classes of models.</p><p>These three types of models are illustrated in Figure <ref type="figure">1</ref>.1, where the camera is shown as a black box. As discussed in more detail later in the monograph, we mainly use back-projection to model cameras, i.e., the mapping from image points to camera rays. Figure <ref type="figure">1</ref>.1 illustrates back-projection for global, discrete and local camera models.</p><p>After describing camera models, we review central concepts of geometric computer vision, including camera calibration, epipolar and multi-view geometry, and structure-from-motion tasks, such as pose and motion estimation. These concepts are exhaustively described for perspective cameras in recent textbooks <ref type="bibr" target="#b136">[137,</ref><ref type="bibr" target="#b212">213,</ref><ref type="bibr" target="#b327">328,</ref><ref type="bibr" target="#b335">336,</ref><ref type="bibr" target="#b512">513]</ref>; our emphasis will thus be on non-perspective cameras. We try to describe the various different approaches that have been developed for camera calibration, including calibration using grids or from images of higher level primitives, like lines and spheres, and self-calibration. Throughout Fig. <ref type="figure">1</ref>.1 Types of camera models. Left: For global models, the camera ray associated with an image point q is determined by the position of q and a set of global camera parameters contained in a vector c. Middle: For discrete models, different image regions are endowed with different parameter sets. Right: For discrete models, the camera rays are directly given for sampled image points, e.g., by a look-up table containing Plücker coordinates, here the Plücker coordinates Lq of the ray associated with image point q.</p><p>this monograph, we aim at describing concepts and ideas rather than all details, which may be found in the original references.</p><p>The monograph is structured as follows. In the following section, we give some background material that aims at making the mathematical treatment presented in this monograph, self-contained. In Section 2, we review image acquisition technologies, with an emphasis on omnidirectional systems. Section 3 gives a survey of computational camera models in the computer vision and photogrammetry literature, again emphasizing omnidirectional cameras. Results on epipolar and multi-view geometry for non-perspective cameras are summarized in Section 4. Calibration approaches are explained in Section 5, followed by an overview of some fundamental modules for structurefrom-motion in Section 6. The monograph ends with conclusions, in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Background Material</head><p>Given the large scope of this monograph, we rather propose summaries of concepts and results than detailed descriptions, which would require an entire book. This allows us to keep the mathematical level at a minimum. In the following, we explain the few notations we use in this monograph. We assume that the reader is familiar with basic notions of projective geometry, such as homogeneous coordinates, homographies, etc. and of multi-view geometry for perspective cameras, such as the fundamental and essential matrices and projection matrices. Good overviews of these concepts are given in <ref type="bibr" target="#b136">[137,</ref><ref type="bibr" target="#b212">213,</ref><ref type="bibr" target="#b327">328,</ref><ref type="bibr" target="#b335">336,</ref><ref type="bibr" target="#b512">513]</ref>.</p><p>Fonts. We denote scalars by italics, e.g., s, vectors by bold characters, e.g., t and matrices in sans serif, e.g., A. Unless otherwise stated, we use homogeneous coordinates for points and other geometric entities. Equality between vectors and matrices, up to a scalar factor, is denoted by ∼. The cross-product of two 3-vectors a and b is written as a × b.</p><p>Plücker coordinates for 3D lines. Three-dimensional lines are represented either by two distinct 3D points, or by 6-vectors of so-called Plücker coordinates. We use the following convention. Let A and B be two 3D points, in homogeneous coordinates. The Plücker coordinates of the line spanned by them, are then given as:</p><formula xml:id="formula_0">B 4 Ā -A 4 B Ā × B , (<label>1.1)</label></formula><p>where Ā is the 3-vector consisting of the first three coordinates of A and likewise for B. The action of displacements on Plücker coordinates is as follows. Let t and R be a translation vector and rotation matrix that map points according to:</p><formula xml:id="formula_1">Q → R t 0 T 1 Q. 1.2 Background Material 7</formula><p>Plücker coordinates are then mapped according to:</p><formula xml:id="formula_2">L → R 0 -[t] × R R L,<label>(1.2)</label></formula><p>where 0 is the 3 × 3 matrix composed of zeroes. Two lines cut one another exactly if</p><formula xml:id="formula_3">L T 2 0 3×3 I 3×3 I 3×3 0 3×3 L 1 = 0. (1.3)</formula><p>Lifted coordinates. It is common practice to linearize polynomial expressions by applying Veronese embeddings. We use the informal term "lifting" for this, for its shortness. Concretely, we apply lifting to coordinate vectors of points. We will call "n-order lifting" of a vector a, the vector L n (a) containing all n-degree monomials of the coefficients of a. For example, second and third order liftings for homogeneous coordinates of 2D points, are as follows:</p><formula xml:id="formula_4">L 2 (q) ∼          q 2 1 q 1 q 2 q 2 2 q 1 q 3 q 2 q 3 q 2 3          L 3 (q) ∼                   q 3 1 q 2</formula><p>1 q 2 q 1 q 2 2 q 3 2 q 2  1 q 3 q 1 q 2 q 3 q 2 2 q 3 q 1 q 2 3 q 2 q 2 3</p><formula xml:id="formula_5">q 3 3                   .</formula><p>(1.4)</p><p>Such lifting operations are useful to describe several camera models. Some camera models use "compacted" versions of lifted image point coordinates, for example:</p><formula xml:id="formula_6">     q 2 1 + q 2 2 q 1 q 3 q 2 q 3 q 2 3      .</formula><p>We will denote these as L2 (q), and use the same notation for other lifting orders.</p><p>We briefly describe various image acquisition technologies, with an emphasis on omnidirectional ones. This section aims at describing the most commonly used technologies in computer vision and related areas, without any claim of being exhaustive. More information, including historical overviews, can be found in the following references, which include textbooks, articles, and webpages <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b102">103,</ref><ref type="bibr" target="#b219">220,</ref><ref type="bibr" target="#b226">227,</ref><ref type="bibr" target="#b293">294,</ref><ref type="bibr" target="#b325">326,</ref><ref type="bibr" target="#b334">335,</ref><ref type="bibr" target="#b527">528,</ref><ref type="bibr" target="#b540">541]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Moving Cameras or Optical Elements</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Slit Imaging</head><p>Slit imaging has been one of the first techniques to acquire panoramic images. Various prototypes existed already in the nineteenth century, usually based on a moving slit-shaped aperture. Historical overviews are given in the references in the first paragraph of this section. In the following, we only review some more recent, digital slit imaging systems, mainly those developed for robotic and computer vision; similar systems were also developed for photogrammetric applications <ref type="bibr" target="#b325">[326]</ref>. Top: "Standard" slit imaging principle and an early realization of it, the cylindrograph of Moëssard <ref type="bibr" target="#b350">[351]</ref>. Bottom: Slit imaging with a tilted 1D sensor, the so-called "cloud camera" and an image acquired with it (see text).</p><p>Most of these systems either use a 2D camera or a 1D camera (also called linear camera or pushbroom camera) which "scans" a scene while moving, generating a panoramic image (cf. Figure <ref type="figure">2</ref>.1). In the 2D camera case, only one or several columns or rows of pixels are usually kept per acquired image, and stitched together to form a panoramic image. Note that pushbroom images are highly related to the so-called epipolar plane images, see for example <ref type="bibr" target="#b55">[56]</ref>.</p><p>Sarachik, Ishiguro et al., and Petty et al. acquired panoramas from a rotating perspective camera by glueing together pixel columns from each image, and used them for map building or 3D measurement <ref type="bibr" target="#b246">[247,</ref><ref type="bibr" target="#b399">400,</ref><ref type="bibr" target="#b433">434]</ref>. <ref type="bibr">Barth</ref> and Barrows developed a panoramic image acquisition system based on a fast panning linear camera, thus significantly decreasing acquisition times <ref type="bibr" target="#b37">[38]</ref>. A similar system was developed by Godber et al. <ref type="bibr" target="#b177">[178]</ref>. Benosman et al. developed a panoramic stereo sensor based on the same principle, consisting of two linear cameras mounted vertically on top of each other <ref type="bibr" target="#b51">[52]</ref>; they rotate together about their baseline and panoramic images are generated by stacking the acquired 1D images together. Issues of calibration, matching, and 3D reconstruction are addressed in <ref type="bibr" target="#b51">[52]</ref>. Klette et al. reviewed several design principles and applications of such devices <ref type="bibr" target="#b280">[281]</ref>.</p><p>Usually, the rotation axis is parallel to the "slits" of pixels used to form the panoramic image. A historic example of rotating a tilted camera, is a design described by Fassig <ref type="bibr" target="#b135">[136]</ref>, called "cloud camera": it acquires a hemispheric field of view by revolving a tilted camera with a wedge-shaped opening around an axis (cf. Figure <ref type="figure">2</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.1).</head><p>There are many more such systems; an exhaustive list is out of scope. The above systems are all designed to deliver central panoramic images: panoramic referring to omnidirectionality in one orientation, central referring to images having a single effective point of view. This is achieved by rotating the 1D or 2D camera about an axis containing its optical center.</p><p>In the following, we review several non-central slit imaging systems.</p><p>One-dimensional cameras, or pushbroom cameras, are routinely used in satellite imaging, since they provide a cheap way of obtaining high-resolution images and since they are well adapted to the way satellites scan planets. Pushbroom panoramas are non-central since each column is acquired at a different position of the satellite. The special case of linear pushbroom panoramas, where the camera motion is assumed to be a straight line (cf. Figure <ref type="figure">2</ref>.2(a)), was extensively modeled by Gupta and Hartley <ref type="bibr" target="#b196">[197]</ref>, see Section 3.1.4. They also proposed a sensor model and a calibration method for the case of pushbroom cameras on an orbiting satellite, e.g., moving along an elliptical trajectory <ref type="bibr" target="#b197">[198]</ref>.</p><p>Concerning close-range applications, Zheng and Tsuji seem to be among the first researchers to have introduced and used non-central panoramic images <ref type="bibr" target="#b569">[570,</ref><ref type="bibr" target="#b570">571,</ref><ref type="bibr" target="#b571">572]</ref>, sometimes also called non-central mosaics, motion panoramas, or omnivergent images. Like in other systems, they proceeded by acquiring images using a moving camera, through a vertical slit, and stacking the acquired slit images one next to the other. If the camera is rotating about the vertical axis through its optical center, then the acquired image is a cylindrical mosaic, or panorama. If the camera is rotating about some other vertical axis, then we obtain non-central mosaics (cf. Figure <ref type="figure">2</ref>.2(b)). Zheng and Tsuji have also proposed a generalization of this principle, for a camera moving on any smooth path. In the case of a straight line, we of course find the above linear pushbroom panoramas. Zheng and Tusji used such motion panoramas for route following of robots. Besides explaining the generation of panoramas, they also analyzed the apparent distortions in the obtained images and other issues. They also used dual-slit panoramas (two panoramas acquired using the same camera and two vertical slits) for stereo computations (cf. Figure <ref type="figure">2</ref>.2(c)), an idea that was later generalized by Peleg et al. <ref type="bibr" target="#b393">[394,</ref><ref type="bibr" target="#b394">395,</ref><ref type="bibr" target="#b396">397]</ref> and Li et al. <ref type="bibr" target="#b313">[314]</ref>, Seitz et al. <ref type="bibr" target="#b443">[444]</ref>.</p><p>Here, 3D points can be reconstructed from point matches between the two non-central images by triangulation.</p><p>Ishiguro et al. used such panoramic views for map generation <ref type="bibr" target="#b245">[246,</ref><ref type="bibr" target="#b246">247]</ref>. They also used the dual-slit panoramas explained above for stereo computations, as well as stereo from panoramas acquired at different positions.</p><p>McMillan and Bishop used panoramic images for image-based rendering, inspired by the plenoptic function concept <ref type="bibr" target="#b337">[338]</ref>.</p><p>Krishnan and Ahuja studied how to obtain the sharpest panoramic images from a panning camera <ref type="bibr" target="#b287">[288]</ref>. They showed that when using a regular camera, whose CCD is perpendicular to the optical axis, the camera should be rotated about an off-center point on the optical axis, together with acquiring images with a varying focus setting. This effectively yields non-central panoramas. Krishnan and Ahuja also proposed another sensor design, where the CCD plane is not perpendicular to the optical axis, and used it for panoramic image acquisition. The advantage of such a sensor is that the depth of field volume is skewed and so, while panning with a fixed focus setting, the union of the images' depth of field volumes, is larger than for a fronto-parallel CCD, hence avoiding to vary the focus setting while panning.</p><p>Usually, cameras are facing outward when acquiring slit images. Inward facing cameras were also considered, as early as in 1895 <ref type="bibr" target="#b104">[105]</ref>, leading to the principle of peripheral photography or images called cyclographs. A sample cyclograph obtained using this approach is shown in Figure <ref type="figure">2</ref>.2(e). Let us mention another acquisition principle that does not produce slit panoramas but is considering the acquisition of images taken around an object: Jones et al. proposed a setup to acquire images as if taken all around an object, with a static camera and object <ref type="bibr" target="#b257">[258]</ref>, whereas the usual approach is to use multiple cameras or a turntable. To do so, they placed a cylindrical mirror around the object and used a rotating planar mirror to select the effective viewpoint.</p><p>Peleg et al. showed how to acquire non-central panoramas without having to move the camera, by using a mirror of a particular shape or a lens with a particular profile <ref type="bibr" target="#b395">[396,</ref><ref type="bibr" target="#b394">395]</ref>. When combining a static central camera with the appropriate mirror or lens, the camera rays of the compound system are distributed the same way as in circular non-central panoramas, i.e., they are incident with a circle containing the camera's optical center (cf. Figure <ref type="figure">2</ref>.2(b)).</p><p>Seitz et al. studied these ideas with the goal of obtaining the most accurate 3D reconstruction from possibly non-central stereo pairs <ref type="bibr" target="#b443">[444]</ref>. Their problem definition is that only a 2D set of lines of sight may be acquired as an image, from viewpoints constrained within a sphere. In order to maximize the reconstruction accuracy of points outside the sphere, each point should be viewed along two lines of sight at least, with maximum vergence angle. The result is that the lines of sight to be stored are tangents of the sphere. How to best choose the appropriate tangents is described in <ref type="bibr" target="#b443">[444]</ref>, resulting in so-called omnivergent images. Interestingly, omnivergent stereo pairs, although being pairs of non-central images, do have a standard epipolar geometry, with horizontal epipolar lines, thus allow to directly apply standard stereo algorithms (more on this in Section 3.4.2). However, acquiring such spherical omnivergent images is not simple, although Nayar and Karmarkar proposed systems for this task, based on catadioptric or fisheye cameras <ref type="bibr" target="#b370">[371]</ref>. An approximate omnivergent stereo pair can be acquired via the dual-slit principle introduced by Zheng et al., see above.</p><p>Agarwala et al. have extended the principle of using slits of images to compose panoramic images, toward the use of image regions, chosen in order to better conform to the scene's shape <ref type="bibr" target="#b3">[4]</ref>. They apply this to generate multi-viewpoint panoramas of urban scenes, such as of the facades of buildings along streets (see Figure <ref type="figure">2</ref>.2(f) for an example).</p><p>Ichimura and Nayar studied the problem of motion and structure recovery from freely moving 1D sensors and rigid rigs of two or three 1D sensors, as well as from special motions; their study subsumes several previously studied cases such as linear pushbroom panoramas <ref type="bibr" target="#b242">[243]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Classical Mosaics</head><p>By "classical" mosaics, we refer to classical in terms of the computer vision community, i.e., mosaics generated by stitching together 2D images, but noting that slit imaging to generate mosaics has been done before, at least with analog cameras. A tutorial on image alignment and stitching has been published in the same journal as this survey, by Szeliski <ref type="bibr" target="#b486">[487]</ref>. Due to this good reference and the fact, that classical mosaic generation is widely known, we do not describe this any further and simply give a few additional references.</p><p>An early and often overlooked work on digital mosaics is by Yelick and Lippman, who showed how to combine images obtained by a rotating camera to generate a mosaic <ref type="bibr" target="#b321">[322,</ref><ref type="bibr" target="#b546">547]</ref>. In his bachelor thesis <ref type="bibr" target="#b546">[547]</ref>, Yelick also discussed other omnidirectional image acquisition techniques, such as the fisheye lens and rotating slit cameras. Later works on digital mosaics include those by Teodosio and Mills <ref type="bibr" target="#b495">[496]</ref>, Teodosio and Bender <ref type="bibr" target="#b494">[495]</ref> and Chen <ref type="bibr" target="#b91">[92]</ref>, to name but a few among the many existing ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Other Technologies</head><p>Murray analyzed the setup by Ishiguro et al. and others (see Section 2.1.1) and proposed an alternative solution where instead of rotating the camera about some axis, the camera looks at a planar mirror which rotates about an axis <ref type="bibr" target="#b361">[362]</ref>. Such a system was already proposed for aerial imaging by Bouwers and van der Sande <ref type="bibr" target="#b57">[58]</ref>. However, the issue of how to compensate for the camera displacement during rotations of the mirror, due to the airplane's motion, was not fully discussed. Other systems that use rotating mirrors or prisms are referenced by Yagi <ref type="bibr" target="#b540">[541]</ref>, see also Section 2.3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Fisheyes</head><p>The concept of fisheye view and lens dates back to more than a century <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b225">226,</ref><ref type="bibr" target="#b349">350,</ref><ref type="bibr" target="#b535">536]</ref>. Fisheye lenses or converters can achieve larger than hemispheric fields of view but are usually still relatively costly. A technical report suggesting inexpensive simple solutions to build fisheye lenses was provided by Dietz <ref type="bibr" target="#b114">[115]</ref>. Some more references on fisheyes are given in Section 3.1.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Catadioptric Systems</head><p>Using external mirrors together with cameras allows for a broad range of design possibilities, which is one of the reasons of the large number of catadioptric devices proposed alone in the computer vision community. Camera design may follow different goals, foremost in our context being a wide field of view, others being for example the compactness of a sensor, a single effective viewpoint, image quality, focusing properties, or a desired projection function. In the following, we exclusively describe geometric properties, in terms of projection function (focusing etc. of course also depend on mirror geometry).</p><p>We may distinguish five types of catadioptric systems: (i) singlemirror central systems, having a single effective viewpoint, (ii) central systems using multiple mirrors, (iii) non-central systems, (iv) singlelens stereo systems, and (v) programmable devices.</p><p>The fourth category, single-lens stereo systems, is of course a subset of the third category, non-central systems, but is singled out here since the goal is to obtain a sufficiently non-central system in order to enable accurate 3D modeling whereas for other systems, "non-centrality" is usually not a design goal but an artifact or a consequence of other design goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Single-Mirror Central Catadioptric Systems</head><p>Baker and Nayar derived all single-mirror central catadioptric systems <ref type="bibr" target="#b21">[22]</ref> (Bruckstein and Richardson obtained some of these results independently <ref type="bibr" target="#b65">[66]</ref>). Essentially the same results were also known outside the scope of catadioptric vision, e.g., were reported in 1637 by Descartes <ref type="bibr" target="#b111">[112]</ref> and later by Feynman et al. <ref type="bibr" target="#b141">[142]</ref> and Drucker and Locke <ref type="bibr" target="#b120">[121]</ref>; further, it is likely that they were already known in antiquity to Greek geometers. Single-mirror central catadioptric systems can only be constructed using mirrors whose surface is obtained by revolving a conic about a symmetry axis of the conic. In addition, the camera looking at the mirror must be central and its optical center Fig. <ref type="figure">2</ref>.3 Illustration of (compositions of) single-mirror central catadioptric systems. Effective viewpoints are shown in green and the position of the true camera, in blue. The latter is only shown for the hyper-catadioptric case, for the para-catadioptric one, the camera is telecentric, thus has an optical center at infinity.</p><p>has to coincide with one of the conic's foci, otherwise the whole system is non-central. 1 The other focus is then the effective viewpoint of the catadioptric system: any (back-projection) line going out from the camera center goes, after being reflected in the mirror, through that second focus (cf. Figure <ref type="figure">2</ref>.3).</p><p>The following special cases exist: hyperboloidal (cf. Figure <ref type="figure">2</ref>.3(a)), paraboloidal (cf. Figure <ref type="figure">2</ref>.3(b)), ellipsoidal, cone-shaped or planar mirrors. As for paraboloidal mirrors, one of the two real focus points is a point at infinity. In order to obtain a wide field of view, the only option is to "position" the camera at that point, which can be achieved using a telecentric lens. As for cone-shaped mirrors, the camera's optical center has to be located at the cone's tip; hence the only part of the mirror the camera sees corresponds to the rays that graze the mirror surface. Cone-shaped mirrors are thus theoretically excluded from the set of useful central catadioptric mirrors, although it has been shown that, with a more general modeling of optics than applied in <ref type="bibr" target="#b21">[22]</ref>, practical central viewing can still be achieved, see further below in this section. Ellipsoidal mirrors do not allow to increase the field of view and are thus not directly appropriate to build omnidirectional cameras; however, they are useful in designing so-called folded catadioptric systems, consisting of two or more mirrors (see Section 2.3.2). As for spherical mirrors, a special case of ellipsoidal ones, both real foci lie at the sphere center; a camera positioned there only sees the reflection of itself, which makes this case impractical. Overall, the only two systems deemed generally practical are the para-catadioptric and hyper-catadioptric ones, based on paraboloidal and hyperboloidal mirrors.</p><p>Central hyper-catadioptric systems seem to have been used first, cf. the patent by Rees in 1970 <ref type="bibr" target="#b423">[424]</ref> and first applications in robotics and computer vision by Yagi and his co-workers <ref type="bibr" target="#b539">[540,</ref><ref type="bibr" target="#b545">546]</ref>. They highlighted the single viewpoint property if the camera is positioned at one of the mirror's foci and also discussed optical properties of the system such as blur. They also combined cameras of different types, such as in the MISS system which is composed of a cone-based catadioptric camera (non-central) and a standard stereo system <ref type="bibr" target="#b543">[544]</ref>. This allows to combine mutual advantages of the sensors, such as good localization using the catadioptric camera and larger resolution with the stereo system. A full trinocular analysis of line images was also proposed.</p><p>Nayar introduced central para-catadioptric systems, consisting of an orthographic camera and a parabolic mirror, positioned such that the viewing direction of the camera is parallel to the mirror axis <ref type="bibr" target="#b366">[367,</ref><ref type="bibr" target="#b367">368]</ref>. He also showed that two para-catadioptric sensors with a field of view of 180 • each, can be put back-to-back to achieve a full spherical field of view while still preserving a single effective optical center (cf. Figure <ref type="figure">2</ref>.3(c)). This is possible since a 180 • field of view is achieved with a para-catadioptric system if the mirror extends till the cross section containing its finite focus. Since the finite focus is exactly the effective optical center, putting two such mirrors back-to-back makes their finite foci coincide and thus also the two effective optical centers.</p><p>Such systems are rather widely used nowadays and commercialized by several companies, see e.g., <ref type="bibr" target="#b102">[103]</ref>.</p><p>As mentioned above, cone-shaped mirrors were predicted as being impractical to achieve central catadioptric systems. This issue was reconsidered by Lin and Bajcsy <ref type="bibr" target="#b318">[319,</ref><ref type="bibr" target="#b319">320]</ref>. Their starting point was that the geometric analysis of Baker and Nayar considered that each 3D point is imaged along a single light path, whereas due to the finite aperture of real lenses, light rays emitted by a point within a finite volume, hit the image area. Reciprocally, a camera whose main lens is located at the tip of a cone (the actual tip being cut off) actually sees the mirror surface and not only rays grazing it. Lin and Bajcsy showed that it is possible to obtain sharp single-viewpoint catadioptric images in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Central Catadioptric Cameras with Multiple Mirrors -Folded Catadioptric Cameras</head><p>Central catadioptric cameras can also be achieved when using more than one mirror. This allows more compact sensor designs and gives additional degrees of freedom to improve optical properties. Such designs are also termed folded catadioptric cameras; an excellent reference is <ref type="bibr" target="#b371">[372]</ref>, where Nayar and Peri presented several designs, design issues, and references to other works in this area. Roughly speaking, when combining conic-shaped mirrors, and positioning them such that foci of successive mirrors in a sequence of reflections, coincide, then central image acquisition is possible: by placing a camera at the left-over focus of the "last" mirror, the compound system has a single effective viewpoint at the left-over focus of the "first" mirror (cf. Figure <ref type="figure">2</ref>.4(a)).</p><p>Previously, Yagi and Yachida proposed such a system, consisting of two paraboloidal mirrors <ref type="bibr" target="#b541">[542]</ref>. Nagahara et al. proposed a design for catadioptric optics for head mounted devices (HMDs), consisting of three mirrors, a planar, a hyperboloidal, and an ellipsoidal one, arranged such as to provide a single effective viewpoint, while achieving a wide field of view and avoiding occlusions of the field of view by the mirrors themselves <ref type="bibr" target="#b362">[363]</ref>. Takeya et al. proposed another similar design <ref type="bibr" target="#b487">[488]</ref>. Kim and Cho addressed the problem of calibrating a system composed of multiple successive mirrors, using a learning-based approach <ref type="bibr" target="#b279">[280]</ref>. Nagahara et al. showed how to achieve uniform angular resolution for a folded catadioptric camera with two mirrors and a single effective viewpoint <ref type="bibr" target="#b363">[364]</ref>.</p><p>Another way of achieving central projection with multiple mirrors is what is often called the "Nalwa pyramid", although an earlier patent on essentially the same system is due to Iwerks <ref type="bibr" target="#b249">[250]</ref>. Iwerks, Nalwa and independently, Kawanishi et al., proposed to use several regular cameras and as many planar mirrors <ref type="bibr" target="#b249">[250,</ref><ref type="bibr" target="#b270">271,</ref><ref type="bibr" target="#b364">365]</ref>. A camera looking at a planar mirror produces the same image (up to side-swapping) as a camera located behind the mirror, at the position obtained by reflecting the original center in the mirror, and "inversely" oriented (cf. Figure <ref type="figure">2</ref>.4(b)). The reflected optical center position is the effective viewpoint here. The main idea consists in arranging camera-mirror pairs such that the effective viewpoints coincide. This is the easiest done in a pyramidal layout (cf. Figure <ref type="figure">2</ref>.4(c)), but others are imaginable. Nalwa's original design consists of four camera-mirror pairs, Kawanishi et al. used six. Main advantages of such a system are omnidirectional viewing (the different camera images can be stitched together), a single effective viewpoint, and high resolution (as compared to mono-camera omnidirectional systems).</p><p>Gao et al. propose a similar design that has a hemispherical field of view <ref type="bibr" target="#b154">[155]</ref>. <ref type="bibr">Majumder et al. and Hua et al.</ref> placed two mirror-camera pyramids back-to-back such that the effective viewpoints of the two pyramids coincide, thus also enhancing the vertical field of view of the compound sensor <ref type="bibr" target="#b233">[234,</ref><ref type="bibr" target="#b234">235,</ref><ref type="bibr" target="#b328">329]</ref>.</p><p>Greguss developed the so-called panoramic annular lens (PAL) which combines reflective and refractive elements in a compact layout and achieves panoramic viewing with a full spherical field of view in horizontal direction and a vertical field of view of about 40 • <ref type="bibr" target="#b187">[188,</ref><ref type="bibr" target="#b424">425]</ref>. Greguss' design has been used and/or improved for example by Powell <ref type="bibr" target="#b406">[407]</ref> and Zhu et al. <ref type="bibr" target="#b573">[574]</ref>.</p><p>Yin and Boult built a sensor consisting of a tree of three differentsized coaxial paraboloidal mirrors and an orthographic camera looking at them <ref type="bibr" target="#b548">[549]</ref>. Their motivation was not to achieve a single-lens stereo system, rather to obtain an image pyramid by optical means: the three mirrors are chosen such that they lead to omnidirectional images where each one doubles the resolution with respect to the previous one. Ideally, the mirrors' foci should coincide in order for the three pyramid images to correspond to the same viewpoint; this is not exactly the case in the proposed system but it was shown that the misalignment is negligible if the scene is sufficiently far away.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Non-central Catadioptric Cameras</head><p>Spherical mirrors. Hong et al. used a non-central catadioptric system with a spherical mirror for robot homing, a visual servoing task <ref type="bibr" target="#b230">[231]</ref>. They actually did not use the entire field of view that the camera offers, but just a region around the horizon plane; the horizon plane, i.e., the plane swept out by horizontal back-projection rays, corresponds to a circle in the image plane. The robot on which the camera is mounted is supposed to move on a horizontal ground plane. Hence, points in the horizon plane are always imaged on the above circle. In order to perform homing, the authors thus exploit a narrow region around that circle, perform image analysis and matching in it and use the result for a 2D visual servoing (translation and rotation in the ground plane). While a spherical mirror always leads to a noncentral catadioptric system (unless the camera is placed at the sphere center), considering only the image portion corresponding to the horizon plane, actually corresponds to using an omnidirectional 1D camera with a single effective viewpoint.</p><p>Cone-shaped mirrors. Yagi and Kawato used a catadioptric system with a cone-shaped mirror (called COPIS) for robot motion estimation and map building <ref type="bibr" target="#b541">[542]</ref>. The system was non-central since the camera was located at a certain distance from the tip of the cone. Yagi and Kawato described the forward projection model as well as 3D point triangulation and epipolar geometry for translational motion. They as well as other researchers have used cone-shaped mirrors mounted vertically on robots since vertical lines, which are omnipresent in man-made scenes, are imaged as (long) radial lines and are thus easy to extract, e.g., <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b320">321,</ref><ref type="bibr" target="#b392">393,</ref><ref type="bibr" target="#b455">456]</ref>. Mouaddib and his co-workers used this idea for pose estimation, motion estimation, map building, etc. with the sensor SYCLOP they developed <ref type="bibr" target="#b392">[393]</ref>. They proposed a calibration method, where the sensor is immersed in a hollow cube with a calibration checkerboard painted on the inside of each face <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b82">83]</ref>. They then use both, extracted points and vertical lines of the calibration object, for calibration. A method for pose estimation (or, absolute localization) is presented in <ref type="bibr" target="#b81">[82]</ref>.</p><p>Purpose-made mirrors optimizing resolution or satisfying other design goals. A drawback of "standard" catadioptric sensors is a significant variation of effective resolution, or spatial/angular resolution, across the image area. Several works aimed at removing or alleviating this, see the nice overview by Hicks and Perline <ref type="bibr" target="#b224">[225]</ref>. Different works aimed at achieving different properties for sensor resolution. Uniform angular resolution is achieved by what is usually called equidistant or equiangular projection: the angle spanned by the backprojection ray of an image point and the optical axis, is proportional to the distance between the image point and the principal point. This is approximately the case for most fisheye lenses. Ollis et al. showed how to compute mirror surfaces that provide an equiangular catadioptric sensor <ref type="bibr" target="#b385">[386]</ref>. Previously, Chahl and Srinivasan obtained a similar result, a mirror shape where the angle between the optical axis and a ray back-projected from the camera, is proportional to the same angle after reflecting the ray in the mirror <ref type="bibr" target="#b83">[84]</ref>. Conroy and Moore achieved a sensor with solid angle pixel density invariance, i.e., where the surface area of a circular image portion is proportional to the solid angle of the corresponding field of view <ref type="bibr" target="#b98">[99]</ref>. A similar work is due to Hicks and Perline <ref type="bibr" target="#b224">[225]</ref>. Another related work by Gächter et al. achieved a similar goal <ref type="bibr" target="#b152">[153]</ref>, but for the case where the camera looking at the mirror has a log-polar sensor arrangement, such as cameras studied by Tistarelli and Sandini <ref type="bibr" target="#b504">[505]</ref> and Pardo et al. <ref type="bibr" target="#b391">[392]</ref>.</p><p>All these works lead to non-central cameras. Nagahara et al. proposed a catadioptric system that is central and has uniform angular resolution <ref type="bibr" target="#b363">[364]</ref>; this was possible by using two appropriately shaped and placed curved mirrors, whereas the above works used a single mirror each.</p><p>Hicks and Bajcsy as well as Kweon et al. showed how to compute mirror shapes such that a particular scene plane that is "frontoparallel" to the mirror is imaged without distortion, while still providing a very wide field of view <ref type="bibr" target="#b221">[222,</ref><ref type="bibr" target="#b222">223,</ref><ref type="bibr" target="#b296">297]</ref>. This was done for both, perspective and orthographic cameras looking at the mirror. Srinivasan contributed a similar result, i.e., a mirror that gives a wide field of view while directly providing a rectified image: in <ref type="bibr" target="#b456">[457]</ref>, he showed how to compute the shape of a mirror that directly provides cylindrical panoramas, i.e., rectangular images where the two coordinates relate to azimuth respective elevation angles of points relative to the camera's optical axis. He found that there is no smooth mirror shape achieving this property, but that this can be approximated by a piecewise planar mirror with a layout similar to a Fresnel lens array.</p><p>Kondo et al. proposed an anisotropic mirror shape, i.e., that is not a surface of revolution, with the aim of obtaining panoramic vision while allocating higher spatial resolution to a preferred azimuth range, e.g., corresponding to the driving direction of a robot <ref type="bibr" target="#b283">[284,</ref><ref type="bibr" target="#b285">286]</ref>.</p><p>Nayar and Karmarkar showed how to acquire 360 × 360 mosaics by stitching together image slices acquired by a rotating slice camera <ref type="bibr" target="#b370">[371]</ref>. The slice camera is designed such as to have a 360 • field of view in one direction, while being orthographic in the other direction. This is achieved by a specially designed mirror; in case the camera looking at the mirror is orthographic itself, the mirror is a cone. Nayar and Karmarkar's design extends slit imaging, cf. Section 2.1.1, to the acquisition of a full spherical field of view, by being based on an omnidirectional slit camera that rotates.</p><p>Peleg et al. showed how to acquire circular non-central mosaics using a mirror of a special shape <ref type="bibr" target="#b394">[395,</ref><ref type="bibr" target="#b395">396]</ref>, see Section 2.1.1.</p><p>A few more general approaches exist for mirror design, as follows. Gaspar et al. proposed a general approach allowing to derive several of the above mirror designs in a unified framework <ref type="bibr" target="#b155">[156,</ref><ref type="bibr" target="#b156">157]</ref>. Hicks as well as Menegatti formulated and provided a solution to the so-called prescribed projection problem <ref type="bibr" target="#b220">[221,</ref><ref type="bibr" target="#b340">341]</ref>: the input is a desired mapping between an object surface and the image plane of a camera in a given position. The goal is to compute a mirror and its location, that together realize this mapping, i.e., the image taken by the resulting catadioptric system is as specified. In many cases, there is no exact solution to the problem, but approximate solutions can be found. Swaminathan et al. addressed the same problem as Hicks in <ref type="bibr" target="#b485">[486]</ref> and proposed a solution that minimizes reprojection errors in the image plane, i.e., where the desired and the actual scene-to-image mapping (the "inverse" of the prescribed projection problem) give image points as close to one another as possible. Kondo et al. proposed another approach for the same problem, akin to photometric stereo <ref type="bibr" target="#b284">[285]</ref>. This approach allows to conceive discontinuous mirror shapes.</p><p>All the above sensors are necessarily non-central (with the exception of <ref type="bibr" target="#b363">[364]</ref>), although for many applications, one may model them sufficiently well using a central approximation, see also the discussion in Section 3.6.</p><p>Krishnan and Nayar recently proposed a catadioptric camera whose main optics is a fisheye, a camera they call cata-fisheye <ref type="bibr" target="#b288">[289]</ref>. The motivation for this design is to achieve a spherical field of view in azimuth while not necessarily having a hemispherical one in elevation. Indeed, in many applications, such as videoconferencing and vehiclemounted computer vision, a vertical field of view of a few dozens of degrees is often sufficient. Krishnan and Nayar proposed to achieve this by putting a convex mirror in front of a fisheye. The produced image consists of two annular regions: the outer one corresponds to the direct view at the scene through the fisheye, whereas the inner one shows the reflection in the mirror. Here, the outer image region would usually correspond to the part of the vertical field of view that is above the camera, while the inner region shows a part that is below. One advantage of this system is thus that the full image resolution is dedicated to the desired vertical field of view (as opposed to "wasting" resolution on a full hemispherical fisheye field of view if parts thereof are not relevant for a given application). Other advantages are a good image quality since a mirror of low curvature may be used, and a rather compact design. It is furthermore easy to regulate the desired vertical field of view by displacing the mirror or employing mirrors of different shapes. Strictly speaking, the system is non-central in general, but the working range in which the parallax is negligible, is explained in <ref type="bibr" target="#b288">[289]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">Single-Lens Stereo Systems</head><p>As mentioned at the beginning of Section 2.3, we consider here catadioptric systems that are intentionally non-central in order to enable stereovision, whereas the systems in the previous section are noncentral "by accident" or due to conforming to design goals prohibiting a single effective viewpoint. Most catadioptric single-lens stereo systems are achieved by setting one or more planar or curved mirrors in front of the camera <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b142">143,</ref><ref type="bibr" target="#b182">183,</ref><ref type="bibr" target="#b202">203,</ref><ref type="bibr" target="#b254">255,</ref><ref type="bibr" target="#b332">333,</ref><ref type="bibr" target="#b365">366,</ref><ref type="bibr" target="#b385">386,</ref><ref type="bibr" target="#b454">455]</ref> or in addition by keeping a direct view on the scene for a part of the camera's field of view <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b260">261,</ref><ref type="bibr" target="#b294">295,</ref><ref type="bibr" target="#b342">343,</ref><ref type="bibr" target="#b421">422,</ref><ref type="bibr" target="#b568">569]</ref>.</p><p>Advantages of mirror-based stereo systems over multi-camera setups are that no camera synchronization and radiometric alignment of camera are required and that only one set of intrinsic parameters has to be calibrated, as opposed to calibrating each camera in a multi-camera setup.</p><p>Mouaddib et al. proposed a set of performance criteria to compare different single-lens catadioptric stereo systems <ref type="bibr" target="#b353">[354,</ref><ref type="bibr" target="#b354">355]</ref>.</p><p>In the following, we describe some systems, first some using planar mirrors.</p><p>Planar mirrors. One of the earliest references suggesting a design for a mirror-based single-lens stereo system is probably a paper of 1899 by Finsterwalder <ref type="bibr" target="#b143">[144]</ref>, a seminal paper which summarized various results on camera calibration, pose and motion estimation, epipolar geometry, and even projective reconstruction from uncalibrated images. On pages 20-22 of this paper (written in German), Finsterwalder proposed to use a setup of three mutually perpendicular planar mirrors to photograph small objects put between the camera and the mirrors. He suggested that this gives up to eight perspective images (direct view up to multiple reflections) which can be used for stereo reconstruction. We do not know if this system has ever been built.</p><p>The idea of using a single planar mirror to acquire stereo images for 3D modeling, a direct and a reflected view of an object, has been considered by various researchers in the early twentieth century, see for example <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b116">117,</ref><ref type="bibr" target="#b408">409,</ref><ref type="bibr" target="#b561">562,</ref><ref type="bibr" target="#b562">563]</ref>. Some of these works were motivated by the modeling of coastal landscapes from images taken aboard a ship, the mirror surface being formed by a sea or lake. Note also that systems composed of planar mirrors were proposed for stereo viewing (as opposed to stereo imaging) as early as in 1838, by Wheatstone <ref type="bibr" target="#b525">[526,</ref><ref type="bibr" target="#b526">527]</ref> (see also <ref type="bibr" target="#b228">[229]</ref>).</p><p>Kaneko and Honda showed that when acquiring an image of an object consisting of a part with a direct view of the object and another with its reflection in a planar mirror, this indeed corresponds to a stereo configuration and allows to reconstruct the object in 3D <ref type="bibr" target="#b260">[261]</ref> (see also a similar approach by Zhang and Tsui <ref type="bibr" target="#b568">[569]</ref>). This approach was extended by Mitsumoto et al. whose approach relaxed the requirement of knowing the mirror's position and who also proposed to use an additional planar mirror, to better recover occluded object parts <ref type="bibr" target="#b342">[343]</ref>. Arnspang et al. also proposed to put planar mirrors in front of the camera <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b421">422]</ref>. One of the setups they suggested consisted of two mirrors facing each other and which are parallel to the camera's optical axis (cf. Figure <ref type="figure">2</ref>.5(a) and (b)). Here, a scene point may be seen multiple times: in the direct camera view, reflected in the mirrors or even after a double reflection, once in each mirror. Arnspang et al. formulated the triangulation problem for their setup and also proposed to extend the setup by arranging more than two mirrors in a cylindrical ring in front of the camera (cf. Figure <ref type="figure">2</ref>.5(c)).</p><p>A similar idea was suggested by Han and Perlin who proposed a single-lens stereo system akin to kaleidoscopes <ref type="bibr" target="#b202">[203]</ref>. The camera looks through a tapered tube whose inside is made of planar mirror facets. Each planar mirror, together with the actual camera, corresponds to a virtual camera. Hence, each scene point can be seen multiple times in what is effectively a multi-view stereo system. Furthermore, multiple reflections of light rays may happen inside the tube, thus multiplying the number of virtual cameras and viewpoints, much like what one can observe in a kaleidoscope. Han and Perlin's idea was to use this acquisition system for the acquisition of the Bidirectional Texture Reflectance of objects, based on the large number of viewpoints contained in a single snapshot. Kuthirummal and Nayar proposed a similar system, see further below.</p><p>Goshtasby and Gruver used two planar mirrors and no direct view of the scene <ref type="bibr" target="#b182">[183]</ref>. Cafforio and Rocca, Inaba et al. as well as Mathieu and Devernay used four planar mirrors, where a pair each gives one virtual camera, via the successive reflections in two mirrors (see Figure <ref type="figure">2</ref>.6); the system is thus basically equivalent to one with two planar mirrors, but allows for an easier setup by more easily avoiding self-reflections of the camera in the mirrors <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b243">244,</ref><ref type="bibr" target="#b332">333]</ref>. Gluckman and Nayar analyzed in detail the relative orientation and epipolar geometry of such systems as well as their self-calibration <ref type="bibr" target="#b173">[174,</ref><ref type="bibr" target="#b174">175]</ref>.</p><p>In <ref type="bibr" target="#b176">[177]</ref>, Gluckman and Nayar studied the question of how to obtain an optically rectified stereo pair using a single-lens catadioptric system with planar mirrors. They found that an odd number of mirrors is required and for the cases of one and three mirrors, derived the constraints on mirror placement that ensure rectified images. They also showed how to optimize the mirror placement in order to minimize the overall size of the entire sensor.</p><p>Avni et al. built a system composed of two cameras and two planar mirrors: the cameras as well as an object to be modeled in 3D are Fig. <ref type="figure">2</ref>.6 Left: Single-lens stereo system using planar mirrors. Here, four mirrors are used for practical reasons, leading to two effective viewpoints, shown in green. Right: A practical realization of the system, image courtesy of Frédéric Devernay <ref type="bibr" target="#b332">[333]</ref>.</p><p>positioned above the mirrors such that each camera sees the object's reflections in both mirrors <ref type="bibr" target="#b19">[20]</ref>. Hence, a total of four images of the object are obtained, allowing a multi-view 3D modeling.</p><p>Non-planar mirrors. Nayar used two specular spheres in the field of view of a camera to obtain stereo information <ref type="bibr" target="#b365">[366]</ref>. Southwell et al. presented a design of a single-lens catadioptric stereo sensor with curved mirrors where one convex mirror is fixed on top of a second, larger one <ref type="bibr" target="#b142">[143,</ref><ref type="bibr" target="#b454">455]</ref>. Hence, points in the common field of view of the two individual catadioptric images are seen twice and can be reconstructed. Similar designs were proposed by Cabral et al. <ref type="bibr" target="#b72">[73]</ref> and Jang et al. <ref type="bibr" target="#b254">[255]</ref>. Jang et al. especially discussed how to maximize the effective stereo baseline.</p><p>Nene and Nayar described several single-lens catadioptric stereo configurations where both (virtual) cameras in a stereo system are central <ref type="bibr" target="#b373">[374]</ref>. The first uses, like in other systems, two planar mirrors; Nene and Nayar described the epipolar geometry of this setup. The other proposed configurations use mirrors of revolution of conic-shape placed such that the camera is at a focus point of each of the mirrors, see Figure <ref type="figure">2</ref>.7(a). Hence, each mirror corresponds to a central catadioptric system and by using two or more mirrors arranged this way, a stereo system is obtained where each (virtual) camera is central. Nene and Nayar described the cases of pairs of ellipsoidal, pairs of hyperboloidal and pairs of paraboloidal mirrors and derived the epipolar geometry for each of those. Other cases are straightforward to imagine, e.g., combing ellipsoidal/hyperboloidal/planar mirrors in the appropriate manner.</p><p>Murphy presented a panoramic imaging system for planetary rovers where a single camera looks at a convex mirror with a hole in the middle, through which the camera sees through a fisheye lens <ref type="bibr" target="#b360">[361]</ref>. Hence, a single image contains two panoramas, one for the lower part (the catadioptric view) and one for the upper one (fisheye). The system is similar to that of Krishnan and Nayar (cf. Section 2.3.3), although its aim is to obtain single-lens stereo while that of Krishnan and Nayar's design is omnidirectional viewing with a desired distribution of spatial resolution across the image. Benosman et al. proposed a sensor where a camera has both, a direct view of the scene and a view on a hyperbolic mirror <ref type="bibr" target="#b48">[49]</ref>. The motivation is to have a high resolution in a dominant direction via the direct perspective image, e.g., the direction ahead of a robot, together with a lower-resolution panoramic view of the surroundings. Yi and Ahuja proposed another single-lens stereo system, consisting of a hyperboloidal mirror and a concave lens <ref type="bibr" target="#b547">[548]</ref>. The camera has a direct view of the mirror and an indirect one of it, through the concave lens, thus effectively producing a stereo pair.</p><p>Sagawa et al. built a single-lens catadioptric stereo system consisting of one camera looking at seven spherical or paraboloidal mirrors <ref type="bibr" target="#b431">[432]</ref>, see Figure <ref type="figure">2</ref>.7(b) and (c). Although in principle such a system can be used for multi-baseline stereo, the prototype shown in <ref type="bibr" target="#b431">[432]</ref> is of small dimension and thus has a small baseline. Its intended application is the detection of close-by objects, for which accurate 3D reconstruction is not necessary. The calibration of the sensor is discussed in <ref type="bibr" target="#b281">[282]</ref>, see also Section 5.3.2.</p><p>Lanman et al. built a similar catadioptric acquisition system consisting of a single high-resolution camera looking at a set of spherical mirrors arranged on a plate <ref type="bibr" target="#b297">[298]</ref>. Their system is larger than Sagawa's since one intended application is 3D modeling. The system is obviously non-central since already a single spherical mirror leads to a non-central image. While a single spherical mirror is only "slightly non-central" (more on this in Section 3.6), the setup by Lanman et al. effectively generates a large "baseline", allowing for single image 3D reconstruction. A practical method for calibrating the system, including the properties of second surface mirrors, is proposed in <ref type="bibr" target="#b297">[298]</ref>, and the multi-mirror view geometry is analyzed, i.e., conditions that hold for points observed in different mirrors, to be images of the same scene point.</p><p>Kuthirummal and Nayar proposed a system similar to the one by Arnspang et al. and Han and Perlin (see above), where the piecewise planar tube is replaced by a cone or cylinder-shaped one <ref type="bibr" target="#b294">[295]</ref>, see Figure <ref type="figure">2</ref>.5(d)-(f). They demonstrated the use of their system for singleimage 3D scene and reflectance recovery.</p><p>Orghidan et al. designed a structured-light type depth sensor composed of a camera looking at two hyperboloidal mirrors and a laser emitter <ref type="bibr" target="#b387">[388]</ref>. They showed how to model and calibrate this system and use it for 3D modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rotating mirrors.</head><p>Other catadioptric single-lens stereo systems have been proposed, using mirrors that rotate between different image acquisitions in order to produce stereo images, e.g., by Teoh and Zhang <ref type="bibr" target="#b496">[497]</ref>, Nishimoto and Shirai <ref type="bibr" target="#b375">[376]</ref>, Murray <ref type="bibr" target="#b361">[362]</ref>, and Gao and Ahuja <ref type="bibr" target="#b153">[154]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.5">Programmable Systems</head><p>Hicks and Nayar et al. proposed methods and system designs allowing to modify the shape of the mirror in a catadioptric system in order to adapt the sensor to a change in the scene or to acquire an image in a desired way <ref type="bibr" target="#b223">[224,</ref><ref type="bibr" target="#b295">296,</ref><ref type="bibr" target="#b369">370]</ref>.</p><p>Nayar et al. proposed to use a programmable array of micromirrors to achieve programmable, or purposive, imaging <ref type="bibr" target="#b369">[370,</ref><ref type="bibr" target="#b368">369]</ref>. They demonstrated the use of digital micro-mirror devices (DMDs), routinely used in projectors, for tasks such as high dynamic range image acquisition, optical appearance matching, or generally speaking, the change of imaging geometry. In a similar work, Hicks et al. also suggested to use micro-mirrors to control the way an image is acquired, e.g., by actively controlling the spatial resolution across an image <ref type="bibr" target="#b223">[224]</ref>.</p><p>The following two ideas are not strictly speaking programmable systems. Kuthirummal and Nayar studied the possibility of using a flexible mirror sheet as reflector in a catadioptric system <ref type="bibr" target="#b295">[296]</ref>. They proposed to recover the current shape of the mirror from its outline in the image and additional assumptions; once the shape is determined, the catadioptric system is effectively calibrated. The system is not programmable in the same sense as those by Hicks and Nayar et al. but is described in this section since it operates by changing the mirror shape during image acquisition.</p><p>Fergus et al. proposed the concept of random lens imaging <ref type="bibr" target="#b139">[140]</ref>. One practical instance of this concept is a camera looking at a collection of randomly positioned small mirrors or refractive elements. A calibration method is proposed and potential applications of the general concept for tasks, such as super-resolution and depth sensing, are described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Stereo and Multi-camera Systems</head><p>Using two or more cameras to achieve omnidirectional viewing is a wellknown principle and will not be covered in great detail here, besides a few historical references and references to stereo systems based on omnidirectional cameras. A nice overview of omnidirectional stereo systems is due to Zhu <ref type="bibr" target="#b572">[573]</ref>.</p><p>Lin and Bajcsy proposed a catadioptric stereo system consisting of two cameras looking at one cone-shaped mirror from different distances <ref type="bibr" target="#b320">[321]</ref>. The cameras are aligned with the cone's axis of revolution; to be precise, one camera lies on the axis, the other one looks at the mirror through a beam splitter such that it virtually comes to lie on the mirror axis. Hence, epipolar planes contain the axis, which simplifies the epipolar geometry and 3D reconstruction.</p><p>Spacek proposed to use two cone-camera pairs, one on top of the other and all cameras and mirrors axis-aligned <ref type="bibr" target="#b455">[456]</ref>. He derives the equations for stereo computations and highlights that cone-based catadioptric cameras, although being non-central, can be used for stereo and may be advantageous over the more common central catadioptric systems due to their higher spatial resolution (lower vertical field of view). A similar system, consisting of two hyper-catadioptric cameras, was used in <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b412">413]</ref>.</p><p>Other stereo systems combine omnidirectional and traditional cameras, to combine their respective advantages, see e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b117">118,</ref><ref type="bibr" target="#b267">268]</ref>.</p><p>Multi-camera systems composed of several perspective cameras have been built at least as early as in 1884, initially mainly if not exclusively for aerial imaging. The earliest work known to us (no effort was made for an exhaustive bibliography research) is that of Triboulet, who, as reported in <ref type="bibr" target="#b503">[504]</ref> experimented from 1884 on with a multicamera system consisting of seven cameras attached to a balloon: one camera looked downward and six cameras were equally distributed around the balloon's circumference (the system thus resembles the popular Ladybug sensor by Point Grey, http://www.ptgrey.com). Similar other multi-camera systems from the early twentieth century include those by Scheimpflug <ref type="bibr" target="#b437">[438]</ref> and Thiele <ref type="bibr" target="#b498">[499]</ref>, cf. Figure <ref type="figure">2</ref>.8. Several other systems were developed throughout the first half of the twentieth century, see for example <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b402">403,</ref><ref type="bibr" target="#b422">423,</ref><ref type="bibr" target="#b501">502]</ref>.</p><p>Other systems used multiple lenses in the same camera body. Gasser as well as Aschenbrenner invented systems where multiple cameras share the same focal plane and film; due to a suitable arrangement Fig. <ref type="figure">2</ref>.8 Three multi-camera systems from the first decade of the twentieth century. Top: two systems developed by Thiele <ref type="bibr" target="#b498">[499]</ref>. The first one, called auto-panoramograph, was attached at the bottom of a balloon. The second system, the stereo-panoramograph, is equivalent to an omnidirectional stereo rig with a baseline of 2 m. Bottom: A system developed by Scheimpflug <ref type="bibr" target="#b437">[438]</ref>, consisting of seven oblique cameras and a central one. Shown are eight original images acquired with the system and a mosaic composed from them using a special projection equipment called photo-perspectograph.</p><p>of prisms and mirrors, the different cameras directly generate a composite wide-angle image on the film <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b159">160]</ref>. These systems were, like most of the above, intended for aerial imagery. Multi-lens cameras were used at least as early as in 1882: Londe used multi-lens cameras to study epileptic seizures, by capturing a succession of time-delayed images <ref type="bibr" target="#b70">[71]</ref>, similar to the famous motion series by Muybridge according to an acquisition principle he developed in 1872 <ref type="bibr" target="#b70">[71]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Others</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">Plenoptic Camera</head><p>Adelson and Wang proposed the so-called plenoptic camera, a design that essentially gives multiple pinhole images while using a single main aperture and a single sensor array <ref type="bibr" target="#b1">[2]</ref>. When placing a lenticular array in front of a camera's sensor plane, each lenslet together with a subset of the sensor array's pixels forms a tiny pinhole camera. That pinhole camera effectively only captures rays entering the camera's main aperture within a subregion of the aperture, as opposed to regular cameras, where each pixel integrates light rays from all over the aperture. Conceptually, a plenoptic camera thus corresponds to a multi-camera system consisting of cameras arranged on a planar grid. Acquired images can be used to perform stereo computations or various computational photography tasks. We will not consider plenoptic cameras further in this article since they are not usually used for panoramic imaging, although in principle nothing would prevent to use them with fisheye lenses (besides the drop of spatial resolution for the already lowresolution individual pinhole sub-cameras).</p><p>More information on plenoptic cameras and similar designs can for example be found on http://en.wikipedia.org/wiki/ Integral photography and in <ref type="bibr" target="#b428">[429,</ref><ref type="bibr" target="#b161">162]</ref> The concept of plenoptic camera is highly related to that of integral imaging <ref type="bibr" target="#b322">[323]</ref> and parallax stereogram <ref type="bibr" target="#b247">[248]</ref> or panoramagram <ref type="bibr" target="#b248">[249]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2">Biprism</head><p>A single-lens stereo system using a biprism rather than a mirror, that directly produces rectified images, has been developed by Lee et al. <ref type="bibr" target="#b302">[303]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.3">Spherical Lens and Spherical Image Area</head><p>Krishnan and Nayar proposed an omnidirectional sensor consisting of a transparent sphere, around which sensing elements are uniformly arranged on a spherical surface, with free space between neighboring elements <ref type="bibr" target="#b289">[290]</ref>. Hence, individual sensing elements can see through the transparent sphere and other sensing elements, to perceive the scene. The design goals of the system are an omnidirectional field of view with a single center of projection and uniform spatial resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.4">Krill-Eye</head><p>Hiura et al. proposed an optical arrangement for wide-angle imaging system, termed krill-eye, that is inspired by compound animal eyes <ref type="bibr" target="#b227">[228]</ref> The system consists of gradient refractive index (GRIN) lenses aligned on a spherical surface. Hiura et al. proposed a theoretical study on image quality and focusing properties and built a proof-of-concept prototype.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.5">Rolling Shutter Cameras</head><p>Meingast et al. and Ait-Aider et al. studied the case of rolling shutter CMOS cameras, where the image area is not exposed simultaneously, but in a rolling fashion across rows of pixels <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b170">171,</ref><ref type="bibr" target="#b339">340]</ref>. The consequence is that when taking a picture of a moving object, it will appear distorted. Meingast et al. developed projection models for the general and several special cases of object motion and showed how optical flow can be modeled <ref type="bibr" target="#b339">[340]</ref>. They also addressed the calibration of the sensor. Ait-Aider et al. used the rolling shutter's apparent drawback to develop an approach that computes both, the pose of a known object and its instantaneous velocity, from a single image thereof <ref type="bibr" target="#b9">[10]</ref>. There is a conceptual link to pushbroom cameras, as follows, highlighted in a similar fashion in <ref type="bibr" target="#b170">[171,</ref><ref type="bibr" target="#b339">340]</ref>. An alternative way of looking at the rolling shutter phenomenon is to consider a static object and a pushbroom camera moving around it with a velocity that combines the inverse velocity of the object and a motion that compensates the difference in pose associated with different rows of pixels in the camera.</p><p>The effect of rolling shutters was also taken into account by Wilburn et al. who built a multi-camera array with CMOS cameras <ref type="bibr" target="#b528">[529,</ref><ref type="bibr" target="#b529">530]</ref>, with several applications such as synthetic aperture imaging or allowing to acquire videos with increased frame rate, dynamic range, resolution, etc. A method for calibrating such a camera array was proposed by Vaish et al. <ref type="bibr" target="#b515">[516]</ref>.</p><p>In the following, we review a certain number of camera models that were proposed in the literature. Some of them are based on physical models, others are of a more algebraic nature. These models can be described and sorted according to various criteria. A first characteristic concerns the spatial distribution of the camera rays along which a camera samples light in the environment. Most models have a single optical center through which all camera rays pass. We also speak of central camera models. For these, the back-projection function (see below) delivers the direction of the camera ray. Noncentral camera models do not possess a single optical center. In that case, the back-projection operation has to deliver not only the direction but also the position of a camera ray, e.g., some finite point on the ray. We will also use Plücker coordinates to represent camera rays. Special cases of non-central cameras are oblique camera, where no two camera rays meet <ref type="bibr" target="#b389">[390]</ref> and axial cameras where there exists a line that cuts all camera rays. A special case of this are x-slit or two-slit models where there exist two lines that cut all camera rays. This is for example the case for linear pushbroom cameras <ref type="bibr" target="#b196">[197]</ref>.</p><p>A second property of camera models and one we would like to stress particularly, concerns how global or local a model is. The following definitions have already been introduced in Section 1.1, cf. also Figure 1.1. Most models have a small set of parameters and in addition, each parameter influences the projection function all across the field of view. We call these, global models, since they hold for the entire field of view/image area. Second, there exist several more local models, e.g., models with different parameter sets for different portions of the image. These models have usually more parameters than global ones, but have a higher descriptive power. Finally, at the extreme, we have discrete models, where the projection or back-projection function is merely sampled at different points, e.g., sampling the back-projection function at every pixel. These models have many parameters, one set of them per sampled location. They are sometimes called non-parametric models, but we do not find this entirely appropriate, since they do have parameters; hence the proposed name of discrete models. Let us note that the transition from global to local to discrete models is not discontinuous: some local models have the same parametric form as global ones, and to use discrete models, one usually needs some interpolation scheme, for example to be able to back-project any image point, not only sampled ones. In that respect, discrete models plus, such as interpolation scheme, are actually local models in our language.</p><p>Many of the global and discrete models described in the following are well known. This is less true for the local models, although they may represent a good compromise between tractability (number of parameters, stability of calibration) and generality. We would like to point out the work of Martins et al. <ref type="bibr" target="#b330">[331]</ref>, which is rather often cited but often described only partially or not appropriately. They proposed three versions of the so-called two-plane model, see Sections 3.1.3 and 3.2.1 for more details. These foreshadowed several important contributions by others, mostly achieved independently. First, it contains one of the first proposals for a local camera model (the spline-based version of the developed two-plane model). Second, it sparked works on discrete camera models, where camera rays are calibrated individually from images of two or more planar calibration grids <ref type="bibr" target="#b84">[85,</ref><ref type="bibr" target="#b188">189]</ref>, an approach rediscovered by <ref type="bibr" target="#b190">[191,</ref><ref type="bibr" target="#b476">477]</ref>. Third, the linear and quadratic versions of the two-plane model, when written in terms of back-projection matrices, are nothing else than particular instances of rational polynomial models; they are thus closely related to the division and rational models explained by Fitzgibbon <ref type="bibr" target="#b144">[145]</ref> and Claus and Fitzgibbon <ref type="bibr" target="#b97">[98]</ref> or the general linear cameras of Yu and McMillan <ref type="bibr" target="#b558">[559]</ref>, as well as others, which found a lot of interest recently. Martins et al. may also be among the first to explicitly use lifted coordinates (cf. Section 1.2) in camera models.</p><p>Finally, a third main property of camera models we consider is that some models have a direct expression for forward projection, others for back-projections, some work easily both ways. This is important since forward and back-projections are convenient for different tasks: forward projection for distortion correction of images and bundle adjustment, back-projection for minimal methods for various structure-from-motion tasks, such as pose and motion estimation.</p><p>A few more general notes and explanations of notations follow. Back-projection versus forward projection. One defines the other but one or the other may be more difficult to formulate algebraically. For example, the classical radial distortion model is written using polynomials; even with only one distortion coefficient, the polynomials are cubic, making it cumbersome to write the inverse model. This model is traditionally used for back-projection although some researchers used the same expression for forward projection. Other polynomial models are also used for either direction.</p><p>Generally speaking though, back-projection is often easier to formulate. This is especially true for catadioptric systems, where backprojection comes down to a deterministic "closed-form" ray-tracing whereas forward projection entails a search for which light ray(s) emitted by a 3D point gets reflected by the mirror(s) into the camera. Also, when using rational polynomial functions in point coordinates to express camera models, this is straightforward for back-projection; using such a model for forward projection, results in general in curved "camera rays", i.e., the set of 3D points mapped onto an image point, form a curve, not a straight line (cf. Section 3.1.8 on the cubic camera model by Hartley and Saxena <ref type="bibr" target="#b209">[210]</ref>).</p><p>For these and other reasons, we will emphasize back-projection formulations in the following. Another reason for this is that backprojection is less often used in the literature although it enables to clearly formulate several structure-from-motion tasks and epipolar geometry.</p><p>Back-projection will be formulated in two different ways, either by giving expressions allowing to compute two points on a camera ray, or by directly giving the Plücker coordinates of the ray. As for the first case, one of the two points on the ray will be its point at infinity and the other, a finite point, denoted by B i and B f , respectively. The point at infinity will be given as a 3-vector, the finite point as a 4-vector, both in homogeneous coordinates. Note that in this survey we do not consider the possibility of camera rays that lie completely on the plane at infinity; such a case is of small practical interest and if necessary, all formulas are easy to adapt. Plücker coordinates of camera rays will be written as</p><formula xml:id="formula_7">B l .</formula><p>They can of course be easily computed from the two points B i and B f , cf. Section 1.2.</p><p>The reason for using both expressions for back-projection is that the two-point expression is better suited to formulate pose estimation, whereas the Plücker-based expression allows to formulate motion estimation and epipolar geometry in a straightforward manner. When we deal with central camera models, we will only give the point at infinity B i of camera rays, i.e., their direction, always assuming that cameras are in canonical position, i.e., with the optical center at the origin.</p><p>Whenever possible, we try to write the back-projection operation using back-projection matrices, operating on (lifted) image point coordinates. We will use full back-projection matrices B l of size 6 × n, that map image points to camera rays in Plücker coordinates, and partial back-projection matrices B i of size 3 × n, that map image points to camera ray directions. The values of n for different models depend on which liftings of image coordinates are required.</p><p>Finally, let us note that the back-projection for a true camera usually gives rise to a half-line in 3D; many camera models do not take this fully into account and only model back-projection via infinite lines. We do the same in this monograph and, for ease of language, use the term ray or camera ray to denote a line of sight of a camera.</p><p>Radial distortion. In this section, models are presented in different forms, depending if they are backward or forward ones, or only model radial distortion or more general distortions. Radial distortion models are given in different forms; they are defined by a 1D (un)distortion profile or (un)distortion function, mapping radial distances in the distorted image (distances of image point to distortion center) to either radial distances in the undistorted image or the incidence angle between camera rays and the optical axis.</p><p>We use the following notations. The radial distance is denoted by r d (in distorted images) and r u (in undistorted images). The angle between a camera ray and the optical axis is denoted by θ. Radial distortion models are usually given as a mapping between any of these three entities. For simplicity, we always assume in the following that the radial distortion center is equal to the principal point and that it is known and located at the origin. When considering a distortion center different from the origin, the given back-projection equations can be modified in a straightforward manner, by preceding them with the translation bringing the distortion center to the origin. When considering a distortion center different from the principal point, the modifications of back-projection equations are equally straightforward. Whereas in the following, we suppose that the perspective backprojection consists merely in "undoing" the focal length, considering a distortion center that is different from the principal point, requires a full perspective back-projection.</p><p>Extrinsic parameters. Camera pose, or extrinsic parameters, is straightforward to take into account. As for back-projection, translations and rotations can be applied as follows (cf. Section 1.2):</p><formula xml:id="formula_8">B f → R t 0 T 1 B f B i → RB i B l → R 0 -[t] × R R B l .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Global Camera Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Classical Models</head><p>By "classical models", we mean those used most often in applications and academic research, i.e., the pinhole model, affine camera models, and pinhole models enhanced with classical terms for radial and tangential distortions.</p><p>Pinhole model. The pinhole model, or perspective projection, assumes that all camera rays pass through a single point, the optical center and that there is a linear relationship between image point position and the direction of the associated camera ray. That relationship can be expressed via a so-called calibration matrix which depends on up to five intrinsic parameters:</p><formula xml:id="formula_9">K =   f u s x 0 0 f v y 0 0 0 1   ,</formula><p>where f u respectively f v is the focal length measured in pixel dimensions, horizontally respectively vertically, s is a so-called skew term which may for example model non-rectangular pixels or synchronization errors in the image read-out, and (x 0 , y 0 ) are the coordinates of the principal point, the orthogonal projection of the optical center onto the image plane. The linear relation between image points and ray directions can be written as:</p><formula xml:id="formula_10">q ∼ KB i .</formula><p>The model is equally easy to use for forward and back-projections. In what follows, we will assume square pixels, i.e., s = 0 and</p><formula xml:id="formula_11">f u = f v = f : K =   f 0 x 0 0 f y 0 0 0 1   . (3.1)</formula><p>Affine models. We also briefly mention affine camera models, where the optical center is a point at infinity. Various submodels thereof exist, often called orthographic, weak-perspective, and para-perspective models, going from the most specific to the most general affine model <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b231">232]</ref>. These models can give good approximations of pinhole cameras in the case of very large focal lengths or if the scene is shallow in the direction of the camera's optical axis as well as far from the camera. Back-projection is slightly different from the pinhole model in that the direction is the same for all camera rays (the direction corresponding to the optical center) and the essential back-projection operation is thus not the computation of the ray direction, but of a finite point on the ray.</p><p>Classical polynomial distortion models. The pinhole model has been enhanced by adding various terms for radial, tangential, and other distortions. The most widely established models are described for example in <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b148">149,</ref><ref type="bibr" target="#b151">152,</ref><ref type="bibr" target="#b450">451]</ref>. The usual model for combining radial and decentering distortion (one type of tangential distortion) is:</p><formula xml:id="formula_12">xd = x d -x 0 ȳd = y d -y 0 r 2 d = x2 d + ȳ2 d (3.</formula><p>2)</p><formula xml:id="formula_13">x u = x d + p 1 (r 2 d + 2x 2 d ) + 2p 2 xd ȳd + xd n i=1 k i r 2i d y u = y d + 2p 1 xd ȳd + p 2 (r 2 d + 2ȳ 2 d ) + ȳd n i=1 k i r 2i d ,</formula><p>where n is the number of radial distortion coefficients used. The distortion corrected image points correspond to a pinhole model, i.e., are related to camera ray directions by:</p><formula xml:id="formula_14">B i ∼ K -1   x u y u 1   .</formula><p>In the above model, it is assumed that the radial distortion center coincides with the principal point. Other models exist that consider them as distinct and/or include additional distortion terms, see e.g., <ref type="bibr" target="#b148">[149,</ref><ref type="bibr" target="#b214">215]</ref>.</p><p>If we neglect tangential distortion and suppose that image coordinates are centered in the distortion center and that the latter is equal to the principal point, then we can deduce, from Equation (3.2), the radial (un)distortion function:</p><formula xml:id="formula_15">r u = r d 1 + n i=1 k i r 2i d . (3.3)</formula><p>This relates the distance of a point from the distortion center, before and after distortion. From this, it is easy to extract the relationship between the radial distance in the distorted image and the incidence angle of the camera ray with the optical axis:</p><formula xml:id="formula_16">θ = atan r u f = atan r d 1 + n i=1 k i r 2i d f ,<label>(3.4)</label></formula><p>which depends on the focal length of the camera.</p><p>Let us now consider a radial distortion with a single coefficient k 1 . We can write down the camera ray direction in the following form:</p><formula xml:id="formula_17">B i ∼ K -1   x d + k 1 xd r 2 d y d + k 1 ȳd r 2 d 1   ∼   (x d -x 0 )(1 + k 1 r 2 d ) (y d -y 0 )(1 + k 1 r 2 d ) f   ∼ B i 3×10 L 3 (q d ),</formula><p>where the lifting operation L 3 (•) is given in Equation (1.4) and the back-projection matrix is:</p><formula xml:id="formula_18">B i ∼   k1 0 k1 0 -3k1x0 -2k1y0 -k1x0 1 + k1a 2k1x0y0 -x0(1 + k1c) 0 k1 0 k1 -k1y0 -2k1x0 -3k1y0 2k1x0y0 1 + k1b -y0(1 + k1c) 0 0 0 0 0 0 0 0 0 f   , with a = 3x 2 0 + y 2 0 , b = x 2 0 + 3y 2 0 and c = x 2 0 + y 2 0 .</formula><p>Since the first and third columns are equal, as well as the second and fourth ones, we may use a more compact back-projection matrix of size 3 × 8:</p><formula xml:id="formula_19">B i c ∼ k1 0 -3k1x0 -2k1y0 -k1x0 1 + k1a 2k1x0y0 -x0(1 + k1c) 0 k1 -k1y0 -2k1x0 -3k1y0 2k1x0y0 1 + k1b -y0(1 + k1c) 0 0 0 0 0 0 0 f</formula><p>, which operates on lifted image coordinate vectors of the form:</p><formula xml:id="formula_20">             q 3 1 + q 1 q 2 2 q 2 1 q 2 + q 3 2 q 2 1 q 3 q 1 q 2 q 3 q 2 2 q 3 q 1 q 2 3 q 2 q 2 3 q 3 3              .</formula><p>If we assume a known principal point (equal to the distortion center here) and translate image points such that the principal point lies at the origin, then the back-projection equations reduce further to:</p><formula xml:id="formula_21">B i c ∼   k 1 0 0 0 0 1 0 0 0 k 1 0 0 0 0 1 0 0 0 0 0 0 0 0 f   ,</formula><p>which can be compacted similarly to the above:</p><formula xml:id="formula_22">B i cc ∼   k 1 0 1 0 0 0 k 1 0 1 0 0 0 0 0 f   operating on        q 3 1 + q 1 q 2 2 q 2 1 q 2 + q 3 2 q 1 q 2 3 q 2 q 2 3 q 3 3        .</formula><p>Back-projection matrices for combined radial and tangential distortion models are equally straightforward to derive. Note that this back-projection formulation allows to easily write down equations for the epipolar geometry, cf. Section 4.2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inversion of models.</head><p>Devernay and Faugeras discussed the analytical inversion of the classical radial distortion model with one coefficient, using Cardan's method. This analysis was adapted by Ma et al. to other variants of polynomial radial distortion models, especially rational polynomials <ref type="bibr" target="#b326">[327]</ref>, cf. also Section 3.1.8. Heikkilä showed how to formulate an approximate inverse model for radial and tangential distortions <ref type="bibr" target="#b214">[215]</ref>. A more detailed and accurate inverse model was proposed by Mallon and Whelan <ref type="bibr" target="#b329">[330]</ref>.</p><p>Other models and works. The standard radial and tangential distortion model is a backward one, going from distorted to undistorted coordinates. Often, the same model is also employed for forward projection, see e.g., <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b215">216,</ref><ref type="bibr" target="#b524">525,</ref><ref type="bibr" target="#b520">521]</ref>, in which case the model coefficients' values will differ of course.</p><p>Many researchers have used other polynomial models for distortion/ undistortion, e.g., the so-called polynomial fisheye transform (PFET) by Basu and Licardie <ref type="bibr" target="#b40">[41]</ref>, which is nothing else than a general polynomial relating r d and r u , including even terms. Such models were for example applied for endoscope calibration, see <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b203">204,</ref><ref type="bibr" target="#b453">454,</ref><ref type="bibr" target="#b564">565]</ref>. Other calibration approaches using such models are <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b133">134,</ref><ref type="bibr" target="#b147">148,</ref><ref type="bibr" target="#b214">215,</ref><ref type="bibr" target="#b215">216,</ref><ref type="bibr" target="#b273">274,</ref><ref type="bibr" target="#b301">302,</ref><ref type="bibr" target="#b329">330,</ref><ref type="bibr" target="#b380">381,</ref><ref type="bibr" target="#b435">436,</ref><ref type="bibr" target="#b436">437,</ref><ref type="bibr" target="#b513">514,</ref><ref type="bibr" target="#b514">515,</ref><ref type="bibr" target="#b520">521,</ref><ref type="bibr" target="#b524">525,</ref><ref type="bibr" target="#b533">534]</ref>. These approaches often differ with respect to the choice of calibration object, potentially carrying out controlled motions to position it in front of the camera, and with respect to the strategy for estimating the unknowns (e.g., estimating some parameters first while keeping others at initial values).</p><p>Kölbl proposed to use a trigonometric series to model radial distortion, in order to better condition the normal equations occurring in distortion estimation <ref type="bibr" target="#b282">[283]</ref>. Several other distortion models are described in <ref type="bibr" target="#b274">[275,</ref><ref type="bibr" target="#b275">276]</ref>.</p><p>Shih et al. established an approach for evaluating the errors made when neglecting lens distortion during calibration <ref type="bibr" target="#b448">[449]</ref>. This is a function of the actual amount of true distortion, the number and positions/distribution of calibration points, the amount of noise and possibly other factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Very wide fields of view.</head><p>It is well known that the above classical radial distortion model is not appropriate for cameras with a very wide field of view. To see this, refer to Equation (3.4): in order to achieve a hemispheric field of view (θ = 90 • ), one would need r d and thus the extent of the image area, to be infinitely large. Many alternative models exist, described in subsequent sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Extensions for Modeling Zooming and Focusing</head><p>In <ref type="bibr" target="#b63">[64]</ref>, Brown explained that lens distortion varies with a camera's focus setting but that knowledge of the distortion for two focal settings allows to interpolate the distortion for any other setting. Also, radial distortion depends on the distance of the object to the camera. These effects are rigorously proven but seem to be used relatively seldomly in practice.</p><p>There exist various approaches for modeling zoom lenses and the influence of zooming on the intrinsic parameters, as well as for modeling other intricate phenomena related to camera calibration, see e.g., <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr" target="#b99">100,</ref><ref type="bibr" target="#b101">102,</ref><ref type="bibr" target="#b127">128,</ref><ref type="bibr" target="#b149">150,</ref><ref type="bibr" target="#b312">313,</ref><ref type="bibr" target="#b468">469,</ref><ref type="bibr" target="#b530">531,</ref><ref type="bibr" target="#b532">533,</ref><ref type="bibr" target="#b531">532]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Two-Plane Model</head><p>Chen et al. introduced implicit camera models in computer vision <ref type="bibr" target="#b88">[89,</ref><ref type="bibr" target="#b89">90]</ref>. Their two-plane model is mostly known through the subsequent paper by Martins et al. <ref type="bibr" target="#b330">[331]</ref> and follow-up papers by Wei and Ma <ref type="bibr" target="#b521">[522,</ref><ref type="bibr" target="#b523">524]</ref> and others. As explained in the introduction of this section, the two-plane model is rather rich in ideas, which is why it will be covered in some detail here.</p><p>The model is defined as follows <ref type="bibr" target="#b330">[331]</ref>. Consider two images of a planar calibration grid, put in known positions and assume that we can extract matches between the image and grid in both images. Since the grid positions are known, points on the grid actually define a 3D calibration object, which may be used as input to classical calibration approaches. The motivations for the two-plane model are to not rely on pre-defined camera models for the calibration and to handle noncentral cameras. This may be achieved as follows. If we have dense matches, we may compute, for each image point for which we have two matches, the associated camera ray, simply by computing the line spanned by the two matched grid points. This idea was fully introduced later, by Gremban et al. <ref type="bibr" target="#b188">[189]</ref> and Grossberg and Nayar <ref type="bibr" target="#b190">[191]</ref>. Martins et al. did not assume dense matches; hence, in order to compute camera rays, one must have recourse to some interpolation scheme. Basically, for an image point we want to back-project, we need to determine the corresponding points on the two grids. To do so, Martins et al. proposed three interpolation schemes to compute corresponding points on calibration grids, from extracted matches: two global schemes (linear </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>First version -"Linear interpolation".</head><p>The mapping between the image plane and each planar calibration grid was supposed to be an affine transformation:</p><formula xml:id="formula_23">  X i Y i Z i   = A i q,</formula><p>where i = 1, 2 is the index of the grid and</p><formula xml:id="formula_24">A i =   A i,11 A i,12 A i,13 A i,11 A i,12 A i,13 0 0 Z i   ,</formula><p>and it is assumed that q 3 = 1. Here, Martins et al. assumed that the two calibration grids are parallel to the X, Y plane, at known depths Z i .</p><p>They stated that other cases can also be handled similarly, but did not develop this further.</p><p>A more proper formulation using homogeneous coordinates also for 3D points, would separate the pose of the grid, parameterized by a rotation and translation, from the interpolation function "inside" the plane, which can be an affine transformation (as in Martins et al.'s work) or a projective one:</p><formula xml:id="formula_25">Q i ∼ R i t i 0 T 3 1 pose matrix      1 0 0 0 1 0 0 0 0 0 0 1      M A i q ∼ Ri t i 0 T 2 1 4×3 A i q,</formula><p>where Ri consists of the first two columns of R i and M maps 2D coordinates on the grid plane, to 3D coordinates. The computation of the interpolation mappings A i (i.e., the calibration) would be carried out between image points and 2D points on calibration grids, instead of 3D points as in the original formulation above. The back-projection ray of an image point q can be computed from the two grid points Q 1 and Q 2 according to Equation (1.1) in Section 1.2. Let P i = Ri t i . Then:</p><formula xml:id="formula_26">B l ∼ (A 2 q) 3 P 1 A 1 q -(A 1 q) 3 P 2 A 2 q (P 1 A 1 q) × (P 2 A 2 q) .</formula><p>This expression is quadratic in the coordinates of the image point q.</p><p>It is thus clear that back-projection can be written via a 6 × 6 back-projection matrix:</p><formula xml:id="formula_27">B l ∼ B l 6×6 L 2 (q) (3.5)</formula><p>operating on lifted image point coordinates L 2 (q) (cf. Equation (1.4)).</p><p>In the special case of calibration planes parallel to the XY -plane and of affine transformations A i , we get:</p><formula xml:id="formula_28">B l =      0 0 0 A 1,11 -A 2,11 A 1,12 -A 2,12 A 1,13 -A 2,13 0 0 0 A 1,21 -A 2,21 A 1,22 -A 2,22 A 1,23 -A 2,23 0 0 0 0 0 Z 1 -Z 2 0 0 0 Z 2 A 1,21 -Z 1 A 2,21 Z 2 A 1,22 -Z 1 A 2,22 Z 2 A 1,23 -Z 1 A 2,23 0 0 0 Z 1 A 2,11 -Z 2 A 1,11 Z 1 A 2,12 -Z 2 A 1,12 Z 1 A 2,13 -Z 2 A 1,13 b 1 b 2 b 3 b 4 b 5 b 6      , (3.6)</formula><p>where the last row is given by the vector</p><formula xml:id="formula_29">b =          A 1,11 A 2,21 -A 1,21 A 2,11 A 1,11 A 2,22 + A 1,12 A 2,21 -A 1,21 A 2,12 -A 1,22 A 2,11 A 1,12 A 2,22 -A 1,22 A 2,12 A 1,11 A 2,23 + A 1,13 A 2,21 -A 1,21 A 2,13 -A 1,23 A 2,11 A 1,12 A 2,23 + A 1,13 A 2,22 -A 1,22 A 2,13 -A 1,23 A 2,12 A 1,13 A 2,23 -A 1,23 A 2,13          .</formula><p>If instead of affine transformations, projective ones are adopted for A 1 and A 2 , then the back-projection matrix B l has no zero coefficients anymore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Second version -"Quadratic interpolation".</head><p>Here, the interpolation function between the image plane and each of the calibration grids, is assumed to be quadratic, and modeled by:</p><formula xml:id="formula_30">  X i Y i Z i   = A i L 2 (q),</formula><p>with 3 × 6 matrices A i . Obviously, camera rays can be expressed via a back-projection matrix operating on degree-4 liftings of the image coordinates:</p><formula xml:id="formula_31">B l ∼ B l 6×15 L 4 (q).</formula><p>Third version -"Linear spline interpolation". This is a local camera model according to our definition and will thus be explained in Section 3.2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Observations.</head><p>In <ref type="bibr" target="#b330">[331]</ref> it is noted that "The basic difference between this model and the pinhole model is that all lines of sight are not forced to go through the same point". A second major difference is that this model allows to handle non-perspective distortions that may in principle be quite general.</p><p>The linear two-plane model is actually identical to the general linear camera (GLC) model of Yu and McMillan <ref type="bibr" target="#b558">[559]</ref>, cf. Section 3.4.4, although it is formulated differently. Also, whereas the two-plane model was proposed in order to interpolate calibration data, the GLC was introduced in order to generate novel spatial distributions of camera rays, with applications for example in image synthesis.</p><p>The linear and quadratic two-plane models are particular instances of rational polynomial camera models, which were later introduced in full generality, cf. Section 3.1.8. Also, the two-plane models may be among the first to explicitly use lifted image coordinates in their formulation.</p><p>Finally, as mentioned above, they inspired works on discrete camera models; with the possibility of getting dense matches, it is indeed natural to calibrate camera rays directly for sampled image positions instead of computing interpolation transformations, as proposed by Gremban et al. <ref type="bibr" target="#b188">[189]</ref>, Grossberg and Nayar <ref type="bibr" target="#b190">[191]</ref>, and others, see Section 3.3.</p><p>Extensions. Izaguirre et al. showed how to incorporate the camera motion in the back-projection matrix <ref type="bibr" target="#b250">[251]</ref>. Since we have explicitly written the back-projection, this can also be done easily by combining back-projection (e.g., Equation (3.5) for the linear two-plane model) with extrinsic parameters, as explained in Equation (1.2).</p><p>The two-plane model is formulated as a back-projection; Gremban et al. proposed an approach to deduce the forward projection operation <ref type="bibr" target="#b188">[189]</ref>. In addition, they already posed the basis for a fully ray-based camera calibration, which is treated in Section 3.3.</p><p>The two-plane model was further generalized by Wei and Ma in several articles, as follows. In <ref type="bibr" target="#b521">[522]</ref> necessary and sufficient constraints were derived for the back-projection matrix to correspond to a perspective back-projection, i.e., when no non-perspective distortion is present. In <ref type="bibr" target="#b522">[523,</ref><ref type="bibr" target="#b523">524]</ref> the authors also modeled the forward projection, using the same algebraic model but with independent parameters and estimation processes. There, they also investigated the epipolar geometry associated with their camera model. The findings of <ref type="bibr" target="#b523">[524]</ref> do not seem to be fully general since both for the forward projection and the epipolar geometry, the existence of an optical center of the cameras seems to be assumed, whereas the considered camera model in general corresponds to non-central cameras.</p><p>It would be worthwhile to establish the conditions under which two cameras following the two-plane model, form a stereo pair, along the lines of Seitz and Kim's study <ref type="bibr" target="#b444">[445]</ref>, see Section 3.4.2. Further, it would be interesting to establish necessary and sufficient constraints on the back-projection matrices under which the camera is central.</p><p>Champleboux et al. extended the two-plane model in two respects <ref type="bibr" target="#b84">[85]</ref>. First, by allowing the use of more than two images of calibration planes. Second, by modeling the function that maps image points to lines of sight, using bicubic B-splines. They applied their approach to the calibration of cameras but also of radiographic and range sensors.</p><p>Fiala and Basu used a 1D version of the two-plane model to calibrate the radial distortion profile of catadioptric cameras <ref type="bibr" target="#b142">[143]</ref>.</p><p>Lavest, Delherm et al. used the two-plane model to calibrate zoom cameras for different zoom settings and to obtain 3D reconstructions from images taken with different zoom factors but without moving the camera <ref type="bibr" target="#b108">[109,</ref><ref type="bibr" target="#b299">300,</ref><ref type="bibr" target="#b300">301]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Models for Slit Cameras</head><p>Consider slit image acquisition proceeding by moving a 1D perspective camera with a constant translational velocity while acquiring images, which are stacked together to form a 2D image, i.e., a linear pushbroom panorama (cf. Section 2.1.1 and <ref type="bibr" target="#b196">[197]</ref>). It is obviously an axial camera since all viewpoints lie on a straight line due to the camera's translational motion. In addition, all camera rays, for all acquisitions, cut the plane at infinity in the same line -the intersection of the 1D camera's view plane with the plane at infinity. Linear pushbroom cameras are thus a special case of two-slit camera, with one slit being a line at infinity <ref type="bibr" target="#b388">[389]</ref>.</p><p>Gupta and Hartley proposed models for the linear pushbroom panorama and its multi-view geometry <ref type="bibr" target="#b196">[197]</ref>. They showed that forward projection can be modeled by a 3 × 4 projection matrix, which relates 3D point coordinates with lifted coordinates of the corresponding image point. They also showed expressions for back-projection. Let us re-derive them here. Let the 1D camera be modeled by the following back-projection operations, where q are the homogeneous coordinates of image points:</p><formula xml:id="formula_32">B f = t 1 + q 2 q 3 v 0 ∼ q 3 t 1 + q 2 v 0 B i = a 0 b q = q 1 a + q 3 b.</formula><p>Here, the second image coordinate is assumed to correspond to the acquisition time and v gives the translational velocity of the camera; t is the optical center position at the first acquisition and vectors a and b depend on the 1D camera's intrinsic parameters and orientation. The latter are constant, thus the point at infinity of a back-projected ray does not depend on the second image coordinate (which explains the column of zeroes in the second above equation).</p><p>The Plücker coordinates of camera rays may now be expressed as follows, based on the above two equations defining B f and B i (cf. Equation (1.2)):</p><formula xml:id="formula_33">B l = -q 3 (q 1 a + q 3 b) (q 3 t + q 2 v) × (q 1 a + q 3 b) = 0 -a 0 -b v × a t × a v × b t × b B l 6×4      q 1 q 2 q 1 q 3 q 2 q 3 q 2 3      .</formula><p>The geometry of more general two-slit cameras was studied by Pajdla and Zomet et al. <ref type="bibr" target="#b388">[389,</ref><ref type="bibr" target="#b575">576]</ref>. Among other issues, Zomet et al. derived the forward projection equations and have shown that 3D lines are imaged as conics. Feldman et al. showed that the back-projection for the same two-slit cameras can be expressed via a 6 × 6 back-projection matrix, operating on second order lifted image coordinates <ref type="bibr" target="#b138">[139]</ref>. As will be seen later (Sections 4.2.1 and 4.3), once it is established that backprojection can be written as a linear mapping operating on order-2 lifted image coordinates, it follows automatically that 3D lines are imaged as conics and that there exists a fundamental matrix of size 6 × 6.</p><p>Models for other slit image systems are in principle straightforward to establish. Consider the case of a 1D camera rotated about some axis (not necessarily containing the 1D camera's optical center). Let the axis be the Z-axis and the optical center at the first acquisition (q 2 = 0) be at position t. Further, let the intrinsic parameters and orientation of the 1D camera be subsumed by vectors a and b, as above for the linear pushbroom case. Then, full back-projection operations can be written as:</p><formula xml:id="formula_34">B f =      cos(kq 2 /q 3 ) -sin(kq 2 /q 3 ) 0 0 sin(kq 2 /q 3 ) cos(kq 2 /q 3 ) 0 0 0 0 1 0 0 0 0 1      t 1 B i =  </formula><p>cos(kq 2 /q 3 )sin(kq 2 /q 3 ) 0 sin(kq 2 /q 3 ) cos(kq 2 /q 3 ) 0 0 0 1   (q 1 a + q 3 b).</p><p>Here, k expresses the rotational velocity and q 2 /q 3 the time. Due to the trigonometric expressions in the rotation matrix, it is not possible to write the back-projection in polynomial form though and for example to establish fundamental matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5">Models for Central Catadioptric Cameras</head><p>Geyer and Daniilidis proposed a unifying model for single-mirror central catadioptric systems (cf. Section 2.3.1) <ref type="bibr" target="#b163">[164,</ref><ref type="bibr" target="#b164">165]</ref>. It is embodied by a two-step projection procedure, defined by a unit sphere and a pinhole camera, as follows (cf. Figure <ref type="figure">3</ref>.2). First, a 3D point is mapped onto the sphere, by intersecting the sphere with the line spanned by the point and the sphere's center. Second, that intersection point is projected into a (virtual) pinhole camera. Geyer and Daniilidis showed that this model subsumes all single-mirror central catadioptric systems An image point is first back-projected relative to the virtual pinhole camera. The intersections of the camera ray with the sphere are determined. The final camera rays are the lines spanned by the intersections points and the center of the sphere. For clarity, the second final camera ray is shown as a dashed line.</p><p>(see Section 2.3.1 for a list of these); the center of the sphere is the effective viewpoint of the modeled system. Note that the first step, projection onto the sphere, has two mathematical solutions; in practice, one usually knows which one to pick but when formulating multi-view geometry for instance, one may have to carry along both solutions. Also note that the virtual pinhole camera of the model is not equivalent to the camera looking at the actual mirror in the true catadioptric system; it is just an algebraic model. Back-projection with this model can be formulated as follows. Without loss of generality, let us assume that the sphere is of radius 1 and centered in the origin. Let the projection matrix of the virtual pinhole camera be given as:</p><formula xml:id="formula_35">M -1 3×3 I 3×3 -t .</formula><p>Then, an image point is back-projected, relative to the virtual pinhole, as:</p><formula xml:id="formula_36">B f v ∼ t 1 B i v ∼ Mq.</formula><p>The back-projection ray intersects the sphere in two points, written in homogeneous coordinates as:</p><formula xml:id="formula_37">(Mq) T (Mq)t -(t T Mq)Mq ± √ DMq (Mq) T (Mq) , with D = (t T Mq) 2 -(t T t -1)(Mq)(Mq) T .</formula><p>The final back-projection directions are then given by the first three coordinates of these two points (since the sphere center is the origin):</p><formula xml:id="formula_38">B i ∼ (Mq) T (Mq)t -(t T Mq)Mq ± √ DMq.</formula><p>Due to the square root in this expression, it is in general not possible to directly obtain polynomial expressions. <ref type="foot" target="#foot_0">1</ref> For the special case where the virtual pinhole lies on the sphere though, i.e., for t T t = 1, the two directions are:</p><formula xml:id="formula_39">B i 1 ∼ t B i 2 ∼ (Mq) T (Mq)t -2(t T Mq)Mq.</formula><p>This special case corresponds to para-catadioptric cameras. The first direction is independent of the image point q and can thus be discarded; the second one can be written via a 3 × 6 back-projection matrix B which depends on M and t:</p><formula xml:id="formula_40">B i 2 ∼ B i 3×6 L 2 (q).</formula><p>Let us finally consider the usual special case where the principal point is supposed to be known and to be the origin and the optical axis of the camera to be aligned with the mirror axis. Further, without loss of generality, we assume that the camera's optical center lies on the Z-axis. Then, M ∼ diag(1, 1, k) and t T = (0, 0, -1) and the backprojection becomes:</p><formula xml:id="formula_41">B i 2 ∼   0 0 0 -2k 0 0 0 0 0 0 -2k 0 1 0 1 0 0 -k 2   L 2 (q).</formula><p>The first and third columns are identical and the second one only contains zeroes. We can thus write the back-projection in a more compact form as:</p><formula xml:id="formula_42">B i 2 ∼   0 2k 0 0 0 0 2k 0 -1 0 0 k 2        q 2 1 + q 2 2 q 1 q 3 q 2 q 3 q 2 3      .</formula><p>Geyer and Daniilidis analyzed various properties of catadioptric systems using their model, among which the projection of points and lines, the concepts of projective duality, epipolar geometry, etc. Barreto and Araújo proposed a modified version of this model and used it to study various geometric properties, especially those related to line images and their application for calibration <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. Mei and Rives proposed to extend the model by adding classical terms for radial and tangential distortion <ref type="bibr" target="#b338">[339]</ref>.</p><p>A similar model was previously proposed by Smith et al. <ref type="bibr" target="#b452">[453]</ref>. Their two-step model is defined by an arbitrary quadric surface, an effective viewpoint and a direction for an orthographic projection. A 3D point is first mapped onto the quadric surface by intersecting the line spanned by the point and the effective viewpoint, with the quadric. The intersection point is then orthographically projected to give the final image point. In practice, Smith et al. assumed that the quadric's highest point, in the direction of the orthogonal projection, lies above the effective viewpoint; the orthographic projection of the effective viewpoint itself is also considered as a distortion center. The final model used by Smith et al. has four parameters and it can model non-radially symmetric systems. It subsumes central para-catadioptric cameras as well as the division model (cf. Section 3.1.8) but not hyper-catadioptric cameras.</p><p>Ying and Hu proposed another extension by replacing the sphere with a general quadric of revolution <ref type="bibr" target="#b549">[550]</ref>. One of the quadric's foci is chosen as effective viewpoint. The projection of a 3D point is modeled in two steps analogous to Geyer and Daniilidis' model: the line spanned by the 3D point and the effective viewpoint is cut by the quadric. The two intersection points are then projected into a virtual pinhole camera whose optical center is located on the mirror's revolution axis. This gives the two theoretical images of a 3D point. Back-projection follows the inverse procedure. Ying and Hu showed that this model, besides central catadioptric cameras, also subsumes some fisheye models, such as stereographic projection (cf. Section 3.1.7) and the division model. A similar extension was proposed by Barreto, who showed that when replacing the sphere with a paraboloid, one may represent the division model <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>Bayro-Corrochano and López-Franco rewrote the model of Geyer and Daniilidis in terms of conformal geometry and gave the corresponding formulas for forward projection of points and lines and the backprojection of image points <ref type="bibr" target="#b43">[44]</ref>. Tolvanen et al. expressed the model of Geyer and Daniilidis in terms of Clifford algebra <ref type="bibr" target="#b506">[507]</ref>. Perwass and Sommer proposed a model similar to Geyer and Daniilidis', expressed in geometrical algebra terms; it subsumes the para-catadioptric and division models <ref type="bibr" target="#b398">[399]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.6">Non-central Catadioptric Cameras</head><p>Consider any non-central catadioptric system. Even if everything is known about the system, i.e., the shape of the mirror, the calibration of the camera looking at it as well as the relative camera-mirror pose, forward projection is a difficult problem. As for back-projection, it suffices to first back-project an image point relative to the camera, to find the (first) intersection point of the back-projected ray with the mirror and to reflect the ray according to Snell's law. For some setups, back-projection may be written analytically, e.g., for spherical or coneshaped mirrors.</p><p>Forward projection is much harder since it is not immediately given at which point on the mirror the reflection happens. Again, for some cases, analytical models can be formulated but in general, there is no closed-form solution for this problem, so researchers have proposed to solve it by non-linear optimization. One possibility is for example to optimize the desired image point coordinates, where the cost function is the distance between the back-projected and reflected ray, and the 3D point whose projection is sought. Another possibility is to optimize the reflection point on the mirror, using for example a similar cost function: the distance between the camera's optical center and the line joining that point and the 3D point, after reflecting that line in the mirror.</p><p>The special case of quadric-shaped mirrors was investigated by Gonçalves and Araújo who showed how to formulate the problem via three polynomials and to reduce it to an optimization in a single variable <ref type="bibr" target="#b178">[179]</ref>. Vandeportaele also characterized the problem using polynomials and studied the number and nature of roots for the general as well as special cases of quadric-shaped mirrors <ref type="bibr" target="#b516">[517]</ref>. Agrawal et al. contributed similar results for different types of quadric-shaped mirrors as well as for the case of cameras looking through refractive spheres <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.7">Fisheye Models</head><p>Classical models. Several models were suggested for fisheye lenses (or rather, the opposite: fisheye lenses are constructed in order to satisfy such models) <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b145">146,</ref><ref type="bibr" target="#b225">226,</ref><ref type="bibr" target="#b349">350,</ref><ref type="bibr" target="#b535">536]</ref> -stereographic, equiangular,<ref type="foot" target="#foot_1">2</ref> sine-law, and equi-solid angle projection:</p><formula xml:id="formula_43">r d = k tan(θ/2)</formula><formula xml:id="formula_44">r d = kθ r d = k sin θ r d = k sin(θ/2).</formula><p>Their back-projection equations are given in Table <ref type="table">3</ref>.1 (where the coefficient k is replaced by f to homogenize notations). We observe that stereographic back-projection can be written using polynomials; it can be written via a 3 × 4 back-projection matrix operating on lifted image coordinates (here, we suppose that the principal point is known and located at the origin) and that it is (not surprisingly) equivalent to the back-projection of para-catadioptric cameras. The other three models do not have back-projection expressions in terms of polynomials.</p><p>In <ref type="bibr" target="#b145">[146]</ref>, Fleck compared these models with respect to several criteria, among which the maximum theoretical field of view and light fall-off. The stereographic model is preferred by Fleck, due to representing a good compromise between field of view, light fall-off, and a good approximate local preservation of Euclidean shapes. It seems though that most fisheyes are designed to approach the equiangular model, which is thus the model used most often for fisheye camera calibration.</p><p>Hall et al. for example used an equiangular model for a fisheye lensbased imaging system used for robot guidance <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b381">382,</ref><ref type="bibr" target="#b382">383]</ref>.</p><p>An experimental evaluation of some of the above fisheye models for a particular camera was performed by Schneider et al. <ref type="bibr" target="#b438">[439]</ref>.</p><p>As a sidenote, let us remark that interestingly, some of the first scientific applications of fisheye lenses concerned the area of forest management (see http://en.wikipedia.org/wiki/ Hemispherical photography), where they were already used in the 1950s. Researchers in agricultural and forestry departments came up with calibration methods, independently from photogrammetrists. For example, Clark and Follin used the equiangular model for fisheyes <ref type="bibr" target="#b93">[94]</ref>. Their application was forest management and panoramic images were used for assessing light interception and leaf areas in forests. Therefore, they were interested in images with equi-area properties, i.e., where the area of an image region is proportional to the solid angle of the corresponding field of view. In <ref type="bibr" target="#b93">[94]</ref>, they assessed how well different fisheye lenses approach this property.</p><p>Other models. Various other models were proposed for fisheye lenses, or lenses with a wide field of view in general, some of them aiming at subsuming several of the above classical models.</p><p>Herbert proposed the following alternative model for fisheye cameras <ref type="bibr" target="#b218">[219]</ref>:</p><formula xml:id="formula_45">r d = θ + 5 i=1 a i sin i π θ θ max ,</formula><p>where θ max is the camera's field of view (assumed known).</p><p>Various fisheye lens designs and some performance characteristics were presented by Kumler and Bauer <ref type="bibr" target="#b293">[294]</ref>. They used the following model:</p><formula xml:id="formula_46">r d = k sin(βθ),</formula><p>which encompasses the sine-law and equi-solid angle models shown above. Also, if β is very small, this model approaches the equiangular one.</p><p>A similar general model was proposed by Gennery <ref type="bibr" target="#b160">[161]</ref>:</p><formula xml:id="formula_47">r d = sin(βθ) k cos(max(0, βθ))</formula><p>.</p><p>For β = 1 this gives the pinhole model, for β = 0.5 stereographic projection, β = -0.5 gives equi-solid angle projection, and β = -1 the sine-law projection. As β approaches zero, this model approaches the equiangular one. Gennery suggested to couple this model (which subsumes ideal fisheye models) with a classical polynomial distortion model. Gennery's full model includes also the position of the effective optical center along the optical axis, as a function of θ or r d ; his full model is thus a non-central one (an axial model to be precise) and resembles in that respect that of Tardif et al. <ref type="bibr" target="#b490">[491]</ref>, see Section 3.3.2. Besides this effect, Gennery also included the orientation of the optical axis relative to the image plane, in order to model non-perpendicularity. Yet another similar model was suggested by Bakstein and Pajdla <ref type="bibr" target="#b22">[23]</ref>, who used a combination of the stereographic and equi-solid angle models, depending on four parameters:</p><formula xml:id="formula_48">r d = a tan θ b + c sin θ d .</formula><p>Basu and Licardie proposed a distortion function called fisheye transform (FET) <ref type="bibr" target="#b40">[41]</ref>, inspired by the work of Schwartz on the geometry of the human visual system <ref type="bibr" target="#b440">[441]</ref>:</p><formula xml:id="formula_49">r d = s log(1 + λr u ) r u = e r d /s -1 λ .</formula><p>Although it is called fisheye transform, this model does not handle fields of view of 180 • or higher. Devernay and Faugeras proposed the FOV model (field-of-view model) <ref type="bibr" target="#b112">[113]</ref>:</p><formula xml:id="formula_50">r u = tan(r d ω) 2 tan ω 2 .</formula><p>According to their description, the model is equivalent to the equiangular model. However, this seems to be the case only for particular values of the parameter ω (cf. Figure <ref type="figure">3</ref>.4(c) in Section 3.5). If we include the focal length of the perspective projection in the model, we get the following expression for the back-projection angle θ:</p><formula xml:id="formula_51">θ = atan tan(r d ω) 2f tan ω 2 . For f = 1 2 tan 1 2ω</formula><p>the FOV model becomes equivalent to the equiangular model:</p><formula xml:id="formula_52">θ = r d ω.</formula><p>Like Gennery (see above), Devernay and Faugeras suggested to complement their model (or other fisheye models) by additional terms, e.g., the classical polynomial radial distortion model, to calibrate cameras.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.8">Polynomial and Rational Polynomial Models</head><p>Most of the models in the previous section were based on trigonometric functions, whereas polynomial expressions were used in the classical distortion models of Section 3.1.1. The two-plane models of Section 3.1.3 were also embodied by polynomials. In this section, we review several other polynomial-based models.</p><p>A first possibility is to model radial distortion using polynomials, but instead of relating the distorted and undistorted radial distances as in Equation (3.3), one considers the relation between distorted radial distances and incidence angles between camera rays and the optical axis:</p><formula xml:id="formula_53">r d = n i=1 c i θ i . (3.7)</formula><p>This model was for example used by Herbert <ref type="bibr" target="#b218">[219]</ref>, Xiong and Turkowksi <ref type="bibr" target="#b538">[539]</ref>, Kannala and Brandt <ref type="bibr" target="#b265">[266]</ref>, Kannada et al. <ref type="bibr" target="#b266">[267]</ref>, Ying et al. <ref type="bibr" target="#b552">[553]</ref>, and Stehle et al. <ref type="bibr" target="#b458">[459]</ref>. Kannala and Brandt further added non-polynomial terms to account for tangential distortions <ref type="bibr" target="#b265">[266]</ref>. They also showed an approximation of the inverse (backward) model. Note that the model in Equation (3.7) subsumes the equiangular model, which corresponds to using only the first coefficient c 1 .</p><p>The reciprocal is of course also possible, i.e., models of the form:</p><formula xml:id="formula_54">θ = n i=1 c i r i d .</formula><p>This was used for example by Inoue et al. where a 12-th degree polynomial was found to be suitable to calibrate a fisheye converter <ref type="bibr" target="#b244">[245]</ref>. Beauchemin et al. proposed a polynomial model, but where polynomials relate polar image point coordinates before and after distortion, instead of Cartesian ones <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref>. Their model consists of two polynomials, one for the radial distortion function (distance from distortion center), one for the radial angle, capturing tangential distortions.</p><p>Hellmeier as well as Remy et al. used splines to model the 1D distortion function <ref type="bibr" target="#b217">[218,</ref><ref type="bibr" target="#b426">427]</ref>.</p><p>Rational polynomials for radial distortion. Finally, several researchers have proposed models based on rational polynomials, as follows.</p><p>Fitzgibbon and independently, Bräuer-Burchardt and Voss, proposed the so-called division model, which has one distortion coefficient c <ref type="bibr" target="#b144">[145,</ref><ref type="bibr" target="#b61">62]</ref>. Before this, that model was already used by Lenz and Tsai <ref type="bibr" target="#b304">[305,</ref><ref type="bibr" target="#b305">306]</ref> (see also the textbook <ref type="bibr" target="#b252">[253,</ref><ref type="bibr" target="#b253">254]</ref> by Jähne); however, they used it as forward model, i.e., in the below equations, one would swap the forward and back-projection equations. The division model as defined by Fitzgibbon and Bräuer-Burchardt and Voss, is given as:</p><formula xml:id="formula_55">r u = r d 1 -cr 2 d r d = 1 + 4cr 2 u -1 2cr u .</formula><p>A nice feature is that the model has a closed-form inverse.</p><p>The division model can be generalized by using general rational polynomials to relate r u and r d , as proposed by Ma et al. <ref type="bibr" target="#b326">[327]</ref>. Rational polynomials were already used before the introduction of the division model, e.g., by Hellmeier <ref type="bibr" target="#b217">[218]</ref>.</p><p>Mičušik and Pajdla <ref type="bibr" target="#b345">[346]</ref> and Mičušik <ref type="bibr" target="#b343">[344]</ref> used a similar model for the relation between image distance r d and the angle between optical axis and camera rays, instead of the relation between image distances in distorted and undistorted images, as above. Their model has two parameters and is of the form:</p><formula xml:id="formula_56">θ = ar d 1 + br 2 d .</formula><p>It is in some sense a division model for the equiangular fisheye model, which it extends. Like for the equiangular model, expressing backprojection directions involves trigonometric functions; hence, Mičušik and Pajdla proposed to linearize their back-projection model using a Taylor development in order to perform an initial estimation of the model parameters during (self-)calibration.</p><p>Rational polynomials for full back-projection models. Instead of using rational functions to model the 1D distortion function, as above, one may use them for the actual back-projection mapping, as foreshadowed by the two-plane methods of Chen and Martins et al. <ref type="bibr" target="#b88">[89,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b330">331]</ref> (see Section 3.1.3) and fully introduced by Claus and Fitzgibbon <ref type="bibr" target="#b97">[98]</ref>. In general, one may write general mappings between distorted and undistorted image points:</p><formula xml:id="formula_57">L a (q u ) ∼ M L b (q d ),</formula><p>where the matrix M would contain the coefficients of homogeneous polynomials relating the lifted coordinates in the distorted and undistorted image planes (for full generality, we assume that different liftings may be possible, i.e., a = b). We speak of rational polynomial models here since "undoing" homogeneous point coordinates involves divisions of coordinates.</p><p>It is more common though to use this kind of model directly for the back-projection mapping of distorted image points to back-projection directions or rays:</p><formula xml:id="formula_58">B i ∼ B i L b (q d ),<label>(3.8)</label></formula><formula xml:id="formula_59">B l ∼ B l L b (q d ). (3.9)</formula><p>In practice, Claus and Fitzgibbon used a bi-quadratic such model for central cameras, i.e., b = 2 and B i of size 3 × 6 <ref type="bibr" target="#b97">[98]</ref>. It is easy to see that this encompasses the division model as well as the para-catadioptric and stereographic models. Note that in order to represent the classical radial distortion model, higher order liftings are required (cubic in the case of a single distortion coefficient, cf. Section 3.1.1).</p><p>Let us also note that the two-plane models of Chen and Martins et al. (see Section 3.1.3) are instances of rational polynomial backprojection models.</p><p>It may be worthwhile to study which instances of the models (Equations (3.8) and (3.9)) correspond for example to radially symmetric, central, perspective, or otherwise special camera models.</p><p>Rational polynomial forward models. Rational polynomial models are usually either applied for the back-projection function as proposed by Claus and Fitzgibbon <ref type="bibr" target="#b97">[98]</ref>, or for the 1D radial undistortion function as proposed by Ma et al. <ref type="bibr" target="#b326">[327]</ref>. Hartley and Saxena also proposed a rational camera model, but for the forward projection function <ref type="bibr" target="#b209">[210]</ref>: the image point coordinates are expressed via rational polynomial functions of the 3D point coordinates (Hartley and Saxena used cubic polynomials). In the general case, this corresponds to curved camera rays, i.e., the set of 3D points that get projected onto the same image point, form a curve. Hartley and Saxena showed that this model is a good fit for SAR cameras. It also subsumes "linear" cameras such as perspective and linear pushbroom ones, although fitting it to data obtained from these may lead to overfitting due to a strong overparameterization.</p><p>Essentially the same approach seems to be in routine use to model satellite imagery, cf. <ref type="bibr" target="#b146">[147]</ref>. In Section 11.3 of that reference, the general term of "replacement sensor models" is employed to denote any camera model that replaces a rigorous, physics-based, model by empirical expressions. The most used ones are rational polynomial functions, much like in the above works.</p><p>Hall et al. proposed the RTcam (rational tensor camera model) <ref type="bibr" target="#b199">[200]</ref>. It essentially proceeds by first transforming 3D points via biquadratic rational functions, followed by a perspective projection to a 2D image plane (the full RTcam model is defined for arbitrary dimensions of scene and image). Together, the two imaging steps still correspond to bi-quadratic rational functions. Similar to the GLC model (see Section 3.4.4), the development of the RTcam is mainly motivated by applications in stylized, or non-photorealistic, rendering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.9">One-Dimensional Radial Models</head><p>Thirthala and Pollefeys formally introduced so-called 1D radial camera models <ref type="bibr" target="#b499">[500,</ref><ref type="bibr" target="#b500">501]</ref>. They hinge on the existence of an optical axis and a center of distortion. Planes containing the optical axis are called radial planes and lines going through the distortion center, radial lines. The only main assumption about the projection function is that all 3D points in a radial plane are imaged to points on a same radial line. This effectively allows completely arbitrary radial distortions, and especially, does not require any radial symmetry of the distortion (the distortion "function" may be different for each radial plane-line pair) and it allows for non-central projections.</p><p>To make the model tractable, a second assumption is used though, namely that the projection can be modeled by a 2 × 4 projection matrix, mapping 3D points onto radial lines, represented by two homogeneous coordinates. In other words, it is assumed that there exists a projective relationship between the pencil of radial planes in 3D and that of radial lines in the image. Thirthala and Pollefeys showed that quadrifocal tensors can be formulated based on these projection matrices, and used for self-calibrating cameras. In general, no matching constraints exist for less than four images. However, for the case of pure rotational camera motion, a trifocal tensor exists and can also be used for self-calibration.</p><p>One-dimensional radial models have actually been used somewhat implicitly before, especially embodied within the radial alignment constraint (RAC) of Tsai <ref type="bibr" target="#b514">[515]</ref>, used also in other works <ref type="bibr" target="#b467">[468,</ref><ref type="bibr" target="#b105">106]</ref>. Tsai proposed a camera calibration approach for cameras with radial distortion, where one of the initial steps concerns the estimation of the calibration grid's pose relative to the camera. At this stage, it is assumed that the camera follows a 1D radial model and that the distortion center is known. Compared to the general model of Thirthala and Pollefeys, Tsai further assumed that angles between radial lines and associated radial planes are identical (e.g., he had to know the aspect ratio). Using these assumptions, the radial plane associated with each extracted calibration point can be computed. One may then estimate the pose of the calibration grid using the constraint that with the correct pose, each point on the grid, must lie in the radial plane of the matched image point. It is clear that the pose cannot be fully computed: any translation along the optical axis will still satisfy the above constraint. Hence, 5 degrees of freedom are computed, from a minimum of 5 point matches.</p><p>A 1D radial model assumption was also used in the calibration approach of Hartley and Kang <ref type="bibr" target="#b208">[209]</ref>. In the initial step, they used this to compute the distortion center from the image of a calibration grid; see more on their approach in Section 5.1.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.10">Neural Networks</head><p>The projection or back-projection function has also been modeled and computed using neural networks, see for example <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b534">535,</ref><ref type="bibr" target="#b576">577]</ref>. This is an interesting approach since it is inherently flexible and can be applied to calibrate various kinds of cameras with various types of distortion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Local Camera Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Two-Plane Model</head><p>The third version of the two-plane model proposed by Chen and Martins et al. <ref type="bibr" target="#b88">[89,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b330">331]</ref> (cf. Section 3.1.3) is a local camera model. Here, for each of the two calibration images, the extracted matches define triangulations of the image and calibration grids respectively, cf. Figure <ref type="figure">3</ref>.3. Back-projection is now modeled separately for each pair of matching triangles, by an affine transformation as in the first version of the two-plane model, computed from the matches associated with the three triangle vertices. To back-project an image point, the following is done. For each calibration image, the triangle into which the image point falls is determined. A point on the calibration grid is computed via the affine transformation associated with that triangle. This is done separately for both calibration images/grids. The back-projected camera ray is then computed by spanning the line between the points on the two calibration grids.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Warping-Based Models</head><p>Very similar to the two-plane approach is the following idea. Whereas the two-plane model interpolates 3D points on calibration grids, the following methods perform an interpolation in the image domain. They can all be applied with a single image of a grid, as for the correction of non-perspective distortions.</p><p>The general idea is as follows: consider one calibration plane and an image thereof, as well as matches between the plane and the image. Consider any warping scheme that maps the acquired image into the "ideal" image of the calibration plane (let's say, a texture map of the plane). Since the ideal image is distortion-free (it is a perspective image), this warping thus removes non-perspective distortions. If we determine the parameters of this warping using the acquired image, we may then apply it to any other image and thus remove its non-perspective distortions. This gives images which are distortioncorrected, i.e., images as acquired by some perspective camera. However, we don't yet know the associated perspective camera. Still, when acquiring and then warping one or more images, the resulting images correspond to images acquired by the same perspective camera. This enables the application of any algorithm valid for perspective cameras, to these images. For example, if images of a calibration grid are acquired (be it planar or three-dimensional), any classical perspective calibration algorithm may be applied to calibrate the virtual camera. The same holds true for self-calibration. The complete calibration of the true camera is then the combination of the warp, with the computed perspective (self-)calibration.</p><p>Several methods following this general idea were proposed; they mostly differ in the choice of interpolation scheme which is required to warp entire images (the role played by the piecewise affine warp in the two-plane method). Without going into details, we cite the approaches of Green et al. <ref type="bibr" target="#b185">[186]</ref>, Goshtasby <ref type="bibr" target="#b181">[182]</ref>, Yokobori et al. <ref type="bibr" target="#b556">[557]</ref>, Brand et al. <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60]</ref>, and Jackowski et al. <ref type="bibr" target="#b251">[252]</ref>. Peuchot and Saint-André <ref type="bibr" target="#b400">[401]</ref> and Bogner <ref type="bibr" target="#b53">[54]</ref> also mentioned the feasibility of this type of approach. The method of Jackowski et al. corrects for color non-linearities in addition to geometrical distortions <ref type="bibr" target="#b251">[252]</ref>.</p><p>Sagawa et al. used the same idea and combined it with a structured light-type approach in order to get dense matches between the image and the calibration grid (a flat screen) <ref type="bibr" target="#b432">[433]</ref>, an approach used also for example by Grossberg and Nayar, Tardif et al. and Dunne et al. (see Section 3.3). From dense matches, the distortion-corrected image can be constructed pixel-by-pixel instead of requiring piecewise warps and an interpolation scheme.</p><p>It seems somewhat surprising that apparently, none of the above works went through the last obvious step to obtain a complete camera calibration. Namely, given these distortion-corrected, hence "virtual" perspective images, one may apply any calibration approach for pinhole cameras to calibrate the perspective part of the projection in addition to the non-perspective one. Such approaches were proposed previously, but only in combination with a plumb-line type distortion correction based on classical polynomial distortion models, see e.g., the work of Grosky and Tamburino <ref type="bibr" target="#b189">[190]</ref>. Dunne et al. seem to be the first to combine the above generic distortion correction with a subsequent pinhole calibration <ref type="bibr" target="#b124">[125]</ref>. They also used a structured-light type approach to get dense, per-pixel, matches between calibration grids and images; due to the per-pixel nature of the approach, we consider it in the discrete class of models and describe it in the next section.</p><p>Comment. All approaches in this section are valid for central cameras only since they are based on the idea of (bijective) warps of distorted images into perspective ones. The two-plane approach of Section 3.2.1 is in some sense a non-central counterpart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Other Local Models</head><p>Qiu and Ma proposed an approach that is similar in spirit to the warpbased ones described above <ref type="bibr" target="#b409">[410]</ref>. They used as input an image of a 3D calibration object, as opposed to a planar grid as in most other similar approaches. From 2D-to-3D matches, they then seemed to compute a best-fit perspective projection matrix for that image and projected the 3D calibration points using it. The result will constitute the ideal image onto which the distorted input image will be warped, for distortion correction. Input to the next step of calibration are the images of calibration points in the distorted image and in the ideal image, respectively. The goal is then to compute distortion functions ∆x and ∆y such that:</p><formula xml:id="formula_60">x d y d = x u + ∆x(x u , y u ) y u + ∆y(x u , y u ) .</formula><p>In <ref type="bibr" target="#b409">[410]</ref>, the distortion functions were estimated at a regular lattice of image pixels using non-parametric regression, and extended to the whole image by linear or nearest-neighbor interpolation.</p><p>As mentioned above, Qiu and Ma used a 3D calibration object and fitted a perspective projection matrix to the 2D-to-3D matches. The approach could easily be adopted to using a 2D calibration grid: then one may directly use the model of the grid as ideal image, such as in most other similar approaches or, one may compute the best-fit homography (instead of projection matrix) between the 2D-to-2D matches and use it to project the calibration points into an ideal image that is "closer" to the actual input image. The rest of the approach would be strictly identical.</p><p>Ragot proposed a similar idea, where 3D-to-2D matches obtained from an image of a calibration grid are interpolated to determine the forward respectively back-projection of generic 3D respectively image points <ref type="bibr" target="#b411">[412]</ref>. The interpolation is done within triangular tessellations of 3D respectively image points.</p><p>Munjy proposed another very similar approach, based on the finite element method <ref type="bibr" target="#b358">[359,</ref><ref type="bibr" target="#b359">360]</ref>. He considered a regular triangular or rectangular tessellation of the image plane and modeled the distortion function with the help of one focal length per vertex (giving the angle between the line of sight associated with the vertex, and the optical axis). Focal lengths of all other image points can then be computed via bilinear interpolation of the focal lengths of the vertices of the triangle/rectangle in which image points lie. The parameters of the model (principal point, focal lengths of vertices) are computed by bundle adjustment (can be done when using a calibration grid but also in full self-calibration mode where 3D point coordinates are also optimized, using multiple images).</p><p>Lichti and Chapman extended this approach by coupling it with the classical radial and tangential distortion terms <ref type="bibr" target="#b314">[315]</ref>. Back-projection is thus carried out by correcting for these classical distortions and then back-projecting the resulting points using the local focal length computed from the finite-element model. Lichti and Chapman also discussed correlations among the model's parameters and how to acquire images in order to minimize them. In <ref type="bibr" target="#b315">[316]</ref>, they further extended the approach by including continuity constraints in order to overcome discontinuities of the distortion function due to the bilinear interpolation carried out over neighboring rectangles.</p><p>Schreier et al. <ref type="bibr" target="#b439">[440]</ref> and Cornille <ref type="bibr" target="#b100">[101]</ref> used a similar model, where B-splines are used to map positions in distorted images to those in undistorted ones. They used that approach to calibrate optical and scanning electron microscopes.</p><p>Stevenson and Fleck computed a 2D distortion function that is piecewise over a triangular tessellation of the image plane <ref type="bibr" target="#b462">[463]</ref>. The basic idea is to apply an affine warp to each individual triangle such that the image becomes distortion-free. A notable difference with most other similar works is that the distortion-free image is generated with respect to stereographic projection instead of perspective, i.e., distortion-free does not mean as usual that straight lines remain straight (as in perspective projection) but rather that any sphere is imaged as a circle (a property of stereographic projection). Consequently, the input are images of spherical objects. In a first step, ellipses are fitted to their outlines in the images (which is an approximation, precluding the use of too large spheres). Then, the Delaunay triangulation of the centers of all ellipses is computed. Let us now consider one triangle and the associated three ellipses, centered in the triangle's vertices. We may compute an affine transformation that maps the three ellipses as closely as possible into circles. This transformation is naturally only defined up to a scale change ("circle-ness" does not depend on scale). The goal is then to find a new triangulation that is consistent with the original one, modulo these local affine transformations. This new triangulation would be nothing else than the stereographic image of the set of sphere centers. This description does not exactly correspond to the approach taken by Stevenson and Fleck, but transcribes its basic idea. Once the new triangulation computed, the input image can be warped into a stereographic image by piecewise warping it from the original into the new triangulation. The result is an uncalibrated stereographic image, i.e., whose "focal length" (overall scale) is arbitrary and whose distortion center is unknown.</p><p>The approach is somewhat similar in spirit to the warping-based ones of Section 3.2.2, in that it results in a piecewise warp of an image, allowing to undistort it. Differences however are the use of stereographic projection as reference instead of perspective and that the warp is not computed on the basis of an ideal input image (the model of a 2D calibration grid), but using images of 3D primitives (here, spheres).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discrete Camera Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Ray-Based Models</head><p>As explained in the introduction to this section, most discrete camera models can be seen as the limiting case of the above local models, where interpolation is required in the immediate vicinity of pixels instead of in larger image regions. The main difference between the following discrete approaches and the local ones is the explicit motivation to calibrate individual camera rays, as opposed to forward or back-projection mappings.</p><p>The idea of calibrating individual camera rays for discrete image locations was formulated by several researchers. Among the first were probably Gremban et al. who proposed extensions of the two-plane method in <ref type="bibr" target="#b188">[189]</ref> and discussed the possibility of a ray-based calibration. However, they concluded that "A lookup table of calibration data for each pixel would be prohibitively expensive". This has to be understood in the historical context; such a lookup table requires four coefficients per pixel, i.e., is of a size which does not pose memory problems anymore nowadays. As alternative solution, Gremban et al. effectively proposed global and local camera models, based on global and local interpolation schemes to compute camera rays for any pixel: global schemes are based on a function that is valid for the whole image plane whereas local schemes look up the closest image points from calibration data (in the case of back-projection) or the closest lines of sight (in the case of forward projection) and interpolate accordingly.</p><p>Using structured light-type acquisition setups, such interpolations can be avoided altogether, by matching image points directly to calibration points. This has been used by various researchers, e.g., Grossberg and Nayar <ref type="bibr" target="#b190">[191]</ref>, Dunne et al. <ref type="bibr" target="#b122">[123,</ref><ref type="bibr" target="#b123">124,</ref><ref type="bibr" target="#b124">125]</ref>, Sagawa et al. <ref type="bibr" target="#b432">[433]</ref> (cf. Section 3.2.2), and Tardif et al. <ref type="bibr" target="#b490">[491]</ref>.</p><p>Southwell et al. proposed an idea for a structured-light type ray-based camera calibration approach <ref type="bibr" target="#b454">[455]</ref>. The idea was to insert the camera in a cylinder painted on the inside in smoothly varying color shades such that each "point" on the cylinder has a unique color. Then, for each camera pixel, the scene point it sees can in theory be uniquely determined. Southwell et al. proposed to calibrate a camera by generating a look-up table where for each pixel, the direction of its camera ray is stored, after computing it from the matched point on the cylinder. They did not use this approach in practice though, due to practical and signal processing problems.</p><p>Grossberg and Nayar proposed a generic imaging model consisting of geometric as well as radiometric and optical parts <ref type="bibr" target="#b190">[191]</ref>. The geometric part is identical in spirit to the two-plane model and its successors. However, Grossberg and Nayar calibrated pixels individually, without any interpolation, using structured light-type approaches allowing to densely match images and calibration objets (e.g., flat screens). Further, instead of computing lines of sight for individual pixels, they actually computed half-lines. This was mainly achieved by computing and using the caustic of the imaging system which is used as the hull of the imaging system's model, from which camera rays emanate in an outward direction. Besides the geometric part of the imaging model, it also contains radiometric and optical aspects: for example, each pixel is associated with individual radiometric response and point spread functions. Overall, a line of sight (or rather, a half-line), together with these non-geometric properties, make what Grossberg and Nayar termed a raxel, a sort of tiny camera associated with each individual pixel of the imaging system. Such a ray-based imaging model has been used recently by several researchers to devise calibration and structure-from-motion algorithms, for example by Ramalingam et al. <ref type="bibr" target="#b413">[414,</ref><ref type="bibr" target="#b414">415,</ref><ref type="bibr" target="#b417">418,</ref><ref type="bibr" target="#b476">477]</ref>, Dunne et al. <ref type="bibr" target="#b122">[123,</ref><ref type="bibr" target="#b123">124,</ref><ref type="bibr" target="#b124">125]</ref>, and Gonçalves and Araújo <ref type="bibr" target="#b179">[180]</ref>. Ray-based calibration approaches are described in Section 5.1.2.</p><p>Debaecker et al. proposed to model the field of view of each pixel by a cone instead of a (half-)line in order to model the fact that pixels gather a volume of light rays <ref type="bibr" target="#b107">[108]</ref>. This may be seen as an alternative to using a point spread function like Grossberg and Nayar.</p><p>Comments. In the above works, it is usually proposed to compute one camera ray per image pixel. Note however that this choice, while being intuitive, is not the only one for a discrete imaging model. Indeed, one may compute rays for any other discrete sample of image locations. With a regular super-pixel-based sampling for example, one ray for every n-th pixel would be computed; to compute rays for every pixel, an interpolation scheme would then be required, i.e., we are looking at local camera models, see the previous section. Even with a per-pixel sampling, interpolation is required to perform back-projection for every image point. Finally, nothing prevents in principle to apply a sub-pixel sampling for ray-based calibration. The appropriate choice of sampling density is a model selection question and depends on several factors, see the general discussion in Section 3.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Discrete Sampling of the Distortion Curve</head><p>Instead of computing camera rays at a discrete set of image locations, which is nothing else than gathering samples of a generic backprojection mapping, one may also gather such samples for any variant of the 1D (un) distortion function: r d (θ), θ(r d ), r d (r u ), or r u (r d ).</p><p>Most calibration approaches based on this idea then fit some global distortion model to these samples. Examples are some collimator-based calibration approaches (cf. Section 5.1.4) or an early approach for planebased calibration by Hallert (cf. Section 5.1.2) <ref type="bibr" target="#b200">[201]</ref>.</p><p>More recent such approaches are the ones by Stevenson and Fleck <ref type="bibr" target="#b461">[462]</ref> (see Section 5.3.2), Hartley and Kang <ref type="bibr" target="#b208">[209]</ref> and Tardif et al. <ref type="bibr" target="#b490">[491]</ref> (see Section 5.1.2), as well as Ying and Hu <ref type="bibr" target="#b551">[552]</ref> (see Section 5.1.1). In addition to modeling the usual 1D distortion function, Tardif et al. and Ying and Hu also considered a non-central radial distortion model: to each radial distance r d , is associated an angle θ as well as an optical center on the optical axis. This is thus a radially symmetric axial model, similar to the one proposed by Gennery (cf. Section 3.1.7) <ref type="bibr" target="#b160">[161]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">"Isolated" Camera Rays</head><p>Grossmann et al. have considered what might be the most discrete camera model, consisting of a sparse set of pixels and associated camera rays, with no requirement that rays have close-by neighboring rays <ref type="bibr" target="#b191">[192,</ref><ref type="bibr" target="#b193">194]</ref>. Their proposed self-calibration approach (see Section 5.3.3) only makes the assumption that the camera is central.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Models for the Distribution of Camera Rays</head><p>What a camera sees of a scene depends on the light rays it captures, not on its intrinsic geometry, i.e., which pixel exactly catches the photons of which light ray. In that respect, we may split the camera's intrinsic parameters into two sets: one that explains which light rays the camera captures (these will then be considered as camera rays) and one that explains the mapping between camera rays and image points. To make the distinction clearer, let us consider any two different central cameras, e.g., a perspective camera and a central catadioptric one. Being central, both cameras catch exactly the same set of light rays: all light rays that contain the optical center (here, we neglect of course the finite aperture of our cameras and their different fields of view). What differs is the mapping from image locations to rays and consequences thereof (e.g., non-perspective distortions and the spatial/angular resolution profile). On top of this come the extrinsic parameters, telling the global orientation and position of the camera, or in other words, of its set of camera rays.</p><p>In this section, we summarize several works which are primarily concerned with the first part of the "intrinsic geometry" of cameras, namely a description of which rays a camera captures: the locus, or spatial distribution, of camera rays. Several taxonomies and computational models have been proposed in the last years <ref type="bibr" target="#b389">[390,</ref><ref type="bibr" target="#b404">405,</ref><ref type="bibr" target="#b444">445,</ref><ref type="bibr" target="#b471">472,</ref><ref type="bibr" target="#b558">559]</ref>. The main motivation of these works is to reason on the spatial distribution of camera rays; the mapping between rays and image points is of secondary importance although some of these works also propose such a mapping in order to obtain a full camera model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">From Central to Oblique Cameras</head><p>Cameras may be classified according to the existence of a geometric entity that is incident with all camera rays <ref type="bibr" target="#b389">[390,</ref><ref type="bibr" target="#b404">405,</ref><ref type="bibr" target="#b471">472]</ref>. For a central camera, there exists a 3D point, the optical center, that is incident with all rays. Any other camera is non-central. Among non-central cameras, several subgroups exist, as follows.</p><p>In an axial camera, all rays cut some real 3D line. A special case is the two-slit camera, where there exist two real lines that cut all rays (for example, linear pushbroom panoramas, cf. Section 2.1.1). Cameras falling outside any of these categories are usually termed fully non-central or the like. A special case here are oblique cameras, for which any two camera rays are either identical or skew <ref type="bibr" target="#b389">[390]</ref>. Oblique cameras are the most non-central cameras possible. A special case are linear oblique cameras, see Section 3.4.3, for which all rays are cut by an imaginary 3D line <ref type="bibr" target="#b388">[389]</ref> (in fact, by two conjugated imaginary lines <ref type="bibr" target="#b404">[405]</ref>).</p><p>Other cases are cameras, whose rays are incident with a circle, as is the case with circular non-central mosaics (cf. Section 2.1.1). Another example is catadioptric camera consisting of a cone-shaped mirror and a camera located on the mirror axis. All reflected rays are incident with a circle centered in the mirror axis. However, such systems are also of the axial type, since all reflected rays also intersect the mirror axis.</p><p>One may define many other categories of cameras, by choosing other geometric primitives to which all rays are incident.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Ray Distributions Allowing for Standard Stereo Geometry</head><p>Pajdla <ref type="bibr" target="#b389">[390]</ref> and Seitz and Kim <ref type="bibr" target="#b444">[445]</ref> were interested in studying which sets of camera rays admit stereo imaging conditions. What is meant by this? To explain this, we look ahead to Section 4 on epipolar geometry. The basic question of epipolar geometry and stereovision is, given a point in one image, which are the potentially matching points in the other image. The answer to this is: all points whose camera rays intersect the ray of the first point. These define an epipolar curve in the second image. Epipolar curves in the first image are defined analogously. Essentially, two cameras are in standard stereo geometry if epipolar curves in the two images are in a one-to-one relationship: the camera ray of any point on an epipolar curve in one image cuts all camera rays of points on the related epipolar curve in the other image (for a more precise definition, see <ref type="bibr" target="#b444">[445]</ref>). An auxiliary requirement is that the set of epipolar curves in each image is one dimensional. Not surprisingly perhaps, the set of rays of two cameras in standard stereo geometry, lie on a set of doubly ruled quadrics<ref type="foot" target="#foot_2">3</ref>  <ref type="bibr" target="#b389">[390,</ref><ref type="bibr" target="#b444">445]</ref>. The two individual cameras then correspond to the two rulings of these quadrics. Note that this comprises several types of non-central cameras, such as circular non-central mosaics and others <ref type="bibr" target="#b444">[445]</ref> as well as linear oblique cameras <ref type="bibr" target="#b389">[390]</ref>.</p><p>This result concerns the locus of camera rays and is independent of the mapping from rays to image points. What it implies is that for any two cameras in standard stereo geometry, the acquired images may be warped into images where epipolar curves are horizontal lines and matching epipolar lines have the same vertical coordinate. In other words, it is possible to rectify images and then apply any standard stereo algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Linear Oblique Cameras</head><p>Pajdla studied the case of linear oblique cameras, already mentioned above <ref type="bibr" target="#b389">[390]</ref>. A defining characteristic of these is the existence of a linear transformation that maps each 3D point to the unique camera ray containing that point. Pajdla showed that all linear oblique cameras are projectively equivalent to the one defined as follows: the camera ray containing a 3D point (X, Y, Z, W ) T is spanned by that point and the point with coordinates (Y, -X, W, -Z) T . This defines the set of camera rays in a linear oblique camera. In order to get a full camera model, including the mapping between rays and image points, Pajdla proposed to use a plane as reference. Let q be the homogeneous 2D coordinates of a point in the reference plane (the image plane if one likes). Each point inside the plane is mapped to a 3D point by some 4 × 3 pose matrix M:</p><formula xml:id="formula_61">Q ∼      X Y Z W      ∼ Mq.</formula><p>The camera ray is then spanned by Q and the second 3D point, defined above. It is easy to prove that the camera ray can be written as:</p><formula xml:id="formula_62">B l ∼ B l L 2 (q),</formula><p>with B l a 6 × 6 back-projection matrix depending on M, operating on lifted image point coordinates. The back-projection matrix is always of rank at most 4.</p><p>For example, if we choose the following pose matrix:</p><formula xml:id="formula_63">M =      1 0 0 0 1 0 0 0 0 0 0 1      ,</formula><p>which corresponds to choosing the XY -plane as reference plane, then one obtains the following back-projection matrix:</p><formula xml:id="formula_64">B l ∼          0 0 0 0 -1 0 0 0 0 1 0 0 0 0 0 0 0 -1 0 0 0 0 1 0 0 0 0 -1 0 0 -1 0 -1 0 0 0          . (3.10)</formula><p>We may see that, similarly to what was shown for the classical radial distortion model (cf. Section 3.1.1) or para-catadioptric cameras (cf. Section 3.1.5), the back-projection matrix may be compacted. The second column only contains zeroes and the first and third ones are identical. Hence, back-projection for linear oblique cameras may be written as:</p><formula xml:id="formula_65">B l ∼          0 0 -1 0 0 1 0 0 0 0 0 -1 0 0 1 0 0 -1 0 0 -1 0 0 0               q 2 1 + q 2 2 q 1 q 3 q 2 q 3 q 2 3      .</formula><p>Observation. Linear oblique cameras are actually subsumed by the linear two-plane model of Chen and Martins et al. (cf. Section 3.1.3): it is easy to prove that the back-projection matrix of Equation (3.10) can be obtained from the one of the two-plane model (cf. Equation (3.6)) by an appropriate choice of affine transformations A 1 and A 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4">General Linear Camera Model (GLC)</head><p>Yu and McMillan proposed the so-called General Linear Camera model (GLC) <ref type="bibr" target="#b558">[559,</ref><ref type="bibr" target="#b557">558]</ref>, which parameterizes the set of camera rays by affine transformations of three basis rays. Concretely, they adopted a twoplane parameterization of rays (see also a paper by Gu et al. <ref type="bibr" target="#b195">[196]</ref> on this parameterization) as follows. The three basis rays are given by points:</p><formula xml:id="formula_66">A i =      u i v i 0 1      B i =      s i t i 1 1      ,</formula><p>where we assume that the two planes are Z = 0 and Z = 1. All other rays are then given by two parameters α and β, defining points in the two planes via affine transformations of the basis points parameterized by two scalars α and β:</p><formula xml:id="formula_67">A =    αu1 + βu2 + (1 -α -β)u3 αv1 + βv2 + (1 -α -β)v3 0 1    B =    αs1 + βs2 + (1 -α -β)s3 αt1 + βt2 + (1 -α -β)t3 1 1   .</formula><p>Yu and McMillan showed that this model allows to generate the set of camera rays of several imaging geometries, such as pinhole and orthographic cameras but also non-central cameras such as some twoslit ones. It also allows to generate less classical imaging geometries, called by Yu and McMillan the pencil camera, the twisted orthographic camera, and the bilinear camera.</p><p>Like for linear oblique cameras, the initial main motivation of the GLC model is to propose a model for the locus of camera rays, not necessarily a full camera model. For linear oblique cameras, a way to get such a full model, mapping image points to rays, was shown above. As for the GLC, a natural choice is to consider the two parameters α and β as 2D image coordinates. Before going further, let us replace the image coordinates by homogeneous ones such that:</p><formula xml:id="formula_68">q ∼   α β 1   .</formula><p>Then:</p><formula xml:id="formula_69">A ∼   (u1 -u3)q1 + (u2 -u3)q2 + u3q3 (v1 -v3)q1 + (v2 -v3)q2 + v3q3 0 q3   B ∼   (s1 -s3)q1 + (s2 -s3)q2 + s3q3 (t1 -t3)q1 + (t2 -t3)q2 + t3q3 q3 q3   .</formula><p>Finally, the camera ray, spanned by A and B, can be written via the following 6 × 6 back-projection matrix, operating on lifted image coordinates L 2 (q):</p><formula xml:id="formula_70">B l =          0 0 0 u 1 -u 3 -s 1 + s 3 u 2 -u 3 -s 2 + s 3 u 3 -s 3 0 0 0 v 1 -v 3 -t 1 + t 3 v 2 -v 3 -t 2 + t 3 v 3 -t 3 0 0 0 0 0 -1 0 0 0 v 1 -v 3 v 2 -v 3 v 3 0 0 0 u 3 -u 1 u 3 -u 2 -u 3 b 1 b 2 b 3 b 4 b 5 b 6          . (3.11)</formula><p>For the sake of conciseness, the coefficients b i are not shown here; they can be easily computed if necessary.</p><p>Observations. We see that the rays in a GLC can be modeled by a quadratic rational camera model, cf. Other considerations on GLCs. In <ref type="bibr" target="#b559">[560]</ref>, Yu and McMillan gave a projection equation for a special case of GLCs (concretely, a transformation that maps each 3D point to the ray it lies on in the GLC). They also showed how to relate two GLCs which differ in the choice of image plane. Whereas for perspective cameras, corresponding points in two different image planes (but with the same optical center) are related by a homography, this relationship is a quartic rational function for the considered special case of GLCs. In the general case of GLCs, this relation will be even more complicated. However, if parallel reference planes are used, then changing the distance between them does not matter: for any GLC with a particular distance between the planes, there is an equivalent GLC defined with planes separated by another distance.</p><p>The GLC model is defined by three basis rays. An extension to multiple basis rays was proposed and analyzed for up to six rays, by Popescu et al. <ref type="bibr" target="#b405">[406]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.5">Linear Families of Lines</head><p>Ponce proposed to model the set of camera rays by two-parameter linear families of straight lines, either degenerate reguli (rank-3 families) and non-degenerate linear congruences (rank-4 families) <ref type="bibr" target="#b404">[405]</ref>. This framework subsumes the GLCs of Yu and McMillan <ref type="bibr" target="#b557">[558,</ref><ref type="bibr" target="#b558">559]</ref> and the linear oblique cameras of Pajdla <ref type="bibr" target="#b389">[390]</ref>. Ponce gives formulas for projection, back-projection, and epipolar and multi-view geometry, formulated such that the two parameters of the considered linear families of camera rays are considered as image point coordinates. The backprojection matrix is of size 4 × 6 for two-slit cameras and linear oblique ones, and 5 × 6 for the "pencil cameras" of Yu and McMillan <ref type="bibr" target="#b558">[559]</ref>. All models subsumed by Ponce's formulation can be expressed by 6 × 6 back-projection matrices, leading to 6 × 6 fundamental matrices (cf. Section 4). Batog et al. generalized the theoretical framework of <ref type="bibr" target="#b404">[405]</ref> by characterizing all admissible maps that allow to model linear cameras, i.e., cameras whose rays form a linear family of lines <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.6">Caustics</head><p>Roughly speaking, the caustic of an imaging system is the surface enveloping the set of camera rays, i.e., it is tangent to all rays. Caustics are an excellent tool to analyze properties of imaging devices, as nicely advocated by Swaminathan et al. <ref type="bibr" target="#b483">[484]</ref>. A caustic provides one possible description of a viewpoint locus of a camera, i.e., a point set such that any camera ray goes through at least one of the points in the set (ideally through exactly one). For a central camera, the caustic degenerates into a point and for axial ones, into a line segment. The caustic allows to study "how central" a camera is, i.e., how much its extent deviates from a single point. Swaminathan et al. showed how to compute the extent and shape of the caustic for axial catadioptric systems based on conic-shaped mirrors and such that the camera's optical center lies on the mirror axis <ref type="bibr" target="#b483">[484]</ref>. They showed how to use this analysis to determine a best approximation of a single viewpoint, useful for example for approximate perspective view generation. They also analyzed how resolution varies across the image area, and reciprocally, proposed an approach to compute a mirror profile that satisfies a desired resolution profile. Finally, the parametric form of the caustic for a class of catadioptric sensors is used in a calibration approach, described in Section 5.3.2.</p><p>Usually, to each image point corresponds exactly one point on the caustic. This corresponds to the "viewpoint" associated with that image point. To compute the entire camera ray, one still needs its direction. For axial systems, it can be easily computed since the camera ray must pass through the axis and also be tangent to the caustic. For fully noncentral systems, the full determination of camera rays requires a second map (of directions), besides a map from image points to the caustic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Overview of Some Models</head><p>In Table <ref type="table">3</ref>.1, some of the models seen in this section are summarized. Note that in the table, we do not always use the same notation for these Table <ref type="table">3</ref>.1. Back-projection and forward projection equations for some camera models. N/A models as in the previous sections; rather, we try to homogenize notations by naming coefficients of camera models k or k 1 , k 2 , . . .. Further, parameters akin to the focal length are denoted by f . The shown models are all radial distortion models; their equations are given under the assumption that the distortion center is identical to the principal point and located at the origin. How to relax these assumptions is outlined in the introduction to this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name of model</head><formula xml:id="formula_71">B i θ(r d ) ru(r d ) P i r d (θ) r d (ru) Perspective   1 0 0 0 1 0 0 0 f   atan r d f r d   f 0 0 0 f 0 0 0 1   f tan θ r u Classical radial dist.   k 0 1 0 0 0 k 0 1 0 0 0 0 0 f         q 3 1 + q1q 2 2 q 2 1 q2 + q 3 2 q1q 2 3 q2q 2 3 q 3 3       atan r d (1+kr 2 d ) f r d (1 + kr 2 d ) Requires solution of cubic equation Para-catadioptric   0 2k 0 0 0 0 2k 0 -1 0 0 k 2       q 2 1 + q 2 2 q1q3 q2q3 q 2 3     atan 2kr d k 2 -r 2 d N/A Requires solution of quadratic equation Division [62, 145]   0 1 0 0 0 0 1 0 kf 0 0 f       q 2 1 + q 2 2 q1q3 q2q3 q 2 3     atan r d f (1+kr 2 d ) r d 1+kr 2 d N/A N/A N/A Mičušik, Pajdla [344] N/A r d f (1+kr 2 d ) N/A N/A N/A N/A Stereographic   0 2f 0 0 0 0 2f 0 -1 0 0 f 2       q 2 1 + q 2 2 q1q3 q2q3 q 2 3     2atan r d f N/A N/A f tan(θ/2) N/A Equidistant/-angular N/A</formula><p>The table contains six different ways of modeling the imaging transformation. Cells annotated by "N/A" correspond to cases where there is no closed-form expression or, if there is no expression of back-projection in terms of polynomials (second column).</p><p>Sample graphs of the function θ(r d ) for several camera models are given in Figure <ref type="figure">3</ref>.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">So Many Models . . .</head><p>This section reviewed some camera models existing in the computer vision literature. Although it is certainly not exhaustive, the list of models is rather long. One may ask, which model one should choose or recommend. For example, recurring questions are, should one use a non-central camera model for a slightly misaligned catadioptric system or stick with a central model, or should one model fisheye lenses with a central or non-central model.</p><p>The general answer to these questions is: it depends . . . The choice of camera model depends of course on how appropriate a model is in general for the type of camera used. For example, one would obviously never use a pinhole model for fisheye cameras. More interesting parts of the question are: how to determine the appropriate order of a polynomial model, how to select among altogether different types of models, how to decide if it is worth using a non-central model rather than a central one, etc.</p><p>The choice of camera model or of the number of coefficients of a model may depend on several factors:</p><p>• The amount of noise in the calibration data.</p><p>• The amount of calibration data: how many images are used and how many calibration points (or other primitives used for calibration)? • The quality of the spatial distribution of calibration data: how well do the data cover the field of view (both across the field of view and in depth), how good is the spatial distribution of the set of camera positions, how different are the camera orientations, etc.? • The "true" camera model: is it central or not, how pronounced are non-perspective distortions, etc.?</p><p>The choice depends on the combination of these and possibly other factors. For example, Zhang showed that the self-calibration of the classical radial distortion model, even with a single coefficient, from matches between images, is only stable if the actual distortion is pronounced and if extracted image points are accurate <ref type="bibr" target="#b565">[566]</ref>.</p><p>Let us consider extreme cases. Without noise in the data, the true model can be estimated without error (assuming we are not in a degenerate situation for the position of the calibration data, such as the twisted cubic for pinhole camera calibration <ref type="bibr" target="#b66">[67]</ref>). Even with noise, the availability of infinitely many calibration data may allow a perfect calibration of the true camera model.</p><p>In practice, there is noise and the number of data is finite and we must consider the interplay of the above factors. To stress that the choice of camera model depends on all of these factors, we may note that even for one and the same camera, different models may be chosen for different scenarios. For example, fisheye lenses are probably not perfectly central, i.e., not all camera rays go through a same 3D point. However, the viewpoint locus is at most as large as the aperture of the lens, i.e., a few centimeters, and in practice, much smaller. Hence, if the fisheye lens is used for an application where the scene of interest is meters away from the camera, then an approximation by a central imaging model should be sufficient (unless we use many images and extremely accurate image points for calibration). However, if the scene in some hypothetical application is much less than a meter away, a non-central model may make sense.</p><p>So far, the discussion was rather qualitative. To actually implement a method for choosing among camera models requires quantitative goodness measures.</p><p>Clearly, our question is one of model selection, for which a large body of literature exists. The main principle of model selection is to find a good compromise between the goodness of fit to the data and the complexity of a model; roughly speaking, a good model should explain the data well but not have too many parameters. The reason for this is that in general, a model with more parameters has a better chance to Fig. <ref type="figure">3</ref>.5 Illustration of overfitting. Seven points are sampled from a quadratic polynomial (red curve) and perturbed by random noise. Then, a quadratic and a degree-6 polynomial are fitted to the points (green and orange curve respectively). The degree-6 polynomial fits the data perfectly as expected but is an extreme overfit as can be seen from the two random trials above, whereas the degree-2 polynomial fits stably and accurately.</p><p>well fit the data. <ref type="foot" target="#foot_3">4</ref> If more parameters than necessary are allocated then this constitutes an overparameterization and it may lead to overfitting: in addition to explaining the phenomenon underlying the observed data (here, an imaging process) the model parameters will tend to explain the noise in the data. <ref type="foot" target="#foot_4">5</ref> If we tip the balance among the above factors, e.g., too few or too badly distributed data compared to the amount of noise, then the model will no longer be a trustworthy explanation of the phenomenon. The usual sample illustration of overfitting concerns the problem of fitting a polynomial to data points, cf. Figure <ref type="figure">3</ref>.5.</p><p>The problem of overfitting in camera calibration is real but can be rather safely reduced by following a few guidelines. If an application allows it, one should always acquire more than the minimum required amount of data (in terms of number of images, number of calibration points, etc.). The image area should be completely covered by calibration points: distortions are usually strongest, i.e., most observable at the image border and even in the absence of distortions, a better coverage of the image area gives a better leverage on calibration and other structure-from-motion tasks (e.g., motion estimation gets stabler the wider the field of view that is effectively used). By design, calibration grids usually have well-distributed calibration points. One of the most important aspects is that images should be taken from as different as possible viewpoints and, crucially, with as different camera orientations as possible. The latter aspect includes rotations about the optical axis (required if an aspect ratio is to be computed) but more importantly, one should acquire images with different directions of the optical axis (or, orientations of the image plane). Naturally, one should also spend an effort to get the most accurate image point extraction.</p><p>These are just a few general guidelines. More intricate ones which may be important for high accuracy calibration are that cameras should best be calibrated in the same conditions as in the final application, e.g., same temperature, atmospheric conditions, even in the same orientation (tilting a camera may already affect its intrinsic parameters), etc. More on these issues can be found in photogrammetric literature, e.g., <ref type="bibr" target="#b335">[336,</ref><ref type="bibr" target="#b450">451]</ref>.</p><p>Coming back to the model selection problem: model selection methods for camera calibration were proposed for example by El-Melegy and Farag <ref type="bibr" target="#b126">[127]</ref>, Ma et al. <ref type="bibr" target="#b326">[327]</ref>, and Orekhov et al. <ref type="bibr" target="#b386">[387]</ref>. See also the good discussion by Derrien and Konolidge in <ref type="bibr" target="#b110">[111]</ref>, who also evaluated the geometric error committed when approximating a non-central camera by a central model. An alternative to usual model selection methods is to evaluate the quality of different camera models (and of the estimated coefficients thereof) using other data than that used for calibration. One possibility is cross-validation, i.e., evaluate calibration results using calibration images not used for calibration. Another one is to judge the quality of calibration within an actual intended application of the calibration: calibration is rarely a goal in its own but is carried out in order to realize some application, such as 3D modeling or egomotion estimation. If ground truth data for these or other applications are available, then it is obviously a good idea to measure the quality of a calibration result via the quality of the subsequent result in the considered application.</p><p>A final comment is that while it is not possible to recommend this or that camera model in general, it seems a good idea to enhance "ideal" models for cameras with large fields of view (e.g., the equiangular one), with classical radial or tangential models, as proposed for example by Devernay and Faugeras <ref type="bibr" target="#b112">[113]</ref>, Kannala and Brandt <ref type="bibr" target="#b265">[266]</ref>, Gennery <ref type="bibr" target="#b160">[161]</ref>, and Mei and Rives <ref type="bibr" target="#b338">[339]</ref>.</p><p>Epipolar geometry is central to stereovision. On the one hand, knowing it enables to strongly constrain the problem of matching two images. On the other hand, it can be estimated from matches and then allows for motion estimation, self-calibration, and triangulation of 3D points or other geometric primitives. Epipolar geometry has been well studied for perspective cameras, since the nineteenth century (see, e.g., the excellent paper <ref type="bibr" target="#b143">[144]</ref> by Finsterwalder on geometric foundations of photogrammetry, as well as modern treatments <ref type="bibr" target="#b136">[137,</ref><ref type="bibr" target="#b212">213]</ref>).</p><p>As for non-perspective cameras, much has been done in the last 20 years and this section aims at giving a concise overview. It will be seen in the following that epipolar geometry is a very powerful tool for the self-calibration of non-perspective distortions; this is based on a strong link to the plumb-line calibration approach (cf. Section 5.2.1), explained in Section 4.3.</p><p>We start the section by considering the case of calibrated cameras, where the epipolar geometry can be represented by an essential matrix. Except the question if cameras are central, non-central, axial, etc., the essential matrix is independent of the camera type. In Section 4.2, we consider the epipolar geometry of uncalibrated cameras and particularly, fundamental matrices for non-perspective cameras. Finally, since epipolar curves are images of straight lines (camera rays of the other camera), we consider the computation of line images in Section 4.3 and, most importantly, the above mentioned strong links between self-calibration and plumb-line calibration of non-perspective cameras.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Calibrated Case</head><p>The epipolar geometry for a pair of calibrated central cameras can be represented by the essential matrix E <ref type="bibr" target="#b323">[324]</ref> and the associated epipolar constraint on back-projected image points:</p><formula xml:id="formula_72">(B i 2 ) T ([t] × R) E B i 1 = 0, (4.1)</formula><p>where t and R are the relative translation and rotation between the two views. Note that since we directly work with the directions of camera rays, the above formulation is independent of the type of central camera and also works when considering two cameras of different types.</p><p>The essential matrix for non-central cameras was introduced by Pless <ref type="bibr" target="#b401">[402]</ref>. For non-central cameras, we must consider the complete camera rays instead of only their directions as in Equation (4.1). Let L 1 and L 2 be the camera rays associated with matching image points in a pair of views. They must intersect one another (in the 3D point whose images are the two matching image points). Putting together Equations (1.2) and (1.3) of Section 1.2, this leads to the essential matrix and epipolar constraint for non-central calibrated cameras:</p><formula xml:id="formula_73">L T 2 -[t] × R R R 0 L 1 = 0. (4.</formula><p>2)</p><p>The essential matrix acts on Plücker coordinates of camera rays and is thus of size 6 × 6.</p><p>In <ref type="bibr" target="#b401">[402]</ref>, Pless also gave the epipolar constraint for continuous camera motion. We refer to Section 6.2 for notes on the estimation of the central and non-central essential matrices and subsequent camera motion estimation.</p><p>Sturm specialized Pless' epipolar constraint to calibrated axial, twoslit, and central cameras, for all cases with finite/infinite slits respectively optical centers <ref type="bibr" target="#b471">[472]</ref>. For axial and two-slit cameras, the essential matrices are of size 5 × 5 and 4 × 4, respectively. It was also shown how to establish trifocal matching tensors for these camera models.</p><p>Gasparini and Sturm showed how to derive matching tensors for line images as opposed to images of points, for central, two-slit, axial, and fully non-central calibrated cameras <ref type="bibr" target="#b157">[158]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Uncalibrated Case</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Generalities</head><p>The epipolar geometry of uncalibrated perspective cameras can be represented by the fundamental matrix <ref type="bibr" target="#b136">[137,</ref><ref type="bibr" target="#b212">213]</ref>. How about other central cameras or the case of non-central ones? The answer is immediate if back-projection can be represented as a matrix-vector product (or, by polynomials or rational polynomials, cf. Section 3.1.8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Central cameras.</head><p>Let B i be the back-projection matrix of a central camera, of size 3 × n and operating on lifted image coordinates La (q), where a and n depend on the camera model (e.g., n = 5 and a = 3 for the one-coefficient classical radial distortion model, cf. Section 3.1.1). By plugging B i 1 ∼ B i La (q 1 ) and B i 2 ∼ B i La (q 2 ) into the epipolar constraint (4.1), we directly get the fundamental matrix:</p><formula xml:id="formula_74">( La (q 2 )) T (B i ) T [t] × R B i F La (q 1 ) = 0. It is of size n × n.</formula><p>It is of course possible to derive the fundamental matrix for two cameras of different types with back-projection matrices of different sizes 3 × m respectively 3 × n:</p><formula xml:id="formula_75">( Lb (q 2 )) T (B i 2 ) T [t] × R B i 1 F La (q 1 ) = 0. (4.</formula><p>3)</p><p>The fundamental matrix will accordingly be of size m × n.</p><p>Due to the presence of the rank-2 skew-symmetric matrix [t] × , the fundamental matrix is always of rank smaller or equal than 2, in general of rank 2.</p><p>Non-central cameras. The situation for non-central cameras is analogous: it suffices to plug back-projection matrices B l of size 6 × n into the non-central epipolar constraint (4.2). A difference from the central case is that the rank of the non-central fundamental matrix depends on the rank of the back-projection matrices as well as on the relative position of the two cameras. For example, the back-projection matrices of two-slit cameras are of rank 4; the rank of the two-slit fundamental matrix is 4 if the two views are in general position, and lower if the two views share one slit or if their slits are incident (cf. <ref type="bibr" target="#b138">[139]</ref> and Section 4.2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Epipolar curves.</head><p>For perspective cameras, Fq 1 represents the epipolar line in the second view, associated with a point q 1 in the first view. In our general setting, epipolar lines are replaced by epipolar curves. It is easy to see that the order of epipolar curves is identical to the order of image coordinate lifting (a and b respectively in Equation (4.3)). For example, for liftings of order 2, 3 respectively 4, epipolar curves are conics, cubic respectively quartic curves. This observation tells us immediately that for all cameras that can be modeled by back-projection matrices operating on second order lifted image coordinates, epipolar curves are conics, e.g.,: linear pushbroom cameras and other types of two slits (cf. Section 3.1.4), linear oblique cameras (cf. Section 3.4.3), para-catadioptric cameras (cf. Section 2.3.1), the division model (cf. Section 3.1.8), all GLCs (cf. Section 3.4.4), etc. Also, epipolar curves for the one-coefficient classical radial distortion model (cf. Section 3.1.1), are obviously cubic (cf. <ref type="bibr" target="#b565">[566]</ref>).</p><p>As described in Section 3.4.2, non-central cameras may or may not be in standard stereo configuration: they are, if epipolar curves in the two images are in a one-to-one correspondence. This is for example the case for two-slit cameras with relative positions as mentioned above (e.g., they share a slit), which corresponds to a fundamental matrix of rank 2. Otherwise, one can still compute epipolar curves in both images, but they now form 2D families in each image and are no longer in one-to-one correspondence. An algebraic consequence is an increase in the rank of the fundamental matrix, for the above example of two-slit cameras, an increase from 2 to 3 or 4.</p><p>Let us note that Rademacher and Bishop already described how to understand epipolar curves for arbitrary non-central images <ref type="bibr" target="#b410">[411]</ref>. They also suggested the concept of an internal epipolar geometry, i.e., an epipolar geometry of a non-central camera with itself: when considering any ray of the camera, all other rays that cut it, are associated with an epipolar curve. For oblique cameras, no internal epipolar curves exist by definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Epipoles.</head><p>For central cameras, one may define epipoles analogously to the pinhole case: the image of the optical center of one camera is the epipole in the other camera's image plane. Further, all epipolar curves go through the epipole. In general, there may be more than one epipole, for example for central catadioptric cameras (see an example in Figure <ref type="figure">4</ref>.1) for which each 3D point is mathematically imaged in two points (cf. Section 3.1.5). In that case, all epipolar curves go through all epipoles. Another example is the classical radial distortion model; in the case of one distortion coefficient, there are three epipoles: either three real points or one real and two conjugate imaginary points.</p><p>For non-central cameras, the notion of optical center and thus of epipole does not exist anymore. However, in the special cases of axial and two-slit cameras, a weaker notion can be defined as follows: the image of the camera axis (or axes in the case of a two-slit camera) in the other camera gives a curve that is incident to all epipolar curves.</p><p>The non-polynomial case. The results in this section so far only apply to cameras whose back-projection can be modeled by matrices. In other cases, e.g., for most of the fisheye models of Section 3.1.7 (cf. also Table <ref type="table">3</ref>.1), fundamental matrices do not exist. The epipolar geometry is still defined of course, it just cannot be expressed in the usual matrix-vector style. The epipolar geometry has been studied for some of these cases, see the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Existing Works</head><p>In the following, a few existing works on the epipolar geometry of nonperspective cameras are reviewed.</p><p>Classical polynomial distortion model. Zhang, in <ref type="bibr" target="#b565">[566]</ref>, studied the epipolar geometry of two radially distorted images and showed that epipolar curves are cubic if only one radial distortion coefficient is considered. He proposed a non-linear optimization method for estimating the distortion coefficients of the two images and the perspective fundamental matrix. <ref type="bibr" target="#b481">[482,</ref><ref type="bibr" target="#b478">479,</ref><ref type="bibr" target="#b480">481]</ref>. They showed that epipolar curves are conics and gave formulas for computing these conics, from known camera calibration and motion. They also showed the existence of two epipoles per image, in general, which are incident with each epipolar conic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Central catadioptric cameras. Svoboda et al. studied the epipolar geometry for central catadioptric cameras</head><p>Geyer and Daniilidis showed that the epipolar geometry of para-catadioptric cameras can be represented via a fundamental matrix <ref type="bibr" target="#b165">[166]</ref>. It acts on lifted coordinates of image points and is of size 4 × 4 if one assumes that images have square pixels (in the general case, it is of size 6 × 6 <ref type="bibr" target="#b470">[471]</ref>). It can thus be estimated linearly from 15 or more point matches. Geyer and Stewénius proposed a minimal 9-point algorithm to compute the fundamental matrix between two para-catadioptric cameras, based on constructing a Gröbner basis of the polynomial equation system given by the point matches <ref type="bibr" target="#b171">[172]</ref>.</p><p>Geyer and Daniilidis <ref type="bibr" target="#b167">[168]</ref> and independently, Sturm <ref type="bibr" target="#b470">[471]</ref>, showed how to systematically derive multi-view relationships for paracatadioptric cameras. Their approaches use a direct extension of the framework used for perspective images by Faugeras and Mourrain <ref type="bibr" target="#b137">[138]</ref> and Triggs <ref type="bibr" target="#b510">[511]</ref>. They allow to re-derive the above para-catadioptric fundamental matrix and in addition, trifocal or quadrifocal tensors. Sturm showed that using the same framework, one may obtain multiview relations for any combination of para-catadioptric, perspective, or orthographic images <ref type="bibr" target="#b470">[471]</ref>. It is clear from Section 4.2.1 that this easily generalizes to all types of camera models represented by backprojection matrices.</p><p>Other central catadioptric cameras than para-catadioptric ones, i.e., those based on hyperboloidal or ellipsoidal mirrors, are less nicely modeled. The main reason for this is that mathematically, each image point is back-projected to two camera rays (cf. Section 3.1.5). Although only one of them is physically plausible, it is not possible to dissociate the two rays while expressing them using polynomials in the image point coordinates. Sturm and Barreto showed that it is possible to represent the two rays by a single geometric object (a line complex) which can be represented using polynomials <ref type="bibr" target="#b472">[473]</ref>. This enabled the definition of back-projection and fundamental matrices, the latter being of size 15 × 15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-central catadioptric cameras.</head><p>Cone-shaped mirrors usually lead to non-central catadioptric cameras (cf. Section 2.3.1). The epipolar geometry for such cameras, when restricted to planar motions, was derived by Yagi and Kawato <ref type="bibr" target="#b541">[542]</ref>. As for two cameras mounted one on top of the other, with aligned mirror axes, the epipolar geometry is simple <ref type="bibr" target="#b455">[456]</ref>: the mirror axes pierce the two image planes in the epipoles and the radial lines going through the epipoles, are epipolar lines. One may see that this situation is always true for pairs of radially symmetric axial cameras with aligned camera axes.</p><p>Würz-Wessel proposed a numerical approach for computing epipolar curves in images of free-form mirror surfaces <ref type="bibr" target="#b537">[538]</ref>.</p><p>Central and non-central panoramas. <ref type="bibr">Ishiguro et al.</ref> showed that in central panoramic images (see Section 2.1.1), 3D lines are imaged as sinusoidal curves <ref type="bibr" target="#b245">[246,</ref><ref type="bibr" target="#b246">247]</ref>. Consequently, epipolar curves are sinusoidal too, a result also derived by McMillan and Bishop <ref type="bibr" target="#b337">[338]</ref> and Bunschoten and Kröse <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b68">69]</ref>. Torii et al. studied the multi-view geometry of central panoramas <ref type="bibr" target="#b508">[509]</ref>.</p><p>Huang et al. studied the epipolar geometry for non-central panoramic images generated by a perspective 1D sensor rotated about an axis parallel to the sensor line but not necessarily passing through the 1D camera's optical center <ref type="bibr" target="#b235">[236,</ref><ref type="bibr" target="#b236">237,</ref><ref type="bibr" target="#b237">238]</ref>. Further, the 1D sensor may be oriented such that the sensor line, the optical center, and the rotation axis are not coplanar. Huang et al. derived the epipolar geometry for such images as well as several special cases <ref type="bibr" target="#b235">[236,</ref><ref type="bibr" target="#b237">238]</ref> and proposed motion estimation methods <ref type="bibr" target="#b236">[237]</ref>. Šivic and Pajdla proposed an in-depth geometric study of non-central panoramic images and their stereo configurations <ref type="bibr" target="#b449">[450]</ref>.</p><p>Menem and Pajdla established the epipolar geometry between a non-central panoramic image and a pinhole image <ref type="bibr" target="#b341">[342]</ref>.</p><p>Polynomial and rational polynomial models. Claus and Fitzgibbon uncovered the fundamental matrix for the bi-quadratic rational function model (see Section 3.1.8) <ref type="bibr" target="#b97">[98]</ref>. It is a 6 × 6 matrix that maps lifted image points to the six coefficients (five plus a scale factor) of the epipolar conics in the other image. If one restricts the model to the one-parameter division model, then image lines are circles and epipolar geometry may be written via a 4 × 4 fundamental matrix <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b97">98]</ref>.</p><p>Barreto and Daniilidis showed that this is the case even if the two cameras have different distortion coefficients <ref type="bibr" target="#b30">[31]</ref>. They also showed how to compute the fundamental matrix and extract the distortion coefficients from it. In <ref type="bibr" target="#b34">[35]</ref>, they showed that the combination of a para-catadioptric camera and one following the division model is also subject to a fundamental matrix. Two-slit cameras. Gupta and Hartley showed that the epipolar geometry of linear pushbroom cameras can be modeled by a 4 × 4 fundamental matrix, expressed in lifted point coordinates <ref type="bibr" target="#b196">[197]</ref>:</p><formula xml:id="formula_76">q 2,1 q 2,3 q 2,1 q 2,2 q 2,2 q 2,3 q 2 2,3 F      q 1,1 q 1,3 q 1,1 q 1,2 q 1,2 q 1,3 q 2 1,3      = 0,</formula><p>where q j,k denotes the kth homogeneous coordinate of a point in image j. Images of straight lines are hyperbolas and consequently, so are epipolar curves. Feldman et al. have developed the fundamental matrix for more general two-slit cameras <ref type="bibr" target="#b138">[139]</ref>. To be precise, they considered the case where a two-slit image is constructed from images acquired by a perspective camera that translates by a constant speed parallel to the image plane, such that at each acquisition instant t, the vertical pixel column at coordinate s(t) = αt + β is taken from the perspective image, where α and β can be arbitrary values. The final image is obtained by concatenating these pixel columns. For any pair of values α and β, this construction corresponds to a two-slit camera. The first slit is the line of optical centers corresponding to the translational motion whereas the second slit depends on the values of α and β. The two slits, when seen along their common perpendicular, form a right angle.</p><p>Feldman et al. showed that two two-slit images constructed from the same original image sequence but with different values α and β are subject to an epipolar constraint embodied by a 6 × 6 fundamental matrix, acting on the lifted image coordinates. They also revealed the general conditions under which two two-slit images have an epipolar geometry (see also Section 3.4.2); this is the case if the two cameras share one slit completely or if each slit of one camera intersects both slits of the other camera. In the first case, the stereo system has a pencil of epipolar planes, in the second case, these are replaced by epipolar quadrics.</p><p>Note that even if the cameras are not in one of these configurations, there exists a fundamental matrix for two-slit images, of size 6 × 6. It allows, as usual, to compute the epipolar curve (here, a conic) for every image point. However, epipolar curves in the two images are no more in a one-to-one correspondence: different points on the same epipolar curve in one image, may be associated with different epipolar curves in the other image.</p><p>Khan et al. showed that there exists a fundamental matrix between a linear pushbroom panorama and a perspective image and used this for estimating the position of a query perspective image with respect to a set of georeferenced panoramic images <ref type="bibr" target="#b272">[273]</ref>.</p><p>One-dimensional radial camera model. Thirthala and Pollefeys showed the existence of quadrifocal tensors for the 1D radial camera model <ref type="bibr" target="#b499">[500]</ref>; refer to Section 3.1.9 for some more details.</p><p>The case of pure rotations. It is well known that two images taken by a pinhole camera undergoing a pure rotation about its optical center are related by a homography. How about other central cameras?</p><p>We consider the case of cameras modeled by back-projection matrices of size 3 × n, acting on a-order lifted image coordinates. For any pair of matching points q 1 and q 2 , we must have:</p><formula xml:id="formula_77">R B i 1 La (q 1 ) ∼ B i 2 La (q 2 ),</formula><p>where R is the rotation relating the two images. This constraint can be written as:</p><formula xml:id="formula_78">{R B i 1 La (q 1 )} × {B i 2 La (q 2 )} = 0 3 ,</formula><p>which is equivalent to three bilinear constraints (only two being independent) on the lifted image coordinate vectors:</p><formula xml:id="formula_79">( La (q 2 )) T M j La (q 1 ) = 0, j = 1• • • 3,</formula><p>with three matrices M j of size n × n which depend on the rotation matrix R and the two back-projection matrices.</p><p>There is no bijective homography in general, unlike in the pinhole case, but it is replaced by the above bilinear constraints, which allow, for any point q 1 , to compute all potential matches q 2 . In general, there are only finitely many matches. To be precise, since the matching constraints are linear in the lifted coordinates of q 2 , there are in general at most a 2 potential matches (two independent constraints on a-order lifted coordinates).</p><p>As for the 1D radial camera model and the case of pure rotations, Thirthala and Pollefeys showed the existence of trifocal tensors <ref type="bibr" target="#b499">[500,</ref><ref type="bibr" target="#b500">501]</ref> (cf. Section 3.1.9).</p><p>The case of planar scenes. Like for pinhole cameras, the case of a planar scene is very similar to that of a rotating central camera. For example, Draréni et al. derived the expression for plane homographies for linear pushbroom cameras and proposed a calibration approach using planar grids <ref type="bibr" target="#b119">[120]</ref>. Fitzgibbon mentioned plane homographies for the division model <ref type="bibr" target="#b144">[145]</ref>. Sturm and Barreto showed the existence of plane homographies for all central catadioptric cameras <ref type="bibr" target="#b472">[473]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Images of Lines and the Link between Plumb-line Calibration and Self-calibration of Non-perspective Cameras</head><p>As will be discussed in Section 5.2.1, non-perspective camera parameters can in general be fully calibrated from images of straight lines.</p><p>There is a strong link between this, epipolar geometry and selfcalibration: from point matches between images, one can compute the epipolar geometry. Epipolar geometry allows to compute epipolar curves, which are nothing else than images of straight lines (of camera rays of the respective other camera). It is thus obvious that epipolar geometry allows for the self-calibration of non-perspective imaging models, at least partially. One may even directly use any plumb-line technique for self-calibration: from the epipolar geometry, one may generate arbitrarily many epipolar curves and use these as input to plumb-line calibration. This direct link between plumb-line calibration, epipolar geometry, and self-calibration is implicitly present in all non-perspective self-calibration approaches, although it has not been explained before in its full generality. It will be developed further in Section 5.3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Line images.</head><p>As for the computation of a 3D line's image, this is particularly simple for cameras modeled by back-projection matrices.</p><p>In the case of non-central cameras and back-projection matrices of size 6 × n, operating on a-order lifted image coordinates L a (q), the image of a 3D line L is computed as follows. An image point q lies on the line image if its back-projected camera ray intersects the line L, i.e., if (cf. Equation (1.3)):</p><formula xml:id="formula_80">L T 0 3×3 I 3×3 I 3×3 0 3×3 B l c T L a (q) = 0.<label>(4.4)</label></formula><p>We immediately see that the image of L is the curve of order a whose coefficients are given in the vector c. In the case of a = 2 for example, the line image is a conic; the six coefficients defining its symmetric matrix representation, are contained in c. An example of a line image in a linear pushbroom panorama is shown in Figure <ref type="figure">4</ref>.2.</p><p>In the case of central cameras, we can obtain the analogous result after computing the full back-projection ray from B f (the origin) and B i ∼ B i L a (q) (the direction of the ray):</p><formula xml:id="formula_81">B i L a (q) 0 ∼ B i 0 6×n L a (q).</formula><p>By inserting this in Equation (4.4), we compute the line image for central cameras as:  The other curves show the images of lines whose interpretation planes (plane spanned by a line and the optical center) form angles of 70, 50, 30, and 10 degrees with the optical axis, respectively. The limiting case of 0 degrees corresponds to lines that are coplanar with the optical axis and whose images are lines going through the distortion center. Although in general, line images are not algebraic curves, the parts within the hemispheric field of view, i.e., within the black circle, can be relatively well approximated by conics.</p><formula xml:id="formula_82">c T ∼ L T 0 3×3 I 3×3 I 3×3 0 3×3 B i 0 ∼ L T 0 B i .</formula><p>Let us denote by l the second sub-vector of length 3 of L. This represents the line at infinity of the plane spanned by the optical center (the origin) and L. The line image can then also be written as:</p><formula xml:id="formula_83">c T ∼ l T B i .</formula><p>Non-polynomial back-projection. In this case, line images cannot be computed by matrix-vector products. Still, it suffices to plug into Equation (4.4) the expression of back-projected image points to get an equation defining the line image, allowing for example to plot it. For example, Figure <ref type="figure">4</ref>.3 shows line images for the equiangular camera model; the figure suggests that although they are not algebraic curves, e.g., are not conics, they can be well approximated by conics within a hemispheric field of view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Calibration Approaches</head><p>This section aims at providing an overview of different calibration approaches, mainly with respect to the type of images or other information that are used for calibration, with an emphasis on approaches developed for omnidirectional cameras. The section is a far from exhaustive treatment of the topic of calibration. A few other references are as follows. An excellent overview of photogrammetric calibration approaches and their historical development, together with the development of distortion models, is the paper <ref type="bibr" target="#b94">[95]</ref> by Clarke and Fryer, see also a paper by Remondino and Fraser <ref type="bibr" target="#b425">[426]</ref> and a book section by Boland <ref type="bibr" target="#b54">[55]</ref>. Among the many books covering the topic, we may cite the Manual of Photogrammetry, its fourth edition of 1980 <ref type="bibr" target="#b450">[451]</ref> and the more recent fifth edition <ref type="bibr" target="#b335">[336]</ref>. The article <ref type="bibr" target="#b429">[430]</ref> by Roelofs is an interesting extended account of the state of the art on calibration up to 1951. Other relevant books are <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b194">195,</ref><ref type="bibr" target="#b212">213,</ref><ref type="bibr" target="#b286">287]</ref> to cite just a few.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Calibration Using Calibration Grids</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Three-Dimensional Grids</head><p>This is probably the most well-known calibration approach and will not be detailed much further. Let us just note that in order to calibrate omnidirectional cameras, several researchers used hollow 3D calibration objects painted with markers or patterns on the inside, such that cameras are put inside the object for calibration, see, e.g., <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b412">413,</ref><ref type="bibr" target="#b454">455,</ref><ref type="bibr" target="#b505">506,</ref><ref type="bibr" target="#b518">519,</ref><ref type="bibr" target="#b551">552]</ref>. Usually, the object was used to gather 3D-to-2D matches which could then be used like in traditional calibration approaches. Ying and Hu used two hemispherical calibration objects with different radius to calibrate a model akin to the two-plane one (cf. Section 3.1.3) <ref type="bibr" target="#b551">[552]</ref>.</p><p>Among the many other approaches, let us mention one by Shih et al. who proposed a method for calibrating one radial distortion coefficient as well as the perspective camera parameters, via the solution of an 8 × 8 eigenvalue problem <ref type="bibr" target="#b447">[448]</ref>. This is very similar to recent approaches for the simultaneous estimation of multi-view geometry (e.g., the fundamental matrix) and a distortion model (e.g., the division model), for example the approaches of Fitzgibbon and Claus <ref type="bibr" target="#b97">[98,</ref><ref type="bibr" target="#b144">145]</ref> and Mičušik and Pajdla <ref type="bibr" target="#b343">[344,</ref><ref type="bibr" target="#b345">346,</ref><ref type="bibr" target="#b346">347,</ref><ref type="bibr" target="#b348">349]</ref>. Shih et al. also gave a similar calibration method for the usage of planar grids.</p><p>Bastanlar et al. developed a calibration approach for central catadioptric cameras that is akin to the DLT approach for pinhole cameras <ref type="bibr" target="#b39">[40]</ref>: computation of a projection matrix from 3D-to-2D matches, followed by the extraction of intrinsic parameters. In the case of catadioptric cameras, the projection matrix is somewhat different from that of a pinhole camera: as shown by Sturm and Barreto <ref type="bibr" target="#b472">[473]</ref>, it is possible to formulate a projection matrix but which does not directly map 3D points to 2D points, the reason being the existence of two theoretical image points (cf. Section 3.1.5). Instead, the projection matrix maps 3D points to a degenerate dual conic, representing these two image points. Besides this difference, DLT-like calibration is also possible, as exploited by Bastanlar et al. <ref type="bibr" target="#b39">[40]</ref>. Gasparini et al. and Barreto et al. developed similar procedures for using planar grids <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b158">159]</ref>.</p><p>Huang et al. proposed a calibration approach for non-central panoramic images and compared it experimentally to a line-based and a self-calibration method <ref type="bibr" target="#b238">[239]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Planar Grids</head><p>Many approaches have been developed for using planar grids in camera calibration, due to their ease of manufacture, storage, and use.</p><p>Planar or near-planar objects, e.g., crosses, are routinely used in photogrammetry, usually via a (self-calibrating) bundle adjustment procedure. Computer vision researchers proposed "closed-form" type solutions <ref type="bibr" target="#b474">[475,</ref><ref type="bibr" target="#b566">567]</ref>, which are suboptimal and should be followed by a bundle adjustment (cf. Section 6.4).</p><p>In this section, we concentrate on plane-based calibration approaches developed for omnidirectional cameras or other cameras with general distortion profiles. There are many approaches for global camera models, which usually apply similar procedures: an optional initialization procedure, followed by a bundle adjustment. We will thus not describe them in any more detail and refer the reader to the references on camera models in Section 3.1. As for the local models, the calibration procedures designed for them are mostly already outlined in Section 3.2. In the following, we give a few details on plane-based approaches for discrete camera models, for two variants thereof -raybased models and discrete samples of the distortion curve.</p><p>Ray-based calibration. Several similar approaches for ray-based or raxel-based calibration were proposed, e.g., by Gremban et al. <ref type="bibr" target="#b188">[189]</ref>, Champleboux et al. <ref type="bibr" target="#b84">[85]</ref>, and Grossberg and Nayar <ref type="bibr" target="#b190">[191]</ref>. The principle is simple: two or more images of planar calibration grids are acquired, with a known motion of the camera or grid between the acquisitions. A dense matching between the grid in each position, and the camera image, is carried out. Hence, for each pixel, one can compute its camera ray by fitting a 3D line to the set of matched grid points, which can be put in the same coordinate frame since the motion between image acquisitions is known. This way, camera rays can be computed for each individual pixel (or some other spatial sampling of the image area, be it sub-pixel or super-pixel-wise).</p><p>Debaecker et al. used a similar procedure to calibrate their conebased camera model from multiple images of planar grids <ref type="bibr" target="#b107">[108]</ref>. These methods can be considered as dense versions of the two-plane approach, cf. Section 3.1.3. This type of approach was generalized by Sturm and Ramalingam in order to allow an image acquisition without having to know the camera/grid motion <ref type="bibr" target="#b476">[477]</ref>. While the approach based on the knowledge of camera/grid motion can in theory work with two images, a minimum of three images is required in the case of unknown motion. Sturm and Ramalingam showed that from matches for three images, the unknown camera/grid motion can be recovered via the computation of particular trifocal tensors. Once the motion is known, the above approach can be readily applied to compute the camera rays and thus to finalize the calibration. In <ref type="bibr" target="#b476">[477]</ref>, the minimal case of three images was considered; this was extended toward using multiple images in <ref type="bibr" target="#b417">[418]</ref>, allowing for an easier calibration of omnidirectional cameras. Sturm and Ramalingam developed several variants of their approach, for using planar as well as 3D calibration grids, as well as for calibrating fully non-central <ref type="bibr" target="#b476">[477]</ref>, central <ref type="bibr" target="#b476">[477]</ref>, or axial cameras <ref type="bibr" target="#b419">[420]</ref>. In <ref type="bibr" target="#b415">[416]</ref>, they gave a minimal solution for the calibration of a central camera using matches for four pixels and three grids.</p><p>Gonçalves and Araújo used this approach to first compute pixel-ray matches, to which they then fitted a catadioptric model <ref type="bibr" target="#b179">[180]</ref>.</p><p>Dunne et al. used a structured-light type approach where an LCD screen is used as planar calibration grid, allowing to achieve dense matches between the grid and the image through the display of a sequence of particular patterns <ref type="bibr" target="#b124">[125]</ref>. Matches between the grid and one of the input images are used to define a warping from the image to the grid. The warping, when applied to that input image, will by definition lead to a perspectively correct image of the grid -the grid itself. This warping thus eliminates all non-perspective distortions. After applying it to other images of grids (they do not need to be of the same grid), one may use any calibration approach for perspective cameras to calibrate the perspective part of the camera projection, e.g., the methods of Zhang or Sturm and Maybank for planar grids <ref type="bibr" target="#b474">[475,</ref><ref type="bibr" target="#b566">567]</ref>. This approach combines the ideas of warp-based approaches (cf. Section 3.2.2) and ray-based calibration.</p><p>Discrete sampling of the distortion curve. In the 1950s, Hallert developed formulas for non-linearly optimizing either intrinsic or extrinsic parameters of a camera looking at a planar grid, supposing the respective other set of parameters is known <ref type="bibr" target="#b200">[201]</ref>. He also included the computation of a radial distortion model in his procedure. The model he used is of the discrete sampling type (cf. Section 3.3.2): he used a calibration grid of 5 × 5 regularly distributed points and seemed to assume that the grid can be positioned such that the center point coincides with the distortion center. Then, he computed the radial distance in the undistorted image, for every circle centered in the grid's center point and going through other points in the grid. From the computed values, he seemed to interpolate the complete distortion function, between distorted and undistorted radial distances. This is probably one of the first examples where radial distortion is determined by computing samples of the distortion function, as also done for example by Hartley and Kang <ref type="bibr" target="#b208">[209]</ref> and Tardif et al. <ref type="bibr" target="#b490">[491]</ref>, see below.</p><p>Hartley and Kang proposed a set of methods for calibrating cameras with general radial distortion <ref type="bibr" target="#b208">[209]</ref>. Their key observation is that point matches between a planar calibration grid and an image with whatever radial distortion are subject to a constraint akin to the epipolar constraint and that the epipole in the image plane is nothing else than the center of radial distortion. The latter may thus be computed by fitting a fundamental matrix to the point matches and extracting the epipole from it (one may also use multiple images and fundamental matrices to get a better estimate). It was shown that this can also be done, in a similar fashion, when using a 3D calibration grid or by selfcalibration, i.e., from images of an unknown scene (see Section 5.3.1). Hartley and Kang then used the assumptions of a radially symmetric camera and of a monotonic distortion curve, to compute the latter, from multiple images of grids. They showed how to compute a discrete sampling of the distortion curve as well as how to fit parametric models thereof.</p><p>Hartley and Kang's approach was studied experimentally by Barreto et al. for the task of calibrating endoscopic cameras using planar grids <ref type="bibr" target="#b36">[37]</ref>. It was found to be superior to using the division or a classical model for radial distortion.</p><p>Tardif et al. proposed two approaches for plane-based calibration of a general radially symmetric radial distortion model (cf. Section 3.3.2) <ref type="bibr" target="#b490">[491]</ref>. The approach is similar in spirit to Hartley and Kang's although the method for finding the distortion center is less elegant. The basic versions of these approaches work for central cameras; an extension for non-central ones was also proposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">One-Dimensional Grids: Sticks or Circular Grids Positioned in Radial Planes</head><p>Adorni et al. proposed a calibration method for the task of backprojecting an omnidirectional image onto a fronto-parallel ground plane <ref type="bibr" target="#b2">[3]</ref>. They made the assumption of a radially symmetric camera and acquired an image of a linear calibration object lying on the ground plane, carefully positioned such that the camera's optical axis "pierces" the object at a known position. For each marker on the calibration object, the distance to the optical axis as well as the distance of the image point from the assumed distortion center is recorded in a look-up table. By interpolating the information contained in this look-up table and using the assumption of radial symmetry, each image point can be mapped to the point on the ground plane that it sees, and vice versa, i.e., back-projecting the image to the ground plane is possible. A similar approach was proposed earlier by Southwell et al. which instead of markers on the calibration object used one with a fractal Grey code pattern printed on it <ref type="bibr" target="#b454">[455]</ref>. As for classical cameras, i.e., non-omnidirectional ones, using sticks with markers is probably the de facto standard approach for calibrating commercial multi-camera systems for motion capture, although many alternative approaches exist, e.g., <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b479">480]</ref>. How to calibrate a single camera using a 1D object, was shown by Zhang <ref type="bibr" target="#b567">[568]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Calibration Grids at Infinity -Collimators, Stellar Calibration, Vanishing Points</head><p>A classical photogrammetric calibration technique exploits arrays of collimators or devices with a movable collimator <ref type="bibr" target="#b94">[95,</ref><ref type="bibr" target="#b201">202,</ref><ref type="bibr" target="#b268">269,</ref><ref type="bibr" target="#b491">492]</ref>.</p><p>The incidence angle of each collimator with the lens to be calibrated is measured, e.g., using theodolites or a goniometric apparatus. One can interpret such an array as a calibration grid lying at infinity: instead of providing matches between image positions and 3D points, they directly provide directions of camera rays for image positions. If a planar array is used (collimator beams are coplanar) and positioned such that that plane contains the distortion center, one can directly sample the radial distortion function from the observations, or fit any radial distortion model to them. In the case of a non-planar array, tangential distortions may be measured as well. More generally speaking, one may fit any global or local central camera model to data obtained from collimators.</p><p>Another classical method termed stellar calibration consisted in exploiting the well-known angular position of stars to calibrate cameras (care had to be taken though to take into account, e.g., atmospheric refractions) <ref type="bibr" target="#b150">[151]</ref>. This may again be seen as using a calibration grid lying at infinity.</p><p>Finally, we may mention calibration techniques exploiting vanishing points arising from scene structure, e.g., associated with parallel edges of buildings. Knowing the angles between the associated directions in 3D gives essentially the same input as collimators and images of stars, although usually with (much) lower accuracy. Also, the number of vanishing points in typical scenes is low, often not higher than three. Thus, this kind of observation could typically not be used to measure non-perspective distortions. However, from three vanishing points with known angles, up to three intrinsic parameters of the pinhole model can be estimated. Methods doing so are for example those by Gracie <ref type="bibr" target="#b184">[185]</ref>, Caprile and Torre <ref type="bibr" target="#b80">[81]</ref>, and Echigo <ref type="bibr" target="#b125">[126]</ref>.</p><p>Hughes et al. showed that vanishing points arising from sets of parallel lines may be used to calibrate the distortion center of radially symmetric distortion models <ref type="bibr" target="#b240">[241]</ref>. They did this for the division model (which was used as an approximation to the equiangular model), by showing the following properties. Lines are imaged to circles and a set of parallel lines are imaged to circles which intersect in two vanishing points; these correspond to the two mathematical images under the division model, of the point at infinity associated with the set of lines. The line joining the two vanishing points also contains the center of radial distortion. Thus, when observing at least two different sets of parallel lines, the distortion center can be computed in a straightforward manner. It seems that this approach may be used for any radially symmetric distortion model. In <ref type="bibr" target="#b239">[240]</ref>, Hughes et al. extended the previous approach by estimating another intrinsic parameter besides the distortion center, namely the proportionality factor k of the equiangular camera model (cf. Section 3.1.7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Using Images of Individual Geometric Primitives</head><p>By individual primitives, we mean that their relative position is not known.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Lines -The Plumb-Line Approach</head><p>Brown introduced the so-called plumb-line approach which allows to calibrate non-perspective distortions from images of straight lines <ref type="bibr" target="#b63">[64]</ref>. Since then, many such approaches have been proposed for different camera models, see below. Line-based calibration approaches are attractive since they allow to separate perspective from non-perspective parts of the imaging model and since their algebraic complexity is typically lower than that of full calibration formulations, allowing for easier closed-form type solutions. Further, since lines are omnipresent in urban or interior scenes or in man-made objects, they allow for a calibration in many applications where calibration grids cannot be employed.</p><p>Classical polynomial distortion model. Kang proposed a plumbline method where snakes that are constrained by the radial distortion model are used to fit to images of straight lines, thereby estimating the distortion coefficients <ref type="bibr" target="#b262">[263]</ref>.</p><p>Alvarez et al. <ref type="bibr" target="#b13">[14]</ref> used a classical distortion model and a plumb-line type method, based on observations of sets of collinear points. They defined a cost function which measures how close undistorted points are from being collinear. That measure is not based on the Euclidean distance between points and lines, i.e., is an algebraic distance. However, Alvarez et al. showed that it is possible to find the distortion coefficients that are globally optimal under that measure, using a standard resultant-based technique. This is possible for up to two distortion coefficients but hints on how to possibly handle more than two are also given in <ref type="bibr" target="#b13">[14]</ref>.</p><p>Many other plumb-line approaches were proposed for the classical as well as other distortion models, e.g., <ref type="bibr" target="#b126">[127,</ref><ref type="bibr" target="#b239">240,</ref><ref type="bibr" target="#b240">241,</ref><ref type="bibr" target="#b271">272,</ref><ref type="bibr" target="#b336">337,</ref><ref type="bibr" target="#b407">408,</ref><ref type="bibr" target="#b426">427,</ref><ref type="bibr" target="#b484">485,</ref><ref type="bibr" target="#b502">503,</ref><ref type="bibr" target="#b509">510,</ref><ref type="bibr" target="#b519">520,</ref><ref type="bibr" target="#b520">521,</ref><ref type="bibr" target="#b552">553,</ref><ref type="bibr" target="#b574">575]</ref>. Ricolfe-Viala and Sánchez-Salmerón combined a plumb-line constraint with a constraint expressing the invariance of the cross-ratio of collinear points, to calibrate a classical distortion model from images of planar grids <ref type="bibr" target="#b427">[428]</ref>.</p><p>Central panoramas. Smadja et al. proposed a plumb-line calibration method for circular central panoramas, acquired by a 1D camera rotating about its optical center <ref type="bibr" target="#b451">[452]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Catadioptric cameras.</head><p>As for catadioptric cameras, Geyer and Daniilidis introduced a plumb-line method <ref type="bibr" target="#b163">[164,</ref><ref type="bibr" target="#b164">165,</ref><ref type="bibr" target="#b166">167]</ref>. Based on their unified model (see Section 3.1.5), they showed that from two respectively three images of lines, one may fully calibrate a central hyper-catadioptric respectively para-catadioptric camera. They gave an algorithm for the para-catadioptric case; Barreto and Araújo developed an algorithm for hyper-catadioptric cameras <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">31]</ref> as well as an improved algorithm for para-catadioptric ones that is able to fully compute the mirror shape <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32]</ref>. Ying and Hu, Vandeportaele et al., and Wu et al. gave similar algorithms <ref type="bibr" target="#b517">[518,</ref><ref type="bibr" target="#b536">537,</ref><ref type="bibr" target="#b550">551]</ref>. Previously, Geyer and Daniilidis also proposed a method for calibrating para-catadioptric cameras from images of sets of parallel lines <ref type="bibr" target="#b162">[163]</ref>.</p><p>It is noteworthy that central catadioptric cameras can be fully calibrated from just images of lines, contrary to perspective cameras, with or without the classical radial or tangential distortion models. In the latter case, images of straight lines only allow to calibrate the camera up to an arbitrary perspective projection, whereas for catadioptric cameras, a full calibration is possible. One explanation for this is that catadioptric cameras cannot be modeled by a perspective projection followed by a distortion in the image plane. Rather, the distortion already happens in the first step of the projection (in the unified model of Geyer and Daniilidis, when mapping 3D points onto a sphere). Every camera model for which this can be said may be fully calibrated from images of lines even if their relative position is unknown.</p><p>Fish-eye models. Devernay and Faugeras presented a plumb-line method in <ref type="bibr" target="#b112">[113]</ref> and applied it to different distortion models: the classical backward model used by Brown <ref type="bibr" target="#b63">[64]</ref>, but also the models proposed by Basu and Licardie and the field-of-view model (see Section 3.1.7).</p><p>Polynomial and rational polynomial models. Claus and Fitzgibbon proposed two plumb-line methods for the bi-quadratic rational function camera model (see Section 3.1.8) <ref type="bibr" target="#b96">[97]</ref>. They are both based on the fact that with that model, lines are imaged as conics. The first method starts with estimating conics from extracted line images and recovers distortion parameters via the factorization of a measurement matrix consisting of the matrices of all image conics. The second method is of the bundle adjustment type; it estimates the distortion model's coefficients using as cost function the Sampson distance between image points and line images (conics). Barreto as well as Strand and Hayman proposed similar plumb-line methods for the 1-parameter division model, for which line images are circles <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b465">466]</ref>. Discrete camera models. <ref type="bibr">Tardif et al.</ref> showed how to calibrate a general radially symmetric radial distortion model with a plumb-line approach <ref type="bibr" target="#b488">[489]</ref>. Their method is applicable to both, a parametric radial distortion curve and a discrete sampling of it.</p><p>General remarks. Whenever the back-projection operation can be written using polynomials, it is easy to set up "closed-form" solutions for plumb-line calibration. Consider the case of back-projection matrices for central cameras, of size 3 × n, for any n. As we have seen in Section 4.3, the image of a line L can be written as:</p><formula xml:id="formula_84">c T ∼ l T B i ,</formula><p>where l is the line at infinity of the plane spanned by the optical center and the line L. If we have m line images, we can set up a system like:</p><formula xml:id="formula_85">   c T 1 . . . c T m    ∼    l T 1 . . . l T m    B i .</formula><p>It is easy to see that the back-projection matrix can be recovered by factorizating the matrix on the left, like in factorization-based methods for structure-from-motion <ref type="bibr" target="#b507">[508]</ref>. It can only be determined up to the pre-multiplication with an arbitrary 3 × 3 matrix, which corresponds to a projective transformation of the directions of backprojected points -the perspective part of the imaging process. In other words, the non-perspective distortions are fully recovered. Once this is done, one may apply any calibration or self-calibration method designed for perspective cameras, to make the calibration complete. This approach can work with a minimum of three line images, although this is not recommended in practice; in that case, the matrix on the left of the above equation can directly be adopted as solution for B i . If prior knowledge on the camera is available, i.e., if constraints on B i are available, fewer line images may be sufficient in theory.</p><p>One may apply the same approach to non-central cameras, with more general back-projection matrices, e.g., of size 6 × n. In that case however, it must be taken into account that often, they will be of rank lower than 6, a fact that has to be taken into consideration when estimating them.</p><p>Finally, as always, the best approach for calibration is to optimize a meaningful geometrical/statistical error criterion, possibly after initializing with any of the above approaches, which usually comes down to a non-linear bundle adjustment like optimization process. For example, one may simultaneously estimate the coefficients of undistorted lines and of camera model parameters where the cost function is the sum of Euclidean distances between undistorted lines and points. It will in general be better even to minimize distances in the original, distorted image, i.e., distances between original image points and line images, where the latter are parameterized by camera model parameters and parameters for line images (two for central cameras, four for non-central ones).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Spheres</head><p>Algorithms using images of spheres for the calibration of perspective cameras were proposed for example by Penna <ref type="bibr" target="#b397">[398]</ref>, Daucher et al. <ref type="bibr" target="#b103">[104]</ref>, Teramoto and Xu <ref type="bibr" target="#b497">[498]</ref>, and Agrawal and Davis <ref type="bibr" target="#b5">[6]</ref>, to name a few.</p><p>Stevenson and Fleck used images of spheres to calibrate a local camera model, see Section 3.2.3 <ref type="bibr" target="#b462">[463]</ref>.</p><p>Ying and Hu showed how to use sphere images for calibrating central catadioptric cameras <ref type="bibr" target="#b550">[551]</ref>. In these cameras, spheres are mapped to conics, like in perspective images (to be precise, the mathematical image of a sphere is the union of two conics, but in true cameras only one of them is usually visible). Ying and Hu showed that from three or more extracted conics, the catadioptric camera can be calibrated. Ying and Zha analyzed the relationships between sphere images and the image of the absolute conic and proposed a linear calibration algorithm based on these <ref type="bibr" target="#b553">[554,</ref><ref type="bibr" target="#b554">555,</ref><ref type="bibr" target="#b555">556]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Circles</head><p>Herbert proposed a calibration procedure where instead of point matches or images of lines, projections of circles painted on the inside of a hemisphere are used <ref type="bibr" target="#b218">[219]</ref>. Coefficients of his distortion models (see Section 3.1.7) are then estimated by measuring and comparing the imaged areas of such circles.</p><p>Many algorithms using circles for calibrating pinhole cameras have been proposed in the literature, see <ref type="bibr" target="#b90">[91,</ref><ref type="bibr" target="#b198">199,</ref><ref type="bibr" target="#b256">257,</ref><ref type="bibr" target="#b475">476]</ref> to name just a few.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Self-calibration</head><p>Self-calibration has attracted great interest in the computer vision community in the last close to 20 years. The possibility of camera selfcalibration was already well known in photogrammetry, although it was usually identified with the non-linear simultaneous optimization of intrinsic and extrinsic camera parameters and scene structure (selfcalibrating bundle adjustment), whereas computer vision researchers aimed at developing initialization approaches.</p><p>Most computer vision self-calibration works concern pinhole cameras; a good general reference is the book by Hartley and Zisserman <ref type="bibr" target="#b212">[213]</ref>. In the following, we describe approaches for non-perspective cameras.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Self-calibration from Image Matches</head><p>General observations. As explained in Section 4.3, there is a strong link between plumb-line calibration (calibration from line images) and self-calibration. From image matches, epipolar geometry can be recovered; epipolar curves can be generated and since they are images of straight lines (camera rays) they can be used as input to any plumbline approach. Instead of this, mainly conceptual, idea of explicitly generating line images from a computed epipolar geometry, one may directly use fundamental matrices for self-calibration.</p><p>In the following, we only consider the self-calibration of the nonperspective part of a central camera model. By this we mean that it suffices to recover its 3 × n back-projection matrix up to the premultiplication by any (invertible) 3 × 3 matrix M. The effect of M is a projective transformation of the directions of back-projected camera rays, i.e., a perspective part of the camera model.</p><p>In the following, we consider the case of a fundamental matrix between two images taken by the same central camera. Let B be its 3 × n back-projection matrix (we omit the superscript i here for ease of notation). The fundamental matrix is then given by (cf. Equation (4.3)):</p><formula xml:id="formula_86">F ∼ B T [t] × R B.</formula><p>Consider rows of F; they are linear combinations of the rows of B, with coefficients of these combinations given by the matrix (B T [t] × R). Similarly, the columns of F are linear combinations of the columns of B T , i.e., of the rows of B. This suggests the idea of determining the rows of B by computing a basis of the space of row and column vectors of F. To do so, let us now look at the singular value decomposition (SVD) of F:</p><formula xml:id="formula_87">F ∼ U Σ V T ,</formula><p>where all matrices are of size n × n. Since F is of rank 2 in general, the SVD can be reduced by only considering the first two columns of U, the first two rows of V T and the two non-zero singular values in Σ:</p><formula xml:id="formula_88">F ∼ Ūn×2 Σ2×2 VT 2×n .</formula><p>The columns of Ū as well as the rows of VT are linearly dependent on the rows of B. Let us stack them together in a 4 × n matrix and compute its SVD:</p><formula xml:id="formula_89">ŪT VT 4×n = W 4×n Γ n×n X T n×n .</formula><p>In the absence of noise, the above matrix is of rank 3 in general, since all its rows are linearly dependent on the three rows of B. The three rows of X T corresponding to the non-zero singular values in Γ can now be directly adopted as the rows of the back-projection matrix B. It is clear that the rows of B are only defined up to linear combination of the rows of X T , but this does not matter: as stated above, B needs only be recovered up to a projective transformation M, and this means nothing else than a linear combination of its rows.</p><p>A more compact alternative procedure would proceed as follows. Stack F on top of F T and compute the SVD of this 2n × n matrix:</p><formula xml:id="formula_90">F F T = U Σ V T .</formula><p>Using similar arguments as above, the matrix on the left is of rank 3 in general and the three rows of V T corresponding to non-zero singular values in Σ can directly be chosen as rows of B, which concludes this self-calibration algorithm.</p><p>A similar procedure was proposed by Claus and Fitzgibbon for the case of 3 × 6 back-projection matrices <ref type="bibr" target="#b97">[98]</ref>. The self-calibration approach for para-catadioptric cameras by Geyer and Daniilidis (cf. Section 5.3.1) follows the same principle <ref type="bibr" target="#b165">[166,</ref><ref type="bibr" target="#b169">170]</ref>. In their case, the back-projection matrix could be fully determined (up to a rotation instead of a projective transformation), due to using constraints on the coefficients of B. More generally speaking, there are two ways of obtaining more than the self-calibration of "just" the non-perspective part of B. (i) Use constraints on the entries of B. (ii) In any case, once B is estimated up to a projective transformation, we are in the situation of an uncalibrated perspective camera. From the fundamental matrix between two perspective images, one may estimate up to two intrinsic parameters <ref type="bibr" target="#b205">[206,</ref><ref type="bibr" target="#b473">474]</ref>; usually one just computes the focal length. Hence, in many cases it should be possible to obtain a full self-calibration of non-perspective cameras from a single fundamental matrix.</p><p>From the above, it is straightforward to extend self-calibration to the use of multiple fundamental matrices for a better stability and/or to the self-calibration of two or more cameras with different parameters/models. Without giving a formal proof, it should be possible for example, to self-calibrate three entirely different cameras (possibly with different sizes of back-projection matrices), from the three pairwise fundamental matrices. This possibility was demonstrated by Geyer and Daniilidis for para-catadioptric cameras <ref type="bibr" target="#b165">[166,</ref><ref type="bibr" target="#b169">170]</ref>.</p><p>In the following, we review a few self-calibration approaches proposed in the literature for different types of non-perspective camera models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classical distortion model.</head><p>Zhang proposed an approach for the simultaneous estimation of one radial distortion coefficient and the perspective fundamental matrix, by non-linear optimization of the epipolar constraint <ref type="bibr" target="#b565">[566]</ref>. Based on his experimental results, Zhang concluded that the distortion coefficients can only be estimated reliably if matching points can be extracted accurately and if the actual distortion is significant (cf. the discussion in Section 3.6). In other cases, the estimation is ill-conditioned or in other words, taking into account distortion coefficients besides the fundamental matrix constitutes an overparameterization. One may further add that it seems to be required that image matches be well distributed across the image planes, as is always required when estimating radial distortion, which is most observable toward the borders of images.</p><p>In <ref type="bibr" target="#b460">[461]</ref>, Stein proposed a virtually identical method, as well as an extension to three views. Here, trifocal transfer errors are used to define a cost function which is optimized simultaneously over the distortion and trifocal tensor coefficients.</p><p>Non-central panoramas. Huang et al. proposed a self-calibration method for non-central panoramic images acquired by rotating 1D cameras <ref type="bibr" target="#b238">[239]</ref>. In their approach, it is required that images be obtained by rotating about the same axis.</p><p>Catadioptric cameras. Gluckman and Nayar showed how to determine the focal length of the camera in a single-lens catadioptric stereo system using one camera and two planar mirrors <ref type="bibr" target="#b173">[174,</ref><ref type="bibr" target="#b174">175]</ref>. Since this setup is identical to a system of two perspective cameras, perspective self-calibration approaches can be readily applied. Gluckman and Nayar used point matches between the two views and the fundamental matrix estimated from them. Their reported results were not perfect. We believe that this is because of a generic degeneracy: whenever the two mirrors are arranged in a rotationally symmetric position with respect to the camera's optical axis, the two virtual cameras (the real camera reflected in the two mirrors) are in a degenerate relative pose for focal length computation <ref type="bibr" target="#b473">[474]</ref>. In situations close to this, results may be expected to be inaccurate.</p><p>The first self-calibration approach for non-perspective catadioptric systems is probably due to Kang, who considered the central paracatadioptric case <ref type="bibr" target="#b261">[262]</ref>. Based on an expression of the epipolar geometry as a function of the relative motion between two views and the relevant intrinsic parameters, he proposed a cost function consisting of the sum of distances between image points and epipolar circles. This cost function was minimized non-linearly.</p><p>Geyer and Daniilidis showed how to self-calibrate para-catadioptric cameras from point matches, via the estimation of the 4 × 4 fundamental matrix <ref type="bibr" target="#b165">[166,</ref><ref type="bibr" target="#b169">170]</ref>. Their key observation was that the nullspace of the fundamental matrix contains the vector representation of the image of the absolute conic (a circle in their case). They showed that the fundamental matrix is of rank 2; if both images are taken by the same camera, then the left and right null-spaces may be intersected to find the image of the absolute conic and thus to calibrate the camera. Geyer and Daniilidis also showed that if the images are taken with different para-catadioptric cameras, then their intrinsic parameters can be found from the fundamental matrices between three images. These observations can be generalized, cf. the above paragraph "General observations".</p><p>Sturm showed that a similar concept can be used for the selfcalibration of para-catadioptric cameras from homographies associated with scene planes <ref type="bibr" target="#b470">[471]</ref>.</p><p>In <ref type="bibr" target="#b343">[344,</ref><ref type="bibr" target="#b346">347,</ref><ref type="bibr" target="#b348">349]</ref>, Mičušik and Pajdla proposed an approach for the estimation of the epipolar geometry of non-central catadioptric cameras, demonstrated for the examples of a spherical mirror (always non-central) and misaligned para-and hyper-catadioptric systems. The approach proceeds in several steps. First, an appropriate central approximation to the full non-central back-projection is derived. This approximate model is then linearized such that its coefficient(s) can be estimated simultaneously with the fundamental matrix, by solving a quadratic eigenvalue problem similarly as in Fitzgibbon's approach for the division model (see below). This allows to initialize and then optimize the complete, non-central model.</p><p>Polynomial and rational polynomial models. Fitzgibbon showed how to simultaneously estimate radial distortion and perspective multi-view relations such as the fundamental matrix, plane homographies, or trifocal tensors, by solving quadratic or cubic eigenvalue problems <ref type="bibr" target="#b144">[145]</ref>. The approach is based on the division model (see Section 3.1.8). This was extended by Mičušik and Pajdla <ref type="bibr" target="#b343">[344,</ref><ref type="bibr" target="#b345">346,</ref><ref type="bibr" target="#b348">349]</ref> (see previous paragraph), <ref type="bibr">Barreto and Daniilidis [33,</ref><ref type="bibr" target="#b33">34]</ref>, Claus and Fitzgibbon <ref type="bibr" target="#b97">[98]</ref>, and Steele and Jaynes <ref type="bibr" target="#b457">[458]</ref>.</p><p>Claus and Fitzgibbon showed how to estimate the coefficients of the full bi-quadratic rational function model for central cameras (i.e., 3 × 6 back-projection matrices) from point matches between two views <ref type="bibr" target="#b97">[98]</ref>. They also analyzed the nature of epipolar curves (conics) and showed how to calibrate the model from one image of a planar calibration grid.</p><p>Mičušik and Pajdla used a division-type model as an extension to the equiangular fisheye model (see Section 3.1.7) and showed, analogously to Fitzgibbon's approach, how to simultaneously estimate its two parameters and the fundamental matrix <ref type="bibr" target="#b343">[344,</ref><ref type="bibr" target="#b345">346,</ref><ref type="bibr" target="#b348">349]</ref>. If both parameters of their model are used, 15 point matches are required; for a one-parameter version 9 matches are sufficient. They used an analog approach to self-calibrate a one-parameter model of para-catadioptric cameras, leading to a quartic instead of a quadratic eigenvalue problem <ref type="bibr" target="#b343">[344,</ref><ref type="bibr" target="#b347">348,</ref><ref type="bibr" target="#b348">349]</ref>.</p><p>Barreto and Daniilidis showed that even if two images with different distortion coefficients for the division model are considered, they are subject to a 4 × 4 fundamental matrix <ref type="bibr" target="#b33">[34]</ref>. They gave an algorithm for linearly estimating the distortion coefficients and the epipolar geometry, from a minimum of 15 point matches. The same factorization approach is used in <ref type="bibr" target="#b32">[33]</ref> to calibrate the nodes of a multi-camera network, including radial distortion, intrinsic and extrinsic parameters.</p><p>Li and Hartley proposed a different method for estimating distortion coefficients of the division model from point matches in two views <ref type="bibr" target="#b310">[311,</ref><ref type="bibr" target="#b311">312]</ref>. They decoupled the estimation of distortion coefficients from that of the fundamental matrix and showed how to estimate one or two coefficients of the division model from 9 point matches.</p><p>Kúkelová and Pajdla proposed a minimal method for the selfcalibration problem with one distortion coefficient, using 8 point matches <ref type="bibr" target="#b291">[292]</ref>. In <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b290">291,</ref><ref type="bibr" target="#b292">293]</ref>, Kúkelová, Byröd et al. gave minimal methods for the following two problems. (i) Estimating the fundamental matrix together with two different coefficients for the division model, from 9 point matches (a non-minimal linear solution for this problem was first given by Barreto and Daniilidis <ref type="bibr" target="#b33">[34]</ref>, see above). (ii) Estimating the epipolar geometry and one distortion coefficient (identical for both images) in the case of partially calibrated images (known principal point and square pixels); a minimal method for estimating the distortion coefficient and the essential matrix (as opposed to the fundamental matrix) was given.</p><p>Steele and Jaynes extended Fitzgibbon's approach in two ways <ref type="bibr" target="#b457">[458]</ref>. First, they solved a more general quadratic eigenvalue problem, allowing to obtain less ambiguous and more accurate results in the case of more than the minimum of 9 point matches. Second, their approach may be used to simultaneously estimate a global distortion coefficient and fundamental matrices for multiple image pairs, thus increasing the accuracy of each of these estimates.</p><p>Kannala et al. proposed a bundle adjustment type approach for the self-calibration of a unified central catadioptric model (Section 3.1.5) and the polynomial model of Equation (3.7), from matches between two images <ref type="bibr" target="#b266">[267]</ref>.</p><p>Discrete camera models. Thirthala and Pollefeys showed how to self-calibrate the 1D radial camera model (see Section 3.1.9) from four images, based on estimating the associated quadrifocal tensor <ref type="bibr" target="#b499">[500]</ref>. Their approach is stratified, starting with a projective reconstruction and converting it to a Euclidean one. At that stage, the radial distortions of the camera are not yet calibrated, just the mapping from radial planes to radial lines. To get a full calibration, Thirthala and Pollefeys used the assumption of radial symmetry. They either fit a division model to the reconstruction <ref type="bibr" target="#b500">[501]</ref> or a discrete camera model.</p><p>In addition to their approaches for grid-based calibration of general radial distortion models (see Section 5.1.2), Hartley and Kang also showed that self-calibration is possible <ref type="bibr" target="#b208">[209]</ref>. They observed however that results are too unstable in the presence of even small amounts of noise.</p><p>Tardif et al. proposed an approach for the self-calibration of a general radially symmetric radial distortion model (cf. Section 3.3.2) from images of a planar scene with unknown structure <ref type="bibr" target="#b488">[489]</ref>. A more robust extension which also works for images with different radial distortions was presented in <ref type="bibr" target="#b489">[490]</ref>. A factorization-based method for the same problem was given by Ramalingam et al. <ref type="bibr" target="#b416">[417]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Special or Known Motions</head><p>Like for perspective cameras, using special types of camera motion or actually knowing the camera motion usually allows to simplify the self-calibration problem. This enables less complex algebraic problem formulations, allows to work with fewer images, and often gives more accurate results. As for perspective cameras <ref type="bibr" target="#b206">[207]</ref>, the most used motion is pure rotation about the optical center; an approximate rotation is sufficient if the scene is sufficiently far away. In the following, we describe several approaches, first ones based on special types of motion (but with unknown motion parameters), then ones using knowledge of the motion parameters.</p><p>Classical distortion model. Stein proposed a method based on pure rotational camera motion <ref type="bibr" target="#b459">[460]</ref>. Matching (undistorted) image points are related by a special homography. Distortion coefficients and that homography are estimated simultaneously, by non-linear optimization, using a reprojection error type cost function.</p><p>Sawhney and Kumar showed the importance of estimating non-perspective distortion for generating mosaics based on images acquired with a rotating camera <ref type="bibr" target="#b434">[435]</ref>. They proposed a direct method (i.e., based on image intensities rather than feature matches) for simultaneously estimating distortions and image alignment transformations. In their work, they only considered a single term of the classical radial distortion model, but it is obvious that other distortion models may be estimated from images taken under pure rotation.</p><p>Fish-eye models. Xiong and Turkowski performed the selfcalibration of a fisheye camera from pure rotations <ref type="bibr" target="#b538">[539]</ref>. The camera model they used is the extension of the equiangular model described in Section 3.1.7. The calibration is performed by non-linear minimization, where the distortion center/principal point is initialized as the center of the image's bounding ellipse.</p><p>Discrete camera models. Thirthala and Pollefeys showed how to self-calibrate the 1D radial camera model (see Section 3.1.9) from three images taken under pure rotational motion <ref type="bibr" target="#b499">[500,</ref><ref type="bibr" target="#b500">501]</ref>. To be precise, in the non-central version of the 1D radial model, pure rotation would rather correspond to camera poses such that all optical axes intersect in a single point. A full self-calibration algorithm was given for central cameras.</p><p>Tardif et al. showed how to self-calibrate a general radially symmetric radial distortion model using either pure rotational or pure translational camera motion <ref type="bibr" target="#b488">[489]</ref>. They used both a parametric radial distortion curve and a discrete sampling thereof.</p><p>The probably most general self-calibration problem considered to date, besides the work of Grossmann et al. (see Section 5.3.3), concerns the ray-based self-calibration of a general central camera. In the following works, the only assumption made is that of continuityneighboring image points have neighboring camera rays.</p><p>Ramalingam et al. showed that from pure translational motions, the self-calibration of camera rays is possible up to a projective transformation of their directions <ref type="bibr" target="#b418">[419]</ref>. They stated that a complete self-calibration is possible from two rotational motions and gave an algorithm for full self-calibration from one translational and two rotational motions. In <ref type="bibr" target="#b420">[421]</ref>, they proposed an algorithm that only requires two rotational motions. <ref type="bibr">Nistér et al. and Grossmann et al. [193]</ref> gave several similar results, mainly for the case of infinitesimal rotations or translations <ref type="bibr" target="#b379">[380,</ref><ref type="bibr" target="#b192">193]</ref>. Their main result is an algorithm for self-calibration from rotational optical flow. They issued the conjecture that two infinitesimal rotations about different axes are sufficient for a complete self-calibration. This was theoretically proven by Espuny and Burgos Gil who also gave a practical algorithm <ref type="bibr" target="#b129">[130,</ref><ref type="bibr" target="#b130">131]</ref>.</p><p>Known motions. Stevenson and Fleck developed an approach using a robot arm and a light source to calibrate a camera <ref type="bibr" target="#b461">[462]</ref>. The camera is mounted on the robot arm which then rotates the camera by known angles (the rotation should be about the camera's optical center, although this is not further discussed in <ref type="bibr" target="#b461">[462]</ref>). The image of the fixed light source is extracted in each frame. The approach calibrates a radially symmetric central model, as well as optical parameters such as light fall-off across the image plane. The gist of the proposed approach is as follows. An initial estimate of the distortion center is assumed; the camera is rotated such that the light source is seen at the distortion center. Then, the camera is successively rotated in one direction by some small angle increment. The observed image positions of the light source, together with the known rotation angles, directly allow to construct a discrete sampling of the distortion function. Stevenson and Fleck also described how to correct the initial estimate of the distortion center.</p><p>Similar active approaches were also proposed by Du and Brady <ref type="bibr" target="#b121">[122]</ref> and Kojima et al. <ref type="bibr" target="#b281">[282]</ref> and seem to be in use for the calibration of cameras for space and planetary exploration, see e.g., <ref type="bibr" target="#b333">[334]</ref>.</p><p>Swaminathan et al. proposed an approach for calibrating noncentral catadioptric cameras, using image matches and knowledge of the camera motion <ref type="bibr" target="#b483">[484]</ref>. The approach is based on a parametric model for the caustic of catadioptric cameras; its coefficients are non-linearly optimized in a bundle adjustment like manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Image Statistics</head><p>Farid and Popescu showed in <ref type="bibr" target="#b134">[135]</ref> that distortion coefficients, as well as coefficients of non-linear intensity transformations, may be estimated from single images without the use of calibration objects. This is based on a frequency analysis of the image and on the assumption that nonlinear transformations (geometric or intensity distortions) introduce specific higher-order correlations in the frequency domain. In some sense, this approach is an example of using natural image statistics, here for camera calibration. A similar approach was developed by Yu <ref type="bibr" target="#b560">[561]</ref>.</p><p>Grossmann et al. showed that a highly discrete camera model, consisting of "isolated" camera rays (see Section 3.3.3), can be selfcalibrated just by taking images of unknown scenes, based on simple assumptions about their appearance <ref type="bibr" target="#b191">[192,</ref><ref type="bibr" target="#b193">194]</ref>. Roughly speaking, it is assumed that the smaller the angle between two camera rays, the more likely it is that the associated pixels produce similar greylevels; this is in some sense, an assumption on the statistics of natural images. When acquiring a sufficiently large set of images, observed differences between greylevels of different pixels, allow to infer the topology of the camera rays. Based on a simple off-line training, one may even recover the angles between rays and finally, their directions in a common coordinate system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Special Approaches Dedicated to Catadioptric Systems</head><p>In this section we only cover methods that were not explained in any of the above sections. When using catadioptric cameras, one rarely does not know anything at all about the shape of the mirror, unless it is taken "off-the-shelf" or if the producer of a catadioptric camera does not disclose any information about the mirror's shape. In case the mirror shape is known, calibration can be considered as estimating the relative pose between camera and mirror, and possibly the camera's intrinsic parameters. Otherwise, several possibilities exist, as explained below.</p><p>Known mirror shape. One of the two (self-)calibration methods proposed in <ref type="bibr" target="#b261">[262]</ref> by Kang is based on the extraction of the mirror's upper circular rim in the image. From this, some or all intrinsic parameters can be computed. This idea was used in several other works, for example <ref type="bibr" target="#b109">[110]</ref>.</p><p>Strelow et al. proposed a calibration method using 3D calibration grids <ref type="bibr" target="#b466">[467]</ref>. They assumed that the perspective camera looking at the mirror is already calibrated. The calibration of the catadioptric sensor then comes down to estimating relative mirror-to-camera pose. Strelow et al. estimated this together with the pose of the calibration grid, by non-linearly optimizing the reprojection error of calibration points. Lhuillier showed how to optimize the mirror-to-camera pose and the 3D position of points observed and matches in several images, via bundle adjustment <ref type="bibr" target="#b307">[308]</ref>.</p><p>Aliaga used a parabolic mirror, looked at by a perspective camera <ref type="bibr" target="#b11">[12]</ref>. Hence, the catadioptric system is non-central. A calibration procedure was proposed that estimates the focal length of the perspective camera and its position relative to the mirror (the orientation seems to be assumed to be perfect, i.e., the optical axis to be parallel to the mirror axis), as well as a radial distortion coefficient.</p><p>Similar but more restrictive approaches exist that require knowledge of the mirror shape and the camera's intrinsic parameters, i.e., calibration comes down to estimating the relative mirror-to-camera pose. Mashita et al. <ref type="bibr" target="#b331">[332]</ref> supposed that the perspective camera is calibrated and that the mirror's circular upper boundary can be extracted in the image. Then, classical algorithms for estimating the pose of a circle in 3D can be applied, e.g., <ref type="bibr" target="#b113">[114]</ref>, after which the pose of the entire mirror, relative to the camera, is known. This effectively gives the calibration of the entire catadioptric system. This is similar to the approach by Kang described above, which was developed for central para-catadioptric cameras, whereas the one by Mashita et al. is for perspective cameras and central as well as non-central setups.</p><p>Another such method was proposed by Fabrizio et al. where besides the upper boundary of the mirror, a second circular section of it must be observable in the image, e.g., the footprint of a black needle glued on the mirror of some prototypes of catadioptric systems <ref type="bibr" target="#b132">[133]</ref>. From the images of these two circles, the mirror pose is computed along with some intrinsic camera parameters.</p><p>Unknown mirror shape. Several possibilities exist to perform calibration in this case, some of which are explained in the following.</p><p>If it is known that the catadioptric camera is one of the central types (cf. Section 2.3.1), then one may for example use any of the unified models (cf. Section 3.1.5) to calibrate it.</p><p>A more general approach was proposed by Caglioti et al. for the case where all that is assumed is that the mirror is a surface of revolution <ref type="bibr" target="#b77">[78]</ref>. The perspective camera looking at the mirror can be positioned arbitrarily and the mirror shape can be arbitrary (besides being a surface of revolution), hence the system is non-central in general. The proposed calibration approach requires to extract the silhouette of the mirror in the image as well as the image of at least one straight line in the scene. This input allows to calibrate intrinsic camera parameters (focal length and principal point), the shape of the mirror (its profile) and the relative pose of mirror and camera.</p><p>It is of course possible to cut the calibration process in two steps, where in a first step one reconstructs the mirror shape using any of the many methods for specular surface reconstruction available in the literature, most of which are based on analyzing images of patterns reflected in the mirror (a representative list of references is out of scope for this manuscript). An alternative to these classical approaches is to use polarization imaging, as proposed for example by Morel et al. <ref type="bibr" target="#b351">[352]</ref>.</p><p>Naturally, another possibility is to calibrate the catadioptric system using any of the global, local, or discrete models described in the previous section. If the mirror is of arbitrary shape, only local or discrete models might work.</p><p>The goal of this section is to briefly describe the main building blocks of structure-from-motion in a simple way, that is independent of the camera model used. This is to highlight the underlying (simple) principles, allowing to establish links between seemingly different approaches that abound in the literature and to easily adapt these principles to new camera models.</p><p>The first three sections are dedicated to the main building blocks for calibrated cameras: pose and motion estimation, and triangulation of points. These will be explained for central and non-central cameras. The fourth section aims at highlighting the central role of bundle adjustment in all of structure-from-motion and calibration. The fifth section lists a few works on 3D scene modeling from omnidirectional cameras, from single or multiple views. In the last section, the related issues of distortion correction and stereo rectification are discussed; they are not really structure-from-motion topics, but necessary for 3D scene modeling.</p><p>Overall, we believe that the most important ingredients of structurefrom-motion systems are: algorithms working with the minimum required amount of data, to be embedded in robust estimation schemes such as RANSAC, and a (robust) bundle adjustment, allowing to get optimal results for all of the individual problems. Other important contributions, not discussed further in this section, are methods determining globally optimal solutions of structure-from-motion problems, see e.g., <ref type="bibr" target="#b259">[260]</ref> and references therein. Besides the minimal methods, other algorithms that give suboptimal results may be useful, e.g., to get a better initial estimate from the set of inliers of a model determined using a minimal method. Finally, this section only covers some main structure-from-motion problems; many special cases exist, using lines or other primitives than the points considered in the following, concerning dynamic scenes or objects, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Pose Estimation</head><p>Computing the pose relative to a calibrated perspective camera, of an object with known structure, is one of the most classical problems in photogrammetry (where it is called "resection") and computer vision, see, e.g., a review paper by Haralick et al. <ref type="bibr" target="#b204">[205]</ref>. Pose, i.e., object position and orientation, is usually computed using point or line correspondences between the object and the image, although other possibilities exist, for example computing the pose of a spherical object from its silhouette in the image <ref type="bibr" target="#b113">[114]</ref>. The minimum number of required point correspondences is three, in which case there may be up to four possible solutions <ref type="bibr" target="#b229">[230]</ref>. Any of the numerous algorithms that were proposed for the perspective camera model can be easily adapted to any other central camera model, since instead of working with the original 2D image coordinates, each algorithm uses, explicitly or implicitly, the directions of the back-projected camera rays. Hence, these algorithms can be used for any central camera as soon as it is calibrated. There exist different formulations for the pose estimation problem. Here, we use one that is directly suited for central as well as non-central cameras. Minimal solutions for the non-central pose estimation problem were proposed independently by Chen and Chang <ref type="bibr" target="#b87">[88]</ref>, Nistér <ref type="bibr" target="#b377">[378]</ref>, Nistér and Stewénius <ref type="bibr" target="#b378">[379]</ref>, and Ramalingam and Sturm <ref type="bibr" target="#b413">[414,</ref><ref type="bibr" target="#b414">415]</ref>. See also papers by Fabrizio and Devars <ref type="bibr" target="#b131">[132]</ref> and Schweighofer and Pinz <ref type="bibr" target="#b441">[442]</ref> about the n-point pose problem for central and non-central cameras.</p><p>General problem formulation. The problem can be formulated as follows. Instead of explicitly estimating the rotation and translation of the object model, we estimate the depth of object points relative to the camera, i.e., their position along the known camera rays, associated with the matched image points. Given the position of 3 points, the rotation and translation for the entire object can then be computed easily (via absolute pose estimation, see for example <ref type="bibr" target="#b232">[233]</ref>). We use the finite and infinite parts of back-projection, cf. the introduction of Section 3: a camera ray is defined by a finite point B f (for central cameras, the optical center) and the direction B i . A point on the ray is then parameterized by a scalar λ:</p><formula xml:id="formula_91">Q ∼ B f + λ B i 0 .</formula><p>With three object points Q j , parameterized by λ j , j = 1• • • 3, we can set up three constraint equations: the Euclidean distances between these points must be identical to the prior known values. The squared distance between two points Q j and Q k is quadratic in λ j and λ k . From the three pairs of points, we thus get a total of three quadratic equations in three unknowns. One may use any appropriate numerical method for solving such a system; it is simple for example to convert the equation system into a degree-8 polynomial in one of the unknowns and then to use any numerical root finding method to get the up to eight real solutions. This should naturally be embedded in a robust estimation scheme, such as RANSAC, where each of the potential solutions is scored against the other point matches.</p><p>Central cameras. The usual special case of central cameras allows for a slight simplification. As finite point B f , the origin may be adopted for all camera rays; our points can then be parameterized as:</p><formula xml:id="formula_92">Q j ∼ λ j B i j 1 .</formula><p>If we swap the sign for all three λ j , the distances between the Q j are not affected. Hence, for each solution of our three constraint equations, there exists a mirror solution with all signs swapped. This allows to reduce the complexity of the estimation problem: instead of solving the equivalent of a degree-8 polynomial, a degree-4 polynomial is sufficient.</p><p>Other pose problems. Josephson and Byröd gave a minimal method for estimating camera pose, together with the focal length and one radial distortion coefficient (division model), from four matches <ref type="bibr" target="#b258">[259]</ref>.</p><p>A special case is that of planar pose, where the unknown object pose consists of a translation in one plane (usually, a horizontal ground plane) and a rotation about the normal of that plane. Minimal solutions requiring 2 point matches only exist <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b392">393]</ref>. Gourichon et al. proposed a bio-inspired method for planar pose estimation <ref type="bibr" target="#b183">[184]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Motion Estimation</head><p>Generalities. Motion or egomotion estimation, or relative orientation, is another classical problem. The goal is to estimate the motion of a camera or the relative pose of calibrated cameras, from matches between images but without using information on the scene structure.</p><p>The first truly minimal solution for central cameras was developed by Nistér <ref type="bibr" target="#b376">[377]</ref>: an approach for estimating all possible essential matrices <ref type="bibr" target="#b323">[324]</ref> from 5 point matches (up to 10 theoretical solutions).</p><p>As for non-central cameras, the concept of essential matrix was introduced by Pless <ref type="bibr" target="#b401">[402]</ref>, cf. Section 4.1. It can be estimated by solving a linear equation system, established from 17 or more point matches; extracting the motion parameters from the essential matrix is then relatively straightforward. Note that if the cameras are not "fully noncentral", e.g., are of the axial type, then such a linear estimation process may be underconstrained, as pointed out by Mouragnon et al. <ref type="bibr" target="#b356">[357]</ref>. Other degeneracies were studied by Kim and Kanade <ref type="bibr" target="#b278">[279]</ref>. Kim et al. proposed two motion estimation algorithms for the case where the non-central system consists of two or more central cameras with nonoverlapping fields of view, i.e., where points are only tracked by individual cameras but where no handover between cameras occurs <ref type="bibr" target="#b276">[277]</ref>. Finally, a minimal solution for the general non-central case, from 6 point matches, was given by Stewénius et al. <ref type="bibr" target="#b463">[464]</ref>.</p><p>Contrary to motion estimation from central cameras, the scale of the translation can also be estimated with non-central cameras. However, this is only accurate if the trade-off of several factors is favorable (cf. the general discussion in Section 3.6): the cameras should be sufficiently non-central, compared to the amount of noise, the distance of the scene from the cameras, etc. The other way round: if a camera is not sufficiently non-central, then it is advisable to model it as a central system and use traditional motion estimation, cf. also a study by Kim et al. <ref type="bibr" target="#b277">[278]</ref>.</p><p>In the following, we briefly cite a few works on motion estimation for omnidirectional cameras.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Works on motion estimation for omnidirectional cameras.</head><p>Yagi et al. considered the motion estimation with a central hypercatadioptric camera <ref type="bibr" target="#b542">[543]</ref>. The robot-mounted camera is supposed to move on a horizontal ground plane; the topic of <ref type="bibr" target="#b542">[543]</ref> is to estimate small perturbances to perfect planar motion (so-called rolling), from optical flow.</p><p>McMillan and Bishop estimated the motion between two calibrated cylindrical images <ref type="bibr" target="#b337">[338]</ref>. This is done by non-linear minimization of the distances between back-projection rays associated with matching image points. Kang and Szeliski <ref type="bibr" target="#b263">[264]</ref> used the classical 8-point method for linearly estimating the essential matrix in the same case. Other approaches for motion estimation that are based on computing the essential matrix, explicitly or implicitly from directions of backprojected image points, are <ref type="bibr" target="#b85">[86,</ref><ref type="bibr" target="#b478">479]</ref>. Svoboda discussed the issue of properly normalizing coordinates of image matches to ensure a good conditioning <ref type="bibr" target="#b390">[391,</ref><ref type="bibr" target="#b478">479]</ref>, along the lines of Hartley's approach <ref type="bibr" target="#b207">[208]</ref>. Bazin et al. proposed a motion estimation method for catadioptric cameras that decouples the estimation of rotation and translation parameters <ref type="bibr" target="#b44">[45]</ref>. The method is designed for man-made environments and requires the detection of sets of parallel lines.</p><p>Gluckmann and Nayar estimated motion from the optical flow <ref type="bibr" target="#b172">[173]</ref>. In order to use a wide field of view and to use the same method for motion estimation for different imaging geometries, they mapped the flow field to a sphere centered in the optical center, using camera calibration information. Then, extensions of classical algorithms for motion estimation from optical flow, from planar to spherical viewing surfaces, can be readily applied, independently of the actual camera's imaging geometry. We also refer to the seminal work on determining camera motion from the flow field on the viewing sphere, by Nelson and Aloimonos <ref type="bibr" target="#b372">[373]</ref>, where they also clearly explained why wide fields of view are superior to narrow ones for motion estimation. See also the papers <ref type="bibr" target="#b140">[141]</ref> by Fermüller and Aloimonos who studied the same issue while drawing analogies with the evolution of biological eyes, <ref type="bibr" target="#b303">[304]</ref> by <ref type="bibr">Lee et al.,</ref><ref type="bibr" target="#b374">and [375]</ref> by Neumann et al. who proposed a mathematical formulation for designing non-central cameras that are optimal for motion estimation. Shakernia et al. proposed a similar approach to that of Gluckman and Nayar, but instead of back-projecting optical flow to a sphere, they used another curved surface which is implicitly defined by the lifting operation carried out when back-projecting image points <ref type="bibr" target="#b445">[446]</ref>.</p><p>In <ref type="bibr" target="#b467">[468]</ref>, Strelow and Singh described how to combine image point matches in omnidirectional cameras with inertial sensor readings, for structure and motion estimation. A first approach assumes a calibrated camera. A second approach works for uncalibrated cameras -only the assumption of a radial projection function is required (cf. Section 3.1.9). This is highly related to work by Thirthala and Pollefeys, see Sections 3.1.9 and 5.3.1, and to the RAC (radial alignment constraint) by Tsai <ref type="bibr" target="#b514">[515]</ref>.</p><p>Shakernia et al. formulated motion estimation under the assumption of small displacements between images <ref type="bibr" target="#b446">[447]</ref>.</p><p>Lim et al. introduced a so-called antipodal-epipolar constraint that allows to decouple the estimation of rotation and translation for cameras with larger than hemispheric fields of view <ref type="bibr" target="#b317">[318]</ref>. This result is based on the notion of antipodal image points: points whose camera rays are aligned but which correspond to opposite viewing directions. If two antipodal points in one camera can both be matched in the other camera, then a specific epipolar constraint becomes available. Lim et al. showed how to independently estimate translation and/or rotation from two or more pairs of such antipodal point matches. In <ref type="bibr" target="#b316">[317]</ref>, they showed a similar decoupling approach whose inputs are optical flows measured at antipodal point pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Triangulation</head><p>Points. Reconstructing a 3D point from matches in two or more calibrated images (called intersection in photogrammetry) is of course an essential module in structure-from-motion. Like for the other problems, an optimal solution requires in general a bundle adjustment or a global optimization approach. For the special cases of two or three perspective images, "direct" optimal solutions were proposed by Hartley and Sturm <ref type="bibr" target="#b210">[211]</ref> and Stewénius et al. respectively <ref type="bibr" target="#b464">[465]</ref>.</p><p>Suboptimal but generic solutions are to estimate the 3D point that minimizes some distance measure relative to the camera rays associated with the input image matches, for example the Euclidean distance <ref type="bibr" target="#b477">[478]</ref> or an angle-based measure, see Section 6.4.</p><p>Lines. The triangulation of lines or line segments is useful, especially in 3D modeling of man-made objects. Whereas for points, it does not matter if they are reconstructed from central or non-central cameras (a single ray is used per camera), there is a difference in the case of lines. As for central cameras, whatever type they are, a line image can be back-projected to an interpretation plane in 3D -the plane spanned by the optical center and the original 3D line. From two images, the line can thus be triangulated by computing the intersection of the two interpretation planes.</p><p>As for non-central cameras, they allow in general the triangulation of 3D lines from single images. The reason is that the back-projection is no longer a plane but some other surface and in general, the original 3D line is the only straight line on that surface. Another way of formulating this is by considering image points lying on a line image, and the associated camera rays. It is well known that for four lines in general position, there exist two lines incident to all of them. If more than four camera rays associated with a line image are considered, the original 3D line can be found as the only one incident to all of them, using the approach of Teller and Hohmeyer <ref type="bibr" target="#b493">[494]</ref>. Of course, if the camera is of the axial type, all camera rays are incident with the camera axis; this can be taken care off by computing all candidate 3D lines (two in general) and by discarding the camera axis. There may be ambiguous solutions for the 3D line, depending on the camera used and the position of the original line.</p><p>Algorithms and in-depth theoretical studies for line triangulation from non-central images are given by Caglioti and Gasparini <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b76">77]</ref> as well as Lanman et al. <ref type="bibr" target="#b298">[299]</ref>. Morita et al. described an early method for reconstructing 3D lines from two fisheye images <ref type="bibr" target="#b352">[353]</ref>. Although not described as such, the method seems equivalent to intersecting interpretation planes. Like other researchers, for example Vasseur and Mouaddib <ref type="bibr" target="#b518">[519]</ref>, Morita et al. used the fact that 3D lines in spherical images, appear as great circles. Yamazawa et al. described a Hough-transformbased approach for detecting line segments in omnidirectional images and demonstrated the 3D reconstruction of lines from image triplets <ref type="bibr" target="#b544">[545]</ref>. Another approach for 3D line (segment) triangulation for omnidirectional images is for example <ref type="bibr" target="#b241">[242]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Bundle Adjustment</head><p>As mentioned earlier, bundle adjustment is imperative for obtaining an accurate solution of any structure-from-motion, calibration, or other similar problem, an exception being problems for which global optimizers exist. Bundle adjustment means in principle nothing else than a simultaneous non-linear optimization of all unknowns; in addition, it implies that a meaningful cost function is used, see below. Other important issues are the exploitation of the problem structure at hand to obtain efficient algorithms; this includes the well-known sparsity of the normal equations solved at each iteration of the usually employed nonlinear least-squares methods but also the question of how to order data in the case of image sequences with multiple overlaps. More on bundle adjustment can be found in the overview paper by Triggs et al. <ref type="bibr" target="#b511">[512]</ref>.</p><p>Bundle adjustment is most often a non-real-time process. Recently, approaches that may be qualified as hybrids between bundle adjustment and simultaneous localization and mapping (SLAM) <ref type="bibr" target="#b106">[107]</ref> were proposed, by Mouragnon et al. <ref type="bibr" target="#b355">[356,</ref><ref type="bibr" target="#b357">358]</ref> and Engels et al. <ref type="bibr" target="#b128">[129]</ref>. SLAM, mostly used in robotics, solves in principle exactly the same problem as structure-from-motion: estimating camera motion and scene structure. The main difference is the real-time requirement in robotics applications, which lead to sequential solutions for cameras and other sensors (SLAM), whereas bundle adjustment and structure-from-motion systems were often developed without having to satisfy this requirement. The hybrid approaches of Mouragnon et al. and Engels et al. allow for real-time performance by following an overall sequential scheme, while continuously performing a complete non-linear optimization on as long subsequences as possible with the available computing power.</p><p>As mentioned above, an important aspect of bundle adjustment is the choice of cost function. As generally agreed, the "best" cost function is based on the reprojection error, i.e., the distance between measured image points and those predicted by the unknowns (scene structure, camera intrinsics or extrinsics). To be more precise, the cost function should be defined based on a statistical model of the noise in the input data, whenever this is available. If the input, as usual, are extracted image points then one should use an uncertainty estimate of the interest point extractor; under the default assumption of i.i.d. Gaussian noise this automatically leads to the standard cost function defined as the sum of squared distances between measured and predicted image points. Ideally, actual uncertainty estimates of image point positions should be used.</p><p>Minimizing reprojection errors requires carrying out forward projections of 3D points; as described elsewhere in this manuscript, for some cameras back-projection is easier to formulate. Further, if already calibrated cameras are considered, then one may develop a generic bundle adjustment working with camera rays instead of image points as basic input. Such procedures were for example proposed by Ramalingam et al. <ref type="bibr" target="#b413">[414,</ref><ref type="bibr" target="#b414">415]</ref>, Schweighofer et al. <ref type="bibr" target="#b442">[443]</ref>, and Lhuillier <ref type="bibr" target="#b308">[309,</ref><ref type="bibr" target="#b309">310]</ref>.</p><p>Ramalingam et al. used as cost function, the distance between camera rays and 3D points, which is simple to use but suboptimal. Schweighofer et al. gave a provably convergent algorithm for estimating camera motion and 3D structure, using the same cost function <ref type="bibr" target="#b442">[443]</ref>. As an alternative solution for non-central cameras, Ramalingam et al. proposed to partition the set of camera rays into clusters of approximately convergent rays, which are then treated as individual pinhole cameras inside a classical reprojection error-based bundle adjustment. Such an idea was also suggested though apparently not implemented, by Martins et al. <ref type="bibr" target="#b330">[331]</ref>.</p><p>Lhuillier proposed and justified the use of an angle-based cost function <ref type="bibr" target="#b308">[309,</ref><ref type="bibr" target="#b309">310]</ref>. To be precise, he suggested to use the squared tangent of the angle between the back-projected camera ray (computed from estimated camera parameters) and the line spanned by the optical center and the estimated 3D point. He also discussed how to minimize the reprojection error for catadioptric cameras.</p><p>Kannala et al. used a different angle-based cost function for bundle adjustment (sum of squared sines) <ref type="bibr" target="#b266">[267]</ref>, inspired by Oliensis <ref type="bibr" target="#b384">[385]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Three-Dimensional Scene Modeling</head><p>In this section, we cite a few approaches for complete 3D scene modeling from omnidirectional images. Most of them rely on a combination of the above structure-from-motion techniques and binocular or multiview dense stereo methods.</p><p>Kato et al. used a shape-from-silhouette approach with a fisheye camera to reconstruct a moving object, whose motion was supposed to be determinable <ref type="bibr" target="#b269">[270]</ref>.</p><p>Kang and Szeliski used mosaics acquired by a calibrated perspective camera and expressed in cylindrical coordinates <ref type="bibr" target="#b263">[264]</ref>. After motion estimation and bundle adjustment based on interest points, they carried out a dense multi-baseline stereo algorithm. A similar system, activevision based, was proposed by Ishiguro et al. <ref type="bibr" target="#b246">[247]</ref>. Teller and his group used georeferenced omnidirectional images for city modeling <ref type="bibr" target="#b492">[493]</ref>. Bunschoten and Kröse <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b68">69]</ref>, Doubek and Svoboda <ref type="bibr" target="#b118">[119]</ref>, Mičušik et al. <ref type="bibr" target="#b344">[345]</ref>, and Havlena et al. <ref type="bibr" target="#b213">[214]</ref> presented other multi-baseline stereo approaches for central omnidirectional images.</p><p>One of the most advanced approaches, demonstrated on long image sequences, is that of Lhuillier <ref type="bibr" target="#b309">[310]</ref>.</p><p>There also exist approaches for 3D modeling from single omnidirectional images. Sturm proposed a method for para-catadioptric cameras, exploiting user-provided geometric scene constraints such as perpendicularity or parallelism of lines and coplanarity of points <ref type="bibr" target="#b469">[470]</ref>. The method consists of two steps, first a calibration step, then the actual 3D modeling. It is important to underline that the second step is not specific to omnidirectional images, but could be applied to any calibrated central image: the available geometric constraints are applied on the back-projected camera rays and it does not matter what the underlying imaging geometry is.</p><p>Another single-view 3D modeling approach for omnidirectional images is due to Chen and Ip <ref type="bibr" target="#b92">[93]</ref>. The same comment as above applies, i.e., the actual 3D modeling method is valid for any central camera and not specific to omnidirectional ones. Chen and Ip used a rectangular object in fronto-parallel position, to calibrate a radial lens distortion model and used that object also as reference plane for the 3D model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Distortion Correction and Rectification</head><p>By distortion correction we understand here the generation of perspectively correct images from distorted ones. This is highly related to the problems of texture mapping and stereo rectification. Let us first consider texture mapping from one image: given a calibrated image and some surface, the goal is to "paint" the surface by back-projecting the image and its intensities onto it. Geometrically, this could be done by considering one pixel after the other; for each pixel, we determine the intersection of its camera ray and the surface and apply the pixel's intensities at that point. It is well known that such a process may lead to holes in the paint and other artifacts and that it is recommended to proceed in the opposite direction. To do so, let us consider a representation of the texture map as a 2D digital image. In addition, we need the mapping between positions in the texture map and points on the 3D surface. The texture map is then computed similar to the above but the other way round: for each pixel of it, we compute the corresponding point on the surface and project it into the input image. The intensities applied to the texture map pixel are then computed using some interpolation scheme, from the intensities in the input image in the vicinity of the computed image position. This is a standard procedure and can be applied to any type of surface and camera.</p><p>The application of this procedure to distortion correction is straightforward: it suffices to choose a plane as 3D surface for this texture mapping. The result will be a perspectively correct image, whenever the input image was taken by a central camera (see Figure <ref type="figure">6</ref>.1). For non-central cameras though, generation of perspectively correct images is only possible in general if the scene geometry is known (one can then texture map the scene and render it from any desired viewpoint with a virtual perspective camera). Swaminathan et al. proposed an approach to obtain good approximations of perspective views if the scene structure is unknown but if simple priors on it are available <ref type="bibr" target="#b482">[483]</ref>. A similar approach, for misaligned hyper-catadioptric cameras, was proposed by Jeng and Tsai <ref type="bibr" target="#b255">[256]</ref>. Ding and Yu proposed an interactive approach for minimizing distortions in multi-perspective images that can be modeled using the general linear camera model (GLC, see Section 3.4.4) <ref type="bibr" target="#b115">[116]</ref>. This problem is an instance of the more general image-based rendering problem <ref type="bibr" target="#b180">[181,</ref><ref type="bibr" target="#b306">307]</ref>.</p><p>As a sidenote, let us mention an optical procedure for generating perspective images from a fisheye image, explained and demonstrated in 1925 by Beck <ref type="bibr" target="#b47">[48]</ref>: he placed the negative of the fisheye image in the camera and reversed the action of the latter, by illuminating the negative and placing a photoplate in front of the camera. The photoplate then gets imprinted with a perspective photograph corresponding to the part of the fisheye's field of view covered by the plate.</p><p>Using planar 3D surfaces for distortion correction has its limits, since a hemispheric field of view would require an infinitely large texture map to hold the entire distortion-corrected input image. An alternative is to use cubemaps, i.e., where partial perspective view are rendered onto the insides of a virtual cube, see Figure <ref type="figure">6</ref>.1 and for example the paper <ref type="bibr" target="#b264">[265]</ref> by Kangni and Laganière. Cubemaps and other useful projections for environment mapping were well explained by Greene <ref type="bibr" target="#b186">[187]</ref>. See also the work <ref type="bibr" target="#b563">[564]</ref> by Zelnik-Manor et al. for various other suggestions for projections used to warp spherical images to flat ones.</p><p>The above texture mapping scheme can be applied for other tasks than the correction of non-perspective distortions. A typical example is to warp the usual circular omnidirectional images, which are hard to interpret by humans, into panoramic images; in that case, the 3D surface is the inside of a cylinder and the texture map corresponds to the unrolled cylinder. Besides this, the generation of the texture map is done exactly the same way as above.</p><p>Another application is stereo rectification. This consists in warping an image pair such that the warped images have an epipolar geometry where epipolar curves correspond to scanlines, i.e., where they are straight lines and such that in addition corresponding epipolar lines have the same vertical image coordinate. This enables efficient implementations of stereo matching algorithms. As for perspective images, rectification can be achieved by applying appropriate homographies to each image (there exist infinitely many solutions for these homographies; different methods differ in the choice of a particular solution), see for example <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b175">176,</ref><ref type="bibr" target="#b211">212,</ref><ref type="bibr" target="#b324">325,</ref><ref type="bibr" target="#b450">451]</ref>.</p><p>Geyer and Daniilidis proposed a stereo rectification method for para-catadioptric cameras, which applies conformal warping transformations <ref type="bibr" target="#b168">[169]</ref>. Banno and Kanade showed a rectification method for spherical images <ref type="bibr" target="#b23">[24]</ref>. Heller and Pajdla proposed a stereographic rectification procedure for omnidirectional stereo pairs <ref type="bibr" target="#b216">[217]</ref>. Instead of mapping epipolar curves to scanlines, they suggested to map them to circles; this is based on the observation that this way of rectifying minimizes, in some sense, the distortions between original and rectified images.</p><p>In 3D, one may picture stereo rectification as follows. Rectified images correspond to perspective cameras which are oriented identically (i.e., their optical axes are parallel and the image planes are rotated the same way) have the same focal length and whose baseline is parallel to the horizontal image direction. Given an input stereo pair, one may transform it into a rectified geometry, as follows. First, compute the baseline and choose any planar surface that is parallel to the baseline, at some finite distance from it. That surface will play the role of image plane for the rectified images. Next, for each camera, compute the rotation that would make the optical axis orthogonal to that plane and such that the image plane's horizontal axis becomes parallel to the baseline. Finally, one has to zoom in or out one or both of the views by appropriate factors that lead to equal focal lengths for both images. Rotating a camera in 3D and changing its focal length is equivalent to applying an appropriate projective transformation on its 2D image, the homography used in rectification algorithms.</p><p>This principle can be applied to any central camera, even nonperspective ones. The warping transformations are no longer homographies in general but this does not matter; all that is needed is the forward projection for the cameras, the determination of the rectified image plane, and the application of the above distortion correction method.</p><p>One problem with this approach is that a planar image surface is not adequate for handling wide field of view cameras, as explained above.</p><p>A second main problem is that the case where an epipole lies inside the image cannot be handled; it would again require an infinitely large rectified image.</p><p>For a fully general solution, one thus has to use other projection surfaces than planes. Roy et al. used a cylindrical surface, whose axis is orthogonal to the baseline <ref type="bibr" target="#b430">[431]</ref>. The final rectified images are obtained by projecting the input images on the cylinder and unrolling it. If a parametric projection model is available for both cameras, then the mapping from rectified to original images can be written in closed form. Abraham and Förstner showed such a rectification procedure for fisheye cameras <ref type="bibr" target="#b0">[1]</ref>. <ref type="bibr">Pollefeys et al.</ref> proposed a yet more general approach, which is applicable even without a closed-form projection model and which exploits the fact that in true cameras, the search for matches can usually be restricted to half-epipolar lines <ref type="bibr" target="#b403">[404]</ref>. Essentially, the approach generates rectified images by straightening individual epipolar curves, as follows (cf. Figure <ref type="figure">6</ref>.2). The set of pairs of epipolar curves are browsed. For each pair, a new row is added to each rectified image. Roughly speaking, the intensities of pixels along each epipolar curve are just plotted one after the other on that new row of pixels. It does not matter if or if not the position of the plotted pixel in the rectified image is in any simple parametric relationship to the pixel in the input image; all that matters is that all intensities found along an epipolar curve are reproduced on a single row in the rectified image. This approach is highly general and is even applicable to non-central cameras, as long as they satisfy the standard stereo geometry, cf. Section 3.4.2. A conceptual difference to the above approaches is that in general, it may not correspond to a projection on (or, texture mapping of) some 3D surface. some application. Our conclusion is that there is no single and simple answer, but that this choice depends on multiple factors such as the "true" camera geometry, the amount of noise in data, the distribution of calibration points, and so forth.</p><p>Finally, we note that this monograph is restricted to geometric aspects of omnidirectional vision. Equally if not more important, are optical and photometric characteristics; these are well explained and studied for example by Okatani and Deguchi <ref type="bibr" target="#b383">[384]</ref> and in various papers by <ref type="bibr">Nayar et al.</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 . 1</head><label>21</label><figDesc>Fig. 2.1 Examples of generation of central slit images. Top: "Standard" slit imaging principle and an early realization of it, the cylindrograph of Moëssard<ref type="bibr" target="#b350">[351]</ref>. Bottom: Slit imaging with a tilted 1D sensor, the so-called "cloud camera" and an image acquired with it (see text).</figDesc><graphic coords="11,334.97,289.45,139.97,138.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 . 2</head><label>22</label><figDesc>Fig. 2.2 Several ways of generating non-central slit or slit-like images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 . 4</head><label>24</label><figDesc>Fig. 2.4 Illustration of multi-mirror central catadioptric systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 . 5</head><label>25</label><figDesc>Fig. 2.5 Illustration of some single-lens catadioptric stereo systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 . 7</head><label>27</label><figDesc>Fig. 2.7 Other single-lens catadioptric stereo systems. Top: System using two hyperbolic mirrors with a coinciding focus point, at which the camera is located. This is a single-lens stereo system where each view is central. Bottom: Single-lens stereo system by Sagawa et al. composed of paraboloidal mirrors and sample acquired image; the camera looks at the mirrors from below. Images courtesy of Ryusuke Sagawa.</figDesc><graphic coords="31,308.94,237.98,165.68,124.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 . 1</head><label>31</label><figDesc>Fig. 3.1 Illustration of the two-plane model. A 1 and A 2 represent the interpolation transformations for the two grids, affine respectively quadratic transformations for the linear respectively quadratic versions of the model.</figDesc><graphic coords="49,162.64,345.03,134.57,100.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 3 . 2</head><label>32</label><figDesc>Fig. 3.2 Unified central catadioptric model of Geyer and Daniilidis. Left: Projection from 3D to 2D. The line joining the 3D point and the center of the sphere, cuts the sphere. The intersection points are then projected by a virtual pinhole camera. Right: Back-projection.An image point is first back-projected relative to the virtual pinhole camera. The intersections of the camera ray with the sphere are determined. The final camera rays are the lines spanned by the intersections points and the center of the sphere. For clarity, the second final camera ray is shown as a dashed line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 3 . 3</head><label>33</label><figDesc>Fig. 3.3 Illustration of a local two-plane model (cf. text). For the second image, the triangulation is shown only partially.</figDesc><graphic coords="69,164.71,343.48,133.63,100.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 3 . 4</head><label>34</label><figDesc>Fig. 3.4 Graphs of the function θ(r d ) for some camera models. In the first two rows, distortion model coefficients k are chosen such that for r d = 1, we have θ = 90 • .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 4 . 1</head><label>41</label><figDesc>Fig. 4.1 Epipolar geometry for a perspective and para-catadioptric image pair. Cyan circles indicate three point matches and associated epipolar lines/curves are shown in red. Epipoles are shown in white; there exist two of them in the para-catadioptric image.</figDesc><graphic coords="96,138.83,452.16,188.48,142.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 4 . 2</head><label>42</label><figDesc>Fig. 4.2 The image of a line in a linear pushbroom panorama.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 4 . 3</head><label>43</label><figDesc>Fig.4.3 Line images for the equiangular camera model. The black curve is the image of any line in the focal plane i.e., the plane containing the optical center and perpendicular to the optical axis. It is naturally a circle and corresponds to the hemispheric part of the field of view. The other curves show the images of lines whose interpretation planes (plane spanned by a line and the optical center) form angles of 70, 50, 30, and 10 degrees with the optical axis, respectively. The limiting case of 0 degrees corresponds to lines that are coplanar with the optical axis and whose images are lines going through the distortion center. Although in general, line images are not algebraic curves, the parts within the hemispheric field of view, i.e., within the black circle, can be relatively well approximated by conics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 6 . 1</head><label>61</label><figDesc>Fig. 6.1 Examples for distortion correction. Left and middle: A fisheye image and distortion corrected sections of it. Right: A cubemap generated from a fisheye image.</figDesc><graphic coords="140,221.44,447.79,174.99,175.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 6 . 2</head><label>62</label><figDesc>Fig. 6.2 Example of the rectification procedure of Pollefeys et al. On the left, two input images (the epipoles are marked as white dots), on the right the two rectified images. Courtesy of Marc Pollefeys [404].</figDesc><graphic coords="143,138.79,482.90,144.94,116.24" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>However, Sturm and Barreto showed that polynomial back-projection expressions can be obtained by using degree-4 liftings of image coordinates<ref type="bibr" target="#b472">[473]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>This is more often called equidistant model, but in the context of back-projection, we prefer to use the word equiangular.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>For central cameras, this is the pencil of epipolar planes.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>The fit gets always better if one extends the number of parameters of the same model, e.g., by using additional coefficients for modeling radial distortion or a finer tessellation for local camera models. As for different models, this needs not always be the case.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Another manifestation of this are correlations between the estimated parameters.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>P. Sturm is grateful to the Alexander-von-Humboldt Foundation for a Research Fellowship supporting his sabbatical at TU Munich, during which the largest part of this manuscript was written. He also acknowledges support by the French ANR project CAVIAR which supported part of the research carried out that eventually lead to the idea of writing this monograph. J. Barreto and P. Sturm are thankful for their support by the French-Portuguese collaboration program PHC Pessoa. The authors wish to thank Tomáš Pajdla for comments on the manuscript and Aseem Agarwala, Frédéric Devernay, Sujit Kuthirummal, Marc Pollefeys, Ryusuke Sagawa, and Steve Seitz for kindly allowing us to use images of their work.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concluding Remarks</head><p>We have described some image acquisition technologies and associated camera models, with an emphasis on omnidirectional cameras. Principles and existing works on basic issues for geometric computer vision, such as epipolar geometry, calibration, and structure-from-motion, were reviewed. One of the goals in writing this monograph was to give a survey of the literature and to highlight links between existing works and underlying principles. While aiming at a certain level of exhaustiveness in the coverage of the literature, such a survey can never be complete; we apologize for any significant omission. The reader is invited to notify the authors of works that should be cited; an addendum to this monograph shall be maintained on the first author's website.</p><p>Let us briefly summarize what we consider some of the main issues we were trying to convey. We concentrated on models for backprojection, in order to stress that these enable a simple and systematic formulation of epipolar geometry, self-calibration, pose and motion estimation, and the projection of 3D lines. For distortion correction and bundle adjustment however, forward projection should ideally be used. An important issue we discussed concerns the choice of camera model, for example if or if not a non-central model is appropriate in</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fish-eye-stereo calibration and epipolar rectification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Förstner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="278" to="288" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Single lens stereo with a plenoptic camera</title>
		<author>
			<persName><forename type="first">E</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="99" to="106" />
			<date type="published" when="1992-02">February 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Omnidirectional stereo systems for robot navigation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Adorni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mordonini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cagnoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sgorbissa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Omnidirectional Vision and Camera Networks</title>
		<meeting>the Workshop on Omnidirectional Vision and Camera Networks<address><addrLine>Madison, Wisconsin, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Photographing long scenes with multi-viewpoint panoramas</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH</title>
		<meeting>SIGGRAPH<address><addrLine>Boston, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="853" to="861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Analytical forward projection for axial non-central dioptric &amp; catadioptric cameras</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Taguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th European Conference on Computer Vision</title>
		<meeting>the 11th European Conference on Computer Vision<address><addrLine>Heraklion, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Camera calibration using spheres: A semi-definite programming approach</title>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th IEEE International Conference on Computer Vision</title>
		<meeting>the 9th IEEE International Conference on Computer Vision<address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="782" to="791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A neural approach to zoom-lens camera calibration from data with outliers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9-10</biblScope>
			<biblScope unit="page" from="619" to="630" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nonmetric calibration of camera lens distortion: Differential methods and robust estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1215" to="1230" />
			<date type="published" when="2005-08">August 2005</date>
		</imprint>
	</monogr>
	<note>References 147</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neurocalibration: A neural network that can tell camera calibration parameters</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hemayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th IEEE International Conference on Computer Vision</title>
		<meeting>the 7th IEEE International Conference on Computer Vision<address><addrLine>Kerkyra, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="463" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simultaneous object pose and velocity computation using a single view from a rolling shutter camera</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ait-Aider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Andreff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Lavest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Martinet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th European Conference on Computer Vision</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</editor>
		<meeting>the 9th European Conference on Computer Vision<address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="56" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Zoom-dependent calibration for consumer gradecameras</title>
		<author>
			<persName><forename type="first">S</forename><surname>Al-Ajlouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fraser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ISPRS Commission V Symposium on Image Engineering and Vision Metrology</title>
		<meeting>the ISPRS Commission V Symposium on Image Engineering and Vision Metrology<address><addrLine>Dresden, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09">September 2006</date>
			<biblScope unit="page" from="20" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Accurate catadioptric calibration for real-time pose estimation in room-size environments</title>
		<author>
			<persName><forename type="first">D</forename><surname>Aliaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th IEEE International Conference on Computer Vision</title>
		<meeting>the 8th IEEE International Conference on Computer Vision<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="127" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Perspective approximations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="179" to="192" />
			<date type="published" when="1990-08">August 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An algebraic approach to lens distortion by line rectification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sendra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="36" to="50" />
			<date type="published" when="2009-09">September 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Using mirror cameras for estimating depth</title>
		<author>
			<persName><forename type="first">J</forename><surname>Arnspang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Henriksen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Computer Analysis of Images and Patterns</title>
		<meeting>the 6th International Conference on Computer Analysis of Images and Patterns<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="711" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Photometric calibration of zoom lens systems</title>
		<author>
			<persName><forename type="first">N</forename><surname>Asada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Amano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Pattern Recognition</title>
		<meeting>the 13th International Conference on Pattern Recognition<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1996-08">August 1996</date>
			<biblScope unit="page" from="186" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A new approach for nonlinear distortion correction in endoscopic images based on least squares estimation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Asari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Radhakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="345" to="354" />
			<date type="published" when="1999-04">April 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neue Geräte und Methoden für die photogrammetrische Erschließung unerforschter Gebiete</title>
		<author>
			<persName><forename type="first">C</forename><surname>Aschenbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bildmessung und Luftbildwesen</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="38" />
			<date type="published" when="1929">1929</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Close Range Photogrammetry and Machine Vision</title>
		<author>
			<persName><forename type="first">K</forename><surname>Atkinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Whittles Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recovery of 3D animal motions using cameras and mirrors</title>
		<author>
			<persName><forename type="first">O</forename><surname>Avni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Katzir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Vision and Applications</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="879" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
		<title level="m">Stereovision and Sensor Fusion</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A theory of single-viewpoint catadioptric image formation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Panoramic mosaicing with a 180 • field of view lens</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bakstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Omnidirectional Vision</title>
		<meeting>the Workshop on Omnidirectional Vision<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="60" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Omnidirectional texturing based on robust 3D registration through Euclidean reconstruction from two spherical images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Banno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="491" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">General central projection systems: Modeling, calibration and visual servoing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barreto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003-09">September 2003</date>
			<pubPlace>Portugal</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Electrical and Computer Engineering, University of Coimbra</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A unifying geometric representation for central projection systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barreto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="208" to="217" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unifying image plane liftings for central catadioptric and dioptric cameras</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barreto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Imaging Beyond the Pinhole Camera</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Klette</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006-08">August 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Issues on the geometry of central catadioptric image formation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Araújo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Kauai, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="422" to="427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Geometric properties of central catadioptric line images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Araújo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th European Conference on Computer Vision</title>
		<meeting>the 7th European Conference on Computer Vision<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="237" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Paracatadioptric camera calibration using lines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Araújo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th IEEE International Conference on Computer Vision</title>
		<meeting>the 9th IEEE International Conference on Computer Vision<address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1359" to="1365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Geometric properties of central catadioptric line images and their application in calibration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Araújo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1327" to="1333" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fitting conics to paracatadioptric projections of lines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Araújo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="151" to="165" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Wide area multiple camera calibration and estimation of radial distortion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras</title>
		<meeting>the 5th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fundamental matrix for cameras with radial distortion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th IEEE International Conference on Computer Vision</title>
		<meeting>the 10th IEEE International Conference on Computer Vision<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="625" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Epipolar geometry of central projection systems using veronese maps</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1258" to="1265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic camera calibration applied to medical endoscopy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Roquette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fonseca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th British Machine Vision Conference</title>
		<meeting>the 20th British Machine Vision Conference<address><addrLine>London, England</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Non parametric distortion correction in endoscopic medical images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Roquette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 3DTV-CON Conference on The True Vision, Capture, Transmission and Display of 3D Video</title>
		<meeting>3DTV-CON Conference on The True Vision, Capture, Transmission and Display of 3D Video<address><addrLine>Kos, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A fast panoramic imaging system and intelligent imaging technique for mobile robots</title>
		<author>
			<persName><forename type="first">M</forename><surname>Barth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barrows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<meeting>the IEEE/RSJ International Conference on Intelligent Robots and Systems<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="626" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Eine einfache Methode der stereophotogrammetrischen Küstenvermessung</title>
		<author>
			<persName><forename type="first">O</forename><surname>Baschin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Petermanns Mitteilungen</title>
		<imprint>
			<date type="published" when="1908">1908</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">DLT-like calibration of central catadioptric cameras</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bastanlar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barreto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras</title>
		<meeting>the 8th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-10">October 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Alternative models for fish-eye lenses</title>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Licardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="433" to="441" />
			<date type="published" when="1995-04">April 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Iterative multi-step explicit camera calibration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Araújo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Almeida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th IEEE International Conference on Computer Vision</title>
		<meeting>the 6th IEEE International Conference on Computer Vision<address><addrLine>Bombay, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-01">January 1998</date>
			<biblScope unit="page" from="709" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Admissible linear map models of linear cameras</title>
		<author>
			<persName><forename type="first">G</forename><surname>Batog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Goaoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>San Francisco, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Omnidirectional vision: Unified model using conformal geometry</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bayro-Corrochano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>López-Franco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th European Conference on Computer Vision</title>
		<meeting>the 8th European Conference on Computer Vision<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="536" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Motion estimation by decoupling rotation and translation in catadioptric vision</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bazin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Demonceaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vasseur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="254" to="273" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Modelling and removing radial and tangential distortions in spherical lenses</title>
		<author>
			<persName><forename type="first">S</forename><surname>Beauchemin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajcsy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Theoretical Foundations of Computer Vision</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Klette</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Gimel'farb</surname></persName>
		</editor>
		<meeting>the 10th International Workshop on Theoretical Foundations of Computer Vision<address><addrLine>Dagstuhl Castle, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2000-03">March 2000</date>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A unified procedure for calibrating intrinsic parameters of fish-eye lenses</title>
		<author>
			<persName><forename type="first">S</forename><surname>Beauchemin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajcsy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Givaty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Vision interface</title>
		<meeting>Vision interface<address><addrLine>Trois-Rivières, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="272" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Apparatus to photograph the whole sky</title>
		<author>
			<persName><forename type="first">C</forename><surname>Beck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Scientific Instruments</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="135" to="139" />
			<date type="published" when="1925">1925</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A new catadioptric sensor for the panoramic vision of mobile robots</title>
		<author>
			<persName><forename type="first">R</forename><surname>Benosman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Deforas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Workshop on Omnidirectional Vision</title>
		<meeting>the IEEE Workshop on Omnidirectional Vision<address><addrLine>Hilton Head Island, South Carolina</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A brief historical perspective on panorama</title>
		<author>
			<persName><forename type="first">R</forename><surname>Benosman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Panoramic Vision: Sensors, Theory, and Applications</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Benosman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Kang</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="5" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m">Panoramic Vision</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Benosman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Kang</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Multidirectional stereovision sensor, calibration and scenes reconstruction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Benosman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Manière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Pattern Recognition</title>
		<meeting>the 13th International Conference on Pattern Recognition<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="161" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Accurate calibration of CCD cameras</title>
		<author>
			<persName><forename type="first">H</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Urbana-Champaign, Illinois, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="96" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">An introduction to panospheric imaging</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bogner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Systems, Man and Cybernetics</title>
		<meeting>the IEEE International Conference on Systems, Man and Cybernetics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="3099" to="3106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Cameras and sensing systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Boland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Manual of Photogrammetry, Fifth Edition</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Mcglone</surname></persName>
		</editor>
		<meeting><address><addrLine>Falls Church, Virginia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>American Society of Photogrammetry and Remote Sensing</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="581" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Epipolar-plane image analysis: An approach to determining structure from motion</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marimont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7" to="55" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">An omnidirectional stereoscopic system for mobile robot navigation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Boutteau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Savatier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Ertaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mazari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Workshop on Robotic and Sensors Environment</title>
		<meeting>the IEEE International Workshop on Robotic and Sensors Environment<address><addrLine>Ottawa, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="138" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A new camera of extremely high luminosity and resolution based on the concentric optical mirror system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bouwers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Der Sande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VII ISPRS-Congress</title>
		<meeting>the VII ISPRS-Congress<address><addrLine>Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1952">1952</date>
			<biblScope unit="page" from="246" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Distorsions optiques: Correction dans un modele projectif</title>
		<author>
			<persName><forename type="first">P</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mohr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bobet</surname></persName>
		</author>
		<idno>1933</idno>
	</analytic>
	<monogr>
		<title level="m">LIFIA-IMAG-INRIA</title>
		<meeting><address><addrLine>Rhône-Alpes, Grenoble, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-06">June 1993</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Distorsion optique: Correction dans un modèle projectif</title>
		<author>
			<persName><forename type="first">P</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mohr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bobet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Actes du 9ème Congrès AFCET de Reconnaissance des Formes et Intelligence Artificielle</title>
		<meeting>s du 9ème Congrès AFCET de Reconnaissance des Formes et Intelligence Artificielle<address><addrLine>Paris, France; Paris</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-01">January 1994</date>
			<biblScope unit="page" from="87" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Experimental results got with the omnidirectional vision sensor: SYCLOP</title>
		<author>
			<persName><forename type="first">E</forename><surname>Brassart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Delahoche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cauchois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Drocourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pegard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mouaddib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Workshop on Omnidirectional Vision</title>
		<meeting>the IEEE Workshop on Omnidirectional Vision<address><addrLine>Hilton Head Island, South Carolina</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="145" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A new algorithm to correct fish-eye and strong wide-angle-lens-distortion from single images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bräuer-Burchardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Voss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="225" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Decentering distortion of lenses</title>
		<author>
			<persName><forename type="first">D</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogrammetric Engineering</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="444" to="462" />
			<date type="published" when="1966-05">May 1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Close-range camera calibration</title>
		<author>
			<persName><forename type="first">D</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogrammetric Engineering</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="855" to="866" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A survey of image registration techniques</title>
		<author>
			<persName><forename type="first">L</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="325" to="376" />
			<date type="published" when="1992-12">December 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Omniview cameras with curved surface mirrors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Workshop on Omnidirectional Vision</title>
		<meeting>the IEEE Workshop on Omnidirectional Vision<address><addrLine>Hilton Head Island, South Carolina</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="79" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The twisted cubic and camera calibration</title>
		<author>
			<persName><forename type="first">T</forename><surname>Buchanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics and Image Processing</title>
		<imprint>
			<date type="published" when="1988-04">April 1988</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="130" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">3D scene reconstruction from cylindrical panoramic images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bunschoten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kröse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="111" to="118" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Robust scene reconstruction from an omnidirectional vision system</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bunschoten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kröse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="351" to="357" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Zoom lens calibration for wind tunnel measurements</title>
		<author>
			<persName><forename type="first">A</forename><surname>Burner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SPIE Conference on Videometrics IV</title>
		<editor>
			<persName><forename type="first">S</forename><surname>El-Hakim</surname></persName>
		</editor>
		<meeting>the SPIE Conference on Videometrics IV<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>SPIE -Society of Photo-Optical Instrumentation Engineers</publisher>
			<date type="published" when="1995-10">October 1995</date>
			<biblScope unit="page" from="19" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">The history of the discovery of cinematography</title>
		<author>
			<persName><forename type="first">P</forename><surname>Burns</surname></persName>
		</author>
		<ptr target="http://www.precinemahistory.net" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Fast and robust numerical solutions to minimal problems for cameras with radial distortion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Byröd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kúkelová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Josephson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Åström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Anchorage, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Omnidirectional stereo vision with a hyperbolic double lobed mirror</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cabral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>De Souza Junior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hunold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Pattern Recognition</title>
		<meeting>the 15th International Conference on Pattern Recognition<address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Precise stereopsis with a single video camera</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cafforio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rocca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd European Signal Processing Conference (EUSIPCO): Theories and Application</title>
		<meeting>the 3rd European Signal Processing Conference (EUSIPCO): Theories and Application<address><addrLine>The Hague, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="641" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Localization of straight lines from single images using off-axis catadioptric cameras</title>
		<author>
			<persName><forename type="first">V</forename><surname>Caglioti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gasparini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras, Beijing</title>
		<meeting>the 6th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras, Beijing<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">On the localization of straight lines in 3D space from single 2D images</title>
		<author>
			<persName><forename type="first">V</forename><surname>Caglioti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gasparini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1129" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">How many planar viewing surfaces are there in noncentral catadioptric cameras?</title>
		<author>
			<persName><forename type="first">V</forename><surname>Caglioti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gasparini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1266" to="1273" />
		</imprint>
	</monogr>
	<note>Towards singe-image localization of space lines</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Singleimage calibration of off-axis catadioptric cameras using lines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Caglioti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Taddei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boracchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gasparini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras</title>
		<meeting>the 7th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras<address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Hybrid stereo sensor with omnidirectional vision capabilities: Overview and calibration procedures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cagnoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mordonini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mussi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Adorni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Image Analysis and Processing</title>
		<meeting>the 14th International Conference on Image Analysis and Processing<address><addrLine>Modena, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="99" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Dynamic omnidirectional vision for mobile robots</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Robotic Systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="17" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Using vanishing points for camera calibration</title>
		<author>
			<persName><forename type="first">B</forename><surname>Caprile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="127" to="140" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">3D localization with conical vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cauchois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brassart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Delahoche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clerentin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Omnidirectional Vision and Camera Networks</title>
		<meeting>the Workshop on Omnidirectional Vision and Camera Networks<address><addrLine>Madison, Wisconsin, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Calibration of the omnidirectional vision sensor: SYCLOP</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cauchois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brassart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Drocourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vasseur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation<address><addrLine>Detroit, Michigan, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-05">May 1999</date>
			<biblScope unit="page" from="1287" to="1292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Reflective surfaces for panoramic imaging</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Optics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">31</biblScope>
			<biblScope unit="page" from="8275" to="8285" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Accurate calibration of cameras and range imaging sensors: The NPBS method</title>
		<author>
			<persName><forename type="first">G</forename><surname>Champleboux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lavallée</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sautot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cinquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation<address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-05">May 1992</date>
			<biblScope unit="page" from="1552" to="1558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Omni-directional structure from motion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hébert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Workshop on Omnidirectional Vision</title>
		<meeting>the IEEE Workshop on Omnidirectional Vision<address><addrLine>Hilton Head Island, South Carolina</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="127" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Appareil photographique multiple pour études sur la prise de vues aériennes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Charriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Valette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the V ISPRS-Congress</title>
		<meeting>the V ISPRS-Congress<address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1938">1938</date>
			<biblScope unit="page" from="154" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">On pose recovery for generalized visual sensors</title>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="848" to="861" />
			<date type="published" when="2004-07">July 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Visually estimating workpiece pose in a robot hand using the feature points method</title>
		<author>
			<persName><forename type="first">N.-Y</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
			<pubPlace>Kingston</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Rhode Island</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Estimating workpiece pose using the feature points method</title>
		<author>
			<persName><forename type="first">N.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Birk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kelley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1027" to="1041" />
			<date type="published" when="1980-12">December 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Camera calibration with two arbitrary coplanar circles</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th European Conference on Computer Vision</title>
		<meeting>the 8th European Conference on Computer Vision<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="521" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Quicktime vr -an image-based approach to virtual environment navigation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH</title>
		<meeting>SIGGRAPH<address><addrLine>Los Angeles, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Single view metrology of wide-angle lens images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Visual Computer</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="445" to="455" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">A simple &quot;equal area&quot; calibration for fisheye photography</title>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Follin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Agricultural and Forest Meteorology</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="19" to="25" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">The development of camera calibration methods and models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fryer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogrammetric Record</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="51" to="66" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">The principal point and CCD cameras</title>
		<author>
			<persName><forename type="first">T</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fryer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogrammetric Record</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="293" to="312" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">A plumbline constraint for the rational function lens distortion model</title>
		<author>
			<persName><forename type="first">D</forename><surname>Claus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th British Machine Vision Conference</title>
		<meeting>the 16th British Machine Vision Conference<address><addrLine>Oxford, England</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="99" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">A rational function lens distortion model for general cameras</title>
		<author>
			<persName><forename type="first">D</forename><surname>Claus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="213" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Resolution invariant surfaces for panoramic vision systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th IEEE International Conference on Computer Vision</title>
		<meeting>the 7th IEEE International Conference on Computer Vision<address><addrLine>Kerkyra, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="392" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Lens distortion recovery for accurate sequential structure and motion recovery</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cornelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th European Conference on Computer Vision</title>
		<meeting>the 7th European Conference on Computer Vision<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="186" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Accurate 3D shape and displacement measurement using a scanning electron microscope</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cornille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005-06">June 2005</date>
		</imprint>
		<respStmt>
			<orgName>University of South Carolina and Institut National des Sciences Appliquées de Toulouse</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Maintaining stereo calibration by tracking image points</title>
		<author>
			<persName><forename type="first">J</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bobet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-06">June 1993</date>
			<biblScope unit="page" from="483" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">The page of omnidirectional vision</title>
		<author>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<ptr target="http://www.cis.upenn.edu/∼kostas/omni.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Camera calibration from sphere images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Daucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dhome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Lapresté</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd European Conference on Computer Vision</title>
		<meeting>the 3rd European Conference on Computer Vision<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="449" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Camera for conical peripheral and panoramic photography</title>
		<author>
			<persName><forename type="first">A</forename><surname>Davidhazy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Course</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Using geometrical constraints for fisheye camera calibration</title>
		<author>
			<persName><forename type="first">C</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras</title>
		<meeting>the 6th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">MonoSLAM: Real-time single camera SLAM</title>
		<author>
			<persName><forename type="first">A</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Molton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Stasse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1052" to="1067" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Cone-pixels camera models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Debaecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benosman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ieng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras</title>
		<meeting>the 8th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Dense reconstruction by zooming</title>
		<author>
			<persName><forename type="first">C</forename><surname>Delherm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Lavest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dhome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Lapresté</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th European Conference on Computer Vision</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Buxton</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</editor>
		<meeting>the 4th European Conference on Computer Vision<address><addrLine>Cambridge, England</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="427" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">An easy calibration method for central catadioptric cameras</title>
		<author>
			<persName><forename type="first">X.-M</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Automatica Sinica</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="801" to="808" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Approximating a single viewpoint in panoramic imaging devices</title>
		<author>
			<persName><forename type="first">S</forename><surname>Derrien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="3931" to="3938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Discours de la méthode pour bien conduire sa raison, et chercher la vérité dans les sciences</title>
		<author>
			<persName><forename type="first">R</forename><surname>Descartes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ian Maire</title>
		<imprint>
			<biblScope unit="page">1637</biblScope>
			<pubPlace>Leyden</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Straight lines have to be straight</title>
		<author>
			<persName><forename type="first">F</forename><surname>Devernay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Faugeras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Vision and Applications</title>
		<imprint>
			<date type="published" when="2001-08">August 2001</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="14" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Spatial localization of modelled objects of revolution in monocular perspective vision</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dhome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lapreste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Richetin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st European Conference on Computer Vision</title>
		<meeting>the 1st European Conference on Computer Vision<address><addrLine>Antibes, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="475" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">Fisheye digital imaging for under twenty dollars</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dietz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006-04">April 2006</date>
		</imprint>
		<respStmt>
			<orgName>University of Kentucky</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Multiperspective distortion correction using collineations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="95" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Photogrammetrische Lösung des Wolkenproblems aus einem Standpunkt unter Verwendung der Reflexe</title>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sitzungsberichte Kaiserliche Akademie der Wissenschaften, mathematisch-naturwissenschaftliche Klasse, Abteilung IIa</title>
		<imprint>
			<date type="published" when="1902">1902</date>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="788" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Image registration for foveated omnidirectional sensing</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dornaika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Elder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th European Conference on Computer Vision</title>
		<meeting>the 7th European Conference on Computer Vision<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="606" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Reliable 3D reconstruction from a few catadioptric images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Doubek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Svoboda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Omnidirectional Vision</title>
		<meeting>the Workshop on Omnidirectional Vision<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Plane-based calibration for linear cameras</title>
		<author>
			<persName><forename type="first">J</forename><surname>Draréni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras</title>
		<meeting>the 8th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-10">October 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">A natural classiffication of curves and surfaces with reflection properties</title>
		<author>
			<persName><forename type="first">D</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Locke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics Magazine</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Self-calibration of the intrinsic parameters of cameras for active vision systems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="477" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">A comparison of new generic camera calibration with the standard parametric approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dunne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mallon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Whelan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IAPR Conference on Machine Vision Applications</title>
		<meeting>IAPR Conference on Machine Vision Applications<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="114" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Efficient generic calibration method for general cameras with single centre of projection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dunne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mallon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Whelan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th IEEE International Conference on Computer Vision</title>
		<meeting>the 11th IEEE International Conference on Computer Vision<address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Efficient generic calibration method for general cameras with single centre of projection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dunne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mallon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Whelan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="220" to="233" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">A camera calibration technique using three sets of parallel lines</title>
		<author>
			<persName><forename type="first">T</forename><surname>Echigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Vision and Applications</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="159" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Nonmetric lens distortion calibration: Closedform solutions, robust estimation and model selection</title>
		<author>
			<persName><forename type="first">M</forename><surname>El-Melegy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th IEEE International Conference on Computer Vision</title>
		<meeting>the 9th IEEE International Conference on Computer Vision<address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="554" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">An affine solution to euclidean calibration for a zoom lens</title>
		<author>
			<persName><forename type="first">R</forename><surname>Enciso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Viéville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ALCATECH Workshop</title>
		<meeting>the ALCATECH Workshop<address><addrLine>Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-07">July 1996</date>
			<biblScope unit="page" from="21" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Bundle adjustment rules</title>
		<author>
			<persName><forename type="first">C</forename><surname>Engels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stewénius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nistér</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ISPRS Symposium on Photogrammetric Computer Vision</title>
		<meeting>ISPRS Symposium on Photogrammetric Computer Vision<address><addrLine>Bonn, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">A closed-form solution for the generic self-calibration of central cameras from two rotational flows</title>
		<author>
			<persName><forename type="first">F</forename><surname>Espuny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision Theory and Applications</title>
		<meeting>the International Conference on Computer Vision Theory and Applications<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Generic self-calibration of central cameras from two &quot;real&quot; rotational flows</title>
		<author>
			<persName><forename type="first">F</forename><surname>Espuny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Burgos</forename><surname>Gil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras</title>
		<meeting>the 8th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras</meeting>
		<imprint>
			<date type="published" when="2008">Marseille, France, 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">An analytical solution to the perspective-n-point problem for common planar camera and for catadioptric sensor</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fabrizio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Image and Graphics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="135" to="155" />
			<date type="published" when="2008-01">January 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Calibration of panoramic catadioptric sensors made easier</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fabrizio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tarel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benosman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Omnidirectional Vision</title>
		<meeting>the Workshop on Omnidirectional Vision<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Calibration of close-range photogrammetric systems: Mathematical formulation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Faig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogrammetric Engineering &amp; Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1479" to="1486" />
			<date type="published" when="1975-12">December 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Blind removal of image non-linearities</title>
		<author>
			<persName><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th IEEE International Conference on Computer Vision, Vancouver</title>
		<meeting>the 8th IEEE International Conference on Computer Vision, Vancouver</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="76" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">A revolving cloud camera</title>
		<author>
			<persName><forename type="first">O</forename><surname>Fassig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monthly Weather Review</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="274" to="275" />
			<date type="published" when="1915">1915</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Faugeras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Papadopoulo</surname></persName>
		</author>
		<title level="m">The Geometry of Multiple Images</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001-03">March 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">On the geometry and algebra of the point and line correspondences between n images</title>
		<author>
			<persName><forename type="first">O</forename><surname>Faugeras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mourrain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th IEEE International Conference on Computer Vision</title>
		<meeting>the 5th IEEE International Conference on Computer Vision<address><addrLine>Cambridge, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-06">June 1995</date>
			<biblScope unit="page" from="951" to="956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">On the epipolar geometry of the crossed-slits projection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th IEEE International Conference on Computer Vision</title>
		<meeting>the 9th IEEE International Conference on Computer Vision<address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="988" to="995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<title level="m" type="main">Random lens imaging</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<idno>MIT- CSAIL-TR-2006-058</idno>
		<imprint>
			<date type="published" when="2006-09">September 2006</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Geometry of eye design: Biology and technology</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fermüller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Theoretical Foundations of Computer Vision</title>
		<meeting>the 10th International Workshop on Theoretical Foundations of Computer Vision<address><addrLine>Dagstuhl Castle, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="22" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Mainly Mechanics, Radiation, and Heat</title>
		<author>
			<persName><forename type="first">R</forename><surname>Feynman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leighton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sands</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Feynman Lectures on Physics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="1963">1963</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Feature extraction and calibration for stereo reconstruction using non-svp optics in a panoramic stereo-vision sensor</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fiala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Omnidirectional Vision</title>
		<meeting>the Workshop on Omnidirectional Vision<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Die geometrischen Grundlagen der Photogrammetrie</title>
		<author>
			<persName><forename type="first">S</forename><surname>Finsterwalder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jahresbericht Deutscher Mathematik</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="44" />
			<date type="published" when="1899">1899</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Simultaneous linear estimation of multiple view geometry and lens distortion</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Kauai, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="125" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<monogr>
		<title level="m" type="main">Perspective projection: The wrong imaging model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fleck</surname></persName>
		</author>
		<idno>TR 95-01</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
			<pubPlace>Iowa City, IA 52242, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Iowa</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Analytical photogrammetric operations</title>
		<author>
			<persName><forename type="first">W</forename><surname>Förstner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wrobel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Paderes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Craig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dolloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Manual of Photogrammetry, Fifth Edition</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Mcglone</surname></persName>
		</editor>
		<meeting><address><addrLine>Falls Church, Virginia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>American Society of Photogrammetry and Remote Sensing</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="763" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Camera calibration for miniature, low-cost, wide-angle imaging system</title>
		<author>
			<persName><forename type="first">O</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Durrant-Whyte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th British Machine Vision Conference</title>
		<meeting>the 18th British Machine Vision Conference<address><addrLine>Warwick, England</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Digital camera self-calibration</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fraser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="149" to="159" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Zoom-dependent calibration for consumer gradecameras</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Al-Ajlouni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogrammetric Engineering &amp; Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="1017" to="1026" />
			<date type="published" when="2006-09">September 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Stellar calibration of the orbigon lens</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogrammetric Engineering</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="115" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Lens distortion for close-range photogrammetry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fryer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogrammetric Engineering &amp; Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="51" to="58" />
			<date type="published" when="1986-01">January 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Mirror design for an omnidirectional camera with a space variant imager</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gächter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mičušík</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Omnidirectional Vision Applied to Robotic Orientation and Nondestructive Testing</title>
		<meeting>the Workshop on Omnidirectional Vision Applied to Robotic Orientation and Nondestructive Testing<address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Single camera stereo using planar parallel plate</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Pattern Recognition</title>
		<meeting>the 15th International Conference on Pattern Recognition<address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="108" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">A hemispherical imaging camera</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="168" to="178" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<monogr>
		<title level="m" type="main">Omnidirectional vision for mobile robot navigation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gaspar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002-12">December 2002</date>
			<publisher>Portgual</publisher>
		</imprint>
		<respStmt>
			<orgName>Universidade Técnica de Lisboa</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Constant resolution omnidirectional cameras</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gaspar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Deccó</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Okamoto</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Santos-Victor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Omnidirectional Vision</title>
		<meeting>the Workshop on Omnidirectional Vision<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Multi-view matching tensors from lines for general camera models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gasparini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tensors in Image Processing and Computer Vision</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Aja-Fernández</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>De Luis García</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Plane-based calibration of central catadioptric cameras</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gasparini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barreto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th IEEE International Conference on Computer Vision</title>
		<meeting>the 12th IEEE International Conference on Computer Vision<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<monogr>
		<title level="m" type="main">Mehrfachkammer für Aufnahmen aus Luftfahrzeugen</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gasser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1926">1926</date>
			<biblScope unit="volume">469</biblScope>
			<pubPlace>Reichspatentamt, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Generalized camera calibration including fish-eye lenses</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gennery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="239" to="266" />
			<date type="published" when="2006-07">July 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<monogr>
		<title level="m" type="main">Light-field capture by multiplexing in the frequency domain</title>
		<author>
			<persName><forename type="first">T</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Intwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Babacan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-04">April 2007</date>
		</imprint>
		<respStmt>
			<orgName>Adobe Systems Incorporated</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Catadioptric camera calibration</title>
		<author>
			<persName><forename type="first">C</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th IEEE International Conference on Computer Vision</title>
		<meeting>the 7th IEEE International Conference on Computer Vision<address><addrLine>Kerkyra, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="398" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">A unifying theory of central panoramic systems and practical applications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th European Conference on Computer Vision</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Vernon</surname></persName>
		</editor>
		<meeting>the 6th European Conference on Computer Vision<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2000-06">June 2000</date>
			<biblScope unit="page" from="445" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Catadioptric projective geometry</title>
		<author>
			<persName><forename type="first">C</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="223" to="243" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Structure and motion from uncalibrated catadioptric views</title>
		<author>
			<persName><forename type="first">C</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Kauai, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="279" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Paracatadioptric camera calibration</title>
		<author>
			<persName><forename type="first">C</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="687" to="695" />
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Properties of the catadioptric fundamental matrix</title>
		<author>
			<persName><forename type="first">C</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th European Conference on Computer Vision</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Heyden</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Sparr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Johansen</surname></persName>
		</editor>
		<meeting>the 7th European Conference on Computer Vision<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="140" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Conformal rectification of an omnidirectional stereo pair</title>
		<author>
			<persName><forename type="first">C</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Omnidirectional Vision and Camera Networks</title>
		<meeting>the Workshop on Omnidirectional Vision and Camera Networks<address><addrLine>Madison, Wisconsin, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Mirrors in motion: Epipolar geometry and motion estimation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th IEEE International Conference on Computer Vision</title>
		<meeting>the 9th IEEE International Conference on Computer Vision<address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="766" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Geometric models of rolling-shutter cameras</title>
		<author>
			<persName><forename type="first">C</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meingast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras</title>
		<meeting>the 6th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">A nine-point algorithm for estimating paracatadioptric fundamental matrices</title>
		<author>
			<persName><forename type="first">C</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stewénius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th IEEE International Conference on Computer Vision</title>
		<meeting>the 11th IEEE International Conference on Computer Vision<address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Ego-motion and omnidirectional cameras</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gluckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th IEEE International Conference on Computer Vision</title>
		<meeting>the 6th IEEE International Conference on Computer Vision<address><addrLine>Bombay, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-01">January 1998</date>
			<biblScope unit="page" from="999" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Planar catadioptric stereo: geometry and calibration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gluckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Fort Collins, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="22" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Catadioptric stereo using planar mirrors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gluckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="65" to="79" />
			<date type="published" when="2001-08">August 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Rectifying transformations that minimize resampling effects</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gluckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Kauai, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="111" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Rectified catadioptric stereo sensors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gluckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="224" to="236" />
			<date type="published" when="2002-02">February 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Panoramic line-scan imaging system for teleoperator control</title>
		<author>
			<persName><forename type="first">S</forename><surname>Godber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Petty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE, Stereoscopic Displays and Virtual Reality Systems</title>
		<meeting>SPIE, Stereoscopic Displays and Virtual Reality Systems</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="247" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Projection model, 3D reconstruction and rigid motion estimation from non-central catadioptric images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gonçalvez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Araújo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Symposium on 3D Data Processing, Visualization and Transmission</title>
		<meeting>the Second International Symposium on 3D Data Processing, Visualization and Transmission<address><addrLine>Chapel Hill, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="325" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Estimating parameters of noncentral catadioptric systems using bundle adjustment</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gonçalvez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Araújo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2009-01">January 2009</date>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="11" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">The lumigraph</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH</title>
		<meeting>SIGGRAPH<address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Correction of image deformation from lens distortion using Bezier patches</title>
		<author>
			<persName><forename type="first">A</forename><surname>Goshtasby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics and Image Processing</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Design of a single-lens stereo camera system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Goshtasby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gruver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="923" to="938" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Estimating ego-motion using a panoramic sensor: Comparison between a bio-inspired and a camera-calibrated method</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gourichon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ieng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Smadja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benosman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AISB Symposium on Biologically Inspired Vision, Theory and Application</title>
		<meeting>the AISB Symposium on Biologically Inspired Vision, Theory and Application</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="91" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Analytical photogrammetry applied to single terrestrial photograph mensuration</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gracie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the XIth International Congress of Photogrammetry</title>
		<meeting>the XIth International Congress of Photogrammetry<address><addrLine>Lausanne, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1968-07">July 1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Removal of instrument signature from mariner 9 television images of mars</title>
		<author>
			<persName><forename type="first">W</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jepsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kreznar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Seidman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Optics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="114" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Environment mapping and other applications of world projections</title>
		<author>
			<persName><forename type="first">N</forename><surname>Greene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="21" to="29" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">The tube peeper: A new concept in endoscopy</title>
		<author>
			<persName><forename type="first">P</forename><surname>Greguss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics &amp; Laser Technology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="45" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Geometric camera calibration using systems of linear equations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gremban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="562" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">A unified approach to the linear camera calibration problem</title>
		<author>
			<persName><forename type="first">W</forename><surname>Grosky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tamburino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="663" to="671" />
			<date type="published" when="1990-07">July 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">The raxel imaging model and ray-based calibration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grossberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="137" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Discrete camera calibration from pixel streams</title>
		<author>
			<persName><forename type="first">E</forename><surname>Grossmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gaspar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Orabona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="198" to="209" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Are two rotational flows sufficient to calibrate a smooth non-parametric sensor</title>
		<author>
			<persName><forename type="first">E</forename><surname>Grossmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hislop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nistér</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stewénius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1222" to="1229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Discrete camera calibration from the information distance between pixel streams</title>
		<author>
			<persName><forename type="first">E</forename><surname>Grossmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Orabona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gaspar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras</title>
		<meeting>the 7th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras<address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<monogr>
		<title level="m" type="main">Calibration and Orientation of Cameras in Computer Vision</title>
		<editor>A. Gruen and T. Huang</editor>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">Polyhedral geometry and the two-plane parameterization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eurographics Workshop on Rendering Techniques</title>
		<meeting>the Eurographics Workshop on Rendering Techniques<address><addrLine>St. Etienne, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Linear pushbroom cameras</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="963" to="975" />
			<date type="published" when="1997-09">September 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<monogr>
		<title level="m" type="main">Camera estimation for orbiting pushbroom imaging systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>unpublished</note>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Euclidean structure from n &gt;= 2 parallel circles: Theory and algorithms</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gurdjos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th European Conference on Computer Vision</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</editor>
		<meeting>the 9th European Conference on Computer Vision<address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-05">May 2006</date>
			<biblScope unit="page" from="238" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">RTcams: A new perspective on non-photorealistic rendering from photographs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="966" to="979" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">A new method for the determination of the distortion and the inner orientation of cameras and projectors</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hallert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogrammetria</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1954" to="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">The method of least squares applied to multicollimator camera calibration</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hallert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogrammetric Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="836" to="840" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">Measuring bidirectional texture reflectance with a kaleidoscope</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Perlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="741" to="748" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">A new method for distortion correction of electronic endoscope images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Haneishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yagihashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Miyake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="548" to="555" />
			<date type="published" when="1995-09">September 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">Review and analysis of solutions of the three point perspective pose estimation problem</title>
		<author>
			<persName><forename type="first">R</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ottenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nolle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="331" to="356" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">Estimation of relative camera positions for uncalibrated cameras</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd European Conference on Computer Vision</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Sandini</surname></persName>
		</editor>
		<meeting>the 2nd European Conference on Computer Vision<address><addrLine>Santa Margherita Ligure, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="579" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">An algorithm for self calibration from several views</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="908" to="912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">In defence of the 8-point algorithm</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th IEEE International Conference on Computer Vision, Cambridge, Massachusetts</title>
		<meeting>the 5th IEEE International Conference on Computer Vision, Cambridge, Massachusetts<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-06">June 1995</date>
			<biblScope unit="page" from="1064" to="1070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">Parameter-free radial distortion correction with center of distortion estimation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1309" to="1321" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">The cubic rational polynomial camera model</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the DARPA Image Understanding Workshop</title>
		<meeting>the DARPA Image Understanding Workshop<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="649" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">Triangulation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="146" to="157" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000-06">June 2000</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004-03">March 2004</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
	<note>2nd Edition</note>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">Randomized structure from motion based on atomic 3D models from camera triplets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Havlena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Knopp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Miami, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2874" to="2881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">Geometric camera calibration using circular control points</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heikkilä</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1066" to="1077" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">Calibration procedure for short focal length off-theshelf CCD cameras</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heikkilä</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Silvén</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Pattern Recognition</title>
		<meeting>the 13th International Conference on Pattern Recognition<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1996-08">August 1996</date>
			<biblScope unit="page" from="166" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">Stereographic rectification of omnidirectional stereo pairs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Miami, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1414" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<monogr>
		<title level="m" type="main">Fisheye-Objektive in der Nahbereichsphotogrammetrie -Theoretische und praktische Untersuchungen</title>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Hellmeier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<pubPlace>Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Technische Universität Braunschweig</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">Calibration of fisheye lenses by inversion of area projections</title>
		<author>
			<persName><forename type="first">T</forename><surname>Herbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Optics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1875" to="1876" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b219">
	<monogr>
		<title level="m" type="main">The page of catadioptric sensor design</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hicks</surname></persName>
		</author>
		<ptr target="http://www.math.drexel.edu/ahicks/design/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">Designing a mirror to realize a given projection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hicks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America A</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="323" to="330" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title level="a" type="main">Catadioptric sensors that approximate wide-angle perspective projections</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajcsy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Hilton Head Island, South Carolina, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="545" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<analytic>
		<title level="a" type="main">Reflective surfaces as computational sensors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajcsy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="773" to="777" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b223">
	<analytic>
		<title level="a" type="main">Programmable imaging with two-axis micromirrors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nasis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kurzweg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics Letters</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1066" to="1068" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b224">
	<analytic>
		<title level="a" type="main">Equi-areal catadioptric sensors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Perline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Omnidirectional Vision</title>
		<meeting>the Workshop on Omnidirectional Vision<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main">A lens for whole sky photographs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of the Royal Meteorological Society</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">211</biblScope>
			<biblScope unit="page" from="227" to="235" />
			<date type="published" when="1924">1924</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<monogr>
		<title level="m" type="main">Illustrated history of photography</title>
		<author>
			<persName><surname>Historiccamera</surname></persName>
		</author>
		<author>
			<persName><surname>Com</surname></persName>
		</author>
		<ptr target="http://www.historiccamera.com/history1/photohistory300.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main">Krill-eye: Superposition compound eye for wide-angle imaging via GRIN lenses</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hiura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras</title>
		<meeting>the 9th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main">The stereoscope and the stereograph</title>
		<author>
			<persName><forename type="first">O</forename><surname>Holmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Atlantic Monthly</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="738" to="749" />
			<date type="published" when="1859-06">June 1859</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b229">
	<analytic>
		<title level="a" type="main">Camera calibration problem: Some new results</title>
		<author>
			<persName><forename type="first">R</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Netravali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics and Image Processing: Image Understanding</title>
		<imprint>
			<date type="published" when="1991-11">November 1991</date>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="368" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b230">
	<analytic>
		<title level="a" type="main">Image-based homing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pinette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Riseman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation<address><addrLine>Sacramento, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991-04">April 1991</date>
			<biblScope unit="page" from="620" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b231">
	<analytic>
		<title level="a" type="main">Object pose: The link between weak perspective, paraperspective and full perspective</title>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dornaika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lamiroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Christy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="173" to="189" />
			<date type="published" when="1997-03">March 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b232">
	<analytic>
		<title level="a" type="main">Closed-form solution of absolute orientation using orthonormal matrices</title>
		<author>
			<persName><forename type="first">B</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hilden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Negahdaripour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America A</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1127" to="1135" />
			<date type="published" when="1988-07">July 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b233">
	<analytic>
		<title level="a" type="main">A high-resolution panoramic camera</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Kauai, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="960" to="967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b234">
	<analytic>
		<title level="a" type="main">Design analysis of a high-resolution panoramic camera using conventional imagers and a mirror pyramid</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="356" to="361" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b235">
	<monogr>
		<title level="m" type="main">Epipolar geometry in concentric panoramas</title>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<idno>CTU-CMP-2000-07</idno>
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>Prague</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Center for Machine Perception, Czech Technical University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b236">
	<analytic>
		<title level="a" type="main">Sensor pose estimation from multi-center cylindrical panoramas</title>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Pacific Rim Symposium on Advances in Image and Video Technology</title>
		<meeting>the Third Pacific Rim Symposium on Advances in Image and Video Technology<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="48" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b237">
	<analytic>
		<title level="a" type="main">Epipolar geometry in polycentric panoramas</title>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Theoretical Foundations of Computer Vision</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Klette</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Gimel'farb</surname></persName>
		</editor>
		<meeting>the 10th International Workshop on Theoretical Foundations of Computer Vision<address><addrLine>Dagstuhl Castle, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="39" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b238">
	<analytic>
		<title level="a" type="main">Comparative studies of line-based panoramic camera calibration</title>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-K</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Omnidirectional Vision and Camera Networks</title>
		<meeting>the Workshop on Omnidirectional Vision and Camera Networks<address><addrLine>Madison, Wisconsin, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b239">
	<analytic>
		<title level="a" type="main">Equidistant fish-eye calibration and rectification by vanishing point extraction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Denny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Glavin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2289" to="2296" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b240">
	<analytic>
		<title level="a" type="main">Equidistant (fθ) fish-eye perspective with application in distortion centre estimation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcfeely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Denny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Glavin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="538" to="551" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b241">
	<analytic>
		<title level="a" type="main">Omnidirectional camera calibration and 3D reconstruction by contour matching</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Symposium on Visual Computing</title>
		<meeting>the Second International Symposium on Visual Computing<address><addrLine>Lake Tahoe, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="881" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b242">
	<monogr>
		<title level="m" type="main">A framework for 3D pushbroom imaging</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ichimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<idno>CUCS-002-03</idno>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Columbia University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b243">
	<analytic>
		<title level="a" type="main">A stereo viewer based on a single camera with view-control mechanisms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Inaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Inoue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<meeting>the IEEE/RSJ International Conference on Intelligent Robots and Systems<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="1857" to="1864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b244">
	<analytic>
		<title level="a" type="main">Calibrating view angle and lens distortion of the Nikon fish-eye converter FC-E8</title>
		<author>
			<persName><forename type="first">A</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mizoue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal for Forestry Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="177" to="181" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b245">
	<analytic>
		<title level="a" type="main">Omni-directional stereo for making global map</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ishiguro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd IEEE International Conference on Computer Vision</title>
		<meeting>the 3rd IEEE International Conference on Computer Vision<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="540" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b246">
	<analytic>
		<title level="a" type="main">Omni-directional stereo</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ishiguro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="262" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b247">
	<analytic>
		<title level="a" type="main">Parallax stereogram and process of making same</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">U.S. Patent</title>
		<imprint>
			<biblScope unit="volume">725</biblScope>
			<biblScope unit="page">567</biblScope>
			<date type="published" when="1903">1903</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b248">
	<analytic>
		<title level="a" type="main">A camera for making parallax panoramagrams</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="435" to="437" />
			<date type="published" when="1928">1928</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b249">
	<analytic>
		<title level="a" type="main">Panoramic motion picture camera arrangement</title>
		<author>
			<persName><forename type="first">U</forename><surname>Iwerks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">U.S. Patent</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">340</biblScope>
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b250">
	<analytic>
		<title level="a" type="main">A new development in camera calibration -calibrating a pair of mobile cameras</title>
		<author>
			<persName><forename type="first">A</forename><surname>Izaguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation<address><addrLine>Saint Louis, Michigan, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="74" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b251">
	<analytic>
		<title level="a" type="main">Correcting the geometry and color of digital images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jackowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goshtasby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1152" to="1158" />
			<date type="published" when="1997-10">October 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b252">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">B</forename><surname>Jähne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Digitale</forename><surname>Bildverarbeitung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
	<note>1st Edition</note>
</biblStruct>

<biblStruct xml:id="b253">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Jähne</surname></persName>
		</author>
		<title level="m">Digital Image Processing: Concepts, Algorithms, and Scientific Applications</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
	<note>1st Edition</note>
</biblStruct>

<biblStruct xml:id="b254">
	<analytic>
		<title level="a" type="main">Single camera catadioptric stereo system</title>
		<author>
			<persName><forename type="first">G</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras</title>
		<meeting>the 6th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b255">
	<analytic>
		<title level="a" type="main">Analytic image unwarping by a systematic calibration method for omni-directional cameras with hyperbolic-shaped mirrors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="690" to="701" />
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b256">
	<analytic>
		<title level="a" type="main">Single axis geometry by fitting conics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th European Conference on Computer Vision</title>
		<meeting>the 7th European Conference on Computer Vision<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="537" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b257">
	<analytic>
		<title level="a" type="main">Concave surround optics for rapid multi-view imaging</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bolas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mcdowall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Army Science Conference</title>
		<meeting>the 25th Army Science Conference<address><addrLine>Orlando, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b258">
	<analytic>
		<title level="a" type="main">Pose estimation with radial distortion and unknown focal length</title>
		<author>
			<persName><forename type="first">K</forename><surname>Josephson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Byröd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Miami, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b259">
	<analytic>
		<title level="a" type="main">Practical global optimization for multiview geometry</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="271" to="284" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b260">
	<analytic>
		<title level="a" type="main">Calculation of polyhedral objects using direct and mirror images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Honda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Japan Society for Precision Engineering</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="149" to="155" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b261">
	<analytic>
		<title level="a" type="main">Catadioptric self-calibration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Hilton Head Island, South Carolina, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="201" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b262">
	<analytic>
		<title level="a" type="main">Radial distortion snakes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Transactions on Information and Systems</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1603" to="1611" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b263">
	<analytic>
		<title level="a" type="main">3-D scene data recovery using omnidirectional multibaseline stereo</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="183" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b264">
	<analytic>
		<title level="a" type="main">Epipolar geometry for the rectification of cubic panoramas</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kangni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Laganière</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Canadian Conference on Computer and Robot Vision</title>
		<meeting>the 3rd Canadian Conference on Computer and Robot Vision<address><addrLine>Québec City, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b265">
	<analytic>
		<title level="a" type="main">A generic camera model and calibration method for conventional, wide-angle, and fish-eye lenses</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1335" to="1340" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b266">
	<analytic>
		<title level="a" type="main">Self-calibration of central cameras by minimizing angular error</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heikkilä</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision Theory and Applications</title>
		<meeting>the International Conference on Computer Vision Theory and Applications<address><addrLine>Funchal, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b267">
	<analytic>
		<title level="a" type="main">3D object localization via stereo vision using an omnidirectional and a perspective camera</title>
		<author>
			<persName><forename type="first">U.-P</forename><surname>Käppeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Höferlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Levi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Omnidirectional Robot Vision</title>
		<meeting>the 2nd Workshop on Omnidirectional Robot Vision<address><addrLine>Anchorage, Alaska</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b268">
	<analytic>
		<title level="a" type="main">Camera calibration by the multicollimator method</title>
		<author>
			<persName><forename type="first">R</forename><surname>Karren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogrammetric Engineering</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="706" to="719" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b269">
	<analytic>
		<title level="a" type="main">Structure from image sequences captured through a monocular extra-wide angle lens</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakanishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="919" to="924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b270">
	<analytic>
		<title level="a" type="main">Generation of high-resolution stereo panoramic images by omnidirectional imaging sensor using hexagonal pyramidal mirrors</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kawanishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yamazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Iwasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Takemura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yokoya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Pattern Recognition</title>
		<meeting>the 14th International Conference on Pattern Recognition<address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="485" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b271">
	<analytic>
		<title level="a" type="main">Precise method of fisheye lens calibration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kedzierski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fryskowska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ISPRS-Congress</title>
		<meeting>the ISPRS-Congress<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="765" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b272">
	<analytic>
		<title level="a" type="main">Where was the picture taken: Image localization in route panoramas using epipolar geometry</title>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Multimedia and Expo</title>
		<meeting>the IEEE International Conference on Multimedia and Expo<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="249" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b273">
	<analytic>
		<title level="a" type="main">Compensation of systematic errors of image and model coordinates</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kilpelä</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="407" to="427" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b274">
	<analytic>
		<title level="a" type="main">Compensation of systematic of image and model coordinates</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kilpelä</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogrammetria</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="44" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b275">
	<analytic>
		<title level="a" type="main">Compensation of systematic errors in bundle adjustment</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kilpelä</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heikkilä</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Inkilä</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogrammetria</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b276">
	<analytic>
		<title level="a" type="main">Motion estimation for nonoverlapping multicamera rigs: Linear algebraic and l ∞ geometric solutions</title>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1044" to="1059" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b277">
	<analytic>
		<title level="a" type="main">Spherical approximation for multiple cameras in motion estimation: Its applicability and advantages</title>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hwangbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1068" to="1083" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b278">
	<analytic>
		<title level="a" type="main">Degeneracy of the linear seventeen-point algorithm for generalized essential matrix</title>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="48" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b279">
	<analytic>
		<title level="a" type="main">Learning-based constitutive parameters estimation in an image sensing system with multiple mirrors</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1199" to="1217" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b280">
	<analytic>
		<title level="a" type="main">On design and applications of cylindrical panoramas</title>
		<author>
			<persName><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gimel'farb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scheibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Scheele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Börner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Reulke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Computer Analysis of Images and Patterns</title>
		<meeting>the 10th International Conference on Computer Analysis of Images and Patterns<address><addrLine>Groningen, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b281">
	<analytic>
		<title level="a" type="main">Calibration and performance evaluation of omnidirectional sensor with compound spherical mirrors</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Echigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras</title>
		<meeting>the 6th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b282">
	<analytic>
		<title level="a" type="main">Analytische Verzeichnungsdarstellung bei der vollständigen Kalibrierung</title>
		<author>
			<persName><forename type="first">O</forename><surname>Kölbl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bildmessung und Luftbildwesen</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="169" to="176" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b283">
	<analytic>
		<title level="a" type="main">Evaluation of HBP mirror system for remote surveillance</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kondo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mukaigawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<meeting>the IEEE/RSJ International Conference on Intelligent Robots and Systems<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="3454" to="3461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b284">
	<analytic>
		<title level="a" type="main">Free-form mirror design inspired by photometric stereo</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kondo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mukaigawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras</title>
		<meeting>the 8th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b285">
	<analytic>
		<title level="a" type="main">Non-isotropic omnidirectional imaging system for an autonomous mobile robot</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kondo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yachida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1228" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b286">
	<monogr>
		<title level="m" type="main">Photogrammetry: Geometry from Images and Laser Scans</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kraus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Walter de Gruyter</publisher>
			<pubPlace>Berlin, 2nd Edition</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b287">
	<analytic>
		<title level="a" type="main">Panoramic image acquisition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="379" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b288">
	<analytic>
		<title level="a" type="main">Cata-fisheye camera for panoramic imaging</title>
		<author>
			<persName><forename type="first">G</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Workshop on Applications of Computer Vision</title>
		<meeting>the IEEE Workshop on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b289">
	<analytic>
		<title level="a" type="main">Towards a true spherical camera</title>
		<author>
			<persName><forename type="first">G</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SPIE -Human Vision and Electronic Imaging XIV</title>
		<meeting>the SPIE -Human Vision and Electronic Imaging XIV</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b290">
	<analytic>
		<title level="a" type="main">Fast and robust numerical solutions to minimal problems for cameras with radial distortion</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kúkelová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Byröd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Josephson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Åström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="234" to="244" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b291">
	<analytic>
		<title level="a" type="main">A minimal solution to the autocalibration of radial distortion</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kúkelová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Minneapolis, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b292">
	<analytic>
		<title level="a" type="main">Two minimal problems for cameras with radial distortion</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kúkelová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras</title>
		<meeting>the 7th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras<address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b293">
	<analytic>
		<title level="a" type="main">Fish-eye lens designs and their relative performance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kumler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SPIE Conference on Current Developments in Lens Design and Optical Systems Engineering</title>
		<meeting>the SPIE Conference on Current Developments in Lens Design and Optical Systems Engineering<address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="360" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b294">
	<analytic>
		<title level="a" type="main">Multiview radial catadioptric imaging for scene capture</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kuthirummal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="916" to="923" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b295">
	<analytic>
		<title level="a" type="main">Flexible mirror imaging</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kuthirummal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras</title>
		<meeting>the 7th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras<address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b296">
	<analytic>
		<title level="a" type="main">Wideangle catadioptric lens with a rectilinear projection scheme</title>
		<author>
			<persName><forename type="first">G.-I</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-B</forename><surname>Seung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Optics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">34</biblScope>
			<biblScope unit="page" from="8659" to="8673" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b297">
	<analytic>
		<title level="a" type="main">Spherical catadioptric arrays: Construction, multi-view geometry, and calibration</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lanman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Crispell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wachs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Taubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Symposium on 3D Data Processing, Visualization, and Transmission</title>
		<meeting>the Third International Symposium on 3D Data Processing, Visualization, and Transmission<address><addrLine>Chapel Hill, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b298">
	<analytic>
		<title level="a" type="main">Reconstructing a 3D line from a single catadioptric image</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lanman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wachs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Taubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cukierman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Symposium on 3D Data Processing, Visualization, and Transmission</title>
		<meeting>the Third International Symposium on 3D Data Processing, Visualization, and Transmission<address><addrLine>Chapel Hill, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b299">
	<analytic>
		<title level="a" type="main">Implicit reconstruction by zooming</title>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Lavest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Delherm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peuchot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Daucher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="301" to="315" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b300">
	<analytic>
		<title level="a" type="main">Reconstruction by zooming from implicit calibration</title>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Lavest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peuchot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Delherm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dhome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st IEEE International Conference on Image Processing</title>
		<meeting>the 1st IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="1012" to="1016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b301">
	<analytic>
		<title level="a" type="main">Do we really need an accurate calibration pattern to achieve a reliable camera calibration?</title>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Lavest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Viala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dhome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th European Conference on Computer Vision</title>
		<meeting>the 5th European Conference on Computer Vision<address><addrLine>Freiburg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-05">May 1998</date>
			<biblScope unit="page" from="158" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b302">
	<analytic>
		<title level="a" type="main">A biprism-stereo camera system</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Fort Collins, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="82" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b303">
	<analytic>
		<title level="a" type="main">Large motion estimation for omnidirectional vision</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Workshop on Omnidirectional Vision</title>
		<meeting>the IEEE Workshop on Omnidirectional Vision<address><addrLine>Hilton Head Island, South Carolina</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b304">
	<analytic>
		<title level="a" type="main">Techniques for calibration of the scale factor and image center for high accuracy 3D machine vision metrology</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation<address><addrLine>Raleigh, North Carolina, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="68" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b305">
	<analytic>
		<title level="a" type="main">Techniques for calibration of the scale factor and image center for high accuracy 3D machine vision metrology</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="713" to="720" />
			<date type="published" when="1988-09">September 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b306">
	<analytic>
		<title level="a" type="main">Light field rendering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGGRAPH</title>
		<meeting>the SIGGRAPH<address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="31" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b307">
	<analytic>
		<title level="a" type="main">Automatic structure and motion using a catadioptric camera</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lhuillier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras</title>
		<meeting>the 6th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b308">
	<analytic>
		<title level="a" type="main">Effective and generic structure from motion using angular error</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lhuillier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Pattern Recognition</title>
		<meeting>the 18th International Conference on Pattern Recognition<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="67" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b309">
	<analytic>
		<title level="a" type="main">Automatic scene structure and camera motion using a catadioptric system</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lhuillier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="186" to="203" />
			<date type="published" when="2008-02">February 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b310">
	<analytic>
		<title level="a" type="main">A non-iterative method for correcting lens distortion from nine point correspondences</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras</title>
		<meeting>the 6th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b311">
	<analytic>
		<title level="a" type="main">Plane-based calibration and auto-calibration of a fisheye camera</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b312">
	<analytic>
		<title level="a" type="main">Some aspects of zoom lens camera calibration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Lavest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1105" to="1110" />
			<date type="published" when="1996-11">November 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b313">
	<analytic>
		<title level="a" type="main">Stereo reconstruction from multiperspective panoramas</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="62" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b314">
	<analytic>
		<title level="a" type="main">CCD camera calibration using the finite element method</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lichti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chapman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SPIE Conference on Videometrics IV</title>
		<editor>
			<persName><forename type="first">S</forename><surname>El-Hakim</surname></persName>
		</editor>
		<meeting>the SPIE Conference on Videometrics IV<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>SPIE -Society of Photo-Optical Instrumentation Engineers</publisher>
			<date type="published" when="1995-10">October 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b315">
	<analytic>
		<title level="a" type="main">Constrained FEM self-calibration</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lichti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chapman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogrammetric Engineering &amp; Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="1111" to="1119" />
			<date type="published" when="1997-09">September 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b316">
	<analytic>
		<title level="a" type="main">Estimation of the epipole using optical flow at antipodal points</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="245" to="253" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b317">
	<analytic>
		<title level="a" type="main">Estimating relative camera motion from the antipodal-epipolar constraint</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1907" to="1914" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b318">
	<analytic>
		<title level="a" type="main">Single-view-point omnidirectional catadioptric cone mirror imager</title>
		<author>
			<persName><forename type="first">S.-S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajcsy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="840" to="845" />
			<date type="published" when="2006-05">May 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b319">
	<analytic>
		<title level="a" type="main">Single-viewpoint, catadioptric cone mirror omnidirectional imaging theory and analysis</title>
		<author>
			<persName><forename type="first">S.-S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajcsy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America A</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2997" to="3015" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b320">
	<analytic>
		<title level="a" type="main">High resolution catadioptric omni-directional stereo sensor for robot vision</title>
		<author>
			<persName><forename type="first">S.-S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajczy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1694" to="1699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b321">
	<analytic>
		<title level="a" type="main">Movie-maps: An application of the optical videodisc to computer graphics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lippman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="32" to="42" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b322">
	<analytic>
		<title level="a" type="main">Épreuves réversibles donnant la sensation du relief</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lippmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal de Physique</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="821" to="825" />
			<date type="published" when="1908">1908</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b323">
	<analytic>
		<title level="a" type="main">A computer program for reconstructing a scene from two projections</title>
		<author>
			<persName><forename type="first">H</forename><surname>Longuet</surname></persName>
		</author>
		<author>
			<persName><surname>Higgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">293</biblScope>
			<biblScope unit="page" from="133" to="135" />
			<date type="published" when="1981-09">September 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b324">
	<analytic>
		<title level="a" type="main">Computing rectifying homographies for stereo vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Loop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Fort Collins, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="125" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b325">
	<analytic>
		<title level="a" type="main">A historical review on panoramic photogrammetry</title>
		<author>
			<persName><forename type="first">T</forename><surname>Luhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ISPRS Workshop on Panorama Photogrammetry</title>
		<meeting>the ISPRS Workshop on Panorama Photogrammetry<address><addrLine>Dresden, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b326">
	<analytic>
		<title level="a" type="main">Rational radial distortion models of camera lenses with analytical solution for distortion correction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Information Acquisition</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="147" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b327">
	<monogr>
		<title level="m" type="main">An Invitation to 3-D Vision -From Images to Geometric Models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b328">
	<analytic>
		<title level="a" type="main">Immersive teleconferencing: A new algorithm to generate seamless panoramic video imagery</title>
		<author>
			<persName><forename type="first">A</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Seales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gopi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh ACM international conference on Multimedia</title>
		<meeting>the seventh ACM international conference on Multimedia<address><addrLine>Orlando, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b329">
	<analytic>
		<title level="a" type="main">Precise radial un-distortion of images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mallon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Whelan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Pattern Recognition</title>
		<meeting>the 15th International Conference on Pattern Recognition<address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="18" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b330">
	<analytic>
		<title level="a" type="main">Camera models based on data from two calibration planes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Birk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kelley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics and Image Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="173" to="180" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b331">
	<analytic>
		<title level="a" type="main">Calibration method for misaligned catadioptric camera</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Iwai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yachida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras</title>
		<meeting>the 6th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b332">
	<analytic>
		<title level="a" type="main">Système de miroirs pour la stéréoscopie</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Devernay</surname></persName>
		</author>
		<idno>0172</idno>
	</analytic>
	<monogr>
		<title level="j">INRIA</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report Rapport Technique</note>
</biblStruct>

<biblStruct xml:id="b333">
	<analytic>
		<title level="a" type="main">Mars rover autonomous navigation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Maurette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="199" to="208" />
			<date type="published" when="2003-03">March 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b334">
	<monogr>
		<title level="m" type="main">A timeline of panoramic cameras</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcbride</surname></persName>
		</author>
		<ptr target="http://www.panoramicphoto.com/timeline.htm" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b335">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Mcglone</surname></persName>
		</author>
		<title level="m">Manual of Photogrammetry</title>
		<meeting><address><addrLine>Falls Church, Virginia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>American Society of Photogrammetry and Remote Sensing</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>5th Edition</note>
</biblStruct>

<biblStruct xml:id="b336">
	<analytic>
		<title level="a" type="main">Image warping for calibration and removal of lens distortion</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mclean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Pacific Rim Conference on Communications, Computers and Signal Processing</title>
		<meeting>the IEEE Pacific Rim Conference on Communications, Computers and Signal Processing<address><addrLine>Victoria, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="170" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b337">
	<analytic>
		<title level="a" type="main">Plenoptic modeling: An image-based rendering system</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mcmillan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGGRAPH</title>
		<meeting>the SIGGRAPH<address><addrLine>Los Angeles, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b338">
	<analytic>
		<title level="a" type="main">Single view point omnidirectional camera calibration from planar grids</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation<address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-04">April 2007</date>
			<biblScope unit="page" from="3945" to="3950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b339">
	<analytic>
		<title level="a" type="main">Geometric models of rolling-shutter cameras</title>
		<author>
			<persName><forename type="first">M</forename><surname>Meingast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras</title>
		<meeting>the 6th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b340">
	<monogr>
		<title level="m" type="main">Omnidirectional Vision for Mobile Robots</title>
		<author>
			<persName><forename type="first">E</forename><surname>Menegatti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002-12">December 2002</date>
			<publisher>Università di Padova</publisher>
			<pubPlace>Italy</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b341">
	<analytic>
		<title level="a" type="main">Constraints on perspective images and circular panoramas</title>
		<author>
			<persName><forename type="first">M</forename><surname>Menem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th British Machine Vision Conference</title>
		<meeting>the 15th British Machine Vision Conference<address><addrLine>Kingston upon Thames, England</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b342">
	<analytic>
		<title level="a" type="main">3-D reconstruction using mirror images based on a plane symmetry recovering method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mitsumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kajimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fukui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="941" to="946" />
			<date type="published" when="1992-09">September 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b343">
	<monogr>
		<title level="m" type="main">Two-view geometry of omnidirectional cameras</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mičušik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004-06">June 2004</date>
			<pubPlace>Prague</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Faculty of Electrical Engineering, Czech Technical University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b344">
	<analytic>
		<title level="a" type="main">3D metric reconstruction from uncalibrated omnidirectional images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mičušik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Martinec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b345">
	<analytic>
		<title level="a" type="main">Estimation of omnidirectional camera model from epipolar geometry</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mičušik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Madison, Wisconsin, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b346">
	<analytic>
		<title level="a" type="main">Autocalibration and 3D reconstruction with noncentral catadioptric cameras</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mičušik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b347">
	<analytic>
		<title level="a" type="main">Para-catadioptric camera auto-calibration from epipolar geometry</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mičušik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b348">
	<analytic>
		<title level="a" type="main">Structure from motion with wide circular field of view cameras</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mičušik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b349">
	<analytic>
		<title level="a" type="main">Fish eye lens</title>
		<author>
			<persName><forename type="first">K</forename><surname>Miyamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1060" to="1061" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b350">
	<monogr>
		<title level="m" type="main">Le cylindrographe, appareil panoramique. Gauthier-Villars et fils</title>
		<author>
			<persName><forename type="first">P</forename><surname>Moëssard</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">1889</biblScope>
			<pubPlace>Paris</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b351">
	<analytic>
		<title level="a" type="main">Catadioptric camera calibration by polarization imaging</title>
		<author>
			<persName><forename type="first">O</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Seulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fofi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Iberian Conference on Pattern Recognition and Image Analysis</title>
		<meeting>the Iberian Conference on Pattern Recognition and Image Analysis<address><addrLine>Girona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="396" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b352">
	<analytic>
		<title level="a" type="main">Measurement in three dimensions by motion stereo and spherical mapping</title>
		<author>
			<persName><forename type="first">T</forename><surname>Morita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yasukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Inamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Takashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kawakami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="422" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b353">
	<analytic>
		<title level="a" type="main">Stereovision with a single camera and multiple mirrors</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mouaddib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Echigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="800" to="805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b354">
	<analytic>
		<title level="a" type="main">Two or more mirrors for the omnidirectional stereovision?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mouaddib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Echigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd IEEE-EURASIP International Symposium on Control, Communications, and Signal Processing</title>
		<meeting>the 2nd IEEE-EURASIP International Symposium on Control, Communications, and Signal Processing<address><addrLine>Marrakech, Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b355">
	<analytic>
		<title level="a" type="main">Real-time localization and 3D reconstruction</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mouragnon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lhuillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dhome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dekeyser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sayd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-06">June 2006</date>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b356">
	<analytic>
		<title level="a" type="main">Generic and real-time structure from motion</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mouragnon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lhuillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dhome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dekeyser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sayd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th British Machine Vision Conference</title>
		<meeting>the 18th British Machine Vision Conference<address><addrLine>Warwick, England</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b357">
	<analytic>
		<title level="a" type="main">Generic and real-time structure from motion using local bundle adjustment</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mouragnon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lhuillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dhome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dekeyser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sayd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1178" to="1193" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b358">
	<analytic>
		<title level="a" type="main">Calibrating non-metric cameras using the finite-element method</title>
		<author>
			<persName><forename type="first">R</forename><surname>Munjy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogrammetric Engineering &amp; Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1201" to="1205" />
			<date type="published" when="1986-08">August 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b359">
	<analytic>
		<title level="a" type="main">Self-calibration using the finite element approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Munjy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogrammetric Engineering &amp; Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="411" to="418" />
			<date type="published" when="1986-03">March 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b360">
	<analytic>
		<title level="a" type="main">Application of panospheric imaging to a teleoperated lunar rover</title>
		<author>
			<persName><forename type="first">J</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Systems, Man and Cybernetics</title>
		<meeting>the IEEE International Conference on Systems, Man and Cybernetics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="3117" to="3121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b361">
	<analytic>
		<title level="a" type="main">Recovering range using virtual multi-camera stereo</title>
		<author>
			<persName><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="285" to="291" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b362">
	<analytic>
		<title level="a" type="main">Super wide field of view head mounted display using catadioptrical optics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nagahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yachida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Presence</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="588" to="598" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b363">
	<analytic>
		<title level="a" type="main">An omnidirectional vision sensor with single view and constant resolution</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nagahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yachida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th IEEE International Conference on Computer Vision</title>
		<meeting>the 11th IEEE International Conference on Computer Vision<address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b364">
	<monogr>
		<title level="m" type="main">Bell Laboratories Technical Memorandum</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nalwa</surname></persName>
		</author>
		<idno>BL0115500-960115-01</idno>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>AT&amp;T Bell Laboratories</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>A true omnidirectional viewer</note>
</biblStruct>

<biblStruct xml:id="b365">
	<analytic>
		<title level="a" type="main">Sphereo: Determining depth using two specular spheres and a single camera</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SPIE Conference on Optics, Illumination, and Image Sensing for Machine Vision III</title>
		<meeting>the SPIE Conference on Optics, Illumination, and Image Sensing for Machine Vision III<address><addrLine>Cambridge, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="245" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b366">
	<analytic>
		<title level="a" type="main">Catadioptric omnidirectional camera</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Puerto Rico, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="482" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b367">
	<analytic>
		<title level="a" type="main">Omnidirectional vision</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eight International Symposium on Robotics Research</title>
		<meeting>the Eight International Symposium on Robotics Research<address><addrLine>Shonan, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-10">October 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b368">
	<analytic>
		<title level="a" type="main">Computational cameras: Redefining the image</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="38" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b369">
	<analytic>
		<title level="a" type="main">Programmable imaging: Towards a flexible camera</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Branzoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="22" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b370">
	<analytic>
		<title level="a" type="main">360 × 360 mosaics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karmarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Hilton Head Island, South Carolina, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="380" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b371">
	<analytic>
		<title level="a" type="main">Folded catadioptric cameras</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Peri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Fort Collins, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="217" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b372">
	<analytic>
		<title level="a" type="main">Finding motion parameters from spherical motion fields (or the advantages of having eyes in the back of your head)</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="261" to="273" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b373">
	<analytic>
		<title level="a" type="main">Stereo with mirrors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th IEEE International Conference on Computer Vision</title>
		<meeting>the 6th IEEE International Conference on Computer Vision<address><addrLine>Bombay, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-01">January 1998</date>
			<biblScope unit="page" from="1087" to="1094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b374">
	<analytic>
		<title level="a" type="main">Polydioptric camera design and 3D motion estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fermüller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Madison, Wisconsin, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="294" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b375">
	<analytic>
		<title level="a" type="main">A feature-based stereo model using small disparities</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nishimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shirai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Workshop on Industrial Applications of Machine Vision and Machine Intelligence</title>
		<meeting>the IEEE International Workshop on Industrial Applications of Machine Vision and Machine Intelligence<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="192" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b376">
	<analytic>
		<title level="a" type="main">An efficient solution to the five-point relative pose problem</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nistér</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="756" to="770" />
			<date type="published" when="2004-06">June 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b377">
	<analytic>
		<title level="a" type="main">A minimal solution to the generalized 3-point pose problem</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nistér</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="560" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b378">
	<analytic>
		<title level="a" type="main">A minimal solution to the generalised 3-point pose problem</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nistér</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stewénius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="79" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b379">
	<analytic>
		<title level="a" type="main">Non-parametric selfcalibration</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nistér</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stewénius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grossmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th IEEE International Conference on Computer Vision</title>
		<meeting>the 10th IEEE International Conference on Computer Vision<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-10">October 2005</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b380">
	<analytic>
		<title level="a" type="main">Simple calibration algorithm for high-distortion-lens camera</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nomura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sagara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Naruse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1095" to="1099" />
			<date type="published" when="1992-11">November 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b381">
	<analytic>
		<title level="a" type="main">Guidance of a mobile robot using an omnidirectional vision navigation system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of spie, Mobile Robots II</title>
		<meeting>spie, Mobile Robots II<address><addrLine>Cambridge, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="288" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b382">
	<analytic>
		<title level="a" type="main">Calibration of an omnidirectional vision navigation system using an industrial robot</title>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="955" to="962" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b383">
	<analytic>
		<title level="a" type="main">On photometric aspects of catadioptric cameras</title>
		<author>
			<persName><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Deguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Kauai, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="1106" to="1113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b384">
	<analytic>
		<title level="a" type="main">Exact two-image structure from motion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Oliensis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1618" to="1633" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b385">
	<monogr>
		<title level="m" type="main">Analysis and design of panoramic stereo vision using equi-angular pixel cameras</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ollis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Herman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<idno>CMU-RI-TR-99- 04</idno>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b386">
	<analytic>
		<title level="a" type="main">Universal camera calibration with automatic distortion model selection</title>
		<author>
			<persName><forename type="first">V</forename><surname>Orekhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Abidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Broaddus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing<address><addrLine>San Antonio, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="397" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b387">
	<analytic>
		<title level="a" type="main">Calibration of a structured lightbased stereo catadioptric sensor</title>
		<author>
			<persName><forename type="first">R</forename><surname>Orghidan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mouaddib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Omnidirectional Vision and Camera Networks</title>
		<meeting>the Workshop on Omnidirectional Vision and Camera Networks<address><addrLine>Madison, Wisconsin, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b388">
	<monogr>
		<title level="m" type="main">Geometry of two-slit camera</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<idno>CTU-CMP-2002- 02</idno>
		<imprint>
			<date type="published" when="2002-03">March 2002</date>
			<pubPlace>Prague</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Center for Machine Perception, Czech Technical University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b389">
	<analytic>
		<title level="a" type="main">Stereo with oblique cameras</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="161" to="170" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b390">
	<analytic>
		<title level="a" type="main">Epipolar geometry of central panoramic cameras</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hlavac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Panoramic Vision: Sensors, Theory, and Applications</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Benosman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Kang</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="85" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b391">
	<analytic>
		<title level="a" type="main">CMOS foveated image sensor: Signal scaling and small geometry effects</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dierickx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scheffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Electron Devices</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1731" to="1737" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b392">
	<analytic>
		<title level="a" type="main">A mobile robot using a panoramic view</title>
		<author>
			<persName><forename type="first">C</forename><surname>Pégard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mouaddib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="1996-04">April 1996</date>
			<biblScope unit="page" from="89" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b393">
	<analytic>
		<title level="a" type="main">Stereo panorama with a single camera</title>
		<author>
			<persName><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ben-Ezra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Fort Collins, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1395" to="1401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b394">
	<analytic>
		<title level="a" type="main">Omnistereo: Panoramic stereo imaging</title>
		<author>
			<persName><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ben-Ezra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="279" to="290" />
			<date type="published" when="2001-03">March 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b395">
	<analytic>
		<title level="a" type="main">Cameras for stereo panoramic imaging</title>
		<author>
			<persName><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ben-Ezra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Hilton Head Island, South Carolina, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="208" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b396">
	<analytic>
		<title level="a" type="main">Mosaicing on adaptive manifolds</title>
		<author>
			<persName><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rousso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zomet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1144" to="1154" />
			<date type="published" when="2000-10">October 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b397">
	<analytic>
		<title level="a" type="main">Camera calibration: A quick and easy way to determine the scale factor</title>
		<author>
			<persName><forename type="first">M</forename><surname>Penna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1240" to="1245" />
			<date type="published" when="1991-12">December 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b398">
	<analytic>
		<title level="a" type="main">The inversion camera model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Perwass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th DAGM Symposium</title>
		<meeting>the 28th DAGM Symposium<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="647" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b399">
	<analytic>
		<title level="a" type="main">3-D vision systems using rotating 1-D sensors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Petty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Godber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEE Colloquium on Application of Machine Vision</title>
		<meeting>the IEE Colloquium on Application of Machine Vision<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="6" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b400">
	<analytic>
		<title level="a" type="main">CCD camera calibration virtual equivalent model</title>
		<author>
			<persName><forename type="first">B</forename><surname>Peuchot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saint-André</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th Annual International Conference IEEE EMBS</title>
		<meeting><address><addrLine>Paris</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-10">October 1992</date>
			<biblScope unit="page" from="1960" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b401">
	<analytic>
		<title level="a" type="main">Using many cameras as one</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Madison, Wisconsin, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-06">June 2003</date>
			<biblScope unit="page" from="587" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b402">
	<analytic>
		<title level="a" type="main">Une chambre photographique quadruple de 20 milllimètres de focale</title>
		<author>
			<persName><forename type="first">G</forename><surname>Poivilliers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IV ISPRS-Congress</title>
		<meeting>the IV ISPRS-Congress<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1934">1934</date>
			<biblScope unit="page" from="132" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b403">
	<analytic>
		<title level="a" type="main">A simple and efficient rectification method for general motion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th IEEE International Conference on Computer Vision</title>
		<meeting>the 7th IEEE International Conference on Computer Vision<address><addrLine>Kerkyra, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="496" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b404">
	<analytic>
		<title level="a" type="main">What is a camera?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Miami, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b405">
	<analytic>
		<title level="a" type="main">An efficient error-bounded general camera model</title>
		<author>
			<persName><forename type="first">V</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dauble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sacks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Symposium on 3D Data Processing, Visualization and Transmission</title>
		<meeting>the Third International Symposium on 3D Data Processing, Visualization and Transmission<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b406">
	<analytic>
		<title level="a" type="main">Panoramic lens</title>
		<author>
			<persName><forename type="first">I</forename><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Optics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">31</biblScope>
			<biblScope unit="page" from="7356" to="7361" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b407">
	<analytic>
		<title level="a" type="main">Line-based correction of radial lens distortion</title>
		<author>
			<persName><forename type="first">B</forename><surname>Prescott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mclean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graphical Models and Image Processing</title>
		<imprint>
			<date type="published" when="1997-01">January 1997</date>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="39" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b408">
	<analytic>
		<title level="a" type="main">Über die stereoskopische Betrachtung eines Gegenstandes und seines Spiegelbildes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Pulfrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zeitschrift für Instrumentenkunde</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="93" to="96" />
			<date type="published" when="1905">1905</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b409">
	<analytic>
		<title level="a" type="main">The nonparametric approach for camera calibration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th IEEE International Conference on Computer Vision</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Grimson</surname></persName>
		</editor>
		<meeting>the 5th IEEE International Conference on Computer Vision<address><addrLine>Cambridge, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1995-06">June 1995</date>
			<biblScope unit="page" from="224" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b410">
	<analytic>
		<title level="a" type="main">Multiple-center-of-projection images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rademacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGGRAPH</title>
		<meeting>the SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="199" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b411">
	<monogr>
		<title level="m" type="main">Conception d&apos;un capteur de stéréovision omnidirectionnelle: Architecture, étalonnage et applications à la reconstruction de scènes 3D</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ragot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<pubPlace>France</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Université de Rouen</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b412">
	<analytic>
		<title level="a" type="main">Calibration of a panoramic stereovision sensor: Analytical vs interpolation-based methods</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ragot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Ertaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Savatier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mazari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd Annual Conference of the IEEE Industrial Electronics Society</title>
		<meeting>the 32nd Annual Conference of the IEEE Industrial Electronics Society<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="4130" to="4135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b413">
	<analytic>
		<title level="a" type="main">A generic structure-from-motion algorithm for cross-camera scenarios</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras</title>
		<meeting>the 5th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-05">May 2004</date>
			<biblScope unit="page" from="175" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b414">
	<analytic>
		<title level="a" type="main">A generic structure-frommotion framework</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="218" to="228" />
			<date type="published" when="2006-09">September 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b415">
	<analytic>
		<title level="a" type="main">Minimal solutions for generic imaging models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Anchorage, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06">June 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b416">
	<analytic>
		<title level="a" type="main">A factorization based selfcalibration for radially symmetric cameras</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Symposium on 3D Data Processing, Visualization and Transmission</title>
		<meeting>the Third International Symposium on 3D Data Processing, Visualization and Transmission<address><addrLine>Chapel Hill, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-06">June 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b417">
	<analytic>
		<title level="a" type="main">Towards complete generic camera calibration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lodha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06">June 2005</date>
			<biblScope unit="page" from="1093" to="1098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b418">
	<analytic>
		<title level="a" type="main">Towards generic self-calibration of central cameras</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lodha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras</title>
		<meeting>the 6th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-10">October 2005</date>
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b419">
	<analytic>
		<title level="a" type="main">Theory and calibration algorithms for axial cameras</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lodha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-01">January 2006</date>
			<biblScope unit="page" from="704" to="713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b420">
	<analytic>
		<title level="a" type="main">Generic self-calibration of central cameras</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lodha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="219" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b421">
	<analytic>
		<title level="a" type="main">Mirror-based trinocular systems in robot-vision</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ramsgaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Balslev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Arnspang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Pattern Recognition</title>
		<meeting>the 15th International Conference on Pattern Recognition<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="499" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b422">
	<analytic>
		<title level="a" type="main">The nine lens air camera of the U.S. coast and geodetic survey</title>
		<author>
			<persName><forename type="first">O</forename><surname>Reading</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogrammetric Engineering</title>
		<imprint>
			<biblScope unit="volume">IV</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="184" to="192" />
			<date type="published" when="1938">1938</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b423">
	<monogr>
		<title level="m" type="main">Panoramic television viewing system, U.S. patent no. 3,505,465</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rees</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b424">
	<monogr>
		<title level="m" type="main">Panoramic imaging block for three-dimensional space</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rees</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="volume">566</biblScope>
			<biblScope unit="page">763</biblScope>
			<pubPlace>U.S.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b425">
	<analytic>
		<title level="a" type="main">Digital camera calibration methods: considerations and comparisons</title>
		<author>
			<persName><forename type="first">F</forename><surname>Remondino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fraser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPRS Commission V Symposium</title>
		<meeting><address><addrLine>Dresden, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="266" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b426">
	<analytic>
		<title level="a" type="main">Estimating the radial distortion of an optical system; effect on a localization process</title>
		<author>
			<persName><forename type="first">S</forename><surname>Remy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dhome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Daucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lapresté</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Pattern Recognition</title>
		<meeting>the 12th International Conference on Pattern Recognition<address><addrLine>Jerusalem, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="997" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b427">
	<analytic>
		<title level="a" type="main">Robust metric calibration of non-linear camera lens distortion</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ricolfe-Viala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-J</forename><surname>Sánchez-Salmerón</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1688" to="1699" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b428">
	<analytic>
		<title level="a" type="main">History of lenticular and related autostereoscopic methods, white paper</title>
		<author>
			<persName><forename type="first">D</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Leap Technologies</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b429">
	<analytic>
		<title level="a" type="main">Distortion, principal point, point of symmetry and calibrated principal point</title>
		<author>
			<persName><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogrammetria</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="49" to="66" />
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b430">
	<analytic>
		<title level="a" type="main">Cylindrical rectification to minimize epipolar distortion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Puerto Rico, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="393" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b431">
	<analytic>
		<title level="a" type="main">Compound catadioptric stereo sensor for omnidirectional object detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kurita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Echigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<meeting>the IEEE/RSJ International Conference on Intelligent Robots and Systems<address><addrLine>Sendai, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-09">September 2004</date>
			<biblScope unit="page" from="2612" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b432">
	<analytic>
		<title level="a" type="main">Calibration of lens distortion by structured-light scanning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Takatsuji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Echigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<meeting>the IEEE/RSJ International Conference on Intelligent Robots and Systems<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-08">August 2005</date>
			<biblScope unit="page" from="1349" to="1354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b433">
	<monogr>
		<title level="m" type="main">Visual navigation: Constructing and utilizing simple maps of an indoor environment</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sarachik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note>published as technical report AITR-1113 of the MIT AI Lab</note>
</biblStruct>

<biblStruct xml:id="b434">
	<analytic>
		<title level="a" type="main">True multi-image alignment and its application to mosaicing and lens distortion correction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="235" to="243" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b435">
	<analytic>
		<title level="a" type="main">A toolbox for easily calibrating omnidirectional cameras</title>
		<author>
			<persName><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Martinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<meeting>the IEEE/RSJ International Conference on Intelligent Robots and Systems<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="5695" to="5701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b436">
	<analytic>
		<title level="a" type="main">A practical toolbox for calibrating omnidirectional cameras</title>
		<author>
			<persName><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision Systems: Applications</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Obinata</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Dutta</surname></persName>
		</editor>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>I-Tech Education and Publishing</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="297" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b437">
	<analytic>
		<title level="a" type="main">Der Perspektograph und seine Anwendung</title>
		<author>
			<persName><forename type="first">T</forename><surname>Scheimpflug</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photographische Korrespondenz</title>
		<imprint>
			<date type="published" when="1906">1906</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b438">
	<analytic>
		<title level="a" type="main">Validation of geometric models for fisheye lenses</title>
		<author>
			<persName><forename type="first">D</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schwalbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-G</forename><surname>Maas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="259" to="266" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b439">
	<analytic>
		<title level="a" type="main">Advances in light microscope stereo vision</title>
		<author>
			<persName><forename type="first">H</forename><surname>Schreier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Experimental Mechanics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="278" to="288" />
			<date type="published" when="2004-06">June 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b440">
	<analytic>
		<title level="a" type="main">Computational anatomy and functional architecture of striate cortex: A spatial mapping approach to perceptual coding</title>
		<author>
			<persName><forename type="first">E</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="645" to="669" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b441">
	<analytic>
		<title level="a" type="main">Globally optimal O(n) solution to the PnP problem for general camera models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schweighofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th British Machine Vision Conference</title>
		<meeting>the 19th British Machine Vision Conference<address><addrLine>Leeds, England</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b442">
	<analytic>
		<title level="a" type="main">Online/realtime structure and motion for general camera models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schweighofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Šegvić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Workshop on Applications of Computer Vision</title>
		<meeting>the IEEE Workshop on Applications of Computer Vision<address><addrLine>Copper Mountain, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b443">
	<analytic>
		<title level="a" type="main">Omnivergent stereo</title>
		<author>
			<persName><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="159" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b444">
	<analytic>
		<title level="a" type="main">The space of all stereo images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="21" to="38" />
			<date type="published" when="2002-06">June 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b445">
	<analytic>
		<title level="a" type="main">Omnidirectional egomotion estimation from back-projection flow</title>
		<author>
			<persName><forename type="first">O</forename><surname>Shakernia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Omnidirectional Vision and Camera Networks</title>
		<meeting>the Workshop on Omnidirectional Vision and Camera Networks<address><addrLine>Madison, Wisconsin, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b446">
	<analytic>
		<title level="a" type="main">Structure from small baseline motion with central panoramic cameras</title>
		<author>
			<persName><forename type="first">O</forename><surname>Shakernia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Omnidirectional Vision and Camera Networks</title>
		<meeting>the Workshop on Omnidirectional Vision and Camera Networks<address><addrLine>Madison, Wisconsin, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b447">
	<analytic>
		<title level="a" type="main">Accurate linear technique for camera calibration considering lens distortion by solving an eigenvalue problem</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="138" to="149" />
			<date type="published" when="1993-01">January 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b448">
	<analytic>
		<title level="a" type="main">When should we consider lens distortion in camera calibration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="447" to="461" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b449">
	<monogr>
		<title level="m" type="main">Geometry of concentric multiperspective panoramas</title>
		<author>
			<persName><forename type="first">J</forename><surname>Šivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<idno>CTU-CMP-2002-05</idno>
		<imprint>
			<date type="published" when="2002">2002</date>
			<pubPlace>Prague</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Center for Machine Perception, Czech Technical University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b450">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Slama</surname></persName>
		</author>
		<title level="m">Manual of Photogrammetry</title>
		<meeting><address><addrLine>Falls Church, Virginia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>American Society of Photogrammetry and Remote Sensing</publisher>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
	<note>4th Edition</note>
</biblStruct>

<biblStruct xml:id="b451">
	<analytic>
		<title level="a" type="main">Cylindrical sensor calibration using lines</title>
		<author>
			<persName><forename type="first">L</forename><surname>Smadja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benosman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1851" to="1854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b452">
	<analytic>
		<title level="a" type="main">Efficient techniques for wide-angle stereo vision using surface projection models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Fort Collins, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="113" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b453">
	<analytic>
		<title level="a" type="main">Correction of distortion in endoscope images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vakil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maislin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="117" to="122" />
			<date type="published" when="1992-03">March 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b454">
	<analytic>
		<title level="a" type="main">Panoramic stereo</title>
		<author>
			<persName><forename type="first">D</forename><surname>Southwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fiala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Reyda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Pattern Recognition</title>
		<meeting>the 13th International Conference on Pattern Recognition<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="378" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b455">
	<analytic>
		<title level="a" type="main">Coaxial omnidirectional stereopsis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Spacek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th European Conference on Computer Vision</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>the 8th European Conference on Computer Vision<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="354" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b456">
	<analytic>
		<title level="a" type="main">New class of mirrors for wide-angle imaging</title>
		<author>
			<persName><forename type="first">M</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Omnidirectional Vision and Camera Networks</title>
		<meeting>the Workshop on Omnidirectional Vision and Camera Networks<address><addrLine>Madison, Wisconsin, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b457">
	<analytic>
		<title level="a" type="main">Overconstrained linear estimation of radial distortion and multi-view geometry</title>
		<author>
			<persName><forename type="first">R</forename><surname>Steele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jaynes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th European Conference on Computer Vision</title>
		<meeting>the 9th European Conference on Computer Vision<address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="253" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b458">
	<analytic>
		<title level="a" type="main">Camera calibration for fish-eye lenses in endoscopy with an application to 3D reconstruction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Stehle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Truhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Trautwein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tischendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE International Symposium on Biomedical Imaging</title>
		<meeting>IEEE International Symposium on Biomedical Imaging<address><addrLine>Washington, D.C.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1176" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b459">
	<analytic>
		<title level="a" type="main">Accurate internal camera calibration using rotation, with analysis of sources of error</title>
		<author>
			<persName><forename type="first">G</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th IEEE International Conference on Computer Vision</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Grimson</surname></persName>
		</editor>
		<meeting>the 5th IEEE International Conference on Computer Vision<address><addrLine>Cambridge, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1995-06">June 1995</date>
			<biblScope unit="page" from="230" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b460">
	<analytic>
		<title level="a" type="main">Lens distortion calibration using point correspondences</title>
		<author>
			<persName><forename type="first">G</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Puerto Rico, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="602" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b461">
	<analytic>
		<title level="a" type="main">Robot aerobics: Four easy steps to a more flexible calibration</title>
		<author>
			<persName><forename type="first">D</forename><surname>Stevenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fleck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th IEEE International Conference on Computer Vision</title>
		<meeting>the 5th IEEE International Conference on Computer Vision<address><addrLine>Cambridge, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="34" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b462">
	<analytic>
		<title level="a" type="main">Nonparametric correction of distortion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Stevenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fleck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Workshop on Applications of Computer Vision</title>
		<meeting>the IEEE Workshop on Applications of Computer Vision<address><addrLine>Sarasota, Florida</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="214" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b463">
	<analytic>
		<title level="a" type="main">Solutions to minimal generalized relative pose problems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Stewénius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nistér</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oskarsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Åström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras</title>
		<meeting>the 6th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b464">
	<analytic>
		<title level="a" type="main">How hard is three-view triangulation really?</title>
		<author>
			<persName><forename type="first">H</forename><surname>Stewénius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schaffalitzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nistér</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th IEEE International Conference on Computer Vision</title>
		<meeting>the 10th IEEE International Conference on Computer Vision<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="686" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b465">
	<analytic>
		<title level="a" type="main">Correcting radial distortion by circle fitting</title>
		<author>
			<persName><forename type="first">R</forename><surname>Strand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hayman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th British Machine Vision Conference</title>
		<meeting>the 16th British Machine Vision Conference<address><addrLine>Oxford, England</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b466">
	<analytic>
		<title level="a" type="main">Precise omnidirectional camera calibration</title>
		<author>
			<persName><forename type="first">D</forename><surname>Strelow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mishler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Kauai, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="689" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b467">
	<analytic>
		<title level="a" type="main">Reckless motion estimation from omnidirectional image and inertial measurements</title>
		<author>
			<persName><forename type="first">D</forename><surname>Strelow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Omnidirectional Vision and Camera Networks</title>
		<meeting>the Workshop on Omnidirectional Vision and Camera Networks<address><addrLine>Madison, Wisconsin, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b468">
	<analytic>
		<title level="a" type="main">Self-calibration of a moving zoom-lens camera by pre-calibration</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="583" to="589" />
			<date type="published" when="1997-08">August 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b469">
	<analytic>
		<title level="a" type="main">A method for 3D reconstruction of piecewise planar objects from single panoramic images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Workshop on Omnidirectional Vision</title>
		<meeting>the IEEE Workshop on Omnidirectional Vision<address><addrLine>Hilton Head Island; South Carolina</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000-06">June 2000</date>
			<biblScope unit="page" from="119" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b470">
	<analytic>
		<title level="a" type="main">Mixing catadioptric and perspective cameras</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Omnidirectional Vision</title>
		<meeting>the Workshop on Omnidirectional Vision<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-06">June 2002</date>
			<biblScope unit="page" from="37" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b471">
	<analytic>
		<title level="a" type="main">Multi-view geometry for general camera models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06">June 2005</date>
			<biblScope unit="page" from="206" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b472">
	<analytic>
		<title level="a" type="main">General imaging geometry for central catadioptric cameras</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barreto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th European Conference on Computer Vision</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</editor>
		<meeting>the 10th European Conference on Computer Vision<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-10">October 2008</date>
			<biblScope unit="page" from="609" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b473">
	<analytic>
		<title level="a" type="main">Focal length calibration from two views: Method and analysis of singular cases</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Poo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="58" to="95" />
			<date type="published" when="2005-07">July 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b474">
	<analytic>
		<title level="a" type="main">On plane-based camera calibration: A general algorithm, singularities, applications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Fort Collins, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-06">June 1999</date>
			<biblScope unit="page" from="432" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b475">
	<analytic>
		<title level="a" type="main">Affine stereo calibration</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
		<idno>LIFIA- 29</idno>
	</analytic>
	<monogr>
		<title level="j">LIFIA-IMAG</title>
		<imprint>
			<date type="published" when="1995-06">June 1995</date>
			<pubPlace>Grenoble, France</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b476">
	<analytic>
		<title level="a" type="main">A generic concept for camera calibration</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th European Conference on Computer Vision</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<meeting>the 8th European Conference on Computer Vision<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2004-05">May 2004</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b477">
	<analytic>
		<title level="a" type="main">On calibration, structure from motion and multi-view geometry for generic camera models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lodha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Imaging Beyond the Pinhole Camera</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Klette</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006-08">August 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b478">
	<monogr>
		<title level="m" type="main">Central Panoramic Cameras: Design, Geometry, Egomotion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Svoboda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999-09">September 1999</date>
			<pubPlace>Prague</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Faculty of Electrical Engineering, Czech Technical University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b479">
	<analytic>
		<title level="a" type="main">A convenient multicamera selfcalibration for virtual environments</title>
		<author>
			<persName><forename type="first">T</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Martinec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Presence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="407" to="422" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b480">
	<analytic>
		<title level="a" type="main">Epipolar geometry for central catadioptric cameras</title>
		<author>
			<persName><forename type="first">T</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="37" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b481">
	<analytic>
		<title level="a" type="main">Epipolar geometry for panoramic cameras</title>
		<author>
			<persName><forename type="first">T</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hlaváč</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th European Conference on Computer Vision</title>
		<meeting>the 5th European Conference on Computer Vision<address><addrLine>Freiburg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="218" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b482">
	<analytic>
		<title level="a" type="main">A perspective on distortions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grossberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Madison, Wisconsin, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-06">June 2003</date>
			<biblScope unit="page" from="594" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b483">
	<analytic>
		<title level="a" type="main">Non-single viewpoint catadioptric cameras: Geometry and analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grossberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="229" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b484">
	<analytic>
		<title level="a" type="main">Nonmetric calibration of wide-angle lenses and polycameras</title>
		<author>
			<persName><forename type="first">R</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1172" to="1178" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b485">
	<analytic>
		<title level="a" type="main">Designing mirrors for catadioptric systems that minimize image errors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grossberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras</title>
		<meeting>the 5th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b486">
	<analytic>
		<title level="a" type="main">Image alignment and stitching: A tutorial</title>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Computer Graphics and Vision</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="104" />
			<date type="published" when="2006-12">December 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b487">
	<analytic>
		<title level="a" type="main">Omnidirectional vision system using two mirrors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Takeya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kuroda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-I</forename><surname>Nishiguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ichikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SPIE, Novel Optical Systems and Large-Aperture Imaging</title>
		<meeting>the SPIE, Novel Optical Systems and Large-Aperture Imaging</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="50" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b488">
	<analytic>
		<title level="a" type="main">Self-calibration of a general radially symmetric distortion model</title>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Tardif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th European Conference on Computer Vision</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</editor>
		<meeting>the 9th European Conference on Computer Vision<address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-05">May 2006</date>
			<biblScope unit="page" from="186" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b489">
	<analytic>
		<title level="a" type="main">Plane-based self-calibration of radial distortion</title>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Tardif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th IEEE International Conference on Computer Vision</title>
		<meeting>the 11th IEEE International Conference on Computer Vision<address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b490">
	<analytic>
		<title level="a" type="main">Calibration of cameras with radially symmetric distortion</title>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Tardif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Trudeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1552" to="1566" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b491">
	<analytic>
		<title level="a" type="main">Analytical multicollimator camera calibration</title>
		<author>
			<persName><forename type="first">W</forename><surname>Tayman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogrammetria</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="179" to="197" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b492">
	<analytic>
		<title level="a" type="main">Toward urban model acquisition from geo-located images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Teller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Pacific Conference on Computer Graphics and Applications</title>
		<meeting>the Pacific Conference on Computer Graphics and Applications<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="45" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b493">
	<analytic>
		<title level="a" type="main">Determining the lines through four lines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Teller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hohmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Graphics Tools</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="11" to="22" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b494">
	<analytic>
		<title level="a" type="main">Salient video stills: Content and context preserved</title>
		<author>
			<persName><forename type="first">L</forename><surname>Teodosio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the first ACM International Conference on Multimedia</title>
		<meeting>the first ACM International Conference on Multimedia<address><addrLine>Anaheim, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b495">
	<analytic>
		<title level="a" type="main">Panoramic overviews for navigating real-world scenes</title>
		<author>
			<persName><forename type="first">L</forename><surname>Teodosio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the first ACM International Conference on Multimedia</title>
		<meeting>the first ACM International Conference on Multimedia<address><addrLine>Anaheim, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="359" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b496">
	<analytic>
		<title level="a" type="main">An inexpensive stereoscopic vision system for robots</title>
		<author>
			<persName><forename type="first">W</forename><surname>Teoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1984">1984</date>
			<biblScope unit="page" from="186" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b497">
	<analytic>
		<title level="a" type="main">Camera calibration by a single image of balls: From conics to the absolute conic</title>
		<author>
			<persName><forename type="first">H</forename><surname>Teramoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Asian Conference on Computer Vision</title>
		<meeting>the Fifth Asian Conference on Computer Vision<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="499" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b498">
	<analytic>
		<title level="a" type="main">Métrophotographie aérienne à l&apos;aide de mon autopanoramographe</title>
		<author>
			<persName><forename type="first">R</forename><surname>Thiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Archives of Photogrammetry</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="45" />
			<date type="published" when="1908">1908</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b499">
	<analytic>
		<title level="a" type="main">Multi-view geometry of 1D radial cameras and its application to omnidirectional camera calibration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thirthala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th IEEE International Conference on Computer Vision</title>
		<meeting>the 10th IEEE International Conference on Computer Vision<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-10">October 2005</date>
			<biblScope unit="page" from="1539" to="1546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b500">
	<analytic>
		<title level="a" type="main">The radial trifocal tensor: A tool for calibrating the radial distortion of wide-angle cameras</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thirthala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b501">
	<analytic>
		<title level="a" type="main">The seven lens air survey camera</title>
		<author>
			<persName><forename type="first">E</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogrammetric Engineering</title>
		<imprint>
			<biblScope unit="volume">IV</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="137" to="145" />
			<date type="published" when="1938">1938</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b502">
	<analytic>
		<title level="a" type="main">Robust line-based calibration of lens distortion from a single view</title>
		<author>
			<persName><forename type="first">T</forename><surname>Thormählen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Broszio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Wassermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the MIRAGE Conference on Computer Vision/Computer Graphics Collaboration for Model-based Imaging, Rendering, Image Analysis and Graphical Special Effects</title>
		<meeting>the MIRAGE Conference on Computer Vision/Computer Graphics Collaboration for Model-based Imaging, Rendering, Image Analysis and Graphical Special Effects<address><addrLine>Rocquencourt, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b503">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Tissandier</surname></persName>
		</author>
		<title level="m">La photographie en ballon</title>
		<imprint>
			<publisher>Gauthier-Villars</publisher>
			<date type="published" when="1886">1886</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b504">
	<analytic>
		<title level="a" type="main">On the advantage of polar and log-polar mapping for direct estimation of time-to-impact from optical flow</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tistarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sandini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="401" to="410" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b505">
	<analytic>
		<title level="a" type="main">A unifying omnidirectional camera model and its applications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Toepfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ehlgen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras</title>
		<meeting>the 7th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras<address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b506">
	<analytic>
		<title level="a" type="main">Projective model for central catadioptric cameras using Clifford algebra</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tolvanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Perwass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th DAGM Symposium</title>
		<meeting>the 27th DAGM Symposium<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="192" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b507">
	<analytic>
		<title level="a" type="main">Shape and motion from image streams under orthography: A factorization method</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="1992-11">November 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b508">
	<analytic>
		<title level="a" type="main">Mathematics of a multiple omnidirectional system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Imiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Omnidirectional Vision and Camera Networks</title>
		<meeting>the Workshop on Omnidirectional Vision and Camera Networks<address><addrLine>Madison, Wisconsin, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b509">
	<analytic>
		<title level="a" type="main">A practical algorithm to correct geometrical distortion of image acquisition cameras</title>
		<author>
			<persName><forename type="first">J</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Menéndez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="2451" to="2454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b510">
	<analytic>
		<title level="a" type="main">Matching constraints and the joint image</title>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th IEEE International Conference on Computer Vision</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Grimson</surname></persName>
		</editor>
		<meeting>the 5th IEEE International Conference on Computer Vision<address><addrLine>Cambridge, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1995-06">June 1995</date>
			<biblScope unit="page" from="338" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b511">
	<analytic>
		<title level="a" type="main">Bundle adjustment -a modern synthesis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mclauchlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Vision Algorithms: Theory and Practice</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</editor>
		<meeting>the International Workshop on Vision Algorithms: Theory and Practice<address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="298" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b512">
	<monogr>
		<title level="m" type="main">Introductory Techniques for 3-D Computer Vision</title>
		<author>
			<persName><forename type="first">E</forename><surname>Trucco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Prentice-Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b513">
	<analytic>
		<title level="a" type="main">An efficient and accurate camera calibration technique for 3D machine vision</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Miami Beach, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="364" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b514">
	<analytic>
		<title level="a" type="main">A versatile camera calibration technique for high-accuracy 3D machine vision metrology using off-the-shelf TV cameras and lenses</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="323" to="344" />
			<date type="published" when="1987-08">August 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b515">
	<analytic>
		<title level="a" type="main">Using plane + parallax for calibrating dense camera arrays</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vaish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wilburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="2" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b516">
	<monogr>
		<title level="m" type="main">Contributions à la vision omnidirectionnelle: Étude, Conception et Étalonnage de capteurs pour l&apos;acquisition d&apos;images et la modélisation 3D</title>
		<author>
			<persName><forename type="first">B</forename><surname>Vandeportaele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006-12">December 2006</date>
			<pubPlace>France</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Institut National Polytechnique de Toulouse</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note>in french</note>
</biblStruct>

<biblStruct xml:id="b517">
	<analytic>
		<title level="a" type="main">A new linear calibration method for paracatadioptric cameras</title>
		<author>
			<persName><forename type="first">B</forename><surname>Vandeportaele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cattoen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Marthon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gurdjos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Pattern Recognition</title>
		<meeting>the 18th International Conference on Pattern Recognition<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="647" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b518">
	<analytic>
		<title level="a" type="main">Central catadioptric line detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vasseur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mouaddib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th British Machine Vision Conference</title>
		<meeting>the 15th British Machine Vision Conference<address><addrLine>Kingston upon Thames, England</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b519">
	<analytic>
		<title level="a" type="main">A simple method of radial distortion correction with centre of distortion estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="165" to="172" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b520">
	<analytic>
		<title level="a" type="main">A new calibration model of camera lens distortion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="607" to="615" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b521">
	<analytic>
		<title level="a" type="main">Two plane camera calibration: a unified model</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Maui, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="133" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b522">
	<analytic>
		<title level="a" type="main">Implicit and explicit camera calibration: Theory and experiments</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="469" to="480" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b523">
	<analytic>
		<title level="a" type="main">A complete two-plane camera calibration method and experimental comparisons</title>
		<author>
			<persName><forename type="first">G.-Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th IEEE International Conference on Computer Vision</title>
		<meeting>the 4th IEEE International Conference on Computer Vision<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="439" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b524">
	<analytic>
		<title level="a" type="main">Camera calibration with distortion models and accurate evaluation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Herniou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="965" to="980" />
			<date type="published" when="1992-10">October 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b525">
	<analytic>
		<title level="a" type="main">Contributions to the physiology of vision -part the first -on some remarkable, and hitherto unobserved, phenomena of binocular vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wheatstone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society of London</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="371" to="394" />
			<date type="published" when="1838">1838</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b526">
	<analytic>
		<title level="a" type="main">Contributions to the physiology of vision -part the second -on some remarkable, and hitherto unobserved, phenomena of binocular vision (continued)</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wheatstone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society of London</title>
		<imprint>
			<biblScope unit="volume">142</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1852">1852</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b527">
	<monogr>
		<title level="m" type="main">Catadioptric system</title>
		<author>
			<persName><surname>Wikipedia</surname></persName>
		</author>
		<ptr target="http://en.wikipedia.org/wiki/Catadioptricsystem" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b528">
	<analytic>
		<title level="a" type="main">High-speed videography using a dense camera array</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wilburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vaish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="294" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b529">
	<analytic>
		<title level="a" type="main">High performance imaging using large camera arrays</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wilburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vaish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-V</forename><surname>Talvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Antunez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="765" to="776" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b530">
	<analytic>
		<title level="a" type="main">International Archives of Photogrammetry and Remote Sensing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wiley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Close-Range Photogrammetry Meets Machine Vision</title>
		<title level="s">also in SPIE</title>
		<imprint>
			<date type="published" when="1990">1990. 1990</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="112" to="118" />
		</imprint>
	</monogr>
	<note>Metric aspects of zoom vision</note>
</biblStruct>

<biblStruct xml:id="b531">
	<analytic>
		<title level="a" type="main">A perspective projection camera model for zoom lenses</title>
		<author>
			<persName><forename type="first">R</forename><surname>Willson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shafer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Optical 3D Measurement Techniques</title>
		<meeting>the Second Conference on Optical 3D Measurement Techniques<address><addrLine>Zürich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-10">October 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b532">
	<analytic>
		<title level="a" type="main">What is the center of the image?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Willson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shafer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="670" to="671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b533">
	<analytic>
		<title level="a" type="main">Photogrammetric calibration of television systems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the XII ISPRS-Congress</title>
		<meeting>the XII ISPRS-Congress<address><addrLine>Ottawa, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b534">
	<analytic>
		<title level="a" type="main">Implicit camera calibration based on a nonlinear modeling function of an artificial neural network</title>
		<author>
			<persName><forename type="first">D.-M</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-C</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Symposium on Neural Networks</title>
		<meeting>the 6th International Symposium on Neural Networks<address><addrLine>Wuhan, China</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="967" to="975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b535">
	<analytic>
		<title level="a" type="main">Fish-eye views, and vision under water</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Magazine Series</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">68</biblScope>
			<biblScope unit="page" from="159" to="162" />
			<date type="published" when="1906">1906</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b536">
	<analytic>
		<title level="a" type="main">A new linear algorithm for calibrating central catadioptric cameras</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3166" to="3172" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b537">
	<monogr>
		<title level="m" type="main">Free-formed Surface Mirrors in Computer Vision Systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Würz-Wessel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003-06">June 2003</date>
			<pubPlace>Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Eberhard-Karls-Universität Tübingen</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b538">
	<analytic>
		<title level="a" type="main">Creating image-based VR using a self-calibrating fisheye lens</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Turkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Puerto Rico, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-06">June 1997</date>
			<biblScope unit="page" from="237" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b539">
	<analytic>
		<title level="a" type="main">Omnidirectional sensing and combined multiple sensing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yachida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE and ATR Workshop on Computer Vision for Virtual Reality Based Human Communications</title>
		<meeting>the IEEE and ATR Workshop on Computer Vision for Virtual Reality Based Human Communications<address><addrLine>Bombay, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b540">
	<analytic>
		<title level="a" type="main">Omnidirectional sensing and applications</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Transactions on Information and Systems</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="568" to="579" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b541">
	<analytic>
		<title level="a" type="main">Panoramic scene analysis with conic projection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kawato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Workshop on Intelligent Robots and Systems</title>
		<meeting>the IEEE International Workshop on Intelligent Robots and Systems<address><addrLine>Ibaraki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="181" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b542">
	<analytic>
		<title level="a" type="main">Rolling motion estimation for mobile robot by using omnidirectional image sensor hyperomnivision</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Nishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yamazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yachida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Pattern Recognition</title>
		<meeting>the 13th International Conference on Pattern Recognition<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="946" to="950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b543">
	<analytic>
		<title level="a" type="main">Multiple visual sensing system for mobile robot</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Okumura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yachida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="1679" to="1684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b544">
	<analytic>
		<title level="a" type="main">3D line segment reconstruction by using hyperomni vision and omnidirectional hough transforming</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yamazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yachida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Pattern Recognition</title>
		<meeting>the 15th International Conference on Pattern Recognition<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="483" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b545">
	<analytic>
		<title level="a" type="main">Omindirectional imaging with hyperboloidal projection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yamazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yachida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<meeting>the IEEE/RSJ International Conference on Intelligent Robots and Systems<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="1029" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b546">
	<monogr>
		<title level="m" type="main">Anamorphic image processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yelick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980-05">May 1980</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Bachelor Thesis</note>
</biblStruct>

<biblStruct xml:id="b547">
	<analytic>
		<title level="a" type="main">An omnidirectional stereo vision system using a single camera</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Pattern Recognition</title>
		<meeting>the 18th International Conference on Pattern Recognition<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="861" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b548">
	<analytic>
		<title level="a" type="main">Physical panoramic pyramid and noise sensitivity in pyramids</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Hilton Head Island, South Carolina, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="90" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b549">
	<analytic>
		<title level="a" type="main">Can we consider central catadioptric cameras and fisheye cameras within a unified imaging model</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th European Conference on Computer Vision</title>
		<meeting>the 8th European Conference on Computer Vision<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="442" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b550">
	<analytic>
		<title level="a" type="main">Catadioptric camera calibration using geometric invariants</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1260" to="1271" />
			<date type="published" when="2004-10">October 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b551">
	<analytic>
		<title level="a" type="main">Distortion correction of fisheye lenses using a nonparametric imaging model</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="527" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b552">
	<analytic>
		<title level="a" type="main">Fisheye lenses calibration using straight-line spherical perspective projection constraint</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="61" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b553">
	<analytic>
		<title level="a" type="main">Linear catadioptric camera calibration from sphere images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras</title>
		<meeting>the 6th Workshop on Omnidirectional Vision, Camera Networks and Non-Classical Cameras<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b554">
	<analytic>
		<title level="a" type="main">Geometric interpretations of the relation between the image of the absolute conic and sphere images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2031" to="2036" />
			<date type="published" when="2006-12">December 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b555">
	<analytic>
		<title level="a" type="main">Identical projective geometric properties of central catadioptric line images and sphere images with applications to calibration</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="89" to="105" />
			<date type="published" when="2008-06">June 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b556">
	<analytic>
		<title level="a" type="main">Selective geometric correction of images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Yokobori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Miami Beach, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="530" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b557">
	<analytic>
		<title level="a" type="main">Multiperspective modeling and rendering using general linear cameras</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mcmillan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Information and Systems</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="359" to="384" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b558">
	<analytic>
		<title level="a" type="main">General linear cameras</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mcmillan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th European Conference on Computer Vision</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<meeting>the 8th European Conference on Computer Vision<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2004-05">May 2004</date>
			<biblScope unit="page" from="14" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b559">
	<analytic>
		<title level="a" type="main">Multiperspective projection and collineation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mcmillan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th IEEE International Conference on Computer Vision</title>
		<meeting>the 10th IEEE International Conference on Computer Vision<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b560">
	<analytic>
		<title level="a" type="main">Image-based lens geometric distortion correction using minimization of average bicoherence index</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1175" to="1187" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b561">
	<analytic>
		<author>
			<persName><forename type="first">K</forename><surname>Zaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spiegelphotographie und ihre Auswertung zu Messzwecken</title>
		<imprint>
			<date type="published" when="1912">1912</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="96" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b562">
	<analytic>
		<title level="a" type="main">Beiträge zur Spiegelphotogrammetrie</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Archives of Photogrammetry</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="269" to="276" />
			<date type="published" when="1913">1913</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b563">
	<analytic>
		<title level="a" type="main">Squaring the circle in panoramas</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th IEEE International Conference on Computer Vision</title>
		<meeting>the 10th IEEE International Conference on Computer Vision<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1292" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b564">
	<analytic>
		<title level="a" type="main">Nonlinear distortion correction in endoscopic video images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Helferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mclennan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Higgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="439" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b565">
	<analytic>
		<title level="a" type="main">On the epipolar geometry between two images with lens distortion</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Pattern Recognition</title>
		<meeting>the 13th International Conference on Pattern Recognition<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1996-08">August 1996</date>
			<biblScope unit="page" from="407" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b566">
	<analytic>
		<title level="a" type="main">A flexible new technique for camera calibration</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1330" to="1334" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b567">
	<analytic>
		<title level="a" type="main">Camera calibration with one-dimensional objects</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="892" to="899" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b568">
	<analytic>
		<title level="a" type="main">3D reconstruction from a single view of an object and its image in a plane mirror</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tsui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Pattern Recognition</title>
		<meeting>the 14th International Conference on Pattern Recognition<address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="1174" to="1176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b569">
	<analytic>
		<title level="a" type="main">From anorthoscope perception to dynamic vision</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="1154" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b570">
	<analytic>
		<title level="a" type="main">Panoramic representation of scenes for route understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Pattern Recognition</title>
		<meeting>the 10th International Conference on Pattern Recognition<address><addrLine>Atlantic City, New Jersey, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="161" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b571">
	<analytic>
		<title level="a" type="main">Panoramic representation for route recognition by a mobile robot</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="76" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b572">
	<analytic>
		<title level="a" type="main">Omnidirectional stereo vision</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Omnidirectional Vision</title>
		<meeting>the Workshop on Omnidirectional Vision<address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b573">
	<monogr>
		<title level="m" type="main">Geometrical modeling and real-time vision applications of a panoramic annular lens (PAL) camera system</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Riseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanson</surname></persName>
		</author>
		<idno>99-11</idno>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts at Amherst</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b574">
	<analytic>
		<title level="a" type="main">A method for determining geometrical distortion of off-the-shelf wide-angle cameras</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zollner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sablatnig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the DAGM Symposium on Pattern Recognition</title>
		<meeting>the DAGM Symposium on Pattern Recognition<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="224" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b575">
	<analytic>
		<title level="a" type="main">Mosaicing new views: The crossed-slit projection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zomet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="741" to="754" />
			<date type="published" when="2003-06">June 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b576">
	<analytic>
		<title level="a" type="main">A neural network-based camera calibration method for mobile robot localization problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Symposium on Neural Networks</title>
		<meeting>the Second International Symposium on Neural Networks<address><addrLine>Chongqing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="277" to="284" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
