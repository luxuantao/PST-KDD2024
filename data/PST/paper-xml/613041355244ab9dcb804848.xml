<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">miniF2F: a cross-system benchmark for formal Olympiad-level mathematics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-08-31">31 Aug 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kunhao</forename><surname>Zheng</surname></persName>
							<email>kunhao.zheng@polytechnique.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jesse</forename><forename type="middle">Michael</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stanislas</forename><surname>Polu</surname></persName>
							<email>spolu@openai.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">École</forename><surname>Polytechnique</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Openai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">miniF2F: a cross-system benchmark for formal Olympiad-level mathematics</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-08-31">31 Aug 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2109.00110v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present miniF2F, a dataset of formal Olympiad-level mathematics problems statements intended to provide a unified cross-system benchmark for neural theorem proving. The miniF2F benchmark currently targets Metamath, Lean, and Isabelle and consists of 488 problem statements drawn from the AIME, AMC, and the International Mathematical Olympiad (IMO), as well as material from high-school and undergraduate mathematics courses. We report baseline results using GPT-f [12], a neural theorem prover based on  and provide an analysis of its performance. We intend for miniF2F to be a community-driven effort and hope that our benchmark will help spur advances in neural theorem proving.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Shared benchmarks and datasets have historically played a crucial role in driving advances in large-scale applications of deep learning, e.g. in computer vision ( <ref type="bibr" target="#b5">[6]</ref>) and natural language processing ( <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b10">11]</ref>). Neural theorem proving is a rapidly developing area which aims to apply techniques from deep learning to interactive theorem proving. To date, most contributions in this area have focused on individual theorem proving systems, each with a separately-implemented mathematics library and with results reported on a dataset-specific test split; examples include the HOList <ref type="bibr" target="#b1">[2]</ref>, CoqGym <ref type="bibr" target="#b23">[24]</ref> and LeanStep <ref type="bibr" target="#b6">[7]</ref> theorem proving environments and benchmarks. However, benchmarks from this paradigm are not ideal for measuring the mathematical reasoning ability of neural theorem provers for several reasons. Library-specific train/test splits are siloed by construction, dependent on how theorems and lemmas are split in these libraries, and as such are not directly comparable across systems. Moreover, formal mathematics libraries are closer to software repositories than informal mathematical exposition, and many lemmas are implementation-specific artifacts without precise informal mathematical or cross-system translations.</p><p>To date, the neural theorem proving community has not organized its efforts around a cross-system benchmark. To address this need and to provide a common resource to research groups working on formal theorem proving, we present miniF2F, a unified cross-system benchmark of formal mathematics of progressively increasing difficulty, centering around Olympiad-level problem statements (AMC, AIME, IMO) as well as high-school and undergraduate maths classes. Both the content and name of miniF2F are inspired by the IMO Grand Challenge <ref type="bibr" target="#b15">[16]</ref>: to build an AI that can win a gold medal in the International Mathematical Olympiad in a formal-to-formal (F2F) format. More precisely, the agent must receive IMO problems written in a formal mathematical format, and must produce a formal (i.e. machine-checkable) proof for that problem.</p><p>We intend for miniF2F to serve as a stepping stone for different formal systems towards the IMO Grand Challenge <ref type="bibr" target="#b15">[16]</ref>, as it is end-to-end verifiable, cross-platform and spans a wide range of difficulty. While we report baseline results on miniF2F using GPT-f, a language model based on GPT-3 which has been finetuned for theorem proving, language models are not a mandatory approach for Olympiad problems and this assumption is not reflected in miniF2F, preserving the generality and widespread applicability of the benchmark to systems similar to DeepHOL <ref type="bibr" target="#b1">[2]</ref> or Holophrasm <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and related work Benchmarks</head><p>In the closely related field of (first-order) automated theorem proving (ATP), the TPTP <ref type="bibr" target="#b17">[18]</ref> benchmark is a library of test problems in a unified format for ATP systems. In interactive theorem proving, the "Freek 100" <ref type="bibr" target="#b21">[22]</ref> tracks progress across various interactive theorem provers on a list of 100 mathematical theorems. Wu et al. <ref type="bibr" target="#b22">[23]</ref> built a simplified formal proof environment INT with an associated synthetic inequality benchmark. Competitions and communal challenges have also spurred development in formal theorem proving. The CADE ATP System Competition (CASC) <ref type="bibr" target="#b16">[17]</ref> is a competition that evaluates the performance of first-order automated theorem proving systems. Proof Ground <ref type="bibr" target="#b7">[8]</ref>, part of the ITP conference, is an interactive proving contest (for humans) that supports Coq, Isabelle, and Lean, which focuses on evaluating the formalization effort of proof to given problems within limited time. Finally, the IMO Grand Challenge <ref type="bibr" target="#b15">[16]</ref>, a proposal from researchers working on the interactive proof assistant Lean, aims to build a system capable of solving IMO problems in the formal-to-formal format.</p><p>Due to its convenient framing as a natural language processing task, the domain of informal mathematical reasoning has received more attention than the formal one. MATH <ref type="bibr" target="#b8">[9]</ref> is a mathematics benchmark comprising 12,500 statements in natural language where exercises are classified into 5 levels of difficulty across various domains. Each exercise is combined with a detailed step-by-step proof in natural language. Scaling state-of-the-art models shows little amelioration on MATH, which requires advanced mathematical reasoning capabilities. miniF2F includes a number of formalized statements from MATH. NaturalProofs <ref type="bibr" target="#b19">[20]</ref> is another benchmark of natural proof in mathematics , containing 32k theorem statements and proofs. It essentially contains the proofs in ProofWiki and other resources. While MATH is more oriented towards mathematics exercises, NaturalProofs is focused on proofs of general mathematics theorems. Saxton et al. <ref type="bibr" target="#b13">[14]</ref> built a mathematics dataset with 2 × 10 6 training data and 10 4 test data, presented in a question-answering format where each statement is paired with a question written in natural language and a direct answer without proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural theorem proving</head><p>HOList <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">10]</ref> provides an environment as well as a benchmark for HOL Light. They also proposes various deep reinforcement learning approaches for theorem proving and report a pass rate of 59.91% on their benchmark. Yang et al. <ref type="bibr" target="#b23">[24]</ref> built CoqGym, a large-scale dataset, which comes also with a learning environment, of 71k human-written proofs in Coq proof assistant. They report a 30.0% pass rate on the held-out test theorems in CoqGym. Polu et al. <ref type="bibr" target="#b11">[12]</ref> applied a decoder-only transformer similar to GPT-3 <ref type="bibr" target="#b3">[4]</ref> to proof steps prediction in Metamath combined with a log-probability based proof search. They also proposed a methodology to train a value function to further guide proof search, achieving a 56.22% pass rate on the held-out test set. Large language models were applied to Lean by Han et al. <ref type="bibr" target="#b6">[7]</ref>. They created an environment around the Lean prover targeted to machine learning and propose a dataset extracted from low level proof artifacts that is shown to boost performance when used as a self-supervised co-training objective. They report a 48.4% pass rate on held-out test statements from mathlib, Lean's mathematical library <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">miniF2F benchmark</head><p>miniF2F is a dataset of manually formalized statements of Olympiad type problems, aligned in Lean, Metamath, and Isabelle (partial at the time of writing), providing a cross-platform benchmark for formal mathematical reasoning. Olympiad type problems are of particular interest to compare automated provers across different formal systems as the theories required to solve them are well identified and they generally do not require the definition of new mathematical concepts (a capability that remains beyond the current neural theorem proving state of the art). The formalized statements in miniF2F are drawn from multiple sources, ranging from high school and undergraduate level exercises to Olympiad problems. miniF2F also covers different sub-subjects in mathematics as well as proof strategies, focusing on the types of exercises whose statements are expressible in most formal systems. This leads to a systemic focus on algebra, number theory and inequalities because, for example, geometry and combinatorial problems are generally challenging to formalize due to only nascent efforts in these areas in most formal systems. The statements in miniF2F are all manually formalized and selected to cover a variety of difficulty levels for both humans and machines. Formal proofs for these statements are optionally attached.</p><p>miniF2F draws from AIME, AMC, IMO problems as well as problems from the MATH <ref type="bibr" target="#b8">[9]</ref> informal dataset. Formalizing problems from the MATH dataset serves two purposes. First, problems in MATH are segmented by difficulty level (from 1 to 5), randomly selecting a subset from each of these difficulty levels allows miniF2F to cover a wider range of difficulty. Second, it provides the community an opportunity to compare capabilities of formal automated prover to their informal counter-parts as discussed in later sections. miniF2F comprises a test set and a validation set, which are a stratified random split from the statements we formalized such that each set equally covers each problem type and difficulty (when available). Table <ref type="table" target="#tab_0">1</ref> shows a detailed distribution of these statements.</p><p>Versioning miniF2F is an evolving effort and new statements will continuously be added. Periodically, we will freeze versions of the benchmark. The current version of the benchmark is v1<ref type="foot" target="#foot_0">1</ref> and results in this paper are reported using this version. v1 comprises 244 test and 244 valid statements. The set of statements of each version is guaranteed to remain stable, only allowing fixes in case errors are later discovered.</p><p>Rules of engagement and License miniF2F is meant to serve as a shared resource for research groups working on applying deep learning to formal theorem proving. There is no formal process to submit evaluation results and researchers are simply invited to cite miniF2F indicating the version used in their evaluations. We also encourage them to contribute proofs found by their approaches back to the benchmark. The parts of the benchmark associated with each theorem prover (Metamath, Lean, Isabelle) are meant to be licensed in a way that is aligned with the licensing usage associated with the theorem prover's main library. As a result, the Metamath version of the benchmark is released under the MIT License, while the Lean and Isabelle versions are released under the Apache License.</p><p>Porting effort In addition to Metamath, Lean and Isabelle (work in progress), we are eager to extend the coverage of miniF2F to HOL Light and Coq, and will welcome any effort in that direction or to extend miniF2F to further systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, in order to study baseline performances associated with existing systems, we report pass rates achieved by GPT-f <ref type="bibr" target="#b11">[12]</ref> applied to Metamath, GPT-f/PACT <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b6">7]</ref> applied to Lean as well as a baseline prover implemented in Lean denoted as the tidy baseline. Pass rates are reported as Pass@N where N is the number of proof search attempts per statement. Pass@N is computed by running more attempts per statement, averaged to get an unbiased, low-variance estimate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Metamath</head><p>Metamath is powered by a meta logic system based on a single substitution rule. It's characterized by its simplicity which makes it convenient to study machine learning. Proofs in Metamath are, as a consequence of the low-level proofsteps, much longer than in other systems as there is no assistance from high-level tactics. Proofs which are trivial in other systems (e.g. n-digit addition or simple ring arithmetic transformations) can be quite tedious in Metamath. The absence of tactics is both a benefit, as the models sees and learns on everything, and a challenge, as proofs of even simple exercises require hundreds of proofsteps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">GPT-f</head><p>We report the pass rate of GPT-f applied to Metamath as described in <ref type="bibr" target="#b11">[12]</ref>. We use a model with 700m learnable parameters. The model is trained on an updated dump of the set.mm library (but similar synthetic datasets), using the log-probability based search as reported in Table <ref type="table">8</ref> of the GPT-f paper <ref type="bibr" target="#b11">[12]</ref>. The model achieves a Pass@1 of 1.3% and a Pass@8 of 1.6% on miniF2F-test. As expected, these numbers are quite low due to the length of typical proofs for even simple math exercises. The average proof length is also reported in Table <ref type="table" target="#tab_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Lean</head><p>In comparison to Metamath, Lean benefits from a large number of powerful tactics to assist formalization efforts. Typical Lean proofs are much shorter than Metamath's. This is also a formal system of interest as it has received a lot of attention from the mathematical community as recent theories have successfully been formalized in Lean (Perfectoid Spaces <ref type="bibr" target="#b4">[5]</ref>, Liquid Tensor experiment <ref type="bibr" target="#b14">[15]</ref>).</p><p>Lean is also associated with the IMO Grand Challenge <ref type="bibr" target="#b15">[16]</ref> which aims to organize a formal-to-formal challenge during the upcoming IMO competitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">tidy baseline</head><p>We use the generic best-first search algorithm presented in PACT <ref type="bibr" target="#b6">[7]</ref>. The algorithm works as follows: Given a list of tactics L with priority, we maintain a priority queue Q of tactic states whose priority is given by the priority of the last applied tactic in L that led to it. While Q is not empty, we pop the top tactic state t from Q. We iterate through L and apply each tactic to t. If no error is raised, we capture the returned tactic states from Lean and insert them back into Q.</p><p>We use the same terminology as in PACT <ref type="bibr" target="#b6">[7]</ref>: maximum queue size ω max , depth limit d max . We also enforce a budget of i max iterations of the outer loop. When Q's size reach q max , all the tactic states to be inserted are discarded. We do not expand the next tactic state when the depth is beyond d max . This loop is run until a proof is found or the iterations budget is exhausted.</p><p>For consistency checking, we run the tidy baseline under the same settings and on the same test set as in PACT <ref type="bibr" target="#b6">[7]</ref> except that we don't set a global timeout. Our implementation achieved a 10.5% pass rate on mathlib's test split. This result is comparable to the reported 9.9% in <ref type="bibr">PACT</ref>  timeout.</p><p>In addition to the curated list of tactics L used in PACT <ref type="bibr" target="#b6">[7]</ref>, we added 4 high-level tactics HL =[nlinarith, linarith, ring nf, norm num] to L with higher priorities than the others. We report our pass rate on miniF2F in Table <ref type="table" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">GPT-f/PACT</head><p>We report the pass rate of GPT-f/PACT as described in <ref type="bibr" target="#b6">[7]</ref>. We use a model with 700M learnable parameters. The model is trained on an updated dump 23 of the mathlib library using the PACT methodology denoted in the paper as mix2 &gt; mix1 + tactic in Figure <ref type="figure">6</ref>.</p><p>The model achieves a Pass@1 of 24.6% and a Pass@8 of 29.2% on miniF2F-test. The average proof length is also reported in Table <ref type="table" target="#tab_2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion</head><p>Access to high-level tactics One goal of miniF2F is to study the comparison of performance across formal systems. In this section we reported the performance of the same methodology (GPT-f <ref type="bibr" target="#b11">[12]</ref>) applied to both Lean and Metamath. Both models are pre-trained on WebMath <ref type="bibr" target="#b11">[12]</ref> and respectively trained on datasets extracted from Lean <ref type="bibr" target="#b6">[7]</ref> and Metamath <ref type="bibr" target="#b11">[12]</ref>. The overall compute deployed at training is comparable in both setup and exactly equivalent at test-time, yet the achieved performance appears drastically superior when applied to Lean. We hypothesize that this is mainly explained by the model's access to high-level tactics when applied to Lean, enabling the model to learn how to guide Lean's automation in an effective way.</p><p>An example of this high-level guidance behavior is well exemplified by the following proof of the statement algebra_sqineq_2unitcircatblt1 where the model heavily relies on Lean's nlinarith solver but provides it with essential premises to successfully guide the search. In Metamath, GPT-f fails to find a proof as it requires a very large number of steps to appropriately rewrite the goal in a way that is amenable to the use of set.mm's existing theorems. The tidy baseline also fails to find a proof of that statement as nlinarith is not capable of solving the goal without being passed extraneous premises.</p><p>These results motivate the use of neural theorem proving with formal systems that expose powerful high level tactics and also suggest the potential of a closer collaboration between formal systems and machine learning practitioners. It also motivates the use of generative models in that setup as the arguments required by high-level tactics to succeed on non trivial problems generally do not exist in the context of the statement and therefore have to be generated ex-nihilo.</p><p>Comparison of informal and formal setups The use of formal systems for neural theorem proving is often motivated by the role of the formal system as a verifier, enabling more advanced neural search strategies than possible in a fully informal setup where the generation of a model can't be verified automatically, as well as the access to powerful tactics. Our formalization of a subset of the MATH <ref type="bibr" target="#b8">[9]</ref> informal dataset provides an interesting approximate quantification of the benefit of having access to a formal system in the context of neural theorem proving. Approximate, because we only formalized a small subset of the MATH statements, but nonetheless useful since we drew uniformly from the 5 difficulty levels.</p><p>In <ref type="bibr" target="#b8">[9]</ref>, the performance of GPT-3 (which is a larger model than the GPT-f model studied here) is reported to be 6.0% in the algebra category and 3.9% in the number theory category. GPT-f applied to Lean by comparison achieves 51.4% in the algebra category and 41.7% in the number theory category. It is also worthwhile to note that the tidy baseline also highly outperforms (31.4% in algebra and 30.0% in number theory) GPT-3 in an informal setup demonstrating the benefit of proof automation alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented miniF2F, a dataset of formal Olympiad-level mathematics problem statements, meant to serve as an initial effort towards cross-system benchmarking of neural mathematical reasoning capabilities in formal environments. We reported the performance of the neural theorem prover GPT-f <ref type="bibr" target="#b11">[12]</ref> on both the Lean and Metamath parts of miniF2F as well as the performance of our non-neural tidy baseline applied to Lean. Then, we discussed these baselines and put them in perspective with previously reported comparable results in informal environments <ref type="bibr" target="#b8">[9]</ref>.</p><p>Finally, we hope that miniF2F will prove to be useful to the scientific community working on neural theorem proving and spur advances in this domain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Counts of successfully proved statements in miniF2F. Green bar: results from Lean GPT-f. Red bar: best result from the tidy baseline. Blue bar: results from Metamath GPT-f.</figDesc><graphic url="image-1.png" coords="5,72.00,108.00,467.99,197.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>theorem algebra_sqineq_2unitcircatblt1 (a b : R) (h 0 : a^2 + b^2 = 2) : a * b ≤ 1 := begin nlinarith [sq_nonneg a,sq_nonneg b,sq_nonneg (a -b)] end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Number of statements and their provenance in miniF2F v1</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Test Set Validation Set</cell></row><row><cell></cell><cell>TOTAL</cell><cell></cell><cell>244</cell><cell>244</cell></row><row><cell></cell><cell>IMO</cell><cell></cell><cell>20</cell><cell>20</cell></row><row><cell></cell><cell>AIME</cell><cell></cell><cell>15</cell><cell>15</cell></row><row><cell></cell><cell>AMC</cell><cell></cell><cell>45</cell><cell>45</cell></row><row><cell></cell><cell></cell><cell>Level 5</cell><cell>14</cell><cell>14</cell></row><row><cell></cell><cell></cell><cell>Level 4</cell><cell>14</cell><cell>14</cell></row><row><cell></cell><cell>Algebra</cell><cell>Level 3</cell><cell>14</cell><cell>14</cell></row><row><cell></cell><cell></cell><cell>Level 2</cell><cell>14</cell><cell>14</cell></row><row><cell>MATH</cell><cell></cell><cell>Level 1 Level 5</cell><cell>14 16</cell><cell>14 16</cell></row><row><cell></cell><cell></cell><cell>Level 4</cell><cell>11</cell><cell>11</cell></row><row><cell></cell><cell>Number Theory</cell><cell>Level 3</cell><cell>11</cell><cell>11</cell></row><row><cell></cell><cell></cell><cell>Level 2</cell><cell>11</cell><cell>11</cell></row><row><cell></cell><cell></cell><cell>Level 1</cell><cell>11</cell><cell>11</cell></row><row><cell></cell><cell>Algebra</cell><cell></cell><cell>18</cell><cell>18</cell></row><row><cell>CUSTOM</cell><cell cols="2">Number Theory</cell><cell>8</cell><cell>8</cell></row><row><cell></cell><cell>Induction</cell><cell></cell><cell>8</cell><cell>8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The table shows the number of solved statement in miniF2F when running the tidy baseline with different values of i max as well Lean's built-in tidy tactic. All tidy baseline experiments are run with ω max = 128, d max = 8 using L + HL. Despite the tidy baseline being deterministic, it is still subject to per-tactic application timeouts, explaining the number 43 reported on miniF2F-test for i max = 32.</figDesc><table><row><cell>given the waived global</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>. Baseline performance on Metamath and Lean. All proof searches are provided with a 128 expansions budget. GPT-f attempts e = 16 tactics per expansion while the tidy baseline attempts e = 17 tactics per expansion (L + HL, see section 4.2.1). Reported proof lengths are averages over all the proofs found in each run. Note that the tidy baseline being deterministic, there is no point attempting a proof search more than once.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">miniF2F-valid</cell><cell></cell><cell cols="2">miniF2F-test</cell><cell></cell></row><row><cell cols="2">Formal System Model</cell><cell>Proof Length</cell><cell cols="2">Pass rate Pass@1 Pass@8</cell><cell>Proof Length</cell><cell cols="2">Pass rate Pass@1 Pass@8</cell></row><row><cell>Metamath</cell><cell>GPT-f</cell><cell>16.2</cell><cell>1.0%</cell><cell>2.0%</cell><cell>20.3</cell><cell>1.3%</cell><cell>1.6%</cell></row><row><cell>Lean</cell><cell>tidy</cell><cell>1.7</cell><cell>16.8%</cell><cell>-</cell><cell>1.8</cell><cell>18.0%</cell><cell>-</cell></row><row><cell>Lean</cell><cell>GPT-f</cell><cell>2.6</cell><cell>23.9%</cell><cell>29.3%</cell><cell>2.5</cell><cell>24.6%</cell><cell>29.2%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/openai/miniF2F/tree/v1</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://github.com/jasonrute/lean_proof_recording/commit/8499f10c2e10dd533152070ed933c4f0b21ecdc0</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://github.com/jesse-michael-han/lean-step-public/commit/a2b83c237bfe4d6f1c48bb48bc0769b5940e614a</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We are grateful to Wenda Li for contributing the Isabelle statements currently available in miniF2F, paving the way towards a full support of Isabelle, as well as his feedback and encouragement in the process.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Example of statement in miniF2F</head><p>shows "a / b + b / a -a * b = 2" using assms by (smt (verit, ccfv threshold) diff divide distrib div self divide divide times eq eq divide imp nonzero mult div cancel left) end Table <ref type="table">4</ref>: Problem 11 of 2000 AMC 12 is formalized with proof in different languages in miniF2F. The proof in Metamath is too long to be fully displayed.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The lean mathematical library</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs</title>
				<editor>
			<persName><forename type="first">Jasmin</forename><surname>Blanchette</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Catalin</forename><surname>Hritcu</surname></persName>
		</editor>
		<meeting>the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs<address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">January 20-21, 2020. 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="367" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Holist: An environment for machine learning of higher order logic theorem proving</title>
		<author>
			<persName><forename type="first">Kshitij</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Loos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stewart</forename><surname>Wilcox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="454" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning to reason in large theories without imitation</title>
		<author>
			<persName><forename type="first">Kshitij</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><forename type="middle">N</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">M</forename><surname>Loos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Toman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10501</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Lean perfectoid spaces</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Buzzard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Commelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Massot</surname></persName>
		</author>
		<ptr target="https://leanprover-community.github.io/lean-perfectoid-spaces/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
				<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Proof artifact co-training for theorem proving with language models</title>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Michael Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Rute</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">W</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislas</forename><surname>Polu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06203</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Maximilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Haslbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Nipkow</surname></persName>
		</author>
		<author>
			<persName><surname>Wimmer</surname></persName>
		</author>
		<ptr target="https://www21.in.tum.de/~wimmers/proofground/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Measuring mathematical problem solving with the math dataset</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akul</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03874</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph representations for higher-order logic and theorem proving</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Paliwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Loos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kshitij</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2967" to="2974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The LAMBADA dataset: Word prediction requiring a broad discourse context</title>
		<author>
			<persName><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ngoc</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaella</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandro</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName><surname>Fernández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computer Linguistics</publisher>
			<date type="published" when="2016">August 7-12, 2016. 2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Generative language modeling for automated theorem proving</title>
		<author>
			<persName><forename type="first">Stanislas</forename><surname>Polu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03393</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Squad: 100, 000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<editor>
			<persName><forename type="first">Jian</forename><surname>Su</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</editor>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2016-11-01">2016. November 1-4, 2016. 2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01557</idno>
		<title level="m">Analysing mathematical reasoning abilities of neural models</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Peter</forename><surname>Scholze</surname></persName>
		</author>
		<ptr target="https://xenaproject.wordpress.com/2020/12/05/liquid-tensor-experiment/" />
		<title level="m">Liquid tensor experiment</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Imo grand challenge</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Selsam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Buzzard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reid</forename><surname>Barton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percey</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Loss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Freek</forename><surname>Wiedijk</surname></persName>
		</author>
		<ptr target="https://imo-grand-challenge.github.io/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Sutcliffe</surname></persName>
		</author>
		<title level="m">The CADE ATP System Competition -CASC. AI Magazine</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="99" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<author>
			<persName><forename type="first">G</forename><surname>Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The TPTP Problem Library and Associated Infrastructure. From CNF to TH0, TPTP v6.4.0</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="483" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">May 6-9, 2019. OpenReview.net, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Le Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01112</idno>
		<title level="m">Naturalproofs: Mathematical theorem proving in natural language</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Holophrasm: a neural automated theorem prover for higher-order logic</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Whalen</surname></persName>
		</author>
		<idno>CoRR, abs/1608.02644</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Formalizing 100 theorems</title>
		<author>
			<persName><forename type="first">Freek</forename><surname>Wiedijk</surname></persName>
		</author>
		<ptr target="https://www.cs.ru.nl/~freek/100/" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Int: An inequality benchmark for evaluating generalization in theorem proving</title>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><forename type="middle">Qiaochu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02924</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to prove theorems via interacting with proof assistants</title>
		<author>
			<persName><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6984" to="6994" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
