<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distances in random graphs with finite variance degrees</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2006-06-03">June 3, 2006</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Remco</forename><surname>Van Der Hofstad</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics and Computer Science</orgName>
								<orgName type="institution">Eindhoven University of Technology</orgName>
								<address>
									<postBox>P.O. Box 513</postBox>
									<postCode>5600 MB</postCode>
									<settlement>Eindhoven</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gerard</forename><surname>Hooghiemstra</surname></persName>
							<email>g.hooghiemstra@ewi.tudelft.nl</email>
						</author>
						<author>
							<persName><forename type="first">Piet</forename><surname>Van Mieghem</surname></persName>
							<email>p.vanmieghem@ewi.tudelft.nl</email>
							<affiliation key="aff1">
								<orgName type="department">Electrical Engineering, Mathematics and Computer Science</orgName>
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<postBox>P.O. Box 5031</postBox>
									<postCode>2600 GA</postCode>
									<settlement>Delft</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Distances in random graphs with finite variance degrees</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2006-06-03">June 3, 2006</date>
						</imprint>
					</monogr>
					<idno type="MD5">026BB8951E2C76A1D465A1EA4AC4735A</idno>
					<idno type="arXiv">arXiv:math.PR/0407092v1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we study a random graph with N nodes, where node j has degree D j and {D j } N j=1 are i.i.d. with P(D j ≤ x) = F (x). We assume that 1 -F (x) ≤ cx -τ +1 for some τ &gt; 3 and some constant c &gt; 0. This graph model is a variant of the so-called configuration model, and includes heavy tail degrees with finite variance.</p><p>The minimal number of edges between two arbitrary connected nodes, also known as the graph distance or the hopcount, is investigated when N → ∞. We prove that the graph distance grows like log ν N , when the base of the logarithm equals ν = E[D j (D j -1)]/E[D j ] &gt; 1. This confirms the heuristic argument of Newman, Strogatz and Watts <ref type="bibr" target="#b34">[35]</ref>. In addition, the random fluctuations around this asymptotic mean log ν N are characterized and shown to be uniformly bounded. In particular, we show convergence in distribution of the centered graph distance along exponentially growing subsequences.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The study of complex networks plays an increasingly important role in science. Examples of such networks are electrical power grids and telephony networks, social relations, the World-Wide Web and Internet, co-authorship and citation networks of scientists, etc. The structure of these networks affects their performance. For instance, the topology of social networks affects the spread of information and disease (see e.g., <ref type="bibr" target="#b36">[37]</ref>). The rapid evolution in, and the success of, the Internet have incited fundamental research on the topology of networks.</p><p>Different scientific disciplines report their own viewpoints and new insights in the broad area of networking. In computer science and electrical engineering, massive Internet measurements have lead to fundamental questions in the modelling and characterization of the Internet topology <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b37">38]</ref>. These modelling questions drive the understanding of the Internet's complex behavior and allow to plan and to control end-to-end communication. The pioneering work of Strogatz and Watts (see e.g. <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b40">41]</ref> and the references therein) have triggered an immense number of research papers in the field of theoretical physics. Strogatz and Watts proposed 'small world networks' and illustrated how such small worlds can arise due to underlying mechanisms in different practical networks such as social networks, growing structures in nature, the Web, etc.</p><p>Albert and Barabási in <ref type="bibr" target="#b2">[3]</ref> showed that preferential attachment of nodes gives rise to a class of graphs often called 'scale free networks'. See also <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref> and the references therein. Scale free networks seem to explain the structure of the World-Wide Web, the autonomous domain structure of Internet, citation graphs and many other complex networks (see e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33]</ref>). The essence of scale free networks is that the nodal degree is a power law, or, alternatively, heavy-tailed, meaning that the number of nodes with degree equal to k is proportional to k -τ for some power exponent τ &gt; 1.</p><p>On the World-Wide Web, it has indeed been shown that there are power law degree sequences, both for the in-and out degrees (see <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29]</ref>). The work of Albert and Barabási have inspired substantial work on scale-free graphs and can be seen as a way to understand the emergence of power law degree sequences. In the model by Albert and Barabási <ref type="bibr" target="#b2">[3]</ref>, this power exponent is restricted to τ = 3 <ref type="bibr" target="#b13">[14]</ref>, but in refinements of the model, different values of τ can be obtained. See, e.g., <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30]</ref> and the references therein. We will comment on the relations between our work and preferential attachment models in Section 1.4 below. For an overview of the extensive field of random graphs, we refer to the books of Bollobás <ref type="bibr" target="#b8">[9]</ref> and Janson et al. <ref type="bibr" target="#b27">[28]</ref>.</p><p>The current paper presents a rigorous mathematical derivation for the random fluctuations of the graph distance between two arbitrary nodes in a graph with finite variance degrees. These finite variance degrees include power laws with power exponent τ &gt; 3. We consider the configuration model with power law degree sequences, a variation on a model originally proposed by Newman, Strogatz and Watts <ref type="bibr" target="#b34">[35]</ref>, prove their conjecture and proceed beyond their results by combining coupling theory, branching processes and shortest path graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Model definition</head><p>Fix an integer N . Consider an i.i.d. sequence D 1 , D 2 , . . . , D N . We will construct an undirected graph with N nodes where node j has degree D j . We will assume that L N = N j=1 D j is even. If L N is odd, then we add a stub to the N th node, so that D N is increased by 1. This single stub will make hardly any difference in what follows, and we will ignore this effect. We will later specify the distribution of D 1 .</p><p>To construct the graph, we have N separate nodes and incident to node j, we have D j stubs. All stubs need to be connected to build the graph. The stubs are numbered in a given order from 1 to L N . We start by connecting at random the first stub with one of the L N -1 remaining stubs. Once paired, two stubs form a single edge of the graph. Hence, a stub can be seen as the left or the right half of an edge. We continue the procedure of randomly choosing and pairing the stubs until all stubs are connected. Unfortunately, nodes having self-loops may occur. However, self-loops are scarce when N → ∞.</p><p>We now specify the degree distribution we will investigate in this paper. The probability mass function and the distribution function of the nodal degree D are denoted by P(D = j) = f j , j = 0, 1, 2, . . . , and</p><formula xml:id="formula_0">F (x) = ⌊x⌋ j=0 f j ,<label>(1.1)</label></formula><p>where ⌊x⌋ is the largest integer smaller than or equal to x. Our main assumption is that for some τ &gt; 3 and some positive constant c,</p><formula xml:id="formula_1">1 -F (x) ≤ cx -τ +1 , (x &gt; 0). (1.2)</formula><p>This condition implies that the second moment of D is finite. The often used condition that 1 -F (x) = x -γ+1 L(x), γ &gt; 3, with L a slowly varying function is covered by (1.2), because by Potter's Theorem <ref type="bibr" target="#b22">[23,</ref><ref type="bibr">Lemma 2,</ref><ref type="bibr">p</ref>. 277], any slowly varying function L(x) can be bounded above and below by an arbitrary small power of x, so that (1.2) holds for any τ &lt; γ.</p><p>The above model is closely related to the so-called configuration model, in which the degrees of the nodes are often assumed to be fixed (rather than i.i.d.). See <ref type="bibr" target="#b32">[33,</ref><ref type="bibr">Section 4.2</ref>.1] and the references therein. We will review some results proved for the configuration model in Section 1.4 below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Main results</head><p>We denote</p><formula xml:id="formula_2">µ = E[D], ν = E[D(D -1)] E[D] ,<label>(1.3)</label></formula><p>and we define the distance or hopcount H N between the nodes 1 and 2 as the minimum number of edges that form a path from 1 to 2 where, by convention, the distance equals ∞ if nodes 1 and 2 are not connected. Since the nodes are exchangeable, the distance between two randomly chosen nodes is equal in distribution to H N . Our main result is the following theorem:</p><p>Theorem 1.1 (Limit law for the typical nodal distance) Assume that τ &gt; 3 in (1.2) and that ν &gt; 1.</p><formula xml:id="formula_3">For k ≥ 1, let a k = ⌊log ν k⌋-log ν k ∈ (-1, 0]. There exist random variables (R a ) a∈(-1,0] such that as N → ∞, P H N -⌊log ν N ⌋ = k H N &lt; ∞ = P(R a N = k) + o(1), k ∈ Z. (1.4)</formula><p>In words, Theorem 1.1 states that for τ &gt; 3, the graph distance H N between two randomly chosen connected nodes grows like the log ν N , where N is the size of the graph, and that the fluctuations around this mean remain uniformly bounded in N . Theorem 1.1 proves the conjecture in Newman, Strogatz and Watts <ref type="bibr">[35,</ref> Section II.F, (54)], where a heuristic is given that the number of edges between arbitrary nodes grows like log ν N . In addition, Theorem 1.1 improves upon that conjecture by specifying the fluctuations around the value log ν N .</p><p>We will identify the laws of (R a ) a∈(-1,0] in Theorem 1.4 below. Before doing so, we state two consequences of the above theorem: Corollary 1.2 (Convergence in distribution along subsequences) Fix an integer N 1 . Under the assumptions in Theorem 1.1, and conditionally on H N &lt; ∞, along the subsequence</p><formula xml:id="formula_4">N k = ⌊N 1 ν k-1 ⌋, the sequence of random variables H N k -⌊log ν N k ⌋ converges in distribution to R a N 1 as k → ∞.</formula><p>Simulations illustrating the convergence in Corollary 1.2 are discussed in Section 1.5. (ii) conditionally on H N &lt; ∞, the random variables H N -log ν N form a tight sequence, i.e., lim K→∞ lim sup</p><formula xml:id="formula_5">N →∞ P |H N -log ν N | ≤ K H N &lt; ∞ = 1. (1.5)</formula><p>We need a limit result from branching process theory before we can identify the limiting random variables (R a ) a∈(-1,0] . In Section 2 below, we introduce a delayed branching process {Z k }, where in the first generation, the offspring distribution is chosen according to (1.1) and in the second and further generations, the offspring is chosen in accordance to g given by</p><formula xml:id="formula_6">g j = (j + 1)f j+1 µ , j = 0, 1, . . . .<label>(1.6)</label></formula><p>The process {Z k /µν k-1 } is a martingale with uniformly bounded expectation and consequently converges almost surely to a limit:</p><formula xml:id="formula_7">lim n→∞ Z n µν n-1 = W a.s. (1.7)</formula><p>In the theorem below we need two independent copies W (1) and W (2) of W.</p><p>Theorem 1.4 (The limit laws) Under the assumptions in Theorem 1.1, and for a ∈ (-1, 0],</p><formula xml:id="formula_8">P(R a &gt; k) = E exp{-κν a+k W (1) W (2) } W (1) W (2) &gt; 0 , (1.8)</formula><p>where W (1) and W (2) are independent limit copies of W in (1.7) and where κ = µ(ν -1) -1 .</p><p>We will also provide an error bound of the convergence stated in Theorem 1.1. Indeed, we show that for any α &gt; 0, and for all k ≤ η log ν N for some η &gt; 0 sufficiently small,</p><formula xml:id="formula_9">P(H N &gt; ⌊log ν N ⌋ + k) = E exp{-κν a N +k W (1) W (2) } + O((log N ) -α ). (1.9)</formula><p>Unfortunately, due to the conditioning in Theorem 1.1, it is hard to obtain an explicit error bound in <ref type="bibr">(1.4)</ref>.</p><p>The law of R a is involved, and can in most cases not be computed exactly. The reason for this is the fact that the random variables W that appear in its statement are hard to compute explicitly. For example, for the power-law degree graph with τ &gt; 3, we do not know what the law of W is. See also Section 2. There are two examples where the law of W is known. The first is when all degrees in the graph are equal to some r &gt; 2, and we obtain the r-regular graph (see also <ref type="bibr" target="#b14">[15]</ref>, where the diameter of this graph is studied). In this case, we have that µ = r, ν = r -1, and W = 1 a.s. In particular, P(H N &lt; ∞) = 1 + o(1). Therefore, we obtain that</p><formula xml:id="formula_10">P(R a &gt; k) = exp{- r r -2 (r -1) a+k },<label>(1.10)</label></formula><p>and H N is asymptotically equal to log r-1 N . The second example is when the law g is geometric, in which case the branching process with offspring g conditioned to be positive converges to an exponential random variable with parameter 1. This example corresponds to</p><formula xml:id="formula_11">g j = p(1 -p) j-1 , so that f j = 1 jc p p(1 -p) j-2 , ∀j ≥ 1,<label>(1.11)</label></formula><p>and c p is the normalizing constant. For p &gt; 1 2 , the law of W has the same law as the sum of D 1 copies of a random variable Y, where Y = 0 with probability 1-p  p and equal to an exponential random variable with parameter 1 with probability 2p-1 p . Even in this simple case, the computation of the exact law of R a is non-trivial. Although the laws R a are hard to compute exactly, Theorems 1.1 and 1.4 make it possible to simulate the hopcount in random graphs of arbitrary size since the law of W is simple to approximate numerically, for example using Fast Fourier Transforms.</p><p>In <ref type="bibr" target="#b26">[27]</ref>, the expected value of the random variable R a is computed numerically, by comparing it to E[log W|W &gt; 0]. One would expect that for some β with 0 &lt; β &lt; α,</p><formula xml:id="formula_12">E[H N |H N &lt; ∞] = ⌊log ν N ⌋ + E[R a ] + O((log N ) -β ).</formula><p>(1.12)</p><p>If so, an accurate computation of E[R a ] would yield the fine asymptotics of the expected hopcount, and this would yield an extension of the conjectured results in <ref type="bibr">[35, (54)</ref>]. Our methods stop short of proving (1.12), and this remains an interesting question.</p><p>Our final result describes the size of the largest connected component and the maximal size of all other connected components. In its statement, we write G for the random graph with degree distribution given by (1.1), and we write q for the survival probability of the delayed branching process {Z k } described above. Thus, 1 -q is the extinction probability of the branching process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Methodology and heuristics</head><p>One can understand Theorems 1.1 and 1.4 intuitively as follows. Denote by Z (1)  k , respectively, Z (2)   k the number of stubs of nodes at distance k -1 from node 1, respectively, node 2 (see Section 3 for the precise definitions). Then for N → ∞, the random process</p><formula xml:id="formula_13">Z (i) 1 , Z (i) 2 , . . . , Z (i)</formula><p>k , which will be called shortest path graphs (SPG's), behave as a delayed branching process as long as Z (i)  k is of small order compared to N . Thus, the local neighborhood of the node i is close in distribution to a branching process. We sample the stubs uniformly from all stubs and thus, for large N , we attach the stubs to the SPG proportionally to jf j . Moreover, when a new stub is attached to the SPG, the chosen stub is used to attach the new node and forms an edge together with the present stub. Therefore, the number of stubs of the freshly chosen node decreases by one and is equal to j if the number of stubs of the chosen node was originally equal to j + 1. This motivates <ref type="bibr">(1.6)</ref>.</p><p>The offspring of the node 1 is distributed as D 1 , whereas the offspring distribution of Z (1)  2 , Z (1)  3 , . . . has (for N → ∞) probability mass function <ref type="bibr">(1.6)</ref>. Consequently, as noted in <ref type="bibr">[35, (51)</ref>], the mean number of free stubs at distance k is close to µν k-1 , where ν = ∞ j=1 jg j is defined in <ref type="bibr">(1.3)</ref>. Moreover, a stub in Z (1)  k is attached with a positive probability to a stub in Z (2)  k whenever Z (1)  k Z (2)   k is of order L N . The total degree L N is proportional to N by the law of large numbers, because</p><formula xml:id="formula_14">µ = E[D 1 ] &lt; ∞.</formula><p>Since both sets grow at the same rate, each has to be of order √ N . Therefore, k is typically 1  2 log ν N , and the typical distance between 1 and 2 is of order 2k = log ν N . This can be made precise by coupling Z (1)  1 , Z (1)  2 , . . . to a branching process Ẑ(1) 1 , Ẑ(1) 2 , . . . having offspring distribution g (N ) j given by</p><formula xml:id="formula_15">g (N ) j = N i=1 I[D i = j + 1] D i L N = j + 1 L N N i=1 I[D i = j + 1],<label>(1.13)</label></formula><p>where I[E] is the indicator of the event E. This coupling will be described in Section 3.1. In turn, the branching process Ẑ(1) 1 , Ẑ(1) 2 , . . . will be coupled, in a conventional way, to a branching process Z (1)  1 , Z (1) 2 , . . . with offspring distribution {g j } defined in (1.6). The limit result of Theorem 1.1 and Theorem 1.4 depends on the martingale limit for super-critical branching processes with finite mean.</p><p>The proof of Theorems 1.1 and 1.4 are based upon a comparison of the local neighborhoods of nodes to branching processes. Such techniques are used extensively in random graph theory. An early example is in <ref type="bibr" target="#b14">[15]</ref>, where the diameter of a random regular graph was investigated. See also <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">Chapter 10]</ref>, where comparisons to branching processes are used to describe the phase transition and the birth of the giant component for the random graph G(p, N ).</p><p>The proof of Theorem 1.5 makes essential use of results by Molloy and Reed <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> for the usual configuration model. We will now describe their result. When the number of nodes with degree i in the graph of size N equals d i (N ) where lim N →∞ d i (N )/N = Q(i), Molloy and Reed <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> identify the condition ∞ i=1 i(i -2)Q(i) &gt; 0 as the necessary and sufficient condition to ensure that a 'giant component' proportional to the size of the graph exists. By rewriting the condition ν &gt; 1 in Theorem 1.1 as E[D 2 ] -2E[D] &gt; 0, we see that a similar condition as in the model of Molloy and Reed is needed here. To prove Theorem 1.5, we need to check that the technical conditions in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> are satisfied in our model. In fact, we need to alter the graph G a little bit in order to apply their results, since in <ref type="bibr" target="#b30">[31]</ref> it is assumed that no nodes of degree larger than N 1 4 -ǫ exist for some ǫ &gt; 0.</p><p>The novelty of our results is that we investigate typical distances in random graphs. In random graph theory, it is more customary to investigate the diameter in the graph, and in fact, this would also be an interesting problem. The research question investigated in this paper is inspired by the Internet. In a seminal paper <ref type="bibr" target="#b21">[22]</ref>, Faloutsos et al. have shown that the degree distribution of autonomous systems in Internet follows a power law with power exponent τ ≈ 2.2. Thus, the power law random graph with this value of τ can possibly lead to a good Internet model on the autonomous systems (AS) level (see <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b37">38]</ref>). For the Internet on the more detailed router level, extensive measurements exist for the hopcount, which is the number of routers traversed between two typical routers, as well as for the AS-count, which is the number of autonomous systems traversed between two typical routers. To validate the configuration model with i.i.d. degrees, we intend to compare the distribution of the distance between pairs of nodes to these measurements in Internet. For this, a good understanding of the typical distances between nodes in the degree random graph are necessary, which formed the main motivation for our work. The hopcount in Internet seems to be close to a Poisson random variable with a fairly large parameter. In turn, a Poisson random variable with large parameter can be approximated by a normal random variable with equal expectation and variance. See e.g. <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b39">40]</ref> for data of the hopcount in Internet.</p><p>From a practical point of view, there are good reasons to study the typical distances in random graphs rather than the diameter. For one, typical distances are simpler to measure, and thus allow for a simpler validation of the model. Also, the diameter is a number, while the distribution of the typical distances contains substantially more information. Finally, the diameter is rather sensitive to small changes to a graph. For instance, when adding a string of a few nodes, one can dramatically alter the diameter, while the typical distances in the graph hardly change. Thus, typical distances in the graph are more robust to modelling discrepancies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Related work</head><p>There is a wealth of related work which we will now summarize. The model investigated here was also studied in <ref type="bibr" target="#b35">[36]</ref>, with 1-F (x) = x -τ +1 L(x), where τ ∈ (2, 3) and L denotes a slowly varying function. It was shown in <ref type="bibr" target="#b35">[36]</ref> that the average distance is bounded from above by</p><formula xml:id="formula_16">2 log log N | log(τ -2)| (1 + o(1)</formula><p>). We plan to return to the question of average distances and connected component sizes when τ &lt; 3 in three future publications <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>There is substantial work on random graphs that are, although different from ours, still similar in spirit. In <ref type="bibr" target="#b0">[1]</ref>, random graphs were considered with a degree sequence that is precisely equal to a power law, meaning that the number of nodes with degree k is precisely proportional to k -τ . Aiello et al. <ref type="bibr" target="#b0">[1]</ref> show that the largest connected component is of the order of the size of the graph when τ &lt; τ 0 = 3.47875 . . ., where τ 0 is the solution of ζ(τ -2)-2ζ(τ -1) = 0, and where ζ is the Riemann Zeta function. When τ &gt; τ 0 , the largest connected component is of smaller order than the size of the graph and more precise bounds are given for the largest connected component. When τ ∈ (1, 2), the graph is with high probability connected. The proofs of these facts use couplings with branching processes and strengthen previous results due to <ref type="bibr">Molloy and Reed [31,</ref><ref type="bibr" target="#b31">32]</ref> described above. For this same model, Dorogovtsev et al. <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> investigate the leading asymptotics and the fluctuations around the mean of the distance between arbitrary nodes in the graph from a theoretical physics point of view, using mainly generating functions.</p><p>A second related model can be found in <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b17">[18]</ref>, where edges between nodes i and j are present with probability equal to w i w j / l w l for some 'expected degree vector' w = (w 1 , . . . , w N ).</p><p>Chung and Lu <ref type="bibr" target="#b16">[17]</ref> show that when w i is proportional to i -1 τ -1 the average distance between pairs of nodes is log ν N (1 + o(1)) when τ &gt; 3, and <ref type="bibr" target="#b2">3)</ref>. The difference between this model and ours is that the nodes are not exchangeable in <ref type="bibr" target="#b16">[17]</ref>, but the observed phenomena are similar. This result can be heuristically understood as follows. Firstly, the actual degree vector in <ref type="bibr" target="#b16">[17]</ref> should be close to the expected degree vector. Secondly, for the expected degree vector, we can compute that the number of nodes for which the degree is less than or equal to k equals |{i :</p><formula xml:id="formula_17">2 log log N | log(τ -2)| (1 + o(1)) when τ ∈ (2,</formula><formula xml:id="formula_18">w i ≤ k}| ∝ |{i : i -1 τ -1 ≤ k}| ≈ k -τ +1 .</formula><p>Thus, one expects that the number of nodes with degree at most k decreases as k -τ +1 , similarly as in our model. In <ref type="bibr" target="#b17">[18]</ref>, Chung and Lu study the sizes of the connected components in the above model. The advantage of this model is that the edges are independently present, which makes the resulting graph closer to a traditional random graph. All the models described above are static, i.e., the size of the graph is fixed, and we have not modeled the growth of the graph. As described in the introduction, there is a large body of work investigating dynamical models for complex networks, often in the context of the World-Wide Web. In various forms, preferential attachment has been shown to lead to power law degree sequences. Therefore, such models intend to explain the occurrence of power law degree sequences in random graphs. See <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30]</ref> and the references therein. In the preferential attachment model, nodes with a fixed degree m are added sequentially. Their stubs are attached to a receiving node with a probability proportionally to the degree of the receiving node, thus favoring nodes with large degrees. For this model, it is shown that the number of nodes with degree k decays proportionally to k -3 <ref type="bibr" target="#b13">[14]</ref>, the diameter is of order log N log log N when m ≥ 2 <ref type="bibr" target="#b10">[11]</ref>, and couplings to a classical random graph G(N, p) are given for an appropriately chosen p in <ref type="bibr" target="#b12">[13]</ref>. See also <ref type="bibr" target="#b11">[12]</ref> for a survey.</p><p>It can be expected that our model is a snapshot of the above models, i.e., a realization of the graph growth processes at the time instant that the graph has a certain prescribed size. Thus, rather than to describe the growth of the model, we investigate the properties of the model at a given time instant. This is suggested in [4, Section VII.D], and it would be very interesting indeed to investigate this further mathematically, i.e., to investigate the relation between the configuration and the preferential attachment models.</p><p>The reason why we study the random graphs at a given time instant is that we are interested in the topology of the random graph. In <ref type="bibr" target="#b37">[38]</ref>, and inspired by the observed power law degree sequence in <ref type="bibr" target="#b21">[22]</ref>, the configuration model with i.i.d. degrees is proposed as a model for the AS-graph in Internet, and it is argued on a qualitative basis that this simple model serves as a better model for the Internet topology than currently used topology generators. Our results can be seen as a step towards the quantitative understanding of whether the hopcount in Internet is described well by the average graph distance in the configuration model.</p><p>In <ref type="bibr">[33,</ref> Table <ref type="table">II</ref>], many more examples are given of real networks that have power law degree sequences. Interestingly, there are also many examples where power laws are not observed, and often the degree law falls off faster than a power law. These observed degrees can be described by a degree distribution as in (1.1) with 1 -F (x) smaller than any power, and the results in this paper thus apply. Such examples are described in more detail in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr">Section II]</ref>. Examples where the tails of the degree distribution are lighter than power laws are power and neural networks [4, Section II.K], where the tails are observed to be exponential, and protein folding [4, Section II.L], where the tails are observed to be Gaussian. In other examples, a degree distribution is found that for small values is a power law, but has an exponential cut off. An example of such a degree distribution is</p><formula xml:id="formula_19">f k = Ck -γ e -k/κ ,<label>(1.14)</label></formula><p>for some κ &gt; 0 and γ ∈ R. The size of κ indicates up to what degree the power law still holds, and where the exponential cut off starts to set in. For this example, our results apply since the exponential tail ensures that (1.2) holds for any τ &gt; 3 by picking c &gt; 0 large enough. Thus, we prove the conjectures on the expected path lengths in <ref type="bibr">[35, (55)</ref>, (56)] and [4, Section V.C, (63) and (64)] for this particular model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5">Simulation for illustration of the main results</head><p>To illustrate Theorem 1.1, we have simulated the random graph with degree distribution D = ⌈U -1 τ -1 ⌉, where U is uniformly distributed over (0, 1) and where for x ∈ R, ⌈x⌉ is the smallest integer greater than or equal to x. Thus,</p><formula xml:id="formula_20">1 -F (k) = P(U -1 τ -1 &gt; k) = k 1-τ , k = 1, 2, 3, . . . ,</formula><p>for which µ = 1 + ζ(τ -1) and ν = 2ζ(τ -2)/µ. We observe that for τ = 3.5 and N = 25, 000 and N = 125, 000, the values a N = -0.62 . . . are identical up to two decimals. We hence expect, on the basis of our main theorem, that the survival functions P(H N &gt; k) for these two cases are similar. Because ⌊log ν 25, 000⌋ = 12 and ⌊log ν 125, 000⌋ = 14, we expect that the empirical survival function for N = 125, 000 is a shift of the empirical survival function for N = 25, 000, over the horizontal distance 14 -12 = 2. Figure <ref type="figure" target="#fig_2">1</ref> supports this claim, given the statistical inaccuracy. In Figure <ref type="figure" target="#fig_2">1</ref> we have also included the empirical survival function for N = 75, 000, for which a N = -0.99 . . ., as the bold line. This empirical survival function clearly has a different shape. Thus, the empirical survival function for N = 75, 000 is not a shift of the empirical survival function for N = 25, 000 or N = 125, 000.</p><p>We finally demonstrate Corollary 1.2 for τ = 3.5 in Figure <ref type="figure" target="#fig_3">2</ref>. In this case ν 2 ≈ 5 and N k = N 1 ν 2k , k = 0, 1, 2, 3. We take N 1 = 5, 000, and so N 2 = 25, 000, N 3 = 125, 000, N 4 = 625, 000. For these values of N 1 , . . . , N 4 , we have simulated the hopcount with 1, 000 replications and we expect from Corollary 1.2 that the survival functions run parallel at mutual distance 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.6">Organization of the paper</head><p>We will first review the relevant literature on branching processes in Section 2. We will then explain how we can couple our degree model to independent branching processes in Section 3. This section is also valuable for our coming paper <ref type="bibr" target="#b23">[24]</ref>, where we study the case τ ∈ (2, 3). In particular, in <ref type="bibr" target="#b23">[24]</ref>, we will use Lemmas A.2.2 and A.2.8 and Proposition A.3.1. The bounds for the coupling are formulated in Sections 3.1, 3.2 and 3.3. In these sections, we will state the results on the coupling that are needed in the proof of the main results, Theorems 1.1 and 1.4. Parts of this section apply more generally, i.e., to τ ∈ (2, 3). We prove Theorems 1.1 and 1.4 in Section 4 and Theorem 1.5 in Section 5. The technical details of the coupling of { Ẑ(i) k } to {Z (i) k } for i = 1, 2 are contained in Section A.1, while the details of the coupling of {Z (i)  k } to { Ẑ(i) k } for i = 1, 2 are in Section A.2. Finally, we prove that at any fixed time m, with probability converging to 1,</p><formula xml:id="formula_21">Z (i) m = Z (i) m for i = 1, 2 in Section A.3.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Review of branching process theory with finite mean</head><p>Since we rely heavily on the theory of branching processes, we will briefly review this theory in the case where the expected value of the offspring distribution is finite. The theory of branching processes is well understood (see e.g. <ref type="bibr" target="#b6">[7]</ref>). For the formal definition of the delayed branching process (BP) that we consider here, we define a double sequence {X n,i } n≥1,i≥1 of i.i.d. random variables each with distribution equal to the offspring distribution {g j } ∞ j=0 , where we recall</p><formula xml:id="formula_22">g j = (j + 1)f j+1 µ , j = 0, 1, . . . . (2.1)</formula><p>We further let X 0,1 have probability mass function f in (1.1), independently from {X n,i } n≥1,i≥1 . The BP {Z n } is now defined by Z 0 = 1 and</p><formula xml:id="formula_23">Z n+1 = Zn i=1 X n,i , n ≥ 0. Because τ &gt; 3, we have that both E[Z 1 ] = E[X 0,1 ] = µ &lt; ∞ and ν = E[X 1,1 ] &lt; ∞. We further assume that ν = E[X 1,1 ] &gt; 1,</formula><p>so that the BP is super-critical. Given that the (n -1) st generation consists of m individuals, the conditional expectation of Z n equals mν, independently of the size of the preceding generations, so that for n ≥ 1, we have</p><formula xml:id="formula_24">E[Z n |Z n-1 ] = Z n-1 ν. Hence, W n = Zn µν n-1 , is a martingale. Since E[|W n |] = E[W n ] = 1, the sequence E[|W n |]</formula><p>is uniformly bounded by 1 and so by Doob's martingale convergence theorem <ref type="bibr">[42, p. 58</ref>] the sequence W n converges almost surely. If we denote the a.s. limit by a proper random variable W, we obtain (1.7).</p><p>There are only few examples where the limit random variable W is known. It is known that W has an atom at 0 of size p ≥ 0, equal to the extinction probability of the (delayed-)BP (q = 1 -p). Conditioned on non-extinction the limit W has an absolute continuous density on (0, ∞).</p><p>We need a result that follows from <ref type="bibr" target="#b5">[6]</ref> concerning the speed of convergence of W n to W. Define</p><formula xml:id="formula_25">R n = W n ν ∞ ν n /n α x dG(x), α &gt; 0,</formula><p>where G is the distribution function of the offspring with probabilities {g j }. Since</p><formula xml:id="formula_26">µ α = ∞ 0 x[log + x] α dG(x) &lt; ∞, (log + x = max(0, log x)),</formula><p>for each α &gt; 0, it follows from ([6, page 8, line 4]) that with probability 1,</p><formula xml:id="formula_27">W -W k + ∞ n=k R n = o(k -α ). (2.2) An immediate consequence of (2.2) is that if |W -W k | &gt; k -α , then ∞ n=k R n &gt; k -α .</formula><p>Hence, using E[W n ] = 1 and partial integration,</p><formula xml:id="formula_28">P(|W -W k | &gt; k -α ) ≤ P ∞ n=k R n &gt; k -α ≤ k α ∞ n=k E[R n ] = - ∞ n=k k α ν ∞ ν n /n α x d [1 -G(x)] = ∞ n=k k α ν [1 -G(ν n /n α )] + ∞ n=k k α ν ∞ ν n /n α [1 -G(x)] dx. Since 1 -F (x) ≤ c • x 1-τ (see (1.2)), we find 1 -G(x) ≤ c ′ • x 2-τ</formula><p>so that for each α &gt; 0, and with</p><formula xml:id="formula_29">k = ⌊ 1 2 log ν N ⌋, P |W -W k | &gt; (log N ) -α ≤ O((log N ) α ) ∞ n=k (ν n /n α ) 3-τ = O(e -β log N ) = O(N -β ),<label>(2.3)</label></formula><p>for some positive β, because τ &gt; 3 and ν &gt; 1.</p><p>3 Graph construction and coupling with a BP</p><p>In this section, we will describe how the shortest path graph (SPG) from node 1 can be obtained, and we will couple it to a BP. This coupling works for any degree distribution. In Sections 3.2 and 3.3 below, we will obtain bounds on the coupling. The SPG from node 1 is the random graph as observed from node 1, and consists of the shortest paths between node 1 and all other nodes {2, . . . , N }. As will be shown below, it is not necessarily a tree because cycles may occur. Recall that two stubs together form an edge. We define Z (1)  1 = D 1 , and for k ≥ 2, we denote by Z (1)  k the number of stubs attached to nodes at distance k -1 from node 1, but are not part of an edge connected to a node at distance k -2. We will refer to such stubs as 'free stubs'. Thus, Z (1)  k is the number of outgoing stubs from nodes at distance k -1. In Section 3.1 we will describe a coupling that, conditionally on D 1 , . . . , D N , couples {Z (1)  k } to a BP { Ẑ <ref type="bibr" target="#b0">(1)</ref> k } with the random offspring distribution</p><formula xml:id="formula_30">g (N ) j = N i=1 I[D i = j + 1]P(a stub from node i is sampled|D 1 , . . . , D N ) = N i=1 I[D i = j + 1] D i L N = j + 1 L N N i=1 I[D i = j + 1],<label>(3.1)</label></formula><p>where as before</p><formula xml:id="formula_31">L N = D 1 + D 2 + . . . + D N .</formula><p>By the strong law of large numbers, for N → ∞,</p><formula xml:id="formula_32">L N N → E[D], and<label>1</label></formula><formula xml:id="formula_33">N N i=1 I[D i = j + 1] → P(D = j + 1), a.s.</formula><p>so that a.s.,</p><formula xml:id="formula_34">g (N ) j → (j + 1)P(D = j + 1)/E[D] = g j , N → ∞. (3.2)</formula><p>Therefore, the BP { Ẑ(1) k } with offspring distribution {g (N ) j } is expected to be close to a BP with offspring distribution {g j } given in <ref type="bibr">(1.6)</ref>. Consequently, in Section 3.3, we will couple the BP { Ẑ <ref type="bibr" target="#b0">(1)</ref> k } to a BP {Z (1)  k } with offspring distribution {g j }. This will allow us to prove Theorems 1.1 and 1.4 in Section 4.</p><p>Throughout the paper we use the following lemma. It shows that</p><formula xml:id="formula_35">L N is close to E[L N ] = µN . Lemma 3.1 (Concentration of L N ) For each 0 &lt; a &lt; 1 2 , b = 1 -2a and some constant c &gt; 0, P L N E[L N ] -1 ≥ N -a ≤ cN -b . (3.3)</formula><p>Proof. The proof is immediate from the Chebychev inequality, since</p><formula xml:id="formula_36">P L N E[L N ] -1 2 ≥ N -2a ≤ N 2a (N µ) 2 Var(L N ) = Var(D) µ 2 N 2a-1 , so that b = 1 -2a &gt; 0 and c = Var(D) µ 2 &lt; ∞.</formula><p>3.1 Coupling with a branching process with offspring g (N)</p><p>We will construct the SPG in such a way that we simultaneously construct a BP with offspring distribution {g (N ) j } in (3.1). This BP is of course purely imaginary. The BP is coupled with the SPG such that it enables us to control their difference.</p><p>As above, we will use the notation Z (1)  k and Z (2)  k to denote the number of stubs attached to nodes at distance k -1 from node 1, respectively, node 2, but not part of an edge connected to a node at distance k -2. For k = 1, Z (i)  k = D i . We start with a description of the coupling of the SPG with root 1, and a BP with offspring distribution g (N ) given in <ref type="bibr">(3.1)</ref>. The first stages of the generation of the SPG are drawn in Figure <ref type="figure" target="#fig_4">3</ref>. We will explain the meaning of the labels 1, 2 and 3 below.</p><p>We draw repeatedly and independently from the distribution {g (N ) j }. This is done conditionally given D 1 , D 2 , . . . , D N , so that we draw from the random distribution (3.1). After each draw we will update the realization of the SPG and the BP, and classify the stubs according to three categories, which will be labelled 1, 2 and 3. These labels will be updated as the growth of the SPG proceeds. The labels have the following meaning:</p><p>1. Stubs with label 1 are stubs belonging to a node that is not yet attached to the SPG.</p><p>2. Stubs with label 2 are attached to the SPG (because the corresponding node has been chosen),</p><p>but not yet paired with another stub. These are called 'free stubs'.</p><p>3. Stubs with label 3 in the SPG are paired with another stub to form an edge in the SPG.</p><p>The growth process as depicted in Figure <ref type="figure" target="#fig_4">3</ref> starts by giving all stubs label 1. Then, because we construct the SPG starting from node 1, we relabel the D 1 stubs of node 1 with the label 2. We note that Z (1)  1 is equal to the number of stubs connected to node 1, and thus Z (1) 1 = D 1 . We next identify Z (1)  j for j &gt; 1. Z (1)  j is obtained by sequentially growing the SPG from the free stubs in generation Z (1)  j-1 . When all free stubs in generation j -1 have chosen their connecting stub, Z (1)   j is equal to the number of stubs labelled 2 (i.e., free stubs) attached to the SPG. Note that not necessarily each stub of Z (1)  j-1 contributes to stubs of Z (1)  j , because a cycle may 'swallow' two free stubs in generation j -1. This is the case precisely when a stub with label 2 is chosen.</p><p>For the BP, we start with Ẑ(1) 1 = D 1 , and grow from the free stubs available in the BP tree by sequentially growing from the stubs (alike for the SPG). For the coupling, as long as there are free SPG stubs with their labels The growth process starts by choosing the first stub of node 1 whose stubs are labeled by 2 as illustrated in the second line, while all the other stubs maintain the label 1. Next, we uniformly choose a stub with label 1 or 2. In the example in line 3, this is the second stub from node 3, whose stubs are labeled by 2 except for the second stub which is labeled 3. The left hand side column visualizes growth of the SPG by the attachment of stub 2 of node 3 to the first stub of node 1. Once an edge is established the paired stubs are labeled 3. In the next step, the next stub of node one is again matched to a uniform stub out of those with label 1 or 2. In the example in line 4, it is the first stub of the last node that will be attached to the second stub of node 1, the next in sequence to be paired. The last line exhibits the result of creating a cycle when the first stub of node 3 is chosen to be attached to the last stub of node 9 (the last node). This process is continued until there are no more stubs with labels 1 or 2. In this example, we have Z (1) 1 = 3 and Z (1) 2 = 6.</p><p>stubs in both the BP and the SPG in a given generation, we couple the BP and SPG in the following way. At each step we will take an independent draw from all stubs, according to the distribution (3.1). Since the stubs are specified by their label (1, 2 or 3), we can now present the construction rules for the BP and the SPG.</p><p>1. If the chosen stub has label 1, then in both the BP and the SPG we will connect the present stub to the chosen stub to form an edge and attach the remaining stubs of the chosen node as children. We update the labels as follows. The present and chosen stub melt together to form an edge and both are assigned label 3. All 'brother' stubs (except for the chosen stub) belonging to the same node of the chosen stub receive label 2.</p><p>2. In this case we choose a stub with label 2, which is already connected to the SPG. For the BP, the chosen stub is simply connected to the stub which is grown, and the number of free stubs is the number of 'brother stubs' of the chosen stub. For the SPG, a self-loop is created when the chosen stub and present stub are 'brother' stubs which belong to the same node. When they are not 'brother' stubs, then a cycle is formed. Neither a self-loop nor a cycle changes the distances in the SPG. Note that for the SPG two free stubs are used, while for the BP only one stub is used. This is illustrated in Figure <ref type="figure">4</ref>.</p><p>The updating of the labels solely consists of changing the label of the present and the chosen stub from 2 to 3.</p><p>3. A stub with label 3 is chosen. This case is illustrated in Figure <ref type="figure">5</ref>. This possibility of choosing an already matched stub with label 3 must be included for the BP which relies on the property</p><formula xml:id="formula_37">D i = 5 D j = 3</formula><p>SPG BP Figure <ref type="figure">4</ref>: Example of the coupling when a cycle occurs. Edges have twice the length of stubs. In the SPG the two dotted stubs in the left picture are to be connected. The middle picture gives the result of creating the cycle in the SPG where the bold line is the edge creating the cycle. The third figure draws the BP where the cycle is removed and the degree of the circled node is 3.</p><formula xml:id="formula_38">D i = 5 SPG D = 2</formula><p>BP Figure <ref type="figure">5</ref>: An example of the coupling where we need to perform a redraw. In the draw from g (N ) , we draw the dotted stub in the SPG with degree 3. In the BP, we keep this degree, while in the SPG we draw again from the conditional distribution given that we do not draw a stub with label 3. In this example, this redraw gives the value D = 2. that all subsequent iterations in the process are i.i.d. Note that this includes the case where we draw the present stub, which of course is impossible for the SPG.</p><p>The rule now for the BP is that the corresponding node with the prescribed number of stubs is simply attached. Since for the SPG, we sample without replacement, we have to resample from distribution (3.1), until we draw a stub with label 1 or 2. This procedure is referred to as a redraw. Since we sample uniformly from all stubs, the conditional sampling until we hit a stub with label 1 or 2 is also uniform out of the set of all stubs with labels 1 and 2, so that it has the correct distribution. Obviously there are two cases: either we draw a stub with label 1 or one with label 2. When we draw a stub with label 1 in the SPG then we update as under rule 1 above, while when we draw a stub having label 2 in the SPG, we update as under rule 2 above.</p><p>Clearly, the redraws and the cycles cause possible differences between the BP and the SPG: the degrees of the chosen node are possibly different. We will need to show that the above difference only leads to an error term.</p><p>The above process stops in the j th generation when there are no more free stubs in generation j -1 for either the BP or for the SPG. When there are no more free stubs for the SPG, we complete the j th generation for the BP by drawing from distribution (3.1) for all the remaining free stubs. The labels of the stubs remain unchanged. When there are no more free stubs for the BP, we complete the j th generation for the SPG by drawing from distribution (3.1) iteratively until we draw a stub with label 1 or 2. This is done for all the remaining free stubs in the j th generation of the SPG. The labels are updated as under 1 and 2 above.</p><p>We continue the above process of drawing stubs until there are no more stubs having label 1 or 2, so that all stubs have label 3. Then, the construction is finalized, and we have generated the SPG as seen from node 1. We have thus obtained the structure of the SPG, and know how many nodes there are at a given distance from node 1.</p><p>The above construction will be performed similarly from node 2. This construction is close to being independent as long as the SPG's from the roots 1 and 2 do not share any nodes. More precisely, the corresponding BP's are independent. Thus, we have now constructed the SPG's and BP's from both node 1 and node 2.</p><p>3.2 Coupling with a BP with offspring distribution {g (N)  j }</p><p>In the previous section, we have obtained a coupling of the SPG and the BP with offspring distribution {g (N ) j }. In this and the next section, we will summarize bounds on the couplings that we need for the proof of Theorems 1.1 and 1.4. These results will be repeated in the appendix together with a full proof. We start with the coupling of the number of stubs Z (1)  j in the SPG and the number of children Ẑ <ref type="bibr" target="#b0">(1)</ref> j in the j th generation of the BP with offspring distribution {g (N ) j }.</p><p>Proposition 3.2 (Coupling SPG with the BP with random offspring distribution) There exist η, β &gt; 0, α &gt; 1 2 + η and a constant C, such that for all j ≤ ( 1 2 + η) log ν N ,</p><formula xml:id="formula_39">P (1 -N -α ν j ) Ẑ(1) j ≤ Z (1) j ≤ (1 + N -α ν j ) Ẑ(1) j ≥ 1 -CjN -β . (3.4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Coupling with a BP with offspring distribution {g j }</head><p>We next describe the coupling with the BP with offspring distribution {g j } and their bounds. A classical coupling argument is used (see e.g. <ref type="bibr" target="#b38">[39]</ref>). Let X (N ) have law {g (N ) j } and X have law {g j }. We define Y (N ) by P(Y (N) = n) = min(g (N )  n , g n ),</p><formula xml:id="formula_40">P(Y (N) = ∞) = 1 - ∞ n=0 min(g (N ) n , g n ) = 1 2 ∞ n=0 |g (N ) n -g n |. (3.5) Let X(N) = Y (N ) when Y (N) &lt; ∞, and P(X (N ) = n, Y (N) = ∞) = g (N )</formula><p>n -min(g (N )  n , g n ), whereas X = X when Y (N ) &lt; ∞, and P(X = n, Y (N ) = ∞) = g n -min(g (N )  n , g n ). Then X(N) has law g (N ) , and X has law g. Moreover, with large probability, X(N) = X due to Proposition 3.4 below.</p><p>This coupling argument is applied to each node in the BP { Ẑ(1) i } i≥0 and { Ẑ(2) i } i≥0 . The BP's with offspring distribution {g j } will be denoted by {Z (1)  i } i≥0 and {Z (2)  i } i≥0 . We can interpret this coupling as follows. Each node has an i.i.d. indicator variable which equals one with probability</p><formula xml:id="formula_41">p N = 1 2 ∞ n=0 |g (N ) n -g n |. (3.6)</formula><p>When at a certain node this indicator variable is 0, then the offspring in { Ẑ(1) i } i≥0 or { Ẑ(2) i } i≥0 equals the one in {Z (1)  i } i≥0 or {Z (2)  i } i≥0 , and the node is successfully coupled. When the indicator is 1, then an error has occurred, and the coupling is not successful. In this case, the laws of the offspring of { Ẑ <ref type="bibr" target="#b0">(1)</ref> i } i≥0 or { Ẑ(2) i } i≥0 is different from the one in {Z (1)  i } i≥0 or {Z (2)  i } i≥0 , and we record an error. Below we will use the notation P N to denote the conditional expectation given D 1 , D 2 , . . . , D N and E N to denotes the expectation with respect to the probability measure P N . Finally, we write</p><formula xml:id="formula_42">ν N = ∞ n=0 ng (N )</formula><p>n .</p><p>(3.7)</p><p>In the following proposition, we prove that at any fixed time, we can couple the SPG to the delayed BP with law {g j }: Proposition 3.3 (Coupling at fixed time) For any m ∈ N fixed, there exist independent branching processes Z (1) , Z (2) , such that lim</p><formula xml:id="formula_43">N →∞ P(Z (i) m = Z (i) m ) = 1. (3.8)</formula><p>In the course of the proof we will also rely on the following more technical claims: Proposition 3.4 (Convergence in total variation distance) There exist α 2 , β 2 &gt; 0 such that</p><formula xml:id="formula_44">P ∞ n=0 (n + 1)|g (N ) n -g n | ≥ N -α 2 ≤ N -β 2 .</formula><p>(3.9)</p><p>Consequently,</p><formula xml:id="formula_45">P(|ν N -ν| &gt; N -α 2 ) ≤ N -β 2 ,<label>(3.10)</label></formula><p>and</p><formula xml:id="formula_46">P(p N &gt; N -α 2 ) ≤ N -β 2 .</formula><p>(3.11)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corollary 3.5 (Coupling of sums)</head><p>There exist ε, β, η &gt; 0 such that for all j ≤ (1 + 2η) log ν N , as N → ∞,</p><formula xml:id="formula_47">P 1 N j i=1</formula><p>Z (1)  ⌈i/2⌉ Z (2)  ⌊i/2⌋ -</p><formula xml:id="formula_48">j i=1 Ẑ(1) ⌈i/2⌉ Ẑ(2) ⌊i/2⌋ &gt; N -ε = O(N -β ). (3.12)</formula><p>4 Proof of Theorem 1.1 and 1.4</p><p>The proof consists of four steps.</p><p>1. We first express the survival probability P(H N &gt; j) in the number of stubs {Z (k) i }, k = 1, 2, of the SPG's. For j ≤ (1 + 2η) log ν N , where η is specified in Proposition 3.2, we will show that</p><formula xml:id="formula_49">P(H N &gt; j) = E exp -j+1 i=2 Z (1)</formula><p>⌈i/2⌉ Z (2)   ⌊i/2⌋</p><formula xml:id="formula_50">L N + RM N (j) ,<label>(4.1)</label></formula><p>with</p><formula xml:id="formula_51">RM N (j) = O   j+1 i=2</formula><p>Z (1)  ⌈i/2⌉ Z (2)   ⌊i/2⌋ ⌈i/2⌉</p><formula xml:id="formula_52">k=1 (Z (1) k + Z (2) k ) L 2 N   .</formula><p>2. We use Proposition 3.2 to show that in (4.1) we can replace</p><formula xml:id="formula_53">{Z (i) k }, i = 1, 2 by the BP { Ẑ(i) k }, i = 1, 2. The error term E[|RM N (j)|]</formula><p>and the error involved in replacing the SPG by the BP is bounded by a constant times N -β , for some β &gt; 0, uniformly in j ≤ (1 + 2η) log ν N .</p><p>3. In this step we show that there exists β &gt; 0 such that for all j ≤ (1 + 2η) log ν N , as N → ∞,</p><formula xml:id="formula_54">P(H N &gt; j) = E exp -j+1 i=2 Z (1) ⌈i/2⌉ Z (2) ⌊i/2⌋ µN + O(N -β ),<label>(4.2)</label></formula><p>where Z (i) k , i = 1, 2, denotes the delayed BP with offspring distribution (1.6).</p><p>4. We complete the proof of Theorem 1.1 and 1.4, using step 3, and the almost sure limit in (1.7) applied to Z (1)  n and Z (2)  n . We finally use the speed of convergence of the above martingale limit result to obtain (1.9).</p><p>Step 1: A formula for P(H N &gt; j). The following lemma expresses P(H N &gt; j) in terms of Q (k,l)  Z , the conditional probabilities given {Z (1)  s } k s=1 and {Z (2)  s } l s=1 . For l = 0, we only condition on {Z (1)  s } k s=1 .</p><p>Lemma 4.1 For j ≥ 1,</p><formula xml:id="formula_55">P(H N &gt; j) = E j+1 i=2 Q (⌈i/2⌉,⌊i/2⌋) Z (H N &gt; i -1|H N &gt; i -2) .<label>(4.3)</label></formula><p>Proof. We first compute that</p><formula xml:id="formula_56">P(H N &gt; j) = E Q (1,1) Z (H N &gt; j) = E Q (1,1) Z (H N &gt; 1)Q (1,1) Z (H N &gt; j|H N &gt; 1)</formula><p>.</p><p>Continuing this further, and writing E (k,l)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Z</head><p>for the expectation with respect to</p><formula xml:id="formula_57">Q (k,l) Z , Q (1,1) Z (H N &gt; j|H N &gt; 1) = E (1,1) Z Q (2,1) Z (H N &gt; j|H N &gt; 1) = E (1,1) Z Q (2,1) Z (H N &gt; 2|H N &gt; 1)Q (2,1) Z (H N &gt; j|H N &gt; 2) .</formula><p>Therefore,</p><formula xml:id="formula_58">P(H N &gt; j) = E Q (1,1) Z (H N &gt; 1)E (1,1) Z Q (2,1) Z (H N &gt; 2|H N &gt; 1)Q (2,1) Z (H N &gt; j|H N &gt; 2) = E E (1,1) Z Q (1,1) Z (H N &gt; 1)Q (2,1) Z (H N &gt; 2|H N &gt; 1)Q (2,1) Z (H N &gt; j|H N &gt; 2) = E Q (1,1) Z (H N &gt; 1)Q (2,1) Z (H N &gt; 2|H N &gt; 1)Q (2,1) Z (H N &gt; j|H N &gt; 2) ,</formula><p>where, in the second equality, we use that Q (1,1)  Z (H N &gt; 1) is measurable with respect to the σ-algebra generated by Z (1,N)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>. This proves the claim for j = 2. More generally, we obtain that for k, l such that k + l ≤ j -1,</p><formula xml:id="formula_59">Q (k,l) Z (H N &gt; j|H N &gt; k + l -1) = E (k,l) Z Q (k,l+1) Z (H N &gt; j|H N &gt; k + l -1) = E (k,l) Z Q (k,l+1) Z (H N &gt; k + l|H N &gt; k + l -1)Q (k,l+1) Z (H N &gt; j|H N &gt; k + l) ,</formula><p>and, similarly,</p><formula xml:id="formula_60">Q (k,l) Z (H N &gt; j|H N &gt; k+l-1) = E (k,l) Z Q (k+1,l) Z (H N &gt; k+l|H N &gt; k+l-1)Q (k+1,l) Z (H N &gt; j|H N &gt; k+l) .</formula><p>In the above formulas, we can choose to increase k or l by one depending on {Z (1,N )   s } k s=1 and {Z (2,N )   s } l s=1 . We will iterate the above recursions, until k + l = j -1, when the last term becomes 1. This yields that</p><formula xml:id="formula_61">P(H N &gt; j) = E j i=1 Q (⌊i/2⌋+1,⌈i/2⌉) Z (H N &gt; i|H N &gt; i -1) .<label>(4.4)</label></formula><p>Renumbering gives the final result. We will next prove (4.1). In order to do so, we start by proving upper and lower bounds on the probabilities of not connecting two sets of stubs to each other. For this, suppose we have two disjoint sets of stubs A with |A| = n and B with |B| = m out of a total of L stubs. We match stubs at random, in such a way that two stubs form one edge, as in the construction of the SPG. In particular, loops are possible.</p><p>Let p(n, m, L) denote the probability that none of the n stubs in A attaches to one of the m stubs in B. Then, by conditioning on whether we choose a stub in A or not, we obtain the recursion</p><formula xml:id="formula_62">p(n, m, L) = n -1 L -1 p(n -2, m, L -2) + 1 - m + n -1 L -1 p(n -1, m, L -2) (4.5) Since p(n -2, m, L -2) ≥ p(n -1, m, L<label>-2)</label></formula><p>, because we have to match one additional stub, we obtain</p><formula xml:id="formula_63">p(n, m, L) ≥ 1 - m L -1 p(n -1, m, L -2) ≥ n-1 i=0 1 - m L -2i -1 .<label>(4.6)</label></formula><p>On the other hand, we can rewrite (4.5) as</p><formula xml:id="formula_64">p(n, m, L) = 1 - m L -1 p(n -1, m, L -2) + n -1 L -1 (p(n -2, m, L -2) -p(n -1, m, L -2)) . (4.7) We claim that p(n -2, m, L -2) -p(n -1, m, L -2) = m L -3 p(n -2, m -1, L -2) ≤ m L -3 .<label>(4.8)</label></formula><p>Indeed, the difference p(n -2, m, L -2) -p(n -1, m, L -2) is equal to the probability of the event that the first n -2 stubs do not connect to B, while the last one does. By exchangeability of the stubs, this probability equals the probability that the first stub is attached to a stub in B, and the remaining n -2 stubs are not. This latter probability is equal to m L-3 p(n -2, m -1, L -2). The equations (4.7) and (4.8) yield</p><formula xml:id="formula_65">p(n, m, L) ≤ 1 - m L -1 p(n -1, m, L -2) + n -1 (L -1) m (L -3)</formula><p>.</p><p>Iteration gives the upper bound</p><formula xml:id="formula_66">p(n, m, L) ≤ n-1 i=0 1 - m L -2i -1 + n 2 m (L -2n) 2 .<label>(4.9)</label></formula><p>Since the event {H N &gt; 1} holds if and only if no stubs of root 1 attaches to one of those of root 2, we obtain, using (4.6) and (4.9), that</p><formula xml:id="formula_67">Z (1) 1 -1 i=0 1 - Z (2) 1 L N -2i -1 ≤ Q (1,1) Z (H N &gt; 1) ≤    Z (1) 1 -1 i=0 1 - Z (2) 1 L N -2i -1    + (Z (1) 1 ) 2 Z (2) 1 (L N -2Z (1) 1 ) 2 . (4.10) Similarly, Q (2,1) Z (H N &gt; 2|H N &gt; 1) ≥ Z (2) 1 -1 i=0 1 - Z (1) 2 L N -2Z (1) 1 -2i -1 ,<label>(4.11)</label></formula><p>with a matching upper bound with an error term bounded by</p><formula xml:id="formula_68">(Z<label>(2) 1 ) 2 Z</label></formula><p>(1) 2</p><formula xml:id="formula_69">(L N -2Z<label>(1)</label></formula><p>1 -2Z</p><p>1 ) 2 . We use that, for natural numbers n, m, M with</p><formula xml:id="formula_71">M + n + m = o(L), n-1 i=0 1 - m L -M -2i -1 = e -nm L + O nm(M + n + m) L 2 , L → ∞.<label>(4.12)</label></formula><p>Using (4.12), the bounds in (4.10) yield</p><formula xml:id="formula_72">Q (1,1) Z (H N &gt; 1) = exp - Z (1) 1 Z (2) 1 L N + O Z (1) 1 Z (2) 1 (Z (1) 1 + Z (2) 1 ) L 2 N .</formula><p>Similarly, we can conclude that, as long as ⌈i/2⌉ k=1 (Z (1)  k + Z (2)  k ) = o(L N ), we have</p><formula xml:id="formula_73">Q (⌈i/2⌉,⌊i/2⌋) Z (H N &gt; i -1|H N &gt; i -2) = exp - Z (1) ⌈i/2⌉ Z (2) ⌊i/2⌋ L N + O   Z (1) ⌈i/2⌉ Z (2) ⌊i/2⌋ ( ⌈i/2⌉ k=1 (Z (1) k + Z (2) k )) L 2 N   .<label>(4.13)</label></formula><p>From (4.3) and taking expectations, the main term in (4.1) is evident. For the error term, we obtain that, as long as</p><formula xml:id="formula_74">⌈i/2⌉ k=1 (Z (1) k + Z (2) k ) = o(L N ), RM N (j) = j+1 i=2</formula><p>Z (1)  ⌈i/2⌉ Z (2)   ⌊i/2⌋ ⌈i/2⌉ k=1 (Z (1)  k + Z (2)  k ) L 2 N , and we will show at the end of step 2 that for all j &lt; (1 + 2η) log N , we have ⌈j/2⌉ k=1 (Z (1)  k + Z (2)  k ) = o(L N ) and that there exists a β &gt; 0 such that</p><formula xml:id="formula_75">E[RM N (j)] = O(N -β ). (4.14)</formula><p>Step 2: Coupling of SPG to the BP with offspring {g (N ) j }. We start by showing that for some β &gt; 0 and uniformly in j ≤ (1 + 2η) log ν N , the main term in (4.1) satisfies</p><formula xml:id="formula_76">E exp -j+1 i=2 Z (1) ⌈i/2⌉ Z (2) ⌊i/2⌋ L N = E exp -j+1 i=2 Ẑ(1) ⌈i/2⌉ Ẑ(2) ⌊i/2⌋ L N + O(N -β ).<label>(4.15)</label></formula><p>We will deal with the error term (4.14) at the end of this step. Bound</p><formula xml:id="formula_77">j+1 i=2</formula><p>Z (1)  ⌈i/2⌉ Z (2)  ⌊i/2⌋ -Ẑ(1)</p><formula xml:id="formula_78">⌈i/2⌉ Ẑ(2) ⌊i/2⌋ ≤ j+1 i=2</formula><p>Z (1)  ⌈i/2⌉ Z (2)  ⌊i/2⌋ -Ẑ(2) ⌊i/2⌋ + j+1 i=2 Ẑ(2) ⌊i/2⌋ Z (1)  ⌈i/2⌉ -Ẑ(1) ⌈i/2⌉ .</p><p>By Proposition 3.2 and uniformly in j ≤ (1 + 2η) log ν N , we have, with probability exceeding 1 -O(N -β log ν N ), that max j+1 i=2 Z (1)  ⌈i/2⌉ Z (2)  ⌊i/2⌋ -Ẑ(2) ⌊i/2⌋ ,</p><formula xml:id="formula_79">j+1 i=2 Ẑ(2) ⌊i/2⌋ Z (1) ⌈i/2⌉ -Ẑ(1) ⌈i/2⌉ = O(ν ( 1 2 +η) log ν N N -α ) j+1 i=2 Ẑ(1) ⌈i/2⌉ Ẑ(2) ⌊i/2⌋ . Since α &gt; 1 2 + η, we have ν ( 1 2 +η) log ν N N -α = N 1 2 +η-α = N -α 1 , for some α 1 &gt; 0.</formula><p>Hence, for any ε with 0 &lt; ε &lt; α 1 , where as before P N denotes the conditional probability given the degrees D 1 , D 2 , . . . , D N , and E N the expectation with respect to P N , we have</p><formula xml:id="formula_80">P N 1 N j+1 i=2</formula><p>Z (1)  ⌈i/2⌉ Z (2)  ⌊i/2⌋ -Ẑ(1)</p><formula xml:id="formula_81">⌈i/2⌉ Ẑ(2) ⌊i/2⌋ &gt; N -ε ≤ O(N -β log ν N ) + P N 1 N j+1 i=2 Ẑ(1) ⌊i/2⌋ Ẑ(2) ⌈i/2⌉ &gt; O N α 1 -ε ≤ O(N -β log ν N ) + O N ε-α 1 j+1 i=2 E N [ Ẑ(1) ⌈i/2⌉ Ẑ(2) ⌊i/2⌋ ].</formula><p>The involved conditional expectation can be computed explicitly and we obtain</p><formula xml:id="formula_82">j+1 i=2 E N [ Ẑ(1) ⌈i/2⌉ Ẑ(2) ⌊i/2⌋ ] = D 1 D 2 j+1 i=2 ν ⌈i/2⌉-1 N ν ⌊i/2⌋-1 N = D 1 D 2 j-1 i=0 ν i N ≤ cD 1 D 2 ν j N ,</formula><p>for some constant c. Proposition 3.4 implies that we can bound ν j N by ν j (1+N -α 2 ) j , with probability exceeding 1-N -β 2 , for some α 2 , β 2 &gt; 0, whereas Lemma 3.1 implies L -1</p><p>N can be replaced by (µN ) -1 with probability exceeding 1 -N -β 3 , for some β 3 &gt; 0. Putting this together we obtain after taking the expectation with respect to D 1 , D 2 , . . . , D N ,</p><formula xml:id="formula_83">P 1 L N j+1 i=2</formula><p>Z (1)  ⌊i/2⌋ Z (2)  ⌈i/2⌉ -Ẑ(1)</p><formula xml:id="formula_84">⌊i/2⌋ Ẑ(2) ⌈i/2⌉ &gt; N -ε ≤ O(N -β log ν N ) + O(N -β 1 ) + O(N -β 2 ) + O(N -β 3 ) + O ν j (1 + O(log ν N/N α 2 )) N 1+α 1 -ε .</formula><p>Since ν j ≤ N 1+2η for j ≤ (1 + 2η) log ν N , we obtain</p><formula xml:id="formula_85">P 1 L N j+1 i=2</formula><p>Z (1)  ⌈i/2⌉ Z (2)  ⌊i/2⌋ -Ẑ( <ref type="formula" target="#formula_0">1</ref>)</p><formula xml:id="formula_86">⌈i/2⌉ Ẑ(2) ⌊i/2⌋ &gt; N -ε = O(N -β ),<label>(4.16)</label></formula><p>for some β &gt; 0 by taking β, β 2 , β 3 , η and ε sufficiently small. For x -y small, and x, y ≥ 0, we find e -y = e -x + O(x -y), so that exp -j+1 i=2 Z (1)  ⌈i/2⌉ Z (2)   ⌊i/2⌋</p><p>L N -exp -</p><formula xml:id="formula_87">j+1 i=2 Ẑ(1) ⌊i/2⌋ Ẑ(2) ⌈i/2⌉ L N = O(N -ε ),</formula><p>with probability exceeding 1 -O(N -β ). In combination with the inequality e -x ≤ 1 for x ≥ 0, we obtain <ref type="bibr">(4.15)</ref>. We turn to the proof of (4.14) and the assumption that ⌈j/2⌉ k=1 (Z (1)  k + Z (2)  k ) = o(L N ). From Proposition 3.2 and, uniformly in j ≤ (1 + 2η) log ν N , we have with probability exceeding 1 -O(N -β log ν N ) that ⌈j/2⌉ k=1 (Z (1)  k + Z (2)  k ) ≤ (1 + O(N</p><formula xml:id="formula_88">1 2 +η-α )) ⌈j/2⌉ k=1 ( Ẑ(1) k + Ẑ(2) k ).</formula><p>(4.17) so that, for all i ≤ j,</p><formula xml:id="formula_89">P N ⌈i/2⌉ k=1 (Z (1) k + Z (2) k ) L 3/4 N &gt; N -ε ≤ O(N -β log ν N ) + (1 + O(N 1 2 +η-α ))E N ⌈j/2⌉ k=1 ( Ẑ(1) k + Ẑ(2) k ) N -ε L 3/4 N .</formula><p>Thus, in particular, using (4.17), ⌈j/2⌉ k=1 (Z (1)  k + Z (2)  k ) = o(L N ) on the above event. Bounding the expectation of Ẑ(i) k , we find for 0 &lt; ε &lt; 1/4 and for all i ≤ j ≤ (1 + 2η) log ν N ,</p><formula xml:id="formula_90">P ⌈j/2⌉ k=1 (Z (1) k + Z (2) k ) L 3/4 N &gt; N -ε ≤ N -β + (1 + O(N -α 1 )) N 1 2 +η N 3 4 -ε = O(N -β ),</formula><p>for some β &gt; 0. Hence, for ε 1 &gt; 0,</p><formula xml:id="formula_91">P   j+1 i=2</formula><p>Z (1)  ⌈i/2⌉ Z (2)   ⌊i/2⌋ ⌈i/2⌉ k=1 (Z (1)  k + Z</p><formula xml:id="formula_92">(2) k ) L 2 N &gt; N -ε 1   ≤ O(N -β ) + P j+1 i=2</formula><p>Z (1)  ⌈i/2⌉ Z (2)   ⌊i/2⌋</p><formula xml:id="formula_93">L 5/4 N &gt; N ε-ε 1 .</formula><p>By Proposition 3.2, the product Z (1)  ⌈i/2⌉ Z (2)  ⌊i/2⌋ can be bounded by (1 + O(N Z (1)  ⌈i/2⌉ Z (2)   ⌊i/2⌋ ⌈i/2⌉</p><formula xml:id="formula_94">k=1 (Z (1) k + Z (2) k ) L 2 N &gt; N -ε 1   ≤ O(N -β ),</formula><p>for some β &gt; 0. Since RM N (j) is the difference of two numbers between 0 and 1 and hence |RM N (j)| ≤ 1, we obtain that, when ε 1 ≥ β,</p><formula xml:id="formula_95">E[RM N (j)] ≤ N -ε 1 + P   1 L 2 N j+1 i=2</formula><p>Z (1)  ⌈i/2⌉ Z (2)   ⌊i/2⌋ ⌈i/2⌉ k=1 (Z (1)  k + Z</p><formula xml:id="formula_96">(2) k ) &gt; N -ε 1   ≤ O(N -β ). (4.18)</formula><p>This proves (4.14).</p><p>Step 3: Coupling to the BP with offspring {g j }. Corollary 3.5 combined with Lemma 3.1 yields</p><formula xml:id="formula_97">P 1 L N j+1 i=2</formula><p>Z (1)  ⌈i/2⌉ Z (1)  ⌊i/2⌋ -</p><formula xml:id="formula_98">j+1 i=2 Ẑ(1) ⌈i/2⌉ Ẑ(2) ⌊i/2⌋ &gt; N -ε = O(N -β ).</formula><p>From this result we obtain, as in the first half of step 2,</p><formula xml:id="formula_99">E exp - j+1 i=2 Ẑ(1) ⌊i/2⌋ Ẑ(2) ⌈/2⌉ L N = E exp - j+1 i=2 Z (1)</formula><p>⌈i/2⌉ Z (2)   ⌊i/2⌋</p><formula xml:id="formula_100">L N + O(N -β ),</formula><p>where, as before, β is a generic small positive number. Using (4.1) and the result of step 2, it follows that</p><formula xml:id="formula_101">P(H N &gt; j) = E exp - j+1 i=2 Z (1)</formula><p>⌈i/2⌉ Z (2)   ⌊i/2⌋</p><formula xml:id="formula_102">L N + O(N -β ).</formula><p>To obtain (4.2), we finally replace, again at the cost of an additional term O(N -β ), the random number L N by µN (1 + O(N -a )).</p><p>Step 4: Evaluation of the limit points. We start from (4.2) with j = k + σ N ≤ (1 + 2η) log ν N , where σ N = ⌊log ν N ⌋, to obtain 1)  ⌈i/2⌉ Z (2)   ⌊i/2⌋ µN + O(N -β ). (</p><formula xml:id="formula_103">P(H N &gt; σ N + k) = E exp -σ N +k+1 i=2 Z<label>(</label></formula><p>We write N = ν log ν N = ν σ N -a N , where we recall that a N = ⌊log ν N ⌋ -log ν N . Then</p><formula xml:id="formula_105">σ N +k+1 i=2</formula><p>Z (1)  ⌈i/2⌉ Z (2)   ⌊i/2⌋ µN = µν a N +k σ N +k+1 i=2</p><p>Z (1)  ⌈i/2⌉ Z (2)   ⌊i/2⌋ µ 2 ν σ N +k .</p><p>In the above expression, the factor ν a N prevents proper convergence. Without the factor µν a N +k , we obtain from (1.7), with probability 1, lim</p><formula xml:id="formula_106">N →∞ σ N +k+1 i=2</formula><p>Z (1)  ⌈i/2⌉ Z (2)   ⌊i/2⌋</p><formula xml:id="formula_107">µ 2 ν σ N +k = W (1) W (2) ν -1 .</formula><p>Using (2.3) we conclude that for each α &gt; 0, there is a β &gt; 0 such that</p><formula xml:id="formula_108">P σ N +k+1 i=2</formula><p>Z (1)  ⌈i/2⌉ Z (2)   ⌊i/2⌋</p><formula xml:id="formula_109">µ 2 ν σ N +k - W (1) W (2) ν -1 &gt; O((log N ) -α ) = O(N -β ).</formula><p>Hence, for k ≤ 2η log ν N and each α &gt; 0,</p><formula xml:id="formula_110">P(H N &gt; σ N + k) = E exp{-κν a N +k W (1) W (2) } + O((log N ) -α ),<label>(4.20)</label></formula><p>where κ = µ/(ν -1). This proves (1.9). We proceed by proving (1.4), with R a given in <ref type="bibr">(1.8)</ref>. For this, we need to condition on node 1 and node 2 being connected. Node 1 and node 2 are connected if and only if H N &lt; ∞. Using (4.20), for <ref type="bibr">(1.4)</ref>, it suffices to prove that P(H N &lt; ∞) = q 2 + o(1), where q = P(W (1) &gt; 0).</p><p>(4.21)</p><p>We prove (4.21) using upper and lower bounds. We note that, with k = η log ν N ,</p><formula xml:id="formula_111">P(H N &lt; ∞) ≥ P(H N ≤ σ N + k) = E 1 -exp{-κν a N +k W (1) W (2) } + O((log N ) -α ). (4.22)</formula><p>Therefore,</p><formula xml:id="formula_112">P(H N &lt; ∞) ≥ q 2 E 1 -exp{-κν a N +k W (1) W (2) } W (1) W (2) &gt; 0 + O((log N ) -α ). (<label>4</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.23)</head><p>By dominated convergence, for k = 2η log ν N , the conditional expectation converges to 1, so that indeed P(H N &lt; ∞) ≥ q 2 + o(1). For the upper bound, we rewrite, for any m,</p><formula xml:id="formula_113">P(H N &lt; ∞) = P(H N &lt; ∞, Z (1) m Z (2) m = 0) + P(H N &lt; ∞, Z (1) m Z (2) m &gt; 0). (4.24)</formula><p>The second term is bounded from above by</p><formula xml:id="formula_114">P(H N &lt; ∞, Z (1) m Z (2) m &gt; 0) ≤ P(Z (1) m Z (2) m &gt; 0) = P(Z (1) m Z (2) m &gt; 0) + o(1) = q 2 m + o(1),<label>(4.25)</label></formula><p>where we use Proposition 3.3, and we write q m = P(Z (1)  m &gt; 0). When m → ∞, we have that q m → q, so that we are done when we can show that for any m fixed, P(H N &lt; ∞, Z (1)  m Z (2) m = 0) = o(1). We note that if Z (1)  m Z (2)  m = 0, then H N ≤ m -1. Therefore, using (4.20) with k = m -σ N -1, we conclude</p><formula xml:id="formula_115">P(H N &lt; ∞, Z (1) m Z (2) m = 0) ≤ P(H N ≤ m-1) = E 1-exp{-κν a N +k W (1) W (2) } +o(1) = o(1). (4.26)</formula><p>This completes the proof of (4.21). We finally complete the proof of Theorems 1.1 and 1.4 using (4.21), which, together with (4.20), implies that, for k ≤ 2η log ν N ,</p><formula xml:id="formula_116">P(H N ≤ σ N + k|H N &lt; ∞) = E 1 -exp{-κν a N +k W (1) W (2) }|W (1) W (2) &gt; 0 + o(1). (4.27)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">On the connected components</head><p>In this section, we will investigate the sizes of the connected components and prove Theorem 1.5.</p><p>Proof of Theorem 1.5. In the proof, we will make essential use of the results in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>, where the statement in Theorem 1.5 is proved for certain degree sequences. Indeed, denote by</p><formula xml:id="formula_117">d i (N ) = N j=1 I[D j = i], i = 0, 1, . . . ,<label>(5.1)</label></formula><p>the degree sequence of our random graph G, where D 1 , D 2 , . . . , D N is the i.i.d. sequence with distribution F introduced in (1.1) and satisfying (1.2). In <ref type="bibr" target="#b30">[31]</ref>, the bounds on the connected components in Theorem 1.5 are proved with only a lower bound on the largest connected component size, while in <ref type="bibr" target="#b31">[32]</ref>, the asymptotic size of the largest connected component is determined. Both papers assume a number of hypotheses on the degree sequence {d i (N )} i≥0 . Thus, Theorem 1.5 follows when we can show that the probability that our degree sequences in (5.1) satisfy the restrictions is at least 1 -o(1). In fact, we need to alter the random graph G in a certain way to meet the conditions of Molloy and Reed, and subsequently need to prove that the alteration does not affect the results. We now go over their conditions and definitions. Firstly, the degree sequence needs to be feasible, meaning that there exists at least one graph with the degree sequence. This is true, since L N is even and we have that</p><formula xml:id="formula_118">∞ i=1 id i (N ) = ∞ i=1 i N j=1 I[D j = i] = N j=1 ∞ i=1 iI[D j = i] = N j=1 D j = L N .</formula><p>Secondly, the degree sequence needs to be smooth, meaning that for some sequence λ i , we have lim</p><formula xml:id="formula_119">N →∞ d i (N ) N = λ i .</formula><p>In our setting, this follows almost surely from the law of large numbers, with</p><formula xml:id="formula_120">λ i = f i = P(D = i).</formula><p>Thirdly, and this is the most serious condition, the degree sequence needs to be well-behaved, meaning that it is smooth, feasible, and that for every ǫ ′ , there exists N ′ = N ′ (ǫ ′ ), such that for all N &gt; N ′ , we have that 1.</p><formula xml:id="formula_121">sup i i(i -2) d i (N ) N -i(i -2)λ i &lt; ǫ ′ ; (5.2) 2. there exists i * i * i=1 i(i -2) d i (N ) N - ∞ i=1 i(i -2)λ i ≤ ǫ ′ ;<label>(5.3)</label></formula><p>3. there exists an ǫ &gt; 0 such that d i (N ) = 0 for i ≥ ⌈N</p><formula xml:id="formula_122">for i ≤ N 1 8 -ǫ . For i &gt; N 1 8 -ǫ , we bound d ′ i (N ) ≤ j≥i d j (N )</formula><p>, so that, again using (5.6-5.7),</p><formula xml:id="formula_123">i 2 d ′ i (N ) N - d i (N ) N ≤ 2i 2 N j≥i d j (N ) = 2i 2 (1 -F (i -1))(1 + o(1)) ≤ 2cN ( 1 8 -ε)(3-τ ) → 0.</formula><p>To check (5.3), we first take i * fixed so that</p><formula xml:id="formula_124">∞ i=i * +1 i(i -2)λ i ≤ ǫ ′ /2.</formula><p>(5.12) This is possible, since E[D 2 ] &lt; ∞. Thus, we are left to show that</p><formula xml:id="formula_125">i * i=1 i(i -2) d i (N ) N -λ i ≤ ǫ ′ /2.</formula><p>(5.13)</p><p>In order to do so, we use the bound in (5.10) to obtain that</p><formula xml:id="formula_126">i * i=1 i(i -2) d i (N ) N -λ i ≤ C i * i=1 i 2 f i log N N 1/2 ≤ C(i * ) 3 log N N 1/2 ≤ ǫ ′ /2,<label>(5.14)</label></formula><p>whenever N is sufficiently large. The same result applies to</p><formula xml:id="formula_127">d ′ i (N ), since |d ′ i (N ) -d i (N )| ≤ R N , and R N = o(N ), so that i * i=1 i(i -2) d i (N ) N d ′ i (N ) N | ≤ (i * ) 3 R N N = o(1)</formula><p>.</p><p>Therefore, we have proved all conditions for the graph G ′ , and thus obtain the result in Theorem 1.5 for G ′ . To complete the proof, we need to show that the result for G ′ implies the result for G.</p><p>This implication is proved in several small steps. First, denote the largest connected components of G and G ′ by LC G and LC G ′ . Since G can be obtained from G ′ by adding the removed edges back, we obtain that (since we put back at most R N connected components of size at most γ log N ), To see that all other connected components in G have size at most γ log N , we note that in G ′ the removed edges are all connected to nodes with degree ⌈N 1 4 -ǫ ⌉. We first show that with overwhelming probability these nodes are already in the largest connected component in G ′ . Since in G ′ only the largest connected component has at least N δ nodes for any δ &gt; 0 and since γ log N = o(N δ ), it suffices to check that nodes in G ′ with degree ⌈N 1 4 -ǫ ⌉ are connected to at least N δ other nodes. Since the probability of picking a node different from the ones already connected to the node under observation is bounded from below by 1 -N 2( 1 4 -ǫ)-1 (since all degrees in G ′ are bounded above by ⌈N 1 4 -ǫ ⌉), the probability that at most N δ different nodes are chosen is bounded by the probability that a binomial random variable, with parameters p = 1 -N 2( 1 4 -ǫ)-1 and n = ⌈N 1 4 -ǫ ⌉, is bounded from above by N δ . By <ref type="bibr">(5.5)</ref>, this probability is negligible whenever δ &lt; 1 4 -ǫ. Thus, we may assume that all nodes with degree ⌈N By picking first b small, and then a small, we see that α 2 , β 2 &gt; 0.</p><formula xml:id="formula_128">|LC G ′ | ≤ |LC G | ≤ |LC G ′ | + R N • γ log N. (<label>5</label></formula><p>Remark A.1.3 When (1.2) holds for some τ &gt; 2 (rather than τ &gt; 3), then the above proof can be repeated to show that</p><formula xml:id="formula_129">P ∞ n=0 |g (N ) n -g n | ≥ N -α 2 ≤ N -β 2 . (A.1.11)</formula><p>Indeed, in the definition of the event F in (A.1.3), we can replace (D i + 1) 2 by (D i + 1) in the second event, and (n + 1) 2 by (n + 1) in the third event. Then, by adapting the above argument, the event F implies that ∞ n=0 |g (N ) n -g n | ≤ N -α 2 . The proof that P(F c ) ≤ N -β 2 can be adapted accordingly.</p><p>Proof of Lemma A.1.2. Define a density f (x) = ∞ j=0 f j I[j ≤ x &lt; j + 1], and the corresponding distribution function F (x) = x 0 f (u) du. Then for integer-valued j &gt; 0, F (j) = f 0 + . . . + f j-1 = F (j -1), F (j -1) ≤ F (x) ≤ F (j), x ∈ (j, j + 1). Using partial integration and the upper bound 1 -F (x) ≤ 1 -F (j -1) ≤ c(j -1) 1-τ , for x ∈ (j, j + 1), we conclude that This yields the upper bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Moreover</head><p>We finally prove Corollary 3.5. In order to do so, we first formulate and prove an intermediate result. This result will be followed by the reformulation of Corollary 3.5, which now becomes Corollary A.1.5, and its proof.</p><p>Proposition A.1.4 There exist ε, β, η &gt; 0 such that for all j ≤ ( 1 2 + η) log ν N , as N → ∞,</p><formula xml:id="formula_130">P 1 √ N j i=1</formula><p>Z (1)  i -</p><formula xml:id="formula_131">j i=1 Ẑ<label>(1)</label></formula><p>i &gt; N -ε = O(N -β ). (A.1.12)</p><p>Thus, we are left to deal with the cases where I &lt; j -1. Then, there exists an i &lt; j -1 such that Ẑ(1) i ≥ N for some β &gt; 0. This proves (A.2.11), since the total number of possible i and s with i ≤ s ≤ j is bounded by (log ν N ) 2 . We use Lemma A.2.3 to see that we may include the indicator on A N for any γ &gt; 1/(τ -1). We will use the Chebychev inequality and Lemma A. We first set the stage for the proof by induction in j. Fix η &lt; δ &lt; 2η, and α &gt; 1 2 + η, and define E j = ∀i ≤ j : (1 -N -α ν i ) Ẑ(1) i ≤ Z (1)  i ≤ (1 + N -α ν i ) Ẑ(1) i .</p><p>(A.2.17)</p><p>We will prove by induction that for all j ≤ (  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Corollary 1 . 3 (</head><label>13</label><figDesc>Concentration of the hopcount) Under the assumptions in Theorem 1.1, (i) with probability 1 -o(1) and conditionally on H N &lt; ∞, the random variable H N is in between (1 ± ε) log ν N for any ε &gt; 0;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Theorem 1 . 5 (</head><label>15</label><figDesc>The sizes of the connected components) With probability 1-o(1), the largest connected component in G has qN (1 + o(1)) nodes, and there exists γ &lt; ∞ such that all other connected components have at most γ log N nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Empirical survival functions of the hopcount for τ = 3.5 and the values N = 25, 000, N = 75, 000 (bold) and N = 125, 000, based on samples of size 1, 000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Empirical survival functions of the hopcount for τ = 3.5 and the four values N k = 5, 000ν 2k , k = 0, 1, 2, 3, based on 1, 000 runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Schematic drawing of the growth of the SPG from the node 1 with N = 9 and the updating of the labels. The stubs without labels have label 1. The first line shows the N different degrees. The growth process starts by choosing the first stub of node 1 whose stubs are labeled by 2 as illustrated in the second line, while all the other stubs maintain the label 1. Next, we uniformly choose a stub with label 1 or 2. In the example in line 3, this is the second stub from node 3, whose stubs are labeled by 2 except for the second stub which is labeled 3. The left hand side column visualizes growth of the SPG by the attachment of stub 2 of node 3 to the first stub of node 1. Once an edge is established the paired stubs are labeled 3. In the next step, the next stub of node one is again matched to a uniform stub out of those with label 1 or 2. In the example in line 4, it is the first stub of the last node that will be attached to the second stub of node 1, the next in sequence to be paired. The last line exhibits the result of creating a cycle when the first stub of node 3 is chosen to be attached to the last stub of node 9 (the last node). This process is continued until there are no more stubs with labels 1 or 2. In this example, we have Z(1)  1 = 3 and Z (1) 2 = 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>. 15 ) 3 4 3 4</head><label>1533</label><figDesc>Thus, since |LCG ′ | = qN (1 + o(1)) and R N ≤ 2N with probability 1 + o(1), we obtain that qN (1 + o(1)) ≤ |LC G | ≤ qN (1 + o(1)) + O(N log N ) = qN (1 + o(1)), (5.16) so that the largest connected component has size qN (1 + o(1)) with probability 1 + o(1), as claimed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>1 4</head><label>1</label><figDesc>-ǫ ⌉ are in the largest connected component in G ′ . Therefore, we obtain that the nodes that must be added to G ′ to form G are attached to the largest connected component of G ′ . Thus, the size of the second largest connected component of G is bounded from above by the size of the second largest connected component of G ′ , which is bounded from above by γ log N .where in the last inequalities, we have used Lemma A.1.2 andE N i=1 [I[D i = n + 1] -f n+1 ] 2 = Var N i=1 I[D i = n + 1] = N f n+1 (1 -f n+1 ) ≤ N f n+1 .Thus, we obtain the statement in Proposition A.1.1 with β 2 = min{1/2 -b -a max{1, 6 -τ }/2, a(τ -3) -b, (2α -1)}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>1) s f j+1 ≤ n+2 m+1 x s f (x) dx = -n+2 m+1 x s d(1 -F (x)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>( 1 -</head><label>1</label><figDesc>1) s f j+1 ≤ (m + 1) s (1 -F (m + 1)) -(n + 2) s (1 -F (n + 2)) + n+2 m+1 F (x)) dx s ≤ c m 1+s-τ + n+1 m y s-τ dy .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>3 2</head><label>3</label><figDesc>δ , A N ](ν N -1) -2 Var N (d 1 ) Ẑ(1) s ≤ CN (4-τ ) + γ-1 2 + 3 2 δ ≤ N -β , with C = 2(ν -1) -2, and since (4 -τ ) + γ &lt; 1/2 and δ &gt; 0 can be taken arbitrarily small.We are now ready to give the proof of Proposition A.2.1. Proof of Proposition A.2.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>1 2For j &lt; ( 1 2 - 1 2 1 2≤ 1 2≤</head><label>12111</label><figDesc>+δ ≤ CjN -β . 2η) log ν N , we bound -δ ≤ N -β + CN -2η+δ ,by the Markov inequality and using Proposition 3.4 in a similar way as in Lemma A.2.4. Hence, the statement in (A.2.18) follows for j &lt; ( 1 2 -2η) log ν N . This initializes the induction in j. To advance the induction, we boundP E c j , N P(E c j-1 ) + P E c j ∩ E j-1 , N C(j -1)N -β + P E c j ∩ E j-1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>-2δ . Thus, there must be a first s ≥ i such that Ẑ(1) s+1 ≤ Ẑ<ref type="bibr" target="#b0">(1)</ref> </figDesc><table><row><cell cols="3">1 2 -3 2 δ , but Ẑ(1) j-1 ≤ N Consequently, Ẑ(1) s ≥ Ẑ(1) i ≥ N 1 2 -3 2 δ . We will bound, uniformly in s, 1 2 s .</cell></row><row><cell>P( Ẑ(1) s+1 ≤ Ẑ(1) s , Ẑ(1) s ≥ N</cell><cell>1 2 -3 2 δ ≤ N -β ,</cell><cell>(A.2.15)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>1 2 + η) log ν N , P E c j ≤ CjN -β , (A.2.18) which implies Proposition A.2.1 by taking the complementary event. First, by Lemma A.2.2 and A.2.4 and since η &lt; δ we see that it is sufficient to prove for j ≤ ( 1 2 + η) log ν N ,</figDesc><table><row><cell>P E c j , N</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. The work of RvdH was supported in part by Netherlands Organisation for Scientific Research (NWO). We thank Dmitri Znamenski for the Figures <ref type="figure">1</ref> and<ref type="figure">2</ref> and for useful comments on a previous version. We thank the two referees for many suggestions that improved on the readability of the paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We start with the last assumption, which is not satisfied by our graph. Indeed, the last restriction means that all nodes have degree at most ⌈N 1 4 -ǫ ⌉ -1. We will first alter the graph, and thus the degree sequences, in the following way. Fix ǫ &gt; 0 small. For nodes j with D j ≥ ⌈N 1 4 -ǫ ⌉, we remove D j -⌈N 1 4 -ǫ ⌉ + 1 edges. We do this by first removing in a uniform way edges between pairs i, j where the degrees of D i and D j both exceed ⌈N 1 4 -ǫ ⌉ -1. When there are no more edges between nodes with degrees exceeding ⌈N 1 4 -ǫ ⌉ -1, we remove edges uniformly from the nodes with degrees exceeding ⌈N 1 4 -ǫ ⌉-1. Thus, we end up with a graph G ′ such that all degrees are at most ⌈N 1 4 -ǫ ⌉-1. Moreover each node j for which D j ≥ ⌈N 1 4 -ǫ ⌉ has degree equal to ⌈N 1 4 -ǫ ⌉ -1 in the altered graph G ′ . This will be the graph to which we apply the results of Molloy and Reed. Let D ′ j be the degree of the node j in G ′ , and write d ′ i (N ) for the number of nodes with degree equal to i in G ′ . Then d ′ i (N ) = 0 for i ≥ ⌈N 1 4 -ǫ ⌉, as required. We first compute the number of removed edges, which we denote by R N . Its expectation is bounded above by</p><p>for τ &gt; 3 and ǫ sufficiently small. We are hence removing only a fraction of the L N available edges and all degrees go down (see Lemma 3.1 that L N is close to µN ). Moreover, with probability converging to one, we have that R N ≤ 2N 3 4 , since by a computation analogous to the one given above for E[R N ], we have Var(R N ) ≤ CN 1-(τ -3)( 1 4 -ǫ) , so that by the Chebychev inequality,</p><p>We start by checking <ref type="bibr">(5.2)</ref> for the graph G ′ , with λ i = f i in <ref type="bibr">(1.1)</ref>. For this, we will use the following bound from [9, Corollary 1.4(i)], which states that if S N is binomial with parameters N and p, and if x = (N p(1 -p)) 1/2 ≥ 1, then</p><p>(5.5)</p><p>We first check condition (5.2) for i = ⌈N Hence, d ′ i (N ) is a binomial random variable with parameters N and p = 1 -F (⌈N 1 4 -ǫ ⌉ -2). Thus, by <ref type="bibr">(5.5)</ref>, with x = C √ log N , we have that</p><p>(5.7)</p><p>Thus, we have that for i = ⌈N</p><p>for τ &gt; 3. This proves (5.2) for i = ⌈N 1 4 -ǫ ⌉ -1. We next prove (5.2) for i &lt; ⌈N 1 4 -ǫ ⌉ -1. For this, we use the triangle inequality</p><p>and we bound these two terms separately. We start with the second term, and use (5.5), which gives that</p><p>(5.9)</p><p>We will take C &gt; 2, so that</p><p>(5.10)</p><p>On the complementary event, we have that</p><p>(5.11)</p><p>Thus, we have bounded the second term in <ref type="bibr">(5.8)</ref>. We next turn to the first term in <ref type="bibr">(5.8)</ref>. First, we clearly have that</p><p>A.1 Proof of Proposition 3.4</p><p>In this part of the appendix, we prove Proposition 3.4, which we restate here for convenience as Proposition A.1.1. At the end of this section, we restate and prove Corollary 3.5.</p><p>Proposition A.1.1 There exist α 2 , β 2 &gt; 0 such that</p><p>In the proof, we need the following lemma.</p><p>Lemma A.1.2 Fix τ &gt; 1. For each non-negative integer s, there exists a constant C &gt; 0, such that n j=m</p><p>where</p><p>We defer the proof of Lemma A.1.2 to the end of this section. Proof of Proposition A.1.1. Fix a, b, α &gt; 0. Define</p><p>The constants a, b and α will be chosen appropriately in the proof. The strategy of the proof is as follows. We will prove that</p><p>for some β 2 &gt; 0, and that on F ,</p><p>for some α 2 . This proves Proposition A.1.1. We start by showing (A.1.5). We bound</p><p>The second term is bounded by (ν + 1)N -α by the first event in F . The first term in (A.1.6) can be bounded, for N sufficiently large, as, again using the first event in F ,</p><p>We next split the sum over n into n &gt; N a and n ≤ N a for some appropriately chosen a ∈ (0, 1]. On F , the contribution from n ≤ N a is at most 1 µ N -b , whereas we can bound the contribution from n &gt; N a by 2 µN</p><p>For τ &gt; 3, the second term is bounded by CN -a(τ -3) by Lemma A.1.2. The first term is bounded by µ 2 N -b by the second event in F . Thus, we obtain (A.1.5) with α 2 = min{b, a(τ -3)}. We now prove (A. <ref type="bibr">1.4)</ref>. For this, we use that F is an intersection of three events which we will write as F 1 , F 2 and F 3 , so that</p><p>The first probability is bounded by P(F c 1 ) ≤ c•N 2α-1 , by Lemma 3.1. For P(F c 2 ), we use the Markov inequality, to obtain that</p><p>by Lemma A.1.2. For P(F c 3 ), we use in turn the Markov inequality, Cauchy-Schwarz in the form</p><p>, and the Jensen inequality applied to x → √ x (a concave function), to obtain</p><p>then according to Proposition A.1.1 we have P(F c N ) ≤ N -β 2 . We claim that for all i ≥ 1,</p><p>where</p><p>n -min{g n , g (N )  n } .</p><p>(A.1.15) We first prove (A. <ref type="bibr">1.14)</ref>. For Z (1)   i = Ẑ <ref type="bibr" target="#b0">(1)</ref> i , the coupling is not successful in at least one of the generations m, 1 ≤ m ≤ i. Let m be the first generation for which the coupling is unsuccessful. There are at most Ẑ <ref type="bibr" target="#b0">(1)</ref> m nodes for which the coupling can fail. If the coupling fails for a node, the expected difference between the offspring of that node is bounded above by max{ν -α N , ν N -α N }. Finally, from generation m+1 on, we again have two BP's with laws g and g (N ) , so that the expected offspring is bounded by (max{ν, ν N }) i-m . This demonstrates the claim (A. <ref type="bibr">1.14)</ref>.</p><p>Furthermore, since</p><p>, we end up with</p><p>Hence, for j ≤ ( 1 2 + η) log ν N , using the abbreviation</p><p>we have</p><p>From (A. <ref type="bibr">1.16</ref>) and the estimates on F N , we obtain</p><p>,</p><p>) can be bounded by any small power of N , and ε and η can both be taken arbitrarily small, whereas α 2 &gt; 0.</p><p>We finally restate and prove Corollary 3.5.</p><p>Corollary A.1.5 There exist ε, β, η &gt; 0 such that for all j ≤ (1 + 2η) log ν N , as N → ∞,</p><p>Z (1)  ⌈i/2⌉ Z (2)  ⌊i/2⌋ -</p><p>Proof. Bound j i=1 Z (1)  ⌈i/2⌉ Z (2)   ⌊i/2⌋</p><p>N -</p><p>Both terms on the right hand side of (A. <ref type="bibr">1.19</ref>) can be treated as in the proof of Proposition A.1.4, because the processes with sources ( <ref type="formula">1</ref>) and ( <ref type="formula">2</ref>) are independent and uniformly in i</p><p>The right-hand side can again be bounded by any small power of N by taking η arbitrarily small. We omit further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof of Proposition 3.2</head><p>In this second part of the appendix, we restate our main result on the coupling between the SPG and the BP with offspring distribution {g (N )  n } once more and give a full proof.</p><p>Proposition A.2.1 There exist η, β &gt; 0, α &gt; 1 2 + η and a constant C, such that for all j ≤ ( 1 2 + η) log ν N ,</p><p>This proof is divided into several lemmas. It is rather involved, and we may think of Proposition A.2.1 as one of the key estimates of the paper. We start with an explanation of the different steps in this proof. The proof of Proposition A.2.1 proceeds by induction with respect to j. Note that for all j ≤ ( 1 2 + η) log ν N , we have N -α ν j ≤ N ( 1 2 +η)-α → 0, as N → ∞ and when α &gt; η. When at level j -1, the event in the statement of the proposition holds, we have</p><p>so that we control the difference between the number of stubs Z (1)  j-1 and the number of children Ẑ(1) j-1 . The absolute value of this difference is bounded by Ẑ(1) j-1 times a fraction that converges to 0. For generation j we have to control the difference Ẑ(1) j -Z (1)  j . Differences in generation j arise from differences in generation j -1 and from drawing stubs with label 2 or label 3. If a label 2 stub is chosen, then the SPG will contain a loop or cycle and hence no free stubs in level j are created, whereas in the BP a non-negative number of offspring is attached. If a label 3 stub is chosen, then the corresponding node with described number of children is attached in the BP, whereas for the SPG we have to resample until we draw a stub labeled 1 or 2. Hence, if Z (1)  j ≥ Ẑ(1) j , so that the number of free stubs attached to nodes at distance j -1 of the SPG exceeds the number of children in generation j of the BP, then this overshoot can only be caused by drawing label 3 stubs. The number of stubs with label 3 is bounded by the total number drawn in the SPG, i.e., by</p><p>For Z (1)  j ≤ Ẑ(1) j , the number of stubs with level 2 or 3 both matter and their total amount is bounded by</p><p>In both cases the probability of drawing a label 2 or 3 stub is bounded by</p><p>on the event where</p><p>), this probability is sufficiently small to allow us to use Chebychev's inequality.</p><p>The main lemmas in this section are Lemma A.2.7 and Lemma A.2.9. Together, they prove the induction step described above. Lemmas A.2.2 up to A.2.6 are preparations, the most important one being Lemma A.2.6. This lemma shows that if the total progeny up to and including generation j of { Ẑ <ref type="bibr" target="#b0">(1)</ref> i } is larger than N 1 2 -δ , for some δ &gt; 0, then with overwhelming probability also each of the sizes of the last two generations, i.e., Ẑ(1) j-1 and Ẑ(1) j , exceed N 1 2 -2δ . As before, we will abbreviate the conditional probability and expectation given D 1 , . . . , D N by P N and E N .</p><p>Lemma A.2.2 For 0 &lt; η &lt; 1  2 and all j ≥ 1,</p><p>Lemma A.2.2 together with Lemma 3.1 prove Proposition A.2.1 for all j such that the total size of the BP is at most N 1 2 -η . Proof. We denote by l the first stub which is grown differently in the SPG and in the BP. Assume that this l th stub is in the j th generation or earlier.</p><p>Before the growth of the l th stub, the BP and the SPG are identical. Thus, we must have that l ≤ j i=1 Ẑ <ref type="bibr" target="#b0">(1)</ref> i . Hence, as we reach to the l th stub, the number of stubs having either label 2 or 3 is bounded above by</p><p>A difference in the SPG and the BP can only arise when we draw a stub for the BP having label 2 or 3. Thus, the probability that the l th stub is the first to create a difference between the SPG and the BP is bounded above by N 1 2 -η /L N . Therefore,</p><p>n is the expected offspring of the BP { Ẑ(1) } j under P N . Note from Proposition A.1.1 that ν N is close to ν with probability close to one. In the statement of the next lemma, we write</p><p>Proof. We use Boole's inequality to obtain from (1.2) that</p><p>), and all j ≤ ( 1 2 + η) log ν N , there exists β 2 &gt; 0 such that</p><p>Proof. By Proposition 3.4, we can include the indicator that |ν N -ν| ≤ N -α 2 ; this explains the additional error term N -β 2 . By the Markov inequality, we obtain for j ≤ ( 1 2 + η) log ν N ,</p><p>The expectation on the right-hand side can be computed by conditioning:</p><p>Hence,</p><p>In the lemma below, we write d for a random variable with discrete distribution {g (N ) n } given in (3.1), and Var N (d) for the variance of d under P N . Furthermore, we let, for any 0 &lt; a &lt; 1 2 ,</p><p>then, according to Proposition 3.4, Lemmas 3.1 and A.2.3, we have</p><p>where ǫ = b ∧ ((τ -1)γ -1) ∧ β 2 &gt; 0 whenever γ &gt; 1/(τ -1). On A N , we have</p><p>This will be used in the following lemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10)</head><p>where x + = max(0, x).</p><p>Proof. Since the variance of a random variable is bounded by its second moment,</p><p>and so, for τ ∈ <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b3">4]</ref>,</p><p>by Lemma A.1.2. For τ &gt; 4, the third moment of D is finite, and the result is also true even without the indicator</p><p>Lemma A.2.6 For all ( 1 2 -2η) log ν N ≤ j ≤ ( 1 2 + 2η) log ν N , there exists δ, β &gt; 0 such that</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12)</head><p>Remark: The statements of the lemma are almost identical, the difference being that the index of Ẑ(1) j-1 in the first statement is replaced by the index j in the second statement. We will be satisfied with a proof for the first statement only, the proof with index j is a straightforward extension.</p><p>We write I for the first i ≤ j such that Ẑ(1)</p><p>The contribution from I = j -1 is 0. When I = j, then Ẑ(1)</p><p>2 -2δ so that from the Markov inequality</p><p>where the last inequality follows by the induction hypothesis. Thus, it suffices to prove that</p><p>where</p><p>Note that</p><p>We write the disjoint events on the right-hand side of (A.2.20) as E c j,&lt; and E c j,&gt; and bound the probability of these events separately. We will start with E c j,&lt; . This result is stated in the following lemma:</p><p>Lemma A.2.7 There exists β &gt; 0 such that for all</p><p>Proof. We note that on E c j,&lt; , we have that</p><p>because α &gt; 1 2 + η. Thus, for every stub which is grown simultaneously for the BP and the SPG, there is a probability bounded from above by 2N 1 2 +δ /L N that a difference is created between the BP and the SPG (such a difference is called a miscoupling). Denote by U the number of stubs where such a difference occurs. Then, U is bounded from above by a binomial random variable with n = N 1 2 +δ and p = 2N 1 2 +δ /L N . Thus, by the Markov inequality, we have,</p><p>Using (A.2.9), we obtain, for 2δ &lt; a,</p><p>Observe that differences between Z (1) j and Ẑ(1) j can only arise through (i) different numbers of stubs in the (j -1) st generation, and (ii) differences created in the j th generation which we previously called miscouplings. In the first case, the difference in the number of stubs is bounded from below by an independent draw from g (N ) . A miscoupling occurs if we draw a stub with label 2 or 3. Hence, Z (1)  j -Ẑ(1) j ≥ -</p><p>where {d i } i≥1 are independent draws from g (N ) and { di } i≥1 are draws conditionally on drawing a stub labeled 2 or 3. On E ′ j-1 , we have that Ẑ(1) j-1 -Z (1)   j-1</p><p>Combining this with (A.2.22) and using the definition of α N ,j , we see that in order to prove (A.2.21) it suffices to show that</p><p>We will first show that on E ′ j-1 the term</p><p>i=1 di is small compared to N -α ν j Ẑ(1) j , if we choose a sufficiently small. On E ′ j-1 , we have j i=1 Z (1)   i ≥ N 1 2 -δ , and so, with probability larger than 1 -CN -β , according to Lemma A.2.6, we have that also Z (1)  j ≥ N 1 2 -2δ . Hence,</p><p>2 , but can be taken arbitrary close to 1 2 . Since τ &gt; 3, we then have that cN 1-(τ -1)γ &lt; N -β .</p><p>Hence it suffices to prove the statement in (A.2.26) without the term</p><p>Since we can write Ẑ(1)</p><p>i=1 d i , and, using again Lemma A.2.6, we have that</p><p>, with probability larger than 1 -CN -β , it is sufficient to prove that</p><p>Therefore, by the Chebychev inequality,</p><p>We use Lemma A.2.5. Hence, by intersecting with the event I[A N ] and its complement, and using (A.2.8), we obtain for j ≥ ( 1 2 -2η) log ν N ,</p><p>by fixing α &gt; 1 2 + η so that the exponent is negative (using that γ &lt; 1 2 and (4 -τ ) + ≤ 1), and writing β = 3  2 -2α -2δ -4η -(4 -τ ) + γ &gt; 0. This proves (A.2.28) and completes the proof of Lemma A.2.7.</p><p>Before turning to the proof of the bound on P(E c j,&gt; ) in Lemma A.2.9 below, we start with a preparatory lemma and some definitions. Suppose we have L objects divided into N groups of sizes d 1 , . . . , d N , so that L = N i=1 d i . Suppose we draw an object at random, and we define a random variable by d I -1 when the object is taken from the I th group. This gives a distribution g ( d) , i.e.,</p><p>Clearly, g (N ) = g ( D) , where D = (D 1 , . . . , D N ).</p><p>We next label M of the L objects, and suppose that the distribution g ( d) (M ) is obtained in a similar way from drawing conditionally on drawing an unlabelled object. More precisely, we remove the labelled objects from all objects thus creating new d ′ 1 , . . . , d ′ N , d ′ i = L -M , and we let g ( d) (M ) = g ( d ′ ) . Even though this is not indicated, the law g ( d) (M ) depends on what objects have been labelled.</p><p>Lemma A.2.8 below shows that the law g ( d) (M ) can be bounded above and below by two specific ways of labeling the M objects. Before we can state the lemma, we need to describe those specific labellings.</p><p>For a vector d, we let d (1) , . . . , d (N ) be the ordered vector, so that d (1) = min i=1,...,N d i and d (N ) = max i=1,...,N d i . Then the laws f ( d) (M ) and h ( d) (M ), respectively, are defined by successively decreasing d (N ) and d (1) respectively, by one. Thus,</p><p>For f ( d) (M ) and h ( d) (M ), respectively, we repeat the above change M times. Here we note that when d (1) = 1, and for h ( d) (1) we decrease it by one, that we only keep the d i ≥ 1. Thus, in this case, the number of groups of objects is decreased by 1.</p><p>Finally, we write that f g when the distribution f is stochastically dominated by g, i.e., when n i=0 f i ≥ n i=0 g i for all n ≥ 0. Similarly, we write that X Y when for the probability mass functions f X , f Y we have that f X f Y .</p><p>We next prove stochastic bounds on the distribution g ( d) (M ) that are uniform in the choice of the M labelled objects.</p><p>Lemma A.2.8 For all choices of M labelled objects</p><p>Thus, the expectation and variance of the random variable X(M ) with probability mass function g</p><p>where X(M ) has probability mass function h ( d) (M ). Moreover, when X 1 , . . . , X l are draws from g ( d) (M 1 ), . . . , g ( d) (M l ), where the only dependence between the X i resides in the labelled objects, then</p><p>where {X i } l i=1 and {X i } l i=1 , respectively, are i.i.d. copies of X and X with laws f ( d) (M ) and h ( d) (M ) for M = max l i=1 M i , respectively.</p><p>In the proof of Proposition A.2.1, we will only use the upper bounds in Lemma A.2.8.</p><p>Proof. In order to prove (A.2.32), we will use induction in M . We note that f d) , and this initializes the induction. To advance the induction, we note that we need to investigate the effect of labelling one extra object. For f ( d) (M ), we need to maximize the cumulative distribution function, whereas for h ( d) (M ), we need to minimize it. Clearly, (A.2.30-A.2.31) are optimal. This advances the induction. The statement in (A.2.33) follows from (A.2.32) To prove (A.2.34), we see that for every j, conditionally on the 'past' (X 1 , . . . , X j-1 ), the random variable X j is stochastically bounded by X j and X j , respectively. This completes the proof of Lemma A.2.8. Lemma A.2.9 There exists β &gt; 0 such that for all j ≤ ( 1 2 + η) log ν N ,</p><p>Proof. The proof of Lemma A.2.9 follows the proof of Lemma A.2.7, and we focus on the differences only. Let V denote the number of stubs out of the Ẑ(1) j-1 stubs that are attached to stubs with label 3 in the BP. Since for each stub in the (j -1) st generation, on E ′ j-1 , we have that there are at most 2 j-1 i=1 Z (1)  i ≤ 2N 1 2 +δ stubs with label 3, we have that V is bounded from above by a binomial random variable with n = N 1 2 +δ and p = 2N 1 2 +δ /L N . Thus, by the Markov inequality, we have that for any a &gt; 2δ,</p><p>where we can take a arbitrarily small by choosing δ &gt; 0 small. We thus assume that V ≤ N a . We next proceed by investigating P(E c j,&gt; ). Now, on E c j,&gt; ∩ E j-1 , we have that Z (1)  j &gt; (1 + N -α ν j ) Ẑ(1) j .</p><p>(A.2.37) Thus, Z (1)  j is larger than Ẑ(1) j . We note that Z (1)  j can only become larger than Ẑ(1) j from (a) a redraw and the redraw exceeds the original draw from g (N ) ; and (b) stubs in Z (1)  j-1 that are not in Ẑ(1)</p><p>which give rise to new stubs. On E j-1 , we thus have that (recalling that α N,j = N -α ν j-1 Ẑ(1) j-1 )</p><p>where d ′ i , d ′′ i are drawn from the appropriate conditional distributions given that we pick a stub with label unequal to 3.</p><p>We note that each of the d ′ i , d ′′ i is obtained by drawing from stubs conditionally on labels not being 3. Since the total number of stubs labeled 3 is throughout the growth process bounded above by 2 j-1 i=1 Z (1)  i ≤ 2N 1 2 +δ , on V ≤ N a , we obtain that by Lemma A.2.8, {d ′ i } α N ,j i=1 and {d ′′ i } V i=1 are bounded above by α N,j + ⌈N a ⌉ independent copies of X i (2N 1 2 +δ ), where for any M , X i (M ) has probability distribution h ( D) (M ).</p><p>We note that by (A.2.33) and Proposition 3.4, the expectation of X i (2N 1 2 +δ ) is bounded above by ν + N -α 2 for some α 2 &gt; 0, and the variance of X i (2N 1 2 +δ ) obeys the same bound as Var N (d) in Lemma A.2.5. Thus, we can copy the remaining part of the proof from the proof of Lemma A.2.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Proof of Proposition 3.3</head><p>In this section, we prove Proposition 3.3. In fact, we will prove a slightly different result, as formulated in the next proposition. This proposition summarizes the coupling results, and will be instrumental both in this paper, as well as in <ref type="bibr" target="#b24">[25]</ref>, in which we investigate the case where τ ∈ (2, 3). there exist independent branching processes Z (1) , Z (2) , such that By the coupling between Ẑ(i) m and Z (i) m , a miscoupling occurs with probability equal to p N defined in <ref type="bibr">(3.6)</ref>. Therefore, by Remark A.1.3, the probability of a miscoupling for the offspring of a given individual is bounded from above by N -α 2 with probability 1 + O(N -β 2 ). On the event that m j=1 Ẑ(i) j &lt; N η , the number of individuals that need to be coupled is bounded from above by N η . We thus obtain that for any η &lt; α 2 , <ref type="formula">1</ref>), (A. <ref type="bibr">3.4)</ref> which completes the proof.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A random graph model for power law graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Aiello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Experiment. Math</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="66" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Random evolution of massive graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Aiello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Massive Data Sets</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Abello</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Pardalos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">G C</forename><surname>Resende</surname></persName>
		</editor>
		<meeting><address><addrLine>Dordrecht</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="97" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Emergence of Scaling in Random Networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barabási</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="page" from="509" to="512" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Statistical mechanics of complex networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barabási</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rev. Mod. Phys</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="47" to="97" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The Probabilistic Method, 2nd Edition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Spencer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>John Wiley and Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Some martingale methods in the limit theory of supercritical branching processes, Branching processes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Asmussen</surname></persName>
		</author>
		<editor>A. Joffe and P. Ney</editor>
		<imprint>
			<date type="published" when="1978">1978</date>
			<publisher>Marcel Dekker</publisher>
			<biblScope unit="page" from="1" to="26" />
			<pubPlace>New York and Basel</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Athreya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Ney</surname></persName>
		</author>
		<title level="m">Branching Processes</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Linked: The New Science of Networks</title>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barabási</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">Massachusetts, 2002</date>
			<publisher>Perseus Publishing</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Random Graphs, 2nd edition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bollobás</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Academic Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Directed scale-free graphs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bollobás</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Borgs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Chayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Riordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms<address><addrLine>Baltimore, MD; New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="page" from="132" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The diameter of scale-free random graphs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bollobás</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Riordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>University of Memphis</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Preprint Dept. of Math. Sciences</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Mathematical results on scale-free random graphs. Handbook of graphs and networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bollobás</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Riordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Wiley-VCH</publisher>
			<biblScope unit="page" from="1" to="34" />
			<pubPlace>Weinheim</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Coupling scale-free and classical random graphs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bollobás</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Riordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The degree sequence of a scale-free random graph process</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bollobás</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Riordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Spencer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tusnády</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Random Structures Algorithms</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="279" to="290" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The diameter of random regular graphs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bollobás</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><surname>Vega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="125" to="134" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Wiener Graph Structure in the Web</title>
		<author>
			<persName><forename type="first">A</forename><surname>Broder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Maghoul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tomkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Networks</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="309" to="320" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The average distances in random graphs with given expected degrees</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PNAS</title>
		<imprint>
			<biblScope unit="issue">25</biblScope>
			<biblScope unit="page" from="15879" to="15882" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Connected components in random graphs with given expected degree sequences</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Combinatorics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="125" to="145" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A general model of web graphs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Frieze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Random Structures Algorithms</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="311" to="335" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pseudofractal scale-free web</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Dorogovtsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Goltsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F F</forename><surname>Mendes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">66122</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Metric structure of random networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Dorogovtsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F F</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Samuhkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucl. Phys. B</title>
		<imprint>
			<biblScope unit="volume">653</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On power-law relationships of the internet topology</title>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Communications Rev</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="251" to="262" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">An Introduction to Probability Theory and Its Applications</title>
		<author>
			<persName><forename type="first">W</forename><surname>Feller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
			<publisher>John Wiley and Sons</publisher>
			<biblScope unit="volume">II</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Distances in random graphs with finite mean and infinite variance degrees</title>
		<author>
			<persName><forename type="first">R</forename><surname>Van Der Hofstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hooghiemstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Znamenski</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In preparation</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Distances in random graphs with infinite mean degrees</title>
		<author>
			<persName><forename type="first">R</forename><surname>Van Der Hofstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hooghiemstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Znamenski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Van Der Hofstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hooghiemstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Znamenski</surname></persName>
		</author>
		<title level="m">Connected components in random graphs with i.i.d. degrees</title>
		<imprint/>
	</monogr>
	<note>In preparation</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">On the mean value of the logarithm of a martingale limit in branching processes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hooghiemstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Van Mieghem</surname></persName>
		</author>
		<ptr target="http://ssor.twi.tudelft.nl∼gerardh/" />
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Random Graphs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Janson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rucinski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Trawling the Web for emerging cyber communities</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tomkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Networks</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1481" to="1493" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sivakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tomkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<title level="m">Stochastic Models for the Web Graph. 42st Annual IEEE Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="57" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A critical point for random graphs with a given degree sequence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Random Structures and Algorithms</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="161" to="179" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The size of the giant component of a random graph with a given degree sequence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combin. Probab. Comput</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="295" to="305" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The structure and function of complex networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E J</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="256" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">End-to-end routing behaviour in the Internet</title>
		<author>
			<persName><forename type="first">V</forename><surname>Paxson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transac. Networking</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="601" to="615" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Random graphs with arbitrary degree distribution and their application</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E J</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Strogatz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Watts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On the power law random graph model of massive data networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Reittu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Norros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Performance Evalution</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="3" to="23" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Exploring complex networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Strogatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">410</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="268" to="276" />
			<date type="published" when="2001-03">March 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Tangmunarunkit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Govindan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jamin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><surname>Willinger</surname></persName>
		</author>
		<title level="m">Network topology generators: Degree-based vs. structural. ACM Sigcomm&apos;02</title>
		<meeting><address><addrLine>Pittsburgh, Pennsylvania; USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="19" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Thorisson</surname></persName>
		</author>
		<title level="m">Coupling, Stationarity and Regeneration</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A scaling law for the hopcount</title>
		<author>
			<persName><forename type="first">P</forename><surname>Van Mieghem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hooghiemstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Der Hofstad</surname></persName>
		</author>
		<ptr target="http://www.nas.its.tudelft.nl/people/Piet/papers/hopcount.pdf" />
		<imprint>
			<date>2000125</date>
			<pubPlace>Delft, The Netherlands</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Delft University of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Report</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Small Worlds, The Dynamics of Networks between Order and Randomness</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Watts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Princeton University Press</publisher>
			<pubPlace>Princeton, New Jersey</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Williams</surname></persName>
		</author>
		<title level="m">Diffussions, Markov Processes and Martingales</title>
		<imprint>
			<publisher>John Wiley and Sons</publisher>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
