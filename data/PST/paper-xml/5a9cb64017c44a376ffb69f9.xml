<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Leveraging Content Sensitiveness and User Trustworthiness to Recommend Fine-Grained Privacy Settings for Social Image Sharing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Jun</forename><surname>Yu</surname></persName>
							<email>yujun@hdu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Zhenzhong</forename><surname>Kuang</surname></persName>
							<email>zzkuang@hdu.edu.cn</email>
							<idno type="ORCID">0000-0002-8768-8324</idno>
						</author>
						<author>
							<persName><forename type="first">Baopeng</forename><surname>Zhang</surname></persName>
							<email>bpzhang@bjtu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dan</forename><surname>Lin</surname></persName>
							<email>lindan@mst.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jianping</forename><surname>Fan</surname></persName>
							<email>jfan@uncc.edu</email>
						</author>
						<author>
							<persName><roleName>Prof</roleName><forename type="first">D</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Rocha</surname></persName>
						</author>
						<author>
							<persName><surname>Yu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Hangzhou Dianzi University</orgName>
								<address>
									<postCode>310018</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">UNC-Charlotte</orgName>
								<address>
									<postCode>28223</postCode>
									<settlement>Charlotte</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Computer and Information Technol-ogy</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<postCode>100044</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">UNC-Charlotte</orgName>
								<address>
									<postCode>28223</postCode>
									<settlement>Charlotte</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">UNC-Charlotte</orgName>
								<address>
									<postCode>28223</postCode>
									<settlement>Charlotte</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Missouri University of Science and Technology</orgName>
								<address>
									<postCode>65409</postCode>
									<settlement>Rolla</settlement>
									<region>MO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">UNC-Charlotte</orgName>
								<address>
									<postCode>28223</postCode>
									<settlement>Charlotte</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Leveraging Content Sensitiveness and User Trustworthiness to Recommend Fine-Grained Privacy Settings for Social Image Sharing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B562D2847744DC656A62FDC63483DBBD</idno>
					<idno type="DOI">10.1109/TIFS.2017.2787986</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Privacy setting recommendation</term>
					<term>image content sensitiveness</term>
					<term>user trustworthiness</term>
					<term>deep multiple instance learning</term>
					<term>tree classifier</term>
					<term>social image sharing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To configure successful privacy settings for social image sharing, two issues are inseparable: 1) content sensitiveness of the images being shared; and 2) trustworthiness of the users being granted to see the images. This paper aims to consider these two inseparable issues simultaneously to recommend fine-grained privacy settings for social image sharing. For achieving more compact representation of image content sensitiveness (privacy), two approaches are developed: 1) a deep network is adapted to extract 1024-D discriminative deep features; and 2) a deep multiple instance learning algorithm is adopted to identify 280 privacysensitive object classes and events. Second, users on the social network are clustered into a set of representative social groups to generate a discriminative dictionary for user trustworthiness characterization. Finally, both the image content sensitiveness and the user trustworthiness are integrated to train a tree classifier to recommend fine-grained privacy settings for social image sharing. Our experimental studies have demonstrated both the efficiency and the effectiveness of our proposed algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>activities and image sharing has now become very popular on social platforms like Facebook, Myspace, Flickr, and Instagram <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b9">[10]</ref>. Since social images can intuitively tell when and where a special moment took place, who participated and what were their relationships, the shared images can reveal much of users' personal and social environments and their private lives <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b7">[8]</ref>. In addition, social network sites may nowadays abuse the technologies of artificial intelligence and facial recognition on automatically tagging objects of interest and human faces <ref type="bibr" target="#b58">[59]</ref>- <ref type="bibr" target="#b68">[69]</ref>. Thus, privacy protection is a critical issue to be addressed during social image sharing <ref type="bibr" target="#b5">[6]</ref>.</p><p>To ensure privacy, most social sites for image sharing allow users to manually specify coarse-grained privacy settings: whether an image is public, private or visible to their family members or friends. Due to the lack of privacy knowledge, it is not easy for common users to correctly configure privacy settings to achieve their desired levels of privacy protection; also, given the large number of images being shared and the tedious steps needed for privacy settings, some users may not be willing to spend extra time on providing their fine-grained privacy settings for image sharing. To reduce users' additional burdens on configuring the privacy settings manually, it is very attractive to develop new techniques that are able to recommend fine-grained privacy settings for social image sharing.</p><p>It is worth noting that the visual properties of the images are the most important resource that can be used to characterize the image content sensitiveness (privacy) <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b37">[38]</ref>: (a) sharing the images with privacy-sensitive objects (persons and others such as locations) and events may result in unwanted privacy disclosure; (b) visually-similar images often contain similar privacy-sensitive objects and events. Thus performing deep image analysis may play an important role in recommending fine-grained privacy settings for social image sharing and privacy protection <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b32">[33]</ref>.</p><p>By assuming that the visual features for image content representation have strong correlations with the image content sensitiveness (privacy), the visual-based approach <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b31">[32]</ref> has leveraged the hand-crafted visual features (such as SIFT (scale-invariant feature transformation) features, GIST, color histograms) to learn the classifiers to recommend appropriate privacy settings for social image sharing. Because deep learning <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b43">[44]</ref> has demonstrated its outstanding abilities on extracting high-level features and significantly boosting the accuracy rates for many image understanding tasks, some researchers have leveraged deep learning to train more discriminative classifiers for image privacy prediction <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b31">[32]</ref>, and they have found that the deep features can be used to recommend more appropriate privacy settings for social image sharing. One major problem for the visual-based approach is its low interpret-ability <ref type="bibr" target="#b32">[33]</ref>: (a) the hand-crafted visual features for image content representation may not have exact correlations with the image content sensitiveness; (b) the deep features may have better interpret-ability at certain levels <ref type="bibr" target="#b53">[54]</ref>, but they may not be able to exactly represent the appearances of the privacy-sensitive object classes and events in the images. In addition, it is worth noting that the fine-grained privacy settings for social image sharing depend on two inseparable issues simultaneously: (1) sensitiveness of visual content of the images being shared; (2) trustworthiness of the users being granted to see the images. Thus, it is very attractive to develop new algorithms that are able to consider these two inseparable issues simultaneously to recommend fine-grained privacy settings for social image sharing.</p><p>Motivated by this observation, a new algorithm is developed in this paper by leveraging both the image content sensitiveness and the user trustworthiness to recommend fine-grained privacy settings for social image sharing. First, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, two approaches are developed for image content sensitivity representation: (a) Feature-based approach: By adapting the structure of the AlexNet <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b40">[41]</ref> to our new task for privacy setting recommendation and integrating user-provided images to fine-tune the underlying kernel weights, 1024-D deep features are learned for image content sensitiveness representation. (b) Object-based approach: The privacy-sensitive object classes and events are identified automatically and they are used for image content sensitiveness representation. Second, the users on the social network are clustered into a set of representative social groups to generate a discriminative dictionary for user trustworthiness characterization. Finally, both the image content sensitiveness and the user trustworthiness are seamlessly integrated to train a tree classifier to recommend fine-grained privacy settings for social image sharing.</p><p>The remaining of the paper is organized as follows. Section 2 reviews the related work briefly; Section 3 introduces our feature-based approach for image content sensitivity representation; Section 4 presents our deep multiple instance learning algorithm for identifying large numbers of privacysensitive object classes automatically and using them for image content sensitiveness representation; Section 5 introduces our algorithm for user trustworthiness characterization; Section 6 presents our tree classifier training algorithm for privacy setting recommendation; Section 7 reports the experimental results for algorithm and system evaluation; Section 8 concludes the paper and outlines the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Many recent works have studied how to leverage machine learning to automate the privacy setting process <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b37">[38]</ref>. These pioneering researches can be partitioned into three categories: (a) Tag-based approach: Social tags are used to learn a classifier for privacy setting recommendation <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b17">[18]</ref>; (b) Topic-based approach: Keywords or topics from users' profiles are used to partition the friend lists into multiple subgroups or circles <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b25">[26]</ref>, and the friends in the same circle are assumed to share similar privacy preferences; (c) Visualbased approach: Visual properties of the images (i.e., visual features or object classes) are leveraged to learn a classifier for privacy setting recommendation <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b37">[38]</ref>.</p><p>By assuming that the social tags for image semantics interpretation can also be used to characterize the image content sensitiveness effectively, the tag-based approach leverages the social tags for privacy policy inference <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b17">[18]</ref>. Vyas et al. <ref type="bibr" target="#b10">[11]</ref> and Squicciarini et al. <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> have leveraged the social tags for privacy policy inference and good performances are reported. Klemperer et al. <ref type="bibr" target="#b15">[16]</ref> studied whether the keywords from social tags can be used to help users create and maintain access-control policies more intuitively. Ravichandran et al. <ref type="bibr" target="#b16">[17]</ref> studied how to leverage zone tags to predict a user's privacy preferences from the location data (i.e., share the locations or not). Yeung et al. <ref type="bibr" target="#b17">[18]</ref> have leveraged social tags and linked data for providing access control to online photo albums.</p><p>When high-quality social tags are available, such tag-based approach can recommend appropriate privacy settings for social image sharing. Because tagging rich image semantics could be a time-consuming process <ref type="bibr" target="#b6">[7]</ref>, most images may be associated with low-quality social tags (i.e., noisy tags, missing tags and spam tags). As a result, such tag-based approach may not be able to recommend appropriate privacy settings for social image sharing <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>. Another shortcoming for the tag-based approach is that it completely ignores the user trustworthiness for privacy setting recommendation, however, the fine-grained privacy settings for social image sharing may also change with different users according to their trustworthiness.</p><p>To automate the privacy setting process, the topicbased approach uses the keywords or topics from users' profiles for privacy preference prediction <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b25">[26]</ref>. Fang and LeFevre <ref type="bibr" target="#b20">[21]</ref> proposed a privacy wizard to help users grant privileges to their friends. The wizard asks users to first assign privacy labels to the selected friends, and then uses this as the input to construct a classifier to group friends based on their profiles and automatically assign privacy labels to the unlabeled friends. Similarly, Danezis <ref type="bibr" target="#b18">[19]</ref> proposed a machine-learning based approach to automatically extract privacy settings from the social context within which the data is produced. Parallel to the work of Danezis <ref type="bibr" target="#b18">[19]</ref>, Adu-Oppong et al. <ref type="bibr" target="#b19">[20]</ref> developed privacy settings based on a concept of "social circles" which consist of clusters of friends formed by partitioning users' friend lists. However, such topic-based approach considers only the user trustworthiness but completely ignores the image content sensitiveness, thus it may not be able to recommend appropriate privacy settings for image sharing because the fine-grained privacy preferences may also change according to the image content sensitiveness.</p><p>The visual properties of the images are recognized as the most important information source that may significantly affect the privacy settings for image sharing <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b37">[38]</ref>. Zerr et al. <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b27">[28]</ref> were the first team to leverage the visual features for supporting privacy-aware image classification, where a large number of participants are asked to label images into two categories: private vs. public. A classifier is learned from user-labeled images and meta data are also integrated to achieve better performance on privacy-aware image classification. Squicciarini et al. <ref type="bibr" target="#b28">[29]</ref> have exploited both SIFT features and facial recognition to achieve automatic image privacy prediction. More recently, Tonge and Caragea <ref type="bibr" target="#b29">[30]</ref> have first integrated the deep features for image privacy prediction, Spyromitros-Xioufis et al. <ref type="bibr" target="#b31">[32]</ref> have recently leveraged user-dependent images and privacy settings to support personalized privacy-aware image classification. Both teams have found that the deep features can yield remarkable improvements on the performance as compared with other hand-crafted visual features such as SIFT, GIST and color histograms. One shortcoming of the visual-based approach is that it just considers the image content sensitiveness but completely ignores the user trustworthiness for privacy setting recommendation, however, the fine-grained privacy settings for social image sharing may also change with different users according to their trustworthiness. As illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>, we have recently developed a deeplearning-based approach called iPrivacy (image Privacy) <ref type="bibr" target="#b32">[33]</ref> which is capable of recognizing human objects in the images, determining their privacy sensitiveness levels and then blur faces of human subjects who have high levels of privacy concerns. However, iPrivacy does not consider the effect of users' social behaviors (i.e., user trustworthiness) on privacy setting recommendation, thus it cannot provide fine-grained access control yet, e.g., an image may be fine to be directly shared with close family members while need to be blurred when showing to the public. In addition, face blurring used in our iPrivacy system may protect image privacy at certain level but it may also raise speculations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. FEATURE-BASED APPROACH FOR IMAGE CONTENT SENSITIVENESS REPRESENTATION</head><p>As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, a feature-based approach is developed to extract discriminative deep features for image content sensitiveness representation. By assuming that the visual properties of the images have hidden correlations with their visual content sensitiveness, the deep features learned for image content representation are further used to approximate the sensitiveness (privacy) of image content. However, it is not a good idea to directly apply the AlexNet (that are optimized for recognizing 1,000 atomic object classes <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b40">[41]</ref>) to extract 4096-D deep features for our new task of privacy setting recommendation, and the reason is that the task space for privacy setting recommendation is much smaller than that for large-scale visual recognition. Our feature-based approach can work for two scenarios: (a) recognizing two categories for our binary approach, e.g., assigning the image-user pairs (i.e., the relationships between the images and the users) into one of two categories of fine-grained privacy settings: share vs. not-share; (b) recognizing four categories for our multi-category approach, e.g., assigning the imageuser pairs into one of four categories of fine-grained privacy settings: {completely-share, not-share, partially-share, sharewith-blurring}.</p><p>Based on this observation, the outputs for the 2 fullyconnected layers (i.e., FC6 and FC7) in our deep network are scaled down to 1,024 rather than 4,096 in the AlexNet <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b40">[41]</ref>, e.g., the number of kernel mappings for the 2 fully-connected layers (i.e., FC6 and FC7) are scaled down into 25% of that for the AlexNet <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b40">[41]</ref>, and Dropout <ref type="bibr" target="#b52">[53]</ref> is applied to 2 fully-connected layers with a value of 0.5 to prevent over-fitting. In addition, we use the kernel weights for the AlexNet <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b40">[41]</ref> to initialize the weights for the mapping kernels on our deep network, so that we can use a small number of user-provided images to fine-tune the node weights and achieve acceptable accuracy rates <ref type="bibr" target="#b50">[51]</ref>.</p><p>Given the user-provided images, the predictions of the categories for their privacy settings are calculated and the errors for these user-provided images are calculated. We formulate the training error rate ξ in the form of softmax regression:</p><formula xml:id="formula_0">ξ(W, x, y) = - τ l=1 I{y j }log ex p(W T l x j + b) τ i=1 ex p(W T i x j + b) (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where τ = 2 or τ = 4 is the total number of categories being recognized for privacy setting recommendation, I{y j } is the indicator function such that I{y j } = 1 if y j = 1 (i.e., (x j , y j ) is the positive training image), otherwise I{y j } = 0. The corresponding gradients for the objective function are calculated as ∂ξ(W,x,y) ∂ W</p><p>, and they are backpropagated <ref type="bibr" target="#b50">[51]</ref> through our deep network to fine-tune the kernel weights.</p><p>By adapting the structure of the AlexNet <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b40">[41]</ref> to our new task (i.e., recognizing two categories or four categories for fine-grained privacy setting recommendation) and integrating user-provided social images to fine-tune the node weights, we can learn more representative deep network to extract more discriminative deep features for image content sensitiveness representation. For a given image I , its visual content sensitiveness can be precisely represented as a histogram of 1024-D deep features x s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. OBJECT-BASED APPROACH FOR IMAGE CONTENT SENSITIVENESS REPRESENTATION</head><p>As shown in Fig. <ref type="figure" target="#fig_3">4</ref>, an object-based approach is developed to achieve more discriminative representations of image content sensitiveness. Our idea is to learn a deep network to automatically identify large numbers of privacy-sensitive object classes and events for image content sensitiveness representation. Our object-based approach contains the following key steps: (a) A category hierarchy is constructed to organize various types of image privacy concerns and their most relevant privacy-sensitive object classes hierarchically; (b) A deep multiple instance learning algorithm is developed to learn the classifier to detect 268 privacy-sensitive object classes automatically; (c) The CRF (conditional random field <ref type="bibr" target="#b49">[50]</ref>) models are further learned for predicting 12 privacysensitive image events; (d) All these privacy-sensitive object classes and events are used to generate a 280-D discriminative   dictionary for image content sensitiveness (privacy) characterization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Category Hierarchy</head><p>The critical challenge to be conquered here is to identify various types of image privacy concerns and their most relevant privacy-sensitive object classes, so that we can use them to achieve more effective characterization of image sensitiveness (privacy). To tackle this challenge, as shown in Fig. <ref type="figure" target="#fig_4">5</ref> and Fig. <ref type="figure" target="#fig_5">6</ref>, a category hierarchy is constructed to organize various types of image privacy concerns and their most relevant privacy-sensitive object classes hierarchically. In this paper, we focus on 7 types of privacy concerns and their most relevant privacy-sensitive object classes: We use an example of outlook privacy to identify its most relevant privacy-sensitive object classes. For example, Our category hierarchy can allow us to identify the most relevant keywords (text terms) to precisely describe various types of privacy concerns and their most relevant privacysensitive object classes, so that we can use these keywords to crawl large-scale training images from multiple social sites. It is worth noting that our category hierarchy contains sufficient numbers of privacy-sensitive object classes to quantify the image content sensitiveness in a fine-grained level, which can allow us to recommend fine-grained privacy settings for social image sharing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep Multiple Instance Learning of Object Detectors</head><p>Even deep learning has demonstrated its outstanding performances on many image understanding tasks, it requires largescale manually-labeled training images <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b43">[44]</ref>, but it is a laborious task to label large-scale object regions manually for learning the object detectors. In this paper, a deep multiple instance learning algorithm is developed to directly leverage the coarsely-labeled images (i.e., object labels are coarsely given at the image level rather than at the region level) for learning the object detectors. Our deep multiple instance learning algorithm takes the following steps as illustrated in Fig. <ref type="figure" target="#fig_7">7:</ref> (a) Each social image is first segmented into a set of semantic object regions (instances); (b) A noise-or model is used to define the error function for supporting deep multiple instance learning, e.g., learning the deep network and the object detectors jointly in an end-to-end manner.</p><p>Deep CNNs have shown their strong ability on supporting pixel-level image classification (i.e., semantic image segmentation by assigning one particular semantic label to every pixel in an image) <ref type="bibr" target="#b44">[45]</ref>- <ref type="bibr" target="#b48">[49]</ref>. To extract semantic object regions from the images, we have trained a deep network in an endto-end way to enable pixel-level prediction and classification, and a CRF (conditional random fields) model <ref type="bibr" target="#b49">[50]</ref> is further learned to merge the neighboring pixels with the same labels to generate semantic object regions <ref type="bibr" target="#b47">[48]</ref>. As shown in Fig. <ref type="figure" target="#fig_8">8</ref>, by integrating deep CNNs with CRF models for semantic image segmentation, we can identify semantic object regions precisely.</p><p>Multiple instance learning (MIL) <ref type="bibr" target="#b54">[55]</ref>- <ref type="bibr" target="#b57">[58]</ref> has been used to deal with the issue of coarse labeling by treating each image (which may contain multiple objects) as a bag of instances. In our deep multiple instance learning algorithm, we use the kernel weights from the AlexNet <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b40">[41]</ref> to initialize the mapping kernels on our deep CNNs, so that we can use a small number of user-provided training images to fine-tune the kernel weights effectively and achieve acceptable accuracy rates. As illustrated in Fig. <ref type="figure" target="#fig_7">7</ref>, a special unit is inserted into our deep CNNs to support multiple instance learning and handle the issue of coarse labeling. Our special unit for multiple instance learning contains two components: (a) Noise-or model <ref type="bibr" target="#b54">[55]</ref> is used to predict the image labels (bag labels) from the instance labels (region labels); (b) The visually-similar image regions are assumed to share the same object label. Given an image region or object proposal, the prediction of its privacysensitive object class is calculated, the error function for our deep multiple instance learning algorithm contains two parts:</p><formula xml:id="formula_2">J (W ) = N k=1 ⎧ ⎨ ⎩ λ R R j =1 bag (B k j , B k j ) + 1 R 2 R h=1 R l=1 δ hl κ(x k h , x k l ) instance (y k h , y k l )I {y k l }log ex p(W T k x k l +b) N l=1 ex p(W T l x l j +b)<label>(2)</label></formula><p>where N = 268 is the total number of privacy-sensitive object classes being recognized, R is the number of training images for each object class, κ(x k h , x k l ) is used to characterize the visual similarity between two image regions from the same image (bag), δ hl indicates that we only consider the visual similarity among the image regions from the same image, bag (B k j , B k j ) is used to calculate the difference between the predicted bag (image) label B k j and the given bag label B k j for the kth image (bag), and instance (y k h , y k l ) is used to calculate the difference on their labels between two visually-similar image regions (x k h , y k h ) and (x k l , y k l ) from the same image (bag) B k j . We use the noise-or model to predict the label B k j for a given positive bag (image) B k j by cumulating all the predictions for its image regions (instances).  The gradients for the objective function are calculated as ∂ J (W ) ∂ W , and they are back-propagated <ref type="bibr" target="#b50">[51]</ref> through our deep CNNs to fine-tune the kernel weights. After such deep CNNs is available, it is used to detect 268 privacy-sensitive object classes from the images being shared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Predicting Privacy-Sensitive Image Events</head><p>Some object classes may not be sensitive individually, but their co-occurrences in the same image may convey privacysensitive image event. Thus it is very attractive to leverage such object co-occurrences to infer the appearances of the most relevant privacy-sensitive image events. First, an object co-occurrence network is constructed and it consists of two key components: (a) object classes; and (b) their inter-class cooccurrences in large-scale social images. Second, as illustrated in Fig. <ref type="figure" target="#fig_9">9</ref>, over the object co-occurrence network, a two-layer CRF (conditional random field <ref type="bibr" target="#b49">[50]</ref>) model is learned for predicting privacy-sensitive image events. In Fig. <ref type="figure" target="#fig_9">9</ref>, the first layer is used to interpret the appearances of the object classes and their co-occurrences (i.e., our object co-occurrence network), the second layer is used to interpret the appearances of the most relevant privacy-sensitive image events, e.g., the co-occurrences of some object classes in the images are sufficient to indicate the appearances of the most relevant privacysensitive image events. Thus our two-layer CRF models are used to learn the conditional probabilities over the appearances of the object classes, their co-occurrences and the appearances of the most relevant privacy-sensitive image events.</p><p>For a given training image I , our deep multiple instance learning algorithm can effectively extract a set of object classes O from the training image I , and their co-occurrences X can further be identified from our object co-occurrence network, we can estimate the probability P(y j |O, X, j ) for the appearance of the most relevant privacy-sensitive image event y j as:</p><formula xml:id="formula_3">P(y j |O, X, j ) = 1 Z ( ) ex p F j (y j | X, O, j ) (3)</formula><p>where F j (y j | X, O, j ) is the classifier for the j th privacysensitive image event y j given the set of object classes O and their co-occurrences X, Z ( ) is the partition function and it is defined as:</p><formula xml:id="formula_4">Z ( ) = 12 j =1 ex p F j (y j | X, O, j ) (4)</formula><p>Such two-layer CRF models are learned from a set of training images and they are then used to predict the presences of the most relevant privacy-sensitive image events in the social images being shared when the appearances of the object classes and their co-occurrences are determined. In our current work, we focus on learning the two-layer CRF models to predict 12 privacy-sensitive image events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Image Content Sensitiveness Representation</head><p>The privacy-sensitive object classes and events, which are identified from large-scale social images and frequently occur in the private (not-share) images, are selected to generate a 280-D discriminative dictionary D I for image content sensitiveness representation. For a given image I , its privacysensitive object classes and event are first detected (by our deep multiple instance learning algorithm and our two-layer CRF models). By projecting its privacy-sensitive object classes and event over such 280-D discriminative dictionary, the content sensitiveness (privacy) of the given image I can precisely be represented as a 280-D histogram (i.e., a 280-D bag of privacy-sensitive object classes and events x s ). Like object bank <ref type="bibr" target="#b51">[52]</ref> for image content representation, such 280-D bags of privacy-sensitive object classes and events can characterize the image content sensitiveness effectively. For a given image, its 280-D bag of privacy-sensitive object classes and events x s is very sparse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. USER TRUSTWORTHINESS CHARACTERIZATION</head><p>The intuition is that how the image owner interacts with others in the social networks show the hints on whom they would share with what types of images. For example, if a user interacts with his/her friend Bob frequently, it is likely that the user would share more images with his/her friend Bob. Yet another example is that if a user usually shares the images of family events only with his/her family members, it is likely the user will do the same for the new images of family events. Therefore, our goal is to characterize these social behaviors and study their correlations with the finegrained privacy settings for social image sharing.</p><p>We have explored multiple factors that describe users' social behaviors, which include: (1) types of relationships (such as friends, family members, colleagues) with the image owner which could help distribute images (with various visual content) to different groups of users; (2) closeness of the relationships with the image owner as the image owner may share sensitive images with people who are very close to him/her; (3) matching scores between the user's interesting topics (from user profiles) and the semantics of the images being shared, e.g., people may have stronger motivations to distribute the image which is very interesting to him/her; (4) interaction intensity between the user and the image owner, which could be another indicator of how likely the image would be shared; (5) user's activity score in social network considering that active users may have a higher chance to distribute the images; (6) stability scores of users' behavior history; (7) reputation scores to assess users' selfrepresentation of honesty and reliability in social network.</p><p>In order to learn the effect of these multi-factors on privacy setting configuration, we first analyze individual factor and define the corresponding function to quantify the value of each factor in a fine granularity. Then, we construct a highdimensional feature vector based on the obtained fine-grained values of all the factors to represent the user's social behaviors, and a joint function is learned to characterize the similarities among the users according to their multi-factors for social behavior characterization. By using multi-factors for user's social behavior representation and treating each user on a social network as one node on a graph, we can use spectral clustering to partition large numbers of users into a set of representative social groups according to their similarities (closeness) on their multi-factors for social behavior characterization. For a given social network with M users, its M users are partitioned into B representative social groups by minimizing inter-group similarity and maximizing intra-group similarity:</p><formula xml:id="formula_5">mi n (M, B) = B l=1 u i ∈G l u j ∈G c /G l κ(u i , u j ) u i ∈G l u j ∈G l κ(u i , u j ) (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where κ(u i , u j ) is the kernel function to characterize the similarity (closeness) between two users u i and u j on their multifactors for social behavior characterization,</p><formula xml:id="formula_7">G c = {G l |l = 1, • • • , B} is used to represent B groups (clusters) of M users on the given social network, G c /G l is used to represent other B -1 groups in G c except G l .</formula><p>As shown in Fig. <ref type="figure" target="#fig_10">10</ref>, the users on the social network are clustered into B representative social groups (such as altruistic users, cynical users, forgiving users, distrusting users, et al.) according to the similarities (closeness) on their multi-factors for social behavior characterization <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b24">[25]</ref>. Such representative social groups can effectively characterize the relationships and trustworthiness among the users, thus they can be used to generate a B-D discriminative dictionary D u for user trustworthiness characterization. The key issue for automating the privacy setting process is to train a classifier for assigning the relationships between the images (represented by their visual content sensitiveness x s ) and the users (characterized by their trustworthiness x u ) into a set of pre-defined categories for fine-grained privacy settings, e.g., assigning the image-user pairs (or the relationships between the images and the users) into a set of pre-defined categories for fine-grained privacy settings.</p><p>In this work, two approaches are developed to simultaneously consider both the image content sensitiveness x s and the user trustworthiness x u in different modalities for fine-grained privacy setting recommendation: (a) binary approach: as shown in Fig. <ref type="figure" target="#fig_0">11</ref>, the image-user pairs (x s ⊕ x u ) are assigned into one of two categories of fine-grained privacy settings, e.g., share or not-share; (b) multi-category approach: as illustrated in Fig. <ref type="figure" target="#fig_1">12</ref>, the image-user pairs (x s ⊕ x u ) are assigned into one of four categories of fine-grained privacy settings, e.g., {completely-share, not-share, partially-share, share-with-blurring}.</p><p>It is worth noting that: (a) both the feature-based approach and the object-based approach for image content sensitiveness representation can be used to support our binary approach for fine-grained privacy setting recommendation; (b) only the object-based approach for image content sensitiveness representation can be used to support our multi-category approach for fine-grained privacy setting recommendation, e.g., supporting more options (more categories) for fine-grained privacy settings requires deeper analysis and semantic understanding of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Binary Approach</head><p>For a given training image I , its visual content sensitiveness is represented as a 1024-D deep feature x s or a 280-D bag of privacy-sensitive object classes and events x s , the privacy setting for one particular user u (who is granted to access the given image I and his/her trustworthiness is represented as a B-D bag of representative social groups x u ) is defined as a binary canonical policy: (a) share: this user u with the trustworthiness representation x u is granted to see the given image I with the visual content sensitiveness representation x s , e.g., the image-user pair (the relationship between the image x s and the user x u : x s ⊕ x u ) is assigned into the category "share"; and (b) not-share: this user u with the trustworthiness representation x u is not granted to see the given image I with the visual content sensitiveness representation x s , e.g., the image-user pair (the relationship between the image x s and the user x u : x s ⊕ x u ) is assigned into the category "not-share".</p><p>The goal of our binary approach for fine-grained privacy setting recommendation is to learn a classifier f (c | x s , x u , ), c ∈ {share, not-share}, to achieve precise assignment between the image x s and the user x u (i.e., image-user pair x s ⊕ x u ), e.g., assigning the relationship between the image x s and the user x u into one of two categories for fine-grained privacy settings (i.e., share &amp; not-share). Because the features for image content sensitiveness representation and user trustworthiness characterization are in different modalities and they are not comparable directly, it is not a good idea to simply concatenate x s for image content sensitiveness representation with x u for user trustworthiness characterization as an unified feature (x s ,x u ). As illustrated in Fig. <ref type="figure" target="#fig_13">13</ref>, a tree classifier f (c | x s , x u , ) is trained to leverage both the image content sensitiveness (x s ) and the user trustworthiness (x u ) in different modalities for fine-grained privacy setting recommendation, where different nodes on the tree classifier can select different features (x u or x s ) for node classifier training.</p><p>The tree classifier f (c | x s , x u , ) is learned from largescale training images and their privacy settings that are Given R training images and their visual content sensitiveness representations x s , their similarities are then calculated and represented as a R × R similarity matrix S and its component S i j is used to characterize the similarity between the i th training image and the j th one according to their content sensitiveness representations x i s and x j s :</p><formula xml:id="formula_8">S i j = exp - d(x i s , x j s ) σ s (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>where d(•, •) is the Euclidean distance between two images on their visual content sensitiveness representations x i s and x j s . Given T users and their trustworthiness representations x u , their similarities are then calculated and represented as a T × T similarity matrix U and its component U kl is used to characterize the similarity between the kth user and the lth one according to their trustworthiness representations x k u and x l u :</p><formula xml:id="formula_10">U kl = exp - d(x k u , x l u ) σ u (<label>7</label></formula><formula xml:id="formula_11">)</formula><p>where d(•, •) is the Euclidean distance between two users on their trustworthiness representations x k u and x l u . Given two categories for fine-grained privacy settings (i.e., share &amp; not-share), the similarity matrix S and U can be used as a proxy to determine the separability of training samples (images and users) on two feature subsets in different modalities: (a) image content sensitiveness representations x s ; and (b) user trustworthiness characterizations x u . As illustrated in Fig. <ref type="figure" target="#fig_13">13</ref>, to make the first decision for tree classifier training, the most discriminative feature subset x best is first selected and the associated samples (either R training images or T users) are then partitioned into two categories for fine-grained privacy settings (i.e., share vs. not-share). The most discriminative feature subset x best is determined automatically by maximizing the separability:</p><formula xml:id="formula_12">x best = max ⎧ ⎨ ⎩ 1 R 2 R j =1 R i=1 S i j , 1 T 2 T k=1 T l=1 U kl , ⎫ ⎬ ⎭ (8)</formula><p>If As illustrated in Fig. <ref type="figure" target="#fig_13">13</ref>, different paths on the tree classifier</p><formula xml:id="formula_13">f (c | x s , x u , ) have different combinations of two binary SVM classifiers f s (c | x s , θ) and f u (c | x u , ϑ).</formula><p>After the tree classifier f (c | x s , x u , ) is learned, it is further used to automatically configure an appropriate privacy setting between a given image I and one particular user u. For a given image I being shared and one particular user u on the social network of the image owner, as illustrated in Fig. <ref type="figure" target="#fig_13">13</ref>, our fine-grained privacy setting recommendation algorithm takes the following key steps to make the decision: (a) our deep network is first used to extract 1024-D deep features or our deep multiple instance learning algorithm is used to detect the privacy-sensitive object classes and event, and the visual content sensitiveness of the given image I is precisely represented by using the 1024-D deep features x s or the 280-D bag of privacy-sensitive object classes and events x s ; (b) the trustworthiness of the user u is characterized as a B-D bag of representative social groups x u , e.g., the user's multifactors for social behavior characterization are used to obtain his/her representative social groups and then project onto the B-D dictionary D u to obtain a B-D bag of representative social groups x u for user trustworthiness characterization; (c) our tree classifier f (c | x s , x u , ) is then used to make the decision {share, not-share} hierarchically according to both the image content sensitiveness (represented by x s ) and the use trustworthiness (characterized by x u ), e.g., project the imageuser pair (x s ⊕ x u ) onto the appropriate category for finegrained privacy settings (i.e., share or not-share).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-Category Approach</head><p>In our multi-category approach for privacy setting recommendation, for a given training image I , its visual content sensitiveness is represented as a 280-D bag of privacy-sensitive object classes and events x s , and the fine-grained privacy setting for one particular user u (who is granted to access the given image I and his/her trustworthiness is represented as a B-D bag of representative social groups x u ) is defined as a multi-category policy: (a) completely-share: this user u with the trustworthiness representation x u is granted to completely see the given image I with the visual content sensitiveness representation x s , e.g., the image-user pair (x s ⊕ x u ) is assigned into the category "completely-share"; (b) not-share/private: this user u with the trustworthiness representation x u is not granted to see the given image I with the visual content sensitiveness representation x s , e.g., the image-user pair (x s ⊕ x u ) is assigned into the category "not-share/private"; (c) partiallyshare: this user u with the trustworthiness representation x u is granted to partially see the given image I with the visual content sensitiveness representation x s , e.g., the image-user pair (x s ⊕ x u ) is assigned into the category "partially-share"; (d) share-with-blurring: this user u with the trustworthiness representation x u is granted to see the blurring image (for example, the privacy-sensitive object classes are blurred), e.g., the image-user pair (x s ⊕ x u ) is assigned into the category "share-with-blurring".</p><p>As illustrated in Fig. <ref type="figure" target="#fig_14">14</ref>, the goal of our fine-grained privacy setting recommendation algorithm is to learn a multi-class tree classifier f (c | x s , x u , ), c ∈ {completely-share, notshare, partially-share, share-with-blurring}, to achieve precise assignments between the image-user pairs (x s ⊕ x u ) and multiple categories (i.e., completely-share, not-share, partiallyshare, share-with-blurring) for fine-grained privacy settings. We use similar techniques as introduced above to train our multi-class tree classifier: (a) The most discriminative feature subset x best is first selected automatically; (b) The multi-class SVM classifier is then trained to achieve optimal partitioning of training samples (images or users) by using the most discriminative feature subset x best (x s or x u ); (c) Different paths on the multi-class tree classifier f (c | x s , x u , ) have different combinations of the multi-class SVM classifiers f s (c | x s , θ) and f u (c | x u , ϑ). Some experimental results for supporting multi-category fine-grained privacy setting recommendation are illustrated in Fig. <ref type="figure" target="#fig_15">15</ref>, where the special category for "not-share/private" is not demonstrated because no further operations are required. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. ALGORITHM AND SYSTEM EVALUATION</head><p>We have evaluated our proposed algorithms over: (a) our large-scale social images; and (b) two public image sets, PicAlert 1 and Mirflickr. 2  We have collected 800, 000 social images and their privacy settings are labeled, where 90, 000 images are treated as the test images and others are used as the training images. In the following experiments, we focus on evaluating the effectiveness of our fine-grained privacy setting recommendation algorithm when different types of features are used for image content sensitiveness representation. Specifically, we compare the predicted privacy settings generated by our proposed algorithms with the original privacy settings provided by humans. For 90, 000 test images, we partition them into 900 subsets according to their scores on the correspondences between the image content sensitiveness (privacy) and the visual features for image content representation. We tested our privacy setting recommendation algorithm by using three approaches for image content sensitiveness representation, e.g., low-level visual features, high-level deep features and privacy-sensitive object classes and events.</p><p>For a given image set with R test images which are shared with T users under different privacy settings f , its privacy disclosure is defined as:</p><formula xml:id="formula_14">S = 1 R × T T j =1 R l=1 δ(c, ĉ) f (c | x l s , x j u , )-f (x l s , x j u )<label>(9)</label></formula><p>where f (c | x l s , x j u , ) is the predicted privacy setting for the lth given image with the visual content sensitiveness representation x l s to the j th user u with the trustworthiness x j u , f (x l s , x j u ) is the human-defined privacy setting assigned between the j th user u with the trustworthiness x j u and the lth given image with the visual content sensitiveness representation x l s , δ(c, ĉ) is used to emphasize that the privacy disclosure is counted differently for various situations.</p><p>For our binary approach: (a) when both the predicted privacy setting ĉ for the given image I and its human-defined one c are not-share, δ(c, ĉ) = 0; (b) when both the predicted privacy setting ĉ for the given image I and its human-defined 1 http://l3s.de/picalert/ustudydata 2 http://s16a.org/mirflickr one c are share, δ(c, ĉ) = 0; (c) when the predicted privacy setting ĉ for the given image I is not-share, but its humandefined one c is share, δ(c, ĉ) = 0.5, such privacy disclosure is the punishment to avoid cheating from the system, e.g., without this penalty, the system may easily achieve low privacy disclosure by recommending the privacy setting for each image as "not-share"; (d) when the predicted privacy setting ĉ for the given image I is share, but its human-defined one c is notshare, δ(c, ĉ) = 1. Thus δ(c, ĉ) is defined as:</p><formula xml:id="formula_15">δ(c, ĉ) = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 0, c = ĉ = not-share 0, c = ĉ = share 0.5, c = share, ĉ = not-share 1.0, c = not-share, ĉ = share<label>(10)</label></formula><p>For our multi-category approach, δ(c, ĉ) is defined as: </p><formula xml:id="formula_16">δ(c, ĉ) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ 0, c = ĉ = not-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparison on Deep Features from Different Networks</head><p>By assuming that the low-level visual features extracted for image content representation can also be used to characterize the image content sensitiveness effectively, the lowlevel visual features are directly used to learn the classifier for privacy setting recommendation, and we have compared two approaches: (1) our feature-based approach: the structure (FC6 and FC7) of the AlexNet <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b40">[41]</ref> is scaled down and adapted to our new task (i.e., assigning the image-user pairs into two categories of fine-grained privacy settings: share vs. not-share or four categories of fine-grained privacy settings: {completely-share, not-share, partially-share, sharewith-blurring}) and only 1024-D deep features are extracted   As shown in Fig. <ref type="figure" target="#fig_17">16</ref>, one can observe that our feature-based approach can significantly outperform the traditional featurebased approach. The reasons are two folds: (1) our featurebased approach can adapt the structure (FC6 and FC7) of our deep CNNs to learn more discriminative deep features for our new task (i.e., recognizing two categories or four categories for fine-grained privacy setting recommendation); and (2) our feature-based approach can incorporate the userprovided images to fine-tune the kernel weights according to our new task (i.e., recognizing two categories or four categories for fine-grained privacy setting recommendation rather than recognizing 1,000 atomic object classes for largescale visual recognition application).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison on Various Visual Features</head><p>It is also very interesting to evaluate whether using different types of visual features for classifier training may bring significant improvement on privacy setting recommendation. As shown in Fig. <ref type="figure" target="#fig_18">17</ref>, we have compared the performance of Fig. <ref type="figure" target="#fig_8">18</ref>.</p><p>The comparison on the computational cost for privacy setting recommendation between our feature-based approach and the traditional one by using the 4096-D deep features directly.</p><p>our feature-based approach when different types of visual features are used for image content sensitiveness representation. From these comparison experiments, one can observe multiple interesting factors: (1) For most of 900 image subsets, the deep features can achieve the best performance as compared with other hand-crafted visual features such as SIFT, GIST and color histograms; (2) For some difficult image subsets, which may have low scores on the correlations between the visual features for image representation and the image content sensitiveness (privacy), all these features (including deep features) may not be able to achieve acceptable performance, e.g., all of them have large privacy disclosures because such visual features for image content representation may not be able to characterize the image content sensitiveness (privacy) effectively;</p><p>(3) For some easy image subsets, which have high scores on the correlations between the visual features for image content representation and the image content sensitiveness, all these features (including hand-crafted visual features) can achieve good performance (resulting in small privacy disclosures).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison on Computational Cost</head><p>We have compared the computational cost between two feature-based approaches: (a) our feature-based approach: the deep network is scaled down and its structure is adapted to our new task and only 1024-D deep features are used for image content sensitiveness representation; (b) the traditional feature-based approach: the 4096-D deep features, which are usually learned for recognizing 1000 atomic object classes, are arbitrarily used for image content sensitiveness representation. As shown in Fig. <ref type="figure" target="#fig_8">18</ref>, one can observe that our feature-based approach can reduce the computational cost significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Effectiveness of Privacy-Sensitive Object Classes</head><p>To evaluate the effectiveness of using privacy-sensitive object classes and events on fine-grained privacy setting recommendation, we have compared two approaches: (1) our object-based approach: 268 privacy-sensitive object classes and 12 privacy-sensitive image events are detected and they are used to generate a 280-D discriminative dictionary, thus 280-D bags of privacy-sensitive object classes and events are used for As shown in Fig. <ref type="figure" target="#fig_20">19</ref>, one can observe that our objectbased approach can significantly outperform the traditional object-based approach. The reason is that the privacy-sensitive object classes and events can characterize the image content sensitiveness effectively, on the other hand, 1000 atomic object classes <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b43">[44]</ref>, that are usually extracted for image semantics interpretation, may not be able to characterize the image content sensitiveness exactly, e.g., the appearances of such 1000 atomic object classes in the images do not exactly relate with the image privacy or cause privacy disclosure directly. Even detecting such 1000 atomic object classes can play important roles on image semantics interpretation, they may not be effective for characterizing the image content sensitiveness precisely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Privacy-Aware Image Classification</head><p>In order to achieve more clear understanding of what kind of visual properties makes images to be private (notshare) or public (share), we have evaluated two approaches for privacy-aware image classification. As illustrated in Fig. <ref type="figure" target="#fig_21">20</ref>, one can observe that whether the recommended privacy settings are appropriate for image sharing largely depends on whether the privacy-sensitive object classes are significant on giving some insights about the image sensitiveness (privacy). By detecting such privacy-sensitive object classes automatically, our object-based approach is able to achieve more effective solution for privacy setting recommendation, however, its performance is still not comparable with human beings and the reasons for this phenomenon are: (1) The set of privacy-sensitive object classes is not complete (only 268 privacy-sensitive object classes are used in our current work), which may not be able to characterize huge diversity of image privacy (sensitiveness) effectively and efficiently;</p><p>(2) Because the training images are insufficient and deep learning scheme usually requires huge numbers of training images, the accuracy rates for detecting such privacy-sensitive object classes may not be high enough for us to harvest the advantages of leveraging them for image sensitiveness characterization; (3) Image sensitiveness (privacy) is a very subjective concept, it may largely depend on both the sensitivities of image content and users' personal conservativeness (i.e., different persons may have different privacy preferences). Based on these observations, it is very attractive to develop new algorithms for leveraging more information sources (such as users' personal preferences and keeping users in the loop to define their personalized privacy-sensitive object classes and events) to achieve more effective solutions for fine-grained privacy setting recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. User Study</head><p>For the same task of privacy setting recommendation, 31 students (16 females and 15 males) are invited to assess the interpret-ability of our object-based approach and our featurebased approach. In our user study, we ask 31 students to score the interpret-ability of our object-based approach and our feature-based approach in 7 levels (6 for the best one and 0 for the worst one). In order to help users understand the correspondences between the image privacy (sensitiveness) and the appearances of privacy-sensitive object classes, as shown in Fig. <ref type="figure" target="#fig_22">21</ref>, the privacy-sensitive objects (human faces in this case) are identified and illustrated. Each user is asked to evaluate both our object-based approach and our featurebased approach over at least 20 image sets and each image set contains at most 100 images, and we average his/her scores for all these image sets. As shown in Fig. <ref type="figure" target="#fig_23">22</ref>, one can observe that our object-based approach can significantly improve the interpret-ability because the appearances of privacy-sensitive object classes and events in the images have exact and explicit correlations with the image privacy (sensitiveness).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Experimental Results on Two Public Image Sets</head><p>We have also evaluated our proposed algorithms over two public image sets: PicAlert and Mirflickr <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. In these two public image sets, we have compared three approaches for privacy setting recommendation: (1) our feature-based approach; (2) our object-based approach; (c) the baseline method by Zerr et al. <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b27">[28]</ref>.</p><p>We first use the deep network learned from our image set to configure the structure of the deep networks for these two public image sets, and the images from these two public image sets are further used to fine-tune the kernel weights effectively. In addition, we partition the test images into 40 subsets and evaluate each subset independently. As shown in Fig. <ref type="figure" target="#fig_24">23</ref> and Fig. <ref type="figure" target="#fig_25">24</ref>, one can observe that our object-based approach can achieve better performance than other two methods (i.e., it may cause lower privacy disclosure for image sharing). The reason for this phenomenon is that: Compared with the 1024-D or 4096-D deep features that are used in our feature-based approach and the baseline method <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, the privacy-sensitive object classes and events (that are used in our object-based approach) have much stronger correlations with the image content sensitiveness (privacy), e.g., their    The comparison between our feature-based approach and our object-based approach for privacy setting recommendation over PicAlert image set. appearances in the images may cause privacy disclosure directly.</p><p>The effectiveness of our object-based approach (on recommending appropriate privacy settings for image sharing) The comparison between three approaches for privacy setting recommendation over Mirflickr image set: (a) our object-based approach; (b) baseline method by Zerr et al. <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b27">[28]</ref>; (c) our feature-based approach, where 40 image subsets are sorted according to their privacy disclosure obtained by our feature-based approach. largely depends on the correlations between the image privacy (sensitiveness) and the privacy-sensitive object classes and events that are extracted for image content sensitiveness representation. As shown in Fig. <ref type="figure" target="#fig_26">25</ref>, when such correlations are low, our object-based approach may induce higher privacy disclosures. The reasons for this phenomenon are: (a) our small set of privacy-sensitive object classes and events could be incomplete because many others may also result in privacy disclosures but they are not detected in our current work; (b) our deep multiple instance learning algorithm may fail to learn discriminative models to detect these privacy-sensitive object classes accurately; and (c) The privacy-sensitive object classes and events could be userdependent and users should be involved in the loop to define their personalized privacy-sensitive object classes and events. As shown in Fig. <ref type="figure" target="#fig_27">26</ref>, we have also demonstrated similar observation for the feature-based approach, when the correlations between the image privacy (sensitiveness) and the 1024-D or 4096-D deep features for image content sensitiveness representation are low, the feature-based approach may induce higher privacy disclosures. The reason for this phenomenon is that such 1024-D or 4096-D deep features for image content representation may not have exact correlations with the image privacy (sensitiveness).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Discussions</head><p>Blurring faces may protect image privacy at certain level but it may also raise speculations. Thus one of our future researches is to use GANs <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b70">[71]</ref> to generate perceptuallysimilar but privacy-free image patches to replace the privacysensitive objects in the images be shared while maintaining their local smoothness among various neighboring image components, so that we can protect image privacy effectively while we may not raise speculations.</p><p>The privacy-sensitive object classes and events and their definitions are user-dependent and context-dependent. Based on this understanding, the privacy-sensitive object classes and events can be partitioned into two categories: (a) common ones; and (b) user-dependent ones or personalized ones. Our current work (presented in this paper) focuses on the common ones and thus one of our future researches is to involve users in the loop to define their personalized privacy-sensitive object classes and events, and we can also leverage personalized information sources (such as users' personal preferences and user-dependent privacy-sensitive object classes) to recommend fine-grained privacy settings for social image sharing.</p><p>The user trustworthiness characterization also plays an important role in supporting fine-grained privacy setting recommendation, thus one of our future researches is to develop new algorithms for achieving more accurate characterization of user trustworthiness: (a) achieving multi-level characterization of user trustworthiness to achieve more accurate assignments of fine-grained privacy settings for social image sharing; (b) using a large number of categories for fine-grained privacy settings and training more discriminative classifiers to achieve better assignments between the images and the users (imageuser pairs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSIONS</head><p>This paper has developed a new approach to recommend fine-grained privacy settings for social image sharing, where both the image content sensitiveness and the user trustworthiness are simultaneously considered and integrated to train more discriminative tree classifier. Our experimental studies have demonstrated both efficiency and effectiveness of our proposed algorithms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The flowchart for our fine-grained privacy setting recommendation algorithm by considering both image content sensitiveness and user trustworthiness simultaneously.</figDesc><graphic coords="2,74.99,58.97,461.42,83.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. The key operations in our iPrivacy system for image privacy protection.</figDesc><graphic coords="3,74.99,58.61,461.42,67.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3.The flowchart of our feature-based approach to extract 1024-D features for image sensitiveness (privacy) representation.</figDesc><graphic coords="3,314.51,175.73,246.02,100.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The flowchart of our object-based approach for image sensitiveness (privacy) representation.</figDesc><graphic coords="4,314.51,58.49,246.02,90.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The category hierarchy for organizing 7 types of image privacy concerns and their most relevant privacy-sensitive object classes.</figDesc><graphic coords="4,314.51,193.01,246.02,58.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The category hierarchy for organizing 7 types of image privacy concerns and their most relevant privacy-sensitive object classes.</figDesc><graphic coords="4,314.51,295.37,246.02,177.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) adult content; (b) locations; (c) religions; (d) age and sexual orientation; (e) human body languages (such as facial expressions); (f) human outlooks and dresses; (h) texts and identifiable personal tags.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The flowchart for our deep multiple instance learning algorithm.</figDesc><graphic coords="5,125.99,58.85,359.42,102.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Our results on semantic image segmentation: (a) original images; and (b) object regions.</figDesc><graphic coords="6,51.47,58.13,246.02,73.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. The two-layer CRF models for image event prediction.</figDesc><graphic coords="6,86.51,176.33,175.22,132.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. The clustering results of different users (a)-(e) for multiple smart parts of a social network, where different user groups are represented in different colors.</figDesc><graphic coords="7,69.47,58.49,472.82,76.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>By assigning each user u (characterized by their multifactors for social behavior characterization) onto one or multiple of B representative social groups in the B-D discriminative dictionary D u , each user and his/her trustworthiness can be represented as a B-D histogram of representative social groups (i.e., a B-D bag of representative social groups x u ). Such B-D bags of representative social groups can characterize the user trustworthiness effectively (e.g., their closeness or similarities on their multi-factors for social behavior characterization can be used to characterize their trustworthiness in certain accuracy). For each user, such B-D bag of representative social groups x u is very sparse.VI. TREE CLASSIFIER FOR FINE-GRAINED PRIVACY SETTING RECOMMENDATIONWithout loss of generality, we consider the privacy policies that contain the following components: (a) Subject S: a set of users who are socially connected to the image owner u and are granted to access the shared images ; (b) Images : a set of images shared from u to S; (c) Action A: a set of actions granted by u to S on .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 11 .Fig. 12 .</head><label>1112</label><figDesc>Fig. 11. The flowchart of our binary approach for privacy setting recommendation.</figDesc><graphic coords="8,51.47,149.33,246.02,51.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. The flowchart of our binary approach for tree classifier training.</figDesc><graphic coords="8,314.51,58.61,246.02,126.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Our multi-class tree classifier training algorithm for supporting fine-grained privacy setting recommendation.</figDesc><graphic coords="9,314.51,58.49,246.02,190.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Examples to illustrate our multi-category approach for fine-grained privacy setting recommendation: (a) completely-share; (b) sharing with blurring (where the sensitive objects are blurred); (c) partially-share by replacing the objects with white silhouettes.</figDesc><graphic coords="10,90.47,58.13,430.34,118.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>= share-with-blurring 0.5, c = completely-share, ĉ = partially-share 0.5, c = completely-share, ĉ = share-with-blurring 0.5, c = completely-share, ĉ = not-share 1.0, c = not-share, ĉ = partially-share 1.0, c = not-share, ĉ = share-with-blurring 1.0, c = not-share, ĉ = completey-share<ref type="bibr" target="#b10">(11)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 16 .</head><label>16</label><figDesc>Fig.<ref type="bibr" target="#b15">16</ref>.The comparison on the effectiveness of our feature-based approach by using 1024-D deep features and the traditional one by using the 4096-D deep features extracted by the AlexNet, where the image sets are sorted according to their privacy disclosure.</figDesc><graphic coords="11,56.99,58.13,234.74,135.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 17</head><label>17</label><figDesc>Fig. 17. The comparison on the effectiveness of the feature-based approach when different types of features are extracted for image sensitiveness representation, where the image sets are sorted according to their privacy disclosure.</figDesc><graphic coords="11,65.51,256.13,217.82,139.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>Fig. 17. The comparison on the effectiveness of the feature-based approach when different types of features are extracted for image sensitiveness representation, where the image sets are sorted according to their privacy disclosure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. The comparison on the effectiveness of our approach by using the privacy-sensitive object classes and events and the traditional one by using 1000 atomic object classes directly, where the image sets are sorted according to their privacy disclosure. image content sensitiveness representation; (2) the traditional object-based approach: 1, 000 atomic object classes, which are originally detected by the AlexNet [39]-[41] for largescale visual recognition application, are arbitrarily used for image content sensitiveness representation and 1000-D bags of atomic object classes are used for image content sensitiveness representation.As shown in Fig.19, one can observe that our objectbased approach can significantly outperform the traditional object-based approach. The reason is that the privacy-sensitive object classes and events can characterize the image content sensitiveness effectively, on the other hand, 1000 atomic object classes<ref type="bibr" target="#b38">[39]</ref>-<ref type="bibr" target="#b43">[44]</ref>, that are usually extracted for image semantics interpretation, may not be able to characterize the image content sensitiveness exactly, e.g., the appearances of such 1000 atomic object classes in the images do not exactly relate with the image privacy or cause privacy disclosure directly. Even detecting such 1000 atomic object classes can play important roles on image semantics interpretation, they may not be effective for characterizing the image content sensitiveness precisely.</figDesc><graphic coords="12,50.99,58.37,246.02,133.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Fig. 20 .</head><label>20</label><figDesc>Fig. 20. Our experimental results on privacy-aware image classification, where the images in the public (share) category are visualized according to their visual similarities.</figDesc><graphic coords="13,83.51,58.73,444.50,186.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Fig. 21 .</head><label>21</label><figDesc>Fig. 21. The privacy-sensitive objects (in red boxes) that are identified from the images.</figDesc><graphic coords="13,51.47,297.17,246.02,96.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Fig. 22 .</head><label>22</label><figDesc>Fig. 22. User evaluation results on the interpret-ability of two approaches for privacy setting recommendation.</figDesc><graphic coords="13,65.51,438.53,217.82,76.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Fig. 23 .</head><label>23</label><figDesc>Fig. 23.The comparison between our feature-based approach and our object-based approach for privacy setting recommendation over PicAlert image set.</figDesc><graphic coords="13,55.19,560.33,234.74,96.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Fig. 24 .</head><label>24</label><figDesc>Fig. 24.The comparison between three approaches for privacy setting recommendation over Mirflickr image set: (a) our object-based approach; (b) baseline method by Zerr et al.<ref type="bibr" target="#b26">[27]</ref> and<ref type="bibr" target="#b27">[28]</ref>; (c) our feature-based approach, where 40 image subsets are sorted according to their privacy disclosure obtained by our feature-based approach.</figDesc><graphic coords="13,314.51,296.45,246.02,105.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Fig. 25 .</head><label>25</label><figDesc>Fig. 25. The effectiveness of using the privacy-sensitive object classes and events for image sensitiveness representation in PicAlert image set: (a) the correlations curve between our 280-D bags of privacy-sensitive object classes and the image content sensitiveness (privacy); (b) the privacy disclosure curve induced by our object-based approach.</figDesc><graphic coords="13,314.03,466.13,246.02,103.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Fig. 26 .</head><label>26</label><figDesc>Fig. 26. The effectiveness of using various deep features for image content sensitiveness representation in Mirflickr image set: (a) the correlations between the traditional 4096-D deep features and the image content sensitiveness; (b) the correlations between our 1024-D deep features and the image content sensitiveness; (c) the privacy disclosure induced by our feature-based approach.</figDesc><graphic coords="14,51.47,58.61,246.02,118.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>the most discriminative feature subset x best is determined as the image content sensitiveness representations x s , a binary SVM classifier f s (c | x s , θ) is first trained over R training images to obtain the optimal recognition of two categories (i.e., share &amp; not-share) for fine-grained privacy setting recommendation and a binary SVM classifier f u (c | x u , ϑ) is then trained over the associated set of users by using the feature subset x u . If the most discriminative feature subset x best is determined as the user trustworthiness characterizations x u , a binary SVM classifier f u (c | x u , ϑ) is first trained over T users to obtain the optimal recognition of two categories (i.e., share &amp; notshare) for fine-grained privacy setting recommendation and a binary SVM classifier f s (c | x s , θ) is then trained over the relevant training images by using the feature subset x s .</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Science Foundation under Grant 1651166-CNS and Grant 1651455-CNS and in part by the NSFC under Grant 61772161. The work of J. Yu was supported in part by the NSFC-61622205 and in part the NSFC-61472110.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A framework for computing the privacy scores of users in online social networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Terzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICDM</title>
		<meeting>IEEE ICDM</meeting>
		<imprint>
			<date type="published" when="2009-12">Dec. 2009</date>
			<biblScope unit="page" from="228" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">CoPE: Enabling collaborative privacy management in online social networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Squicciarini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Soc. Inf. Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="521" to="534" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Collocated photo sharing, story-telling, and the performance of self</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Van House</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Hum.-Comput. Stud</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="1073" to="1086" />
			<date type="published" when="2009-12">Dec. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Third-party apps on Facebook: Privacy and the illusion of control</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grossklags</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM CHIMIT</title>
		<meeting>ACM CHIMIT</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multiparty authorization framework for data sharing in online social networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. DBSec</title>
		<meeting>DBSec</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="29" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Moving beyond untagging: Photo privacy in a tagged world</title>
		<author>
			<persName><forename type="first">A</forename><surname>Besmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Lipford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI</title>
		<meeting>CHI</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1563" to="1572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Motivational, structural and tenure factors that impact online community photo sharing</title>
		<author>
			<persName><forename type="first">O</forename><surname>Nov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICWSM</title>
		<meeting>ICWSM</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="138" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Handling uncertain tags in visual recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2013-12">Dec. 2013</date>
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tag-based Web photo retrieval improved by batch mode re-tagging</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="3440" to="3446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Resolving tag ambiguity</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Slaney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Zwol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM MM</title>
		<meeting>ACM MM</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards automatic privacy management in Web 2.0 with semantic analysis on annotations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Squicciarini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CollaborateCom</title>
		<meeting>IEEE CollaborateCom</meeting>
		<imprint>
			<date type="published" when="2009-11">Nov. 2009</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic social group organization and privacy management</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Squicciarini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karumanchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Desisto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CollaborateCom</title>
		<meeting>CollaborateCom</meeting>
		<imprint>
			<date type="published" when="2012-10">Oct. 2012</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Privacy policy inference of user-uploaded images on content sharing sites</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Squicciarini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sundareswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wede</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="206" />
			<date type="published" when="2014-01">Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A3P: Adaptive policy prediction for shared images over popular content sharing sites</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Squicciarini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sundareswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wede</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd ACM Conf. Hypertext Hypermedia</title>
		<meeting>22nd ACM Conf. Hypertext Hypermedia</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Connecting content to community in social media via image content, user tags and user communication</title>
		<author>
			<persName><forename type="first">M</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Seligmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICME</title>
		<meeting>IEEE ICME</meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="1238" to="1241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tag, you can see it!: Using tags for access control in photo sharing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Klemperer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI</title>
		<meeting>CHI</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="377" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Capturing social networking privacy preferences</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Benisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Sadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SOUPS</title>
		<meeting>SOUPS</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Providing access control to online albums based on tags and linked data</title>
		<author>
			<persName><forename type="first">C.-M</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kagal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gibbins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shadbolt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Symp</title>
		<meeting>AAAI Symp</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inferring privacy policies for social networking services</title>
		<author>
			<persName><forename type="first">G</forename><surname>Denezis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Workshop Secur</title>
		<meeting>2nd Workshop Secur</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="5" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Social circles: Tackling privacy in social networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Adu-Opong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Gardiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Symp. Usable Privacy Secur. (SOUPS)</title>
		<meeting>Symp. Usable Privacy Secur. (SOUPS)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Privacy wizards for social networking sites</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lefevre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM WWW</title>
		<meeting>ACM WWW</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="351" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Characterizing privacy in online social networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Krishnamurphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Wills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st Workshop</title>
		<meeting>1st Workshop</meeting>
		<imprint>
			<publisher>Online Soc. Netw</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="37" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the need for user-defined fine-grained access control policies for social networking applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Simpson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop Secur</title>
		<meeting>Workshop Secur</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Monitoring and recommending privacy settings in social networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ghazinour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Matwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sokolova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EDBT/ICDT</title>
		<meeting>EDBT/ICDT</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="164" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to discover social circles in ego networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPDS</title>
		<meeting>NIPDS</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="539" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Social circle discovery in ego-networks by mining the latent structure of user connections and profile attributes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Petkos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kompatsiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2015-08">Aug. 2015</date>
			<biblScope unit="page" from="880" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Privacyaware image classification and search</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zerr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Siersdorfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Demidova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGIR</title>
		<meeting>ACM SIGIR</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">PicAlert!: A system for privacyaware image classification and retrieval</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zerr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Siersdorfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM CIKM</title>
		<meeting>ACM CIKM</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2710" to="2712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Analyzing images&apos; privacy for the modern Web</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Squicciarini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Caragea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balakavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Hypertext</title>
		<meeting>ACM Hypertext</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="136" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Privacy prediction of images shared on social media sites using deep features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tonge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Caragea</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1510.08583" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automatic privacy classification of personal photos</title>
		<author>
			<persName><forename type="first">D</forename><surname>Buschek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Van Zezschwitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Luca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">in Proc. LNCS</title>
		<meeting>LNCS</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9297</biblScope>
			<biblScope unit="page" from="428" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Personalized privacy-aware image classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Spyromitros-Xioufis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kompatsiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM ICMR</title>
		<meeting>ACM ICMR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">iPrivacy: Image privacy protection by identifying sensitive objects via deep multi-task learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Security</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1005" to="1016" />
			<date type="published" when="2017-05">May 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scrambling for privacy protection in video surveillance systems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dufaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1168" to="1174" />
			<date type="published" when="2008-08">Aug. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Toward privacy-preserving photo sharing</title>
		<author>
			<persName><forename type="first">M.-R</forename><surname>Ra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Govindan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th USENIX Symp. Netw. Design Implement</title>
		<meeting>10th USENIX Symp. Netw. Design Implement</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="515" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hiding privacy information in video surveillance system</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<idno>II-868-II-871</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICIP</title>
		<meeting>IEEE ICIP</meeting>
		<imprint>
			<date type="published" when="2005-09">Sep. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cybercasing the Joint: On the privacy implications of geo-tagging</title>
		<author>
			<persName><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX Workshop Hot Topics Secur</title>
		<meeting>USENIX Workshop Hot Topics Secur</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Privacy protection for social video via background estimation and CRF-based videographer&apos;s intention modeling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nakashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Babaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1221" to="1233" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM MM</title>
		<meeting>ACM MM</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">DeCAF: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkions</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1412.7062" />
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">From image-level to pixel-level labeling with convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="1713" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Conditional random fields as recurrent neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1502.03240" />
		<imprint>
			<date type="published" when="2015-02">Feb. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998-11">Nov. 1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Object bank: A high-level image representation for scene classification &amp; semantic feature sparsification</title>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1378" to="1386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Network dissection: Quantifying interpretability of deep visual representations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017-07">Jul. 2017</date>
			<biblScope unit="page" from="3319" to="3327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Solving the multiple instance problem with axis-parallel rectangles</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lozano-Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="31" to="71" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">From group to individual labels using deep features</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kotzias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD</title>
		<meeting>ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="597" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep multiple instance learning for image classification and auto-annotation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="3460" to="3469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Fully convolutional multi-class multiple instance learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Walmart Wants to Monitor Shoppers&apos; Facial Expressions</title>
		<ptr target="https://www.usatoday.com/story/money/2017/08/08/" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">I know what you did last summer: Risks of location data leakage in mobile and social computing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jedrzejczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Bandara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nuseibeh</surname></persName>
		</author>
		<idno>Rep. TR2009-11</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
			<pubPlace>Milton Keynes, U.K., Tech</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Open Univ</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Large-scale visual sentiment ontology and detectors using adjective noun pairs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st ACM Int. Conf. Multimedia</title>
		<meeting>21st ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="223" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Landmark classification in large-scale image collections</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 12th Int. Conf. Comput. Vis</title>
		<meeting>IEEE 12th Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2009-09">Sep. 2009</date>
			<biblScope unit="page" from="1957" to="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Autotagging Facebook: Social network context improves photo annotation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zickler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW)</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2008-06">Jun. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Tour the world: Building a Web-scale landmark recognition engine</title>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="1085" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Evaluation of face recognition techniques for application to facebook</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Ortiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th IEEE Int. Conf. Auto. Face Gesture Recognit. (FG)</title>
		<meeting>8th IEEE Int. Conf. Auto. Face Gesture Recognit. (FG)</meeting>
		<imprint>
			<date type="published" when="2008-09">Sep. 2008</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2015-09">Sep. 2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">100% accuracy in automatic face recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Burton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">319</biblScope>
			<biblScope unit="issue">5862</biblScope>
			<biblScope unit="page">435</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">DeepFace: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kruahenbuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1604.07379" />
		<imprint>
			<date type="published" when="2016-04">Apr. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1406.2661" />
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1611.07004" />
		<imprint>
			<date type="published" when="2016-11">Nov. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
